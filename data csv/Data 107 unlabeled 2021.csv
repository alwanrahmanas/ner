id,content
221710087,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pembangunan R Package pada Model Fay-Herriot 
Multivariat untuk Pendugaan Area Kecil Metode 
Ratio Benchmarking  

Zenda Oka Briantiko (221710087, 4SD2) 
Dosen Pembimbing: Dr. Azka Ubaidillah 

Ringkasan—  Small  Area  Estimation  (SAE)  merupakan  salah 
satu teknik estimasi parameter area kecil dengan cara meminjam 
kekuatan  area  sekitarnya.  SAE  dengan  model  Fay-Herriot 
menjadi  model  yang  paling  banyak  digunakan.  Fay-Herriot 
multivariat  merupakan  pengembangan  dari  model  Fay-Herriot 
univariat dengan memanfaatkan korelasi antarvariabel. Estimasi 
dari  model  multivariat  terbukti  lebih  efisien  dibandingkan 
dengan  model  univariat.  Namun,  konsistensi  agregasi  sebagai 
syarat  dari  official  statistics  tidak  bisa  didapatkan  dari  estimasi 
tidak  langsung.  Salah  satu  metode  yang  dilakukan  untuk 
ini  adalah  metode  benchmarking.  Ratio 
mengatasi  hal 
benchmarking 
satu  bentuk  dari  metode 
benchmarking  mempunyai  kekurangan  yaitu  tidak  tersedianya 
estimator Mean Squared Error (MSE) yang tidak bias. MSE dari 
ratio benchmarking  dapat  diestimasi dengan  metode  resampling, 
salah  satunya  dengan  parametric  bootstrap.  Penulis  membangun 
algoritma 
dan 
mengimplementasikannya  dalam  bentuk  R  Package.  Hasil 
estimasi menunjukkan bahwa package yang dibangun telah sesuai 
dan bisa digunakan. 

benchmarking 

sebagai 

model 

salah 

ratio 

dari 

Kata  Kunci—  Small  Area  Estimation,  Multivariate,  Ratio 

Benchmarking, Bootstrap, R Package. 

I.  LATAR BELAKANG 

Permintaan  statistik  area  kecil  meningkat  selama  beberapa 
tahun terakhir karena semakin banyak pembuat kebijakan yang 
menuntut  perkiraan  domain  kecil  untuk  digunakan  dalam 
pengambilan kebijakan. Kebijakan yang tepat yaitu kebijakan 
yang  berdasakan  data,  di  mana  data  diperoleh  dengan 
pengumpulan  data  melalui  sensus  maupun  survei.  Sensus 
sebagai pengumpulan data dengan cara cacah lengkap kurang 
mampu menjawab kebutuhan data secara up to date. Selain itu 
sensus  memerlukan  biaya  yang  cukup  besar.  Survei  sampel 
sebagai  cara  pengumpulan  data  sampel  dapat  dilaksanakan 
secara efektif dari segi waktu dan biaya. Akan tetapi, estimasi 
langsung dari survei sampel bias terhadap parameternya karena 
terdapat  sampling  error.  Semakin  sedikitnya  jumlah  sampel 
menyebabkan  sampling  error  semakin  besar,  sehingga 
estimasinya  semakin  tidak  reliable.  Small  Area  Estimation 
(SAE)  dapat  mengatasi  masalah  untuk  area  dengan  sampel 
yang  kurang  memadai.  SAE  dapat  meningkatkan  efektifitas 
ukuran  sampel  survei  dengan  meminjam  kekuatan  area 
tetangga  dan  hubungan  antara  kumpulan  auxiliary  variable 
dengan variable of interest [1]. 

SAE dengan model Fay-Herriot menjadi model yang paling 
banyak  digunakan  dalam  estimasi  area  kecil  [2].  Metode  dari 

(EBLUP).  Referensi 

model Fay-Herriot yang estimatornya diperoleh ketika varians 
random  effect  tidak  diketahui  disebut  sebagai  Empirical  Best 
Linear  Unbiased  Prediction 
[3] 
menunjukkan  bahwa  terdapat  banyak variabel penelitan yang 
memiliki korelasi kuat, di mana variabel-variabel tersebut dapat 
diestimasi  secara  bersama-sama  menggunakan  metode  SAE 
multivariat.  Dengan  menggunakan  keuntungan  korelasi  antar 
variabel, menunjukkan bahwa model-model multivariat dapat 
menghasilkan  estimator  yang  lebih  efisien  daripada  model-
model univariat [1]. 

lebih 

estimasi  yang 

Estimasi  tidak  langsung  dengan  menggunakan  SAE  dapat 
berbeda dengan estimasi langsung ketika diagregasikan. Hal ini 
menjadi masalah karena survei asli dirancang untuk mencapai 
akurasi  inferensia  tertentu  pada  tingkat  agregasi  yang  lebih 
tinggi  [4].  Di  sisi  lain,  estimasi  tidak  langsung  dengan  SAE 
reliable.  Metode 
menghasilkan 
benchmarking  memperkuat 
inferensia  suatu  model-based 
predictor  agar  agregasi  dari  estimasi  area-area  kecilnya  bisa 
sesuai dengan estimasi langsung (design based) yang reliable 
[5]. Namun,  terkadang  estimasi  langsung  untuk  beberapa  area 
tidak dapat diperoleh karena tidak adanya sampel. Area dengan 
ukuran sampel nol dapat diestimasi dengan bantuan informasi 
klaster [6], sehingga estimasi tidak langsungnya dapat diperoleh. 
Ratio  benchmarking  merupakan  salah  satu  metode  yang 
paling  banyak  digunakan  dalam  memodifikasi  penduga 
EBLUP. Namun, estimator Mean Squared Error (MSE) yang 
tidak bias tidak tersedia  [7]. Sebagai salah satu metode untuk 
memodifikasi penduga EBLUP yang paling banyak digunakan, 
peneliti  mengembangan  alat  komputasi  untuk  metode 
benchmarking. Peneliti mengembangkan alat komputasi  ratio 
benchmarking multivariat karena alat komputasi benchmarking 
univariat  telah  dibuat  dan  dikembangkan  oleh  peneiliti 
sebelumnya. 

Berdasarkan permasalahan di atas, penelitian ini akan fokus 
membangun  model  estimasi  dan  MSE  untuk 
ratio 
benchmarking  univariat  dan  multivariat  serta  penerapannya 
pada R package. R package tersebut terdiri dari fungsi estimasi 
dan MSE  untuk  ratio benchmaking  univariat dan multivariat. 
Kemudian peneliti akan mengimplementasikan R package pada 
data bangkitan dan data studi kasus. 

 1 / 8 

 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

II.  TUJUAN PENELITIAN 

Berdasarkan latar belakang, tujuan dilakukannya penelitian 

ini yaitu: 

1.  Mengkaji  dan  membangun  estimasi  dan  MSE  metode 
ratio  benchmarking  pada  model  Fay-Herriot  univariat 
dan multivariat. 

2.  Membangun  R  package  untuk  metode 

ratio 
benchmarking  pada  model  Fay-Herriot  univariat  dan 
multivariat. 

3.  Mengimplementasikan R package pada data bangkitan 

dan data studi kasus. 

III. PENELITIAN TERKAIT 

Tabel di bawah ini merupakan penelitian sebelumnya yang 

berkaitan dengan penelitian yang akan dilakukan. 

5  New Important 
Developments 
in Small Area 
Estimation 

Danny 
Pfeffermann 

6  Mean Squared 

Error of Non-
Sampled Area 
in Small Area 
Estimation 
Small Area 
Estimation 

7 

Faisal Haris dan 
Azka Ubaidillah 

J.N.K. Rao dan 
Isabel Molina 

Benchmarking 
memperkuat 
inferensia  dengan  memaksa 
prediktor  model-based  untuk 
sesuai dengan estimator design-
based  dimana  agregasi  dari 
design-based  estimator  yaitu 
reliable (11) 
Area dengan ukuran sampel nol 
dapat 
dengan 
diestimasi 
bantuan  dari  informasi  klaster 
(1) 

sangat 
yang 
Modifikasi 
banyak 
namun 
sederhana 
digunakan 
EBLUP 
untuk 
disebut  ratio  benchmarking. 
Penduga MSE orde kedua yang 
tidak bias tjdak tersedia (184). 

TABEL I 
Penelitian Terkait 

Penulis 

Azka 
Ubaidillah, 
Khairil Anwar 
Notodiputro, 
Anang Kurnia, 
dan I. Wayan 
Mangku 

Judul 

No 
1  Multivariate 
Fay-Herriot 
models for 
small area 
estimation with 
application to 
household 
consumption 
per capita 
expenditure in 
Indonesia 

Roberto 
Benavent dan 
Domingo 
Morales 

Novia 
Permatasari 

3 

2  Multivariate 
Fay-Herriot 
models for 
small area 
estimation 
Pembangunan 
Paket R pada 
Model Fay 
Herriot 
Multivariat 
untuk 
Pendugaan 
Arae Kecil 

4  Bayesian 

Benchmarking 
with 
Applications to 
Small Area 
Estimation 

G. S. Datta, M. 
Ghosh, R. 
Streorts, dan J . 
Maples 

Tertulis 
Small  Area  Estimation  (SAE) 
dapat meningkatkan efektivitas 
ukuran  sampel  survei  dengan 
meminjam  kekuatan  daerah 
tetangga  dan  hubungan  antara 
dengan 
variable 
auxiliary 
variable  of  interest  (2845  - 
2846). 

Kita dapat menunjukkan bahwa 
model Multivariate Fay-Herriot 
estimasi 
menghasilkan 
parameter  yang  lebih  efisien 
daripada model Univariate Fay-
Herriot (2859). 
Model 
telah 
Fay-Herriot 
menjadi salah satu model yang 
paling banyak digunakan dalam 
pendugaan area kecil (372). 

tersebut 

umumnya, 

terdapat 
Pada 
banyak variabel penelitian yang 
memiliki  korelasi  kuat,  tidak 
terkecuali  beberapa  variabel 
hasil 
survei  BPS.  Dengan 
menggunakan  SAE,  variabel-
dapat 
variabel 
diestimasi secara bersama-sama 
menggunakan  metode  SAE 
multivariat (2) 
Meskipun  estimasi  area  kecil 
berbasis model sangat berguna, 
satu  masalah  potensial  dengan 
estimasi  tersebut  adalah  ketika 
digabungkan, 
estimasi 
keseluruhan  untuk  wilayah 
luas 
geografis  yang 
mungkin  cukup  berbeda  dari 
estimasi langsung terkait, yang 
terakhir 
diyakini 
biasanya 
cukup  andal.  Ini  karena  survei 
asli  dirancang  untuk  mencapai 
akurasi inferensial tertentu pada 
tingkat  agregasi  yang 
lebih 
tinggi. (1) 

lebih 

IV. METODE PENELITIAN  

A.  Metode Analisis 

yang 

univariat 

Fay-Herriot Multivariat 
Model  Fay-Herriot  multivariat  merupakan  model  Fay-
Herriot 
dengan 
memperhitungkan  korelasi  antarvariabel.  Model  Fay-Herriot 
multivariat didefinisikan dalam dua  tahap. Misalkan populasi 
terbatas  dibagi  menjadi  𝐷  domain  dan  diasumsikan  𝜇𝑑 
dihubungkan  dengan  auxiliary  variable  𝑥𝑑  dan  termasuk 
random effect 𝑢𝑑, maka linking model dapat dituliskan sebagai 
berikut: 

dikembangkan 

𝜇𝑑 = 𝑥𝑑𝛽 + 𝑢𝑑,    𝑢𝑑~𝑁(0, 𝑉𝑢𝑑),     𝑑 = 1, … , 𝐷 

(1) 

dengan  𝜇𝑑 = (𝜇𝑑1, … , 𝜇𝑑𝑅)′  dan  𝑥𝑑 = 𝑑𝑖𝑎𝑔(𝑥𝑑1, … , 𝑥𝑑𝑅)𝑅∗𝑝 
dengan  𝑝 = ∑
.  𝛽  merupakan  vektor  kolom  dengan 
ukuran 𝑝 𝑥 1.  Selanjutnya  penduga  langsung  tidak  bias  atau 
disebut sampling model bisa dituliskan sebagai berikut: 

𝑅
𝑟=1

𝑝𝑟

𝑦𝑑 = 𝑥𝑑𝛽 + 𝑢𝑑 + 𝑒𝑑,    𝑒𝑑~𝑁(0, 𝑉𝑒𝑑) 

(2) 

dengan  menggabungkan  persamaan  (1)  dan  (2)    diperoleh 
model Fay-Herriot multivariat yang dituliskan sebagai berikut: 

𝑦 = 𝑋𝛽 + 𝑍𝑢 + 𝑒 

(3) 

dengan  𝑦  adalah  hasil  estimasi  langsung,  𝑋  adalah  matriks 
variabel  penyerta,  𝛽  adalah  nilai  koefisien  regresi,  𝑍  adalah 
matriks  identitas, 𝑢  adalah  pengaruh  acak  area  dan 𝑒 adalah 
sampling error. Komponen 𝑒 dan 𝑢 bersifat independen. 

Empirical Best Linear Unbiased Prediction (EBLUP) 
Empirical  Best  Linear  Unbiased  Prediction  (EBLUP) 
merupakan salah satu metode untuk mengestimasi model Fay-
Herriot  multivariat  dengan  mengganti  ragam  (𝑉𝑢)  pada 
persamaan  Best  Linear  Unbiased  Prediction  (BLUP)  dengan 
penduga  ragam  (𝑉𝑢̂ )  yang  diestimasi  menggunakan  metode 
Restricted Maximum Likelihood (REML) . Persamaan EBLUP 
dituliskan sebagai berikut: 

𝜇̂𝐸 = 𝑋𝛽̂

𝐸 + 𝑍𝑉̂𝑢𝑍′𝑉̂ −1(𝑦 − 𝑋𝛽̂
𝑉̂ = 𝑍𝑉̂𝑢𝑍′ + 𝑉𝑒 

𝐸) 

𝛽̂
𝐸 = (𝑋′𝑉̂ −1𝑋)

−1

𝑋′𝑉̂ −1𝑦 

(4) 
(5) 

(6) 

 2 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Ratio Benchmarking 
Benchmarking  yaitu  metode  yang  dilakukan  agar  hasil 
agregasi  estimasi  area  kecil  𝜇̂𝐸𝑑  sesuai  dengan  estimasi 
langsung  yang  sudah  reliable.  Metode  ini  dilakukan  dengan 
memodifikasi  𝜇̂𝐸𝑑.  Ratio  Benchmarking  merupakan  metode 
yang  dilakukan  dengan  mengalikan 𝜇̂𝐸𝑑  dengan  suatu  faktor 
ratio 
pengali  yang 

sama  yaitu 

 [7].  Model 

𝐷
∑
𝑑=1
𝐷
𝑑=1

∑

𝑤𝑑𝜇̂𝑑
𝑤𝑑𝜇̂𝐸𝑑

benchmarking dapat dituliskan sebagai berikut: 

𝜇̂𝑅𝐵𝑑 = 𝜇̂𝐸𝑑 (

𝐷
∑
𝑑=1
𝐷
∑
𝑑=1

𝑤𝑖𝜇̂𝑑
𝑤𝑖𝜇̂𝐸𝑑

) 

(7) 

Ratio  benchmarking  merupakan  modifikasi  EBLUP  yang 
sederhana  tetapi  paling  banyak  digunakan.  Namun,  estimator 
MSE yang tidak bias tidak tersedia, tidak seperti pada kasus 𝜇̂𝐸 
[8]. 

Estimasi dari Mean Squared Error (MSE) 
MSE  akan  berfungsi  sebagai  alat  untuk  membandingkan 
metode alternatif untuk memilih metode mana yang terbaik dan 
paling  akurat 
[2]  menunjukkan  bahwa 
mengestimasi  nilai  MSE  pada  model  Fay-Herriot  multivariat 
melalui persamaan Prasad-Rao yang dituliskan sebagai berikut: 

[9].  Referensi 

2) + 𝑔2𝑑(𝜎̂𝑢

2)]                   

2∗(𝑏)) + 𝑔2(𝜎̂𝑢

2∗(𝑏))]

−

1
𝐵

𝑚𝑠𝑒(𝜇̂𝐸𝑑) = 2[𝑔1𝑑(𝜎̂𝑢
𝐵
∑[𝑔1(𝜎̂𝑢
𝑏=1
𝐵
∑[𝜇̂𝐸𝑑(𝜎̂𝑢
𝑏=1
− 𝜇̂𝐸𝑑(𝜎̂𝑢

2(𝑏))]

1
𝐵

+

2

2∗(𝑏))

(14) 

EBLUP dengan informasi klaster  
Referensi  [6]  menunjukkan  bahwa  terdapat  pendekatan 
dengan  menambahkan  rata-rata  estimator  dari  random  effect 
area dan auxiliary variable di setiap klaster untuk memodifikasi 
intercept  maupun  slope  model  prediksi.  Model  estimasi 
EBLUP adalah sebagai berikut: 

𝜇̂𝐸 = 𝑋𝛽̂

𝐸 + 𝑍𝑢̂𝐸 

(15) 

Sedangkan penduga EBLUP untuk daerah tidak tersampel 

adalah sebagai berikut: 

 𝜇̂𝐸 = 𝑋𝛽̂

𝐸 

(16) 

Rata-rata  random  effect  untuk  daerah  tidak  tersampel 

𝑚𝑠𝑒(𝜇̂𝐸) = 𝑔1(𝜎̂𝑢

2) + 𝑔2(𝜎̂𝑢

2) + 2𝑔3(𝜎̂𝑢

2) 

(8) 

dituliskan sebagai berikut: 

dengan komponen sebagai berikut: 
2) = Γ𝑉𝑒 
2) = (1 − Γ)𝑋𝑄𝑋′(1 − Γ)′ 
′
2 ) Γ(𝑘)𝑉̂ Γ(𝑙)
2 , 𝜎̂𝑢𝑙
=   1, … , 𝑞  

2) = ∑ ∑ 𝑐𝑜𝑣(𝜎̂𝑢𝑘

𝑔2(𝜎̂𝑢

𝑔1(𝜎̂𝑢

(𝜎̂𝑢

, 𝑘, 𝑙

(9) 
(10) 

(11) 

𝑢̂̅

𝑑(𝑘) =

1
𝑚𝑘

𝑚𝑘

∑ 𝑢̂𝑑
𝑑=1

(17) 

𝑑(𝑘) = Rata-rata random effect pada klaster ke-𝑘 

𝑢̂̅
𝑚𝑘 = Banyaknya area sampel pada klaster ke-𝑘 
𝑢̂𝑑 = Random effect area sampel ke-𝑑 

di mana, Γ = 𝑍(𝑉̂𝑢)𝑍′𝑉̂ , Γ(𝑘) =

∂Γ
𝜕𝜎𝑢

2 , 𝑄 = (𝑥′𝑉−1𝑥)−1,

𝑐𝑜𝑣(𝜎̂𝑢𝑘

2 , 𝜎̂𝑢𝑙

2 ) = (𝐹𝑎,𝑏)

−1

MSE  ratio  benchmarking  tidak  tersedia,  maka  dilakukan 
proses  resampling  yaitu  dengan  parametric  bootstrap  untuk 
mengestimasi  nilai  MSE.  Bootstrap  resampling  dari  data 
sampel  dapat  digunakan  untuk  mengestimasi  MSE  EBLUPs, 
𝜇̂𝐸𝑑, dan MSE yang lebih kompleks [2]. Pendekatan 𝑚𝑠𝑒𝐵(𝜇̂𝐸) 
oleh Monte Carlo: 

𝐵

𝑚𝑠𝑒𝐵(𝜇̂𝐸) = 𝐵−1 ∑[𝜇̂𝐸𝑑

∗(𝑏) − 𝜇𝑑

∗(𝑏)]

𝑏=1

2

(12) 

Model  kedua,  hybrid  bootstrap  MSE  estimator  dapat 

dituliskan sebagai berikut: 

𝑀𝑆𝐸(𝜇̂𝐸𝑑) = [𝑔1𝑑(𝜎̂𝑢

2) + 𝑔2𝑑(𝜎̂𝑢

2)]

+ 𝐸(𝜇̂𝐸𝑑 − 𝜇̃𝐸𝑑)2 

(13) 

Model  ketiga,  MSE  parametric  bootstrap  bias  correction 

dapat dituliskan sebagai berikut: 

𝑑 

𝐸 + 𝑍𝑢̂̅

Dengan memasukkan informasi pada persamaan ke (17) ke 
dalam persamaan ke (16), maka diperoleh model EBLUP untuk 
daerah tidak tersampel yaitu sebagai berikut: 
𝜇̂𝐸 = 𝑋𝛽̂
MSE non-sampled area  
MSE  untuk  non-sampled  area  bisa  diperoleh  dengan 
memodifikasi 
[10] 
menunjukkan bahwa untuk memodifikasi estimator Prasad-Rao 
2), dan  Γ̅ dengan 
penulis  mengusulkan  menggunakan 𝑉𝑒̅ , 𝑔3̅̅̅(𝜎̂𝑢
informasi  klaster. 
estimator  Prasad-Rao  untuk 
mengestimasi area non-sampled adalah sebagai berikut: 
2)(𝑘) + 𝑔2(𝜎̂𝑢

Prasad-Rao.  Referensi 

2)(𝑘) + 2𝑔3̅̅̅(𝜎̂𝑢

𝑛𝑠) = 𝑔1(𝜎̂𝑢

estimator 

𝑚𝑠𝑒(𝜇̂𝐸

2)(𝑘) 

(18) 

(19) 

Jadi 

dengan komponen sebagai berikut: 
2)(𝑘) = Γ̅𝑉̅𝑒 
2)(𝑘) = (1 − Γ̅)𝑋𝑄𝑋′(1 − Γ̅)′ 

𝑔2(𝜎̂𝑢

𝑔1(𝜎̂𝑢

(20) 
(21) 

System Usability Scale (SUS) 
System  Usability  Scale  (SUS)  adalah  skala  sepuluh  item 
sederhana  yang  memberikan  pandangan  global 
tentang 
subjektif  penilaian  kegunaan  [11].  Sepuluh  item  tersebut 
dituliskan  dengan  𝑅1  sampai  dengan  𝑅10 .  SUS  memiliki 
bentuk  skala  likert  dengan  lima  pilihan  jawaban.  Cara 
menghitung skor SUS adalah sebagai berikut: 

 3 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
          𝑆𝑘𝑜𝑟 𝑆𝑈𝑆 = {(𝑅1 − 1) + (5 − 𝑅2)
+ (𝑅3 − 1) + (5 − 𝑅4)
+ (𝑅5 − 1) + (5 − 𝑅6)
+ (𝑅7 − 1) + (5 − 𝑅8)
+ (𝑅9 − 1) + (5 − 𝑅10)} ∗ 2,5 

(22) 

B.  Metode Pengumpulan Data 

Data  yang  digunakan  pada  penelitian  ini  terdiri  dari  data 
studi kasus dan data simulasi. Data studi kasus yang digunakan 
adalah  data  Susenas  Maret  2019  dan  Podes  2019  (Desa) 
Provinsi  D.I.  Yogyakarta  untuk  mengestimasi  pengeluaran 
makanan dan  non-makanan  di  seluruh  kecamatan  di  Provinsi 
D.I. Yogyakarta 2019. 

Data  simulasi  terdiri  dari  Data  on  fresh  milk  expenditure 
yang  diperoleh  dari  package  ‘sae’  dan  data  bangkitan  yang 
diperoleh dengan membangkitkan data. Kondisi data merujuk 
pada penelitian rujukan, yaitu penelitian [1] dan [2]. Pada data 
bangkitan digunakan nilai 𝐷 = 50, 100, dan 200 observasi dan 
𝑅 = 2 variabel. Simulasi ini bertujuan untuk validasi nilai MSE 
dari univariat dan multivariat EBLUP dan ratio benchmarking. 
Model simulasinya dapat dituliskan sebagai berikut: 

𝑦1 = 𝛽01 + 𝛽11𝑥1 + 𝛽21𝑥2 + 𝑢1 + 𝑒1 
𝑦2 = 𝛽02 + 𝛽12𝑥1 + 𝛽22𝑥2 + 𝑢2 + 𝑒2 
𝑢𝑑~𝑁(0, 𝑉𝑢𝑑),       𝑒𝑑~𝑁(0, 𝑉𝑒𝑑) 

(23) 
(24) 
(25) 

  𝜎𝑒22 = 0.2,

Inisiasi  pengaruh  acak  area  𝜎𝑢11 = 0.2  dan  𝜎𝑢22 = 0.3 , 
diperoleh 𝑉𝑢𝑑 = (𝜎𝑢𝑖𝑖)𝑖=1,…,𝑅 .  Inisiasi  sampling  error 𝜎𝑒11 =
 dan  kovarians  sampling  error  𝜎𝑒𝑖𝑗 =
0.1,
𝜌𝑒√𝜎𝑒𝑖𝑖𝜎𝑒𝑗𝑗  dengan  nilai  koefisien  korelasi  𝜌𝑒 = 0,  dan 
0.5, diperoleh 𝑉𝑒𝑑 = (𝜎𝑒𝑖𝑗)
. Inisiasi  𝛽01 = 5, 𝛽02 = 4,
𝑖,𝑗=1,…,𝑅
𝛽11 = −0.15, 𝛽12 = 0.1, 𝛽21 = 0.25,  dan  𝛽22 = −0.05 . 
Auxiliary 
 dan 
𝑥2~𝑈(9.5, 10.5) dan  penimbang  dibangkitkan 𝑤1,2~(10, 20) 
lalu diproporsikan setiap variabelnya. 

variable  𝑥  dibangkitkan  𝑥1~𝑁(10, 1)

Simulasi  dilakukan  untuk  menguji  perbandingan  MSE 
EBLUP  dan  ratio  benchmarking  untuk  univariat  dan 
multivariat. Langkah-langkah simulasi sebagai berikut: 
1.  Ulangi sebanyak 𝐼  =  50 kali (𝑖 = 1, … , 𝐼) 

(𝑖), 𝑦𝑑𝑟

(𝑖), 𝑢𝑑𝑟
1.1 Bangkitkan {(𝑒𝑑𝑟
1.2 Hitung nilai parameter 
(𝑖), 𝑑 = 1, … , 𝐷, 𝑟 = 1, 2 
(𝑖) = 𝑋𝑑𝛽 + 𝑢𝑑, 𝜎̂𝑢𝑟
𝐸𝑟
1.3 Hitung nilai MSE Prasad-Rao  

(𝑖), 𝑥𝑑𝑟): 𝑑 = 1, … , 𝐷, 𝑟 = 1,2} 

2(𝑖), 𝛽̂

 𝜇𝑑

𝑚𝑠𝑒𝑑

1(𝑖) = 𝑔1𝑑(𝜎̂𝑢

2) 
2) + 2𝑔3𝑑(𝜎̂𝑢
1.4 Bootstrap sebanyak 𝐵  =  200 kali (𝑏 = 1, … , 𝐵) 
{(

1.4.1  Bangkitkan 

2) + 𝑔2𝑑(𝜎̂𝑢

∗(𝑖𝑏), 𝑥𝑑𝑟): 𝑑 =
∗(𝑖𝑏), 𝑢𝑑𝑟
𝑒𝑑𝑟
2(𝑖) 
1, … , 𝐷, 𝑟 = 1,2},  mengambil  informasi  𝜎̂𝑢𝑟
dan 𝛽̂
2  dan 𝛽𝑟, 𝑟 = 1,2 

(𝑖) sebagai ganti dari 𝜎𝑢𝑟

∗(𝑖𝑏), 𝑦𝑑𝑟

𝐸𝑟

1.4.2  Hitung nilai parameter 
(𝑖) + 𝑢𝑑
(𝑖𝑏) = 𝑋𝑑𝛽̂
𝜇𝑑
𝑑 = 1, … , 𝐷, 𝑟 = 1, 2 

𝐸𝑟

∗(𝑖𝑏), 𝜎̂𝑢𝑟

2∗(𝑖𝑏), 𝛽̂

∗(𝑖𝑏), 𝛽̂
𝐵𝑟

∗(𝑖𝑏),  
𝐸𝑟

1.4.3  Hitung nilai berikut 

∗(𝑖𝑏) + 𝐼2𝑢̂𝐵𝑑
∗(𝑖𝑏) = 𝑋𝑑𝛽̂
∗(𝑖𝑏), selanjutnya estimasi 
𝜇̂𝐵𝑑
𝐵
∗(𝑖𝑏)  
ratio benchmarking  𝜇̂𝑅𝐵𝐵𝑑

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

∗(𝑖𝑏) + 𝐼2𝑢̂𝐸𝑑

∗(𝑖𝑏),

 selanjutnya  estimasi 

∗(𝑖𝑏) = 𝑋𝑑𝛽̂
𝜇̂𝐸𝑑
𝐸
∗(𝑖𝑏)   
ratio  𝜇̂𝑅𝐵_𝐸𝑑
∗(𝑖𝑏) = ( 𝜇̂𝑅𝐵𝐸𝑑
𝛿𝑅𝐵𝑑

∗(𝑖𝑏) − 𝜇̂𝑅𝐵𝐵𝑑

∗(𝑖𝑏) )  

𝑚𝑠𝑒𝑑

2(𝑖) = 2[𝑔1𝑑

1.5 Untuk 𝑑 = 1, … , 𝐷, hitung 𝑚𝑠𝑒 bootstrap 
(𝑖)(𝜎̂𝑢
2(𝑖)) + 𝑔2𝑑
1
𝐵

(𝑖)(𝜎̂𝑢

(𝑖)(𝜎̂𝑢

2∗(𝑖𝑏))

2(𝑖))]

𝐵
∑[𝑔1𝑑
𝑏=1

−

+ 𝑔2𝑑

(𝑖)(𝜎̂𝑢

2∗(𝑖𝑏))] +

2.  Hitung nilai MSE Bootstrap 

𝑚𝑠𝑒𝑑

1 =

𝑚𝑠𝑒𝑑

2 =

1
𝐼

1
𝐼

𝐼
∑ 𝑚𝑠𝑒𝑑
𝑖=1
𝐼
∑ 𝑚𝑠𝑒𝑑
𝑖=1

1(𝑖)

2(𝑖)

1
𝐵

𝐷
∑ 𝛿∗(𝑖𝑏)𝛿∗(𝑖𝑏)𝑡
𝑏=1

C.  Tahapan Penelitian  

Penelitian dilakukan merujuk pada design science research 

dengan tahapan-tahapan sebagai berikut: 
1.  Problem identification 

Pada  tahap  ini  peneliti  melakukan  identifikasi  masalah, 
perumusan  masalah,  dan  perumusan  tujuan  penelitian 
terkait model ratio benchmarking. 
2.  Define the objective for a solution 

ini  dilakukan  pencarian  solusi  dengan 

tahap 

Pada 
melakukan studi literatur. 
3.  Design and development 
tahap 

Pada 
benchmarking dan pembuatan R Package. 

ini  dilakukan  pembuatan  model  ratio 

4.  Demonstration 

Pada tahap ini dilakukan implementasi pada data bangkitan 
dan data studi kasus.  

5.  Evaluation 

Hasil dari demonstration selanjutnya dievaluasi dengan uji 
validitas, uji performa, dan uji SUS. 

6.  Communication 

Pada tahap ini penulis mendokumentasikan R Package pada 
situs CRAN, memaparkan hasil penelitian di seminar, dan 
mendokumentasikan dalam bentuk hard file. 

V.  KERANGKA PIKIR 

Kerangka  pikir  pada  penelitian 

ini  menggambarkan 
kebutuhan  aplikasi  untuk  estimasi  small  area  estimation 
multivariat yang konsisten yaitu dengan metode benchmarking 
yang dapat ditunjukkan pada gambar berikut. 

 4 / 8 

 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Berdasarkan gambar 2 terlihat bahwa MSE bootstrap model 
pertama  memiliki  pola  yang  mirip  dengan  MSE  Prasad-Rao. 
Namun,  nilainya  cenderung  berbeda.  Sedangkan  MSE 
bootstrap model kedua dan ketiga memiliki pola dan nilai yang 
mendekati  MSE  Prasad-Rao.  Ketiga  model 
tersebut 
dibandingkan  dengan  melihat  perbandingan  rata-rata  dan 
standar  deviasi  dari  selisih  tiap-tiap  model  MSE  bootstrap 
terhadap MSE Prasad-Rao . Perbandingan tersebut dapat dilihat 
pada tabel berikut: 

TABEL II 
Perbandingan MSE Bootstrap 

Selisih I 

Selisih II 

Selisih III 

Rata-rata 

-6,4 x 10−4 

9,24 x 10−4 

2,34 x 10−5 

Standar 
Deviasi 

7,77 x 10−3 

7,09 x 10−4 

5,07 x 10−5 

Berdasarkan  Tabel  II  dapat  ditunjukkan  bahwa  nilai  rata-
rata  dan  standar  deviasi  dari  selisih  MSE  bootstrap  model 
ketiga  terhadap  MSE  Prasad-Rao  lebih  kecil  dibandingan 
dengan  selisih  MSE  bootstrap  model  pertama  dan  kedua 
terhadap MSE Prasad-Rao. Maka dari itu penulis menggunakan 
MSE  bootstrap  model  ketiga  atau  MSE  bootstrap  bias 
correction untuk mengestimasi MSE dari ratio benchmarking. 
Dengan mengganti parameter 𝜇̂𝐸𝑑 dengan 𝜇̂𝑅𝐵𝑑, persamaan 
untuk  MSE  ratio  benchmarking  dapat  dituliskan  sebagai 
berikut: 

     𝑚𝑠𝑒(𝜇̂𝑅𝐵𝑑) = 2[𝑔1𝑑(𝜎̂𝑢

2) + 𝑔2𝑑(𝜎̂𝑢

2)]

−  

1
𝐵

𝐵
∑[𝑔1(𝜎̂𝑢
𝑏=1

2∗(𝑏)) + 𝑔2(𝜎̂𝑢

2∗(𝑏))]

+

1
𝐵

𝐵
∑[𝜇̂𝑅𝐵𝑑(𝜎̂𝑢
𝑏=1

2∗(𝑏))

− 𝜇̂𝑅𝐵𝑑(𝜎̂𝑢

2(𝑏))]

2

(27) 

Langkah-langkah  estimasi  MSE  dengan  parametric 

bootstrap dapat dituliskan sebagai berikut:  
1.  Estimasi nilai random effect (𝜎̂𝑢
2.  Hitung komponen 𝑔1(𝜎̂𝑢
3.  Bootstrap B kali (𝑏 = 1, … , 𝐵) 

2) dan 𝑔2(𝜎̂𝑢

2) 

2) dan koefisien beta (𝛽̂) 

3.1 Bangkitkan 𝑒𝑑𝑟
3.2 Hitung  nilai  parameter  (𝜇𝑑

∗(𝑏), 𝑢𝑑𝑟

∗(𝑏) 

∗(𝑏)) 

(𝜇̂𝑑

∗(𝑏))  dan  direct  bootstrap 

2∗(𝑏)) dan beta (𝛽̂ ∗(𝑏)) 

3.3 Estimasi nilai random effect (𝜎̂𝑢
3.4 Hitung EBLUP bootstrap I: (𝜇̂𝐵𝑑) 
3.5 Estimasi ratio benchmarking bootstrap I: (𝜇̂𝑅𝐵𝐵𝑑
3.6 Hitung komponen 𝑔1(𝜎̂𝑢
3.7 Estimasi EBLUP bootstrap II: (𝜇̂𝐸𝑑) 
3.8 Estimasi ratio benchmarking bootstrap II: (𝜇̂𝑅𝐵𝐸𝑑

2∗(𝑏))dan 𝑔2(𝜎̂𝑢

2∗(𝑏)) 

∗(𝑏) ) 

∗(𝑏) ) 

Gambar 1. Kerangka Pikir 

VI. HASIL DAN PEMBAHASAN 

A.   Design and development 

Model estimasi ratio benchmarking multivariat 
Model  multivariat 

ratio  benchmarking  merupakan 
modifikasi  dari  model  univariat  ratio  benchmarking  pada 
persamaan  ke 
(7).  Pengaplikasian  multivariat  hanya 
memanfaatkan  korelasi  antar  variabel  untuk  mengestimasi 
EBLUP,  Sehingga  model  ratio  benchmarking  multivariat 
merupakan  model  univariat  setiap  variabelnya.  Model  ratio 
benchmarking multivariat dapat dituliskan sebagai berikut: 

𝜇̂𝑅𝐵𝑑𝑟 = 𝜇̂𝐸𝑑𝑟 (

𝐷
∑
𝑑=1
𝐷
∑
𝑑=1

𝑤𝑑𝑟𝜇̂𝑑𝑟
𝑤𝑑𝑟𝜇̂𝐸𝑑𝑟

) 

(26) 

Estimasi MSE dengan parametric bootstrap 
Dari  ketiga  model  bootstrap  akan  dipilih  model  terbaik 
yang  paling  mendekati  estimasi  MSE  Prasad-Rao.  Berikut 
merupakan grafik perbandingan MSE Prasad-Rao dengan tiga 
model 
dilakukan 
bootstrap. 
menggunakan data milk yang terdapat pada package ‘sae’. 

pendekatan 

Pemodelan 

Gambar 2. Perbandingan MSE Prasad-Rao dengan MSE bootstrap 

4.  Hitung MSE ratio benchmarking:  

 5 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
     𝑚𝑠𝑒(𝜇̂𝑅𝐵𝑑) = 2[𝑔1𝑑(𝜎̂𝑢

2) + 𝑔2𝑑(𝜎̂𝑢

2)]

−

+

1
𝐵

1
𝐵

𝐵
∑[𝑔1(𝜎̂𝑢
𝑏=1
𝐵
∑[𝜇̂𝑅𝐵𝑑(𝜎̂𝑢
𝑏=1

2∗(𝑏)) + 𝑔2(𝜎̂𝑢

2∗(𝑏))]

2∗(𝑏)) − 𝜇̂𝑅𝐵𝑑(𝜎̂𝑢

2(𝑏))]

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

3.  Folder R 

Folder ini berisi script empat function dan satu dataset. 

4.  Folder data 

Folder ini berisi dataset dengan format ‘rda’. 

5.  Folder man 

Folder  ini  berisi  komponen  dan  informasi  penting  yang 
berkaitan dengan function. 

2

Metode  bootstrap 

ini  mempunyai  kelemahan  yaitu 
penduganya  bisa  negatif.  Seperti  yang  dijelaskan  oleh  [13] 
bahwa penulis tidak merekomendasikan penggunaan bootstrap 
jika 𝜎̂𝑢 terlalu mendekati nol. 

Pembuatan R Package 
R  Package  yang  dibangun  bernama  ‘msaeRB’  yang 
merupakan singkatan dari Multivariate Small Area Estimation 
Ratio Benchmarking. Package ini dependent dengan beberapa 
package, diantaranya yaitu ‘MASS’, ‘abind’, ‘magic’, ‘Matrix’, 
dan  ‘stats’.  Package  ini  terdiri  dari  delapan  function dan  dua 
dataset.  Function  tersebut  yaitu  est_saeRB(),  est_msaeRB(), 
est_saeRBns(), est_msaeRBns(), mse_saeRB(),  mse_msaeRB(), 
mse_saeRBns(). Berikut merupakan rincian dari deskripsi tiap-
tiap function. 

Function 

est_saeRB() 

est_msaeRB() 

mse_saeRB() 

mse_msaeRB() 

est_saeRBns() 

est_msaeRBns() 

mse_saeRBns() 

mse_msaeRBns() 

TABEL III 
Deskripsi Function 

Deskripsi 

Estimasi EBLUP dan ratio 
benchmarking univariat 
Estimasi EBLUP dan ratio 
benchmarking multivariat 
MSE EBLUP dan ratio 
benchmarking univariat 
MSE EBLUP dan ratio 
benchmarking multivariat 
Estimasi EBLUP dan ratio 
benchmarking univariat untuk 
area tidak tersampel 
Estimasi EBLUP dan ratio 
benchmarking multivariat untuk 
area tidak tersampel 
MSE EBLUP dan ratio 
benchmarking univariat untuk 
area tidak tersampel 
MSE EBLUP dan ratio 
benchmarking multivariat untuk 
area tidak tersampel 

Package msaeRB dibangun dari algoritma dan pemodelan 
yang  sudah  disusun  dengan  komponen-komponen  sebagai 
berikut: 
1.  File DESCRIPTION 

File ini berisi nama package, author(s), maintainer, version, 
description, link repository, dan informasi lainnya. 

2.  File NAMESPACE 

lain 

File  ini berisi informasi tentang interaksi  package dengan 
package 
function 
“importFrom(‘package’,  ‘function’)  dan  interaksi  dengan 
pengguna 
function 
dituliskan 
yang 
“export(‘function’). 

dituliskan 

dengan 

dengan 

yang 

B.  Evaluasi 

Uji Validitas 
Uji  validitas  digunakan  untuk  menguji  apakah  algoritma 
yang dibuat telah menghasilkan output yang sesuai dan benar. 
Uji  validitas  ini  akan  membandingkan  hasil  estimasi  MSE 
univariat dan multivariat EBLUP dan Ratio Benchmarking. 

TABEL IV 
Perbandingan Rata-Rata MSE 

𝐷 

𝜌𝑒 

Metode 

EBLUP 

Variabel 1  Variabel 2 
0,128949 
0,070561 

50 

100 

200 

0 

0,5 

0 

0,5 

0 

0,5 

Ratio Benchmarking 

0,070456 

0,128588 

EBLUP 

0,065498 

0,120429 

Ratio Benchmarking 

0,065438 

0,120524 

EBLUP 

0,068660 

0,125543 

Ratio Benchmarking 

0,068663 

0,125295 

EBLUP 

0,064250 

0,116047 

Ratio Benchmarking 

0,064245 

0,116017 

EBLUP 

0,068604 

0,119801 

Ratio Benchmarking 

0,068329 

0,120028 

EBLUP 

0,063196 

0,114284 

Ratio Benchmarking 

0,063057 

0,113811 

Untuk  domain  dan  metode  yang  sama,  hasil  MSE 
multivariat selalu lebih kecil daripada MSE univariat, sehingga 
dapat  dikatakan  bahwa  model  multivariat  menghasilkan 
estimasi yang lebih efisien daripada model univariat. Selain itu, 
semakin banyaknya jumlah sampel, maka MSE semakin kecil. 
Selanjutnya  pada  studi  kasus  akan  dibandingkan  estimasi 
langsung dengan estimasi tidak langsung secara multivariat. 

Uji Performa 
Uji  performa  digunakan  untuk  melihat  keadaan  program 
yang telah dibuat dengan melihat waktu komputasi untuk tiap-
tiap  simulasi.  Waktu  komputasi  untuk  tiap-tiap  simulasi 
disajikan dalam tabel di bawah ini. 

TABEL V 
Waktu Komputasi  

Function 

est_saeRB() 

est_msaeRB() 

Jumlah Sampel 
100 

50 

200 

0,341 s 

0,394 s 

3,001 s 

0,403 s 

0,772 s 

4,192 s 

 6 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
mse_saeRB() 

mse_msaeRB() 

1,963 m 

12,259 m 

1,470 h 

2,998 m 

17,677 m 

2,150 h 

dan 

est_msaeRB(), 
lebih 

Berdasarkan  waktu  komputasinya,  dapat  diurutkan  model 
dari  yang  terlama  yaitu  sebagai  berikut:  mse_msaeRB(), 
est_saeRB.  Fungsi 
mse_saeRB(), 
multivariat  cenderung 
lama  waktu  komputasinya 
dibandingkan  dengan  fungsi  univariat  karena  matriks  𝑉𝑒 
multivariat  mengandung  kovarians.  Selain  itu  fungsi  MSE 
cenderung  lebih  lama  waktu  komputasinya  dibandingkan 
fungsi  estimasi  karena  dalam  fungsi  MSE  algoritmanya 
menggunakan teknik bootstrap untuk mendapatkan nilai MSE. 
Kemudian  untuk  fungsi  non-sampel  dilakukan  uji  coba 
untuk melihat pengaruh banyaknya area non-sampel terhadap 
terhadap  fungsi 
waktu  komputasi.  Uji  coba  dilakukan 
est_msaeRBns() dan mse_msaeRBns() dengan domain D = 50 
area, dan banyaknya area-non sampel mulai dari 1 sampai 25 
area.  

Gambar 3. Hubungan banyaknya area non-sampel terhadap waktu komputasi 

tren  yang  menurun.  Hal 

Berdasarkan gambar 3 terlihat bahwa semakin banyak area 
yang tidak tersampel, waktu komputasinya akan semakin cepat 
dibuktikan  dengan  garis 
ini 
dikarenakan  pemrosesan  yg  dilakukan  pada  area  non-sampel 
adalah area tersampel yang diproses dengan algoritma estimasi 
ataupun  MSE  yang  terdapat  estimasi  varians  random  effect 
dengan  metode  REML  dengan  iterasi  sampai  mendapatkan 
hasil  konvergen,  maksimal  100  iterasi.  Selanjutnya,  random 
effect  untuk  area  non-sampel  diperoleh  dari  rata-rata  random 
effect area yang tersampel. 

Uji SUS 
Uji SUS dilakukan untuk mengetahui apakah program yang 
digunakan  dapat  diterima  pengguna  atau  tidak.  Uji  ini 
dilakukan  terhadap  8  responden  yang  merupakan  mahasiswa 
Politeknik  Statistika  STIS.  Skor  rata-rata  SUS  adalah  81,25 
yang berarti bahwa package yang dibangun telah dapat diterima. 

Studi Kasus 
Untuk melakukan estimasi tidak langsung dibutuhkan peran 
auxiliary  variable  yang  digunakan  untuk  menduga  rata-rata 
pengeluaran  makanan  ( 𝑌1)  dan  rata-rata  pengeluaran  non-
makanan (𝑌2) sehingga  dapat  meningkatkan  presisi  estimasi. 
Untuk  pemodelan  digunakan  42  variabel.  Auxiliary  variable 
berasal dari data Podes 2019 (Desa) yang diagregasi ke tingkat 
kecamatan.  Untuk  mendapatkan  variabel  yang  signifikan, 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

digunakanlah metode stepwise dengan hasil yang ditunjukkan 
pada tabel berikut:  

TABEL VI 
Hasil Seleksi Variabel dengan Metode Stepwise 

Variabel 
Respon 
(1) 

Variabel 
Penyerta 

(2) 

beta 

std.error 

t-statistics 

p-value 

(3) 

(4) 

(5) 

(6) 

(Intercept) 

5,448 

0,370 

14,734 

0,0000 

𝑌1 

𝑌2 

𝑋1 

𝑋2 

𝑋6 

𝑋8 

-0,053 

0,023 

-2,284 

0,0256 

0,331 

0,161 

2,053 

0,0440 

-0,234 

0,088 

-2,649 

0,0101 

0,060 

0,029 

2,054 

0,0440 

(Intercept) 

9,020 

0,804 

11,213 

0,0000 

𝑋1 

𝑋2 

𝑋3 

𝑋4 

𝑋5 

𝑋6 

𝑋7 

𝑋8 

𝑋9 

𝑋10 

-0,150 

0,050 

-2,987 

0,0040 

0,934 

0,364 

2,567 

0,0127 

0,494 

0,150 

3,301 

0,0016 

-0,988 

0,280 

-3,532 

0,0008 

0,624 

0,230 

2,716 

0,0086 

-0,666 

0,187 

-3,562 

0,0007 

0,341 

0,101 

3,358 

0,0013 

0,297 

0,064 

4,610 

0,0000 

0,448 

0,117 

3,841 

0,0002 

-0,670 

0,233 

-2,876 

0,0055 

Auxiliary variable yang digunakan yaitu jumlah SD negeri 
(𝑋1), jumlah MI negeri (𝑋2), Jumlah MI  swasta (𝑋3),  Jumlah 
SMP  swasta  (𝑋4),  Jumlah  SMA  swasta  (𝑋5),  Jumlah  SMK 
swasta (𝑋6), jumlah tempat praktik bidan (𝑋7), jumlah apotek 
(𝑋8),  jumlah  bank  umum  swasta  (𝑋9), dan  jumlah  koperasi 
industri kecil dan kerajinan rakyat (𝑋10). 

Berdasarkan  auxiliary  variable  di  atas,  diperoleh  jumlah 
klaster  optimal  yaitu  sebanyak  dua  klaster  untuk  masing-
masing  𝑌1  dan  𝑌2  dengan  menggunakan  silhouette  method. 
Banyaknya klaster optimal ditunjukkan oleh gambar 4 berikut. 

Gambar 4. Jumlah klaster optimal untuk variabel respon 𝑌1 (kiri) dan variabel 
respon 𝑌2 (kanan) 

 7 / 8 

 
 
 
  
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

2.  R  package  untuk  metode  ratio  benchmarking  pada  model 
Fay-Herriot  multivariat  telah  berhasil  dibangun  dengan 
nama msaeRB. dan dapat diakses pada situs CRAN melalui 
https://cran.r-project.org/web/packages/msaeRB/  

3.  R  package  telah  berhasil  diimplementasikan  pada  data 

bangkitan dan data studi kasus. 

Dari hasil dan kesimpulan penelitian, penulis memberikan 

saran sebagai berikut: 
1.  Model  ratio  benchmarking  multivariat  sebagai  salah  satu 
dari  lima  metode  benchmarking  telah  berhasil  dibangun 
secara  multivariat.  Namun,  tiga  metode  benchmarking 
lainnya  belum  dikembangkan.  Maka  dari  itu,  dapat 
dikembangkan  tiga  model  benchmarking  lainnya  secara 
multivariat. 

2.  Function  yang  dibangun  masih  menghasilkan  waktu 
pengolahan  yang  lama,  maka  dari  itu  perlu  dilakukan 
optimasi terhadap implementasi algoritma. 

3.  Pada  studi  kasus,  pemilihan  variabel  penyerta  dapat 
dilakukan dengan lebih baik lagi sehingga dapat dihasilkan 
model dan hasil estimasi yang lebih baik juga. 

DAFTAR PUSTAKA 
[1]  A.  Ubaidillah,  K.  A.  Notodiputro,  A.  Kurnia,  and  I.  W.  Mangku,  
“Multivariate  Fay-Herriot  models  for  small  area  estimation  with 
application to household consumption per capita expenditure in Indonesia,” 
Journal of Applied Statistics, vol.46, pp. 2845-2861, 2019. 

[2]  R. Benavent and D. Morales. “Multivariate Fay-Herriot models for small 
area estimation,” Computational Statistics and Data Analysis, vol.94, pp. 
372-390, 2016. 

[3]  N. Permatasari. Pembangunan paket R pada model Fay Herriot multivariat 
untuk pendugaan area kecil. Jakarta: Politeknik Statistika STIS, 2020. 
[4]  G. S. Datta, M. Ghosh., R. Steorts and J. Maples, “Bayesian benchmarking 
with applications to small area estimation,”  TEST, vol.20, pp. 574-588, 
2011. 

[5]  D. Pfeffermann, “New Important Developments in Small Area Estimation.” 

Statistical Science, vol.28, pp. 40-68, 2013. 

[6]  F. Haris and A. Ubaidillah, “Mean Square Error of Non-Sampled Area in 
Small Area Estimation,” in 2019 International Conference on Statistics and 
Analytics. Bogor, Indonesia: ICSA, Jan 2019, pp. 1-12. 

[7]  J. N. K. Rao and I. Molina, Small Area Estimation 2nd Edition. New Jersey: 

John Wiley and Sons Inc, 2015. 

[8]  D.  Pfeffermann,  A.  Sikov,  and  R.  Tiller,  “Single-  and  two-stage  cross-
sectional  and  time  series  benchmarking  procedures  for  small  area 
estimation,” TEST, vol.23, pp. 631-666.  

[9]  P. P. Biemer and L. E. Lyberg. Introduction to Survey Quality. New Jersey: 

Wiley Series in Survey Methodology, 2003. 

[10] R. Anisa, A. Kurnia, and Indahwati, “Cluster Information of Non-sampled 
Area in Small Area Estimation,” IOSR Journal of Mathematics, vol.10, pp. 
15-19. 

[11] I.  H.  N,  P.  Nugroho  and  R.  Ferdiana,  “Pengujian  Usability  Website 
Menggunakan System Usability Scale,” IPTEK-KOM, vol.17, pp. 31-38, 
2015. 

[12] K. Peffers, T. Tuunanen, M. Rothenberger et al, “A design science research 
methodology for information systems research,” Journal of Management 
Information Systems, vol.24, pp. 45–77. 

[13] R. C. Steorts and M. Ghosh, “On Estimation of Mean Squared Errors of 
Empirical Bayes Estimators,” Statistics Sinica, vol.23, pp. 749-767. 

Adapun perbandingan hasil estimasi langsung dan estimasi 

tidak langsungnya ditunjukkan pada gambar 5. 

Gambar 5. Perbandingan estimasi langsung dan tidak langsung variabel 
respon 𝑌1 (kiri) dan variabel respon 𝑌2 (kanan) 

Berdasarkan  gambar  5  dapat  dilihat  bahwa  hasil  estimasi 
langsung,  EBLUP  multivariat,  dan  ratio  benchmarking 
multivariat  memiliki  pola  yang  mirip.  Kemudian  dilakukan 
perhitungan  agregasi  dari  ketiga  estimasi  tersebut  yang 
hasilnya disajikan dalam tabel berikut berikut. 

TABEL VII 
Agregasi Data Studi Kasus 

Variabel 
Respon 

Estimasi 
Langsung 

EBLUP 
Multivariat 

Ratio 
Benchmarking 
Multivariat 

𝑌1 

𝑌2 

5,359760 

5,255307 

5,359760 

7,470244 

6,976199 

7,470244 

Berdasarkan  Tabel  VII  dapat  dilihat  bahwa  agregasi  dari 
estimasi langsung sesuai dengan agregasi ratio benchmarking 
multivariat  untuk  masing-masing  variabel  𝑌1  dan  𝑌2.  Selain 
memiliki  agregasi  yang  sama  dengan  agregasi  estimasi 
langsung,  estimasi  dengan  ratio  benchmarking  multivariat 
terbukti lebih efisien. Terlihat dari gambar  6, secara rata-rata 
RSE dari ratio benchmarking multivariat lebih kecil daripada 
RSE estimasi langsung. 

Gambar 6. Perbandingan RSE estimasi langsung dan tidak langsung variabel 
respon 𝑌1 (kiri) dan variabel respon 𝑌2 (kanan) 

VII. 

PENUTUP 

Berdasarkan  hasil  dan  pembahasan,  didapatkan  beberapa 

kesimpulan sebagai berikut: 
1.  Estimasi dan MSE metode ratio benchmarking pada model 
telah  berhasil 
Fay-Herriot  univariat  dan  multivariat 
dibangun  dengan  nilai  agregasi  ratio  benchmarking  yang 
sudah sesuai dengan nilai agregasi dari estimasi langsung. 

 8 / 8 

 
 
 
 
 
 
 
  
 
 
 
"
221710086,"(cid:0)

(cid:0)

(cid:1)(cid:2)(cid:3)(cid:2)(cid:4)(cid:2)(cid:5)(cid:0)(cid:6)(cid:3)(cid:7)(cid:8)(cid:9)(cid:10)(cid:8)(cid:0)(cid:11)(cid:0)(cid:12)(cid:7)(cid:13)(cid:14)(cid:7)(cid:2)(cid:15)(cid:0)(cid:6)(cid:16)(cid:17)(cid:18)(cid:8)(cid:0)(cid:19)(cid:20)(cid:21)(cid:22)(cid:0)(cid:23)(cid:13)(cid:15)(cid:9)(cid:17)(cid:16)(cid:2)(cid:10)(cid:8)(cid:0)(cid:6)(cid:16)(cid:2)(cid:16)(cid:8)(cid:10)(cid:16)(cid:8)(cid:3)(cid:0)

(cid:12)(cid:24)(cid:15)(cid:25)(cid:2)(cid:26)(cid:14)(cid:17)(cid:26)(cid:2)(cid:26)(cid:0)(cid:27)(cid:28)(cid:29)(cid:30)(cid:28)(cid:31) !""(cid:0)(cid:17)(cid:26)(cid:16)(cid:17)(cid:3)(cid:0)(cid:12)(cid:24)(cid:26)(cid:18)(cid:17)(cid:14)(cid:2)(cid:2)(cid:26)(cid:0)#(cid:7)(cid:24)(cid:2)(cid:0)
(cid:23)(cid:24)$(cid:8)(cid:4)(cid:0)(cid:1)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)%(cid:2)&(cid:20)’(cid:24)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)(cid:1)(cid:17)(cid:4)(cid:16)(cid:8)((cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:18)(cid:24)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)(cid:1)(cid:24)(cid:16)(cid:13)(cid:18)(cid:24)(cid:0)
)*++ , -(cid:29) !. -(cid:29)/0(cid:28),(cid:30)*-(cid:31)!
(cid:0)

(cid:0)

1(cid:2)2(cid:2)(cid:0)3(cid:17)(cid:18)(cid:2)(cid:0)(cid:12)(cid:24)(cid:7)4(cid:8)(cid:7)(cid:2)(cid:0)56678799:;<(cid:0)=(cid:6)(cid:19)6>(cid:0)

(cid:19)(cid:13)(cid:10)(cid:24)(cid:26)(cid:0)(cid:12)(cid:24)(cid:15)(cid:25)(cid:8)(cid:15)(cid:25)(cid:8)(cid:26)(cid:14)?(cid:0)(cid:19)(cid:7)@(cid:0)#2(cid:3)(cid:2)(cid:0)A(cid:25)(cid:2)(cid:8)(cid:18)(cid:8)(cid:4)(cid:4)(cid:2)(cid:5)(cid:0)

(cid:0)

BCDEFGHGDIJKLMNOLJPQGRRSTUVGSWHXCQGXCYDJZ[\\]^JK_‘MabcdacMJ
eLd_fcgchJfLhiLejchichJOcdaJeLMNOLJ[\\]J_habcdacMJkchiJfcOcJ
LlMaeclahkcJeLefLdmaM_higchJgNdL‘claJchMcdJbcdacjL‘JdLlfNhhkcJ
lLmahiicJeLhimcla‘gchJLlMaeclaJkchiJ‘LjamJjcagnJKNOL‘Jockp
qLddaNMJK_‘MabcdacMJcOc‘cmJlLj_MchJ_hM_gJeNOL‘J[\\]JK_‘MabcdacMJ
kchiJ eLhii_hcgchJ LlMaeclaJ ]rstunJ uLhii_hcchJ [\\]J
K_‘MabcdacMJ_hM_gJYvvCwCGRSHXGXCHXCwHSmcd_lJeLeLh_maJlkcdcMJ
gNhlalMLhlaJcidLiclaJfcOcJcdLcJkchiJ‘LjamJjLlcdxJhce_hJfcOcJ
fLhiLlMaeclachJ[\\]JK_‘MabcdacMJeNOL‘J]rstuJMaOcgJOcfcMJ
eLhimcla‘gchJcidLiclaJkchiJgNhlalMLhJOLhichJLlMaeclaJ‘chil_hiJ
cdLcJkchiJ‘LjamJjLlcdySuLhL‘aMachJahaJeLhicz_gchJl_cM_JeLMNOLJ
{VDw|QGUFCDESfcOcJeNOL‘JockpqLddaNMJe_‘MabcdacMJ_hM_gJ
eLhzceahJgNhlalMLhlaJciidLiclaJmcla‘JLlMaeclaJfcOcJeNOL‘J}G~(cid:127)
(cid:128)VUUCYXJe_‘MabcdacMnJthM_gJeLe_OcmgchJfLhii_hcJOc‘ceJ
eLhicf‘agclagchJeNOL‘JkchiJOajLhM_gxJfLhL‘aMaJjLdmcla‘J
eLejchi_hJl_cM_J(cid:129)p(cid:130)GwFGEVS_hM_gJeNOL‘S(cid:131)VDw|QGUFCDES
(cid:132)(cid:133)RXC(cid:134)GUCGXVSPQGRRSTUVGSWHXCQGXCYDSkchiJOcfcMJOacglLlJfcOcJlaM_lJ
mMMf(cid:135)(cid:136)(cid:136)(cid:137)(cid:129)\\(cid:138)n(cid:129)pfdNzL(cid:139)MnNdi(cid:136)fc(cid:139)gciL(cid:140)elcL(cid:141)rnJ(cid:142)GwFGEVJkchiJ
ML‘cmJOajchi_hJgLe_OachJOaLbc‘_claJeLhii_hcgchJjLdjcicaJ_zaJ
lLfLdMaJbc‘aOaMclxJfLd(cid:143)NdecxJlM_OaJgcl_lxJOchJ[t[nJqcla‘JLbc‘_claJ
eLh_hz_gchJjcm(cid:144)cJ(cid:130)GwFGEVJkchiJML‘cmJOajchi_hJl_OcmJlLl_caJ
OchJ‘ckcgJOai_hcgchnJ
(cid:0)
(cid:145)GXGS(cid:145)(cid:133)DwCIJ[\\]xS(cid:132)(cid:133)RXC(cid:134)GUCGXVS}G~(cid:127)(cid:128)VUUCYX(cid:146)S(cid:131)VDw|QGUF(cid:146)SBnJ

(cid:21)@(cid:147)(cid:148)#(cid:149)#(cid:150)(cid:0)(cid:151)(cid:152)(cid:148)#(cid:23)#(cid:153)(cid:154)(cid:0)
(cid:151)(cid:2)(cid:18)(cid:2)(cid:26)(cid:0)(cid:12)(cid:17)(cid:10)(cid:2)(cid:16)(cid:0)(cid:6)(cid:16)(cid:2)(cid:16)(cid:8)(cid:10)(cid:16)(cid:8)(cid:3)(cid:0)(cid:2)(cid:18)(cid:2)(cid:4)(cid:2)(cid:5)(cid:0)(cid:25)(cid:2)(cid:18)(cid:2)(cid:26)(cid:0)(cid:9)(cid:24)(cid:15)(cid:24)(cid:7)(cid:8)(cid:26)(cid:16)(cid:2)(cid:5)(cid:0)&(cid:2)(cid:26)(cid:14)(cid:0)
(cid:25)(cid:24)(cid:7)(cid:16)(cid:2)(cid:26)(cid:14)(cid:14)(cid:17)(cid:26)(cid:14)(cid:155)(cid:2)4(cid:2)(cid:25)(cid:0)(cid:18)(cid:2)(cid:4)(cid:2)(cid:15)(cid:0)(cid:15)(cid:24)(cid:26)&(cid:24)(cid:18)(cid:8)(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)(cid:10)(cid:16)(cid:2)(cid:16)(cid:8)(cid:10)(cid:16)(cid:8)(cid:3)(cid:0)(cid:7)(cid:24)(cid:10)(cid:15)(cid:8)(cid:0)5(cid:156)++*(cid:29)*(cid:28)(cid:157)!
(cid:158)(cid:159)(cid:28)(cid:159)*(cid:158)(cid:159)*(cid:29)(cid:158)(cid:160)!(cid:18)(cid:8)(cid:0)(cid:21)(cid:26)(cid:18)(cid:13)(cid:26)(cid:24)(cid:10)(cid:8)(cid:2)@(cid:0)(cid:19)(cid:2)(cid:4)(cid:2)(cid:15)(cid:0)(cid:15)(cid:24)(cid:4)(cid:2)(cid:3)(cid:10)(cid:2)(cid:26)(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)(cid:16)(cid:17)(cid:14)(cid:2)(cid:10)(cid:0)(cid:26)&(cid:2)(cid:0)(cid:151)(cid:12)(cid:6)(cid:0)
(cid:15)(cid:24)(cid:4)(cid:2)(cid:3)(cid:10)(cid:2)(cid:26)(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)(cid:18)(cid:17)(cid:2)(cid:0)(cid:3)(cid:24)(cid:14)(cid:8)(cid:2)(cid:16)(cid:2)(cid:26)(cid:0)(cid:17)(cid:16)(cid:2)(cid:15)(cid:2)(cid:0)&(cid:2)(cid:8)(cid:16)(cid:17)(cid:0)(cid:6)(cid:24)(cid:26)(cid:10)(cid:17)(cid:10)(cid:0)(cid:18)(cid:2)(cid:26)(cid:0)(cid:6)(cid:17)(cid:7)((cid:24)(cid:8)@(cid:0)
(cid:6)(cid:17)(cid:7)((cid:24)(cid:8)(cid:0)&(cid:2)(cid:26)(cid:14)(cid:0)(cid:18)(cid:8)(cid:4)(cid:2)(cid:3)(cid:17)(cid:3)(cid:2)(cid:26)(cid:0)(cid:151)(cid:12)(cid:6)(cid:0)(cid:3)(cid:24)(cid:25)(cid:2)(cid:26)&(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)(cid:18)(cid:24)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)(cid:18)(cid:24)(cid:10)(cid:2)(cid:8)(cid:26)(cid:0)(cid:10)(cid:2)(cid:15)(cid:9)(cid:24)(cid:4)(cid:0)
&(cid:2)(cid:26)(cid:14)(cid:0)$(cid:17)(cid:3)(cid:17)(cid:9)(cid:0)(cid:17)(cid:26)(cid:16)(cid:17)(cid:3)(cid:0)(cid:15)(cid:24)(cid:26)(cid:14)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:10)(cid:24)$(cid:2)(cid:7)(cid:2)(cid:0)(cid:4)(cid:2)(cid:26)(cid:14)(cid:10)(cid:17)(cid:26)(cid:14)(cid:0)(cid:18)(cid:8)(cid:0)(cid:16)(cid:8)(cid:26)(cid:14)(cid:3)(cid:2)(cid:16)(cid:0)
(cid:9)(cid:7)(cid:13)((cid:8)(cid:26)(cid:10)(cid:8)(cid:0)(cid:18)(cid:2)(cid:26)(cid:0)(cid:3)(cid:2)(cid:25)(cid:17)(cid:9)(cid:2)(cid:16)(cid:24)(cid:26)<(cid:0)(cid:26)(cid:2)(cid:15)(cid:17)(cid:26)(cid:0)(cid:25)(cid:24)(cid:4)(cid:17)(cid:15)(cid:0)$(cid:17)(cid:3)(cid:17)(cid:9)(cid:0)(cid:17)(cid:26)(cid:16)(cid:17)(cid:3)(cid:0)
(cid:15)(cid:24)(cid:26)(cid:14)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:3)(cid:24)$(cid:2)(cid:15)(cid:2)(cid:16)(cid:2)(cid:26)(cid:0)(cid:10)(cid:24)$(cid:2)(cid:7)(cid:2)(cid:0)(cid:4)(cid:2)(cid:26)(cid:14)(cid:10)(cid:17)(cid:26)(cid:14)(cid:0)(cid:3)(cid:2)(cid:7)(cid:24)(cid:26)(cid:2)(cid:0)(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)(cid:16)(cid:24)(cid:7)(cid:155)(cid:2)(cid:18)(cid:8)(cid:0)
(cid:3)(cid:24)(cid:3)(cid:17)(cid:7)(cid:2)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)(cid:17)(cid:3)(cid:17)(cid:7)(cid:2)(cid:26)(cid:0)(cid:10)(cid:2)(cid:15)(cid:9)(cid:24)(cid:4)@(cid:0)#(cid:7)(cid:24)(cid:2)(cid:0)&(cid:2)(cid:26)(cid:14)(cid:0)&(cid:2)(cid:26)(cid:14)(cid:0)(cid:15)(cid:24)(cid:15)(cid:8)(cid:4)(cid:8)(cid:3)(cid:8)(cid:0)(cid:17)(cid:3)(cid:17)(cid:7)(cid:2)(cid:26)(cid:0)
(cid:10)(cid:2)(cid:15)(cid:9)(cid:24)(cid:4)(cid:0)&(cid:2)(cid:26)(cid:14)(cid:0)(cid:3)(cid:17)(cid:7)(cid:2)(cid:26)(cid:14)(cid:0)(cid:18)(cid:8)(cid:10)(cid:24)(cid:25)(cid:17)(cid:16)(cid:0)(cid:2)(cid:7)(cid:24)(cid:2)(cid:0)(cid:3)(cid:24)$(cid:8)(cid:4)@(cid:0)’(cid:2)(cid:10)(cid:8)(cid:4)(cid:0)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:4)(cid:2)(cid:26)(cid:14)(cid:10)(cid:17)(cid:26)(cid:14)(cid:0)
(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:2)(cid:7)(cid:24)(cid:2)(cid:0)(cid:3)(cid:24)$(cid:8)(cid:4)(cid:0)(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)(cid:15)(cid:24)(cid:15)(cid:8)(cid:4)(cid:8)(cid:3)(cid:8)(cid:0)(cid:26)(cid:8)(cid:4)(cid:2)(cid:8)(cid:0)(cid:158)(cid:159)(cid:28)-¡(cid:28),¡! ,,(cid:156),(cid:0)&(cid:2)(cid:26)(cid:14)(cid:0)(cid:25)(cid:24)(cid:10)(cid:2)(cid:7)(cid:0)
(cid:10)(cid:24)(cid:5)(cid:8)(cid:26)(cid:14)(cid:14)(cid:2)(cid:0)(cid:10)(cid:16)(cid:2)(cid:16)(cid:8)(cid:10)(cid:16)(cid:8)(cid:3)(cid:0)&(cid:2)(cid:26)(cid:14)(cid:0)(cid:18)(cid:8)(cid:5)(cid:2)(cid:10)(cid:8)(cid:4)(cid:3)(cid:2)(cid:26)(cid:0)(cid:16)(cid:8)(cid:18)(cid:2)(cid:3)(cid:0)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:18)(cid:8)(cid:5)(cid:2)(cid:26)(cid:18)(cid:2)(cid:4)(cid:3)(cid:2)(cid:26)(cid:0)¢7£@(cid:0)
(cid:12)(cid:24)(cid:7)(cid:15)(cid:2)(cid:10)(cid:2)(cid:4)(cid:2)(cid:5)(cid:2)(cid:26)(cid:0)(cid:10)(cid:17)(cid:7)((cid:24)(cid:8)(cid:0)(cid:16)(cid:24)(cid:7)(cid:10)(cid:24)(cid:25)(cid:17)(cid:16)(cid:0)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:18)(cid:8)(cid:2)(cid:16)(cid:2)(cid:10)(cid:8)(cid:0)(cid:18)(cid:24)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)
(cid:16)(cid:8)(cid:18)(cid:2)(cid:3)(cid:0)(cid:4)(cid:2)(cid:26)(cid:14)(cid:10)(cid:17)(cid:26)(cid:14)<(cid:0)&(cid:2)(cid:8)(cid:16)(cid:17)(cid:0)(cid:15)(cid:24)(cid:16)(cid:13)(cid:18)(cid:24)(cid:0)⁄0(cid:28)(cid:157)(cid:157)!¥, (cid:28)!ƒ(cid:158)(cid:159)*0(cid:28)(cid:159)*(cid:156)-!5(cid:6)#(cid:152)>@(cid:0)
(cid:6)#(cid:152)(cid:0)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:15)(cid:24)(cid:26)(cid:8)(cid:26)(cid:14)(cid:3)(cid:2)(cid:16)(cid:3)(cid:2)(cid:26)(cid:0)(cid:24)§(cid:24)(cid:3)(cid:16)(cid:8)((cid:8)(cid:16)(cid:2)(cid:10)(cid:0)(cid:17)(cid:3)(cid:17)(cid:7)(cid:2)(cid:26)(cid:0)(cid:10)(cid:2)(cid:15)(cid:9)(cid:24)(cid:4)(cid:0)(cid:18)(cid:8)(cid:0)(cid:2)(cid:7)(cid:24)(cid:2)(cid:0)
(cid:3)(cid:24)$(cid:8)(cid:4)(cid:0)(cid:18)(cid:24)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)(cid:15)(cid:24)(cid:15)(cid:8)(cid:26)(cid:155)(cid:2)(cid:15)(cid:0)(cid:3)(cid:24)(cid:3)(cid:17)(cid:2)(cid:16)(cid:2)(cid:26)(cid:0)(cid:18)(cid:2)(cid:7)(cid:8)(cid:0)(cid:2)(cid:7)(cid:24)(cid:2)(cid:0)&(cid:2)(cid:26)(cid:14)(cid:0)(cid:25)(cid:24)(cid:7)(cid:10)(cid:24)(cid:10)(cid:17)(cid:2)(cid:8)(cid:2)(cid:26)(cid:0)
(cid:18)(cid:2)(cid:26)(cid:0)(cid:5)(cid:17)(cid:25)(cid:17)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)(cid:2)(cid:26)(cid:16)(cid:2)(cid:7)(cid:2)(cid:0)(cid:9)(cid:24)(cid:17)(cid:25)(cid:2)(cid:5)(cid:0)(cid:9)(cid:24)(cid:26)&(cid:24)(cid:7)(cid:16)(cid:2)(cid:0)5(cid:28)¤'*(cid:157)*(cid:28),“!«(cid:28),*(cid:28)‹(cid:157) >(cid:0)(cid:18)(cid:2)(cid:26)(cid:0)
(cid:9)(cid:24)(cid:17)(cid:25)(cid:2)(cid:5)(cid:0)(cid:2)(cid:15)(cid:2)(cid:16)(cid:2)(cid:26)(cid:0)¢7£@(cid:0)(cid:0)

(cid:1)(cid:24)(cid:26)(cid:17)(cid:7)(cid:17)(cid:16)(cid:0)(cid:7)(cid:24)§(cid:24)(cid:7)(cid:24)(cid:26)(cid:10)(cid:8)(cid:0)¢6£(cid:0)(cid:25)(cid:2)(cid:5)4(cid:2)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:17)(cid:15)(cid:17)(cid:15)(cid:26)&(cid:2)(cid:0)(cid:16)(cid:24)(cid:7)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:25)(cid:2)(cid:26)&(cid:2)(cid:3)(cid:0)
((cid:2)(cid:7)(cid:8)(cid:2)(cid:25)(cid:24)(cid:4)(cid:0)(cid:9)(cid:24)(cid:26)(cid:24)(cid:4)(cid:8)(cid:16)(cid:8)(cid:2)(cid:26)(cid:0)(cid:15)(cid:24)(cid:15)(cid:8)(cid:4)(cid:8)(cid:3)(cid:8)(cid:0)(cid:3)(cid:13)(cid:7)(cid:24)(cid:4)(cid:2)(cid:10)(cid:8)(cid:0)&(cid:2)(cid:26)(cid:14)(cid:0)(cid:3)(cid:17)(cid:2)(cid:16)<(cid:0)(cid:16)(cid:8)(cid:18)(cid:2)(cid:3)(cid:0)(cid:16)(cid:24)(cid:7)(cid:3)(cid:24)$(cid:17)(cid:2)(cid:4)(cid:8)(cid:0)
(cid:25)(cid:24)(cid:25)(cid:24)(cid:7)(cid:2)(cid:9)(cid:2)(cid:0)((cid:2)(cid:7)(cid:8)(cid:2)(cid:25)(cid:24)(cid:4)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:10)(cid:17)(cid:7)((cid:24)(cid:8)(cid:0)(cid:151)(cid:12)(cid:6)@(cid:0)(cid:150)(cid:24)§(cid:24)(cid:7)(cid:24)(cid:26)(cid:10)(cid:8)(cid:0)¢7£(cid:0)(cid:15)(cid:24)(cid:26)&(cid:2)(cid:16)(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)
(cid:16)(cid:24)(cid:7)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:10)(cid:17)(cid:2)(cid:16)(cid:17)(cid:0)(cid:15)(cid:24)(cid:16)(cid:13)(cid:18)(cid:24)(cid:0)(cid:6)#(cid:152)(cid:0)&(cid:2)(cid:26)(cid:14)(cid:0)(cid:18)(cid:8)(cid:10)(cid:24)(cid:25)(cid:17)(cid:16)(cid:0)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)%(cid:2)&(cid:20)(cid:5)(cid:24)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)
(cid:15)(cid:17)(cid:4)(cid:16)(cid:8)((cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)&(cid:2)(cid:26)(cid:14)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:26)&(cid:2)(cid:0)(cid:15)(cid:24)(cid:15)(cid:9)(cid:24)(cid:7)(cid:5)(cid:8)(cid:16)(cid:17)(cid:26)(cid:14)(cid:3)(cid:2)(cid:26)(cid:0)(cid:3)(cid:13)(cid:7)(cid:24)(cid:4)(cid:2)(cid:10)(cid:8)(cid:0)
(cid:2)(cid:26)(cid:16)(cid:2)(cid:7)(cid:0)((cid:2)(cid:7)(cid:8)(cid:2)(cid:25)(cid:24)(cid:4)@(cid:0)(cid:1)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)(cid:8)(cid:26)(cid:8)(cid:0)(cid:15)(cid:2)(cid:15)(cid:9)(cid:17)(cid:0)(cid:15)(cid:24)(cid:26)(cid:14)(cid:5)(cid:2)(cid:10)(cid:8)(cid:4)(cid:3)(cid:2)(cid:26)(cid:0)(cid:9)(cid:24)(cid:26)(cid:14)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)
&(cid:2)(cid:26)(cid:14)(cid:0)(cid:4)(cid:24)(cid:25)(cid:8)(cid:5)(cid:0)(cid:25)(cid:2)(cid:8)(cid:3)(cid:0)(cid:3)(cid:2)(cid:7)(cid:24)(cid:26)(cid:2)(cid:0)(cid:15)(cid:24)(cid:15)(cid:2)(cid:26)§(cid:2)(cid:2)(cid:16)(cid:3)(cid:2)(cid:26)(cid:0)(cid:3)(cid:13)(cid:7)(cid:24)(cid:4)(cid:2)(cid:10)(cid:8)(cid:0)(cid:2)(cid:26)(cid:16)(cid:2)(cid:7)(cid:0)(cid:3)(cid:13)(cid:15)(cid:9)(cid:13)(cid:26)(cid:24)(cid:26)(cid:0)
((cid:2)(cid:7)(cid:8)(cid:2)(cid:25)(cid:24)(cid:4)(cid:0)&(cid:2)(cid:26)(cid:14)(cid:0)(cid:18)(cid:8)(cid:2)(cid:15)(cid:2)(cid:16)(cid:8)(cid:0)(cid:10)(cid:24)(cid:5)(cid:8)(cid:26)(cid:14)(cid:14)(cid:2)(cid:0)(cid:24)(cid:7)(cid:7)(cid:13)(cid:7)(cid:0)&(cid:2)(cid:26)(cid:14)(cid:0)(cid:18)(cid:8)(cid:5)(cid:2)(cid:10)(cid:8)(cid:4)(cid:3)(cid:2)(cid:26)(cid:0)(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)(cid:4)(cid:24)(cid:25)(cid:8)(cid:5)(cid:0)
(cid:3)(cid:24)$(cid:8)(cid:4)(cid:0)(cid:18)(cid:8)(cid:25)(cid:2)(cid:26)(cid:18)(cid:8)(cid:26)(cid:14)(cid:3)(cid:2)(cid:26)(cid:0)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)(cid:17)(cid:26)(cid:8)((cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)¢›£@(cid:0)

(cid:1)(cid:24)(cid:26)(cid:17)(cid:7)(cid:17)(cid:16)(cid:0)¢=£(cid:0)(cid:9)(cid:24)(cid:26)(cid:14)(cid:14)(cid:17)(cid:26)(cid:2)(cid:2)(cid:26)(cid:0)(cid:15)(cid:24)(cid:16)(cid:13)(cid:18)(cid:24)(cid:0)(cid:6)#(cid:152)(cid:0)(cid:15)(cid:24)(cid:15)(cid:8)(cid:4)(cid:8)(cid:3)(cid:8)(cid:0)(cid:3)(cid:13)(cid:26)(cid:10)(cid:24)(cid:3)(cid:17)(cid:24)(cid:26)(cid:10)(cid:8)(cid:0)
(cid:18)(cid:8)(cid:15)(cid:2)(cid:26)(cid:2)(cid:0)(cid:10)(cid:2)(cid:2)(cid:16)(cid:0)(cid:5)(cid:2)(cid:10)(cid:8)(cid:4)(cid:0)(cid:18)(cid:2)(cid:7)(cid:8)(cid:0)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:2)(cid:7)(cid:24)(cid:2)(cid:0)(cid:3)(cid:24)$(cid:8)(cid:4)(cid:0)(cid:18)(cid:8)(cid:2)(cid:14)(cid:7)(cid:24)(cid:14)(cid:2)(cid:10)(cid:8)(cid:3)(cid:2)(cid:26)<(cid:0)(cid:25)(cid:24)(cid:4)(cid:17)(cid:15)(cid:0)
(cid:16)(cid:24)(cid:26)(cid:16)(cid:17)(cid:0)(cid:10)(cid:24)(cid:10)(cid:17)(cid:2)(cid:8)(cid:0)(cid:18)(cid:24)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:4)(cid:2)(cid:26)(cid:14)(cid:10)(cid:17)(cid:26)(cid:14)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:2)(cid:7)(cid:24)(cid:2)(cid:0)&(cid:2)(cid:26)(cid:14)(cid:0)(cid:4)(cid:24)(cid:25)(cid:8)(cid:5)(cid:0)
(cid:25)(cid:24)(cid:10)(cid:2)(cid:7)@(cid:0)(cid:6)(cid:24)(cid:18)(cid:2)(cid:26)(cid:14)(cid:3)(cid:2)(cid:26)(cid:0)(cid:17)(cid:26)(cid:16)(cid:17)(cid:3)(cid:0)(cid:15)(cid:24)(cid:26)(cid:155)(cid:2)(cid:18)(cid:8)(cid:3)(cid:2)(cid:26)(cid:0)(cid:5)(cid:2)(cid:10)(cid:8)(cid:4)(cid:0)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:18)(cid:2)(cid:7)(cid:8)(cid:0)(cid:6)#(cid:152)(cid:0)
(cid:10)(cid:24)(cid:25)(cid:2)(cid:14)(cid:2)(cid:8)(cid:0)(cid:10)(cid:16)(cid:2)(cid:16)(cid:8)(cid:10)(cid:16)(cid:8)(cid:3)(cid:0)(cid:7)(cid:24)(cid:10)(cid:15)(cid:8)<(cid:0)(cid:5)(cid:2)(cid:10)(cid:8)(cid:4)(cid:0)&(cid:2)(cid:26)(cid:14)(cid:0)(cid:18)(cid:8)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:3)(cid:2)(cid:26)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:2)(cid:7)(cid:24)(cid:2)(cid:0)(cid:3)(cid:24)$(cid:8)(cid:4)(cid:0)
(cid:2)(cid:14)(cid:14)(cid:7)(cid:24)(cid:14)(cid:2)(cid:10)(cid:8)(cid:0)(cid:26)&(cid:2)(cid:0)(cid:5)(cid:2)(cid:7)(cid:17)(cid:10)(cid:0)(cid:3)(cid:13)(cid:26)(cid:10)(cid:8)(cid:10)(cid:16)(cid:24)(cid:26)(cid:0)(cid:18)(cid:2)(cid:26)(cid:0)(cid:4)(cid:24)(cid:25)(cid:8)(cid:5)(cid:0)(cid:24)§(cid:8)(cid:10)(cid:8)(cid:24)(cid:26)(cid:0)(cid:18)(cid:8)(cid:25)(cid:2)(cid:26)(cid:18)(cid:8)(cid:26)(cid:14)(cid:0)
(cid:2)(cid:14)(cid:14)(cid:7)(cid:24)(cid:14)(cid:2)(cid:10)(cid:8)(cid:0)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:4)(cid:2)(cid:26)(cid:14)(cid:10)(cid:17)(cid:26)(cid:14)(cid:0)&(cid:2)(cid:26)(cid:14)(cid:0)(cid:16)(cid:8)(cid:18)(cid:2)(cid:3)(cid:0)(cid:25)(cid:8)(cid:2)(cid:10)@(cid:0)(cid:150)(cid:24)§(cid:24)(cid:7)(cid:24)(cid:26)(cid:10)(cid:8)(cid:0)¢ﬁ£(cid:0)
(cid:15)(cid:24)(cid:26)(cid:155)(cid:24)(cid:4)(cid:2)(cid:10)(cid:3)(cid:2)(cid:26)(cid:0)(cid:25)(cid:2)(cid:5)4(cid:2)(cid:0)¢;£(cid:0)(cid:15)(cid:24)(cid:26)(cid:14)(cid:2)(cid:155)(cid:17)(cid:3)(cid:2)(cid:26)(cid:0)(cid:15)(cid:24)(cid:16)(cid:13)(cid:18)(cid:24)(cid:0)&(cid:2)(cid:26)(cid:14)(cid:0)(cid:18)(cid:8)(cid:3)(cid:24)(cid:26)(cid:2)(cid:4)(cid:0)
(cid:18)(cid:24)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0). -(cid:29)/0(cid:28),(cid:30)*-(cid:31)J(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:6)#(cid:152)@(cid:0)(cid:12)(cid:2)(cid:18)(cid:2)(cid:0)(cid:15)(cid:24)(cid:16)(cid:13)(cid:18)(cid:24)(cid:0)(cid:8)(cid:26)(cid:8)(cid:0)
(cid:15)(cid:24)(cid:26)(cid:14)(cid:5)(cid:2)(cid:10)(cid:8)(cid:4)(cid:3)(cid:2)(cid:26)(cid:0)(cid:10)(cid:17)(cid:2)(cid:16)(cid:17)(cid:0)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:16)(cid:13)(cid:7)(cid:0)&(cid:2)(cid:26)(cid:14)(cid:0)(cid:18)(cid:8)(cid:10)(cid:24)(cid:25)(cid:17)(cid:16)(cid:0),(cid:156)‹¤(cid:158)(cid:159)!ﬂ, ¡*(cid:29)(cid:159)(cid:156),!
(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:6)#(cid:152)@(cid:0)(cid:27), ¡*(cid:29)(cid:159)(cid:156),!(cid:6)#(cid:152)(cid:0)(cid:18)(cid:8)(cid:15)(cid:13)(cid:18)(cid:8)§(cid:8)(cid:3)(cid:2)(cid:10)(cid:8)(cid:0)(cid:10)(cid:24)(cid:5)(cid:8)(cid:26)(cid:14)(cid:14)(cid:2)(cid:0)(cid:10)(cid:24)$(cid:2)(cid:7)(cid:2)(cid:0)(cid:7)(cid:2)(cid:16)(cid:2)(cid:0)(cid:11)(cid:0)
(cid:7)(cid:2)(cid:16)(cid:2)(cid:0)(cid:6)#(cid:152)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:2)(cid:7)(cid:24)(cid:2)(cid:0)(cid:3)(cid:24)$(cid:8)(cid:4)(cid:0)(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)(cid:15)(cid:24)(cid:15)(cid:25)(cid:24)(cid:7)(cid:8)(cid:3)(cid:2)(cid:26)(cid:0)(cid:5)(cid:2)(cid:10)(cid:8)(cid:4)(cid:0)&(cid:2)(cid:26)(cid:14)(cid:0)(cid:3)(cid:13)(cid:26)(cid:10)(cid:8)(cid:10)(cid:16)(cid:24)(cid:26)(cid:0)
(cid:18)(cid:24)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:4)(cid:2)(cid:26)(cid:14)(cid:10)(cid:17)(cid:26)(cid:14)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:2)(cid:7)(cid:24)(cid:2)(cid:0)&(cid:2)(cid:26)(cid:14)(cid:0)(cid:4)(cid:24)(cid:25)(cid:8)(cid:5)(cid:0)(cid:25)(cid:24)(cid:10)(cid:2)(cid:7)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)
(cid:10)(cid:17)(cid:7)((cid:24)(cid:8)(cid:0)(cid:16)(cid:24)(cid:7)(cid:10)(cid:24)(cid:25)(cid:17)(cid:16)@(cid:0)

(cid:12)(cid:2)(cid:18)(cid:2)(cid:0)(cid:7)(cid:24)§(cid:24)(cid:7)(cid:24)(cid:26)(cid:10)(cid:8)(cid:0)¢ﬁ£(cid:0)(cid:16)(cid:24)(cid:4)(cid:2)(cid:5)(cid:0)(cid:18)(cid:8)(cid:3)(cid:2)(cid:155)(cid:8)(cid:0)(cid:25)(cid:24)(cid:25)(cid:24)(cid:7)(cid:2)(cid:9)(cid:2)(cid:0)(cid:15)(cid:24)(cid:16)(cid:13)(cid:18)(cid:24)(cid:0). -(cid:29)/0(cid:28),(cid:30)*-(cid:31)(cid:0)
(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)%(cid:2)&(cid:20)(cid:5)(cid:24)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)(cid:17)(cid:26)(cid:8)((cid:2)(cid:7)(cid:8)(cid:2)(cid:16)<(cid:0)(cid:2)(cid:26)(cid:16)(cid:2)(cid:7)(cid:2)(cid:0)(cid:4)(cid:2)(cid:8)(cid:26)(cid:0))*++ , -(cid:29) !
. -(cid:29)/0(cid:28),(cid:30)*-(cid:31)(cid:176)!""(cid:28)(cid:159)*(cid:156)!. -(cid:29)/0(cid:28),(cid:30)*-(cid:31)(cid:176)!–ﬂ(cid:159)*0(cid:28)(cid:157)!. -(cid:29)/0(cid:28),(cid:30)*-(cid:31)(cid:176)!
†(cid:156)¤‡,(cid:28)(cid:156)!. -(cid:29)/0(cid:28),(cid:30)*-(cid:31)(cid:176)!(cid:18)(cid:2)(cid:26)!(cid:28)¤(cid:31)0 -(cid:159) ¡!. -(cid:29)/0(cid:28),(cid:30)*-(cid:31)@(cid:0)
(cid:19)(cid:24)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)(cid:9)(cid:24)(cid:4)(cid:17)(cid:2)(cid:26)(cid:14)(cid:0)(cid:15)(cid:24)(cid:15)(cid:2)(cid:26)§(cid:2)(cid:2)(cid:16)(cid:3)(cid:2)(cid:26)(cid:0)(cid:15)(cid:24)(cid:16)(cid:13)(cid:18)(cid:24)(cid:0). -(cid:29)/0(cid:28),(cid:30)*-(cid:31)(cid:0)(cid:10)(cid:24)(cid:7)(cid:16)(cid:2)(cid:0)
(cid:15)(cid:24)(cid:15)(cid:2)(cid:26)§(cid:2)(cid:2)(cid:16)(cid:3)(cid:2)(cid:26)(cid:0)(cid:3)(cid:13)(cid:7)(cid:24)(cid:4)(cid:2)(cid:10)(cid:8)(cid:0)(cid:2)(cid:26)(cid:16)(cid:2)(cid:7)(cid:0)((cid:2)(cid:7)(cid:8)(cid:2)(cid:25)(cid:24)(cid:4)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:10)(cid:17)(cid:7)((cid:24)(cid:8)(cid:0)(cid:151)(cid:12)(cid:6)<(cid:0)
(cid:16)(cid:24)(cid:7)(cid:17)(cid:16)(cid:2)(cid:15)(cid:2)(cid:0)(cid:18)(cid:2)(cid:4)(cid:2)(cid:15)(cid:0)(cid:15)(cid:24)(cid:26)(cid:14)(cid:5)(cid:2)(cid:10)(cid:8)(cid:4)(cid:3)(cid:2)(cid:26)(cid:0)(cid:10)(cid:17)(cid:2)(cid:16)(cid:17)(cid:0)(cid:10)(cid:16)(cid:2)(cid:16)(cid:8)(cid:10)(cid:16)(cid:8)(cid:3)(cid:0)(cid:7)(cid:24)(cid:10)(cid:15)(cid:8)(cid:0)&(cid:2)(cid:26)(cid:14)(cid:0)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)
(cid:15)(cid:24)(cid:26)$(cid:2)(cid:3)(cid:17)(cid:9)(cid:0)(cid:5)(cid:8)(cid:26)(cid:14)(cid:14)(cid:2)(cid:0)(cid:4)(cid:24)((cid:24)(cid:4)(cid:0)(cid:16)(cid:24)(cid:7)(cid:3)(cid:24)$(cid:8)(cid:4)(cid:0)&(cid:2)(cid:26)(cid:14)(cid:0)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:18)(cid:8)(cid:5)(cid:2)(cid:26)(cid:18)(cid:2)(cid:4)(cid:3)(cid:2)(cid:26)<(cid:0)(cid:15)(cid:2)(cid:3)(cid:2)(cid:0)
(cid:9)(cid:24)(cid:26)(cid:14)(cid:24)(cid:15)(cid:25)(cid:2)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)(cid:15)(cid:24)(cid:16)(cid:13)(cid:18)(cid:24)(cid:0). -(cid:29)/0(cid:28),(cid:30)*-(cid:31)!(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)%(cid:2)&(cid:20)(cid:5)(cid:24)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)
(cid:15)(cid:17)(cid:4)(cid:16)(cid:8)((cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:10)(cid:2)(cid:26)(cid:14)(cid:2)(cid:16)(cid:0)(cid:18)(cid:8)(cid:9)(cid:24)(cid:7)(cid:4)(cid:17)(cid:3)(cid:2)(cid:26)@J(cid:150)(cid:24)§(cid:24)(cid:7)(cid:24)(cid:26)(cid:10)(cid:8)(cid:0)¢7£(cid:0)(cid:15)(cid:24)(cid:26)&(cid:24)(cid:25)(cid:17)(cid:16)(cid:3)(cid:2)(cid:26)(cid:0)
(cid:16)(cid:24)(cid:7)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:10)(cid:17)(cid:2)(cid:16)(cid:17)(cid:0)(cid:15)(cid:24)(cid:16)(cid:13)(cid:18)(cid:24)(cid:0)‹ -(cid:29)/0(cid:28),(cid:30)*-(cid:31)(cid:0)&(cid:2)(cid:26)(cid:14)(cid:0)(cid:9)(cid:2)(cid:4)(cid:8)(cid:26)(cid:14)(cid:0)(cid:10)(cid:24)(cid:18)(cid:24)(cid:7)(cid:5)(cid:2)(cid:26)(cid:2)(cid:0)
(cid:26)(cid:2)(cid:15)(cid:17)(cid:26)(cid:0)(cid:25)(cid:2)(cid:26)&(cid:2)(cid:3)(cid:0)(cid:18)(cid:8)(cid:14)(cid:17)(cid:26)(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)&(cid:2)(cid:8)(cid:16)(cid:17)(cid:0))*++ , -(cid:29) !. -(cid:29)/(cid:28)0(cid:28),(cid:30)*-(cid:31)@(cid:0)
(cid:6)(cid:24)(cid:4)(cid:2)(cid:8)(cid:26)(cid:0)(cid:8)(cid:16)(cid:17)(cid:0)(cid:155)(cid:17)(cid:14)(cid:2)(cid:0)(cid:9)(cid:24)(cid:7)(cid:4)(cid:17)(cid:0)(cid:18)(cid:8)(cid:25)(cid:2)(cid:26)(cid:14)(cid:17)(cid:26)(cid:0)(cid:10)(cid:17)(cid:2)(cid:16)(cid:17)(cid:0)(cid:150)(cid:0)ﬂ(cid:28)(cid:29)(cid:30)(cid:28)(cid:31) (cid:0)&(cid:2)(cid:26)(cid:14)(cid:0)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)
(cid:15)(cid:24)(cid:15)(cid:17)(cid:18)(cid:2)(cid:5)(cid:3)(cid:2)(cid:26)(cid:0)(cid:18)(cid:2)(cid:4)(cid:2)(cid:15)(cid:0)(cid:9)(cid:24)(cid:26)(cid:14)(cid:14)(cid:17)(cid:26)(cid:2)(cid:2)(cid:26)(cid:0)(cid:15)(cid:24)(cid:16)(cid:13)(cid:18)(cid:24)(cid:0)(cid:16)(cid:24)(cid:7)(cid:10)(cid:24)(cid:25)(cid:17)(cid:16)(cid:0)(cid:17)(cid:26)(cid:16)(cid:17)(cid:3)(cid:0)
(cid:18)(cid:8)(cid:16)(cid:24)(cid:7)(cid:2)(cid:9)(cid:3)(cid:2)(cid:26)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:18)(cid:2)(cid:16)(cid:2)(cid:0)(cid:151)(cid:12)(cid:6)@(cid:0)(cid:6)(cid:16)(cid:17)(cid:18)(cid:8)(cid:0)(cid:3)(cid:2)(cid:10)(cid:17)(cid:10)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:9)(cid:24)(cid:26)(cid:24)(cid:4)(cid:8)(cid:16)(cid:8)(cid:2)(cid:26)(cid:0)(cid:8)(cid:26)(cid:8)(cid:0)
(cid:2)(cid:18)(cid:2)(cid:4)(cid:2)(cid:5)(cid:0)(cid:12)(cid:24)(cid:26)(cid:14)(cid:24)(cid:4)(cid:17)(cid:2)(cid:7)(cid:2)(cid:26)(cid:0)(cid:15)(cid:2)(cid:3)(cid:2)(cid:26)(cid:2)(cid:26)(cid:0)(cid:18)(cid:2)(cid:26)(cid:0)(cid:26)(cid:13)(cid:26)(cid:0)(cid:15)(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:10)(cid:24)(cid:4)(cid:17)(cid:7)(cid:17)(cid:5)(cid:0)

(cid:0)

(cid:0)

(cid:1)(cid:2)(cid:3)(cid:2)(cid:4)(cid:2)(cid:5)(cid:0)(cid:6)(cid:3)(cid:7)(cid:8)(cid:9)(cid:10)(cid:8)(cid:0)(cid:11)(cid:0)(cid:12)(cid:7)(cid:13)(cid:14)(cid:7)(cid:2)(cid:15)(cid:0)(cid:6)(cid:16)(cid:17)(cid:18)(cid:8)(cid:0)(cid:19)(cid:20)(cid:21)(cid:22)(cid:0)(cid:23)(cid:13)(cid:15)(cid:9)(cid:17)(cid:16)(cid:2)(cid:10)(cid:8)(cid:0)(cid:6)(cid:16)(cid:2)(cid:16)(cid:8)(cid:10)(cid:16)(cid:8)(cid:3)(cid:0)

(cid:3)(cid:24)(cid:25)(cid:2)(cid:15)(cid:2)(cid:16)(cid:2)(cid:26)(cid:0)(cid:18)(cid:8)(cid:0)(cid:12)(cid:7)(cid:13)(cid:27)(cid:8)(cid:26)(cid:10)(cid:8)(cid:0)(cid:19)(cid:28)(cid:21)(cid:28)(cid:0)(cid:29)(cid:13)(cid:14)(cid:30)(cid:2)(cid:3)(cid:2)(cid:7)(cid:16)(cid:2)(cid:0)(cid:31) !""(cid:0)(cid:18)(cid:24)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)(cid:27)(cid:2)(cid:7)(cid:8)(cid:2)#(cid:24)(cid:4)(cid:0)
#(cid:2)(cid:26)(cid:16)(cid:17)(cid:0)(cid:18)(cid:2)(cid:16)(cid:2)(cid:0)(cid:12)(cid:13)(cid:18)(cid:24)(cid:10)(cid:0)$(cid:18)(cid:24)(cid:10)(cid:2)%(cid:0)(cid:31) !""&’’

(cid:1)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)!(cid:0)(cid:15)(cid:24)(cid:7)(cid:17)(cid:9)(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)(cid:9)(cid:24)(cid:26)(cid:14)(cid:24)(cid:15)#(cid:2)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)(cid:18)(cid:2)(cid:7)(cid:8)(cid:0)(cid:1)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0) 1(cid:0)(cid:26)(cid:2)(cid:15)(cid:17)(cid:26)(cid:0)
(cid:15)(cid:2)(cid:10)(cid:8)(cid:5)(cid:0)(cid:15)(cid:24)(cid:26)(cid:14)(cid:2)#(cid:2)(cid:8)(cid:3)(cid:2)(cid:26)(cid:0)(cid:3)(cid:13)(cid:7)(cid:24)(cid:4)(cid:2)(cid:10)(cid:8)(cid:0)(cid:9)(cid:24)(cid:26)(cid:14)(cid:2)(cid:7)(cid:17)(cid:5)(cid:0)(cid:2)(cid:25)(cid:2)(cid:3)(cid:0)(cid:26)(cid:30)(cid:2)(cid:28)(cid:0)

(cid:0)

(cid:21)(cid:21)(cid:28)()*+*,-(cid:0)(cid:12).-./(cid:21))(cid:21),-(cid:0)

0(cid:24)(cid:7)(cid:18)(cid:2)(cid:10)(cid:2)(cid:7)(cid:3)(cid:2)(cid:26)(cid:0)(cid:4)(cid:2)(cid:16)(cid:2)(cid:7)(cid:0)#(cid:24)(cid:4)(cid:2)(cid:3)(cid:2)(cid:26)(cid:14)1(cid:0)(cid:16)(cid:17)2(cid:17)(cid:2)(cid:26)(cid:0)(cid:18)(cid:8)(cid:4)(cid:2)(cid:3)(cid:17)(cid:3)(cid:2)(cid:26)(cid:26)(cid:30)(cid:2)(cid:0)

(cid:9)(cid:24)(cid:26)(cid:24)(cid:4)(cid:8)(cid:16)(cid:8)(cid:2)(cid:26)(cid:0)(cid:8)(cid:26)(cid:8)(cid:0)(cid:2)(cid:18)(cid:2)(cid:4)(cid:2)(cid:5)3(cid:0)

!(cid:28)((cid:1)(cid:24)(cid:26)(cid:14)(cid:3)(cid:2)2(cid:8)(cid:0) (cid:18)(cid:2)(cid:26)(cid:0) (cid:15)(cid:24)(cid:15)#(cid:2)(cid:26)(cid:14)(cid:17)(cid:26)(cid:0) (cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0) 45667879:7;
<79:=>?8@59A;(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)B(cid:2)(cid:30)(cid:20)C(cid:24)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)(cid:1)(cid:17)(cid:4)(cid:16)(cid:8)(cid:27)(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:28)(cid:0)
(cid:31)(cid:28)((cid:1)(cid:24)(cid:15)#(cid:2)(cid:26)(cid:14)(cid:17)(cid:26)(cid:0)(cid:12)(cid:2)(cid:25)(cid:3)(cid:2)(cid:14)(cid:24)(cid:0)D(cid:0)(cid:17)(cid:26)(cid:16)(cid:17)(cid:3)(cid:0)(cid:15)(cid:24)(cid:16)(cid:13)(cid:18)(cid:24)(cid:0)45667879:7;
<79:=>?8@59A;(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)B(cid:2)(cid:30)(cid:20)C(cid:24)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)(cid:1)(cid:17)(cid:4)(cid:16)(cid:8)(cid:27)(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:28)(cid:0)
E(cid:28)((cid:1)(cid:24)(cid:26)(cid:14)(cid:3)(cid:2)2(cid:8)(cid:0)(cid:24)F(cid:8)(cid:10)(cid:8)(cid:24)(cid:26)(cid:10)(cid:8)(cid:0)(cid:15)(cid:24)(cid:16)(cid:13)(cid:18)(cid:24)(cid:0)45667879:7;<79:=>?8@59A(cid:0)

(cid:0)

(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)B(cid:2)(cid:30)(cid:20)C(cid:24)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)(cid:1)(cid:17)(cid:4)(cid:16)(cid:8)(cid:27)(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:28)(cid:0)

G(cid:28)((cid:1)(cid:24)(cid:26)(cid:24)(cid:7)(cid:2)(cid:9)(cid:3)(cid:2)(cid:26)(cid:0)(cid:15)(cid:24)(cid:16)(cid:13)(cid:18)(cid:24)(cid:0)45667879:7;<79:=>?8@59A(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)
(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)B(cid:2)(cid:30)(cid:20)C(cid:24)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)(cid:1)(cid:17)(cid:4)(cid:16)(cid:8)(cid:27)(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:18)(cid:2)(cid:16)(cid:2)(cid:0)0(cid:12)(cid:6)(cid:0)
(cid:15)(cid:24)(cid:26)(cid:14)(cid:14)(cid:17)(cid:26)(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)D(cid:0)H?:@?A7;(cid:30)(cid:2)(cid:26)(cid:14)(cid:0)(cid:16)(cid:24)(cid:4)(cid:2)(cid:5)(cid:0)(cid:18)(cid:8)#(cid:2)(cid:26)(cid:14)(cid:17)(cid:26)(cid:0)$(cid:10)(cid:16)(cid:17)(cid:18)(cid:8)(cid:0)
(cid:3)(cid:2)(cid:10)(cid:17)(cid:10)(cid:0)(cid:9)(cid:24)(cid:26)(cid:14)(cid:24)(cid:4)(cid:17)(cid:2)(cid:7)(cid:2)(cid:26)(cid:0)(cid:3)(cid:13)(cid:26)(cid:10)(cid:17)(cid:15)(cid:10)(cid:8)(cid:0)(cid:15)(cid:2)(cid:3)(cid:2)(cid:26)(cid:2)(cid:26)(cid:0)(cid:18)(cid:2)(cid:26)(cid:0)(cid:26)(cid:13)(cid:26)(cid:0)
(cid:15)(cid:2)(cid:3)(cid:2)(cid:26)(cid:2)(cid:26)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:4)(cid:24)(cid:27)(cid:24)(cid:4)(cid:0)(cid:3)(cid:24)(cid:25)(cid:2)(cid:15)(cid:2)(cid:16)(cid:2)(cid:26)(cid:0)(cid:18)(cid:8)(cid:0)(cid:19)(cid:28)(cid:21)(cid:28)(cid:0)(cid:29)(cid:13)(cid:14)(cid:30)(cid:2)(cid:3)(cid:2)(cid:7)(cid:16)(cid:2)(cid:0)
#(cid:17)(cid:4)(cid:2)(cid:26)(cid:0)(cid:15)(cid:2)(cid:7)(cid:24)(cid:16)(cid:0)(cid:31) !""%(cid:0)

(cid:21)(cid:21)(cid:21)(cid:28)((cid:12).-./(cid:21))(cid:21),-(cid:0)).D(cid:23),(cid:21))(cid:0)

)(cid:24)(cid:7)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0) (cid:9)(cid:24)(cid:26)(cid:24)(cid:4)(cid:8)(cid:16)(cid:8)(cid:2)(cid:26)(cid:0) (cid:16)(cid:24)(cid:7)(cid:3)(cid:2)(cid:8)(cid:16)1(cid:0) (cid:30)(cid:2)(cid:3)(cid:26)(cid:8)(cid:0) IJK(cid:0) (cid:30)(cid:2)(cid:26)(cid:14)(cid:0)
(cid:15)(cid:24)(cid:26)(cid:14)(cid:24)(cid:15)#(cid:2)(cid:26)(cid:14)(cid:3)(cid:2)(cid:26)(cid:0)(cid:2)(cid:9)(cid:4)(cid:8)(cid:3)(cid:2)(cid:10)(cid:8)(cid:0)(cid:10)(cid:15)(cid:2)(cid:4)(cid:4)(cid:0)(cid:2)(cid:7)(cid:24)(cid:2)(cid:0)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:16)(cid:8)(cid:13)(cid:26)(cid:0)$(cid:17)(cid:26)(cid:8)(cid:27)(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)%(cid:0)
(cid:18)(cid:24)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)(cid:15)(cid:24)(cid:16)(cid:13)(cid:18)(cid:24)(cid:0)#(cid:24)(cid:26)(cid:25)(cid:5)(cid:15)(cid:2)(cid:7)(cid:3)(cid:28)(cid:0)(cid:12)(cid:2)(cid:18)(cid:2)(cid:0)(cid:9)(cid:24)(cid:15)#(cid:2)(cid:26)(cid:14)(cid:17)(cid:26)(cid:2)(cid:26)(cid:0)(cid:2)(cid:9)(cid:4)(cid:8)(cid:3)(cid:2)(cid:10)(cid:8)(cid:0)(cid:6),.(cid:0)
(cid:1)(cid:17)(cid:4)(cid:16)(cid:8)(cid:27)(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:16)(cid:24)(cid:4)(cid:2)(cid:5)(cid:0)(cid:18)(cid:8)(cid:4)(cid:2)(cid:3)(cid:17)(cid:3)(cid:2)(cid:26)(cid:0)(cid:9)(cid:24)(cid:26)(cid:24)(cid:4)(cid:8)(cid:16)(cid:2)(cid:8)(cid:26)(cid:0)(cid:13)(cid:4)(cid:24)(cid:5)(cid:0)ILK(cid:0)(cid:30)(cid:2)(cid:26)(cid:14)(cid:0)
(cid:15)(cid:24)(cid:26)(cid:24)(cid:7)(cid:2)(cid:9)(cid:3)(cid:2)(cid:26)(cid:0)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)#(cid:8)(cid:27)(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)1(cid:0)(cid:30)(cid:2)(cid:26)(cid:14)(cid:0)(cid:3)(cid:24)(cid:15)(cid:17)(cid:18)(cid:8)(cid:2)(cid:26)(cid:0)(cid:13)(cid:4)(cid:24)(cid:5)(cid:0)I(cid:31)K(cid:0)(cid:18)(cid:8)(cid:4)(cid:2)(cid:3)(cid:17)(cid:3)(cid:2)(cid:26)(cid:0)
(cid:9)(cid:24)(cid:26)(cid:24)(cid:4)(cid:8)(cid:16)(cid:8)(cid:2)(cid:26)(cid:0)(cid:18)(cid:24)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)(cid:15)(cid:24)(cid:15)#(cid:2)(cid:26)(cid:14)(cid:17)(cid:26)(cid:0)M;H?:@?A7(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)B(cid:2)(cid:30)(cid:20)
(cid:5)(cid:24)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)(cid:1)(cid:17)(cid:4)(cid:16)(cid:8)(cid:27)(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:30)(cid:2)(cid:26)(cid:14)(cid:0)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:18)(cid:8)(cid:14)(cid:17)(cid:26)(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)(cid:10)(cid:2)(cid:15)(cid:9)(cid:2)(cid:8)(cid:0)(cid:26)(cid:0)(cid:27)(cid:2)(cid:7)(cid:8)(cid:2)#(cid:24)(cid:4)(cid:0)
(cid:7)(cid:24)(cid:10)(cid:9)(cid:13)(cid:26)(cid:28)(cid:0)(cid:19)(cid:2)(cid:4)(cid:2)(cid:15)(cid:0)(cid:9)(cid:24)(cid:26)(cid:14)(cid:2)(cid:9)(cid:4)(cid:8)(cid:3)(cid:2)(cid:10)(cid:8)(cid:2)(cid:26)(cid:0)(cid:6),.(cid:0)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)B(cid:2)(cid:30)(cid:20)(cid:5)(cid:24)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)(cid:16)(cid:24)(cid:4)(cid:2)(cid:5)(cid:0)
(cid:18)(cid:8)(cid:3)(cid:2)2(cid:8)(cid:0)(cid:13)(cid:4)(cid:24)(cid:5)(cid:0)INK(cid:0)(cid:30)(cid:2)(cid:26)(cid:14)(cid:0)(cid:15)(cid:24)(cid:26)(cid:14)(cid:2)(cid:9)(cid:4)(cid:8)(cid:3)(cid:2)(cid:10)(cid:8)(cid:3)(cid:2)(cid:26)(cid:0)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)F(cid:2)(cid:30)(cid:20)(cid:5)(cid:24)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)
(cid:15)(cid:17)(cid:4)(cid:16)(cid:8)(cid:27)(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:9)(cid:24)(cid:26)(cid:14)(cid:24)(cid:4)(cid:17)(cid:2)(cid:7)(cid:2)(cid:26)(cid:0)(cid:3)(cid:13)(cid:26)(cid:10)(cid:17)(cid:15)(cid:10)(cid:8)(cid:0)(cid:7)(cid:17)(cid:15)(cid:2)(cid:5)(cid:0)(cid:16)(cid:2)(cid:26)(cid:14)(cid:14)(cid:2)(cid:0)(cid:9)(cid:24)(cid:7)(cid:0)
(cid:3)(cid:2)(cid:9)(cid:8)(cid:16)(cid:2)(cid:0)(cid:18)(cid:8)(cid:0)(cid:21)(cid:26)(cid:18)(cid:13)(cid:26)(cid:24)(cid:10)(cid:8)(cid:2)(cid:28)(cid:0)(cid:0)

(cid:0)

(cid:0)

(cid:0)

G(cid:28)!(cid:28)(cid:31)(cid:28)(t>H585:?R;<7gS;u597?8;v9w5?g7Z;x87Z5:S5Y9;yt<uvxz(cid:0)
*(cid:26)(cid:16)(cid:17)(cid:3)(cid:0)(cid:15)(cid:24)(cid:26)(cid:14)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)B(cid:2)(cid:30)(cid:20)C(cid:24)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)(cid:1)(cid:17)(cid:4)(cid:16)(cid:8)(cid:27)(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)
(cid:15)(cid:24)(cid:26)(cid:14)(cid:14)(cid:17)(cid:26)(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)(cid:15)(cid:24)(cid:16)(cid:13)(cid:18)(cid:24)(cid:0)t<uvx;#(cid:24)(cid:7)(cid:18)(cid:2)(cid:10)(cid:2)(cid:7)(cid:3)(cid:2)(cid:26)(cid:0)PQRS5T?85?S;
u597?8;P5{7Z;PYZ7R(cid:0)$(cid:1)/(cid:1)(cid:1)%(cid:0)(cid:18)(cid:24)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)(cid:7)(cid:17)(cid:15)(cid:17)(cid:10)(cid:0)(cid:10)(cid:24)#(cid:2)(cid:14)(cid:2)(cid:8)(cid:0)#(cid:24)(cid:7)(cid:8)(cid:3)(cid:17)(cid:16)3(cid:0)

;

|}\\]^~

(cid:127)_‘n(cid:128)

r‘(cid:129)n(cid:128)(cid:130)ck[(cid:131)]f^~
n(cid:128)\\‘n(cid:128)
r‘(cid:129)_n(cid:128)
o(cid:0)
(cid:0)
^~
(cid:127)(cid:0)(cid:15)(cid:24)(cid:7)(cid:17)(cid:9)(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)(cid:9)(cid:24)(cid:26)(cid:18)(cid:17)(cid:14)(cid:2)(cid:0)(cid:30)(cid:2)(cid:26)(cid:14)(cid:0)w7gS;R597?8;Q9w5?g7Z;7gS5>?SY8;
y<uvtz;#(cid:2)(cid:14)(cid:8)(cid:0)^(cid:0)3(cid:0)
(cid:0)

(cid:127)pf(cid:0)

$E%(cid:0)

(cid:130)c
^~
(cid:127)\\(cid:132)](cid:129)n(cid:128)(cid:130)c](cid:133)

](cid:129)n(cid:128)(cid:130)c[(cid:0)

(cid:0)

(cid:19)(cid:24)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)(cid:15)(cid:2)(cid:16)(cid:7)(cid:8)(cid:3)(cid:10)(cid:0)(cid:7)(cid:2)(cid:14)(cid:2)(cid:15)(cid:0)^~

(cid:127)(cid:134)(cid:0)

(cid:135)(cid:136)(cid:137)(cid:132)^~

(cid:130)c
(cid:127)(cid:133)\\(cid:132)](cid:129)n(cid:128)(cid:130)c](cid:133)

(cid:0)(cid:0)(cid:0)

$G%(cid:0)

$J%(cid:0)

G(cid:28)!(cid:28)E(cid:28)(P(cid:138)t;PQRS5T?85?S7;U?VWX7885YS;PYZ7R(cid:0)

(cid:1)(cid:6).(cid:0)(cid:1)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)B(cid:2)(cid:30)(cid:20)C(cid:24)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)(cid:1)(cid:17)(cid:4)(cid:16)(cid:8)(cid:27)(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:18)(cid:8)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)

(cid:18)(cid:24)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)(cid:9)(cid:24)(cid:7)(cid:10)(cid:2)(cid:15)(cid:2)(cid:2)(cid:26)(cid:0)#(cid:24)(cid:7)(cid:8)(cid:3)(cid:17)(cid:16)(cid:0)I""K3(cid:0)
(cid:0)

(cid:139)(cid:140)bk|}p\\(cid:141)c(cid:142)k(cid:143)(cid:144)(cid:145)

rp_(cid:141)(cid:145)(cid:142)k(cid:143)(cid:144)(cid:145)

rp_(cid:141)(cid:146)(cid:142)k(cid:143)(cid:144)(cid:145)

rp(cid:0) $J%(cid:0)

(cid:18)(cid:24)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)(cid:15)(cid:2)(cid:10)(cid:8)(cid:26)(cid:14)(cid:20)(cid:15)(cid:2)(cid:10)(cid:8)(cid:26)(cid:14)(cid:0)(cid:3)(cid:13)(cid:15)(cid:9)(cid:13)(cid:26)(cid:24)(cid:26)(cid:0)(cid:10)(cid:24)#(cid:2)(cid:14)(cid:2)(cid:8)(cid:0)#(cid:24)(cid:7)(cid:8)(cid:3)(cid:17)(cid:16)3(cid:0)
(cid:141)c(cid:142)k(cid:143)(cid:144)(cid:145)

rp\\(cid:147)no(cid:0)

$(cid:148)%(cid:0)

(cid:0)

(cid:141)(cid:145)(cid:142)k(cid:143)(cid:144)(cid:145)

(cid:130)c
rp\\k(cid:149)(cid:131)(cid:147)p](cid:132)](cid:129)n(cid:128)(cid:130)c](cid:133)

](cid:129)k(cid:149)(cid:131)(cid:147)p(cid:0) $L%(cid:0)

(cid:141)(cid:146)(cid:142)k(cid:143)(cid:144)(cid:145)

rp(cid:150)(cid:151)(cid:151)(cid:135)(cid:136)(cid:137)k(cid:143)(cid:144)(cid:145)

r(cid:152)m(cid:143)(cid:144)(cid:145)

r(cid:153)p(cid:147)k(cid:152)pn(cid:128)(cid:147)k(cid:153)p

(cid:129)ff(cid:0) $N%(cid:0)

(cid:0)

(cid:154)m(cid:155)\\(cid:149)m(cid:156)m(cid:157)fq(cid:0)(cid:0)
r‘(cid:129)n(cid:128)qff(cid:147)k(cid:152)p\\
r(cid:152)m(cid:143)(cid:144)(cid:145)

(cid:158)(cid:147)
(cid:158)(cid:143)(cid:144)(cid:145)
r
r(cid:153)p\\k(cid:159)(cid:160)m¡p(cid:130)c(cid:0)

fqf(cid:0)

(cid:147)\\‘n(cid:128)

(cid:135)(cid:136)(cid:137)k(cid:143)(cid:144)(cid:145)

(cid:0)

(cid:21)(cid:22)(cid:28)((cid:1).)O(cid:19).(cid:0)(cid:12).-./(cid:21))(cid:21),-(cid:0)(cid:0)

G(cid:28)!(cid:28)G(cid:28)(<79:=>?8@59A(cid:0)

G(cid:28)!(cid:28)( (cid:1)(cid:24)(cid:16)(cid:13)(cid:18)(cid:24)(cid:0),(cid:26)(cid:2)(cid:4)(cid:8)(cid:10)(cid:8)(cid:10)(cid:0)

G(cid:28)!(cid:28)!(cid:28)(PQRS5T?85?S7;U?VWX7885YS;PYZ7R(cid:0)

(cid:1)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)(cid:8)(cid:26)(cid:8)(cid:0)(cid:15)(cid:24)(cid:7)(cid:17)(cid:9)(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)(cid:30)(cid:2)(cid:26)(cid:14)(cid:0)(cid:18)(cid:8)(cid:3)(cid:24)(cid:15)#(cid:2)(cid:26)(cid:14)(cid:3)(cid:2)(cid:26)(cid:0)(cid:18)(cid:2)(cid:7)(cid:8)(cid:0)
(cid:1)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)B(cid:2)(cid:30)(cid:20)C(cid:24)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)(cid:17)(cid:26)(cid:8)(cid:27)(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:30)(cid:2)(cid:26)(cid:14)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:26)(cid:30)(cid:2)(cid:0)
(cid:15)(cid:24)(cid:15)(cid:9)(cid:24)(cid:7)(cid:16)(cid:8)(cid:15)#(cid:2)(cid:26)(cid:14)(cid:3)(cid:2)(cid:26)(cid:0)(cid:3)(cid:13)(cid:7)(cid:24)(cid:4)(cid:2)(cid:10)(cid:8)(cid:0)(cid:2)(cid:26)(cid:16)(cid:2)(cid:7)(cid:0)(cid:27)(cid:2)(cid:7)(cid:8)(cid:2)#(cid:24)(cid:4)(cid:0)(cid:7)(cid:24)(cid:10)(cid:9)(cid:13)(cid:26)(cid:28)(cid:0)(cid:1)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)
B(cid:2)(cid:30)(cid:20)C(cid:24)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)(cid:1)(cid:17)(cid:4)(cid:16)(cid:8)(cid:27)(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:18)(cid:8)(cid:16)(cid:17)(cid:4)(cid:8)(cid:10)(cid:0)(cid:18)(cid:2)(cid:4)(cid:2)(cid:15)(cid:0)#(cid:24)(cid:26)(cid:16)(cid:17)(cid:3)(cid:0)(cid:15)(cid:2)(cid:16)(cid:7)(cid:8)(cid:3)(cid:10)(cid:0)
(cid:10)(cid:24)#(cid:2)(cid:14)(cid:2)(cid:8)(cid:0)#(cid:24)(cid:7)(cid:8)(cid:3)(cid:17)(cid:16)3(cid:0)
[\\]^_‘a_b\\]^_‘cac_d_‘eae_b(cid:0) $!%(cid:0)

(cid:0)
[(cid:0)(cid:15)(cid:24)(cid:7)(cid:17)(cid:9)(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)(cid:5)(cid:2)(cid:10)(cid:8)(cid:4)(cid:0)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:16)(cid:8)(cid:18)(cid:2)(cid:3)(cid:0)(cid:4)(cid:2)(cid:26)(cid:14)(cid:10)(cid:17)(cid:26)(cid:14)1(cid:0)](cid:0)
(cid:15)(cid:24)(cid:7)(cid:17)(cid:9)(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)(cid:15)(cid:2)(cid:16)(cid:7)(cid:8)(cid:3)(cid:10)(cid:0)(cid:27)(cid:2)(cid:7)(cid:8)(cid:2)#(cid:24)(cid:4)(cid:0)(cid:9)(cid:24)(cid:26)(cid:30)(cid:24)(cid:7)(cid:16)(cid:2)1(cid:0)^f(cid:15)(cid:24)(cid:7)(cid:17)(cid:9)(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)(cid:3)(cid:13)(cid:24)F(cid:8)(cid:10)(cid:8)(cid:24)(cid:26)(cid:0)
(cid:9)(cid:24)(cid:26)(cid:14)(cid:2)(cid:7)(cid:17)(cid:5)(cid:0)(cid:16)(cid:24)(cid:16)(cid:2)(cid:9)1(cid:0)‘(cid:0)(cid:15)(cid:24)(cid:7)(cid:17)(cid:9)(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)(cid:15)(cid:2)(cid:16)(cid:7)(cid:8)(cid:3)(cid:10)(cid:0)(cid:8)(cid:18)(cid:24)(cid:26)(cid:16)(cid:8)(cid:16)(cid:2)(cid:10)1(cid:0)a(cid:0)(cid:15)(cid:24)(cid:7)(cid:17)(cid:9)(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)
(cid:27)(cid:24)(cid:3)(cid:16)(cid:13)(cid:7)(cid:0)(cid:9)(cid:24)(cid:26)(cid:14)(cid:2)(cid:7)(cid:17)(cid:5)(cid:0)(cid:2)(cid:25)(cid:2)(cid:3)(cid:0)(cid:2)(cid:7)(cid:24)(cid:2)(cid:0)(cid:18)(cid:2)(cid:26)(cid:0)bf(cid:15)(cid:24)(cid:7)(cid:17)(cid:9)(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)788Y8(cid:0)(cid:30)(cid:2)(cid:26)(cid:14)(cid:0)
#(cid:24)(cid:7)(cid:2)(cid:10)(cid:2)(cid:4)(cid:0)(cid:18)(cid:2)(cid:7)(cid:8)(cid:0)g?>HR59Ah;(cid:19)(cid:24)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)#(cid:24)#(cid:24)(cid:7)(cid:2)(cid:9)(cid:2)(cid:0)(cid:2)(cid:10)(cid:17)(cid:15)(cid:10)(cid:8)(cid:0)(cid:10)(cid:24)#(cid:2)(cid:14)(cid:2)(cid:8)(cid:0)
#(cid:24)(cid:7)(cid:8)(cid:3)(cid:17)(cid:16)3(cid:0)

(cid:0)

(cid:0)

bijklmnopqfffffaijklmnrpqffffffasijklmnrsp(cid:0) $(cid:31)%(cid:0)

(cid:0)

I""K(cid:0)(cid:15)(cid:24)(cid:26)(cid:14)(cid:24)(cid:26)(cid:2)(cid:4)(cid:3)(cid:2)(cid:26)(cid:0)(cid:10)(cid:2)(cid:4)(cid:2)(cid:5)(cid:0)(cid:10)(cid:2)(cid:16)(cid:17)(cid:0)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)(cid:15)(cid:17)(cid:4)(cid:16)(cid:8)(cid:27)(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)1(cid:0)(cid:30)(cid:2)(cid:8)(cid:16)(cid:17)(cid:0)
(cid:1)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)!(cid:0)(cid:30)(cid:2)(cid:26)(cid:14)(cid:0)(cid:15)(cid:24)(cid:7)(cid:17)(cid:9)(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)(cid:15)(cid:17)(cid:4)(cid:16)(cid:8)(cid:27)(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:9)(cid:2)(cid:4)(cid:8)(cid:26)(cid:14)(cid:0)(cid:10)(cid:24)(cid:18)(cid:24)(cid:7)(cid:5)(cid:2)(cid:26)(cid:2)(cid:28)(cid:0)

(cid:0)

(cid:0)

'

§
(cid:153)¤c

§
(cid:153)¤c

¥ \\(cid:151) ƒ(cid:153)¢(cid:128)

(cid:1)(cid:8)(cid:10)(cid:2)(cid:4)(cid:3)(cid:2)(cid:26)(cid:0)¢(cid:142)(cid:0)(cid:2)(cid:18)(cid:2)(cid:4)(cid:2)(cid:5)(cid:0)(cid:26)(cid:8)(cid:4)(cid:2)(cid:8)(cid:0)(cid:7)(cid:2)(cid:16)(cid:2)(cid:20)(cid:7)(cid:2)(cid:16)(cid:2)(cid:0)(cid:17)(cid:26)(cid:16)(cid:17)(cid:3)(cid:0)£(cid:8)(cid:4)(cid:2)(cid:30)(cid:2)(cid:5)(cid:0)⁄1(cid:0)(cid:18)(cid:2)(cid:26)(cid:0)
(cid:153)¤c f(cid:2)(cid:18)(cid:2)(cid:4)(cid:2)(cid:5)(cid:0)(cid:2)(cid:14)(cid:7)(cid:24)(cid:14)(cid:2)(cid:10)(cid:8)(cid:0)(cid:7)(cid:2)(cid:16)(cid:2)(cid:20)(cid:7)(cid:2)(cid:16)(cid:2)1(cid:0)(cid:18)(cid:8)(cid:15)(cid:2)(cid:26)(cid:2)(cid:0)ƒ(cid:153)\\'“
§
¢¥ \\(cid:151) ƒ(cid:153)¢(cid:153)
(cid:0)
(cid:2)(cid:18)(cid:2)(cid:4)(cid:2)(cid:5)(cid:0)(cid:9)(cid:24)(cid:26)(cid:8)(cid:15)#(cid:2)(cid:26)(cid:14)(cid:0)(cid:17)(cid:26)(cid:16)(cid:17)(cid:3)(cid:0)(cid:16)(cid:8)(cid:2)(cid:9)(cid:0)(cid:2)(cid:7)(cid:24)(cid:2)(cid:28)(cid:0)O(cid:4)(cid:24)(cid:5)(cid:0)(cid:3)(cid:2)(cid:7)(cid:24)(cid:26)(cid:2)(cid:0)(cid:10)(cid:24)(cid:16)(cid:8)(cid:2)(cid:9)(cid:0)(cid:2)(cid:7)(cid:24)(cid:2)(cid:0)
(cid:16)(cid:24)(cid:7)(cid:10)(cid:2)(cid:15)(cid:9)(cid:24)(cid:4)1(cid:0)(cid:15)(cid:2)(cid:3)(cid:2)(cid:0)(cid:151) ƒ(cid:153)\\(cid:149)
«(cid:0)(cid:1)(cid:8)(cid:10)(cid:2)(cid:4)(cid:3)(cid:2)(cid:26)(cid:0)(cid:10)(cid:17)(cid:2)(cid:16)(cid:17)(cid:0)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)
(cid:4)(cid:2)(cid:26)(cid:14)(cid:10)(cid:17)(cid:26)(cid:14)(cid:0)¢(cid:128)
(cid:0)(cid:18)(cid:2)(cid:7)(cid:8)(cid:0)¢¥ (cid:0)#(cid:24)(cid:7)(cid:10)(cid:8)F(cid:2)(cid:16)(cid:0)87R5?wR7‹;(cid:15)(cid:2)(cid:3)(cid:2)(cid:0)
(cid:142)
(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:2)(cid:7)(cid:24)(cid:2)(cid:0)(cid:3)(cid:24)(cid:25)(cid:8)(cid:4)(cid:0)¢(cid:142)(cid:0)(cid:18)(cid:8)(cid:5)(cid:2)(cid:7)(cid:2)(cid:9)(cid:3)(cid:2)(cid:26)(cid:0)(cid:2)(cid:14)(cid:2)(cid:7)(cid:0)(cid:10)(cid:2)(cid:2)(cid:16)(cid:0)(cid:18)(cid:8)(cid:2)(cid:14)(cid:7)(cid:24)(cid:14)(cid:2)(cid:10)(cid:8)(cid:3)(cid:2)(cid:26)1(cid:0)
(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)(cid:10)(cid:24)(cid:10)(cid:17)(cid:2)(cid:8)(cid:0)(cid:18)(cid:24)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:4)(cid:2)(cid:26)(cid:14)(cid:10)(cid:17)(cid:26)(cid:14)(cid:0)(cid:30)(cid:2)(cid:26)(cid:14)(cid:0)(cid:10)(cid:17)(cid:18)(cid:2)(cid:5)(cid:0)87R5?wR7;¢(cid:128)
¥;
IEK(cid:28)(cid:0)
(cid:129)^~(cid:133)(cid:176)l
§
|}¥(cid:131)(cid:0)(cid:151) ƒ(cid:153)|}›
(cid:142)\\(cid:151) ƒ(cid:153)k(cid:149)(cid:131)ﬁ(cid:144)(cid:153)p(cid:132)|}(cid:153)(cid:131)ﬂ(cid:142)
(cid:153)¤c
(cid:1)(cid:2)(cid:3)(cid:2)(cid:0) (cid:18)(cid:8)#(cid:17)(cid:16)(cid:17)(cid:5)(cid:3)(cid:2)(cid:26)(cid:0) (cid:15)(cid:13)(cid:18)(cid:8)F(cid:8)(cid:3)(cid:2)(cid:10)(cid:8)(cid:0) (cid:9)(cid:2)(cid:18)(cid:2)(cid:0)¢(cid:128)
› (cid:0)(cid:2)(cid:14)(cid:2)(cid:7)(cid:0)
(cid:142)
<79:=>?8@59A;(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:18)(cid:8)(cid:4)(cid:2)(cid:3)(cid:17)(cid:3)(cid:2)(cid:26)(cid:28)(cid:0)D(cid:24)F(cid:24)(cid:7)(cid:24)(cid:26)(cid:10)(cid:8)(cid:0)I!K(cid:0)(cid:15)(cid:24)(cid:26)(cid:30)(cid:24)#(cid:17)(cid:16)(cid:3)(cid:2)(cid:26)(cid:0)
(cid:16)(cid:24)(cid:7)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:6)(cid:17)(cid:2)(cid:16)(cid:17)(cid:0)(cid:15)(cid:24)(cid:16)(cid:13)(cid:18)(cid:24)(cid:0)w79:=>?8@59A(cid:0)(cid:30)(cid:2)(cid:26)(cid:14)(cid:0)(cid:9)(cid:2)(cid:4)(cid:8)(cid:26)(cid:14)(cid:0)(cid:10)(cid:24)(cid:18)(cid:24)(cid:7)(cid:5)(cid:2)(cid:26)(cid:2)(cid:0)
(cid:26)(cid:2)(cid:15)(cid:17)(cid:26)(cid:0)#(cid:2)(cid:26)(cid:30)(cid:2)(cid:3)(cid:0)(cid:18)(cid:8)(cid:14)(cid:17)(cid:26)(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)(cid:30)(cid:2)(cid:8)(cid:16)(cid:17)(cid:0)45667879:7;<79:=?>?8@59A;
(cid:18)(cid:2)(cid:26)(cid:0)M?S5Y;<79:=>?8@59Ah;(cid:12)(cid:2)(cid:18)(cid:2)(cid:0)(cid:9)(cid:24)(cid:26)(cid:24)(cid:4)(cid:8)(cid:16)(cid:8)(cid:2)(cid:26)(cid:0)(cid:8)(cid:26)(cid:8)(cid:0)(cid:15)(cid:24)(cid:16)(cid:13)(cid:18)(cid:24)(cid:0)(cid:30)(cid:2)(cid:26)(cid:14)(cid:0)
(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)(cid:18)(cid:8)(cid:14)(cid:17)(cid:26)(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)(cid:2)(cid:18)(cid:2)(cid:4)(cid:2)(cid:5)(cid:0)45667879:7;<79:=>?8@59A;(cid:30)(cid:2)(cid:26)(cid:14)(cid:0)
(cid:18)(cid:8)(cid:4)(cid:2)(cid:3)(cid:17)(cid:3)(cid:2)(cid:26)(cid:0)(cid:18)(cid:24)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)(cid:15)(cid:24)(cid:26)(cid:2)(cid:15)#(cid:2)(cid:5)(cid:3)(cid:2)(cid:26)(cid:0)¢(cid:128)
›(cid:0)(cid:18)(cid:24)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)(cid:10)(cid:17)(cid:2)(cid:16)(cid:17)(cid:0)F(cid:2)(cid:3)(cid:16)(cid:13)(cid:7)(cid:0)
(cid:142)
›
(cid:3)(cid:13)(cid:7)(cid:24)(cid:3)(cid:10)(cid:8)(cid:0)(cid:151) ƒ(cid:153)|}(cid:153)(cid:131)(cid:151) ƒ(cid:153)|}(cid:153)
›
› _k(cid:151) ƒ(cid:153)|}(cid:153)(cid:131)(cid:151) ƒ(cid:153)|}(cid:153)

§
(cid:153)¤c
e– \\|}(cid:153)
|}(cid:153)

mf(cid:10)(cid:24)#(cid:2)(cid:14)(cid:2)(cid:8)(cid:0)#(cid:24)(cid:7)(cid:8)(cid:3)(cid:17)(cid:16)3(cid:0)

§
(cid:153)¤c
§
(cid:153)¤c

(cid:0) $""%(cid:0)

$! %(cid:0)

§
(cid:153)¤c

§
(cid:153)¤c

p(cid:0)(cid:0)

(cid:0)

(cid:0)

(cid:1)(cid:2)(cid:3)(cid:2)(cid:4)(cid:2)(cid:5)(cid:0)(cid:6)(cid:3)(cid:7)(cid:8)(cid:9)(cid:10)(cid:8)(cid:0)(cid:11)(cid:0)(cid:12)(cid:7)(cid:13)(cid:14)(cid:7)(cid:2)(cid:15)(cid:0)(cid:6)(cid:16)(cid:17)(cid:18)(cid:8)(cid:0)(cid:19)(cid:20)(cid:21)(cid:22)(cid:0)(cid:23)(cid:13)(cid:15)(cid:9)(cid:17)(cid:16)(cid:2)(cid:10)(cid:8)(cid:0)(cid:6)(cid:16)(cid:2)(cid:16)(cid:8)(cid:10)(cid:16)(cid:8)(cid:3)(cid:0)

(cid:6)(cid:24)(cid:5)(cid:8)(cid:25)(cid:14)(cid:14)(cid:2)(cid:0)(cid:18)(cid:24)(cid:25)(cid:14)(cid:2)(cid:25)(cid:0)(cid:15)(cid:13)(cid:18)(cid:8)(cid:26)(cid:8)(cid:3)(cid:2)(cid:10)(cid:8)(cid:0)(cid:16)(cid:24)(cid:7)(cid:10)(cid:24)(cid:27)(cid:17)(cid:16)(cid:28)(cid:0)(cid:15)(cid:2)(cid:3)(cid:2)(cid:0)(cid:2)(cid:3)(cid:2)(cid:25)(cid:0)(cid:18)(cid:8)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:29)(cid:0)

(cid:0)

(cid:0)

(cid:30) (cid:31) !"" 

&
 ’(

&
#$ %(cid:30) (cid:31) !""  %
 ’(

(cid:0)!"")(cid:0)

*++,(cid:0)

-.+./.012345677898:;84<8:;=>?9@6:A42B3(cid:0)

C+DE(cid:0)(cid:15)(cid:24)(cid:7)(cid:17)(cid:15)(cid:17)(cid:10)(cid:3)(cid:2)(cid:25)(cid:0)(cid:1)(cid:6)F(cid:0)G677898:;84H8:;=>?9@6:A(cid:0)
(cid:10)(cid:24)(cid:27)(cid:2)(cid:14)(cid:2)(cid:8)(cid:0)(cid:27)(cid:24)(cid:7)(cid:3)(cid:17)(cid:16)(cid:29)(cid:0)

(cid:0)

(cid:0)

(cid:0)

(cid:0)

(cid:0)

(cid:0)

(cid:0)

IJKLMNOPQRIJKLMNSQTUVWXYZ(cid:0)
(cid:18)(cid:8)(cid:15)(cid:2)(cid:25)(cid:2)(cid:0)UVWXYZ\\(cid:18)(cid:8)(cid:26)(cid:13)(cid:7)(cid:15)(cid:17)(cid:4)(cid:2)(cid:10)(cid:8)(cid:3)(cid:2)(cid:25)(cid:0)(cid:10)(cid:24)(cid:27)(cid:2)(cid:14)(cid:2)(cid:8)(cid:0)(cid:27)(cid:24)(cid:7)(cid:8)(cid:3)(cid:17)(cid:16)(cid:29)(cid:0)
^Tbe]Q‘
(cid:0)(cid:0)

(cid:30) (cid:30) (cid:31)](cid:31)fL_‘a]QW_‘afZg]f

UVWXYZ%\\(cid:30) (cid:31)]

^L_‘a]Q^Lbcd

&
]’(

&
f’(

&
]’(

(cid:0)

*+[,(cid:0)

*+h,(cid:0)

(cid:18)(cid:8)(cid:15)(cid:2)(cid:25)(cid:2)(cid:0)

g]f%i]

&
jk(cid:30) i Lbcd
 ’(\\

l(
j
^Tbe]Ql(if
m
^Tbe]Ql((cid:0)(cid:0)

if(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) *+-,(cid:0)
*+/,(cid:0)

n]%be]Lbcd

-.+.o.0 Fpqr(cid:12)(cid:0)(cid:18)(cid:24)(cid:25)(cid:14)(cid:2)(cid:25)(cid:0)(cid:8)(cid:25)(cid:26)(cid:13)(cid:7)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)stuvw89(cid:0)

C+[E(cid:0)(cid:15)(cid:24)(cid:15)(cid:13)(cid:18)(cid:8)(cid:26)(cid:8)(cid:3)(cid:2)(cid:10)(cid:8)(cid:0)(cid:9)(cid:24)(cid:7)(cid:10)(cid:2)(cid:15)(cid:2)(cid:2)(cid:25)(cid:0)Fpqr(cid:12)(cid:0)(cid:17)(cid:25)(cid:16)(cid:17)(cid:3)(cid:0)(cid:2)(cid:7)(cid:24)(cid:2)(cid:0)
(cid:16)(cid:8)(cid:18)(cid:2)(cid:3)(cid:0)(cid:16)(cid:24)(cid:7)(cid:10)(cid:2)(cid:15)(cid:9)(cid:24)(cid:4)(cid:0)(cid:18)(cid:24)(cid:25)(cid:14)(cid:2)(cid:25)(cid:0)(cid:15)(cid:24)(cid:15)(cid:2)(cid:10)(cid:17)(cid:3)(cid:2)(cid:25)(cid:0)(cid:8)(cid:25)(cid:26)(cid:13)(cid:7)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0);tuvw89x4
(cid:10)(cid:24)(cid:5)(cid:8)(cid:25)(cid:14)(cid:14)(cid:2)(cid:0)(cid:9)(cid:24)(cid:25)(cid:18)(cid:17)(cid:14)(cid:2)(cid:0)Fpqr(cid:12)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:18)(cid:2)(cid:24)(cid:7)(cid:2)(cid:5)(cid:0)(cid:16)(cid:8)(cid:18)(cid:2)(cid:3)(cid:0)(cid:16)(cid:24)(cid:7)(cid:10)(cid:2)(cid:15)(cid:9)(cid:24)(cid:4)(cid:0)
(cid:2)(cid:18)(cid:2)(cid:4)(cid:2)(cid:5)(cid:0)(cid:10)(cid:24)(cid:27)(cid:2)(cid:14)(cid:2)(cid:8)(cid:0)(cid:27)(cid:24)(cid:7)(cid:8)(cid:3)(cid:17)(cid:16)(cid:29)(cid:0)

yLz{Q%i]
!""]

j|YT}c~
(cid:18)(cid:8)(cid:15)(cid:2)(cid:25)(cid:2)(cid:0)}c~
(cid:127)(cid:0)(cid:15)(cid:24)(cid:7)(cid:17)(cid:9)(cid:2)(cid:3)(cid:2)(cid:25)(cid:0)(cid:7)(cid:2)(cid:16)(cid:2)(cid:20)(cid:7)(cid:2)(cid:16)(cid:2)(cid:0)(cid:9)(cid:24)(cid:25)(cid:14)(cid:2)(cid:7)(cid:17)(cid:5)(cid:0)(cid:2)(cid:128)(cid:2)(cid:3)(cid:0)(cid:2)(cid:7)(cid:24)(cid:2)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)
;tuvw894(cid:3)(cid:24)(cid:20)(cid:3).(cid:0)C+hE(cid:0)(cid:15)(cid:24)(cid:25)(cid:14)(cid:2)(cid:129)(cid:17)(cid:3)(cid:2)(cid:25)(cid:0)(cid:1)(cid:6)F(cid:0)(cid:2)(cid:7)(cid:24)(cid:2)(cid:0)(cid:16)(cid:8)(cid:18)(cid:2)(cid:3)(cid:0)(cid:16)(cid:24)(cid:7)(cid:10)(cid:2)(cid:15)(cid:9)(cid:24)(cid:4)(cid:0)
(cid:18)(cid:24)(cid:25)(cid:14)(cid:2)(cid:25)(cid:0)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:16)(cid:13)(cid:7)(cid:0)(cid:12)(cid:7)(cid:2)(cid:10)(cid:2)(cid:18)(cid:20)(cid:130)(cid:2)(cid:13)(cid:0)(cid:10)(cid:24)(cid:27)(cid:2)(cid:14)(cid:2)(cid:8)(cid:0)(cid:27)(cid:24)(cid:7)(cid:3)(cid:17)(cid:16)(cid:29)(cid:0)

(cid:127)(cid:0)

*+o,(cid:0)

(cid:131)(cid:132)(cid:133)W!""]

Lz{QZ%U(]Lbc^
T(cid:134)U(cid:135)(cid:136)Lbc^

dQL(cid:127)QTU^]Lbc^
~~~~~~~~~~~
dQ
L(cid:127)Q(cid:0)

dQL(cid:127)Q(cid:0)

U(]Lbc^

dQL(cid:127)Q%

(cid:138)N(cid:139)
(cid:138)N(cid:139)

(cid:140)\\(cid:138)N(cid:139)~~~~
(cid:140))(cid:138)N(cid:139)~~~~

(cid:141)L(cid:142)Q

(cid:0)(cid:0)

(cid:141)L(cid:142)Q

*+(cid:137),(cid:0)

*+(cid:143),(cid:0)

U^]Lbc^
~~~~~~~~~~~
dQ

dQL(cid:127)Q%W_‘ac(cid:144)
L(cid:127)Q(cid:0)(cid:15)(cid:24)(cid:7)(cid:17)(cid:9)(cid:2)(cid:3)(cid:2)(cid:25)(cid:0)(cid:7)(cid:2)(cid:16)(cid:2)(cid:20)(cid:7)(cid:2)(cid:16)(cid:2)(cid:0)(cid:3)(cid:13)(cid:15)(cid:9)(cid:13)(cid:25)(cid:24)(cid:25)(cid:0)U(cid:135)]Lbc^

^
j (cid:145)L(cid:127)Qi]L(cid:127)Q(cid:0) *+(cid:146),(cid:0)
i]L(cid:127)Q
L(cid:127)QZ

dQ(cid:0)

(cid:18)(cid:2)(cid:25)(cid:0)U(cid:135)(cid:136)Lbc^
(cid:18)(cid:2)(cid:24)(cid:7)(cid:2)(cid:5)(cid:0)(cid:18)(cid:24)(cid:25)(cid:14)(cid:2)(cid:25)(cid:0);tuvw89(cid:0)(cid:147)(cid:2)(cid:25)(cid:14)(cid:0)(cid:10)(cid:2)(cid:15)(cid:2).(cid:0)(cid:0)

-.[.0 (cid:1)(cid:24)(cid:16)(cid:13)(cid:18)(cid:24)(cid:0)(cid:12)(cid:24)(cid:25)(cid:14)(cid:17)(cid:15)(cid:9)(cid:17)(cid:4)(cid:2)(cid:25)(cid:0)(cid:19)(cid:2)(cid:16)(cid:2)(cid:0)

(cid:19)(cid:2)(cid:16)(cid:2)(cid:0)(cid:147)(cid:2)(cid:25)(cid:14)(cid:0)(cid:18)(cid:8)(cid:14)(cid:17)(cid:25)(cid:2)(cid:3)(cid:2)(cid:25)(cid:0)(cid:147)(cid:2)(cid:8)(cid:16)(cid:17)(cid:0)(cid:18)(cid:2)(cid:16)(cid:2)(cid:0)(cid:10)(cid:16)(cid:17)(cid:18)(cid:8)(cid:0)(cid:3)(cid:2)(cid:10)(cid:17)(cid:10)(cid:0)(cid:18)(cid:2)(cid:25)(cid:0)(cid:18)(cid:2)(cid:16)(cid:2)(cid:0)
(cid:10)(cid:8)(cid:15)(cid:17)(cid:4)(cid:2)(cid:10)(cid:8)(cid:0)(cid:27)(cid:24)(cid:7)(cid:18)(cid:2)(cid:10)(cid:2)(cid:7)(cid:3)(cid:2)(cid:25)(cid:0)(cid:7)(cid:24)(cid:26)(cid:24)(cid:7)(cid:24)(cid:25)(cid:10)(cid:8)(cid:0)C(cid:146)E(cid:28)(cid:0)(cid:18)(cid:2)(cid:16)(cid:2)(cid:0)(cid:10)(cid:16)(cid:17)(cid:18)(cid:8)(cid:0)(cid:3)(cid:2)(cid:10)(cid:17)(cid:10)(cid:0)(cid:147)(cid:2)(cid:25)(cid:14)(cid:0)
(cid:18)(cid:8)(cid:14)(cid:17)(cid:25)(cid:2)(cid:3)(cid:2)(cid:25)(cid:0)(cid:2)(cid:18)(cid:2)(cid:4)(cid:2)(cid:5)(cid:0)(cid:18)(cid:2)(cid:16)(cid:2)(cid:0)(cid:6)(cid:17)(cid:10)(cid:24)(cid:25)(cid:2)(cid:10)(cid:0)(cid:1)(cid:2)(cid:7)(cid:24)(cid:16)(cid:0)[D+(cid:146)(cid:0)(cid:17)(cid:25)(cid:16)(cid:17)(cid:3)(cid:0)
(cid:15)(cid:24)(cid:25)(cid:14)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:9)(cid:24)(cid:25)(cid:14)(cid:24)(cid:4)(cid:17)(cid:2)(cid:7)(cid:2)(cid:25)(cid:0)(cid:15)(cid:2)(cid:3)(cid:2)(cid:25)(cid:2)(cid:25)(cid:0)(cid:18)(cid:2)(cid:25)(cid:0)(cid:25)(cid:13)(cid:25)(cid:0)(cid:15)(cid:2)(cid:3)(cid:2)(cid:25)(cid:2)(cid:25)(cid:0)(cid:18)(cid:8)(cid:0)
(cid:10)(cid:24)(cid:16)(cid:8)(cid:2)(cid:9)(cid:0)(cid:3)(cid:24)(cid:128)(cid:2)(cid:15)(cid:2)(cid:16)(cid:2)(cid:25)(cid:0)(cid:18)(cid:8)(cid:0)(cid:12)(cid:7)(cid:13)(cid:148)(cid:8)(cid:25)(cid:10)(cid:8)(cid:0)(cid:19).(cid:21).(cid:0)(cid:149)(cid:13)(cid:14)(cid:147)(cid:2)(cid:3)(cid:2)(cid:7)(cid:16)(cid:2)(cid:0)(cid:18)(cid:24)(cid:25)(cid:14)(cid:2)(cid:25)(cid:0)(cid:148)(cid:2)(cid:7)(cid:8)(cid:2)(cid:27)(cid:24)(cid:4)(cid:0)
(cid:27)(cid:2)(cid:25)(cid:16)(cid:17)(cid:0)(cid:18)(cid:2)(cid:16)(cid:2)(cid:0)(cid:12)(cid:13)(cid:18)(cid:24)(cid:10)(cid:0)[D+(cid:146).(cid:0)(cid:0)

(cid:19)(cid:2)(cid:16)(cid:2)(cid:0)(cid:10)(cid:8)(cid:15)(cid:17)(cid:4)(cid:2)(cid:10)(cid:8)(cid:0)(cid:18)(cid:8)(cid:27)(cid:2)(cid:25)(cid:14)(cid:3)(cid:8)(cid:16)(cid:3)(cid:2)(cid:25)(cid:0)(cid:15)(cid:24)(cid:25)(cid:14)(cid:8)(cid:3)(cid:17)(cid:16)(cid:8)(cid:0)(cid:4)(cid:2)(cid:25)(cid:14)(cid:3)(cid:2)(cid:5)(cid:0)
(cid:10)(cid:8)(cid:15)(cid:17)(cid:4)(cid:2)(cid:10)(cid:8)(cid:0)(cid:147)(cid:2)(cid:25)(cid:14)(cid:0)(cid:18)(cid:8)(cid:4)(cid:2)(cid:3)(cid:17)(cid:3)(cid:2)(cid:25)(cid:0)(cid:13)(cid:4)(cid:24)(cid:5)(cid:0)C(cid:143)(cid:28)(cid:146)E(cid:0)(cid:18)(cid:24)(cid:25)(cid:14)(cid:2)(cid:25)(cid:0)(cid:16)(cid:8)(cid:14)(cid:2)(cid:0)(cid:148)(cid:2)(cid:7)(cid:8)(cid:2)(cid:27)(cid:24)(cid:4)(cid:0)
(cid:7)(cid:24)(cid:10)(cid:9)(cid:13)(cid:25)(cid:0)(cid:10)(cid:24)(cid:27)(cid:2)(cid:14)(cid:2)(cid:8)(cid:0)(cid:27)(cid:24)(cid:7)(cid:8)(cid:3)(cid:17)(cid:16)(cid:29)(cid:0)

+.0p(cid:2)(cid:25)(cid:14)(cid:3)(cid:8)(cid:16)(cid:3)(cid:2)(cid:25)i(cid:150)(cid:151)

L(cid:152)Q(cid:153)}(cid:150)(cid:151)
_(cid:153)(cid:157)¡(cid:159)¢%_(cid:153)(cid:157)(cid:153)n£(cid:0)

L(cid:152)Q(cid:153)(cid:133)(cid:150)(cid:151)

L(cid:152)Q(cid:153)(cid:154)(cid:150)(cid:151)

L(cid:152)Q(cid:155)(cid:0)(cid:156)%_(cid:153)(cid:157)(cid:158)(cid:159)\\(cid:160)\\%

[.0⁄(cid:8)(cid:16)(cid:17)(cid:25)(cid:14)(cid:0)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:4)(cid:2)(cid:25)(cid:14)(cid:10)(cid:17)(cid:25)(cid:14)(cid:0)!""(

(cid:152)(cid:153)!""^

(cid:152)(cid:153)!""(cid:135)

(cid:152)(cid:28)(cid:0)(cid:18)(cid:8)(cid:15)(cid:2)(cid:25)(cid:2)(cid:0)¥]%

ƒ|T}]T(cid:133)](cid:0)

h.0⁄(cid:8)(cid:16)(cid:17)(cid:25)(cid:14)(cid:0)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)Fpqr(cid:12)(cid:28)(cid:0)!""](cid:150)

y(cid:0)

#$(cid:0)
-.0⁄(cid:8)(cid:16)(cid:17)(cid:25)(cid:14)(cid:0)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)G677898:;84H8:;=>?9@6:Ax4!""](cid:150)
/.0⁄(cid:8)(cid:16)(cid:17)(cid:25)(cid:14)(cid:0)(cid:25)(cid:8)(cid:4)(cid:2)(cid:8)(cid:0)(cid:1)(cid:6)F(cid:0)(cid:18)(cid:2)(cid:7)(cid:8)(cid:0)(cid:15)(cid:2)(cid:10)(cid:8)(cid:25)(cid:14)(cid:20)(cid:15)(cid:2)(cid:10)(cid:8)(cid:25)(cid:14)(cid:0)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4).(cid:0)
o.0r(cid:4)(cid:2)(cid:25)(cid:14)(cid:8)(cid:0)(cid:4)(cid:2)(cid:25)(cid:14)(cid:3)(cid:2)(cid:5)(cid:0)+(cid:0)(cid:10).(cid:18)(cid:0)/(cid:0)(cid:15)(cid:2)(cid:10)(cid:8)(cid:25)(cid:14)(cid:20)(cid:15)(cid:2)(cid:10)(cid:8)(cid:25)(cid:14)(cid:0)(cid:10)(cid:24)(cid:27)(cid:2)(cid:25)(cid:147)(cid:2)(cid:3)(cid:0)

n%_§§(cid:0)(cid:3)(cid:2)(cid:4)(cid:8)(cid:0)(cid:9)(cid:24)(cid:7)(cid:17)(cid:4)(cid:2)(cid:25)(cid:14)(cid:2)(cid:25).(cid:0)

¤(cid:17)(cid:15)(cid:4)(cid:2)(cid:5)(cid:0)(cid:18)(cid:13)(cid:15)(cid:2)(cid:8)(cid:25)(cid:0)(cid:147)(cid:2)(cid:25)(cid:14)(cid:0)(cid:18)(cid:8)(cid:14)(cid:17)(cid:25)(cid:2)(cid:3)(cid:2)(cid:25)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:9)(cid:24)(cid:25)(cid:24)(cid:4)(cid:8)(cid:16)(cid:8)(cid:2)(cid:25)(cid:0)(cid:8)(cid:25)(cid:8)(cid:0)(cid:10)(cid:24)(cid:27)(cid:25)(cid:147)(cid:2)(cid:3)(cid:0)
(cid:158)%'§(cid:153)“§(cid:153)_§§(cid:153)\\(cid:18)(cid:2)(cid:25)\\(cid:134)§§(cid:0)(cid:13)(cid:27)(cid:10)(cid:24)(cid:7)(cid:148)(cid:2)(cid:10)(cid:8)(cid:0)(cid:18)(cid:24)(cid:25)(cid:14)(cid:2)(cid:25)(cid:0)¡%'(cid:0)(cid:148)(cid:2)(cid:7)(cid:8)(cid:2)(cid:27)(cid:24)(cid:4).(cid:0)
(cid:21)(cid:25)(cid:8)(cid:10)(cid:8)(cid:2)(cid:10)(cid:8)(cid:0)(cid:17)(cid:25)(cid:16)(cid:17)(cid:3)(cid:0)(cid:9)(cid:24)(cid:25)(cid:14)(cid:2)(cid:7)(cid:17)(cid:5)(cid:0)(cid:2)(cid:128)(cid:2)(cid:3)(cid:0)(cid:2)(cid:7)(cid:24)(cid:2)(cid:0)bd((%§£(cid:134)(cid:153)bd^^%
§£«(cid:153)bd(cid:135)(cid:135)%_£‹(cid:28)(cid:0)(cid:17)(cid:25)(cid:16)(cid:17)(cid:3)(cid:0)v?>›t6:A4899ﬁ94be((%§£_“(cid:153)\\be^^%
§£(cid:134)“(cid:153)be(cid:135)(cid:135)%§£'“4(cid:18)(cid:2)(cid:25)(cid:0)(cid:25)(cid:8)(cid:4)(cid:2)(cid:8)(cid:0)(cid:3)(cid:13)(cid:24)(cid:26)(cid:8)(cid:10)(cid:8)(cid:24)(cid:25)(cid:0)(cid:27)(cid:24)(cid:16)(cid:2)(cid:0)|(%|^%_(cid:0)(cid:10)(cid:24)(cid:7)(cid:16)(cid:2)(cid:0)
(cid:25)(cid:8)(cid:4)(cid:2)(cid:8)(cid:0)ﬂe(cid:0)(cid:18)(cid:2)(cid:25)(cid:0)ﬂ(cid:0)(cid:18)(cid:8)(cid:16)(cid:24)(cid:25)(cid:16)(cid:17)(cid:3)(cid:2)(cid:25)(cid:0)(cid:10)(cid:24)(cid:27)(cid:24)(cid:10)(cid:2)(cid:7)(cid:0)D./(cid:0)(cid:17)(cid:25)(cid:16)(cid:17)(cid:3)(cid:0)(cid:10)(cid:24)(cid:16)(cid:8)(cid:2)(cid:9)(cid:0)(cid:8)(cid:16)(cid:24)(cid:7)(cid:2)(cid:10)(cid:8).(cid:0)

-.h.0 (cid:176)(cid:2)(cid:5)(cid:2)(cid:9)(cid:2)(cid:25)(cid:0)(cid:12)(cid:24)(cid:25)(cid:24)(cid:4)(cid:8)(cid:16)(cid:8)(cid:2)(cid:25)(cid:0)

(cid:12)(cid:2)(cid:18)(cid:2)(cid:0)(cid:9)(cid:24)(cid:25)(cid:24)(cid:4)(cid:8)(cid:16)(cid:8)(cid:2)(cid:25)(cid:0)(cid:8)(cid:25)(cid:8)(cid:0)(cid:15)(cid:24)(cid:25)(cid:14)(cid:14)(cid:17)(cid:25)(cid:2)(cid:3)(cid:2)(cid:25)(cid:0)(cid:15)(cid:24)(cid:16)(cid:13)(cid:18)(cid:24)(cid:0)G8v6A:4

v;68:;8498v8?9;=(cid:0)(cid:18)(cid:24)(cid:25)(cid:14)(cid:2)(cid:25)(cid:0)(cid:16)(cid:2)(cid:5)(cid:2)(cid:9)(cid:2)(cid:25)(cid:0)(cid:10)(cid:24)(cid:27)(cid:2)(cid:14)(cid:2)(cid:8)(cid:0)(cid:27)(cid:24)(cid:7)(cid:8)(cid:3)(cid:17)(cid:16)(cid:0)C+hE(cid:29)(cid:0)
+.0B–?98:8vv4ﬁ74›9ﬁHt8>(cid:0)

(cid:176)(cid:2)(cid:5)(cid:2)(cid:9)(cid:0)(cid:9)(cid:24)(cid:25)(cid:14)(cid:8)(cid:25)(cid:18)(cid:24)(cid:25)(cid:16)(cid:8)(cid:26)(cid:8)(cid:3)(cid:2)(cid:10)(cid:8)(cid:2)(cid:25)(cid:0)(cid:15)(cid:2)(cid:10)(cid:2)(cid:4)(cid:2)(cid:5)(cid:0)(cid:16)(cid:24)(cid:7)(cid:3)(cid:2)(cid:8)(cid:16)(cid:0)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)†(cid:2)(cid:147)(cid:20)
⁄(cid:24)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)(cid:15)(cid:17)(cid:4)(cid:16)(cid:8)(cid:148)(cid:2)(cid:7)(cid:8)(cid:2)(cid:16).(cid:0)

[.02uA8vw6ﬁ:(cid:0)

(cid:176)(cid:2)(cid:5)(cid:2)(cid:9)(cid:0)(cid:9)(cid:24)(cid:25)(cid:128)(cid:2)(cid:7)(cid:8)(cid:2)(cid:25)(cid:0)(cid:10)(cid:13)(cid:4)(cid:17)(cid:10)(cid:8)(cid:0)(cid:18)(cid:2)(cid:7)(cid:8)(cid:0)(cid:15)(cid:2)(cid:10)(cid:2)(cid:4)(cid:2)(cid:5)(cid:0)(cid:147)(cid:2)(cid:25)(cid:14)(cid:0)(cid:16)(cid:24)(cid:4)(cid:2)(cid:5)(cid:0)
(cid:18)(cid:8)(cid:8)(cid:18)(cid:24)(cid:25)(cid:16)(cid:8)(cid:26)(cid:8)(cid:3)(cid:2)(cid:10)(cid:8).(cid:0) (cid:6)(cid:16)(cid:17)(cid:18)(cid:8)(cid:0) (cid:4)(cid:8)(cid:16)(cid:24)(cid:7)(cid:2)(cid:16)(cid:17)(cid:7)(cid:0) (cid:18)(cid:8)(cid:4)(cid:2)(cid:3)(cid:17)(cid:3)(cid:2)(cid:25)(cid:0) (cid:17)(cid:25)(cid:16)(cid:17)(cid:3)(cid:0)
(cid:15)(cid:24)(cid:25)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:3)(cid:2)(cid:25)(cid:0) (cid:10)(cid:13)(cid:4)(cid:17)(cid:10)(cid:8)(cid:0) (cid:16)(cid:24)(cid:7)(cid:3)(cid:2)(cid:8)(cid:16)(cid:0) (cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0) †(cid:2)(cid:147)(cid:20)⁄(cid:24)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)
(cid:15)(cid:17)(cid:4)(cid:16)(cid:8)(cid:148)(cid:2)(cid:7)(cid:8)(cid:2)(cid:16).(cid:0)

h.058‡8tﬁ›>8:w(cid:0)

(cid:176)(cid:2)(cid:5)(cid:2)(cid:9)(cid:0)(cid:9)(cid:24)(cid:15)(cid:27)(cid:2)(cid:25)(cid:14)(cid:17)(cid:25)(cid:2)(cid:25)(cid:0)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)(cid:18)(cid:2)(cid:25)(cid:0)(cid:9)(cid:7)(cid:13)(cid:14)(cid:7)(cid:2)(cid:15)(cid:0)(cid:2)(cid:9)(cid:4)(cid:8)(cid:3)(cid:2)(cid:10)(cid:8)(cid:0)(cid:17)(cid:25)(cid:16)(cid:17)(cid:3)(cid:0)
(cid:15)(cid:24)(cid:25)(cid:129)(cid:2)·(cid:2)(cid:27)(cid:0)(cid:9)(cid:24)(cid:7)(cid:15)(cid:2)(cid:10)(cid:2)(cid:4)(cid:2)(cid:5)(cid:2)(cid:25)(cid:0)(cid:27)(cid:24)(cid:7)(cid:18)(cid:2)(cid:10)(cid:2)(cid:7)(cid:3)(cid:2)(cid:25)(cid:0)(cid:10)(cid:13)(cid:4)(cid:17)(cid:10)(cid:8)(cid:0)(cid:147)(cid:2)(cid:25)(cid:14)(cid:0)(cid:16)(cid:24)(cid:4)(cid:2)(cid:5)(cid:0)
(cid:18)(cid:8)(cid:0)(cid:16)(cid:24)(cid:25)(cid:16)(cid:17)(cid:3)(cid:2)(cid:25)(cid:0)(cid:10)(cid:24)(cid:27)(cid:24)(cid:4)(cid:17)(cid:15)(cid:25)(cid:147)(cid:2).(cid:0)

-.03‡?tu?w6ﬁ:(cid:0)

(cid:176)(cid:2)(cid:5)(cid:2)(cid:9)(cid:0)(cid:9)(cid:24)(cid:25)(cid:14)(cid:24)(cid:148)(cid:2)(cid:4)(cid:17)(cid:2)(cid:10)(cid:8)(cid:2)(cid:25)(cid:0)(cid:2)(cid:4)(cid:14)(cid:13)(cid:7)(cid:8)(cid:16)(cid:15)(cid:2)(cid:0)(cid:18)(cid:2)(cid:25)(cid:0)(cid:9)(cid:7)(cid:13)(cid:14)(cid:7)(cid:2)(cid:15)(cid:0)(cid:147)(cid:2)(cid:25)(cid:14)(cid:0)
(cid:16)(cid:24)(cid:4)(cid:2)(cid:5)(cid:0)(cid:18)(cid:8)(cid:27)(cid:2)(cid:14)(cid:17)(cid:25).(cid:0)p(cid:24)(cid:27)(cid:24)(cid:7)(cid:2)(cid:9)(cid:2)(cid:0)(cid:17)(cid:129)(cid:8)(cid:0)(cid:10)(cid:24)(cid:9)(cid:24)(cid:7)(cid:16)(cid:8)(cid:0)(cid:17)(cid:129)(cid:8)(cid:0)(cid:148)(cid:2)(cid:4)(cid:8)(cid:18)(cid:8)(cid:16)(cid:2)(cid:10)(cid:28)(cid:0)(cid:17)(cid:129)(cid:8)(cid:0)
(cid:9)(cid:24)(cid:7)(cid:26)(cid:13)(cid:7)(cid:15)(cid:2)(cid:28)(cid:0)(cid:17)(cid:129)(cid:8)(cid:0)(cid:6)r(cid:6)(cid:28)(cid:0)(cid:18)(cid:2)(cid:25)(cid:0)(cid:10)(cid:16)(cid:17)(cid:18)(cid:8)(cid:0)(cid:3)(cid:2)(cid:10)(cid:17)(cid:10)(cid:0)(cid:18)(cid:8)(cid:4)(cid:2)(cid:3)(cid:10)(cid:2)(cid:25)(cid:2)(cid:3)(cid:2)(cid:25)(cid:0)(cid:18)(cid:2)(cid:4)(cid:2)(cid:15)(cid:0)
(cid:16)(cid:2)(cid:5)(cid:2)(cid:9)(cid:0)(cid:24)(cid:148)(cid:2)(cid:4)(cid:17)(cid:2)(cid:10)(cid:8)(cid:0)(cid:8)(cid:25)(cid:8).(cid:0)

(cid:181)¶•sﬁ:;tuv6ﬁ:4

(cid:176)(cid:2)(cid:5)(cid:2)(cid:9)(cid:0)(cid:9)(cid:24)(cid:25)(cid:2)(cid:7)(cid:8)(cid:3)(cid:2)(cid:25)(cid:0)(cid:3)(cid:24)(cid:10)(cid:8)(cid:15)(cid:9)(cid:17)(cid:4)(cid:2)(cid:25)(cid:0)(cid:18)(cid:2)(cid:25)(cid:0)(cid:10)(cid:2)(cid:7)(cid:2)(cid:25)(cid:0)(cid:16)(cid:24)(cid:7)(cid:5)(cid:2)(cid:18)(cid:2)(cid:9)(cid:0)(cid:10)(cid:24)(cid:4)(cid:17)(cid:7)(cid:17)(cid:5)(cid:0)
(cid:16)(cid:2)(cid:5)(cid:2)(cid:9)(cid:2)(cid:25)(cid:0)(cid:9)(cid:24)(cid:25)(cid:24)(cid:4)(cid:8)(cid:16)(cid:8)(cid:2)(cid:25)(cid:0)(cid:147)(cid:2)(cid:25)(cid:14)(cid:0)(cid:16)(cid:24)(cid:4)(cid:2)(cid:5)(cid:0)(cid:18)(cid:8)(cid:4)(cid:2)(cid:3)(cid:10)(cid:2)(cid:25)(cid:2)(cid:3)(cid:2)(cid:25).(cid:0)

(cid:22).0(cid:23)F(cid:130)‚„”(cid:23)‚(cid:0)(cid:12)(cid:21)(cid:23)(cid:21)(cid:130)(cid:0)

(cid:23)(cid:24)(cid:7)(cid:2)(cid:25)(cid:14)(cid:3)(cid:2)(cid:0)(cid:9)(cid:8)(cid:3)(cid:8)(cid:7)(cid:0)(cid:9)(cid:24)(cid:25)(cid:24)(cid:4)(cid:8)(cid:16)(cid:8)(cid:2)(cid:25)(cid:0)(cid:8)(cid:25)(cid:8)(cid:0)(cid:18)(cid:8)(cid:16)(cid:17)(cid:25)(cid:129)(cid:17)(cid:3)(cid:2)(cid:25)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:14)(cid:2)(cid:15)(cid:27)(cid:2)(cid:7)(cid:0)+(cid:0)

(cid:27)(cid:24)(cid:7)(cid:8)(cid:3)(cid:17)(cid:16)(cid:29)(cid:0)

”(cid:2)(cid:15)(cid:27)(cid:2)(cid:7)(cid:0)+.(cid:0)(cid:23)(cid:24)(cid:7)(cid:2)(cid:25)(cid:14)(cid:3)(cid:2)(cid:0)(cid:9)(cid:8)(cid:3)(cid:8)(cid:7)(cid:0)

(cid:0)

(cid:0)

(cid:0)

(cid:1)(cid:2)(cid:3)(cid:2)(cid:4)(cid:2)(cid:5)(cid:0)(cid:6)(cid:3)(cid:7)(cid:8)(cid:9)(cid:10)(cid:8)(cid:0)(cid:11)(cid:0)(cid:12)(cid:7)(cid:13)(cid:14)(cid:7)(cid:2)(cid:15)(cid:0)(cid:6)(cid:16)(cid:17)(cid:18)(cid:8)(cid:0)(cid:19)(cid:20)(cid:21)(cid:22)(cid:0)(cid:23)(cid:13)(cid:15)(cid:9)(cid:17)(cid:16)(cid:2)(cid:10)(cid:8)(cid:0)(cid:6)(cid:16)(cid:2)(cid:16)(cid:8)(cid:10)(cid:16)(cid:8)(cid:3)(cid:0)

(cid:1)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)(cid:25)(cid:26)(cid:8)(cid:27)(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:24)(cid:0)(cid:28)(cid:2)(cid:29)(cid:20)(cid:30)(cid:24)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)(cid:18)(cid:24)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)(cid:18)(cid:2)(cid:26)(cid:0)(cid:16)(cid:2)(cid:26)(cid:9)(cid:2)(cid:0)(cid:31) !!""#""$%""&
’""$%()*#+ $,&(cid:16)(cid:24)(cid:4)(cid:2)(cid:5)(cid:0)(cid:18)(cid:8)(cid:8)(cid:15)(cid:9)(cid:4)(cid:24)(cid:15)(cid:24)(cid:26)(cid:16)(cid:2)(cid:10)(cid:8)(cid:3)(cid:2)(cid:26)(cid:0)-(cid:2)(cid:8)(cid:3)(cid:0)(cid:18)(cid:2)(cid:4)(cid:2)(cid:15)(cid:0)-(cid:24)(cid:26)(cid:16)(cid:17)(cid:3)(cid:0)
.(cid:25)(cid:21)(cid:0)(cid:15)(cid:2)(cid:17)(cid:9)(cid:17)(cid:26)(cid:0)/0(cid:21)(cid:0)(cid:29)(cid:2)(cid:26)(cid:14)(cid:0)-(cid:24)(cid:7)-(cid:24)(cid:26)(cid:16)(cid:17)(cid:3)(cid:0)(cid:9)(cid:2)1(cid:3)(cid:2)(cid:14)(cid:24)(cid:0)(cid:18)(cid:8)(cid:0)23(cid:0)4(cid:24)(cid:14)(cid:8)(cid:16)(cid:17)(cid:0)(cid:9)(cid:17)(cid:4)(cid:2)(cid:0)
(cid:18)(cid:24)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)(cid:1)(cid:17)(cid:4)(cid:16)(cid:8)(cid:27)(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:24)(cid:0)(cid:28)(cid:2)(cid:29)(cid:20)(cid:30)(cid:24)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)(cid:16)(cid:2)(cid:26)(cid:9)(cid:2)(cid:0)-(cid:24)(cid:26)1(cid:5)(cid:15)(cid:2)(cid:7)(cid:3)(cid:8)(cid:26)(cid:14)(cid:0)
(cid:29)(cid:2)(cid:26)(cid:14)(cid:0)5*%+*,""&(cid:26)(cid:29)(cid:2)(cid:0)(cid:16)(cid:24)(cid:4)(cid:2)(cid:5)(cid:0)(cid:18)(cid:8)-(cid:2)(cid:26)(cid:14)(cid:17)(cid:26)(cid:0)(cid:13)(cid:4)(cid:24)(cid:5)(cid:0)6(cid:13)(cid:27)(cid:8)(cid:2)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:16)(cid:2)(cid:5)(cid:17)(cid:26)(cid:0)78783(cid:0)
6(cid:2)(cid:15)(cid:17)(cid:26)(cid:0)(cid:9)(cid:24)(cid:26)(cid:14)(cid:8)(cid:15)(cid:9)(cid:4)(cid:24)(cid:15)(cid:24)(cid:26)(cid:16)(cid:2)(cid:10)(cid:8)(cid:2)(cid:26)(cid:0)(cid:15)(cid:24)(cid:16)(cid:13)(cid:18)(cid:24)(cid:0)(cid:19)(cid:8)99(cid:24)(cid:7)(cid:24)(cid:26)1(cid:24)(cid:0)4(cid:24)(cid:26)1(cid:5)(cid:15)(cid:2)(cid:7)(cid:3)(cid:8)(cid:26)(cid:14)(cid:0)
(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)(cid:1)(cid:17)(cid:4)(cid:16)(cid:8)(cid:27)(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:24)(cid:0)(cid:28)(cid:2)(cid:29)(cid:20)(cid:30)(cid:24)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)-(cid:24)(cid:4)(cid:17)(cid:15)(cid:0)(cid:16)(cid:24)(cid:7)(cid:8)(cid:15)(cid:9)(cid:4)(cid:24)(cid:15)(cid:24)(cid:26)(cid:16)(cid:2)(cid:10)(cid:8)(cid:3)(cid:0)
-(cid:2)(cid:8)(cid:3)(cid:0)(cid:18)(cid:2)(cid:4)(cid:2)(cid:15)(cid:0)-(cid:24)(cid:26)(cid:16)(cid:17)(cid:3)(cid:0).(cid:25)(cid:21)(cid:0)(cid:15)(cid:2)(cid:17)(cid:9)(cid:17)(cid:26)(cid:0)/0(cid:21)3(cid:0)(cid:6)(cid:24)(cid:5)(cid:8)(cid:26)(cid:14)(cid:14)(cid:2)(cid:0)(cid:9)(cid:24)(cid:26)(cid:17)(cid:4)(cid:8)(cid:10)(cid:0)
-(cid:24)(cid:7)(cid:16)(cid:17):(cid:17)(cid:2)(cid:26)(cid:0)(cid:17)(cid:26)(cid:16)(cid:17)(cid:3)(cid:0)(cid:15)(cid:24)(cid:15)-(cid:2)(cid:26)(cid:14)(cid:17)(cid:26)(cid:0)(cid:10)(cid:17)(cid:2)(cid:16)(cid:17)(cid:0)(cid:9)(cid:2)1(cid:3)(cid:2)(cid:14)(cid:24)(cid:0)2(cid:0)(cid:29)(cid:2)(cid:26)(cid:14)(cid:0)
(cid:15)(cid:24)(cid:26)(cid:14)(cid:8)(cid:15)(cid:9)(cid:4)(cid:24)(cid:15)(cid:24)(cid:26)(cid:16)(cid:2)(cid:10)(cid:8)(cid:3)(cid:2)(cid:26)(cid:0)(cid:15)(cid:24)(cid:16)(cid:13)(cid:18)(cid:24)(cid:0)(cid:19)(cid:8)99(cid:24)(cid:7)(cid:24)(cid:26)1(cid:24)(cid:0)4(cid:24)(cid:26)1(cid:5)(cid:15)(cid:2)(cid:7)(cid:3)(cid:8)(cid:26)(cid:14)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)
(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)(cid:1)(cid:17)(cid:4)(cid:16)(cid:8)(cid:27)(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:24)(cid:0)(cid:28)(cid:2)(cid:29)(cid:20)(cid:30)(cid:24)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)(cid:29)(cid:2)(cid:26)(cid:14)(cid:0)(cid:15)(cid:17)(cid:18)(cid:2)(cid:5)(cid:0)(cid:18)(cid:8)(cid:2)(cid:3)(cid:10)(cid:24)(cid:10)(cid:0)(cid:10)(cid:24)(cid:7)(cid:16)(cid:2)(cid:0)
(cid:15)(cid:17)(cid:18)(cid:2)(cid:5)(cid:0)(cid:18)(cid:8)(cid:14)(cid:17)(cid:26)(cid:2)(cid:3)(cid:2)(cid:26)3(cid:0)

(cid:22)(cid:21)3;(cid:30)<(cid:6)(cid:21)0(cid:0)(cid:19)<6(cid:0)(cid:12)=(cid:1)4<(cid:30)<(cid:6)<6(cid:0)

>3?3;@A*#""$""BB&C!&5#CDE"")(cid:0)

(cid:12)(cid:24)(cid:26)(cid:14)(cid:14)(cid:17)(cid:26)(cid:2)(cid:2)(cid:26)(cid:0)(cid:6)<=(cid:0)(cid:1)(cid:17)(cid:4)(cid:16)(cid:8)(cid:27)(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:17)(cid:26)(cid:16)(cid:17)(cid:3)(cid:0)(cid:13)99(cid:8)1(cid:8)(cid:2)(cid:4)(cid:0)(cid:10)(cid:16)(cid:2)(cid:16)(cid:8)(cid:10)(cid:16)(cid:8)1(cid:10)(cid:0)
(cid:5)(cid:2)(cid:7)(cid:17)(cid:10)(cid:0)(cid:15)(cid:24)(cid:15)(cid:24)(cid:26)(cid:17)(cid:5)(cid:8)(cid:0)(cid:10)(cid:29)(cid:2)(cid:7)(cid:2)(cid:16)(cid:0)(cid:3)(cid:13)(cid:26)(cid:10)(cid:8)(cid:10)(cid:16)(cid:24)(cid:26)(cid:10)(cid:8)(cid:0)(cid:2)(cid:14)(cid:7)(cid:24)(cid:14)(cid:2)(cid:10)(cid:8)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:2)(cid:7)(cid:24)(cid:2)(cid:0)(cid:29)(cid:2)(cid:26)(cid:14)(cid:0)
(cid:4)(cid:24)-(cid:8)(cid:5)(cid:0)-(cid:24)(cid:10)(cid:2)(cid:7)F(cid:0)(cid:26)(cid:2)(cid:15)(cid:17)(cid:26)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:9)(cid:24)(cid:26)(cid:14)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:2)(cid:26)(cid:0)(cid:6)<=(cid:0)(cid:1)(cid:17)(cid:4)(cid:16)(cid:8)(cid:27)(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)
(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)=40(cid:25)(cid:12)(cid:0)(cid:16)(cid:8)(cid:18)(cid:2)(cid:3)(cid:0)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:15)(cid:24)(cid:26)(cid:14)(cid:5)(cid:2)(cid:10)(cid:8)(cid:4)(cid:3)(cid:2)(cid:26)(cid:0)(cid:2)(cid:14)(cid:7)(cid:24)(cid:14)(cid:2)(cid:10)(cid:8)(cid:0)(cid:29)(cid:2)(cid:26)(cid:14)(cid:0)
(cid:3)(cid:13)(cid:26)(cid:10)(cid:8)(cid:10)(cid:16)(cid:24)(cid:26)(cid:0)(cid:18)(cid:24)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:4)(cid:2)(cid:26)(cid:14)(cid:10)(cid:17)(cid:26)(cid:14)(cid:0)(cid:2)(cid:7)(cid:24)(cid:2)(cid:0)(cid:18)(cid:8)(cid:2)(cid:16)(cid:2)(cid:10)(cid:26)(cid:29)(cid:2)3(cid:0)(cid:6)(cid:24)(cid:4)(cid:2)(cid:8)(cid:26)(cid:0)(cid:8)(cid:16)(cid:17)(cid:0)
:(cid:17)(cid:14)(cid:2)(cid:0)-(cid:24)(cid:4)(cid:17)(cid:15)(cid:0)(cid:2)(cid:18)(cid:2)(cid:0)(cid:16)(cid:13)(cid:13)(cid:4)(cid:10)(cid:0)(cid:10)(cid:16)(cid:2)(cid:16)(cid:8)(cid:10)(cid:16)(cid:8)(cid:3)(cid:0)(cid:29)(cid:2)(cid:26)(cid:14)(cid:0)(cid:15)(cid:24)(cid:26)(cid:14)(cid:8)(cid:15)(cid:9)(cid:4)(cid:24)(cid:15)(cid:24)(cid:26)(cid:16)(cid:2)(cid:10)(cid:8)(cid:3)(cid:2)(cid:26)(cid:0)
(cid:15)(cid:24)(cid:16)(cid:13)(cid:18)(cid:24)(cid:0)G !!""#""$%""&D""$%()*#+ $,&(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)(cid:28)(cid:2)(cid:29)(cid:20)(cid:30)(cid:24)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)
(cid:15)(cid:17)(cid:4)(cid:16)(cid:8)(cid:27)(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)-(cid:2)(cid:8)(cid:3)(cid:0)(cid:18)(cid:2)(cid:4)(cid:2)(cid:15)(cid:0)-(cid:24)(cid:26)(cid:16)(cid:17)(cid:3)(cid:0).(cid:25)(cid:21)(cid:0)(cid:15)(cid:2)(cid:17)(cid:9)(cid:17)(cid:26)(cid:0)/0(cid:21)3(cid:0)

(cid:0)

>373; HI,,""BJ C$(cid:0)

(cid:6)(cid:13)(cid:4)(cid:17)(cid:10)(cid:8)(cid:0) (cid:29)(cid:2)(cid:26)(cid:14)(cid:0) (cid:9)(cid:24)(cid:26)(cid:17)(cid:4)(cid:8)(cid:10)(cid:0) (cid:16)(cid:2)K(cid:2)(cid:7)(cid:3)(cid:2)(cid:26)(cid:0) (cid:17)(cid:26)(cid:16)(cid:17)(cid:3)(cid:0) (cid:15)(cid:24)(cid:26):(cid:2)K(cid:2)-(cid:0)
(cid:9)(cid:24)(cid:7)(cid:15)(cid:2)(cid:10)(cid:2)(cid:4)(cid:2)(cid:5)(cid:2)(cid:26)(cid:0)(cid:16)(cid:24)(cid:7)(cid:10)(cid:24)-(cid:17)(cid:16)(cid:0)(cid:2)(cid:18)(cid:2)(cid:0)(cid:9)(cid:24)(cid:15)-(cid:2)(cid:26)(cid:14)(cid:17)(cid:26)(cid:2)(cid:26)(cid:0)(cid:10)(cid:17)(cid:2)(cid:16)(cid:17)(cid:0)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)(cid:29)(cid:2)(cid:26)(cid:14)(cid:0)
(cid:15)(cid:24)(cid:26)(cid:14)(cid:8)(cid:15)(cid:9)(cid:4)(cid:24)(cid:15)(cid:24)(cid:26)(cid:16)(cid:2)(cid:10)(cid:8)(cid:3)(cid:2)(cid:26)(cid:0)(cid:15)(cid:24)(cid:16)(cid:13)(cid:18)(cid:24)(cid:0)G !!""#""$%""&D""$%()*#+ $,(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)
(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)(cid:28)(cid:2)(cid:29)(cid:20)(cid:30)(cid:24)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)(cid:15)(cid:17)(cid:4)(cid:16)(cid:8)(cid:27)(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:24)(cid:0)(cid:10)(cid:24)(cid:7)(cid:16)(cid:2)(cid:0)(cid:15)(cid:24)(cid:26)(cid:14)(cid:8)(cid:15)(cid:9)(cid:4)(cid:24)(cid:15)(cid:24)(cid:26)(cid:16)(cid:2)(cid:10)(cid:8)(cid:3)(cid:2)(cid:26)(cid:0)
(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)(cid:29)(cid:2)(cid:26)(cid:14)(cid:0)(cid:16)(cid:24)(cid:4)(cid:2)(cid:5)(cid:0)(cid:18)(cid:8)-(cid:2)(cid:26)(cid:14)(cid:17)(cid:26)(cid:0)(cid:18)(cid:2)(cid:4)(cid:2)(cid:15)(cid:0)(cid:10)(cid:24)-(cid:17)(cid:2)(cid:5)(cid:0)(cid:9)(cid:2)(cid:3)(cid:24)(cid:16)(cid:0)23(cid:0)

nwpqrsXRxyzRSxSd{{R|x}~R(cid:127)

Sx(cid:128)

S(cid:0)

(cid:0)
(cid:18)(cid:8)(cid:15)(cid:2)(cid:26)(cid:2)(cid:0)
(cid:0)

(cid:0)

~X(cid:130)o

Sp(cid:130)Syzbcs

(cid:130)(cid:131)(cid:0)(cid:132)(cid:0)xX(cid:133)yz

(cid:134)(cid:133)Syz(cid:0)

bc

Y7(cid:129)Z(cid:0)

Y7(cid:135)Z(cid:0)

>3L373; (cid:19)(cid:8)99(cid:24)(cid:7)(cid:24)(cid:26)1(cid:24)(cid:0) ’""$%()*#+ $,(cid:0) (cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0) (cid:28)(cid:2)(cid:29)(cid:20)(cid:30)(cid:24)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)

(cid:15)(cid:17)(cid:4)(cid:16)(cid:27)(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:18)(cid:24)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)(cid:8)(cid:26)9(cid:13)(cid:7)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:136)EIBJ""#(cid:0)

(cid:12)(cid:2)(cid:18)(cid:2)(cid:0)(cid:9)(cid:24)(cid:26)(cid:24)(cid:4)(cid:8)(cid:16)(cid:8)(cid:2)(cid:26)(cid:0)(cid:8)(cid:26)(cid:8)(cid:0):(cid:17)(cid:14)(cid:2)(cid:0)(cid:15)(cid:24)(cid:26)(cid:14)(cid:2):(cid:17)(cid:3)(cid:2)(cid:26)(cid:0)(cid:10)(cid:24)-(cid:17)(cid:2)(cid:5)(cid:0)(cid:15)(cid:24)(cid:16)(cid:13)(cid:18)(cid:24)(cid:0)
(cid:19)(cid:8)99(cid:24)(cid:7)(cid:24)(cid:26)1(cid:24)(cid:0)4(cid:24)(cid:26)1(cid:5)(cid:15)(cid:2)(cid:7)(cid:3)(cid:8)(cid:26)(cid:14)(cid:0)(cid:1)(cid:17)(cid:4)(cid:16)(cid:8)(cid:27)(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:24)(cid:0)(cid:28)(cid:2)(cid:29)(cid:20)(cid:30)(cid:24)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)(cid:1)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)
(cid:17)(cid:26)(cid:16)(cid:17)(cid:3)(cid:0)(cid:2)(cid:7)(cid:24)(cid:2)(cid:0)(cid:29)(cid:2)(cid:26)(cid:14)(cid:0)(cid:16)(cid:8)(cid:18)(cid:2)(cid:3)(cid:0)(cid:16)(cid:24)(cid:7)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:10)(cid:2)(cid:15)(cid:9)(cid:24)(cid:4)(cid:0)(cid:15)(cid:24)(cid:26)(cid:14)(cid:14)(cid:17)(cid:26)(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)(cid:8)(cid:26)9(cid:13)(cid:7)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)
1(cid:4)(cid:17)(cid:10)(cid:16)(cid:24)(cid:7)3(cid:0)(cid:12)(cid:24)(cid:26)(cid:14)(cid:5)(cid:8)(cid:16)(cid:17)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)(cid:9)(cid:24)(cid:26)(cid:18)(cid:17)(cid:14)(cid:2)(cid:0)=40(cid:25)(cid:12)(cid:0)(cid:18)(cid:24)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)1(cid:4)(cid:17)(cid:10)(cid:16)(cid:24)(cid:7)(cid:0)
(cid:18)(cid:8)(cid:4)(cid:2)(cid:3)(cid:17)(cid:3)(cid:2)(cid:26)(cid:0)(cid:18)(cid:24)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)1(cid:2)(cid:7)(cid:2)(cid:0)(cid:29)(cid:2)(cid:26)(cid:14)(cid:0)(cid:10)(cid:2)(cid:15)(cid:2)(cid:0)(cid:5)(cid:2)(cid:4)(cid:26)(cid:29)(cid:2)(cid:0)(cid:15)(cid:24)(cid:26)(cid:14)(cid:14)(cid:17)(cid:26)(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)(cid:15)(cid:24)(cid:16)(cid:13)(cid:18)(cid:24)(cid:0)
(cid:29)(cid:2)(cid:26)(cid:14)(cid:0)(cid:18)(cid:8)(cid:2):(cid:17)(cid:3)(cid:2)(cid:26)(cid:0)(cid:13)(cid:4)(cid:24)(cid:5)(cid:0)N??O3(cid:0)(cid:6)(cid:24)(cid:4)(cid:2)(cid:26):(cid:17)(cid:16)(cid:26)(cid:29)(cid:2)(cid:0)(cid:5)(cid:2)(cid:10)(cid:8)(cid:4)(cid:0)(cid:9)(cid:24)(cid:26)(cid:18)(cid:17)(cid:14)(cid:2)(cid:0)=40(cid:25)(cid:12)(cid:0)
(cid:18)(cid:24)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)1(cid:4)(cid:17)(cid:10)(cid:16)(cid:24)(cid:7)(cid:8)(cid:26)(cid:14)(cid:0)(cid:18)(cid:8)(cid:15)(cid:2)(cid:10)(cid:17)(cid:3)(cid:2)(cid:26)(cid:0)(cid:18)(cid:2)(cid:4)(cid:2)(cid:15)(cid:0)(cid:9)(cid:24)(cid:7)(cid:10)(cid:2)(cid:15)(cid:2)(cid:2)(cid:26)(cid:0)Y78Z(cid:0)(cid:17)(cid:26)(cid:16)(cid:17)(cid:3)(cid:0)
(cid:18)(cid:8)(cid:4)(cid:2)(cid:3)(cid:17)(cid:3)(cid:2)(cid:26)(cid:0)(cid:9)(cid:24)(cid:26)(cid:18)(cid:17)(cid:14)(cid:2)(cid:2)(cid:26)(cid:0)(cid:15)(cid:24)(cid:26)(cid:14)(cid:14)(cid:17)(cid:26)(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)(cid:15)(cid:24)(cid:16)(cid:13)(cid:18)(cid:24)(cid:0) G !!""#""$%""&
D""$%()*#+ $,(cid:137)(cid:0)

6(cid:2)(cid:15)(cid:17)(cid:26)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:3)(cid:2)(cid:10)(cid:17)(cid:10)(cid:0)(cid:16)(cid:8)(cid:18)(cid:2)(cid:3)(cid:0)(cid:16)(cid:24)(cid:7)(cid:10)(cid:2)(cid:15)(cid:9)(cid:24)(cid:4)F(cid:0)(cid:16)(cid:24)(cid:7)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:10)(cid:24)(cid:18)(cid:8)(cid:3)(cid:8)(cid:16)(cid:0)
(cid:9)(cid:24)(cid:7)(cid:17)-(cid:2)(cid:5)(cid:2)(cid:26)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:9)(cid:24)(cid:7)(cid:10)(cid:2)(cid:15)(cid:2)(cid:2)(cid:26)(cid:0)Y78Z(cid:0)(cid:18)(cid:8)(cid:3)(cid:2)(cid:7)(cid:24)(cid:26)(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)(cid:9)(cid:24)(cid:7)-(cid:24)(cid:18)(cid:2)(cid:2)(cid:26)(cid:0):(cid:17)(cid:15)(cid:4)(cid:2)(cid:5)(cid:0)
(cid:2)(cid:7)(cid:24)(cid:2)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:4)(cid:2)(cid:26)(cid:14)(cid:10)(cid:17)(cid:26)(cid:14)(cid:0)(cid:18)(cid:2)(cid:26)(cid:0)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)=40(cid:25)(cid:12)(cid:0)(cid:18)(cid:24)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)
%EIBJ""#F(cid:0)(cid:18)(cid:8)(cid:15)(cid:2)(cid:26)(cid:2)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:15)(cid:2)(cid:16)(cid:7)(cid:8)(cid:3)(cid:10)(cid:0)(cid:2)(cid:14)(cid:14)(cid:7)(cid:24)(cid:14)(cid:2)(cid:10)(cid:8)(cid:0)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:4)(cid:2)(cid:26)(cid:14)(cid:10)(cid:17)(cid:14)(cid:0)
(cid:18)(cid:8)(cid:5)(cid:2)(cid:10)(cid:8)(cid:4)(cid:3)(cid:2)(cid:26)(cid:0)(cid:18)(cid:2)(cid:7)(cid:8)(cid:0)(cid:2)(cid:7)(cid:24)(cid:2)(cid:0)(cid:29)(cid:2)(cid:26)(cid:14)(cid:0)(cid:16)(cid:24)(cid:7)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:10)(cid:2)(cid:15)(cid:9)(cid:24)(cid:4)(cid:0)(cid:10)(cid:24)(cid:18)(cid:2)(cid:26)(cid:14)(cid:3)(cid:2)(cid:26)(cid:0)(cid:15)(cid:2)(cid:16)(cid:7)(cid:8)(cid:3)(cid:10)(cid:0)
(cid:2)(cid:14)(cid:14)(cid:7)(cid:24)(cid:14)(cid:2)(cid:10)(cid:8)(cid:0)=40(cid:25)(cid:12)(cid:0)(cid:18)(cid:24)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)1(cid:4)(cid:17)(cid:10)(cid:16)(cid:24)(cid:7)(cid:0)(cid:16)(cid:24)(cid:4)(cid:2)(cid:5)(cid:0)(cid:15)(cid:24)(cid:26)1(cid:2)(cid:3)(cid:17)(cid:9)(cid:0)(cid:3)(cid:24)(cid:10)(cid:24)(cid:4)(cid:17)(cid:7)(cid:17)(cid:5)(cid:2)(cid:26)(cid:0)
(cid:2)(cid:7)(cid:24)(cid:2)(cid:0)-(cid:2)(cid:8)(cid:3)(cid:0)(cid:29)(cid:2)(cid:26)(cid:14)(cid:0)(cid:16)(cid:24)(cid:7)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:10)(cid:2)(cid:15)(cid:9)(cid:24)(cid:4)(cid:0)(cid:15)(cid:2)(cid:17)(cid:9)(cid:17)(cid:26)(cid:0)(cid:29)(cid:2)(cid:26)(cid:14)(cid:0)(cid:16)(cid:8)(cid:18)(cid:2)(cid:3)(cid:0)(cid:16)(cid:24)(cid:7)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)
(cid:10)(cid:2)(cid:15)(cid:9)(cid:24)(cid:4)3(cid:0)(cid:6)(cid:24)(cid:5)(cid:8)(cid:26)(cid:14)(cid:14)(cid:2)(cid:0)(cid:9)(cid:24)(cid:7)(cid:10)(cid:2)(cid:15)(cid:2)(cid:2)(cid:26)(cid:0)Y78Z(cid:0)(cid:18)(cid:2)(cid:4)(cid:2)(cid:15)(cid:0)(cid:3)(cid:2)(cid:10)(cid:17)(cid:10)(cid:0)(cid:16)(cid:8)(cid:18)(cid:2)(cid:3)(cid:0)(cid:16)(cid:24)(cid:7)(cid:10)(cid:2)(cid:15)(cid:9)(cid:24)(cid:4)(cid:0)
(cid:18)(cid:8)(cid:15)(cid:13)(cid:18)(cid:8)9(cid:8)(cid:3)(cid:2)(cid:10)(cid:8)(cid:0)(cid:10)(cid:24)-(cid:2)(cid:14)(cid:2)(cid:8)(cid:0)-(cid:24)(cid:7)(cid:8)(cid:3)(cid:17)(cid:16)Q(cid:0)

>3L3;(cid:31)""M""EC5)""$J(cid:0)
>3L3?3; (cid:31) !!""#""$%""& ’""$%()*#+ $,(cid:0) (cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0) (cid:28)(cid:2)(cid:29)(cid:20)(cid:30)(cid:24)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)

(cid:0)

RSTU(cid:138)

VW XR(cid:139)

STU(cid:139)(cid:0)

Y7>Z(cid:0)

(cid:15)(cid:17)(cid:4)(cid:16)(cid:27)(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)

(cid:19)(cid:24)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)(cid:15)(cid:24)(cid:26)(cid:14)(cid:8)(cid:15)(cid:9)(cid:4)(cid:24)(cid:15)(cid:24)(cid:26)(cid:16)(cid:2)(cid:10)(cid:8)(cid:3)(cid:2)(cid:26)(cid:0)(cid:3)(cid:13)(cid:26)(cid:10)(cid:24)(cid:9)(cid:0)(cid:18)(cid:8)99(cid:24)(cid:7)(cid:24)(cid:26)1(cid:24)(cid:0)
-(cid:24)(cid:26)1(cid:5)(cid:15)(cid:2)(cid:7)(cid:3)(cid:8)(cid:26)(cid:14)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)N?O(cid:0)(cid:16)(cid:24)(cid:7)(cid:5)(cid:2)(cid:18)(cid:2)(cid:9)(cid:0)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)(cid:28)(cid:2)(cid:29)(cid:20)(cid:30)(cid:24)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)
(cid:15)(cid:17)(cid:4)(cid:16)(cid:27)(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)NPO(cid:0)(cid:18)(cid:24)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)(cid:9)(cid:24)(cid:7)(cid:10)(cid:2)(cid:15)(cid:2)(cid:2)(cid:26)(cid:0)-(cid:24)(cid:7)(cid:8)(cid:3)(cid:17)(cid:16)Q(cid:0)

RSTUVW XRSTU(cid:0)

Y78Z(cid:0)

R(cid:139)(cid:0)(cid:15)(cid:24)(cid:7)(cid:17)(cid:9)(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)(cid:15)(cid:2)(cid:16)(cid:7)(cid:8)(cid:3)(cid:10)(cid:0)-(cid:24)(cid:7)(cid:17)(cid:3)(cid:17)(cid:7)(cid:2)(cid:26)(cid:0)fgk(cid:0)-(cid:24)(cid:7)(cid:8)(cid:10)(cid:8)(cid:0)(cid:9)(cid:7)(cid:13)(cid:9)(cid:13)(cid:7)(cid:10)(cid:8)(cid:0)(cid:17)(cid:26)(cid:8)(cid:16)(cid:0)
(cid:17)(cid:26)(cid:16)(cid:17)(cid:3)(cid:0)(cid:2)(cid:7)(cid:24)(cid:2)(cid:0)(cid:29)(cid:2)(cid:26)(cid:14)(cid:0)(cid:16)(cid:24)(cid:7)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:10)(cid:2)(cid:15)(cid:9)(cid:24)(cid:4)3(cid:0)R (cid:0)(cid:15)(cid:24)(cid:7)(cid:17)(cid:9)(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)(cid:15)(cid:2)(cid:16)(cid:7)(cid:8)(cid:3)(cid:10)(cid:0)
-(cid:24)(cid:7)(cid:17)(cid:3)(cid:17)(cid:7)(cid:2)(cid:26)(cid:0)fgj_-(cid:24)(cid:7)(cid:8)(cid:10)(cid:8)(cid:0)(cid:9)(cid:7)(cid:13)(cid:9)(cid:13)(cid:7)(cid:10)(cid:8)(cid:0)(cid:17)(cid:26)(cid:8)(cid:16)(cid:0)(cid:17)(cid:26)(cid:16)(cid:17)(cid:3)(cid:0)(cid:10)(cid:24)(cid:4)(cid:17)(cid:7)(cid:17)(cid:5)(cid:0)(cid:2)(cid:7)(cid:24)(cid:2)(cid:0)-(cid:2)(cid:8)(cid:3)(cid:0)
(cid:29)(cid:2)(cid:26)(cid:14)(cid:0)(cid:16)(cid:24)(cid:7)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:10)(cid:2)(cid:15)(cid:9)(cid:24)(cid:4)(cid:0)(cid:15)(cid:2)(cid:17)(cid:9)(cid:17)(cid:26)(cid:0)(cid:16)(cid:8)(cid:18)(cid:2)(cid:3)(cid:0)(cid:16)(cid:24)(cid:7)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:10)(cid:2)(cid:15)(cid:9)(cid:24)(cid:4)3(cid:0)(cid:19)(cid:8)(cid:15)(cid:2)(cid:26)(cid:2)(cid:0)
(cid:15)(cid:24)(cid:15)(cid:24)(cid:26)(cid:17)(cid:5)(cid:8)(cid:0)(cid:2)(cid:10)(cid:17)(cid:15)(cid:10)(cid:8)(cid:0)(cid:10)(cid:24)-(cid:2)(cid:14)(cid:2)(cid:8)(cid:0)-(cid:24)(cid:7)(cid:8)(cid:3)(cid:17)(cid:16)Q(cid:0)

(cid:19)(cid:8)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)G !!""#""$%""&D""$%()*#+ $,(cid:0)(cid:28)(cid:2)(cid:29)(cid:20)(cid:30)(cid:24)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)

(cid:1)(cid:17)(cid:4)(cid:16)(cid:8)(cid:27)(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:18)(cid:2)(cid:4)(cid:2)(cid:15)(cid:0)-(cid:24)(cid:26)(cid:16)(cid:17)(cid:3)(cid:0)(cid:15)(cid:2)(cid:16)(cid:7)(cid:8)(cid:3)(cid:10)Q(cid:0)(cid:0)

TUVW(cid:0)[(cid:0)TU\\ ]^(cid:0)

(cid:0)

Y7?Z(cid:0)

(cid:140) Ro‘(cid:141)a

V
o(cid:143)c

X(cid:142)

(cid:144)(cid:145)(cid:146)_(cid:140) R(cid:139)o‘(cid:141)aX(cid:142)

V
o(cid:143)c

(cid:0)(cid:0)

Y7(cid:147)Z(cid:0)

(cid:0)

TUVW(cid:0)[(cid:0)(cid:0)TU\\ ]_‘RSabc‘RSTUdRSTU\\a(cid:0) Y77Z(cid:0)

fX_k](cid:148)ke_(cid:0)

TUe(cid:0)TU\\_(cid:18)(cid:2)(cid:26)(cid:0)TUVW_(cid:10)(cid:24)1(cid:2)(cid:7)(cid:2)(cid:0)-(cid:24)(cid:7)(cid:17)(cid:7)(cid:17)(cid:16)(cid:0)(cid:15)(cid:24)(cid:7)(cid:17)(cid:9)(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)(cid:15)(cid:2)(cid:16)(cid:7)(cid:8)(cid:3)(cid:10)(cid:0)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)
(cid:4)(cid:2)(cid:26)(cid:14)(cid:10)(cid:17)(cid:26)(cid:14)F(cid:0)(cid:15)(cid:2)(cid:16)(cid:7)(cid:8)(cid:3)(cid:10)(cid:0)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)=40(cid:25)(cid:12)F(cid:0)(cid:18)(cid:2)(cid:26)(cid:0)(cid:15)(cid:2)(cid:16)(cid:7)(cid:8)(cid:3)(cid:10)(cid:0)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)
=40(cid:25)(cid:12)(cid:0)(cid:18)(cid:24)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)G !!""#""$%""&D""$%("")*#+ $,(cid:0)(cid:29)(cid:2)(cid:26)(cid:14)(cid:0)(cid:15)(cid:2)(cid:10)(cid:8)(cid:26)(cid:14)(cid:20)
(cid:15)(cid:2)(cid:10)(cid:8)(cid:26)(cid:14)(cid:0)-(cid:24)(cid:7)(cid:17)(cid:3)(cid:17)(cid:7)(cid:2)(cid:26)(cid:0)fgh3(cid:0)R (cid:0)(cid:15)(cid:24)(cid:7)(cid:17)(cid:9)(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)(cid:15)(cid:2)(cid:16)(cid:7)(cid:8)(cid:3)(cid:10)(cid:0)(cid:9)(cid:24)(cid:26)(cid:8)(cid:15)-(cid:2)(cid:26)(cid:14)(cid:0)
-(cid:24)(cid:7)(cid:17)(cid:3)(cid:17)(cid:7)(cid:2)(cid:26)(cid:0)fghi(cid:0)(cid:19)(cid:2)(cid:26)(cid:0)^(cid:0)(cid:15)(cid:24)(cid:7)(cid:17)(cid:9)(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)(cid:15)(cid:2)(cid:16)(cid:7)(cid:8)(cid:3)(cid:10)(cid:0)-(cid:24)(cid:7)(cid:17)(cid:3)(cid:17)(cid:7)(cid:2)(cid:26)(cid:0)fgh(cid:0)
(cid:29)(cid:2)(cid:26)(cid:14)(cid:0)-(cid:24)(cid:7)(cid:8)(cid:10)(cid:8)(cid:0)(cid:3)(cid:13)(cid:26)(cid:10)(cid:16)(cid:2)(cid:26)(cid:16)(cid:2)(cid:0)(cid:29)(cid:2)(cid:26)(cid:14)(cid:0)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:15)(cid:24)(cid:26)(cid:14)(cid:3)(cid:13)(cid:26)(cid:10)(cid:8)(cid:10)(cid:16)(cid:24)(cid:26)(cid:10)(cid:8)(cid:3)(cid:2)(cid:26)(cid:0)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)
=40(cid:25)(cid:12)3(cid:0)

(cid:1)(cid:24)(cid:16)(cid:13)(cid:18)(cid:24)(cid:0)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:17)(cid:26)(cid:16)(cid:17)(cid:3)(cid:0)(cid:15)(cid:24)(cid:26)(cid:14)(cid:5)(cid:8)(cid:16)(cid:17)(cid:26)(cid:14)(cid:0)(cid:1)(cid:6)=(cid:0)G !!""#""$%""&
’""$%()*#+ $,(cid:0)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)(cid:28)(cid:2)(cid:29)(cid:20)(cid:30)(cid:24)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)(cid:15)(cid:17)(cid:4)(cid:16)(cid:27)(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:15)(cid:24)(cid:26)(cid:14)(cid:14)(cid:17)(cid:26)(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)
(cid:12)(cid:7)(cid:2)(cid:10)(cid:2)(cid:18)(cid:20)(cid:7)(cid:2)(cid:13)(cid:0)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:16)(cid:13)(cid:7)(cid:0)(cid:10)(cid:24)-(cid:2)(cid:14)(cid:2)(cid:8)(cid:0)-(cid:24)(cid:7)(cid:8)(cid:3)(cid:17)(cid:16)Q(cid:0)

(cid:0)

jkl‘TUamncopqrs]ntopqrs]nuo‘qra(cid:0)v(cid:0)nwo‘qra(cid:0) Y7LZ(cid:0)
(cid:18)(cid:8)(cid:15)(cid:2)(cid:26)(cid:2)(cid:0)nwpqrs_(cid:18)(cid:8)9(cid:13)(cid:7)(cid:15)(cid:17)(cid:4)(cid:2)(cid:10)(cid:8)(cid:3)(cid:2)(cid:26)(cid:0)(cid:18)(cid:2)(cid:4)(cid:2)(cid:15)(cid:0)-(cid:24)(cid:26)(cid:16)(cid:17)(cid:3)(cid:0)(cid:15)(cid:2)(cid:16)(cid:7)(cid:8)(cid:3)(cid:10)(cid:0)(cid:10)(cid:24)-(cid:2)(cid:14)(cid:2)(cid:8)(cid:0)
-(cid:24)(cid:7)(cid:8)(cid:3)(cid:17)(cid:16)Q(cid:0)

k(cid:0)__X(cid:0)-(cid:2)(cid:26)(cid:29)(cid:2)(cid:3)(cid:26)(cid:29)(cid:2)(cid:0)(cid:2)(cid:7)(cid:24)(cid:2)(cid:0)(cid:29)(cid:2)(cid:26)(cid:14)(cid:0)(cid:16)(cid:24)(cid:7)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:10)(cid:2)(cid:15)(cid:9)(cid:24)(cid:4)F(cid:0)

(cid:148)k(cid:0)[(cid:0)-(cid:2)(cid:26)(cid:29)(cid:2)(cid:3)(cid:26)(cid:29)(cid:2)(cid:0)(cid:2)(cid:7)(cid:24)(cid:2)(cid:0)(cid:29)(cid:2)(cid:26)(cid:14)(cid:0)(cid:16)(cid:8)(cid:18)(cid:2)(cid:3)(cid:0)(cid:16)(cid:24)(cid:7)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:10)(cid:2)(cid:15)(cid:9)(cid:24)(cid:4)F(cid:0)

(cid:12)(cid:24)(cid:26)(cid:18)(cid:17)(cid:14)(cid:2)(cid:0)(cid:18)(cid:8)99(cid:24)(cid:7)(cid:24)(cid:26)1(cid:24)(cid:0)-(cid:24)(cid:26)1(cid:5)(cid:15)(cid:2)(cid:7)(cid:3)(cid:8)(cid:26)(cid:14)(cid:0)(cid:15)(cid:17)(cid:4)(cid:16)(cid:8)(cid:27)(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:24)(cid:0)(cid:28)(cid:2)(cid:29)(cid:20)(cid:30)(cid:24)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)
(cid:18)(cid:24)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)(cid:8)(cid:26)9(cid:13)(cid:7)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)1(cid:4)(cid:17)(cid:10)(cid:16)(cid:24)(cid:7)(cid:0)(cid:10)(cid:24)-(cid:2)(cid:14)(cid:2)(cid:8)(cid:0)-(cid:24)(cid:7)(cid:8)(cid:3)(cid:17)(cid:16)Q(cid:0)

(cid:0)

VW_[(cid:0)_TU\\ ](cid:149)R(cid:139)
TU(cid:138)

STU(cid:139)dRSTU(cid:138)

\\(cid:150)(cid:0)(cid:0)

(cid:0)

Y7(cid:151)Z(cid:0)

(cid:0)

TU(cid:139)_(cid:15)(cid:24)(cid:7)(cid:17)(cid:9)(cid:2)(cid:3)(cid:2)(cid:26)(cid:152)(cid:15)(cid:2)(cid:16)(cid:7)(cid:8)(cid:3)(cid:10)(cid:0)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:4)(cid:2)(cid:26)(cid:14)(cid:10)(cid:17)(cid:26)(cid:14)(cid:0)(cid:29)(cid:2)(cid:26)(cid:14)(cid:0)(cid:16)(cid:24)(cid:7)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:10)(cid:2)(cid:15)(cid:9)(cid:24)(cid:4)(cid:0)
\\ (cid:152)(cid:15)(cid:24)(cid:7)(cid:17)(cid:9)(cid:2)(cid:3)(cid:2)(cid:26)(cid:0)(cid:15)(cid:2)(cid:16)(cid:7)(cid:8)(cid:3)(cid:10)(cid:0)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)=40(cid:25)(cid:12)(cid:0)(cid:18)(cid:24)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)
YkghaeTU(cid:138)
VW_(cid:153)(cid:154)(cid:155)(cid:156)(cid:157)(cid:145)(cid:158)(cid:145)(cid:146)_(cid:15)(cid:2)(cid:16)(cid:7)(cid:8)(cid:3)(cid:10)(cid:0)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)
(cid:8)(cid:26)9(cid:13)(cid:7)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)%EIBJ""#(cid:0)Yfghae_TU(cid:138)
=40(cid:25)(cid:12)(cid:0)(cid:18)(cid:24)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)(cid:31) !!""#""$%""&’""$%()*#+ $,(cid:0)(cid:18)(cid:24)(cid:26)(cid:14)(cid:2)(cid:26)(cid:0)(cid:8)(cid:26)9(cid:13)(cid:7)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)

(cid:0)

(cid:0)

(cid:1)(cid:2)(cid:3)(cid:2)(cid:4)(cid:2)(cid:5)(cid:0)(cid:6)(cid:3)(cid:7)(cid:8)(cid:9)(cid:10)(cid:8)(cid:0)(cid:11)(cid:0)(cid:12)(cid:7)(cid:13)(cid:14)(cid:7)(cid:2)(cid:15)(cid:0)(cid:6)(cid:16)(cid:17)(cid:18)(cid:8)(cid:0)(cid:19)(cid:20)(cid:21)(cid:22)(cid:0)(cid:23)(cid:13)(cid:15)(cid:9)(cid:17)(cid:16)(cid:2)(cid:10)(cid:8)(cid:0)(cid:6)(cid:16)(cid:2)(cid:16)(cid:8)(cid:10)(cid:16)(cid:8)(cid:3)(cid:0)

(cid:24)(cid:25)(cid:26)(cid:27)(cid:28)(cid:29)(cid:30)(cid:31)   !""#$%& (cid:0)(cid:2)(cid:18)(cid:2)(cid:4)(cid:2)(cid:5)(cid:0) (cid:15)(cid:2)(cid:16)(cid:7)(cid:8)(cid:3)(cid:10)(cid:0) (cid:3)(cid:13)’(cid:10)(cid:16)(cid:2)’(cid:16)(cid:2)(cid:0) ((cid:2)’(cid:14)(cid:0)
(cid:15))’(cid:14)(cid:3)(cid:13)’(cid:10)(cid:8)(cid:10)(cid:16))’(cid:10)(cid:8)(cid:3)(cid:2)’(cid:0))(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)*+,-(cid:12)(cid:0) !""#$%./(cid:15))(cid:7)(cid:17)(cid:9)(cid:2)(cid:3)(cid:2)’(cid:0)
(cid:15)(cid:2)(cid:16)(cid:7)(cid:8)(cid:3)(cid:10)(cid:0)(cid:9)(cid:7)(cid:13)(cid:9)(cid:13)(cid:7)(cid:10)(cid:8)(cid:0)(cid:10)(cid:2)(cid:15)(cid:9))(cid:4)(cid:0)((cid:2)’(cid:14)(cid:0)(cid:18)(cid:8)(cid:3))(cid:16)(cid:2)(cid:5)(cid:17)(cid:8)(cid:0) !""#$(cid:0)(cid:18)(cid:2)’(cid:0).0(cid:0)
(cid:15))(cid:7)(cid:17)(cid:9)(cid:2)(cid:3)(cid:2)’/(cid:15)(cid:2)(cid:16)(cid:7)(cid:8)(cid:3)(cid:10)(cid:0)(cid:9)(cid:7)(cid:13)(cid:9)(cid:13)(cid:7)(cid:10)(cid:8)(cid:0)(cid:10)(cid:2)(cid:15)(cid:9))(cid:4)(cid:0)((cid:2)’(cid:14)(cid:0)(cid:15))(cid:15)(cid:8)(cid:4)(cid:8)(cid:3)(cid:8)(cid:0)(cid:10)(cid:2)(cid:15)(cid:9))(cid:4)(cid:0)
 1""#$2(cid:0)(cid:0)

(cid:1))(cid:16)(cid:13)(cid:18))(cid:0))(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:17)’(cid:16)(cid:17)(cid:3)(cid:0)(cid:15))’(cid:14)(cid:5)(cid:8)(cid:16)(cid:17)’(cid:14)(cid:0)(cid:1)(cid:6)*(cid:0)3455(cid:29)(cid:30)(cid:29)6(cid:24)(cid:29)(cid:31)
7(cid:29)6(cid:24)89:(cid:30);46<(cid:0)(cid:15)(cid:13)(cid:18))(cid:4)(cid:0)=(cid:2)((cid:20)>)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)(cid:15)(cid:17)(cid:4)(cid:16)?(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:18))’(cid:14)(cid:2)’(cid:0)(cid:8)’@(cid:13)(cid:7)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)
(cid:24)(cid:25)(cid:26)(cid:27)(cid:28)(cid:29)(cid:30)(cid:0)(cid:15))’(cid:14)(cid:14)(cid:17)’(cid:2)(cid:3)(cid:2)’(cid:0)(cid:12)(cid:7)(cid:2)(cid:10)(cid:2)(cid:18)(cid:20)(cid:7)(cid:2)(cid:13)(cid:0))(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:16)(cid:13)(cid:7)(cid:0)(cid:10))A(cid:2)(cid:14)(cid:2)(cid:8)(cid:0)A)(cid:7)(cid:8)(cid:3)(cid:17)(cid:16)B(cid:0)

(cid:0)
C1DEFGH
UUUUUUUUUUUUUUUUUUUUUUUUUUTVMWXIOPQ

IJ0$KLMNHIOPQ

YYYYYYYYYYY
R$

R$IS$TMQHIOPQ

R$IS$(cid:0)
Q$U(cid:0)(cid:0)

IS$TMZHIOPR

(cid:18))’(cid:14)(cid:2)’(cid:0)(cid:15)(cid:2)(cid:10)(cid:8)’(cid:14)(cid:20)(cid:15)(cid:2)(cid:10)(cid:8)’(cid:14)(cid:0)(cid:3)(cid:13)(cid:15)(cid:9)(cid:13)’)’(cid:0)(cid:10))A(cid:2)(cid:14)(cid:2)(cid:8)(cid:0)A)(cid:7)(cid:8)(cid:3)(cid:17)(cid:16)B(cid:0)

MNHIOPQ

R$IS$L^IS$_IS$(cid:0)

(cid:0)

(cid:0)

(cid:0)

(cid:0)

MQHIOPQ

R$IS$LEbc^IS$KdIS$EdIS$

e fg

hNdIS$K
IS$

e UEb

dIS$

hN

c^IS$K(cid:0)

^IS$Ljfg

RIS$jefg

IS$

hNkUU^IS$L

l^IS$
lmn

Uk(cid:0)

opqEmn

S%mn

rKLEst%uK

hN

(cid:0)

v2‘2‘2w x(cid:7)(cid:10)(cid:8)(cid:16))(cid:3)(cid:16)(cid:17)(cid:7)(cid:0)y(cid:31)z:(cid:24);:<(cid:29)(cid:0)

{(cid:2)(cid:15)A(cid:2)(cid:7)(cid:0)[2(cid:0)x(cid:7)(cid:10)(cid:8)(cid:16))(cid:3)(cid:16)(cid:17)(cid:7)(cid:0)y(cid:31)z:(cid:24);:<(cid:29)(cid:31)

(cid:0)

 i](cid:0)

 [](cid:0)

 ‘](cid:0)

 (cid:152)](cid:0)

 ƒ](cid:0)

 v](cid:0)

⁄i(cid:0)

⁄[(cid:0)

⁄‘(cid:0)

(cid:10)(cid:2))(cid:0)

a2a\\\\‘va(cid:0) a2a\\i[§§(cid:0) a2a§\\\\§§(cid:0) a2a§v\\§¤(cid:0)

(cid:10)(cid:2))(cid:19)+(cid:0) a2a\\\\‘vƒ(cid:0) a2a\\i[\\[(cid:0) a2a§\\\\\\a(cid:0) a2a§v\\§§(cid:0)

(cid:15)(cid:10)(cid:2))(cid:0) a2a\\‘§iƒ(cid:0) a2a§¤a[i(cid:0) a2a§(cid:152)via(cid:0) a2a§i§i(cid:152)(cid:0)

(cid:15)(cid:10)(cid:2))(cid:19)+(cid:0) a2a\\(cid:152)[v[(cid:0) a2a§¤[§(cid:152)(cid:0) a2a§(cid:152)¤(cid:152)‘(cid:0) a2a§i§§i(cid:0)

(cid:10)(cid:2))(cid:0)

a2i\\ia§[(cid:0) a2i§(cid:152)§[a(cid:0) a2i§ia\\§(cid:0) a2i¤¤v‘v(cid:0)

(cid:10)(cid:2))(cid:19)+(cid:0) a2i\\ia§§(cid:0) a2i§(cid:152)§[(cid:152)(cid:0) a2i§iiaa(cid:0) a2i¤¤v‘¤(cid:0)

(cid:15)(cid:10)(cid:2))(cid:0) a2i¤ƒ§i§(cid:0) a2iv§\\‘[(cid:0) a2iv(cid:152)¤(cid:152)‘(cid:0) a2iva¤¤¤(cid:0)

(cid:15)(cid:10)(cid:2))(cid:19)+(cid:0) a2i¤v§§(cid:152)(cid:0) a2iv\\ƒ¤(cid:152)(cid:0) a2ivƒaƒƒ(cid:0) a2iva\\‘‘(cid:0)

(cid:10)(cid:2))(cid:0)

a2‘a‘(cid:152)a[(cid:0) a2[\\\\(cid:152)ƒƒ(cid:0) a2[\\ƒ‘v\\(cid:0) a2[\\(cid:152)‘ƒƒ(cid:0)

(cid:10)(cid:2))(cid:19)+(cid:0) a2‘a‘(cid:152)a§(cid:0) a2[\\\\(cid:152)ƒ§(cid:0) a2[\\ƒ‘¤a(cid:0) a2[\\(cid:152)‘ƒv(cid:0)

(cid:15)(cid:10)(cid:2))(cid:0) a2[¤aƒ(cid:152)ƒ(cid:0) a2[v‘\\¤[(cid:0) a2[ƒ§¤§¤(cid:0) a2[ƒviƒa(cid:0)

(cid:15)(cid:10)(cid:2))(cid:19)+(cid:0) a2[¤[‘¤i(cid:0) a2[vƒavv(cid:0) a2[ƒ\\‘i¤(cid:0) a2[ƒv(cid:152)i§(cid:0)

 [\\](cid:0)

 ‘a](cid:0)

 ‘i](cid:0)

(cid:0)

 ‘[](cid:0)

 ‘‘](cid:0)

>(cid:2)(cid:10)(cid:8)(cid:4)(cid:0))(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:18))’(cid:14)(cid:2)’(cid:0)(cid:1))(cid:16)(cid:13)(cid:18))(cid:0)3455(cid:29)(cid:30)(cid:29)6(cid:24)(cid:29)(cid:31)(cid:154)(cid:29)6(cid:24)89:(cid:30);46<(cid:0)
(cid:15)(cid:13)(cid:18))(cid:4)(cid:0)=(cid:2)((cid:20)(cid:1)(cid:17)(cid:4)(cid:16)(cid:8)?(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:15))’(cid:14)(cid:5)(cid:2)(cid:10)(cid:8)(cid:4)(cid:3)(cid:2)’(cid:0)’(cid:8)(cid:4)(cid:2)(cid:8)(cid:0)(cid:1)(cid:6)*(cid:0)((cid:2)’(cid:14)(cid:0)(cid:4))A(cid:8)(cid:5)(cid:0)
(cid:3))'(cid:8)(cid:4)(cid:0)(cid:18)(cid:2)(cid:7)(cid:8)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:15))(cid:16)(cid:13)(cid:18))(cid:0)3455(cid:29)(cid:30)(cid:29)6(cid:24)(cid:29)(cid:31)(cid:154)(cid:29)6(cid:24)89:(cid:30);46<(cid:0)(cid:15)(cid:13)(cid:18))(cid:4)(cid:0)
(cid:17)’(cid:8)?(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)’((cid:2)(cid:0) (cid:10))(cid:5)(cid:8)’(cid:14)(cid:14)(cid:2)(cid:0) (cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0) (cid:18)(cid:8)(cid:3)(cid:2)(cid:16)(cid:2)(cid:3)(cid:2)’(cid:0) 3455(cid:29)(cid:30)(cid:29)6(cid:24)(cid:29)(cid:31)
(cid:154)(cid:29)6(cid:24)89:(cid:30);46<(cid:0)(cid:15)(cid:13)(cid:18))(cid:4)(cid:0)=(cid:2)((cid:20)(cid:1)(cid:17)(cid:4)(cid:16)(cid:8)?(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:15))’(cid:14)(cid:5)(cid:2)(cid:10)(cid:8)(cid:4)(cid:3)(cid:2)’(cid:0)’(cid:8)(cid:4)(cid:2)(cid:8)(cid:0)
)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)((cid:2)’(cid:14)(cid:0)(cid:4))A(cid:8)(cid:5)(cid:0))@(cid:8)(cid:10)(cid:8))’2(cid:0)(cid:19)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:18)(cid:8)(cid:10)(cid:8)(cid:15)(cid:9)(cid:17)(cid:4)(cid:3)(cid:2)’(cid:0)A(cid:2)(cid:5)“(cid:2)(cid:0)
(cid:2)(cid:4)(cid:14)(cid:13)(cid:7)(cid:8)(cid:16)(cid:15)(cid:2)(cid:0)((cid:2)’(cid:14)(cid:0)(cid:18)(cid:8)A(cid:2)’(cid:14)(cid:17)’(cid:0)(cid:10)(cid:17)(cid:18)(cid:2)(cid:5)(cid:0)(cid:10))(cid:10)(cid:17)(cid:2)(cid:8)(cid:0)(cid:3)(cid:2)(cid:7))’(cid:2)(cid:0)(cid:16))(cid:4)(cid:2)(cid:5)(cid:0)
(cid:15))’(cid:17)’(cid:153)(cid:17)(cid:3)(cid:2)’(cid:0)(cid:9)(cid:13)(cid:4)(cid:2)(cid:0)((cid:2)’(cid:14)(cid:0)(cid:18)(cid:8)(cid:5)(cid:2)(cid:7)(cid:2)(cid:9)(cid:3)(cid:2)’2(cid:0)(cid:0)

(cid:0)

v2(cid:152)2[2w (cid:0)-(cid:153)(cid:8)(cid:0)(cid:12))(cid:7)@(cid:13)(cid:7)(cid:15)(cid:2)(cid:0)

-(cid:153)(cid:8)(cid:0)(cid:9))(cid:7)@(cid:13)(cid:7)(cid:15)(cid:2)(cid:0)(cid:18)(cid:8)(cid:4)(cid:2)(cid:3)(cid:17)(cid:3)(cid:2)’(cid:0)(cid:17)’(cid:16)(cid:17)(cid:3)(cid:0)(cid:15))(cid:4)(cid:8)(cid:5)(cid:2)(cid:16)(cid:0)(cid:3))(cid:2)’(cid:18)(cid:2)(cid:4)(cid:2)’(cid:0)
(cid:9)(cid:7)(cid:13)(cid:14)(cid:7)(cid:2)(cid:15)(cid:0)((cid:2)’(cid:14)(cid:0)(cid:16))(cid:4)(cid:2)(cid:5)(cid:0)(cid:18)(cid:8)A(cid:17)(cid:2)(cid:16)(cid:0)A)(cid:7)(cid:18)(cid:2)(cid:10)(cid:2)(cid:7)(cid:3)(cid:2)’(cid:0)“(cid:2)(cid:3)(cid:16)(cid:17)(cid:0)(cid:3)(cid:13)(cid:15)(cid:9)(cid:17)(cid:16)(cid:2)(cid:10)(cid:8)(cid:0)(cid:18)(cid:2)(cid:7)(cid:8)(cid:0)
(cid:15)(cid:2)(cid:10)(cid:8)’(cid:14)(cid:20)(cid:15)(cid:2)(cid:10)(cid:8)’(cid:14)(cid:0)(cid:10)(cid:8)(cid:15)(cid:17)(cid:4)(cid:2)(cid:10)(cid:8)2(cid:0)-(cid:7)(cid:17)(cid:16)(cid:2)’(cid:0)(cid:15)(cid:13)(cid:18))(cid:4)(cid:0)A)(cid:7)(cid:18)(cid:2)(cid:10)(cid:2)(cid:7)(cid:3)(cid:2)’(cid:0)“(cid:2)(cid:3)(cid:16)(cid:17)(cid:0)
(cid:3)(cid:13)(cid:15)(cid:9)(cid:17)(cid:16)(cid:2)(cid:10)(cid:8)(cid:0)’((cid:2)(cid:0)(cid:10))'(cid:2)(cid:7)(cid:2)(cid:0)A)(cid:7)(cid:17)(cid:7)(cid:17)(cid:16)(cid:0)(cid:18)(cid:2)(cid:7)(cid:8)(cid:0)((cid:2)’(cid:14)(cid:0)(cid:16))(cid:7)(cid:4)(cid:2)(cid:15)(cid:2)(cid:0)(cid:2)(cid:18)(cid:2)(cid:4)(cid:2)(cid:5)(cid:0)(cid:10))A(cid:2)(cid:14)(cid:2)(cid:8)(cid:0)
A)(cid:7)(cid:8)(cid:3)(cid:17)(cid:16)B(cid:0)(cid:15)(cid:10)(cid:2))(cid:19)+(cid:149)(cid:0)(cid:15)(cid:10)(cid:2))(cid:149)(cid:0)(cid:10)(cid:2))(cid:19)+(cid:149)(cid:0)(cid:10)(cid:2))2(cid:0)(cid:1))’(cid:17)(cid:7)(cid:17)(cid:16)(cid:0)«[‹(cid:0)(cid:9))(cid:7)A)(cid:18)(cid:2)(cid:2)’(cid:0)
“(cid:2)(cid:3)(cid:16)(cid:17)(cid:0)(cid:8)’(cid:8)(cid:0)(cid:18)(cid:8)(cid:9))’(cid:14)(cid:2)(cid:7)(cid:17)(cid:5)(cid:8)(cid:0)(cid:13)(cid:4))(cid:5)(cid:0)(cid:3))(cid:7)(cid:17)(cid:15)(cid:8)(cid:16)(cid:2)’(cid:0)(cid:15)(cid:2)(cid:16)(cid:7)(cid:8)(cid:3)(cid:10)(cid:0)(cid:3)(cid:13)?(cid:2)(cid:7)(cid:8)(cid:2)’(cid:10)(cid:0)(cid:18)(cid:2)(cid:7)(cid:8)(cid:0)
(cid:15)(cid:2)(cid:10)(cid:8)’(cid:14)(cid:20)(cid:15)(cid:2)(cid:10)(cid:8)’(cid:14)(cid:0)(cid:15)(cid:13)(cid:18))(cid:4)(cid:0)((cid:2)’(cid:14)(cid:0)(cid:18)(cid:8)A(cid:2)’(cid:14)(cid:17)’(cid:0)(cid:18)(cid:8)(cid:0)(cid:10))(cid:16)(cid:8)(cid:2)(cid:9)(cid:0)(cid:8)(cid:16))(cid:7)(cid:2)(cid:10)(cid:8)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)
(cid:9)(cid:7)(cid:13)(cid:10))(cid:10)(cid:0)¢*(cid:1),2(cid:0)(cid:19)(cid:8)(cid:15)(cid:2)’(cid:2)(cid:0)(cid:10))(cid:15)(cid:2)(cid:3)(cid:8)’(cid:0)(cid:7)(cid:17)(cid:15)(cid:8)(cid:16)(cid:0)(cid:15)(cid:2)(cid:16)(cid:7)(cid:8)(cid:3)(cid:10)(cid:0)(cid:3)(cid:13)?(cid:2)(cid:7)(cid:8)(cid:2)’(cid:10)(cid:0)(cid:2)(cid:3)(cid:2)’(cid:0)
(cid:15))(cid:15)A(cid:17)(cid:16)(cid:17)(cid:5)(cid:3)(cid:2)’(cid:0)“(cid:2)(cid:3)(cid:16)(cid:17)(cid:0)(cid:3)(cid:13)(cid:15)(cid:9)(cid:17)(cid:16)(cid:2)(cid:10)(cid:8)(cid:0)((cid:2)’(cid:14)(cid:0)(cid:10))(cid:15)(cid:2)(cid:3)(cid:8)’(cid:0)(cid:4)(cid:2)(cid:15)(cid:2)(cid:0)(cid:9)(cid:17)(cid:4)(cid:2)2(cid:0)

(cid:0)

y|z:(cid:24);:<(cid:29)(cid:31)}~(cid:127)(cid:128)(cid:129)(cid:130)(cid:131)(cid:132)~(cid:127)(cid:128)(cid:133)(cid:127)(cid:129)(cid:132)(cid:134)(cid:135)(cid:127)~(cid:136)~(cid:129)(cid:137)(cid:136)(cid:138)~(cid:134)(cid:139)(cid:140)(cid:141)(cid:129)}~(cid:127)(cid:128)(cid:129)
(cid:15))(cid:7)(cid:17)(cid:9)(cid:2)(cid:3)(cid:2)’(cid:0)(cid:10)(cid:8)’(cid:14)(cid:3)(cid:2)(cid:16)(cid:2)’(cid:0)(cid:18)(cid:2)(cid:7)(cid:8)(cid:0)(cid:142)(cid:26)(cid:25)(cid:28)4(cid:143):(cid:30)4:(cid:28)(cid:29)(cid:31)(cid:144)9:(cid:25)(cid:25)(cid:31)(cid:145)(cid:30)(cid:29):(cid:31)(cid:146)(cid:27)(cid:28)49:(cid:28)4(cid:147)6(cid:31)
(cid:148)455(cid:29)(cid:30)(cid:29)6(cid:24)(cid:29)(cid:31)7(cid:29)6(cid:24)89:(cid:30);46<2(cid:0)-’(cid:16)(cid:17)(cid:3)(cid:0)(cid:15))(cid:15)(cid:17)(cid:18)(cid:2)(cid:5)(cid:3)(cid:2)’(cid:0)(cid:9)(cid:7)(cid:13)(cid:10))(cid:10)(cid:0)
(cid:3)(cid:13)(cid:15)(cid:9)(cid:17)(cid:16)(cid:2)(cid:10)(cid:8)(cid:149)(cid:0)(cid:150):(cid:24);:<(cid:29)(cid:31)(cid:18)(cid:8)A(cid:2)’(cid:14)(cid:17)’(cid:0)(cid:15))’(cid:14)(cid:14)(cid:17)’(cid:2)(cid:3)(cid:2)’(cid:0)(cid:18))(cid:9))’(cid:18))’(cid:10)(cid:8)(cid:0)
(cid:150):(cid:24);:<(cid:29)(cid:0)(cid:10))(cid:7)(cid:16)(cid:2)(cid:0)A)A)(cid:7)(cid:2)(cid:9)(cid:2)(cid:0)(cid:150):(cid:24);:<(cid:29)(cid:0)((cid:2)’(cid:14)(cid:0)A)(cid:7)@(cid:17)’(cid:14)(cid:10)(cid:8)(cid:0)(cid:10))A(cid:2)(cid:14)(cid:2)(cid:8)(cid:0)(cid:28)(cid:147)(cid:147)(cid:25)(cid:27)(cid:0)
(cid:18)(cid:2)(cid:4)(cid:2)(cid:15)(cid:0)(cid:9))(cid:15)A(cid:2)’(cid:14)(cid:17)’(cid:2)’(cid:0)y(cid:31)(cid:150):(cid:24);:<(cid:29)2(cid:0)z:(cid:24);:<(cid:29)(cid:31)(cid:137)(cid:136)(cid:138)~(cid:134)(cid:139)(cid:140)(cid:141)(cid:129)(cid:151)(cid:134)(cid:135)(cid:138)(cid:133)(cid:138)(cid:133)(cid:127)(cid:129)
(cid:18)(cid:2)(cid:7)(cid:8)(cid:0)A)A)(cid:7)(cid:2)(cid:9)(cid:2)(cid:0)5(cid:26)6(cid:24)(cid:28)4(cid:147)6(cid:0)(cid:18)(cid:2)’(cid:0)(cid:18)(cid:2)(cid:16)(cid:2)(cid:10))(cid:16)2(cid:31)(cid:31)

v2(cid:152)2w (cid:146)(cid:143):(cid:25)(cid:26):(cid:28)4(cid:147)6(cid:0)
v2(cid:152)2i2w-(cid:153)(cid:8)(cid:0)(cid:22)(cid:2)(cid:4)(cid:8)(cid:18)(cid:8)(cid:16)(cid:2)(cid:10)(cid:0)

-(cid:153)(cid:8)(cid:0)?(cid:2)(cid:4)(cid:8)(cid:18)(cid:8)(cid:16)(cid:2)(cid:10)(cid:0)(cid:18)(cid:8)(cid:4)(cid:2)(cid:3)(cid:17)(cid:3)(cid:2)’(cid:0)(cid:17)’(cid:16)(cid:17)(cid:3)(cid:0)(cid:15))(cid:4)(cid:8)(cid:5)(cid:2)(cid:16)(cid:0)(cid:3))(cid:10))(cid:10)(cid:17)(cid:2)(cid:8)(cid:2)’(cid:0)
(cid:13)(cid:17)(cid:16)(cid:9)(cid:17)(cid:16)(cid:0)((cid:2)’(cid:14)(cid:0)(cid:18)(cid:8)(cid:5)(cid:2)(cid:10)(cid:8)(cid:4)(cid:3)(cid:2)’(cid:0)(cid:18)(cid:2)(cid:7)(cid:8)(cid:0)(cid:2)(cid:4)(cid:14)(cid:13)(cid:7)(cid:8)(cid:16)(cid:15)(cid:2)(cid:0)((cid:2)’(cid:14)(cid:0)(cid:16))(cid:4)(cid:2)(cid:5)(cid:0)(cid:18)(cid:8)A(cid:17)(cid:2)(cid:16)2(cid:0)>(cid:2)(cid:10)(cid:8)(cid:4)(cid:0)
(cid:10)(cid:8)(cid:15)(cid:17)(cid:4)(cid:2)(cid:10)(cid:8)(cid:0)(cid:18)(cid:2)(cid:7)(cid:8)(cid:0)(cid:15))(cid:16)(cid:13)(cid:18))(cid:0)3455(cid:29)(cid:30)(cid:29)6(cid:24)(cid:29)(cid:31)(cid:154)(cid:29)6(cid:24)89:(cid:30);46<(cid:0)(cid:15)(cid:13)(cid:18))(cid:4)(cid:0)=(cid:2)((cid:20)
>)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)(cid:15)(cid:17)(cid:4)(cid:16)(cid:8)?(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:18))’(cid:14)(cid:2)’(cid:0)(cid:16)(cid:8)(cid:14)(cid:2)(cid:0)?(cid:2)(cid:7)(cid:8)(cid:2)A)(cid:4)(cid:0)(cid:7))(cid:10)(cid:9)(cid:13)’(cid:0)(cid:18)(cid:2)’(cid:0)(cid:153)(cid:17)(cid:15)(cid:4)(cid:2)(cid:5)(cid:0)
(cid:18)(cid:13)(cid:15)(cid:2)(cid:8)’(cid:0)(cid:155)L(cid:156)(cid:157)%(cid:158)(cid:157)%b(cid:157)(cid:157)U(cid:18)(cid:2)’UV(cid:157)(cid:157)(cid:0)(cid:18)(cid:8)(cid:16)(cid:17)’(cid:153)(cid:17)(cid:3)(cid:2)’(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:16)(cid:2)A)(cid:4)(cid:0)i(cid:0)
A)(cid:7)(cid:8)(cid:3)(cid:17)(cid:16)B(cid:0)

(cid:159)x+*,(cid:0)(cid:21)(cid:0)
(cid:160)(cid:21),x(cid:21)(cid:0)(cid:1)(cid:6)*(cid:0)>x(cid:6)(cid:21),(cid:0)(cid:6)(cid:21)(cid:1)-,x(cid:6)(cid:21)(cid:0)(cid:1)*(cid:159)¡(cid:19)*(cid:0)(cid:19)(cid:21)==*¢*(cid:160)£*(cid:0)+*(cid:160)£>(cid:1)x¢(cid:23)(cid:21)(cid:160){(cid:0)(cid:1)¡(cid:19)*,(cid:0)
=x⁄(cid:20)>*¢¢(cid:21)¡(cid:159)(cid:0)(cid:1)-,(cid:159)(cid:21)(cid:22)x¢x(cid:21)(cid:159)(cid:0)

(cid:22)(cid:2)(cid:7)(cid:8)(cid:2)A)(cid:4)(cid:0) (cid:1))(cid:16)(cid:13)(cid:18))(cid:0)

¥(cid:17)(cid:15)(cid:4)(cid:2)(cid:5)(cid:0)(cid:6)(cid:2)(cid:15)(cid:9))(cid:4)(cid:0)

‘a(cid:0)

ƒa(cid:0)

iaa(cid:0)

[aa(cid:0)

(cid:0)
v2(cid:152)2‘2w (cid:0)-(cid:153)(cid:8)(cid:0)(cid:6)-(cid:6)(cid:0)

-(cid:153)(cid:8)(cid:0)(cid:6)-(cid:6)(cid:0)(cid:18)(cid:8)(cid:4)(cid:2)(cid:3)(cid:17)(cid:3)(cid:2)’(cid:0)(cid:17)’(cid:16)(cid:17)(cid:3)(cid:0)(cid:15))(cid:4)(cid:8)(cid:5)(cid:2)(cid:16)(cid:0)(cid:2)(cid:9)(cid:2)(cid:3)(cid:2)(cid:5)(cid:0)(cid:9)(cid:7)(cid:13)(cid:14)(cid:7)(cid:2)(cid:15)(cid:0)
((cid:2)’(cid:14)(cid:0)(cid:18)(cid:8)A(cid:17)(cid:2)(cid:16)(cid:0)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:18)(cid:8)(cid:16))(cid:7)(cid:8)(cid:15)(cid:2)(cid:0)(cid:13)(cid:4))(cid:5)(cid:0)(cid:9))’(cid:14)(cid:14)(cid:17)’(cid:2)(cid:0)(cid:2)(cid:16)(cid:2)(cid:17)(cid:0)(cid:16)(cid:8)(cid:18)(cid:2)(cid:3)2(cid:0)(cid:12)(cid:2)(cid:18)(cid:2)(cid:0)
(cid:9))’)(cid:4)(cid:8)(cid:16)(cid:8)(cid:2)’(cid:0)(cid:8)’(cid:8)(cid:0)(cid:17)(cid:153)(cid:8)(cid:0)(cid:6)-(cid:6)(cid:0)(cid:18)(cid:8)(cid:0)(cid:4)(cid:2)(cid:3)(cid:17)(cid:3)(cid:2)’(cid:0)(cid:16))(cid:7)(cid:5)(cid:2)(cid:18)(cid:2)(cid:9)(cid:0)i(cid:152)(cid:0)(cid:7))(cid:10)(cid:9)(cid:13)’(cid:18))’(cid:0)((cid:2)’(cid:14)(cid:0)
(cid:15))(cid:7)(cid:17)(cid:9)(cid:2)(cid:3)(cid:2)’(cid:0)(cid:15)(cid:2)(cid:5)(cid:2)(cid:10)(cid:8)(cid:10)“(cid:2)(cid:0)(cid:10))(cid:7)(cid:16)(cid:2)(cid:0)(cid:4)(cid:17)(cid:4)(cid:17)(cid:10)(cid:2)’(cid:0)(cid:12)(cid:13)(cid:4)(cid:8)(cid:16))(cid:3)’(cid:8)(cid:3)(cid:0)(cid:6)(cid:16)(cid:2)(cid:16)(cid:8)(cid:10)(cid:16)(cid:8)(cid:3)(cid:2)(cid:0)(cid:6)(cid:159)(cid:21)(cid:6)2(cid:0)
(cid:18))’(cid:14)(cid:2)’(cid:0)(cid:5)(cid:2)(cid:10)(cid:8)(cid:4)(cid:0)(cid:17)(cid:153)(cid:8)(cid:0)(cid:6)-(cid:6)(cid:0)(cid:10))A)(cid:10)(cid:2)(cid:7)(cid:0)¤v(cid:149)a¤(cid:0)A)(cid:7)(cid:2)(cid:7)(cid:16)(cid:8)(cid:0)(cid:150):(cid:24);:<(cid:29)(cid:31)((cid:2)’(cid:14)(cid:0)
(cid:18)(cid:8)A(cid:2)’(cid:14)(cid:17)’(cid:0)(cid:15))’(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:3)(cid:2)(cid:16))(cid:14)(cid:13)(cid:7)(cid:8)(cid:0)A(cid:2)(cid:8)(cid:3)(cid:0)(cid:18)(cid:2)’(cid:0)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:18)(cid:8)(cid:16))(cid:7)(cid:8)(cid:15)(cid:2)(cid:0)(cid:13)(cid:4))(cid:5)(cid:0)
(cid:9))’(cid:14)(cid:14)(cid:17)’(cid:2)2(cid:0)

(cid:0)

v2(cid:152)2[2(cid:0) (cid:6)(cid:16)(cid:17)(cid:18)(cid:8)(cid:0)(cid:23)(cid:2)(cid:10)(cid:17)(cid:10)(cid:0)

(cid:1))(cid:16)(cid:13)(cid:18))(cid:0)3455(cid:29)(cid:30)(cid:29)6(cid:24)(cid:29)(cid:31)(cid:154)(cid:29)6(cid:24)89:(cid:30);46<(cid:0)(cid:15)(cid:13)(cid:18))(cid:4)(cid:0)=(cid:2)((cid:20)>)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)
(cid:15)(cid:17)(cid:4)(cid:16)(cid:8)?(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:18)(cid:8)(cid:8)(cid:15)(cid:9)(cid:4))(cid:15))’(cid:16)(cid:2)(cid:10)(cid:8)(cid:3)(cid:2)’(cid:0)(cid:17)’(cid:16)(cid:17)(cid:3)(cid:0)(cid:15))’(cid:18)(cid:17)(cid:14)(cid:2)(cid:0)(cid:7)(cid:2)(cid:16)(cid:2)(cid:20)(cid:7)(cid:2)(cid:16)(cid:2)(cid:0)
(cid:9))’(cid:14))(cid:4)(cid:17)(cid:2)(cid:7)(cid:2)’(cid:0)(cid:15)(cid:2)(cid:3)(cid:2)’(cid:2)’(cid:0)(cid:18)(cid:2)’(cid:0)’(cid:13)’(cid:20)(cid:15)(cid:2)(cid:3)(cid:2)’(cid:2)’(cid:0)(cid:16)(cid:8)’(cid:14)(cid:3)(cid:2)(cid:16)(cid:0)(cid:3))'(cid:2)(cid:15)(cid:2)(cid:16)(cid:2)’(cid:0)(cid:18)(cid:8)(cid:0)
(cid:12)(cid:7)(cid:13)?(cid:8)’(cid:10)(cid:8)(cid:0)(cid:19)2(cid:21)2(cid:0)⁄(cid:13)(cid:14)((cid:2)(cid:3)(cid:2)(cid:7)(cid:16)(cid:2)(cid:0)(cid:5)(cid:2)(cid:10)(cid:8)(cid:4)(cid:0)(cid:6)(cid:17)(cid:10))’(cid:2)(cid:10)(cid:0)(cid:15)(cid:2)(cid:7))(cid:16)(cid:0)[ai\\(cid:0)(cid:18))’(cid:14)(cid:2)’(cid:0)
?(cid:2)(cid:7)(cid:8)(cid:2)A)(cid:4)(cid:0)(cid:9))’()(cid:7)(cid:16)(cid:2)(cid:0)(cid:18)(cid:2)(cid:7)(cid:8)(cid:0)(cid:18)(cid:2)(cid:16)(cid:2)(cid:0)(cid:12)(cid:13)(cid:18))(cid:10)(cid:0)[ai\\2(cid:0)(cid:12))’(cid:14)(cid:13)(cid:4)(cid:2)(cid:5)(cid:2)’(cid:0)(cid:18)(cid:2)(cid:16)(cid:2)(cid:0)
(cid:15))’(cid:14)(cid:14)(cid:17)’(cid:2)(cid:3)(cid:2)’(cid:0)y(cid:31)z:(cid:24);:<(cid:29)(cid:31)(cid:137)(cid:136)(cid:138)~(cid:134)(cid:139)(cid:140)(cid:141)(cid:129)((cid:2)’(cid:14)(cid:0)(cid:16))(cid:4)(cid:2)(cid:5)(cid:0)(cid:18)(cid:8)A(cid:2)’(cid:14)(cid:17)’2(cid:0)

(cid:0)
(cid:12)(cid:2)(cid:18)(cid:2)(cid:0)(cid:9))’)(cid:4)(cid:8)(cid:16)(cid:8)(cid:2)’(cid:0)(cid:8)’(cid:8)(cid:0))(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:7)(cid:2)(cid:16)(cid:2)(cid:20)(cid:7)(cid:2)(cid:16)(cid:2)(cid:0)(cid:9))’(cid:14))(cid:4)(cid:17)(cid:2)(cid:7)(cid:2)’(cid:0)
(cid:15)(cid:2)(cid:3)(cid:2)’(cid:2)’(cid:0) ⁄i](cid:0)(cid:18)(cid:2)’(cid:0)’(cid:13)’(cid:0)(cid:15)(cid:2)(cid:3)(cid:2)’(cid:2)’(cid:0) ⁄[](cid:0)(cid:15))’(cid:14)(cid:14)(cid:17)’(cid:2)(cid:3)(cid:2)’(cid:0)(cid:15))(cid:16)(cid:13)(cid:18))(cid:0)
3455(cid:29)(cid:30)(cid:29)6(cid:24)(cid:29)(cid:31)(cid:154)(cid:29)6(cid:24)89:(cid:30);46<(cid:0)(cid:15)(cid:13)(cid:18))(cid:4)(cid:0)=(cid:2)((cid:20)>)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)(cid:15)(cid:17)(cid:4)(cid:16)(cid:8)?(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)2(cid:0)
(cid:23))(cid:18)(cid:17)(cid:2)(cid:0)?(cid:2)(cid:7)(cid:8)(cid:2)A)(cid:4)(cid:0)(cid:16))(cid:7)(cid:10))A(cid:17)(cid:16)(cid:0)(cid:15))(cid:15)(cid:8)(cid:4)(cid:8)(cid:3)(cid:8)(cid:0)(cid:3)(cid:13)(cid:7))(cid:4)(cid:2)(cid:10)(cid:8)(cid:0)(cid:10))A)(cid:10)(cid:2)(cid:7)(cid:0)a(cid:149)§ƒ(cid:0)

(cid:0)

(cid:0)

(cid:1)(cid:2)(cid:3)(cid:2)(cid:4)(cid:2)(cid:5)(cid:0)(cid:6)(cid:3)(cid:7)(cid:8)(cid:9)(cid:10)(cid:8)(cid:0)(cid:11)(cid:0)(cid:12)(cid:7)(cid:13)(cid:14)(cid:7)(cid:2)(cid:15)(cid:0)(cid:6)(cid:16)(cid:17)(cid:18)(cid:8)(cid:0)(cid:19)(cid:20)(cid:21)(cid:22)(cid:0)(cid:23)(cid:13)(cid:15)(cid:9)(cid:17)(cid:16)(cid:2)(cid:10)(cid:8)(cid:0)(cid:6)(cid:16)(cid:2)(cid:16)(cid:8)(cid:10)(cid:16)(cid:8)(cid:3)(cid:0)

(cid:10)(cid:24)(cid:5)(cid:8)(cid:25)(cid:14)(cid:14)(cid:2)(cid:0)(cid:9)(cid:24)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:2)(cid:25)(cid:0)(cid:6)(cid:26)(cid:27)(cid:0)(cid:1)(cid:17)(cid:4)(cid:16)(cid:8)(cid:28)(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:4)(cid:2)(cid:29)(cid:2)(cid:3)(cid:0)(cid:18)(cid:8)(cid:14)(cid:17)(cid:25)(cid:2)(cid:3)(cid:2)(cid:25)(cid:30)(cid:0)(cid:31)(cid:4)(cid:24)(cid:5)(cid:0)
(cid:3)(cid:2)(cid:7)(cid:24)(cid:25)(cid:2)(cid:0)(cid:18)(cid:2)(cid:7)(cid:8)(cid:0) !(cid:0)(cid:3)(cid:24)""(cid:2)(cid:15)(cid:2)(cid:16)(cid:2)(cid:25)(cid:0)(cid:29)(cid:2)(cid:25)(cid:14)(cid:0)(cid:2)(cid:18)(cid:2)(cid:0)(cid:18)(cid:8)(cid:0)(cid:19)(cid:30)(cid:21)(cid:30)(cid:0)#(cid:13)(cid:14)(cid:29)(cid:2)(cid:3)(cid:2)(cid:7)(cid:16)(cid:2)$(cid:0)
(cid:16)(cid:24)(cid:7)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:10)(cid:2)(cid:16)(cid:17)(cid:0)(cid:3)(cid:24)""(cid:2)(cid:15)(cid:2)(cid:16)(cid:2)(cid:25)(cid:0)(cid:29)(cid:2)(cid:25)(cid:14)(cid:0)(cid:16)(cid:8)(cid:18)(cid:2)(cid:3)(cid:0)(cid:16)(cid:24)(cid:7)(cid:10)(cid:2)(cid:15)(cid:9)(cid:24)(cid:4)(cid:0)(cid:10)(cid:24)(cid:5)(cid:8)(cid:25)(cid:14)(cid:14)(cid:2)(cid:0)(cid:17)(cid:25)(cid:16)(cid:17)(cid:3)(cid:0)
(cid:15)(cid:24)(cid:25)(cid:14)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:25)(cid:29)(cid:2)(cid:0)(cid:18)(cid:8)%(cid:17)(cid:16)(cid:17)(cid:5)(cid:3)(cid:2)(cid:25)(cid:0)(cid:8)(cid:25)&(cid:13)(cid:7)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)’()*+,-(cid:0)(cid:16)(cid:2)(cid:15)%(cid:2)(cid:5)(cid:2)(cid:25)(cid:0)
%(cid:24)(cid:7)(cid:17)(cid:9)(cid:2)(cid:0)’()*+,-(cid:0)(cid:18)(cid:2)(cid:7)(cid:8)(cid:0)(cid:10)(cid:24)(cid:16)(cid:8)(cid:2)(cid:9)(cid:0).(cid:8)(cid:4)(cid:2)(cid:29)(cid:2)(cid:5)(cid:0)(cid:3)(cid:24)""(cid:2)(cid:15)(cid:2)(cid:16)(cid:2)(cid:25)(cid:0)(cid:18)(cid:8)(cid:0)(cid:19)(cid:30)(cid:21)(cid:30)(cid:0)
#(cid:13)(cid:14)(cid:29)(cid:2)(cid:3)(cid:2)(cid:7)(cid:16)(cid:2)(cid:30)(cid:0)(cid:12)(cid:24)(cid:15)(cid:8)(cid:4)(cid:8)(cid:5)(cid:2)(cid:25)(cid:0)’()*+,-(cid:0)(cid:15)(cid:24)(cid:25)(cid:14)(cid:14)(cid:17)(cid:25)(cid:2)(cid:3)(cid:2)(cid:25)(cid:0)(cid:8)(cid:25)&(cid:13)(cid:7)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)
(cid:28)(cid:2)(cid:7)(cid:8)(cid:2)%(cid:24)(cid:4)(cid:0)(cid:29)(cid:2)(cid:25)(cid:14)(cid:0)(cid:18)(cid:8)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:18)(cid:2)(cid:7)(cid:8)(cid:0)(cid:5)(cid:2)(cid:10)(cid:8)(cid:4)(cid:0)(cid:9)(cid:24)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:2)(cid:25)(cid:0)(cid:1)(cid:6)(cid:26)(cid:27)(cid:0)(cid:29)(cid:2)(cid:25)(cid:14)(cid:0)
(cid:3)(cid:24)(cid:15)(cid:17)(cid:18)(cid:8)(cid:2)(cid:25)(cid:0)(cid:18)(cid:8)(cid:4)(cid:2)(cid:3)(cid:17)(cid:3)(cid:2)(cid:25)(cid:0)(cid:9)(cid:7)(cid:13)(cid:10)(cid:24)(cid:10)(cid:0)’()*+,-/012(cid:15)(cid:24)(cid:25)(cid:14)(cid:14)(cid:17)(cid:25)(cid:2)(cid:3)(cid:2)(cid:25)(cid:0)(cid:15)(cid:24)(cid:16)(cid:13)(cid:18)(cid:24)(cid:0)
345,60*2’()*+,-/017(cid:0)(cid:0)
(cid:0)
(cid:22)(cid:2)(cid:7)(cid:8)(cid:2)%(cid:24)(cid:4)(cid:0)(cid:9)(cid:24)(cid:25)(cid:29)(cid:24)(cid:7)(cid:16)(cid:2)(cid:0)(cid:29)(cid:2)(cid:25)(cid:14)(cid:0)(cid:18)(cid:8)(cid:14)(cid:17)(cid:25)(cid:2)(cid:3)(cid:2)(cid:25)(cid:0)(cid:29)(cid:2)(cid:8)(cid:16)(cid:17)(cid:0)8(cid:17)(cid:15)(cid:4)(cid:2)(cid:5)(cid:0)
(cid:6)(cid:1)(cid:12)9(cid:1):(cid:6)(cid:0);<=>$(cid:0)8(cid:17)(cid:15)(cid:4)(cid:2)(cid:5)(cid:0)(cid:6)(cid:1)(cid:26)9(cid:1)(cid:26)(cid:0);<?>$(cid:0)8(cid:17)(cid:15)(cid:4)(cid:2)(cid:5)(cid:0)(cid:6)(cid:1)(cid:23)(cid:0);<@>$(cid:0)
8(cid:17)(cid:15)(cid:4)(cid:2)(cid:5)(cid:0)(cid:9)(cid:7)(cid:2)(cid:3)(cid:16)(cid:8)(cid:3)(cid:0)(cid:18)(cid:13)(cid:3)(cid:16)(cid:24)(cid:7)(cid:0);<A>$(cid:0)8(cid:17)(cid:15)(cid:4)(cid:2)(cid:5)(cid:0)(cid:7)(cid:24)(cid:10)(cid:16)(cid:13)(cid:7)(cid:2)(cid:25)9(cid:7)(cid:17)(cid:15)(cid:2)(cid:5)(cid:0)(cid:15)(cid:2)(cid:3)(cid:2)(cid:25)(cid:0)
;<B>$(cid:0)8(cid:17)(cid:15)(cid:4)(cid:2)(cid:5)(cid:0)(cid:5)(cid:13)(cid:16)(cid:24)(cid:4)(cid:0);<C>$(cid:0)8(cid:17)(cid:15)(cid:4)(cid:2)(cid:5)(cid:0)(cid:16)(cid:13)(cid:3)(cid:13)9.(cid:2)(cid:7)(cid:17)(cid:25)(cid:14)(cid:0)(cid:3)(cid:4)(cid:13)(cid:25)(cid:16)(cid:13)(cid:25)(cid:14)(cid:0);< >$(cid:0)
8(cid:17)(cid:15)(cid:4)(cid:2)(cid:5)(cid:0)%(cid:2)(cid:25)(cid:3)(cid:0)(cid:17)(cid:15)(cid:17)(cid:15)(cid:0)(cid:9)(cid:24)(cid:15)(cid:24)(cid:7)(cid:8)(cid:25)(cid:16)(cid:2)(cid:5)(cid:0);<!>$(cid:0)8(cid:17)(cid:15)(cid:4)(cid:2)(cid:5)(cid:0)D(cid:12)E(cid:0);<F>$(cid:0)(cid:18)(cid:2)(cid:25)(cid:0)
8(cid:17)(cid:15)(cid:4)(cid:2)(cid:5)(cid:0)(cid:23)(cid:13)(cid:9)(cid:24)(cid:7)(cid:2)(cid:10)(cid:8)(cid:0)G(cid:25)(cid:8)(cid:16)(cid:0)(cid:19)(cid:24)(cid:10)(cid:2)(cid:0);<=H>(cid:30)(cid:0)I(cid:2)(cid:10)(cid:8)(cid:4)(cid:0)(cid:9)(cid:24)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:2)(cid:25)(cid:0)
(cid:15)(cid:24)(cid:25)(cid:14)(cid:14)(cid:17)(cid:25)(cid:2)(cid:3)(cid:2)(cid:25)(cid:0)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)J(cid:2)(cid:29)(cid:20)I(cid:24)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)(cid:1)(cid:17)(cid:4)(cid:16)(cid:8)(cid:28)(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:18)(cid:8)(cid:16)(cid:17)(cid:25)8(cid:17)(cid:3)(cid:2)(cid:25)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)
(cid:16)(cid:2)%(cid:24)(cid:4)(cid:0)?(cid:0)%(cid:24)(cid:7)(cid:8)(cid:3)(cid:17)(cid:16)K(cid:0)

:(cid:26)D(cid:27)L(cid:0)(cid:21)(cid:21)(cid:0)
I(cid:26)(cid:6)(cid:21)L(cid:0)(cid:12)(cid:27)(cid:1)(cid:31)(cid:19)(cid:27)L(cid:26)M(cid:0)(cid:1)(cid:31)(cid:19)(cid:27)L(cid:0)J(cid:26)#(cid:20)I(cid:27)EE(cid:21)(cid:31):(cid:0)(cid:1)GL:(cid:21)(cid:22)(cid:26)E(cid:21)(cid:26):(cid:0)

(cid:22)(cid:2)(cid:7)(cid:8)(cid:2)%(cid:24)(cid:4)(cid:0)
E(cid:24)(cid:10)(cid:9)(cid:13)(cid:25)(cid:0)

(cid:22)(cid:2)(cid:7)(cid:8)(cid:2)%(cid:24)(cid:4)(cid:0)
(cid:9)(cid:24)(cid:25)(cid:29)(cid:24)(cid:7)(cid:16)(cid:2)(cid:0)

%(cid:24)(cid:16)(cid:2)(cid:0) (cid:10)(cid:16)(cid:18)(cid:30)(cid:24)(cid:7)(cid:7)(cid:13)(cid:7)(cid:0) (cid:16)(cid:30)(cid:10)(cid:16)(cid:2)(cid:16)(cid:8)(cid:10)(cid:16)(cid:8)""(cid:10)(cid:0) (cid:9)(cid:30)(cid:28)(cid:2)(cid:4)(cid:17)(cid:24)(cid:0)

;=>(cid:0)

;?>(cid:0)

;@>(cid:0)

;A>(cid:0)

;B>(cid:0)

;C>(cid:0)

;(cid:21)(cid:25)(cid:16)(cid:24)(cid:7)""(cid:24)(cid:9)(cid:16)>(cid:0) B(cid:30)HBH(cid:0) H(cid:30)?FF(cid:0) =C(cid:30)F=C(cid:0) H(cid:30)HHH(cid:0)

#=(cid:0)

#?(cid:0)

<=(cid:0)

<?(cid:0)

<@(cid:0)

<A(cid:0)

<C(cid:0)

(cid:20)H(cid:30)=@@(cid:0) H(cid:30)HBC(cid:0) (cid:20)?(cid:30)@C?(cid:0) H(cid:30)H=!(cid:0)

H(cid:30)?=C(cid:0) H(cid:30)H  (cid:0)

?(cid:30)!=A(cid:0) H(cid:30)HHB(cid:0)

(cid:20)H(cid:30)=  (cid:0) H(cid:30)H!?(cid:0) (cid:20)?(cid:30)=CA(cid:0) H(cid:30)H@H(cid:0)

H(cid:30)HCB(cid:0) H(cid:30)H=F(cid:0)

@(cid:30)AFC(cid:0) H(cid:30)HHH(cid:0)

H(cid:30)H@C(cid:0) H(cid:30)H==(cid:0)

@(cid:30)??A(cid:0) H(cid:30)HH=(cid:0)

;(cid:21)(cid:25)(cid:16)(cid:24)(cid:7)""(cid:24)(cid:9)(cid:16)>(cid:0) C(cid:30)H!A(cid:0) H(cid:30)C= (cid:0)

F(cid:30)!CA(cid:0) H(cid:30)HHH(cid:0)

<=(cid:0)

<?(cid:0)

<@(cid:0)

<A(cid:0)

<B(cid:0)

<C(cid:0)

< (cid:0)

<!(cid:0)

<F(cid:0)

(cid:20)H(cid:30)@  (cid:0) H(cid:30)==B(cid:0) (cid:20)@(cid:30)?FA(cid:0) H(cid:30)HH=(cid:0)

H(cid:30)A=F(cid:0) H(cid:30)= A(cid:0)

?(cid:30)A=B(cid:0) H(cid:30)H=C(cid:0)

(cid:20)H(cid:30)CH!(cid:0) H(cid:30)=CB(cid:0) (cid:20)@(cid:30)CF=(cid:0) H(cid:30)HHH(cid:0)

H(cid:30)=!C(cid:0) H(cid:30)HB?(cid:0)

@(cid:30)BBC(cid:0) H(cid:30)HHH(cid:0)

(cid:20)H(cid:30)H?B(cid:0) H(cid:30)H=@(cid:0) (cid:20)=(cid:30)!!F(cid:0) H(cid:30)HBF(cid:0)

H(cid:30)HF!(cid:0) H(cid:30)H? (cid:0)

@(cid:30)C @(cid:0) H(cid:30)HHH(cid:0)

H(cid:30)HH=(cid:0) H(cid:30)HH=(cid:0)

=(cid:30)AB=(cid:0) H(cid:30)=A (cid:0)

H(cid:30)=BC(cid:0) H(cid:30)=HH(cid:0)

=(cid:30)BB!(cid:0) H(cid:30)==F(cid:0)

H(cid:30)=!@(cid:0) H(cid:30)==?(cid:0)

=(cid:30)C@=(cid:0) H(cid:30)=H@(cid:0)

<=H(cid:0) (cid:20)H(cid:30)AF!(cid:0) H(cid:30)?=A(cid:0) (cid:20)?(cid:30)@??(cid:0) H(cid:30)H?H(cid:0)

(cid:0)

D(cid:24)(cid:7)(cid:18)(cid:2)(cid:10)(cid:2)(cid:7)(cid:3)(cid:2)(cid:25)(cid:0)(cid:28)(cid:2)(cid:7)(cid:8)(cid:2)%(cid:24)(cid:4)(cid:0)(cid:9)(cid:24)(cid:25)(cid:29)(cid:24)(cid:7)(cid:16)(cid:2)(cid:0)(cid:5)(cid:2)(cid:10)(cid:8)(cid:4)(cid:0)(cid:9)(cid:24)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:2)(cid:25)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0):(cid:2)%(cid:24)(cid:4)(cid:0)
?(cid:0)(cid:18)(cid:8)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)8(cid:17)(cid:15)(cid:4)(cid:2)(cid:5)(cid:0)’()*+,-2(cid:15)(cid:2)(cid:3)(cid:10)(cid:8)(cid:15)(cid:2)(cid:4)2(cid:18)(cid:24)(cid:25)(cid:14)(cid:2)(cid:25)(cid:0)*/(NO),++,45,+NOP2
(cid:10)(cid:24)%(cid:2)(cid:25)(cid:29)(cid:2)(cid:3)(cid:0)?(cid:0)’()*+,-2(cid:17)(cid:25)(cid:16)(cid:17)(cid:3)(cid:0)(cid:15)(cid:2)(cid:10)(cid:8)(cid:25)(cid:14)(cid:0)(cid:15)(cid:2)(cid:10)(cid:8)(cid:25)(cid:14)(cid:0)(cid:28)(cid:2)(cid:7)(cid:8)(cid:2)%(cid:24)(cid:4)(cid:0)(cid:3)(cid:24)(cid:15)(cid:17)(cid:18)(cid:8)(cid:2)(cid:25)(cid:0)
(cid:18)(cid:8)8(cid:2)(cid:4)(cid:2)(cid:25)(cid:3)(cid:2)(cid:25)(cid:0)(cid:9)(cid:7)(cid:13)(cid:10)(cid:24)(cid:10)(cid:0)’()*+,-/012(cid:15)(cid:24)(cid:25)(cid:14)(cid:14)(cid:17)(cid:25)(cid:2)(cid:3)(cid:2)(cid:25)(cid:0)(cid:15)(cid:24)(cid:16)(cid:13)(cid:18)(cid:24)(cid:0)345,60*2
’()*+,-/01(cid:30)(cid:0) Q60PO52 ,RR,’+2 (cid:2)(cid:7)(cid:24)(cid:2)(cid:0) (cid:16)(cid:24)(cid:7)(cid:10)(cid:2)(cid:15)(cid:9)(cid:24)(cid:4)(cid:0) (cid:3)(cid:24)(cid:15)(cid:17)(cid:18)(cid:8)(cid:2)(cid:25)(cid:0)
(cid:18)(cid:8)(cid:15)(cid:2)(cid:25)&(cid:2)(cid:2)(cid:16)(cid:3)(cid:2)(cid:25)(cid:0)(cid:10)(cid:24)%(cid:2)(cid:14)(cid:2)(cid:8)(cid:0)(cid:8)(cid:25)&(cid:13)(cid:7)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:16)(cid:2)(cid:15)%(cid:2)(cid:5)(cid:2)(cid:25)(cid:0)%(cid:2)(cid:14)(cid:8)(cid:0)(cid:2)(cid:7)(cid:24)(cid:2)(cid:0)(cid:29)(cid:2)(cid:25)(cid:14)(cid:0)(cid:16)(cid:8)(cid:18)(cid:2)(cid:3)(cid:0)
(cid:16)(cid:24)(cid:7)(cid:10)(cid:2)(cid:15)(cid:9)(cid:24)(cid:4)(cid:0)(cid:10)(cid:24)(cid:10)(cid:17)(cid:2)(cid:8)(cid:0)(cid:18)(cid:24)(cid:25)(cid:14)(cid:2)(cid:25)(cid:0)(cid:9)(cid:24)(cid:7)(cid:10)(cid:2)(cid:15)(cid:2)(cid:2)(cid:25)(cid:0);=C>(cid:0)(cid:18)(cid:8)(cid:15)(cid:2)(cid:25)(cid:2)(cid:0)(cid:5)(cid:2)(cid:10)(cid:8)(cid:4)(cid:0)(cid:7)(cid:2)(cid:16)(cid:2)(cid:20)(cid:7)(cid:2)(cid:16)(cid:2)(cid:0)
-60PO52,RR,’+(cid:0)(cid:18)(cid:2)(cid:7)(cid:8)(cid:0)(cid:3)(cid:24)""(cid:2)(cid:15)(cid:2)(cid:16)(cid:2)(cid:25)(cid:0)(cid:29)(cid:2)(cid:25)(cid:14)(cid:0)(cid:15)(cid:24)(cid:15)(cid:8)(cid:4)(cid:8)(cid:3)(cid:8)(cid:0)(cid:10)(cid:2)(cid:15)(cid:9)(cid:24)(cid:4)(cid:0)(cid:18)(cid:8)(cid:10)(cid:24)(cid:16)(cid:8)(cid:2)(cid:9)(cid:0)
’()*+,-2(cid:3)(cid:24)(cid:15)(cid:17)(cid:18)(cid:8)(cid:2)(cid:25)(cid:0)(cid:18)(cid:8)(cid:15)(cid:2)(cid:10)(cid:17)(cid:3)(cid:3)(cid:2)(cid:25)(cid:0)(cid:10)(cid:24)%(cid:2)(cid:14)(cid:2)(cid:8)(cid:0)(cid:8)(cid:25)&(cid:13)(cid:7)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:16)(cid:2)(cid:15)%(cid:2)(cid:5)(cid:2)(cid:25)(cid:0)
(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:3)(cid:24)""(cid:2)(cid:15)(cid:2)(cid:16)(cid:2)(cid:25)(cid:0)(cid:29)(cid:2)(cid:25)(cid:14)(cid:0)(cid:16)(cid:8)(cid:18)(cid:2)(cid:3)(cid:0)(cid:15)(cid:24)(cid:15)(cid:8)(cid:4)(cid:8)(cid:3)(cid:8)(cid:0)(cid:10)(cid:2)(cid:15)(cid:9)(cid:24)(cid:4)(cid:0)(cid:10)(cid:24)(cid:10)(cid:17)(cid:2)(cid:8)(cid:0)(cid:18)(cid:24)(cid:25)(cid:14)(cid:2)(cid:25)(cid:0)
’()*+,-0S6(cid:30)(cid:0)(cid:0)

(cid:0)

(cid:2)>TI(cid:2)(cid:10)(cid:8)(cid:4)(cid:0)(cid:27)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:18)(cid:2)(cid:25)(cid:0)E(cid:6)(cid:27)(cid:0)(cid:28)(cid:2)(cid:7)(cid:8)(cid:2)%(cid:24)(cid:4)(cid:0)(cid:9)(cid:24)(cid:25)(cid:14)(cid:24)(cid:4)(cid:17)(cid:2)(cid:7)(cid:2)(cid:25)(cid:0)(cid:15)(cid:2)(cid:3)(cid:2)(cid:25)(cid:2)(cid:25)(cid:0)

;#=>(cid:0)

I(cid:2)(cid:10)(cid:8)(cid:4)(cid:0) (cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0) (cid:28)(cid:2)(cid:7)(cid:8)(cid:2)%(cid:24)(cid:4)(cid:0) (cid:9)(cid:24)(cid:25)(cid:14)(cid:24)(cid:4)(cid:17)(cid:2)(cid:7)(cid:2)(cid:25)(cid:0) (cid:15)(cid:2)(cid:3)(cid:2)(cid:25)(cid:2)(cid:25)(cid:0)
(cid:15)(cid:24)(cid:25)(cid:14)(cid:14)(cid:17)(cid:25)(cid:2)(cid:3)(cid:2)(cid:25)(cid:0)(cid:15)(cid:24)(cid:16)(cid:13)(cid:18)(cid:24)(cid:0)P/RR,-,0’,2U,0’N56-3/01(cid:0)5OP,(2V6S4
W,--/O+(cid:0)(cid:15)(cid:17)(cid:4)(cid:16)(cid:8)(cid:28)(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:18)(cid:8)(cid:0)(cid:16)(cid:2)(cid:15)(cid:9)(cid:8)(cid:4)(cid:3)(cid:2)(cid:25)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)X(cid:2)(cid:15)%(cid:2)(cid:7)(cid:0)@(cid:0)%(cid:24)(cid:7)(cid:8)(cid:3)(cid:17)(cid:16)K(cid:0)

(cid:0)
X(cid:2)(cid:15)%(cid:2)(cid:7)(cid:0)@(cid:30)(cid:0)(cid:27)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:12)(cid:24)(cid:25)(cid:14)(cid:24)(cid:4)(cid:17)(cid:2)(cid:7)(cid:2)(cid:25)(cid:0)(cid:1)(cid:2)(cid:3)(cid:2)(cid:25)(cid:2)(cid:25)(cid:0);#=>(cid:0)
(cid:0)

(cid:12)(cid:2)(cid:18)(cid:2)(cid:0)X(cid:2)(cid:15)%(cid:2)(cid:7)(cid:0)@(cid:0)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:18)(cid:8)(cid:4)(cid:8)(cid:5)(cid:2)(cid:16)(cid:0)%(cid:2)(cid:5).(cid:2)(cid:0)(cid:5)(cid:2)(cid:10)(cid:8)(cid:4)(cid:0)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)
(cid:3)(cid:24)(cid:4)(cid:8)(cid:15)(cid:2)(cid:0)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)(cid:10)(cid:17)(cid:18)(cid:2)(cid:5)(cid:0)(cid:15)(cid:24)(cid:15)%(cid:24)(cid:7)(cid:8)(cid:3)(cid:2)(cid:25)(cid:0)(cid:9)(cid:13)(cid:4)(cid:2)(cid:0)(cid:29)(cid:2)(cid:25)(cid:14)(cid:0)(cid:10)(cid:2)(cid:15)(cid:2)$(cid:0)(cid:2)(cid:7)(cid:24)(cid:2)(cid:0)(cid:18)(cid:24)(cid:25)(cid:14)(cid:2)(cid:25)(cid:0)
(cid:25)(cid:8)(cid:4)(cid:2)(cid:8)(cid:0)(cid:9)(cid:24)(cid:25)(cid:14)(cid:24)(cid:4)(cid:17)(cid:2)(cid:7)(cid:2)(cid:25)(cid:0)(cid:15)(cid:2)(cid:3)(cid:2)(cid:25)(cid:0)(cid:29)(cid:2)(cid:25)(cid:14)(cid:0)(cid:16)(cid:8)(cid:25)(cid:14)(cid:14)(cid:8)(cid:0)(cid:18)(cid:8)(cid:0)(cid:10)(cid:2)(cid:16)(cid:17)(cid:0)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)(cid:2)(cid:3)(cid:2)(cid:25)(cid:0)
(cid:15)(cid:24)(cid:15)(cid:8)(cid:4)(cid:8)(cid:3)(cid:8)(cid:0)(cid:25)(cid:8)(cid:4)(cid:2)(cid:8)(cid:0)(cid:29)(cid:2)(cid:25)(cid:14)(cid:0)(cid:16)(cid:8)(cid:25)(cid:14)(cid:14)(cid:8)(cid:0)(cid:9)(cid:17)(cid:4)(cid:2)(cid:0)(cid:18)(cid:8)(cid:0)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)(cid:4)(cid:2)(cid:8)(cid:25)(cid:25)(cid:29)(cid:2)$(cid:0)%(cid:24)(cid:14)(cid:8)(cid:16)(cid:17)(cid:0)(cid:9)(cid:17)(cid:4)(cid:2)(cid:0)
(cid:10)(cid:24)%(cid:2)(cid:4)(cid:8)(cid:3)(cid:25)(cid:29)(cid:2)(cid:0)(cid:2)(cid:7)(cid:24)(cid:2)(cid:0)(cid:18)(cid:24)(cid:25)(cid:14)(cid:2)(cid:25)(cid:0)(cid:25)(cid:8)(cid:4)(cid:2)(cid:8)(cid:0)(cid:9)(cid:24)(cid:25)(cid:14)(cid:24)(cid:4)(cid:17)(cid:2)(cid:7)(cid:2)(cid:25)(cid:0)(cid:15)(cid:2)(cid:3)(cid:2)(cid:25)(cid:0)(cid:29)(cid:2)(cid:25)(cid:14)(cid:0)(cid:7)(cid:24)(cid:25)(cid:18)(cid:2)(cid:5)(cid:0)
(cid:18)(cid:8)(cid:0)(cid:10)(cid:2)(cid:16)(cid:17)(cid:0)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)(cid:2)(cid:3)(cid:2)(cid:25)(cid:0)(cid:15)(cid:24)(cid:15)(cid:8)(cid:4)(cid:8)(cid:3)(cid:8)(cid:0)(cid:25)(cid:8)(cid:4)(cid:2)(cid:8)(cid:0)(cid:29)(cid:2)(cid:25)(cid:14)(cid:0)(cid:7)(cid:24)(cid:25)(cid:18)(cid:2)(cid:5)(cid:0)(cid:9)(cid:17)(cid:4)(cid:2)(cid:0)(cid:18)(cid:8)(cid:0)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)
(cid:4)(cid:2)(cid:8)(cid:25)(cid:25)(cid:29)(cid:2)(cid:30)(cid:0)(cid:0)

(cid:6)(cid:24)(cid:4)(cid:2)(cid:25)8(cid:17)(cid:16)(cid:25)(cid:29)(cid:2)(cid:0)(cid:5)(cid:2)(cid:10)(cid:8)(cid:4)(cid:0)(cid:24)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)E(cid:6)(cid:27)(cid:0)(cid:18)(cid:8)(cid:16)(cid:2)(cid:15)(cid:9)(cid:8)(cid:4)(cid:3)(cid:2)(cid:25)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)

X(cid:2)(cid:15)%(cid:2)(cid:7)(cid:0)A(cid:0)%(cid:24)(cid:7)(cid:8)(cid:3)(cid:17)(cid:16)K(cid:0)

X(cid:2)(cid:15)%(cid:2)(cid:7)(cid:0)A(cid:30)(cid:0)I(cid:2)(cid:10)(cid:8)(cid:4)(cid:0)(cid:27)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)E(cid:6)(cid:27)(cid:0)(cid:9)(cid:24)(cid:25)(cid:14)(cid:24)(cid:4)(cid:17)(cid:2)(cid:7)(cid:2)(cid:25)(cid:0)(cid:15)(cid:2)(cid:3)(cid:2)(cid:25)(cid:2)(cid:25)(cid:0)
(cid:0)

(cid:0)

E(cid:2)(cid:25)(cid:14)(cid:3)(cid:17)(cid:15)(cid:2)(cid:25)(cid:0)(cid:25)(cid:8)(cid:4)(cid:2)(cid:8)(cid:0)E(cid:6)(cid:27)(cid:0)(cid:18)(cid:2)(cid:7)(cid:8)(cid:0)(cid:3)(cid:24)(cid:4)(cid:8)(cid:15)(cid:2)(cid:0)(cid:15)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0)(cid:18)(cid:8)(cid:16)(cid:2)(cid:15)(cid:9)(cid:8)(cid:4)(cid:3)(cid:2)(cid:25)(cid:0)

(cid:9)(cid:2)(cid:18)(cid:2)(cid:0):(cid:2)%(cid:24)(cid:4)(cid:0)@(cid:0)%(cid:24)(cid:7)(cid:8)(cid:3)(cid:17)(cid:16)K(cid:0)

:(cid:26)D(cid:27)L(cid:0)(cid:21)(cid:21)(cid:21)(cid:0)
E(cid:26)MX(cid:23)G(cid:1)(cid:26)M(cid:0)E(cid:6)(cid:27)(cid:0)(cid:22)(cid:26)E(cid:21)(cid:26)D(cid:27)L(cid:0)(cid:12)(cid:27)MX(cid:27)LG(cid:26)E(cid:26)M(cid:0)(cid:1)(cid:26)(cid:23)(cid:26)M(cid:26)M(cid:0);#=>(cid:0)(cid:0)

(cid:1)(cid:13)(cid:18)(cid:24)(cid:4)(cid:0) (cid:1)(cid:8)(cid:25)(cid:0) =(cid:10)(cid:16)(cid:0)Y(cid:17)(cid:0) (cid:1)(cid:24)(cid:18)(cid:8)(cid:2)(cid:25)(cid:0) (cid:1)(cid:24)(cid:2)(cid:25)(cid:0) @(cid:7)(cid:18)(cid:0)Y(cid:17)(cid:0) (cid:1)(cid:2)Z(cid:0)

;=>(cid:0)

;?>(cid:0)

;@>(cid:0)

;A>(cid:0)

;B>(cid:0)

;C>(cid:0)

; >(cid:0)

(cid:19)(cid:8)(cid:7)(cid:24)""(cid:16)(cid:0) H(cid:30)HHHHH(cid:0) H(cid:30)HBHC!(cid:0) H(cid:30)H BCB(cid:0) H(cid:30)H !AB(cid:0) H(cid:30)===@ (cid:0) H(cid:30)= FH (cid:0)

(cid:6)(cid:26)(cid:27)(cid:0) H(cid:30)HHHHH(cid:0) H(cid:30)HA?!=(cid:0) H(cid:30)HBCFC(cid:0) H(cid:30)HBBFC(cid:0) H(cid:30)H B?A(cid:0) H(cid:30)=HFHA(cid:0)

(cid:6)(cid:26)(cid:27)(cid:19)D(cid:0) H(cid:30)HHHH (cid:0) H(cid:30)HA?A=(cid:0) H(cid:30)HBCA=(cid:0) H(cid:30)HBBAB(cid:0) H(cid:30)H ABC(cid:0) H(cid:30)=H CH(cid:0)

(cid:1)(cid:6)(cid:26)(cid:27)(cid:0) H(cid:30)HHHHH(cid:0) H(cid:30)HAHFF(cid:0) H(cid:30)HBA C(cid:0) H(cid:30)HB@@?(cid:0) H(cid:30)H HFH(cid:0) H(cid:30)=HA@!(cid:0)

(cid:1)(cid:6)(cid:26)(cid:27)(cid:19)D(cid:0) H(cid:30)HHH==(cid:0) H(cid:30)HAHB@(cid:0) H(cid:30)HBAH (cid:0) H(cid:30)HB?!@(cid:0) H(cid:30)H H=B(cid:0) H(cid:30)=H@@!(cid:0)

(cid:0)

(cid:0)

(cid:1)(cid:2)(cid:3)(cid:2)(cid:4)(cid:2)(cid:5)(cid:0)(cid:6)(cid:3)(cid:7)(cid:8)(cid:9)(cid:10)(cid:8)(cid:0)(cid:11)(cid:0)(cid:12)(cid:7)(cid:13)(cid:14)(cid:7)(cid:2)(cid:15)(cid:0)(cid:6)(cid:16)(cid:17)(cid:18)(cid:8)(cid:0)(cid:19)(cid:20)(cid:21)(cid:22)(cid:0)(cid:23)(cid:13)(cid:15)(cid:9)(cid:17)(cid:16)(cid:2)(cid:10)(cid:8)(cid:0)(cid:6)(cid:16)(cid:2)(cid:16)(cid:8)(cid:10)(cid:16)(cid:8)(cid:3)(cid:0)

(cid:12)(cid:2)(cid:18)(cid:2)(cid:0)(cid:24)(cid:2)(cid:15)(cid:25)(cid:2)(cid:7)(cid:0)(cid:26)(cid:0)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:18)(cid:8)(cid:4)(cid:8)(cid:5)(cid:2)(cid:16)(cid:0)(cid:25)(cid:2)(cid:5)(cid:27)(cid:2)(cid:0)(cid:28)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:18)(cid:28)(cid:29)(cid:14)(cid:2)(cid:29)(cid:0)
(cid:15)(cid:28)(cid:29)(cid:14)(cid:14)(cid:17)(cid:29)(cid:2)(cid:3)(cid:2)(cid:29)(cid:0)(cid:15)(cid:13)(cid:18)(cid:28)(cid:4)(cid:0)(cid:30)(cid:31) !""#$%&’ ((cid:0)(cid:15)(cid:17)(cid:4)(cid:16)(cid:8))(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:15)(cid:28)(cid:15)(cid:8)(cid:4)(cid:8)(cid:3)(cid:8)(cid:0)(cid:29)(cid:8)(cid:4)(cid:2)(cid:8)(cid:0)
(cid:7)(cid:2)(cid:16)(cid:2)(cid:20)(cid:7)(cid:2)(cid:16)(cid:2)(cid:0)*(cid:6)+(cid:0),(cid:2)(cid:29)(cid:14)(cid:0)(cid:4)(cid:28)(cid:25)(cid:8)(cid:5)(cid:0)(cid:3)(cid:28)-(cid:8)(cid:4)(cid:0)(cid:18)(cid:8)(cid:25)(cid:2)(cid:29)(cid:18)(cid:8)(cid:29)(cid:14)(cid:0)(cid:30)(cid:31) !""#$%&’ ((cid:0)
(cid:17)(cid:29)(cid:8))(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:29),(cid:2).(cid:0)(cid:25)(cid:28)(cid:7)(cid:18)(cid:2)(cid:10)(cid:2)(cid:7)(cid:3)(cid:2)(cid:29)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:5)(cid:2)(cid:10)(cid:8)(cid:4)(cid:0)(cid:7)(cid:2)(cid:29)(cid:14)(cid:3)(cid:17)(cid:15)(cid:2)(cid:29)(cid:0)*(cid:6)+(cid:0)(cid:17)(cid:29)(cid:16)(cid:17)(cid:3)(cid:0)
(cid:28)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0))(cid:2)(cid:7)(cid:8)(cid:2)(cid:25)(cid:28)(cid:4)(cid:0)(cid:9)(cid:28)(cid:29)(cid:14)(cid:28)(cid:4)(cid:17)(cid:2)(cid:7)(cid:2)(cid:29)(cid:0)(cid:15)(cid:2)(cid:3)(cid:2)(cid:29)(cid:2)(cid:29)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)/(cid:2)(cid:25)(cid:28)(cid:4)(cid:0)0.(cid:0)(cid:15)(cid:13)(cid:18)(cid:28)(cid:4)(cid:0)
(cid:25)(cid:28)(cid:29)-(cid:5)(cid:15)(cid:2)(cid:7)(cid:3)(cid:8)(cid:29)(cid:14)(cid:0)(cid:15)(cid:17)(cid:4)(cid:8)(cid:16))(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:15)(cid:28)(cid:15)(cid:8)(cid:4)(cid:8)(cid:3)(cid:8)(cid:0)(cid:7)(cid:2)(cid:16)(cid:2)(cid:20)(cid:7)(cid:2)(cid:16)(cid:2)(cid:0)*(cid:6)+(cid:0)(cid:10)(cid:28)(cid:25)(cid:28)(cid:10)(cid:2)(cid:7)(cid:0)
1213450.(cid:0)(cid:29)(cid:8)(cid:4)(cid:2)(cid:8)(cid:0)(cid:8)(cid:29)(cid:8)(cid:0)(cid:4)(cid:28)(cid:25)(cid:8)(cid:5)(cid:0)(cid:3)(cid:28)-(cid:8)(cid:4)(cid:0)(cid:18)(cid:2)(cid:7)(cid:8)(cid:0)(cid:28)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:4)(cid:2)(cid:29)(cid:14)(cid:10)(cid:17)(cid:29)(cid:14)(cid:0)(cid:18)(cid:28)(cid:29)(cid:14)(cid:2)(cid:29)(cid:0)
(cid:7)(cid:2)(cid:16)(cid:2)(cid:20)(cid:7)(cid:2)(cid:16)(cid:2)(cid:0)*(cid:6)+(cid:0)(cid:10)(cid:28)(cid:25)(cid:28)(cid:10)(cid:2)(cid:7)(cid:0)12165(cid:26)3(cid:0)(cid:10)(cid:28)(cid:7)(cid:16)(cid:2)(cid:0)(cid:4)(cid:28)(cid:25)(cid:8)(cid:5)(cid:0)(cid:3)(cid:28)-(cid:8)(cid:4)(cid:0)(cid:18)(cid:2)(cid:7)(cid:8)(cid:0)(cid:15)(cid:13)(cid:18)(cid:28)(cid:4)(cid:0)
(cid:30)(cid:31) !""#$%&’ ((cid:0)(cid:17)(cid:29)(cid:8))(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:29),(cid:2)(cid:0)12133(cid:26)32(cid:0)(cid:0)

(cid:0)
(cid:6)(cid:28)(cid:4)(cid:2)(cid:29)7(cid:17)(cid:16)(cid:29),(cid:2)(cid:0)(cid:5)(cid:2)(cid:10)(cid:8)(cid:4)(cid:0)(cid:2)(cid:14)(cid:7)(cid:28)(cid:14)(cid:2)(cid:10)(cid:8)(cid:0)(cid:17)(cid:29)(cid:16)(cid:17)(cid:3)(cid:0)(cid:3)(cid:28)(cid:4)(cid:8)(cid:15)(cid:2)(cid:0)(cid:15)(cid:13)(cid:18)(cid:28)(cid:4)(cid:0)

(cid:18)(cid:8)(cid:16)(cid:2)(cid:15)(cid:9)(cid:8)(cid:4)(cid:3)(cid:2)(cid:29)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)/(cid:2)(cid:25)(cid:28)(cid:4)(cid:0)(cid:26)(cid:0)(cid:25)(cid:28)(cid:7)(cid:8)(cid:3)(cid:17)(cid:16)8(cid:0)

(cid:0)

/9:+;(cid:0)(cid:21)(cid:22)(cid:0)
<9(cid:6)(cid:21);(cid:0)9(cid:24)*+(cid:24)9(cid:6)(cid:21)(cid:0)(cid:22)9*(cid:21)9:+;(cid:0)(cid:12)+=(cid:24)+;>9*9=(cid:0)(cid:1)9(cid:23)9=9=(cid:0)?@AB(cid:0)(cid:0)

(cid:1)(cid:13)(cid:18)(cid:28)(cid:4)(cid:0)
?AB(cid:0)

+(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:4)(cid:2)(cid:29)(cid:14)(cid:10)(cid:17)(cid:29)(cid:14)(cid:0)
(cid:6)9+(cid:19):(cid:0)
(cid:1)(cid:6)9+(cid:19):(cid:0)
(cid:6)9+(cid:0)
(cid:1)(cid:6)9+(cid:0)

9(cid:14)(cid:14)(cid:7)(cid:28)(cid:14)(cid:2)(cid:10)(cid:8)(cid:0)
?4B(cid:0)
*(cid:9)2(cid:0)30AC1424(cid:0)
*(cid:9)2(cid:0)30AC1424(cid:0)
*(cid:9)2(cid:0)30AC1424(cid:0)
*(cid:9)2(cid:0)34D05320(cid:0)
*(cid:9)2(cid:0)3406(cid:26)623(cid:0)

(cid:0)

(cid:19)(cid:2)(cid:7)(cid:8)(cid:0)(cid:16)(cid:2)(cid:25)(cid:28)(cid:4)(cid:0)(cid:26)(cid:0)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:18)(cid:8)(cid:4)(cid:8)(cid:5)(cid:2)(cid:16)(cid:0)(cid:25)(cid:2)(cid:5)(cid:27)(cid:2)(cid:0)(cid:15)(cid:28)(cid:16)(cid:13)(cid:18)(cid:28)(cid:0)E’FF(cid:31)%(cid:31) !(cid:31)G
(cid:30)(cid:31) !""#$%&’ ((cid:0)(cid:25)(cid:2)(cid:8)(cid:3)(cid:0)(cid:10)(cid:28)-(cid:2)(cid:7)(cid:2)(cid:0)(cid:17)(cid:29)(cid:8))(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:15)(cid:2)(cid:17)(cid:9)(cid:17)(cid:29)(cid:0)(cid:15)(cid:17)(cid:4)(cid:16)(cid:8))(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:17)(cid:29)(cid:16)(cid:17)(cid:3)(cid:0)
)(cid:2)(cid:7)(cid:8)(cid:2)(cid:25)(cid:28)(cid:4)(cid:0)(cid:7)(cid:2)(cid:16)(cid:2)(cid:20)(cid:7)(cid:2)(cid:16)(cid:2)(cid:0)(cid:9)(cid:28)(cid:29)(cid:14)(cid:28)(cid:4)(cid:17)(cid:2)(cid:7)(cid:2)(cid:29)(cid:0)(cid:15)(cid:2)(cid:3)(cid:2)(cid:29)(cid:2)(cid:29)(cid:0)(cid:16)(cid:28)(cid:4)(cid:2)(cid:5)(cid:0)(cid:15)(cid:28)(cid:15)(cid:25)(cid:28)(cid:7)(cid:8)(cid:3)(cid:2)(cid:29)(cid:0)
(cid:2)(cid:14)(cid:7)(cid:28)(cid:14)(cid:2)(cid:10)(cid:8)(cid:0),(cid:2)(cid:29)(cid:14)(cid:0)(cid:3)(cid:13)(cid:29)(cid:10)(cid:8)(cid:10)(cid:16)(cid:28)(cid:29)(cid:0)(cid:18)(cid:28)(cid:29)(cid:14)(cid:2)(cid:29)(cid:0)(cid:2)(cid:14)(cid:7)(cid:28)(cid:14)(cid:2)(cid:10)(cid:8)(cid:0)(cid:28)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:4)(cid:2)(cid:29)(cid:14)(cid:10)(cid:17)(cid:29)(cid:14)(cid:0)
(cid:29),(cid:2).(cid:0)(cid:10)(cid:28)(cid:18)(cid:2)(cid:29)(cid:14)(cid:3)(cid:2)(cid:29)(cid:0)(cid:15)(cid:13)(cid:18)(cid:28)(cid:4)(cid:0),(cid:2)(cid:29)(cid:14)(cid:0)(cid:16)(cid:8)(cid:18)(cid:2)(cid:3)(cid:0)(cid:15)(cid:28)(cid:29)(cid:14)(cid:14)(cid:17)(cid:29)(cid:2)(cid:3)(cid:2)(cid:29)(cid:0)(cid:15)(cid:28)(cid:16)(cid:13)(cid:18)(cid:28)(cid:0)
(cid:30)(cid:31) !""#$%&’ ((cid:0)(cid:2)(cid:14)(cid:14)(cid:7)(cid:28)(cid:14)(cid:2)(cid:10)(cid:8)(cid:0)(cid:29),(cid:2)(cid:0)(cid:16)(cid:8)(cid:18)(cid:2)(cid:3)(cid:0)(cid:3)(cid:13)(cid:29)(cid:10)(cid:8)(cid:10)(cid:16)(cid:28)(cid:29)(cid:0)(cid:18)(cid:28)(cid:29)(cid:14)(cid:2)(cid:29)(cid:0)(cid:2)(cid:14)(cid:14)(cid:7)(cid:28)(cid:14)(cid:2)(cid:10)(cid:8)(cid:0)
(cid:28)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:4)(cid:2)(cid:29)(cid:14)(cid:10)(cid:17)(cid:29)(cid:14)(cid:0)(cid:29),(cid:2)2(cid:0)

(cid:0)

(cid:25)BH <(cid:2)(cid:10)(cid:8)(cid:4)(cid:0)+(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:18)(cid:2)(cid:29)(cid:0)*(cid:6)+(cid:0))(cid:2)(cid:7)(cid:8)(cid:2)(cid:25)(cid:28)(cid:4)(cid:0)(cid:9)(cid:28)(cid:29)(cid:14)(cid:28)(cid:4)(cid:17)(cid:2)(cid:7)(cid:2)(cid:29)(cid:0)(cid:29)(cid:13)(cid:29)(cid:0)

(cid:15)(cid:2)(cid:3)(cid:2)(cid:29)(cid:2)(cid:29)(cid:0)?@4B(cid:0)
(cid:0)
<(cid:2)(cid:10)(cid:8)(cid:4)(cid:0)(cid:28)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0))(cid:2)(cid:7)(cid:8)(cid:2)(cid:25)(cid:28)(cid:4)(cid:0)(cid:9)(cid:28)(cid:29)(cid:14)(cid:28)(cid:4)(cid:17)(cid:2)(cid:7)(cid:2)(cid:29)(cid:0)(cid:29)(cid:13)(cid:29)(cid:0)(cid:15)(cid:2)(cid:3)(cid:2)(cid:29)(cid:2)(cid:29)(cid:0)
(cid:15)(cid:28)(cid:29)(cid:14)(cid:14)(cid:17)(cid:29)(cid:2)(cid:3)(cid:2)(cid:29)(cid:0)(cid:15)(cid:28)(cid:16)(cid:13)(cid:18)(cid:28)(cid:0)E’FF(cid:31)%(cid:31) !(cid:31)G(cid:30)(cid:31) !""#$%&’ ((cid:0)#IE(cid:31)JGK$LM
N(cid:31)%%’IO(cid:0)(cid:15)(cid:17)(cid:4)(cid:16)(cid:8))(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:18)(cid:8)(cid:0)(cid:16)(cid:2)(cid:15)(cid:9)(cid:8)(cid:4)(cid:3)(cid:2)(cid:29)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:14)(cid:2)(cid:15)(cid:25)(cid:2)(cid:7)(cid:0)3(cid:0)(cid:25)(cid:28)(cid:7)(cid:8)(cid:3)(cid:17)(cid:16)8(cid:0)

(cid:25)(cid:28)(cid:14)(cid:8)(cid:16)(cid:17)(cid:0)(cid:9)(cid:17)(cid:4)(cid:2)(cid:0)(cid:10)(cid:28)(cid:25)(cid:2)(cid:4)(cid:8)(cid:3)(cid:29),(cid:2)(cid:0)(cid:2)(cid:7)(cid:28)(cid:2)(cid:0)(cid:18)(cid:28)(cid:29)(cid:14)(cid:2)(cid:29)(cid:0)(cid:29)(cid:8)(cid:4)(cid:2)(cid:8)(cid:0)(cid:7)(cid:2)(cid:16)(cid:2)(cid:20)(cid:7)(cid:2)(cid:16)(cid:2)(cid:0)(cid:9)(cid:28)(cid:29)(cid:14)(cid:28)(cid:4)(cid:17)(cid:2)(cid:7)(cid:2)(cid:29)(cid:0)
(cid:29)(cid:13)(cid:29)(cid:0)(cid:15)(cid:2)(cid:3)(cid:2)(cid:29)(cid:0),(cid:2)(cid:29)(cid:14)(cid:0)(cid:7)(cid:28)(cid:29)(cid:18)(cid:2)(cid:5)(cid:0)(cid:18)(cid:8)(cid:0)(cid:10)(cid:2)(cid:16)(cid:17)(cid:0)(cid:15)(cid:13)(cid:18)(cid:28)(cid:4)(cid:0)(cid:2)(cid:3)(cid:2)(cid:29)(cid:0)(cid:15)(cid:28)(cid:15)(cid:8)(cid:4)(cid:8)(cid:3)(cid:8)(cid:0)(cid:29)(cid:8)(cid:4)(cid:2)(cid:8)(cid:0)
,(cid:2)(cid:29)(cid:14)(cid:0)(cid:7)(cid:28)(cid:29)(cid:18)(cid:2)(cid:5)(cid:0)(cid:9)(cid:17)(cid:4)(cid:2)(cid:0)(cid:18)(cid:8)(cid:0)(cid:15)(cid:13)(cid:18)(cid:28)(cid:4)(cid:0)(cid:4)(cid:2)(cid:8)(cid:29)(cid:29),(cid:2)2(cid:0)(cid:0)

(cid:0)
(cid:6)(cid:28)(cid:4)(cid:2)(cid:29)7(cid:17)(cid:16)(cid:29),(cid:2)(cid:0)(cid:5)(cid:2)(cid:10)(cid:8)(cid:4)(cid:0)(cid:28)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)*(cid:6)+(cid:0)(cid:18)(cid:8)(cid:16)(cid:2)(cid:15)(cid:9)(cid:8)(cid:4)(cid:3)(cid:2)(cid:29)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)

(cid:24)(cid:2)(cid:15)(cid:25)(cid:2)(cid:7)(cid:0)D(cid:0)(cid:25)(cid:28)(cid:7)(cid:8)(cid:3)(cid:17)(cid:16)8(cid:0)

(cid:24)(cid:2)(cid:15)(cid:25)(cid:2)(cid:7)(cid:0)D2(cid:0)<(cid:2)(cid:10)(cid:8)(cid:4)(cid:0)+(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)*(cid:6)+(cid:0))(cid:2)(cid:7)(cid:8)(cid:2)(cid:25)(cid:28)(cid:4)(cid:0)(cid:9)(cid:28)(cid:29)(cid:14)(cid:28)(cid:4)(cid:17)(cid:2)(cid:7)(cid:2)(cid:29)(cid:0)(cid:29)(cid:13)(cid:29)(cid:0)
(cid:15)(cid:2)(cid:3)(cid:2)(cid:29)(cid:2)(cid:29)(cid:0)
(cid:0)

(cid:0)

*(cid:2)(cid:29)(cid:14)(cid:3)(cid:17)(cid:15)(cid:2)(cid:29)(cid:0)(cid:29)(cid:8)(cid:4)(cid:2)(cid:8)(cid:0)*(cid:6)+(cid:0)(cid:18)(cid:2)(cid:7)(cid:8)(cid:0)(cid:3)(cid:28)(cid:4)(cid:8)(cid:15)(cid:2)(cid:0)(cid:15)(cid:13)(cid:18)(cid:28)(cid:4)(cid:0)(cid:18)(cid:8)(cid:16)(cid:2)(cid:15)(cid:9)(cid:8)(cid:4)(cid:3)(cid:2)(cid:29)(cid:0)

(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)/(cid:2)(cid:25)(cid:28)(cid:4)(cid:0)3(cid:0)(cid:25)(cid:28)(cid:7)(cid:8)(cid:3)(cid:17)(cid:16)8(cid:0)

/9:+;(cid:0)(cid:22)(cid:0)
*9=(cid:24)(cid:23)>(cid:1)9=(cid:0)*(cid:6)+(cid:0)(cid:22)9*(cid:21)9:+;(cid:0)(cid:12)+=(cid:24)+;>9*9=(cid:0)=P=(cid:0)(cid:1)9(cid:23)9=9=(cid:0)?@4B(cid:0)(cid:0)

(cid:1)(cid:13)(cid:18)(cid:28)(cid:4)(cid:0) (cid:1)(cid:8)(cid:29)(cid:0) A(cid:10)(cid:16)(cid:0)Q(cid:17)(cid:0) (cid:1)(cid:28)(cid:18)(cid:8)(cid:2)(cid:29)(cid:0) (cid:1)(cid:28)(cid:2)(cid:29)(cid:0) 0(cid:7)(cid:18)(cid:0)Q(cid:17)(cid:0) (cid:1)(cid:2)R(cid:0)

?AB(cid:0)

?4B(cid:0)

?0B(cid:0)

?(cid:26)B(cid:0)

?3B(cid:0)

?DB(cid:0)

?6B(cid:0)

(cid:19)(cid:8)(cid:7)(cid:28)-(cid:16)(cid:0) 1211111(cid:0) 121C3C6(cid:0) 12A0C51(cid:0) 12ADA11(cid:0) 124A0D5(cid:0) 12(cid:26)63A3(cid:0)

(cid:6)9+(cid:0) 1211111(cid:0) 1215(cid:26)C(cid:26)(cid:0) 12A4004(cid:0) 12A01DD(cid:0) 12A6(cid:26)14(cid:0) 124C(cid:26)5(cid:26)(cid:0)

(cid:6)9+(cid:19):(cid:0) 12111A0(cid:0) 1215ACC(cid:0) 12AA6A3(cid:0) 12A4D04(cid:0) 12AD6C5(cid:0) 12455AD(cid:0)

(cid:1)(cid:6)9+(cid:0) 1211111(cid:0) 1215145(cid:0) 12AA431(cid:0) 12A4AC(cid:26)(cid:0) 12AD1A3(cid:0) 1246DDC(cid:0)

(cid:1)(cid:6)9+(cid:19):(cid:0) 121114(cid:26)(cid:0) 1216C6C(cid:0) 12A156(cid:26)(cid:0) 12AA5DC(cid:0) 12A3331(cid:0) 124DC64(cid:0)

(cid:0)
(cid:12)(cid:2)(cid:18)(cid:2)(cid:0)(cid:24)(cid:2)(cid:15)(cid:25)(cid:2)(cid:7)(cid:0)D2(cid:0)(cid:19)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:18)(cid:8)(cid:4)(cid:8)(cid:5)(cid:2)(cid:16)(cid:0)(cid:25)(cid:2)(cid:5)(cid:27)(cid:2)(cid:0)(cid:28)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:18)(cid:28)(cid:29)(cid:14)(cid:2)(cid:29)(cid:0)
(cid:15)(cid:28)(cid:29)(cid:14)(cid:14)(cid:17)(cid:29)(cid:2)(cid:3)(cid:2)(cid:29)(cid:0)(cid:15)(cid:13)(cid:18)(cid:28)(cid:4)(cid:0)(cid:30)(cid:31) !""#$%&’ ((cid:0)(cid:15)(cid:17)(cid:4)(cid:16)(cid:8))(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:15)(cid:28)(cid:15)(cid:8)(cid:4)(cid:8)(cid:3)(cid:8)(cid:0)(cid:29)(cid:8)(cid:4)(cid:2)(cid:8)(cid:0)
(cid:7)(cid:2)(cid:16)(cid:2)(cid:20)(cid:7)(cid:2)(cid:16)(cid:2)(cid:0)*(cid:6)+(cid:0),(cid:2)(cid:29)(cid:14)(cid:0)(cid:4)(cid:28)(cid:25)(cid:8)(cid:5)(cid:0)(cid:3)(cid:28)-(cid:8)(cid:4)(cid:0)(cid:18)(cid:8)(cid:25)(cid:2)(cid:29)(cid:18)(cid:8)(cid:29)(cid:14)(cid:0)(cid:30)(cid:31) !""#$%&’ ((cid:0)
(cid:17)(cid:29)(cid:8))(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:29),(cid:2).(cid:0)(cid:25)(cid:28)(cid:7)(cid:18)(cid:2)(cid:10)(cid:2)(cid:7)(cid:3)(cid:2)(cid:29)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:5)(cid:2)(cid:10)(cid:8)(cid:4)(cid:0)(cid:7)(cid:2)(cid:29)(cid:14)(cid:3)(cid:17)(cid:15)(cid:2)(cid:29)(cid:0)*(cid:6)+(cid:0)(cid:17)(cid:29)(cid:16)(cid:17)(cid:3)(cid:0)
(cid:28)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0))(cid:2)(cid:7)(cid:8)(cid:2)(cid:25)(cid:28)(cid:4)(cid:0)(cid:9)(cid:28)(cid:29)(cid:14)(cid:28)(cid:4)(cid:17)(cid:2)(cid:7)(cid:2)(cid:29)(cid:0)(cid:29)(cid:13)(cid:29)(cid:0)(cid:15)(cid:2)(cid:3)(cid:2)(cid:29)(cid:2)(cid:29)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:16)(cid:2)(cid:25)(cid:28)(cid:4)(cid:0)3.(cid:0)
(cid:15)(cid:13)(cid:18)(cid:28)(cid:4)(cid:0)(cid:30)(cid:31) !""#$%&’ ((cid:0)(cid:15)(cid:17)(cid:4)(cid:8)(cid:16))(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:15)(cid:28)(cid:15)(cid:8)(cid:4)(cid:8)(cid:3)(cid:8)(cid:0)(cid:7)(cid:2)(cid:16)(cid:2)(cid:20)(cid:7)(cid:2)(cid:16)(cid:2)(cid:0)*(cid:6)+(cid:0)
(cid:10)(cid:28)(cid:25)(cid:28)(cid:10)(cid:2)(cid:7)(cid:0)12AA5DC.(cid:0)(cid:29)(cid:8)(cid:4)(cid:2)(cid:8)(cid:0)(cid:8)(cid:29)(cid:8)(cid:0)(cid:4)(cid:28)(cid:25)(cid:8)(cid:5)(cid:0)(cid:3)(cid:28)-(cid:8)(cid:4)(cid:0)(cid:18)(cid:2)(cid:7)(cid:8)(cid:0)(cid:28)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:4)(cid:2)(cid:29)(cid:14)(cid:10)(cid:17)(cid:29)(cid:14)(cid:0)
(cid:18)(cid:28)(cid:29)(cid:14)(cid:2)(cid:29)(cid:0)(cid:7)(cid:2)(cid:16)(cid:2)(cid:20)(cid:7)(cid:2)(cid:16)(cid:2)(cid:0)*(cid:6)+(cid:0)(cid:10)(cid:28)(cid:25)(cid:28)(cid:10)(cid:2)(cid:7)(cid:0)12ADA11(cid:0)(cid:10)(cid:28)(cid:7)(cid:16)(cid:2)(cid:0)(cid:4)(cid:28)(cid:25)(cid:8)(cid:5)(cid:0)(cid:3)(cid:28)-(cid:8)(cid:4)(cid:0)(cid:18)(cid:2)(cid:7)(cid:8)(cid:0)
(cid:15)(cid:13)(cid:18)(cid:28)(cid:4)(cid:0)(cid:30)(cid:31) !""#$%&’ ((cid:0)(cid:17)(cid:29)(cid:8))(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:29),(cid:2)(cid:0)12A4D042(cid:0)

(cid:0)
(cid:6)(cid:28)(cid:4)(cid:2)(cid:29)7(cid:17)(cid:16)(cid:29),(cid:2)(cid:0)(cid:5)(cid:2)(cid:10)(cid:8)(cid:4)(cid:0)(cid:2)(cid:14)(cid:7)(cid:28)(cid:14)(cid:2)(cid:10)(cid:8)(cid:0)(cid:17)(cid:29)(cid:16)(cid:17)(cid:3)(cid:0)(cid:3)(cid:28)(cid:4)(cid:8)(cid:15)(cid:2)(cid:0)(cid:15)(cid:13)(cid:18)(cid:28)(cid:4)(cid:0)

(cid:18)(cid:8)(cid:16)(cid:2)(cid:15)(cid:9)(cid:8)(cid:4)(cid:3)(cid:2)(cid:29)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)/(cid:2)(cid:25)(cid:28)(cid:4)(cid:0)D(cid:0)(cid:25)(cid:28)(cid:7)(cid:8)(cid:3)(cid:17)(cid:16)8(cid:0)

(cid:0)

/9:+;(cid:0)(cid:22)(cid:21)(cid:0)
<9(cid:6)(cid:21);(cid:0)9(cid:24)*+(cid:24)9(cid:6)(cid:21)(cid:0)(cid:22)9*(cid:21)9:+;(cid:0)(cid:12)+=(cid:24)+;>9*9=(cid:0)=P=(cid:0)(cid:1)9(cid:23)9=9=(cid:0)?@4B(cid:0)(cid:0)

(cid:24)(cid:2)(cid:15)(cid:25)(cid:2)(cid:7)(cid:0)32(cid:0)+(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:12)(cid:28)(cid:29)(cid:14)(cid:28)(cid:4)(cid:17)(cid:2)(cid:7)(cid:2)(cid:29)(cid:0)(cid:29)(cid:13)(cid:29)(cid:0)(cid:15)(cid:2)(cid:3)(cid:2)(cid:29)(cid:2)(cid:29)(cid:0)?@4B(cid:0)

(cid:0)

(cid:0)

(cid:12)(cid:2)(cid:18)(cid:2)(cid:0)(cid:24)(cid:2)(cid:15)(cid:25)(cid:2)(cid:7)(cid:0)3(cid:0)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:18)(cid:8)(cid:4)(cid:8)(cid:5)(cid:2)(cid:16)(cid:0)(cid:25)(cid:2)(cid:5)(cid:27)(cid:2)(cid:0)(cid:5)(cid:2)(cid:10)(cid:8)(cid:4)(cid:0)(cid:28)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)
(cid:3)(cid:28)(cid:4)(cid:8)(cid:15)(cid:2)(cid:0)(cid:15)(cid:13)(cid:18)(cid:28)(cid:4)(cid:0)(cid:10)(cid:17)(cid:18)(cid:2)(cid:5)(cid:0)(cid:15)(cid:28)(cid:15)(cid:25)(cid:28)(cid:7)(cid:8)(cid:3)(cid:2)(cid:29)(cid:0)(cid:9)(cid:13)(cid:4)(cid:2)(cid:0),(cid:2)(cid:29)(cid:14)(cid:0)(cid:10)(cid:2)(cid:15)(cid:2).(cid:0)(cid:2)(cid:7)(cid:28)(cid:2)(cid:0)(cid:18)(cid:28)(cid:29)(cid:14)(cid:2)(cid:29)(cid:0)
(cid:29)(cid:8)(cid:4)(cid:2)(cid:8)(cid:0)(cid:7)(cid:2)(cid:16)(cid:2)(cid:20)(cid:7)(cid:2)(cid:16)(cid:2)(cid:0)(cid:9)(cid:28)(cid:29)(cid:14)(cid:28)(cid:4)(cid:17)(cid:2)(cid:7)(cid:2)(cid:29)(cid:0)(cid:29)(cid:13)(cid:29)(cid:0)(cid:15)(cid:2)(cid:3)(cid:2)(cid:29)(cid:0),(cid:2)(cid:29)(cid:14)(cid:0)(cid:16)(cid:8)(cid:29)(cid:14)(cid:14)(cid:8)(cid:0)(cid:18)(cid:8)(cid:0)(cid:10)(cid:2)(cid:16)(cid:17)(cid:0)
(cid:15)(cid:13)(cid:18)(cid:28)(cid:4)(cid:0)(cid:2)(cid:3)(cid:2)(cid:29)(cid:0)(cid:15)(cid:28)(cid:15)(cid:8)(cid:4)(cid:8)(cid:3)(cid:8)(cid:0)(cid:29)(cid:8)(cid:4)(cid:2)(cid:8)(cid:0),(cid:2)(cid:29)(cid:14)(cid:0)(cid:16)(cid:8)(cid:29)(cid:14)(cid:14)(cid:8)(cid:0)(cid:9)(cid:17)(cid:4)(cid:2)(cid:0)(cid:18)(cid:8)(cid:0)(cid:15)(cid:13)(cid:18)(cid:28)(cid:4)(cid:0)(cid:4)(cid:2)(cid:8)(cid:29)(cid:29),(cid:2).(cid:0)

(cid:1)(cid:13)(cid:18)(cid:28)(cid:4)(cid:0)
?AB(cid:0)

+(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:4)(cid:2)(cid:29)(cid:14)(cid:10)(cid:17)(cid:29)(cid:14)(cid:0)
(cid:6)9+(cid:19):(cid:0)
(cid:1)(cid:6)9+(cid:19):(cid:0)
(cid:6)9+(cid:0)
(cid:1)(cid:6)9+(cid:0)

9(cid:14)(cid:14)(cid:7)(cid:28)(cid:14)(cid:2)(cid:10)(cid:8)(cid:0)
?4B(cid:0)
*(cid:9)2(cid:0)6062511.D(cid:0)
*(cid:9)2(cid:0)6062511.D(cid:0)
*(cid:9)2(cid:0)6062511.D(cid:0)
*(cid:9)2(cid:0)6A52(cid:26)65.4(cid:0)
*(cid:9)2(cid:0)6A(cid:26)2C63.6(cid:0)

(cid:0)

(cid:0)

(cid:1)(cid:2)(cid:3)(cid:2)(cid:4)(cid:2)(cid:5)(cid:0)(cid:6)(cid:3)(cid:7)(cid:8)(cid:9)(cid:10)(cid:8)(cid:0)(cid:11)(cid:0)(cid:12)(cid:7)(cid:13)(cid:14)(cid:7)(cid:2)(cid:15)(cid:0)(cid:6)(cid:16)(cid:17)(cid:18)(cid:8)(cid:0)(cid:19)(cid:20)(cid:21)(cid:22)(cid:0)(cid:23)(cid:13)(cid:15)(cid:9)(cid:17)(cid:16)(cid:2)(cid:10)(cid:8)(cid:0)(cid:6)(cid:16)(cid:2)(cid:16)(cid:8)(cid:10)(cid:16)(cid:8)(cid:3)(cid:0)

(cid:19)(cid:2)(cid:7)(cid:8)(cid:0)(cid:24)(cid:2)(cid:25)(cid:26)(cid:4)(cid:0)(cid:27)(cid:0)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:18)(cid:8)(cid:4)(cid:8)(cid:5)(cid:2)(cid:16)(cid:0)(cid:25)(cid:2)(cid:5)(cid:28)(cid:2)(cid:0)(cid:15)(cid:26)(cid:16)(cid:13)(cid:18)(cid:26)(cid:0)(cid:29)(cid:30)(cid:31)(cid:31) ! ""# $
% ""#&’(!)(cid:30)""*$(cid:25)(cid:2)(cid:8)(cid:3)(cid:0)(cid:10)(cid:26)+(cid:2)(cid:7)(cid:2)(cid:0)(cid:17),(cid:8)-(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:15)(cid:2)(cid:17)(cid:9)(cid:17),(cid:0)(cid:15)(cid:17)(cid:4)(cid:16)(cid:8)-(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:16)(cid:26)(cid:4)(cid:2)(cid:5)(cid:0)
(cid:15)(cid:26)(cid:15)(cid:25)(cid:26)(cid:7)(cid:8)(cid:3)(cid:2),(cid:0)(cid:2)(cid:14)(cid:7)(cid:26)(cid:14)(cid:2)(cid:10)(cid:8)(cid:0).(cid:2),(cid:14)(cid:0)(cid:3)(cid:13),(cid:10)(cid:8)(cid:10)(cid:16)(cid:26),(cid:0)(cid:18)(cid:26),(cid:14)(cid:2),(cid:0)(cid:2)(cid:14)(cid:7)(cid:26)(cid:14)(cid:2)(cid:10)(cid:8)(cid:0)(cid:26)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)
(cid:4)(cid:2),(cid:14)(cid:10)(cid:17),(cid:14)(cid:0),.(cid:2)/(cid:0)(cid:10)(cid:26)(cid:18)(cid:2),(cid:14)(cid:3)(cid:2),(cid:0)(cid:15)(cid:13)(cid:18)(cid:26)(cid:4)(cid:0).(cid:2),(cid:14)(cid:0)(cid:16)(cid:8)(cid:18)(cid:2)(cid:3)(cid:0)(cid:15)(cid:26),(cid:14)(cid:14)(cid:17),(cid:2)(cid:3)(cid:2),(cid:0)
(cid:15)(cid:26)(cid:16)(cid:13)(cid:18)(cid:26)(cid:0)% ""#&’(!)(cid:30)""*(cid:0)(cid:2)(cid:14)(cid:14)(cid:7)(cid:26)(cid:14)(cid:2)(cid:10)(cid:8)(cid:0),.(cid:2)(cid:0)(cid:16)(cid:8)(cid:18)(cid:2)(cid:3)(cid:0)(cid:3)(cid:13),(cid:10)(cid:8)(cid:10)(cid:16)(cid:26),(cid:0)(cid:18)(cid:26),(cid:14)(cid:2),(cid:0)
(cid:2)(cid:14)(cid:14)(cid:7)(cid:26)(cid:14)(cid:2)(cid:10)(cid:8)(cid:0)(cid:26)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:4)(cid:2),(cid:14)(cid:10)(cid:17),(cid:14)(cid:0),.(cid:2)0(cid:0)

(cid:0)

(cid:0)

5061(cid:23)(cid:26)(cid:10)(cid:8)(cid:15)(cid:9)(cid:17)(cid:4)(cid:2),(cid:0)

(cid:0)

(cid:22)(cid:21)(cid:21)01 (cid:12)234(cid:24)4(cid:12)(cid:0)

671 (cid:0)(cid:1)(cid:13)(cid:18)(cid:26)(cid:4)(cid:0)(cid:17),(cid:16)(cid:17)(cid:3)(cid:0)(cid:15)(cid:26),(cid:14)(cid:8)(cid:15)(cid:9)(cid:4)(cid:26)(cid:15)(cid:26),(cid:16)(cid:2)(cid:10)(cid:8)(cid:3)(cid:2),(cid:0)(cid:15)(cid:26)(cid:16)(cid:13)(cid:18)(cid:26)(cid:0)8(cid:30)(cid:31)(cid:31) ! ""# $
9 ""#&’(!)(cid:30)""*$(cid:15)(cid:13)(cid:18)(cid:26)(cid:4)(cid:0):(cid:2).(cid:20);(cid:26)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)(cid:1)(cid:17)(cid:4)(cid:16)(cid:8)-(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:25)(cid:26)(cid:7)(cid:5)(cid:2)(cid:10)(cid:8)(cid:4)(cid:0)
(cid:18)(cid:8)(cid:25)(cid:2),(cid:14)(cid:17),0$
$
<71 =(#)(* $ >$ .(cid:2),(cid:14)(cid:0) (cid:15)(cid:26),(cid:14)(cid:8)(cid:15)(cid:9)(cid:4)(cid:26)(cid:15)(cid:26),(cid:16)(cid:2)(cid:10)(cid:8)(cid:3)(cid:2),(cid:0) (cid:15)(cid:26)(cid:16)(cid:13)(cid:18)(cid:26)(cid:0)
8(cid:30)(cid:31)(cid:31) ! ""# $9 ""#&’(!)(cid:30)""*(cid:0)(cid:15)(cid:13)(cid:18)(cid:26)(cid:4)(cid:0):(cid:2).(cid:20);(cid:26)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)(cid:1)(cid:17)(cid:4)(cid:16)(cid:8)-(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)
(cid:25)(cid:26)(cid:7)(cid:5)(cid:2)(cid:10)(cid:8)(cid:4)(cid:0) (cid:18)(cid:8)(cid:25)(cid:2),(cid:14)(cid:17),(cid:0) (cid:18)(cid:2),(cid:0) (cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0) (cid:18)(cid:8)(cid:2)(cid:3)(cid:10)(cid:26)(cid:10)(cid:0) (cid:9)(cid:2)(cid:18)(cid:2)(cid:0) (cid:4)(cid:2)(cid:15)(cid:2),(cid:0)
(cid:5)(cid:16)(cid:16)(cid:9)?@@ABC30B(cid:20)(cid:9)(cid:7)(cid:13)D(cid:26)+(cid:16)0(cid:13)(cid:7)(cid:14)@(cid:9)(cid:2)+(cid:3)(cid:2)(cid:14)(cid:26)E(cid:15)(cid:10)(cid:2)(cid:26)(cid:19)F0(cid:0) F(cid:26)(cid:7)(cid:18)(cid:2)(cid:10)(cid:2)(cid:7)(cid:3)(cid:2),(cid:0)
(cid:17)D(cid:8)(cid:0)GH(%(cid:30)I(cid:30)JK$.(cid:2),(cid:14)(cid:0)(cid:18)(cid:8)(cid:4)(cid:2)(cid:3)(cid:17)(cid:3)(cid:2),/(cid:0)L(#)(* $B(cid:0).(cid:2),(cid:14)(cid:0)(cid:18)(cid:8)(cid:25)(cid:2),(cid:14)(cid:17),(cid:0)(cid:10)(cid:17)(cid:18)(cid:2)(cid:5)(cid:0)
(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:18)(cid:8)(cid:16)(cid:26)(cid:7)(cid:8)(cid:15)(cid:2)(cid:0)(cid:18)(cid:26),(cid:14)(cid:2),(cid:0)(cid:25)(cid:2)(cid:8)(cid:3)(cid:0)(cid:13)(cid:4)(cid:26)(cid:5)(cid:0)(cid:9)(cid:26),(cid:14)(cid:14)(cid:17),(cid:2)0$
$
M71 F(cid:26)(cid:7)(cid:18)(cid:2)(cid:10)(cid:2)(cid:7)(cid:3)(cid:2),(cid:0)(cid:17)D(cid:8)(cid:0)-(cid:2)(cid:4)(cid:8)(cid:18)(cid:8)(cid:16)(cid:2)(cid:10)/(cid:0)L(#)(* $B(cid:0).(cid:2),(cid:14)(cid:0)(cid:18)(cid:8)(cid:25)(cid:2),(cid:14)(cid:17),(cid:0)(cid:16)(cid:26)(cid:4)(cid:2)(cid:5)(cid:0)
(cid:15)(cid:26)(cid:15)(cid:25)(cid:26)(cid:7)(cid:8)(cid:3)(cid:2),(cid:0)NGJLGJ(cid:0)(cid:18)(cid:26),(cid:14)(cid:2),(cid:0)(cid:9)(cid:13)(cid:4)(cid:2)(cid:0).(cid:2),(cid:14)(cid:0)(cid:10)(cid:26)(cid:10)(cid:17)(cid:2)(cid:8)/(cid:0)(cid:18)(cid:8)(cid:15)(cid:2),(cid:2)(cid:0)(cid:15)(cid:13)(cid:18)(cid:26)(cid:4)(cid:0)
(cid:29)(cid:30)(cid:31)(cid:31) ! ""# $ % ""#&’(!)(cid:30)""*$ ’GIJ(cid:30)O(!(cid:30)(J (cid:0) P(KQR !!(cid:30)NJ(cid:0)
(cid:15)(cid:26)(cid:15)(cid:25)(cid:26)(cid:7)(cid:8)(cid:3)(cid:2),(cid:0),(cid:8)(cid:4)(cid:2)(cid:8)(cid:0)(cid:1)(cid:6)2(cid:0).(cid:2),(cid:14)(cid:0)(cid:4)(cid:26)(cid:25)(cid:8)(cid:5)(cid:0)(cid:3)(cid:26)+(cid:8)(cid:4)(cid:0)(cid:18)(cid:8)(cid:25)(cid:2),(cid:18)(cid:8),(cid:14)(cid:0)(cid:26)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)
(cid:4)(cid:2),(cid:14)(cid:10)(cid:17),(cid:14)(cid:0)(cid:18)(cid:2),(cid:0)(cid:15)(cid:13)(cid:18)(cid:26)(cid:4)(cid:0)(cid:17),(cid:8)-(cid:2)(cid:7)(cid:8)(cid:2)(cid:16),.(cid:2)0(cid:0)(cid:6)(cid:26)(cid:4)(cid:2)(cid:8),(cid:0)(cid:8)(cid:16)(cid:17)(cid:0)(cid:1)(cid:6)2(cid:0)(cid:29)(cid:30)(cid:31)(cid:31) ! ""# $
% ""#&’(!)(cid:30)""*(cid:0)(cid:15)(cid:26)(cid:15)(cid:8)(cid:4)(cid:8)(cid:3)(cid:8)(cid:0),(cid:8)(cid:4)(cid:2)(cid:8)(cid:0)(cid:1)(cid:6)2(cid:0).(cid:2),(cid:14)(cid:0)(cid:10)(cid:26)(cid:4)(cid:2)(cid:4)(cid:17)(cid:0)(cid:4)(cid:26)(cid:25)(cid:8)(cid:5)(cid:0)(cid:25)(cid:26)(cid:10)(cid:2)(cid:7)(cid:0)
(cid:18)(cid:8)(cid:25)(cid:2),(cid:18)(cid:8),(cid:14)(cid:3)(cid:2),(cid:0)(cid:15)(cid:13)(cid:18)(cid:26)(cid:4)(cid:0)(cid:16)(cid:2),(cid:9)(cid:2)(cid:0)(cid:15)(cid:26)(cid:16)(cid:13)(cid:18)(cid:26)(cid:0)(cid:25)(cid:26),+(cid:5)(cid:15)(cid:2)(cid:7)(cid:3)(cid:8),(cid:14)/(cid:0)(cid:5)(cid:2)(cid:4)(cid:0)(cid:8),(cid:8)(cid:0)
(cid:10)(cid:17)(cid:18)(cid:2)(cid:5)(cid:0)(cid:10)(cid:26)(cid:10)(cid:17)(cid:2)(cid:8)(cid:0)(cid:18)(cid:26),(cid:14)(cid:2),(cid:0)(cid:9)(cid:26),(cid:26)(cid:4)(cid:8)(cid:16)(cid:8)(cid:2),(cid:0).(cid:2),(cid:14)(cid:0)(cid:18)(cid:8)(cid:4)(cid:2)(cid:3)(cid:17)(cid:3)(cid:2),(cid:0)(cid:13)(cid:4)(cid:26)(cid:5)(cid:0)(cid:6)(cid:16)(cid:26)(cid:13)(cid:7)(cid:16)(cid:10)(cid:0)
(cid:18)(cid:2),(cid:0)S(cid:5)(cid:13)(cid:10)(cid:5)(cid:0)T<U6M70$
$
571 (cid:19)(cid:2)(cid:7)(cid:8)(cid:0)(cid:10)(cid:26)(cid:14)(cid:8)(cid:0)(cid:9)(cid:26)(cid:7)V(cid:13)(cid:7)(cid:15)(cid:2)(cid:0).(cid:2),(cid:14)(cid:0)(cid:18)(cid:8)(cid:4)(cid:8)(cid:5)(cid:2)(cid:16)(cid:0)(cid:25)(cid:26)(cid:7)(cid:18)(cid:2)(cid:10)(cid:2)(cid:7)(cid:3)(cid:2),(cid:0)(cid:28)(cid:2)(cid:3)(cid:16)(cid:17)(cid:0)
(cid:3)(cid:13)(cid:15)(cid:9)(cid:17)(cid:16)(cid:2)(cid:10)(cid:8)/(cid:0)V(cid:17),(cid:14)(cid:10)(cid:8)(cid:0)(cid:17),(cid:16)(cid:17)(cid:3)(cid:0)(cid:15)(cid:26),(cid:14)(cid:5)(cid:8)(cid:16)(cid:17),(cid:14)(cid:0)(cid:15)(cid:13)(cid:18)(cid:26)(cid:4)(cid:0)(cid:15)(cid:17)(cid:4)(cid:16)(cid:8)-(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:2)(cid:3)(cid:2),(cid:0)
(cid:15)(cid:26)(cid:15)(cid:25)(cid:17)(cid:16)(cid:17)(cid:5)(cid:3)(cid:2),(cid:0)(cid:28)(cid:2)(cid:3)(cid:16)(cid:17)(cid:0).(cid:2),(cid:14)(cid:0)(cid:4)(cid:26)(cid:25)(cid:8)(cid:5)(cid:0)(cid:4)(cid:2)(cid:15)(cid:2)(cid:0)(cid:18)(cid:8)(cid:25)(cid:2),(cid:18)(cid:8),(cid:14)(cid:3)(cid:2),(cid:0)(cid:15)(cid:13)(cid:18)(cid:26)(cid:4)(cid:0)
(cid:17),(cid:8)-(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)/(cid:0)(cid:5)(cid:2)(cid:4)(cid:0)(cid:8),(cid:8)(cid:0)(cid:18)(cid:8)(cid:10)(cid:26)(cid:25)(cid:2)(cid:25)(cid:3)(cid:2),(cid:0)(cid:17)(cid:3)(cid:17)(cid:7)(cid:2),(cid:0)(cid:15)(cid:2)(cid:16)(cid:7)(cid:8)(cid:3)(cid:10)(cid:0)-(cid:2)(cid:7)(cid:8)(cid:2),(cid:10)(cid:0)(cid:3)(cid:13)-(cid:2)(cid:7)(cid:8)(cid:2),(cid:10)(cid:0)
(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:15)(cid:13)(cid:18)(cid:26)(cid:4)(cid:0)(cid:15)(cid:17)(cid:4)(cid:16)(cid:8)-(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0).(cid:2),(cid:14)(cid:0)(cid:4)(cid:26)(cid:25)(cid:8)(cid:5)(cid:0)(cid:7)(cid:17)(cid:15)(cid:8)(cid:16)(cid:0)(cid:18)(cid:8)(cid:25)(cid:2),(cid:18)(cid:8),(cid:14)(cid:0)(cid:15)(cid:13)(cid:18)(cid:26)(cid:4)(cid:0)
(cid:17),(cid:8)-(cid:2)(cid:7)(cid:8)(cid:2)(cid:16),.(cid:2)0$
$
W71 (cid:1)(cid:26)(cid:16)(cid:13)(cid:18)(cid:26)(cid:0)8(cid:30)(cid:31)(cid:31) ! ""# $9 ""#&’(!)(cid:30)""*(cid:0)(cid:15)(cid:13)(cid:18)(cid:26)(cid:4)(cid:0):(cid:2).(cid:20);(cid:26)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)
(cid:1)(cid:17)(cid:4)(cid:16)(cid:8)-(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:25)(cid:26)(cid:7)(cid:5)(cid:2)(cid:10)(cid:8)(cid:4)(cid:0)(cid:18)(cid:8)(cid:8)(cid:15)(cid:9)(cid:4)(cid:26)(cid:15)(cid:26),(cid:16)(cid:2)(cid:10)(cid:8)(cid:3)(cid:2),(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:18)(cid:2)(cid:16)(cid:2)(cid:0)F(cid:12)(cid:6)(cid:0)(cid:17),(cid:16)(cid:17)(cid:3)(cid:0)
(cid:15)(cid:26),(cid:14)(cid:26)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)-(cid:2)(cid:7)(cid:8)(cid:2)(cid:25)(cid:26)(cid:4)(cid:0)(cid:7)(cid:2)(cid:16)(cid:2)(cid:20)(cid:7)(cid:2)(cid:16)(cid:2)(cid:0)(cid:9)(cid:26),(cid:14)(cid:26)(cid:4)(cid:17)(cid:2)(cid:7)(cid:2),(cid:0)(cid:3)(cid:13),(cid:10)(cid:17)(cid:15)(cid:10)(cid:8)(cid:0)
(cid:15)(cid:2)(cid:3)(cid:2),(cid:2),(cid:0)(cid:18)(cid:2),(cid:0),(cid:13),(cid:0)(cid:15)(cid:2)(cid:3)(cid:2),(cid:2),(cid:0)(cid:9)(cid:26)(cid:7)(cid:3)(cid:2)(cid:9)(cid:8)(cid:16)(cid:2)(cid:0)(cid:16)(cid:8)(cid:2)(cid:9)(cid:0)(cid:3)(cid:26)+(cid:2)(cid:15)(cid:2)(cid:16)(cid:2),(cid:0)(cid:18)(cid:8)(cid:0)
(cid:12)(cid:7)(cid:13)-(cid:8),(cid:10)(cid:8)(cid:0)(cid:19)0(cid:21)0(cid:0)X(cid:13)(cid:14).(cid:2)(cid:3)(cid:2)(cid:7)(cid:16)(cid:2)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:9)(cid:26)(cid:7)(cid:8)(cid:13)(cid:18)(cid:26)(cid:0)(cid:10)(cid:17)(cid:7)-(cid:26)(cid:8)(cid:0)(cid:25)(cid:17)(cid:4)(cid:2),(cid:0)(cid:15)(cid:2)(cid:7)(cid:26)(cid:16)(cid:0)
<U6Y(cid:0) .(cid:2),(cid:14)(cid:0) (cid:15)(cid:26),(cid:17),D(cid:17)(cid:3)(cid:2),(cid:0) (cid:25)(cid:2)(cid:5)(cid:28)(cid:2)(cid:0) (cid:15)(cid:26)(cid:16)(cid:13)(cid:18)(cid:26)(cid:0) 8(cid:30)(cid:31)(cid:31) ! ""# $
9 ""#&’(!)(cid:30)""*(cid:0)(cid:15)(cid:13)(cid:18)(cid:26)(cid:4)(cid:0):(cid:2).(cid:20);(cid:26)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)(cid:1)(cid:17)(cid:4)(cid:16)(cid:8)-(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:15)(cid:26),(cid:14)(cid:5)(cid:2)(cid:10)(cid:8)(cid:4)(cid:3)(cid:2),(cid:0)
(cid:9)(cid:26),(cid:18)(cid:17)(cid:14)(cid:2)(cid:0)(cid:18)(cid:26),(cid:14)(cid:2),(cid:0)(cid:2)(cid:14)(cid:7)(cid:26)(cid:14)(cid:2)(cid:10)(cid:8)(cid:0).(cid:2),(cid:14)(cid:0)(cid:3)(cid:13),(cid:10)(cid:8)(cid:10)(cid:16)(cid:26),(cid:0)(cid:18)(cid:8)(cid:25)(cid:2),(cid:18)(cid:8),(cid:14)(cid:3)(cid:2),(cid:0)
(cid:26)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)(cid:4)(cid:2),(cid:14)(cid:10)(cid:17),(cid:14)/(cid:0)(cid:10)(cid:26)(cid:7)(cid:16)(cid:2)(cid:0)(cid:15)(cid:26)(cid:15)(cid:25)(cid:26)(cid:7)(cid:8)(cid:3)(cid:2),(cid:0)(cid:9)(cid:26),(cid:18)(cid:17)(cid:14)(cid:2)(cid:0).(cid:2),(cid:14)(cid:0)(cid:4)(cid:26)(cid:25)(cid:8)(cid:5)(cid:0)
(cid:26)V(cid:8)(cid:10)(cid:8)(cid:26),(cid:0)(cid:18)(cid:8)(cid:25)(cid:2),(cid:18)(cid:8),(cid:14)(cid:0)(cid:9)(cid:26),(cid:18)(cid:17)(cid:14)(cid:2)(cid:0)(cid:4)(cid:2),(cid:14)(cid:10)(cid:17),(cid:14)(cid:0)(cid:18)(cid:2),(cid:0)(cid:15)(cid:13)(cid:18)(cid:26)(cid:4)(cid:0)(cid:17),(cid:8)-(cid:2)(cid:7)(cid:8)(cid:2)(cid:16),.(cid:2)0$

$

$

50<1(cid:6)(cid:2)(cid:7)(cid:2),$
$

671 (cid:12)(cid:26)(cid:7)(cid:4)(cid:17)(cid:0)(cid:18)(cid:8)(cid:4)(cid:2)(cid:3)(cid:17)(cid:3)(cid:2),(cid:0)(cid:13)(cid:9)(cid:16)(cid:8)(cid:15)(cid:2)(cid:4)(cid:8)(cid:10)(cid:2)(cid:10)(cid:8)(cid:0)(cid:16)(cid:26)(cid:7)(cid:5)(cid:2)(cid:18)(cid:2)(cid:9)(cid:0)(cid:2)(cid:4)(cid:14)(cid:13)(cid:7)(cid:8)(cid:16)(cid:15)(cid:2)(cid:0).(cid:2),(cid:14)(cid:0)
(cid:18)(cid:8)(cid:25)(cid:17)(cid:2)(cid:16)(cid:0)(cid:16)(cid:26)(cid:7)(cid:17)(cid:16)(cid:2)(cid:15)(cid:2)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:9)(cid:7)(cid:13)(cid:10)(cid:26)(cid:10)(cid:0)B2(cid:1)Z(cid:0)(cid:10)(cid:26)(cid:5)(cid:8),(cid:14)(cid:14)(cid:2)(cid:0)(cid:28)(cid:2)(cid:3)(cid:16)(cid:17)(cid:0)
(cid:9)(cid:26),(cid:14)(cid:13)(cid:4)(cid:2)(cid:5)(cid:2),(cid:0)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:4)(cid:26)(cid:25)(cid:8)(cid:5)(cid:0)+(cid:26)(cid:9)(cid:2)(cid:16)0(cid:0)
(cid:0)
<71 (cid:1)(cid:13)(cid:18)(cid:26)(cid:4)(cid:0)(cid:15)(cid:17)(cid:4)(cid:16)(cid:8)-(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0).(cid:2),(cid:14)(cid:0)(cid:18)(cid:8)(cid:14)(cid:17),(cid:2)(cid:3)(cid:2),(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:9)(cid:26),(cid:26)(cid:4)(cid:8)(cid:16)(cid:8)(cid:2),(cid:0)(cid:8),(cid:8)(cid:0)
(cid:15)(cid:2)(cid:10)(cid:8)(cid:5)(cid:0)(cid:16)(cid:26)(cid:7)(cid:25)(cid:2)(cid:16)(cid:2)(cid:10)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:15)(cid:13)(cid:18)(cid:26)(cid:4)(cid:0):(cid:2).(cid:20);(cid:26)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)(cid:15)(cid:17)(cid:4)(cid:16)(cid:8)-(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)0(cid:0)(cid:6)(cid:26)(cid:18)(cid:2),(cid:14)(cid:3)(cid:2),(cid:0)
(cid:15)(cid:2)(cid:10)(cid:8)(cid:5)(cid:0)(cid:25)(cid:2),.(cid:2)(cid:3)(cid:0)(cid:15)(cid:26)(cid:16)(cid:13)(cid:18)(cid:26)(cid:0)(cid:4)(cid:2)(cid:8),(cid:0).(cid:2),(cid:14)(cid:0)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:18)(cid:8)(cid:3)(cid:26)(cid:15)(cid:25)(cid:2),(cid:14)(cid:3)(cid:2),(cid:0)(cid:10)(cid:26)+(cid:2)(cid:7)(cid:2)(cid:0)
(cid:15)(cid:17)(cid:4)(cid:16)(cid:8)-(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)(cid:15)(cid:8)(cid:10)(cid:2)(cid:4),.(cid:2)(cid:0)[’L(cid:30)!(cid:30)#(I$9(K H(cid:0)(cid:18)(cid:2),(cid:0)R(cid:30) !(!#&(cid:30)#(I$9(K H0(cid:0)

(cid:0)

M71 (cid:1)(cid:26)(cid:16)(cid:13)(cid:18)(cid:26)(cid:0)% ""#&’(!)(cid:30)""*(cid:0).(cid:2),(cid:14)(cid:0)(cid:18)(cid:8)(cid:14)(cid:17),(cid:2)(cid:3)(cid:2),(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:9)(cid:26),(cid:26)(cid:4)(cid:8)(cid:16)(cid:8)(cid:2),(cid:0)(cid:8),(cid:8)(cid:0)
(cid:15)(cid:2)(cid:10)(cid:8)(cid:5)(cid:0)(cid:16)(cid:26)(cid:7)(cid:25)(cid:2)(cid:16)(cid:2)(cid:10)(cid:0)(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)’ JN(cid:29) $(cid:29)(cid:30)(cid:31)(cid:31) ! ""# $% ""#&’(!)(cid:30)""*0(cid:0)
(cid:6)(cid:26)(cid:18)(cid:2),(cid:14)(cid:3)(cid:2),(cid:0)(cid:15)(cid:2)(cid:10)(cid:8)(cid:5)(cid:0)(cid:25)(cid:2),.(cid:2)(cid:3)(cid:0)(cid:15)(cid:26)(cid:16)(cid:13)(cid:18)(cid:26)(cid:0)(cid:4)(cid:2)(cid:8),(cid:0).(cid:2),(cid:14)(cid:0)(cid:15)(cid:2)(cid:10)(cid:8)(cid:5)(cid:0)(cid:25)(cid:26)(cid:4)(cid:17)(cid:15)(cid:0)(cid:18)(cid:2),(cid:0)
(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:18)(cid:8)(cid:3)(cid:26)(cid:15)(cid:25)(cid:2),(cid:14)(cid:3)(cid:2),(cid:0)(cid:10)(cid:26)(cid:9)(cid:26)(cid:7)(cid:16)(cid:8)(cid:0)\\NG$>(N$% ""#&’(!)(cid:30)""*/(cid:0)]LJ(cid:30)’G’$
% ""#&’(!)(cid:30)""*(cid:0)(cid:18)(cid:2),(cid:0)^:_$% ""#&’(!)(cid:30)""*(cid:0)
(cid:0)
571 (cid:12)(cid:2)(cid:18)(cid:2)(cid:0)(cid:18)(cid:2)(cid:16)(cid:2)(cid:0)(cid:10)(cid:16)(cid:17)(cid:18)(cid:8)(cid:0)(cid:3)(cid:2)(cid:10)(cid:17)(cid:10)/(cid:0)(cid:9)(cid:26)(cid:15)(cid:8)(cid:4)(cid:8)(cid:5)(cid:2),(cid:0)-(cid:2)(cid:7)(cid:8)(cid:2)(cid:25)(cid:26)(cid:4)(cid:0)(cid:9)(cid:26),.(cid:26)(cid:7)(cid:16)(cid:2)(cid:0)(cid:18)(cid:2),(cid:0)
D(cid:17)(cid:14)(cid:2)(cid:0)(cid:9)(cid:26),(cid:8)(cid:15)(cid:25)(cid:2),(cid:14)(cid:0)(cid:25)(cid:26),+(cid:5)(cid:15)(cid:2)(cid:7)(cid:3)(cid:8),(cid:14)(cid:0)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:18)(cid:8)(cid:4)(cid:2)(cid:3)(cid:17)(cid:3)(cid:2),(cid:0)(cid:18)(cid:26),(cid:14)(cid:2),(cid:0)(cid:4)(cid:26)(cid:25)(cid:8)(cid:5)(cid:0)
(cid:25)(cid:2)(cid:8)(cid:3)(cid:0)(cid:10)(cid:26)(cid:5)(cid:8),(cid:14)(cid:14)(cid:2)(cid:0)(cid:18)(cid:2)(cid:9)(cid:2)(cid:16)(cid:0)(cid:15)(cid:26),(cid:14)(cid:5)(cid:2)(cid:10)(cid:8)(cid:4)(cid:3)(cid:2),(cid:0)(cid:15)(cid:13)(cid:18)(cid:26)(cid:4)(cid:0)(cid:18)(cid:2),(cid:0)D(cid:17)(cid:14)(cid:2)(cid:0)(cid:26)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:10)(cid:8)(cid:0)
.(cid:2),(cid:14)(cid:0)(cid:4)(cid:26)(cid:25)(cid:8)(cid:5)(cid:0)(cid:25)(cid:2)(cid:8)(cid:3)0(cid:0)

(cid:19)C:(cid:24)CB(cid:0)(cid:12)4(cid:6)(cid:24)C(cid:23)C(cid:0)

‘ab1 B(cid:2)(cid:13)/(cid:0)c0/(cid:0)d(cid:0)(cid:1)(cid:13)(cid:4)(cid:8),(cid:2)/(cid:0)(cid:21)0(cid:0)T<U6W70(cid:0)(cid:6)(cid:15)(cid:2)(cid:4)(cid:4)(cid:0)C(cid:7)(cid:26)(cid:2)(cid:0)2(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:16)(cid:8)(cid:13),(cid:0)<,(cid:18)(cid:0)2(cid:18)(cid:8)(cid:16)(cid:8)(cid:13),0(cid:0)3(cid:26)(cid:28)(cid:0)

c(cid:26)(cid:7)(cid:10)(cid:26).?(cid:0)c(cid:13)(cid:5),(cid:0)^(cid:8)(cid:4)(cid:26).(cid:0)(cid:2),(cid:18)(cid:0)(cid:6)(cid:13),(cid:10)(cid:0)(cid:21),+0(cid:24)(cid:26)+(cid:5),(cid:13)(cid:4)(cid:13)(cid:14).0(cid:0)

‘eb1 (cid:12)(cid:26)(cid:7)(cid:15)(cid:2)(cid:16)(cid:2)(cid:10)(cid:2)(cid:7)(cid:8)/(cid:0)3(cid:13)-(cid:8)(cid:2)0(cid:0)T<U6f70(cid:0)(cid:12)(cid:26)(cid:15)(cid:25)(cid:2),(cid:14)(cid:17),(cid:2),(cid:0)(cid:12)(cid:2)(cid:3)(cid:26)(cid:16)(cid:0)B(cid:0)(cid:12)(cid:2)(cid:18)(cid:2)(cid:0)(cid:1)(cid:13)(cid:18)(cid:26)(cid:4)(cid:0):(cid:2).(cid:0)
;(cid:26)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)(cid:1)(cid:17)(cid:4)(cid:16)(cid:8)-(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)4,(cid:16)(cid:17)(cid:3)(cid:0)(cid:12)(cid:26),(cid:18)(cid:17)(cid:14)(cid:2)(cid:2),(cid:0)C(cid:7)(cid:26)(cid:2)(cid:0)(cid:23)(cid:26)+(cid:8)(cid:4)(cid:0)g(cid:6)(cid:3)(cid:7)(cid:8)(cid:9)(cid:10)(cid:8)h0(cid:0)c(cid:2)(cid:3)(cid:2)(cid:7)(cid:16)(cid:2)?(cid:0)
(cid:12)(cid:13)(cid:4)(cid:10)(cid:16)(cid:2)(cid:16)(cid:0)(cid:6)(cid:24)(cid:21)(cid:6)0(cid:0)

‘ib1 (cid:19)(cid:2)(cid:16)(cid:16)(cid:2)/(cid:0)S0(cid:6)0/(cid:0)B(cid:2)(cid:13)/(cid:0)c030(cid:23)0/(cid:0)(cid:2),(cid:18)(cid:0)(cid:6)(cid:15)(cid:8)(cid:16)(cid:5)/(cid:0)(cid:19)0(cid:19)0(cid:0)T<UUW7/(cid:0)j,(cid:0)(cid:1)(cid:26)(cid:2)(cid:10)(cid:17)(cid:7)(cid:8),(cid:14)(cid:0)(cid:16)(cid:5)(cid:26)(cid:0)
(cid:22)(cid:2)(cid:7)(cid:8)(cid:2)(cid:25)(cid:8)(cid:4)(cid:8)(cid:16).(cid:0)(cid:13)V(cid:0)(cid:6)(cid:15)(cid:2)(cid:4)(cid:4)(cid:0)C(cid:7)(cid:26)(cid:2)(cid:0)2(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:16)(cid:13)(cid:7)(cid:10)(cid:0)(cid:17),(cid:18)(cid:26)(cid:7)(cid:0)(cid:2)(cid:0)F(cid:2)(cid:10)(cid:8)+(cid:0)C(cid:7)(cid:26)(cid:2)(cid:0)Z(cid:26)-(cid:26)(cid:4)(cid:0)(cid:1)(cid:13)(cid:18)(cid:26)(cid:4)/(cid:0)
F(cid:8)(cid:13)(cid:15)(cid:26)(cid:16)(cid:7)(cid:8)(cid:3)(cid:2)/(cid:0)Y</(cid:0)6fM(cid:11)6Y(cid:27)0(cid:0)

‘kb1 (cid:24)l(cid:2)(cid:18)(cid:8)-(cid:8)(cid:10)(cid:0)3/(cid:0)m(cid:5)(cid:2),(cid:14)(cid:0)ZA/(cid:0)Z(cid:17),(cid:2)(cid:0)C/(cid:0)(cid:6)+(cid:5)(cid:15)(cid:8)(cid:18)(cid:0)(cid:24)/(cid:0)B(cid:13)D(cid:2)(cid:10)(cid:20)(cid:12)(cid:26)(cid:7)(cid:8)(cid:4)(cid:4)(cid:2)(cid:0)3(cid:0)T<U6f70(cid:0):(cid:7)(cid:13)(cid:15)(cid:0)
(cid:6)(cid:16)(cid:2)(cid:7)(cid:16)(cid:0)(cid:16)(cid:13)(cid:0):(cid:8),(cid:8)(cid:10)(cid:5)?(cid:0)C(cid:0):(cid:7)(cid:2)(cid:15)(cid:26)(cid:28)(cid:13)(cid:7)(cid:3)(cid:0)V(cid:13)(cid:7)(cid:0)(cid:16)(cid:5)(cid:26)(cid:0)(cid:12)(cid:7)(cid:13)(cid:18)(cid:17)+(cid:16)(cid:8)(cid:13),(cid:0)(cid:13)V(cid:0)(cid:6)(cid:15)(cid:2)(cid:4)(cid:4)(cid:0)C(cid:7)(cid:26)(cid:2)(cid:0)jVV(cid:8)+(cid:8)(cid:2)(cid:4)(cid:0)
(cid:6)(cid:16)(cid:2)(cid:16)(cid:8)(cid:10)(cid:16)(cid:8)+(cid:10)0(cid:0)c(cid:13)(cid:17)(cid:7),(cid:2)(cid:4)(cid:0)(cid:13)V(cid:0)(cid:16)(cid:5)(cid:26)(cid:0)B(cid:13).(cid:2)(cid:4)(cid:0)(cid:6)(cid:16)(cid:2)(cid:16)(cid:8)(cid:10)(cid:16)(cid:8)+(cid:2)(cid:4)(cid:0)(cid:6)(cid:13)+(cid:8)(cid:26)(cid:16).(cid:0)(cid:6)(cid:26)(cid:7)(cid:8)(cid:26)(cid:10)(cid:0)C/(cid:0)6f6T57/(cid:0)6(cid:11)
MM0(cid:0)

‘nb1 B(cid:2)(cid:17)V/(cid:0)C(cid:4)(cid:18)(cid:8)(cid:0)(cid:6)(cid:2)(cid:26)V(cid:17)(cid:4)0(cid:0)T<U6Y70(cid:0)(cid:12)(cid:26)(cid:15)(cid:25)(cid:2),(cid:14)(cid:17),(cid:2),(cid:0)C(cid:9)(cid:4)(cid:8)(cid:3)(cid:2)(cid:10)(cid:8)(cid:0)(cid:6)(cid:15)(cid:2)(cid:4)(cid:4)(cid:0)C(cid:7)(cid:26)(cid:2)(cid:0)

2(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:16)(cid:8)(cid:13),(cid:0)(cid:1)(cid:13)(cid:18)(cid:26)(cid:4)(cid:0)F(cid:26),+(cid:5)(cid:15)(cid:2)(cid:7)(cid:3)(cid:0)g(cid:6)(cid:3)(cid:7)(cid:8)(cid:9)(cid:10)(cid:8)h0(cid:0)c(cid:2)(cid:3)(cid:2)(cid:7)(cid:16)(cid:2)?(cid:0)(cid:12)(cid:13)(cid:4)(cid:10)(cid:16)(cid:2)(cid:16)(cid:0)(cid:6)(cid:24)(cid:21)(cid:6)0(cid:0)

‘ob1 (cid:12)V(cid:26)VV(cid:26)(cid:7)(cid:15)(cid:2),,/(cid:0)(cid:19)0(cid:0)(cid:2),(cid:18)(cid:0)F(cid:2)(cid:7),(cid:2)(cid:7)(cid:18)/(cid:0)A0(cid:0)T6YY670(cid:0)(cid:6)(cid:13)(cid:15)(cid:26)(cid:0)3(cid:26)(cid:28)(cid:0)2(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:16)(cid:13)(cid:7)(cid:10)(cid:0)V(cid:13)(cid:7)(cid:0)
(cid:6)(cid:15)(cid:2)(cid:4)(cid:4)(cid:0)C(cid:7)(cid:26)(cid:2)(cid:0)(cid:1)(cid:26)(cid:2),(cid:10)(cid:0)(cid:28)(cid:8)(cid:16)(cid:5)(cid:0)C(cid:9)(cid:9)(cid:4)(cid:8)+(cid:2)(cid:16)(cid:8)(cid:13),(cid:10)(cid:0)(cid:16)(cid:13)(cid:0)(cid:16)(cid:5)(cid:26)(cid:0)C(cid:10)(cid:10)(cid:26)(cid:10)(cid:10)(cid:15)(cid:26),(cid:16)(cid:0)(cid:13)V(cid:0):(cid:2)(cid:7)(cid:15)(cid:4)(cid:2),(cid:18)(cid:0)
(cid:22)(cid:2)(cid:4)(cid:17)(cid:26)(cid:10)/(cid:0)c(cid:13)(cid:17)(cid:7),(cid:2)(cid:4)(cid:0)(cid:13)V(cid:0)F(cid:17)(cid:10)(cid:8),(cid:26)(cid:10)(cid:10)(cid:0)(cid:2),(cid:18)(cid:0)2+(cid:13),(cid:13)(cid:15)(cid:8)+(cid:0)(cid:6)(cid:16)(cid:2)(cid:16)(cid:8)(cid:10)(cid:16)(cid:8)+(cid:10)/(cid:0)Y/(cid:0)pM(cid:11)f50(cid:0)
‘qb1 C.(cid:2)(cid:10)(cid:5)(cid:8)/(cid:0)(cid:1)(cid:0)Cl(cid:15)(cid:2),0(cid:0)T<U6Y70(cid:0)(cid:12)(cid:26)(cid:15)(cid:25)(cid:2),(cid:14)(cid:17),(cid:2),(cid:0)(cid:24)(cid:13)(cid:13)(cid:4)(cid:0)(cid:6)(cid:15)(cid:2)(cid:4)(cid:4)(cid:0)C(cid:7)(cid:26)(cid:2)(cid:0)2(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:16)(cid:8)(cid:13),(cid:0)
(cid:1)(cid:13)(cid:18)(cid:26)(cid:4)(cid:0):(cid:2).;(cid:26)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)(cid:1)(cid:17)(cid:4)(cid:16)(cid:8)-(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)?(cid:0)(cid:12)(cid:26),(cid:26)(cid:7)(cid:2)(cid:9)(cid:2),(cid:0)F(cid:8)-(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:0)g(cid:6)(cid:3)(cid:7)(cid:8)(cid:9)(cid:10)(cid:8)h0(cid:0)c(cid:2)(cid:3)(cid:2)(cid:7)(cid:16)(cid:2)?(cid:0)
(cid:12)(cid:13)(cid:4)(cid:10)(cid:16)(cid:2)(cid:16)(cid:0)(cid:6)(cid:24)(cid:21)(cid:6)0(cid:0)

‘rb1 4(cid:25)(cid:2)(cid:8)(cid:18)(cid:8)(cid:4)(cid:4)(cid:2)(cid:5)/(cid:0)C0/(cid:0)3(cid:13)(cid:16)(cid:13)(cid:18)(cid:8)(cid:9)(cid:17)(cid:16)(cid:7)(cid:13)/(cid:0)(cid:23)0(cid:0)C0/(cid:0)(cid:23)(cid:17)(cid:7),(cid:8)(cid:2)/(cid:0)C0/(cid:0)d(cid:0)(cid:1)(cid:2),(cid:14)(cid:3)(cid:17)/(cid:0)(cid:21)0(cid:0)^0(cid:0)T<U6Y70(cid:0)
(cid:1)(cid:17)(cid:4)(cid:16)(cid:8)-(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:26)(cid:0):(cid:2).(cid:20);(cid:26)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)(cid:15)(cid:13)(cid:18)(cid:26)(cid:4)(cid:10)(cid:0)V(cid:13)(cid:7)(cid:0)(cid:10)(cid:15)(cid:2)(cid:4)(cid:4)(cid:0)(cid:2)(cid:7)(cid:26)(cid:2)(cid:0)(cid:26)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:16)(cid:8)(cid:13),(cid:0)(cid:28)(cid:8)(cid:16)(cid:5)(cid:0)
(cid:2)(cid:9)(cid:9)(cid:4)(cid:8)+(cid:2)(cid:16)(cid:8)(cid:13),(cid:0)(cid:16)(cid:13)(cid:0)(cid:5)(cid:13)(cid:17)(cid:10)(cid:26)(cid:5)(cid:13)(cid:4)(cid:18)(cid:0)+(cid:13),(cid:10)(cid:17)(cid:15)(cid:9)(cid:16)(cid:8)(cid:13),(cid:0)(cid:9)(cid:26)(cid:7)(cid:0)+(cid:2)(cid:9)(cid:8)(cid:16)(cid:2)(cid:0)(cid:26)s(cid:9)(cid:26),(cid:18)(cid:8)(cid:16)(cid:17)(cid:7)(cid:26)(cid:0)(cid:8),(cid:0)
(cid:21),(cid:18)(cid:13),(cid:26)(cid:10)(cid:8)(cid:2)0(cid:0)c(cid:13)(cid:17)(cid:7),(cid:2)(cid:4)(cid:0)(cid:13)V(cid:0)C(cid:9)(cid:9)(cid:4)(cid:8)(cid:26)(cid:18)(cid:0)(cid:6)(cid:16)(cid:2)(cid:16)(cid:8)(cid:10)(cid:16)(cid:8)+(cid:10)/(cid:0)<f5W(cid:20)<f(cid:27)60(cid:0)

‘tb1 F(cid:26),(cid:2)-(cid:26),(cid:16)/(cid:0)B0/(cid:0)d(cid:0)(cid:1)(cid:13)(cid:7)(cid:2)(cid:4)(cid:26)(cid:10)/(cid:0)(cid:19)0(cid:0)T<U6(cid:27)70(cid:0)(cid:1)(cid:17)(cid:4)(cid:16)(cid:8)-(cid:2)(cid:7)(cid:8)(cid:2)(cid:16)(cid:26)(cid:0):(cid:2).(cid:0)(cid:11)(cid:0);(cid:26)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)(cid:15)(cid:13)(cid:18)(cid:26)(cid:4)(cid:10)(cid:0)
V(cid:13)(cid:7)(cid:0)(cid:10)(cid:15)(cid:2)(cid:4)(cid:4)(cid:0)(cid:2)(cid:7)(cid:26)(cid:2)(cid:0)(cid:26)(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:16)(cid:8)(cid:13),0(cid:0)A(cid:13)(cid:15)(cid:9)(cid:17)(cid:16)(cid:2)(cid:16)(cid:8)(cid:13),(cid:2)(cid:4)(cid:0)(cid:6)(cid:16)(cid:2)(cid:16)(cid:8)(cid:10)(cid:16)(cid:8)+(cid:10)(cid:0)(cid:2),(cid:18)(cid:0)(cid:19)(cid:2)(cid:16)(cid:2)(cid:0)C,(cid:2)(cid:4).(cid:10)(cid:8)(cid:10)0(cid:0)
A(cid:13)(cid:15)(cid:9)(cid:17)(cid:16)(cid:2)(cid:16)(cid:8)(cid:13),(cid:2)(cid:4)(cid:0)(cid:6)(cid:16)(cid:2)(cid:16)(cid:8)(cid:10)(cid:16)(cid:8)+(cid:10)(cid:0)(cid:2),(cid:18)(cid:0)(cid:19)(cid:2)(cid:16)(cid:2)(cid:0)C,(cid:2)(cid:4).(cid:10)(cid:8)(cid:10)/(cid:0)Mp<(cid:20)MYU0(cid:0)

‘aub1(cid:6)(cid:16)(cid:26)(cid:13)(cid:7)(cid:16)(cid:10)/(cid:0)B0/(cid:0)d(cid:0)S(cid:5)(cid:13)(cid:10)(cid:5)/(cid:0)(cid:1)0(cid:0)T<U6M70(cid:0)j,(cid:0)2(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:16)(cid:8)(cid:13),(cid:0)(cid:13)V(cid:0)(cid:1)(cid:26)(cid:2),(cid:0)(cid:6)v(cid:17)(cid:2)(cid:7)(cid:26)(cid:18)(cid:0)2(cid:7)(cid:7)(cid:13)(cid:7)(cid:10)(cid:0)
(cid:13)V(cid:0)F(cid:26),+(cid:5)(cid:15)(cid:2)(cid:7)(cid:3)(cid:26)(cid:18)(cid:0)2(cid:15)(cid:9)(cid:8)(cid:7)(cid:8)+(cid:2)(cid:4)(cid:0)F(cid:2).(cid:26)(cid:10)(cid:0)2(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:16)(cid:13)(cid:7)(cid:10)0(cid:0)(cid:6)(cid:16)(cid:2)(cid:16)(cid:8)(cid:10)(cid:16)(cid:8)+(cid:2)(cid:0)(cid:6)(cid:8),(cid:8)+(cid:2)/(cid:0)<MT<7/(cid:0)
p5Y(cid:20)p(cid:27)p0(cid:0)

‘aab1C,(cid:8)(cid:10)(cid:2)(cid:0)B/(cid:0)(cid:23)(cid:17)(cid:7),(cid:8)(cid:2)(cid:0)C/(cid:0)(cid:21),(cid:18)(cid:2)(cid:5)(cid:28)(cid:2)(cid:16)(cid:8)0(cid:0)T<U6570(cid:0)A(cid:4)(cid:17)(cid:10)(cid:16)(cid:26)(cid:7)(cid:0)(cid:21),V(cid:13)(cid:7)(cid:15)(cid:2)(cid:16)(cid:8)(cid:13),(cid:0)(cid:13)V(cid:0)3(cid:13),(cid:20)
(cid:10)(cid:2)(cid:15)(cid:9)(cid:4)(cid:26)(cid:18)(cid:0)C(cid:7)(cid:26)(cid:2)(cid:0)(cid:8),(cid:0)(cid:6)(cid:15)(cid:2)(cid:4)(cid:4)(cid:0)C(cid:7)(cid:26)(cid:2)(cid:0)2(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:16)(cid:8)(cid:13),0(cid:0)(cid:21)j(cid:6)B(cid:0)c(cid:13)(cid:17)(cid:7),(cid:2)(cid:4)(cid:0)(cid:13)V(cid:0)(cid:1)(cid:2)(cid:16)(cid:5)(cid:26)(cid:15)(cid:2)(cid:16)(cid:8)+(cid:10)(cid:0)
6UT67?6W(cid:20)6Y0(cid:0)

‘aeb1;(cid:2)(cid:7)(cid:8)(cid:10)/(cid:0):(cid:2)(cid:8)(cid:10)(cid:2)(cid:4)0(cid:0)T<U6Y70(cid:0)(cid:23)(cid:2)D(cid:8)(cid:2),(cid:0)(cid:1)(cid:26)(cid:2),(cid:0)(cid:6)v(cid:17)(cid:2)(cid:7)(cid:26)(cid:18)(cid:0)2(cid:7)(cid:7)(cid:13)(cid:7)(cid:0)C(cid:7)(cid:26)(cid:2)(cid:0)(cid:24)(cid:8)(cid:18)(cid:2)(cid:3)(cid:0)(cid:24)(cid:26)(cid:7)(cid:10)(cid:2)(cid:15)(cid:9)(cid:26)(cid:4)(cid:0)

(cid:9)(cid:2)(cid:18)(cid:2)(cid:0)(cid:6)(cid:15)(cid:2)(cid:4)(cid:4)(cid:0)C(cid:7)(cid:26)(cid:2)(cid:0)2(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:16)(cid:8)(cid:13),(cid:0)g(cid:6)(cid:3)(cid:7)(cid:8)(cid:9)(cid:10)(cid:8)h0(cid:0)c(cid:2)(cid:3)(cid:2)(cid:7)(cid:16)(cid:2)?(cid:0)(cid:12)(cid:13)(cid:4)(cid:10)(cid:16)(cid:2)(cid:16)(cid:0)(cid:6)(cid:24)(cid:21)(cid:6)0(cid:0)

‘aib1(cid:22)(cid:2)(cid:8)(cid:10)(cid:5),(cid:2)-(cid:8)/(cid:0)(cid:22)(cid:8)D(cid:2).(cid:0)d(cid:0)(cid:23)(cid:0)4(cid:26)+(cid:5)(cid:4)(cid:26)(cid:7)/(cid:0)^0(cid:0)T<UUf70(cid:0)(cid:19)(cid:26)(cid:10)(cid:8)(cid:14),(cid:0)(cid:6)+(cid:8)(cid:26),+(cid:26)(cid:0)B(cid:26)(cid:10)(cid:26)(cid:2)(cid:7)+(cid:5)(cid:0)
(cid:1)(cid:26)(cid:16)(cid:5)(cid:13)(cid:18)(cid:10)(cid:0)(cid:2),(cid:18)(cid:0)(cid:12)(cid:2)(cid:16)(cid:16)(cid:26)(cid:7),(cid:10)?(cid:0)(cid:21),,(cid:13)-(cid:2)(cid:16)(cid:8),(cid:14)(cid:0)(cid:21),V(cid:13)(cid:7)(cid:15)(cid:2)(cid:16)(cid:8)(cid:13),(cid:0)(cid:2),(cid:18)(cid:0)A(cid:13)(cid:15)(cid:15)(cid:17),(cid:8)+(cid:2)(cid:16)(cid:8)(cid:13),(cid:0)
(cid:24)(cid:26)+(cid:5),(cid:13)(cid:4)(cid:13)(cid:14).0(cid:0)

‘akb1X(cid:13)(cid:17)/(cid:0)X0/(cid:0)B(cid:2)(cid:13)/(cid:0)c030(cid:23)0/(cid:0)(cid:2),(cid:18)(cid:0);(cid:8)(cid:18)(cid:8)(cid:7)(cid:13)(cid:14)(cid:4)(cid:13)(cid:17)/(cid:0)(cid:1)0C0(cid:0)T<U6M70(cid:0)j,(cid:0)(cid:16)(cid:5)(cid:26)(cid:0)(cid:12)(cid:26)(cid:7)V(cid:13)(cid:7)(cid:15)(cid:2),+(cid:26)(cid:0)
(cid:13)V(cid:0)(cid:6)(cid:26)(cid:4)V(cid:20)F(cid:26),+(cid:5)(cid:15)(cid:2)(cid:7)(cid:3)(cid:8),(cid:14)(cid:0)(cid:6)(cid:15)(cid:2)(cid:4)(cid:4)(cid:0)C(cid:7)(cid:26)(cid:2)(cid:0)2(cid:10)(cid:16)(cid:8)(cid:15)(cid:2)(cid:16)(cid:13)(cid:7)(cid:10)(cid:0)4,(cid:18)(cid:26)(cid:7)(cid:0)(cid:16)(cid:5)(cid:26)(cid:0):(cid:2).(cid:20);(cid:26)(cid:7)(cid:7)(cid:8)(cid:13)(cid:16)(cid:0)
C(cid:7)(cid:26)(cid:2)(cid:0)Z(cid:26)-(cid:26)(cid:4)(cid:0)(cid:1)(cid:13)(cid:18)(cid:26)(cid:4)0(cid:0)(cid:6)(cid:17)(cid:7)-(cid:26).(cid:0)(cid:1)(cid:26)(cid:16)(cid:5)(cid:13)(cid:18)(cid:13)(cid:4)(cid:13)(cid:14)./(cid:0)MY/(cid:0)<6p(cid:11)(cid:0)<<Y0(cid:0)

"
221710082,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Kajian Implementasi Pengenalan Tulisan Tangan 
Pada Entri Data Survei Angkatan Kerja Nasional 
(SAKERNAS) 

Yusron Farid Mustafa (221710082, 4SD1) 
Dosen Pembimbing: Farid Ridho, S.S.T., M.T. 

Ringkasan— Penggunaan Paper and Pencil Interviewing (PAPI) 
di BPS memerlukan kegiatan entri data secara manual yang tidak 
lepas dari kemampuan manusia untuk mengenali tulisan tangan. 
Bagi komputer, pengenalan tulisan tangan merupakan pekerjaan 
yang  rumit  sehingga  memerlukan  algoritme  yang  kompleks. 
Convolutional Neural Network (CNN) merupakan algoritme yang 
dapat  mengakomodasi  kompleksitas  pengenalan  tulisan  tangan. 
Penelitian  ini  bermaksud  untuk  melakukan  kajian  mengenai 
implementasi  model  pengenalan  tulisan  tangan  menggunakan 
CNN  dalam  mengenali  tulisan  tangan  pada  kegiatan  entri  data 
kuesioner  PAPI.  Model  pengenalan  tulisan  tangan  dibangun 
menggunakan  dataset  EMNIST  secara  terpisah  sesuai  jenis 
karakter  dan  memberikan  akurasi  89%  untuk  karakter  berupa 
huruf  dan  angka,  95%  untuk  karakter  berupa  huruf,  dan  99% 
untuk  karakter berupa angka. Implementasi  pengenalan tulisan 
tangan pada image kuesioner menunjukkan hasil yang cukup baik 
dengan akurasi 83,33%. Namun, terdapat temuan masalah pada 
proses segmentasi karakter dimana karakter tidak tersegmentasi 
secara  benar  dikarenakan  garis  tulisan  yang  bersambung  pada 
karakter  yang  seharusnya  terpisah  dan  karakter  yang  terputus 
padahal  seharusnya  tergabung.  Hasil  kajian  yang  didapat 
diharapkan dapat menjadi pertimbangan mengenai metode entri 
data yang digunakan BPS selanjutnya. 

Kata  Kunci—  Pengenalan  Tulisan  Tangan,  Convolutional 

Neural Network,  Entri Data. 

I.  LATAR BELAKANG 

Pada  era  digital  ini,  penggunaan  perangkat  digital  telah 
mengubah  bagaimana  pekerjaan  terkait  teks  dan  dokumen 
dikerjakan  [1].  Meskipun  demikian,  penggunaan  media 
berbasis kertas masih akan terus menjadi pilihan bagi beberapa 
orang ataupun organisasi [2]. Salah satu contoh yang tidak bisa 
lepas  dari  penggunaan  media  kertas  adalah  Badan  Pusat 
Statistik (BPS).  

BPS  merupakan  Lembaga  Pemerintah  Non  Kementerian 
yang  berperan  untuk  menyediakan  kebutuhan  data  bagi 
pemerintah dan masyarakat. Dalam kegiatan pengumpulan data, 
metode yang masih dominan digunakan oleh BPS adalah PAPI 
(Paper and Pencil Interviewing) [3]. PAPI merupakan metode 
wawancara  dimana  pencacah  memegang  kuesioner  kertas, 
membacakan  pertanyaan  kepada  responden,  lalu  mengisikan 
jawaban  responden  pada  kuesioner  dengan  tulisan  tangan. 
Tidak seperti CAPI (Computer-Assisted Personal Interviewing) 
atau wawancara yang  menggunakan bantuan komputer, PAPI 
hampir tidak memerlukan kemampuan teknis untuk diterapkan 
dan  lebih  mudah  bagi  pencacah  atau  responden  untuk 
memberikan jawaban terbuka atau kualitatif [4]. Konsekuensi 
dari diterapkannya PAPI adalah diperlukan kegiatan entri data 
secara manual dari data isian kuesioner berupa tulisan tangan 

pada kuesioner PAPI ke dalam database menggunakan tenaga 
manusia.  

Juru ketik profesional memiliki rata-rata kecepatan mengetik 
sebanyak  50  hingga  80  kata  per  menit  sehingga 
jika 
diperkirakan  sekitar  200  halaman  per  jam  untuk  formulir 
dengan  15  blok  satu  kata  dan  belum  termasuk  waktu  untuk 
membaca  dan  menyortir  halaman  [5].  Dari  segi  biaya, 
Peraturan  Kepala  BPS  RI  No.  51  tahun  2013  menyebutkan 
bahwa rata-rata biaya kegiatan data entry sebesar Rp 10.000 per 
kuesioner.  

Manusia memiliki kemampuan untuk mengenali suatu objek 
atau  pola  dengan  mudah.  Semakin  sering  seseorang  melihat 
suatu objek maka semakin familiar pula dengan objek tersebut. 
Dalam mengenali objek, manusia memproses informasi visual 
dalam ruang semantik dengan mengekstraksi fitur yang berarti 
seperti  segmen  garis, batas, bentuk, dan sebagainya [6]. Bagi 
komputer,  kemampuan  pengenalan  objek  mempunyai 
tantangan tersendiri dibanding manusia yang menganggapnya 
sebagai suatu hal yang mudah [7].  

tulisan 

Dalam  ranah  pengenalan  pola,  pengenalan  tulisan  tangan 
pada khususnya telah menjadi subjek dari banyak penelitian [8]. 
Beberapa permasalahan yang dijumpai pada pengenalan tulisan 
tangan  meliputi  tingginya  ketidakpastian  data  dikarenakan 
karakteristik 
tangan  setiap  orang  berbeda-beda, 
beberapa  karakter  memiliki  bentuk  yang  mirip,  terdapat 
karakter  yang  terputus  atau  terdistorsi,  adanya  perbedaan 
ketebalan karakter yang ditulis, dan penggunaan scanner yang 
bermacam-macam  [9].  Hal  tersebut  menunjukkan  bahwa 
pengenalan  tulisan  tangan  merupakan  pekerjaan  yang  rumit 
bagi komputer. 

Pengenalan  objek  atau  pola  biasanya  dilakukan  dengan 
ekstraksi  fitur  dan  klasifikasi.  Proses  ekstraksi  fitur  secara 
khusus  menggunakan  pelbagai  metode  untuk  mendapatkan 
representasi  dari  data.  Setelah  itu,  digunakan  classifier  untuk 
memproses  pengklasifikasian  data.  Proses-proses  tersebut 
masih dilakukan  secara  manual dan terpisah. Belakangan ini, 
ekstraksi  fitur  dan  klasifikasi  diintegrasikan  secara  otomatis 
pada satu proses atau metode. Metode tersebut biasa digunakan 
untuk  memodelkan  data  dengan  tingkat  abstraksi  yang  tinggi 
dan sering dikenal sebagai teknik deep learning [10]. 

Convolutional Neural Network (CNN) merupakan salah satu 
arsitektur deep learning. CNN  dapat mengekstrak  fitur  mulai 
dari fitur rendah sampai fitur tinggi secara otomatis [11].  CNN 
merupakan  jenis  neural  network  khusus  yang  didesain  untuk 
mengenali  pola  visual  melalui  piksel  dari  image  secara 
langsung  dengan  preprocessing  minimum.  Dengan  kata  lain, 
CNN yang khusus untuk mengolah data yang memiliki topologi 

 1 / 9 

 
 
 
 
 
 
berbentuk  grid.  Dalam  proses  pengolahan  image,  CNN 
mengubah  piksel  gambar  menjadi  array  yang  kemudian 
dilakukan proses konvolusi.  

memberikan  hasil  yang  paling  baik  dengan  tingkat  akurasi 
sebesar  98  %.  Hal  ini  menunjukan  metode  CNN  sangat  baik 
untuk diimplementasikan pada klasifikasi sebuah image. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Salah satu platform yang dapat digunakan untuk menerapkan 
CNN  adalah  Tensorflow.  Tensorflow  dapat  mengakomodasi 
berbagai  macam  algoritme,  termasuk  pelatihan  dan  inferensi 
algoritme untuk model deep neural network. Tensorflow sudah 
digunakan  untuk  melakukan  berbagai  penelitian  dan 
menerapkan  sistem  machine  learning  ke  produksi  di  banyak 
bidang ilmu komputer dan bidang lainnya, seperti pengenalan 
suara, computer vision, robotika, pencarian informasi, natural 
language processing, dan ekstraksi informasi geografis [12]. 

Penelitian ini mengambil studi kasus kuesioner SAKERNAS 
karena  Analisis  Hasil  Survei  Kebutuhan  Data  (SKD)  2019 
menunjukkan  bahwa  jika  dilihat  dari  sumber  datanya,  data 
mikro  yang  paling  banyak  diperoleh  dari  penyedia  data  BPS 
Pusat adalah data mikro SAKERNAS. Selain itu, SAKERNAS 
merupakan  survei  yang  dilaksanakan  dalam  frekuensi  yang 
terbilang sering yaitu dua kali dalam setahun. Oleh karena itu, 
upaya  dalam  mengefisiensikan  tahapan  pekerjaan  yang  ada 
pada pelaksanaan SAKERNAS akan dibutuhkan oleh BPS. 

tulisan 

Berdasarkan uraian permasalahan di atas, penelitian ini akan 
membangun  model  pengenalan 
tangan  untuk 
diimplementasikan pada kuesioner SAKERNAS Agustus 2020 
lalu  melakukan  kajian  mengenai  metode  entri  data 
menggunakan  model  pengenalan 
tangan.  Model 
pengenalan tulisan  tangan  yang dibangun akan  menggunakan 
algoritme  Convolutional  Neural  Network  dengan  library 
Tensorflow. Hasil dari kajian metode entri data menggunakan 
model  pengenalan  tulisan  tangan  diharapkan  dapat  dijadikan 
pertimbangan  dalam  menentukan  metode  entri  data  di  BPS 
kedepannya.  Selanjutnya,  model  pengenalan  tulisan  tangan 
yang  berhasil  dibangun  diharapkan  dapat  diterapkan  pada 
kuesioner kegiatan BPS lainnya. 

tulisan 

II.  TUJUAN PENELITIAN 

Tujuan dalam penelitian ini adalah sebagai berikut: 
1.  Membuat  model 
menggunakan CNN 

pengenalan 

tulisan 

tangan 

2.  Mengimplementasikan  model  pengenalan 

tulisan 
tangan  untuk  mengenali  isian  kuesioner  SAKERNAS 
2020 

III. PENELITIAN TERKAIT 

Terdapat  beberapa  penelitian  terdahulu  yang  memiliki 
keterkaitan dengan topik pengenalan tulisan tangan dan dapat 
menjadi gambaran untuk penelitian ini. 

Penelitian  [13]  melakukan  perbandingan  CNN  dengan 
beberapa metode klasifikasi lain dalam melakukan pengenalan 
objek  hewan.  Metode-metode  klasifikasi  yang  dibandingkan 
dengan  CNN  yaitu  Principal  Component  Analisys  (PCA), 
Linear  Discriminant  Analisys  (LDA),  Local  Binary  Patterns 
Histograms  (LBPH),  dan  Support  Vector  Machine  (SVM). 
Dataset yang digunakan adalah  animal dataset sebanyak 500 
subjek  yang  dibagi  rata  menjadi  5  kelas.  Hasil  penelitian 
menunjukan  bahwa  dari  seluruh  metode  yang  dibandingkan 
dalam  melakukan  klasifikasi,  penggunaan  metode  CNN 

Pada  penelitian  [14]  pengenalan  tulisan  tangan  dilakukan 
dengan metode Optical Character Recognition (OCR). Prinsip 
pengerjaannya dibagi menjadi enam tahapan yaitu memperoleh 
gambar, preprocessing, segmentasi, ekstraksi fitur, klasifikasi, 
dan  post  processing.    OCR  menawarkan  pemrosesan  dengan 
kecepatan dan akurasi yang tinggi.  

Penelitian  [15]  menyatakan  bahwa  efisiensi  pengenalan 
tulisan  tangan  tergantung  dari  metode  ekstraksi  fitur  dan 
klasifikasi  yang  digunakan.  Pada  penelitian  tersebut  diulas 
mengenai  perbandingan  metode  dan  akurasi  dari  beberapa 
penelitian  terkait  pengenalan  tulisan  tangan  dari  berbagai 
bahasa dan metode klasifikasi yang terbaik adalah feed forward 
neural network dan counterlet extract. 

Penelitian lain mengenai pengenalan tulisan tangan pada [16] 
mengimplentasikan  Neural  Network 
kemudian 
dikembangkan  ke  CNN  untuk  mengklasifikasikan  dataset 
MNIST Digits. Hasil penelitian menunjukan bahwa arsitektur 
model  CNN 
dalam 
menyelesaikan  masalah  pengenalan  tulisan  tangan  berupa 
angka.  

dapat  meningkatkan 

performa 

yang 

Penelitian  [17]  mengenai  pengenalan  tulisan  tangan  pada 
dokumen  formulir  mengimplementasikan  CNN  pada  proses 
ekstraksi  fitur  SVM  pada  proses  klasifikasinya.  Sistem  yang 
dibangun  pada  penelitian 
ini  mencakup  preprocessing, 
segmentasi,  dan  pengenalan  karakter.  Pada  preprocessing 
penelitian  tersebut  digunakan  teknik  Median  filtering  untuk 
menghilangkan  noise  seperti  yang  akan  digunakan  pada 
penelitian ini. 

[3] 

Pada  penelitian 

telah  berhasil  dibangun  model 
pengenalan  tulisan  tangan  pada  level  prediksi  perkarakter 
menggunakan  CNN  yang  diimplementasikan  pada  form 
kuesioner  PAPI  Sensus  Penduduk  2020  dalam  memprediksi 
isian kuesionernya. Dalam tahapan form processing, digunakan 
template bounding box yang belum mengakomodasi jenis isian 
terbuka  yang  tiap  isian  karakternya  tidak  dibatasi  oleh  garis 
vertical.  Hasilnya,  pada  skema  prediksi  tersebut  memiliki 
akurasi sebesar 78.35 % dalam mengenali tulisan tangan pada 
kuesioner  Sensus  Penduduk  2020.  Pada  penelitian  tersebut 
belum  diterapkan  metode  segmentasi  seperti  yang  akan 
digunakan pada penelitian ini.  

IV. METODE PENELITIAN  
Bagian  ini berisi  uraian  metode penelitian  untuk  mencapai 
tujuan  penelitian.  Pada  penelitian  ini  digunakan  bahasa 
pemrograman  Python  dengan 
library  Tensorflow  dalam 
pemrosesannya. Berikut penyajiannya dalam Gambar 1. 

 2 / 9 

 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

dari CNN adalah convolutional layers, pooling layers, dan fully 
connected layers [20]. 

Pada convolutional layers, dilakukan proses konvolusi pada 
data input menggunakan filter/kernel untuk menghasilkan peta 
fitur. Filter tersebut mengambil piksel-piksel berdekatan dalam 
data image untuk kemudian diekstrak hubungan posisionalnya. 
Konvolusi  dilakukan  dengan  menggeser  filter  pada  seluruh 
bagian 
input  dimana  dilakukan  operasi  “dot”  sehingga 
menghasilkan peta fitur. 

Gambar 2. Ilustrasi Proses Konvolusi [21] 

Setelah  mendapatkan  peta  fitur,  pada  pooling 

layer 
dilakukan  downsampling  atau  pengurangan  dimensi  terhadap 
peta fitur sehingga dapat  mempercepat proses  komputasi  dan 
mencegah  overfitting.  Teknik  pooling  yang  digunakan  disini 
adalah  Max  pooling  yang  mengambil  nilai  terbesar  dari 
kumpulan nilai dalam filter. Berikut ilustrasinya pada Gambar 
3. 

Gambar 3. Ilustrasi Teknik Max Pooling [22] 

Layers  tersebut tergabung  menjadi bagian  feature learning 
pada arsitektur CNN. Sebelum pooling dilakukan, setiap peta 
fitur  melewati  fungsi  aktivasi  (ReLU),  seperti  diilustrasikan 
pada Gambar 4.  

Gambar 4. Arsitektur CNN [23] 

Selanjutnya,  peta  fitur  yang  masih  berbentuk  array 
multidimensi ditransformasikan menjadi sebuah vector kolom 
sebagai  input  untuk  fully  connected  layer.  Fully  connected 
layer  merupakan  layer  dimana  seluruh  neuron  aktivasi  dari 
layer  sebelumnya  terhubung  untuk  selanjutnya  diolah  untuk 
melakukan klasifikasi. 

Fungsi 

yang 
menggambarkan hubungan antara tingkat aktivitas internal dan 

aktivasi  merupakan 

sebuah 

fungsi 

 3 / 9 

Gambar 1. Metode Penelitian 

PEMBANGUNAN MODEL CNN 
A.  Persiapan Dataset 

Pada  tahapan  pembangunan  model  pengenalan  tulisan 
tangan, digunakan dataset Extended Modified NIST (EMNIST). 
Dataset EMNIST merupakan pengembangan dari dataset NIST 
Special Database 19 yang berisi kumpulan tulisan tangan baik 
dalam bentuk huruf maupun angka dari 500 lebih orang [19]. 
Dataset  EMNIST  dibagi  menjadi  enam  subdataset  yaitu 
EMNIST  Balanced,  EMNIST  by  Merge,  EMNIST  by  Class, 
EMNIST  Letters,  EMNIST  Digits,  dan  EMNIST  MNIST. 
Digunakannya dataset ini dikarenakan sifatnya yang gratis dan 
telah  banyak  digunakan  sebagai  data  latih  pada  beberapa 
penelitian mengenai pengenalan tulisan tangan yang dijelaskan 
pada bagian penelitian terkait. 

Pada penelitian ini, digunakan tiga subdataset yaitu EMNIST 
Balanced,  EMNIST  Letters,  dan  EMNIST  MNIST.  EMNIST 
Balanced terdiri atas karakter berupa huruf dan angka sebanyak 
112.800 data latih dan 18.800 data uji. EMNIST Letters terdiri 
atas  karakter  berupa  huruf  sebanyak  124.800  data  latih  dan 
20.800  data  uji.  EMNIST  MNIST  terdiri  atas  karakter  berupa 
angka sebanyak 60.000 data latih dan 10.000 data uji. Dengan 
digunakannya  ketiga  subdataset  tersebut  akan  dibangun  3 
model terpisah untuk memprediksi isian kuesioner yang sesuai 
dengan karakteristik subdataset masing-masing.  

B.  Permodelan 

Model  pengenalan  tulisan  tangan  yang  dibangun  pada 
penelitian 
ini  menggunakan  CNN.  Arsitektur  CNN 
mengakomodasi  proses  ekstraksi  fitur  serta  klasifikasi,  CNN 
lalu 
mengekstrak  peta 
menggunakannya untuk mengklasifikasikan image tersebut.  

image  2  dimensi 

fitur  dari 

Hampir  semua  arsitektur  CNN  mempunyai  prinsip  desain 
umum yang sama yaitu mengaplikasikan convolutional layers 
pada  data  input,  melakukan  downsampling  (Max  pooling) 
terhadap  dimensi  spasial  bersamaaan  dengan  menambah 
jumlah peta fitur secara periodik. Selain itu, terdapat juga fully 
connected  layers,  fungsi  aktivasi,  dan  loss  function.  Namun 
dari seluruh operasi dalam CNN tersebut, yang merupakan inti 

 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

dapat  berbentuk  baik  linier  maupun  non-linier  [24].  Fungsi 
inilah  yang  menentukan apakah neuron diaktifkan  atau  tidak. 
ReLU  merupakan  fungsi  aktivasi  nonlinear  pada  neural 
network    yang  mengembalikan  hanya  nilai  positif  saja  dan 
selain  itu  bernilai  0  [25].  Fungsi  ReLu  dapat  digambarkan 
dengan persamaan 1 berikut: 

𝑓(𝑥) = 𝑚𝑎𝑥(0, 𝑥)   

(1) 

 Selain  itu,  digunakan  fungsi  aktivasi  softmax  dalam 
melakukan  klasifikasi.  Softmax  merupakan  fungsi  yang 
menghitung nilai probabilitas dari setiap kategori/label. Dapat 
digambarkan penghitungannya pada persamaan 2 berikut: 

𝑠𝑜𝑓𝑡𝑚𝑎𝑥(𝑥)(cid:3036) =

(cid:3032)(cid:3299)(cid:3284)
(cid:3289)
(cid:3285)(cid:3128)(cid:3117)

∑

(cid:3032)(cid:3299)(cid:3285)

(2) 

Dalam upaya mencegah overfitting dan mempercepat proses 
learning, dilakukan teknik dropout. Dropout merupakan teknik 
regularisasi  neural  network  dimana  beberapa  neuron  akan 
dipilih secara acak dan tidak dipakai selama pelatihan.  

Gambar 5. Ilustrasi Teknik Dropout. 

yang 

arsitektur  model 

Tahapan  learning  model  dilengkapi  dengan  loss  function, 
optimizer,  dan  metrics.  Loss 
function  berfungsi  untuk 
menghitung  kerugian  performa  model  dalam  melakukan 
prediksi  terhadap  target.  Nilai  kerugian  dapat  diminimalisasi 
dengan  menggunakan  optimizer.  Pada  penelitian  ini,  loss 
function  yang  digunakan  merupakan  categorical  loss  entropy 
dengan  optimizer  adam.  Metrics  yang  digunakan  adalah 
accuracy  untuk  menggambarkan  seberapa  tepat  model  dapat 
memprediksi dengan benar. 
Dalam  menentukan 

akan 
diimplementasikan,  digunakan  pendekatan  percobaan  seperti 
pada  gambar  6.  Pertama  yaitu  menentukan  berapa  pasang 
convolutional-pooling yang akan digunakan. Dengan dimensi 
image 28x28 piksel, hanya akan dicoba apakah satu, dua, atau 
tiga pasang yang menunjukkan hasil optimal. Selain itu, tidak 
mungkin  diterapkan  4  pasang  karena  dimensi  image  akan 
menjadi terlalu kecil. Selanjutnya, dilakukan pendekatan yang 
serupa dalam  menentukan ukuran dari peta fitur, dense layer, 
dan  dropout.  Terakhir,  dilakukan  percobaan  dengan 
menerapkan  beberapa  fitur  lanjutan  yang  diklaim  dapat 
meningkatkan kinerja dari model yang telah dibuat [26] [27]. 
Fitur  lanjutan  yang  akan  diuji  berupa  pengubahan  konvolusi 
5x5  dengan  dua  konvolusi  3x3,  penggantian  pooling  layer 
dengan  konvolusi  5x5  dengan  strides  2,  penerapan  batch 
normalization, dan penerapan data augmentation.  

Gambar 6. Pendekatan dalam Menentukan Arsitektur CNN 

Percobaan  penentuan  arsitektur  model  yang  pertama  akan 
menggunakan  EMNIST  MNIST  untuk  membangun  model 
jumlah 
angka  dengan  mempertimbangkan  ukuran  serta 
kelasnya  yang lebih  kecil  sehingga  lama percobaan pelatihan 
model  relatif  akan  lebih  cepat.  Selanjutnya,  arsitektur  yang 
telah dipilih akan diterapkan untuk model huruf menggunakan 
EMNIST Letters dan model campuran menggunakan EMNIST 
Balanced. 

C.  Model Terpilih 

Model-model  yang 

telah  dibangun  selanjutnya  akan 
diimplementasikan  sesuai  dengan  karakteristiknya  masing-
masing. Model huruf untuk mengenali isian huruf, model angka 
untuk mengenali isian angka, dan model campuran digunakan 
untuk  mengenali  isian  yang  berisi  baik  huruf  maupun  angka. 
Selain  itu  akan  dihitung  akurasi  dari  tiap-tiap  model  dalam 
mengenali 
tangan.  Akurasi  didefinisikan  sebagai 
persentase dari data yang diklasifikasikan ke kelas yang benar 
dan dihitung menggunakan persamaan 1 berikut: 

tulisan 

𝐴𝑘𝑢𝑟𝑎𝑠𝑖 =

(cid:3021)(cid:3017)

(cid:3017)(cid:2878)(cid:3015)

(1) 

IMPLEMENTASI MODEL PADA KUESIONER 
A.  Perolehan Kuesioner 

Pada  tahapan  ini,  dilakukan  scanning  terhadap  kuesioner 
SAKERNAS  2020  yang  telah  diisi  oleh  petugas  cacah. 
Selanjutnya  image  hasil  scanning  akan  digunakan  untuk 
mengimplementasikan  model  pengenalan  tulisan tangan  yang 
telah dibangun. 

B.  Image Preprocessing 

Tahapan preprocessing pada image sangatlah penting dalam 
pengenalan  karakter  yang  meliputi  penghilangan  noise, 
binarisasi, skeletonisasi, dan normalisasi [28]. 

Noise  merupakan  tingkatan  nilai  tidak  diinginkan  pada 
image yang tidak memberikan nilai signifikan pada output [15]. 
Teknik  penghilangan  noise  yang  digunakan  adalah  median 
filter. Median filter mengurutkan semua piksel di area tertentu 
dan  mengganti  piksel  tengah  dengan  median  nilai  yang 
diurutkan.  

 4 / 9 

 
 
 
 
     
 
 
 
 
 
   
 
 
 
 
Binarisasi  merupakan  proses  mengubah  image  berwarna 
atau  grayscale  menjadi  image  bilevel.  Metode  threshholding 
adaptif  merupakan  metode  yang  digunakan  dalam  proses 
binarisasi  ini.  Metode  tersebut  menghitung  nilai  threshold 
untuk  sebagian  kecil  gambar  sehingga  memungkinkan  untuk 
didapatkan  nilai  berbeda  pada  tiap-tiap  bagian  dalam  image 
yang sama, 

Skeletonisasi merupakan proses morfologi yang mengubah 
image  menjadi  berepresentasi  selebar  satu  pixel 
tanpa 
memengaruhi  konektivitasnya.  Proses  penipisan  diperlukan 
untuk  mendapatkan  kerangka  image  dengan  menghilangkan 
pixel yang memiliki lebih dari dua neighbours. 

Normalisasi mencakup normalisasi ukuran dan kemiringan 
pada image. Normalisasi ukuran dilakukan dengan mengubah 
ukurannya menjadi ukuran kertas A4. Normalisasi kemiringan 
dilakukan  dengan  melakukan  matching  pada  image  input 
terhadap image yang telah dijadikan template. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

D.  Segmentasi 

Image kuesioner yang telah dilakukan preprocessing akan 
dicrop  pada bagian  isiannya,  lalu untuk setiap hasil cropping 
dilakukan  segmentasi  hingga  level  karakter  untuk  diprediksi 
menggunakan model.  

Gambar 9. Contoh Segmentasi Karakter 

E.  Pengenalan Karakter 

Image  karakter  hasil  segmentasi  dijadikan  sebagai  data 
input pada model pengenalan tulisan tangan sesuai dengan jenis 
isiannya untuk dikenali karakternya. Untuk jenis isian berupa 
checkbox,  metode  yang  digunakan  untuk  mengenali  karakter 
isiannya  adalah  dengan  menghitung  jumlah  piksel  berwarna 
hitam yang ada pada image hasil cropping berjenis checkbox. 
Berikut skemanya pada Gambar 8. 

Gambar 7. Contoh Matching Image Input untuk Halaman 2 Kuesioner 

C.  Cropping Zona Isian 

Setelah  image  melalui  tahap  preprocessing,  dilakukan 
cropping  untuk 
isian  pada  kuesioner 
menggunakan templat bounding box yang sudah dibuat dengan 
menandai koordinat dari setiap kotak isian. 

tiap-tiap  zona 

Gambar 10. Skema Pengenalan Karakter 

V.  KERANGKA PIKIR 

Kerangka  pikir  menggambarkan  alur  pemikiran  peneliti 
dalam  melakukan  penelitian  yang  disesuaikan  dengan 
permasalahan  penelitian.  Adapun  gambaran  mengenai 
kerangka pikir penelitian ini terdapat pada gambar 8. 

Gambar 8. Halaman 2 Kuesioner dan Zona Isiannya Menggunakan Templat 
Bounding Box 

Gambar 11. Kerangka Pikir Penelitian 

 5 / 9 

 
 
 
 
 
 
 
 
 
 
 
 
VI. HASIL DAN PEMBAHASAN 

Pencarian  arsitektur  model  CNN  yang  akan  digunakan 
diawali  dengan  menentukan  berapa  pasang  convolutional-
pooling  yang  optimal.  Berikut  hasil  akurasi  yang  didapatkan 
pada running model 1 pasang, 2 pasang, dan 3 pasang. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

kombinasi pasangan peta fitur 48-96 dengan besar dua kali lipat 
hanya berkinerja sedikit lebih baik dan tidak sebanding dengan 
tambahan  waktu  komputasinya.  Selanjutnya  dilakukan 
percobaan  untuk  mencari  ukuran  dense 
layer  dengan 
menerapkan  pasangan  peta  fitur  sebanyak  24  dan  48. 
Selanjutnya dilakukan percobaan untuk mencari ukuran dense 
layer dengan menerapkan pasangan peta fitur sebanyak 24 dan 
48. 

Gambar 12. Akurasi Setiap Pasangan Convolutional-Pooling 
TABEL I 
Hasil Percobaan dalam Menentukan Berapa Pasang Convolutional-Pooling 
Layer (20 epoch) 
Convolutional-Pooling  Akurasi Latih 
1 pasang 

Akurasi Validasi 
0.99167 

0.99996 

Gambar 14. Akurasi dari Setiap Ukuran Dense Layer 
TABEL III 
Hasil Percobaan dalam Menentukan Ukuran Dense Layer (20 epoch) 

Ukuran Dense Layer 

2 pasang 

3 pasang 

0.99996 

0.99325 

0.99981 

0.99442 

Dari  percobaan  di  atas, 

terlihat  bahwa  3  pasang 
convolutional-pooling  sedikit  lebih  baik  daripada  2  pasang. 
Namun  demi  mengefisiensikan  biaya  komputasi  tambahan, 
dipilih  2  pasang  convolutional-pooling  sebagai  parameter 
arsitektur  yang  pertama.  Selanjutnya  dilakukan  percobaan 
untuk menentukan besarnya peta fitur yang akan dimasukkan 
pada 2 pasang convolutional-pooling layers tersebut. 

0 

32 

64 

128 

256 

512 

1024 

2048 

Akurasi Latih 
0.99994 

Akurasi Validasi 
0.99233 

0.99996 

0.99350 

0.99996 

0.99367 

0.99996 

0.99998 

0.99996 

0.99458 

0.99517 

0.99483 

0.99990 

0.99475 

0.99998 

0.99342 

Terlihat bahwa  ukuran  yang terbaik adalah 256  unit layer. 
Langkah  selanjutnya  adalah  menerapkannya  pada  percobaan 
untuk mencari ukuran dropout yang optimal. 

Gambar 13. Akurasi dari Setiap Ukuran Peta Fitur 
TABEL II 
Hasil Percobaan dalam Menentukan Ukuran Peta Fitur (20 epoch) 

Ukuran Peta Fitur 

8 – 16 maps 

Akurasi Latih 
0.99981 

Akurasi Validasi 
0.99150 

16 – 32 maps 

0.99998 

0.99383 

24 - 48 maps 

0.99998 

0.99408 

32 – 64 maps 

0.99998 

0.99408  

48 – 96 maps 

0.99998 

0.99442 

64 – 128 maps 

0.99998 

0.99392 

Dari grafik di atas, dipilih bahwa 24 peta fitur pada lapisan 
konvolusi  pertama  dan  48  peta  fitur  pada  lapisan  konvolusi 
kedua merupakan kombinasi yang terbaik. Kombinasi peta fitur 
32-64 menghasilkan  akurasi  yang sama dengan  24-48  namun 
tidak  dipilih  karena  jumlahnya  yang  lebih  besar.  Selain  itu, 

Gambar 15. Akurasi dari Setiap Ukuran Dropout 
TABEL IV 
Hasil Percobaan dalam Menentukan Ukuran Dropout (30 epoch) 
Akurasi Validasi 
0.99325 

Akurasi Latih 
0.99998 

Ukuran Dropout 

0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.99985 

0.99442 

0.99944 

0.99442 

0.99881 

0.99433 

0.99710 

0.99417 

0.99475 

0.99475 

 6 / 9 

 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Dari implementasi model yang telah dilakukan, akan dihitung 
akurasi  dari  proses  segmentasi  karakter  serta  akurasi  model 
dalam  mengenali  karakter  isian.  Dengan  proses  segmentasi 
yang  benar,  akan  didapatkan  tepat  1  karakter  yang  utuh 
kemudian  dari  situ  akan  dihitung  akurasi  model  dalam 
mengenali karakter tulisan tangan. 

Berikut  hasil dari  penghitungan implementasinya disajikan 

pada tabel VII.  

TABEL VII 
Akurasi Implementasi Model pada Kuesioner 
Jenis 

Total Proses  Proses Benar  Akurasi 
82.85% 
1021 

846 

Segmentasi Karakter Secara 
Keseluruhan 
Segmentasi Karakter pada 
Kotak Isian Terbuka 
Segmentasi Karakter pada 
Kotak Isian Tertutup  
Pengenalan Karakter dari Model 
CNN  
Pengenalan Karakter pada Isian 
Checkbox 

685 

336 

846 

68 

519 

327 

705 

68 

75.76% 

97.32% 

83.33% 

100% 

Karena  model  yang  dibangun  masih 

terbatas  pada 
pengenalan  level  karakter,  saat  image  hasil  cropping  tidak 
tersegmentasi dengan benar, hasil pengenalan karakter menjadi 
tidak akurat. Hal ini dikarenakan model tidak berkemampuan 
untuk mengenali dua atau lebih karakter sekaligus dalam satu 
kali  proses  pengenalan.  Berikut  contohnya  pada  pengenalan 
isian terbuka di Gambar 10. Segmentasi berjalan dengan baik 
apabila  kontur  tiap-tiap  karakter  terpisah  dengan  jelas  atau 
mempunyai  batas  yang  jelas.  Apabila  kontur  karakter  tidak 
terpisah  dengan  jelas  atau  tidak  terdapat  batas  yang  jelas, 
segmentasi  akan  menghasilkan  segmen  yang  tidak  tepat 
sehingga  dua  atau  lebih  karakter  yang  seharusnya  terpisah 
menjadi tergabung sebagai satu karakter. 

0.6 

0.7 

0.98971 

0.99392 

0.98258 

0.99133 

Terlihat bahwa dropout sebesar 0,3 merupakan ukuran yang 
paling baik. Dengan ini, telah didapatkan arsitektur CNN basic 
(tanpa  penerapan  fitur  lanjutan)  yang  menunjukkan  hasil 
terbaik.  Namun,  akan  dilakukan  percobaan  terakhir  dengan 
menerapkan  fitur  lanjutan  untuk  mengetahui  apakah  terdapat 
perbedaan yang signifikan terhadap akurasi modelnya. 

Gambar 16. Contoh Hasil Segmentasi dan Klasifikasinya 
TABEL V 
Hasil Percobaan dalam Fitur Lanjutan (35 epoch) 

Fitur Lanjutan 

CNN basic 

Akurasi Latih 
0.99779 

Akurasi Validasi 
0.99517 

32C3-32C3 

0.99854 

0.99550 

32C5S2 

both+BN 

0.99925 

0.99492 

0.99919 

0.99583 

both+BN+DA 

0.99616 

0.99675 

Terlihat bahwa penerapan fitur lanjutan berupa pengubahan 
konvolusi 5x5 menjadi dua konvolusi 3x3, penggantian pooling 
layer dengan konvolusi 5x5 dengan strides 2, penerapan batch 
normalization, dan penerapan data augmentation terbukti dapat 
meningkatkan  akurasi  model  CNN  basic.  Oleh  karena  itu, 
arsitektur  model  yang  dipilih  untuk  diimplementasikan  pada 
pembangunan  model  CNN  di  penelitian  ini  adalah  berupa 
convolutional 
fitur  dilanjutkan 
convolutional  layer  dengan  48  peta  fitur,  dense  layer  dengan 
ukuran  256  unit,  dropout  sebesar  30%  serta  penerapan  fitur 
lanjutan. 

layer  dengan  24  peta 

Dengan  arsitektur  terpilih  yang  telah  diimplementasikan 
untuk  pembangunan  ketiga  model,  didapatkan  akurasi  untuk 
tiap-tiap model pada tabel VI. 

TABEL VI 
Akurasi Model Pengenalan Tulisan Tangan yang Telah Dibangun 
Dataset  
EMNIST Balanced 

Akurasi 
89% 

Campuran 

Model 

Huruf 

Angka 

EMNIST Letters 

EMNIST MNIST 

95% 

99% 

Selanjutnya,  model  diimplementasikan  pada 

image 
kuesioner  halaman  2  untuk  dilakukan  pengenalan  karakter 
isiannya. Dipilihnya halaman 2 dikarenakan halaman tersebut 
sudah  mengandung  seluruh  jenis  isian  pada  kuesioner  serta 
jumlah karakter isiannya paling banyak dibanding halaman lain.  

Gambar 17. Contoh Hasil Segmentasi Isian Terbuka dan Klasifikasinya 
Selain  itu,  pada  isian  tertutup  juga  terdapat  permasalahan 
segmentasi  karakter.  Hal  tersebut  dikarenakan  karakter  yang 
seharusnya  merupakan  satu  karakter  utuh  terputus.  Berikut 
contohnya pada pengenalan isian tertutup di Gambar 11. 

 7 / 9 

 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

[10]   Janalta Interactive Inc., “definition,” 7 March 2021. [Online]. 
Available: https://www.techopedia.com/definition/30325/deep-
learning. 

[11]   L. Yann and Y. Bengio, “Convolutional Networks for Images, Speech, 
and Time-Series,” The handbook of brain theory and neural networks 
MIT Press, p. 255–258, 1995.  

[12]   Google Research, “Tensorflow: Large-Scale Machine Learning on 

Heterogeneous Distributed Systems,” Preliminary White Paper, 2015.  

[13]   T. Trnovszky, P. Kamencay, R. Orjesek, M. Benco and P. Sykora, 
“Animal Recognition System Base On Convolutional Neural 
Network,” Digital Image Processing And Computer Graphics, pp. 
517-525, 2017.  

[14]   M. S. S. Bhagat, M. A. R. Joshi, M. V. S. Gajbhiye, M. S. R. 

Nandanwar and P. M.M.Ingle, “Handwritten Character Detection 
Using Optical Character Recognition Method,” International Journal 
for Research in Applied Science & Engineering Technology 
(IJRASET), vol. 6, pp. 4724-4726, 2018.  

[15]   S. N. R. S and A. S, “Handwritten Character Recognition – A 

Review,” International Journal of Scientific and Research 
Publications, vol. 5, no. 3, 2015.  

[16]   W. Zhu, “Classification of MNIST Handwritten Digit Database using 

Neural Network,” 2018.  

[17]   Darmatasia and M. I. Fanany, “Handwriting Recognition on Form 

Document Using Convolutional Neural Network and Support Vector 
Machines (CNN-SVM),” 2017 Fifth International Conference on 
Information and Communication Technology (ICoICT), p. 1, 2017.  

[18]   W. Zhu, “Classification of MNIST Handwritten Digit Database using 

Neural Network,” 2018.  

[19]   G. A. S. T. J. &. v. S. A. Cohen, “EMNIST: An extension of MNIST 

to handwritten letters,” ArXiv., 2017.  

[20]   M. A. Hossain and M. M. Ali, “Recognition of Handwritten Digit 
using Convolutional Neural Network (CNN),” Global Journal of 
Computer Science and Technology: D Neural & Artificial Intelligence, 
vol. 19, no. 2, pp. 27-33, 2019.  

[21]   H. &. S. S. &. N. R. &. O. Y. &. S. Yakura, “Malware Analysis of 

Imaged Binary Samples by Convolutional Neural Network with 
Attention Mechanism,” The 8th ACM Conference on Data and 
Application Security and Privacy (ACM CODASPY '18), pp. 127-134, 
2018.  

[22]   D. Nielsen, “Deep Learning Cage Match: Max Pooling vs 

Convolutions,” Medium, 9 9 2018. [Online]. Available: 
https://medium.com/@duanenielsen/deep-learning-cage-match-max-
pooling-vs-convolutions-e42581387cb9. [Accessed 26 3 2021]. 

[23]   The MathWorks, Inc., “Convolutional Neural Network,” The 

MathWorks, Inc., [Online]. Available: 
https://www.mathworks.com/discovery/convolutional-neural-network-
matlab.html. [Accessed 26 3 2021]. 

[24]   T. Nurhikmat, Implementasi Deep Learning Untuk Image 

Classification Menggunakan Algoritma Convolutional Neural 
Networks (CNN) Pada Citra Wayang Golek, Yogyakarta: Universitas 
Islam Indonesia, 2018.  

Gambar 18. Contoh Hasil Segmentasi Isian Tertutup yang Karakternya 
Terputus 

VII. 

PENUTUP 

Telah  berhasil  dibangun  tiga  model  pengenalan  tulisan 
tangan  menggunakan  CNN  untuk  mengenali  isian  pada 
kuesioner  SAKERNAS  2020.  Saat  akurasi  model  di  tes 
menggunakan data uji EMNIST, model untuk mengenali isian 
huruf mempunyai akurasi sebesar 95%, model untuk mengenali 
isian angka mempunyai akurasi sebesar 99%, dan model untuk 
mengenali isian baik huruf maupun angka mempunyai akurasi 
89%.  

Saat model diimplementasikan pada image kuesioner, hasil 
pengenalan karakter yang dihasilkan sudah cukup akurat yaitu 
sebesar  83.33%.  Pada  proses  segmentasi  karakter,  secara 
keseluruhan  didapatkan  akurasi  sebesar  82.85%.  Pada  isian 
terbuka,  terdapat  temuan  pada  proses  segmentasi  dimana 
beberapa karakter yang seharusnya terpisah tergabung menjadi 
satu  karakter  karena  garis  tulisan  antar  karakter-karakter 
tersebut  menyatu  atau  tidak  memiliki  ruang  kosong  sebagai 
pemisahnya sehingga akurasi yang didapatkan sebesar 75.76%. 
Sedangkan  pada  isian  tertutup,  proses  segmentasi  terkendala 
pada  karakter  yang  terputus  sehingga  didapatkan  akurasi 
97.32%.  Selain  itu,  hal  tersebut  dapat  dikarenakan  parameter 
image  preprocessing  yang  kurang  baik  atau  kualitas  input 
image  yang  buruk.  Oleh  karena  itu,  perlu  diperhatikan  pula 
mengenai  perangkat  yang  digunakan  untuk  proses  scanning 
kuesioner dan kondisi dari kuesioner yang digunakan. 

DAFTAR PUSTAKA 

[1]   J. Harber, M. A. Nacenta and S. Carpendale, “Paper vs. Tablets: The 
Effect of Document Media in Co-located Collaborative Work,” AVI 
'14: Proceedings of the 2014 International Working Conference on 
Advanced Visual Interfaces, pp. 89-96, 2014.  

[2]   A. Sellen and R. Harper, The Myth of The Paperless Office, MIT 

Press, 2003.  

[3]   A. L. Irfiansyah, “Pembangunan Model Tulisan Tangan Pada Form 
Kuesioner PAPI (Studi Kasus Kuesioner Sensus Penduduk 2020),” 
2020. 

[4]   The World Bank Group, “DIME Wiki,” 2021. [Online]. Available: 

[25]   T. Wood, “Activation Function,” DeepAI, [Online]. Available: 

https://dimewiki.worldbank.org/wiki/Pen-and-
Paper_Personal_Interviews_(PAPI). 

[5]   T. R. Ostrach, “Typing Speed: How Fast is Average,” 2012.  

[6]   B. Zhang, “Computer Vision vs. Human Vision,” in Proc. 9th IEEE 

Int. Conf. on Cognitive Informatics (ICCI’10), 2010.  

[7]   N. Pinto, D. D. Cox and J. J. D. Carlo, “Why is Real-World Visual 
Object Recognition Hard?,” PLoS Computational Biology, pp. 151-
156, 2008.  

[8]   K. A. Hamad and M. Kaya, “A Detailed Analysis of Optical Character 

Recognition Technology,” International Journal of Applied 
Mathematics, Electronics and Computers, pp. 244-249., 2016.  

[9]   N. Sharma, T. Patnaik and B. Kumar, “Recognition for Handwritten 
English Letters: A Review,” Vols. 2, no. 7, pp. 318-321, 2013.  

https://deepai.org/machine-learning-glossary-and-terms/activation-
function#:~:text=What%20is%20an%20Activation%20Function,%22
%2C%20otherwise%20it%20does%20nothing.. [Accessed 27 3 2021]. 

[26]   J. T. Springenberg, A. Dosovitskiy, T. Brox and M. Riedmiller, 

“Striving for Simplicity: The All Convolutional Net,” in International 
Conference on Learning Representations, San Diego, 2015.  

[27]   S. Ioffe and C. Szegedy, “Batch Normalization: Accelerating Deep 

Network Training by Reducing Internal Covariate Shift,” 2015.  

[28]   A. &. S. D. Pal, “Handwritten English Character Recognition Using 

Neural Network,” vol. 1, no. 2, p. 141–144., 2010.  

[29]   A. George and F. Gafoor, “Contourlet Transform Based Feature 

Extraction For Handwritten Malayalam Character Recognition Using 
Convolutional Neural Network,” International Journal of Industrial 
Electronics and Electrical Engineering, vol. 2, no. 4, pp. 19-22, 2014.  

 8 / 9 

 
 
  
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

 9 / 9 

 
 
 
 
 
"
221710079,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Deteksi Perkebunan Kelapa Sawit di Indonesia
Menggunakan Citra Satelit Optik
Sentinel-2 dan Landsat-8
Studi Kasus: Kabupaten Rokan Hulu, Provinsi Riau
Yunita Nurmasari (221710079, 4SD1)

Dosen Pembimbing: Dr. Eng. Arie Wahyu Wijayanto, SST., MT.

kelapa

perkebunan

Ringkasan— Sebagai produsen minyak sawit

terbesar di
lebih banyak
dunia, Indonesia menyediakan pasokan global
daripada gabungan seluruh negara-negara lain. Oleh karena itu,
Indonesia membutuhkan informasi yang akurat dan andal
dalam memantau
sawit. Metode
penginderaan jauh merupakan salah satu pendekatan yang
menjanjikan dikarenakan efisiensi sumber dayanya dalam hal
tenaga, biaya, dan waktu dibandingkan dengan survei
konvensional. Penelitian ini bertujuan mendeteksi perkebunan
kelapa sawit melalui citra satelit multispektral di Kabupaten
Rokan Hulu, Riau, salah satu produsen minyak sawit terbesar di
Indonesia. Data diambil dari dua satelit optik beresolusi
menengah Sentinel-2 dan Landsat-8 yang terdiri dari 1.800 titik
yang diklasifikasikan ke dalam 6 kelas tutupan lahan. Penelitian
ini membandingkan performa metode klasifikasi machine
learning Random Forest, serta deep learning arsitektur Multi
Layer Perceptron (MLP) dan Convolutional Neural Network
(CNN). Dengan penggunaan pita multispektral dan indeks
diperoleh
komposit
pengklasifikasi
92%.
Penelitian ini bermanfaat bagi pemerintah dalam membantu
menyediakan data pendukung statistik perkebunan yang lebih
efisien.

antarkelas,
akurasi mencapai

sebagai

terbaik

dengan

penciri

fitur

Kata Kunci— Penginderaan jauh, kelapa sawit, Random

Forest, Multi Layer Perceptron, Convolutional Neural Network.

I. LATAR BELAKANG

Kelapa sawit (Elaeis guineensis) merupakan salah satu
tanaman minyak nabati paling produktif di dunia [1]. Minyak
sawit digunakan dalam berbagai macam produk makanan dan
non-makanan. Karena kebutuhannya, permintaan pasar global
terhadap minyak sawit
telah tumbuh secara eksponensial
selama 50 tahun terakhir [2]. Departemen Pertanian Amerika
Serikat (USDA) secara resmi menyatakan bahwa Indonesia
menduduki peringkat pertama sebagai negara penghasil kelapa
sawit di dunia [3]. Pada tahun 2019, Indonesia memproduksi
42,5 juta ton atau setara 58 persen dari seluruh pasokan
minyak sawit dunia [4]. USDA memetakan produksi minyak
sawit menurut provinsi dan menunjukkan bahwa provinsi
Riau memberikan persentase produksi
terbesar di antara
provinsi lainnya. Senada dengan USDA, Badan Pusat Statistik
(BPS) mencatat pada tahun 2019, produksi minyak sawit
Indonesia mencapai 45,86 juta ton di mana Riau menempati
urutan pertama dengan produksi 9,12 juta ton [5].

Pengumpulan informasi tentang perkebunan kelapa sawit
sangat penting untuk memantau perubahan luas perkebunan
kelapa sawit maupun jumlah produksinya. Pemanfaatan
teknologi penginderaan jauh sebagai alternatif dari metode
konvensional
telah mengantarkan banyak peneliti dalam
teknik dan cara untuk meningkatkan
menemukan berbagai
pengumpulan data perkebunan kelapa sawit [6]. Pendekatan
konvensional untuk mendeteksi perkebunan kelapa sawit saat
ini hanya melalui Survei Perusahaan Perkebunan (SKB). Data
utama yang disediakan oleh Survei Perusahaan Perkebunan
dalam Publikasi Statistik Kelapa Sawit Indonesia adalah luas
lahan, jumlah produksi, dan status tanaman perkebunan kelapa
sawit. Akan tetapi, pendekatan tersebut membutuhkan lebih
banyak tenaga, waktu, dan biaya khususnya untuk wilayah
perkebunan yang luas. Di samping itu, terdapat pula wilayah
yang sulit dijangkau (remote area) yang menjadi tantangan
tersendiri.

Berbagai

teknik telah dikembangkan untuk memetakan
tutupan lahan perkebunan kelapa sawit menggunakan citra
satelit resolusi menengah ataupun tinggi. Beberapa penelitian
[7-12] mengungkapkan bahwa deteksi perkebunan kelapa
sawit dapat dilakukan dengan menggunakan informasi pita
spektral. Penelitian-penelitian tersebut menggunakan berbagai
macam pendekatan penginderaan jauh yang berbeda seperti
jenis satelit, pita spektral, indeks komposit, dan metode yang
digunakan. Setiap jenis satelit memiliki resolusi dan pita
multispektral
yang
didapatkan pun bisa berbeda pula. Hal ini membuka peluang
bagi peneliti untuk melakukan deteksi perkebunan kelapa
sawit dengan mengkombinasikan pita multispektral dan
indeks komposit, serta membandingkan performa beberapa
metode pengklasifikasi. Pada penelitian ini, peneliti berfokus
pada penyediaan estimasi luas lahan perkebunan kelapa sawit
langkah awal penyediaan data yang lebih lanjut,
sebagai
misalnya estimasi
jumlah produksi, klasifikasi berdasarkan
status usia tanaman, produktivitas tanaman, dsb.

berbeda-beda,

sehingga

yang

hasil

II. TUJUAN DAN BATASAN PENELITIAN

A. Tujuan

Berdasarkan latar belakang yang telah diuraikan, adapun

tujuan penelitian skripsi ini adalah sebagai berikut:

1. Mengidentifikasi fitur terbaik penciri lahan kelapa sawit.

1 / 8

2. Mengidentifikasi model

klasifikasi

terbaik

dalam

mendeteksi perkebunan kelapa sawit.

3. Mengestimasi luas lahan perkebunan kelapa sawit.

B. Batasan Penelitian

Dalam proses identifikasi model klasifikasi lahan kelapa
terbatas pada
sawit, penentuan label pada penelitian ini
validasi visual secara tidak langsung oleh peneliti dengan alat
bantu pembanding berupa citra satelit resolusi tinggi Google
Earth, peta Google Maps, tangkapan kamera Google Street
View, dan data pendukung lainnya. Hal ini dikarenakan tidak
tersedianya data administratif resmi tutupan lahan secara rinci
untuk kelas perkebunan kelapa sawit pada setiap titik-titik
koordinat di Indonesia dan keterbatasan jangkauan survei
langsung di lapangan.

Semua model diuji pada data label yang sama untuk
mencapai prinsip fairness and equality. Metode pemeriksaan
lapangan (ground checking) yang sesuai kaidah kerangka
sampel acak pada skala yang lebih luas belum dapat dilakukan
karena kendala pandemi Covid-19 dan keterbatasan sumber
daya manusia. Langkah penelitian selanjutnya adalah bekerja
sama dengan instansi terkait, misalnya Badan Pusat Statistik,
untuk dapat melakukan tahapan validasi lapangan.

III. PENELITIAN TERKAIT

Dalam beberapa tahun terakhir, metode penginderaan jauh
dengan citra satelit optik beresolusi tinggi menjadi semakin
populer dan banyak diaplikasikan termasuk untuk mendeteksi
pohon kelapa sawit secara otomatis. Triscowati & Wijayanto
[13] menyatakan bahwa metode klasifikasi gambar citra satelit
terbagi menjadi
statistika
konvensional; (2) metode machine learning populer dan (3)
metode deep learning. Di Indonesia, sudah terdapat banyak

tiga kelompok:

(1) metode

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

penelitian terkait
teknik penginderaan jauh baik dengan
metode statistik konvensional, machine learning maupun deep
learning. Namun, belum banyak penelitian terkait deteksi
perkebunan kelapa sawit dengan teknik penginderaan jauh.
Sebaliknya, penelitian terkait
telah banyak
dilakukan di luar negeri. Gambar 1 menunjukkan peta literatur
yang berisi penelitian terkait topik skripsi ini.

topik tersebut

Gambar 1. Peta literatur (literature map).

ini dimuat ke dalam tabel

Selanjutnya, jurnal penelitian yang bersinggungan dengan
topik skripsi
literatur sehingga
terlihat perbedaan antara penelitian yang telah dilakukan
sebelumnya dengan penelitian ini. Tabel 1 menunjukkan tabel
perbandingan literatur.

No Pengarang, Tahun
G. Dwinita & S. H.
1
Murti, 2016 [10]
H. A. Setyowati & S.
H. Murti, 2015 [18]

2

Tujuan
Identifikasi dan estimasi
produksi kelapa sawit.
Identifikasi dan estimasi
produksi kelapa sawit.

Metode
Transformasi Indeks
Vegetasi, RLS
Maximum Likelihood,
RLS

TABEL I
TABEL LITERATUR

Sumber Data

Jenis

Akses

Lokasi Studi

Satelit SPOT-5

Optik

Berbayar

Bengkulu, Indonesia.

Satelit SPOT-6

Optik

Berbayar

J. S. H. Lee dkk, 2016
[9]

Deteksi perkebunan
kelapa sawit.

CART & Random Forest

Satelit Landsat 8 Optik

A. Tridawati & S.
Darmawan, 2015 [11]

Deteksi perkebunan
kelapa sawit.

Maximum Likelihood,
Minimum Distance, SVM

Satelit Landsat 8 Optik

Terbuka dan
gratis

Terbuka dan
gratis

Kabupaten Indragiri
Hulu, Riau, Indonesia.
Kec. Tripa, Aceh,
Indonesia.

Indonesia.

3

4

5

6

7

A. Descals dkk, 2019
[25]

Deteksi perkebunan
kelapa sawit

Random Forest

T. Sarzynski dkk, 2020
[26]

Deteksi perkebunan
kelapa sawit

S. Agustin dkk, 2020
[27]

8 M. Freudenberg dkk,

2019 [12]

9

Topik Skripsi

Pemetaan wilayah
perkebunan kelapa sawit.
Deteksi perkebunan
kelapa sawit.

Random Forest

SVM, CNN

Sentinel-1 &
Sentinel-2

PALSAR-2 &
JAXA

Radar &
Optik

Terbuka dan
gratis

Riau, Indonesia

Radar &
Optik

Terbuka dan
gratis

Jambi, Riau, &
Sumatera Selatan

Satelit Ikonos

Optik

Berbayar

Indonesia.

CNN arsitektur U-Net.

WorldView-2

Optik

Berbayar

Jambi, Indonesia.

Deteksi perkebunan
kelapa sawit

Random Forest, MLP,
CNN

Sentinel-2 &
Landsat 8

Optik

Terbuka
dan gratis

Rokan Hulu, Riau

2 / 8

IV. METODE PENELITIAN

A. Wilayah Studi

Riau adalah provinsi penghasil kelapa sawit

terbesar di
Indonesia. Menurut BPS, pada tahun 2019 tercatat produksi
minyak sawit Indonesia mencapai 45,86 juta ton, di mana
Provinsi Riau menempati urutan pertama dengan jumlah
produksi mencapai 9,12 juta ton. Rokan Hulu merupakan
kabupaten yang memiliki
lahan perkebunan kelapa sawit
terluas diantara kabupaten & kota di Riau yaitu mencapai
480.665 hektar. Oleh karena itu, Rokan Hulu dipilih sebagai
wilayah studi kasus atau Region of Interest (ROI).

B. Pengumpulan Data

a) Data Citra
Data yang dikumpulkan adalah citra satelit Sentinel-2 Multi
Spectral Instrument dan Landsat-8 Surface Reflectance Tier
1 sepanjang tahun 2019 dari katalog data Google Earth
Engine (GEE). Selanjutnya citra satelit memasuki proses
seleksi awan. Pada citra Sentinel-2, terpilih citra terbaik
pada bulan Juli 2019 dengan persentase awan sebesar
0,05%, 1,26 %, 5,91 %, dan 7,34 %. Sedangkan pada citra
Landsat-8,
terpilih citra terbaik pada bulan Maret 2019
dengan persentase awan sebesar 7,15%, 18,3 %, 23,03 %,
dan 23,2 %. Setelah itu, citra satelit memasuki proses cloud
masking, lalu citra satelit siap untuk diolah.

b) Data Sampel
Titik sampel yang diambil terbagi ke dalam enam kelas
target yang terdiri dari sawit, hutan, lahan terbuka hijau
(non sawit dan non hutan), tanah, lahan terbangun (jalan
dan bangunan), dan badan air. Sebanyak 1.800 titik sampel
tersebar di seluruh Rokan Hulu, di mana setiap kelas
memiliki ukuran yang sama yaitu 300 titik. Kemudian titik
sampel dibagi ke dalam data latih dan data uji dengan
persentase 70 persen data latih dan 30 persen data uji.

c) Metode Pemberian Label
Metode pemberian label dilakukan secara manual oleh
peneliti, di mana setiap tahapannya mengacu pada tahapan
yang umumnya dilakukan dalam penelitian penginderaan
jauh. Perlu diketahui bahwa saat ini belum tersedia data
lahan perkebunan kelapa sawit pada
administratif resmi
setiap titik-titik koordinat di
Indonesia. Namun pohon
kelapa sawit mudah dideteksi karena karakteristik dan
morfologinya yang unik [7], yaitu memiliki tajuk berbentuk
bintang jika dilihat dari atas. Berdasarkan beberapa
penelitian sebelumnya [11], [26], [30]-[35], berikut adalah
metode
dalam melakukan
pelabelan titik sampel.

dilakukan

validasi

yang

1. Melakukan pengecekkan antara titik sampel dengan
label resmi yang tersedia saat ini yaitu peta tutupan lahan
dari Kementerian Lingkungan Hidup dan Kehutanan
(KLHK). Peneliti memastikan titik sampel kelapa sawit
berada dalam cakupan kelas perkebunan umum. Selain itu
pengecekkan
lokasi
perkebunan kelapa sawit oleh PTPN V (Riau).

dilakukan

terhadap

juga

data

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

2. Melakukan pemeriksaan visual koordinat titik sampel
melalui satelit resolusi lebih tinggi Google Earth, Google
Maps, dan Google Street View. Gambar 2 menunjukkan
perbandingan antara tangkapan citra satelit Sentinel-2
dengan citra satelit resolusi yang lebih tinggi.

(a)

(b)

Gambar 2. Perbandingan tangkapan citra satelit optik Sentinel-2 (a)

dengan citra satelit resolusi lebih tinggi CNES Airbus (b).

C. Pengumpulan Fitur

Koleksi fitur terdiri dari beberapa pita multispektral dan
indeks komposit
sebagai variabel prediktor. Sentinel-2
memiliki pita spektral pada tiga resolusi spasial yang berbeda
dan Landsat-8 memiliki resolusi spasial yang sama untuk
semua pita spektral. Tabel 2 menunjukkan daftar pita spektral
terpilih dari satelit Sentinel-2 dan landsat-8.

TABEL II
DAFTAR PITA SPEKTRAL SENTINEL-2 DAN LANDSAT-8

Sentinel-2

Landsat-8

Deskripsi

Nama Pita

Resolusi

Nama pita

Resolusi

B2

B3

B4

B7

B8

B8A

B11

10 m

10 m

10 m

20 m

10 m

20 m

20 m

B2

B3

B4

-

B5

-

B6

30 m

30 m

30 m

-

30 m

Biru (Blue)

Hijau (Green)

Merah (Red)

Vegetation Red
Edge (VRE)
Near Infrared
(NIR)

-

Narrow NIR

30 m

Shortwave
Infrared (SWIR)

Indeks komposit terbentuk dari transformasi beberapa pita
spektral. Indeks komposit biasanya digunakan untuk tugas
tertentu [14]. Karena metode yang diusulkan berorientasi pada
segmentasi hijau, yaitu pohon kelapa sawit, indeks vegetasi
merupakan fitur yang sesuai untuk tujuan ini. Indeks vegetasi
adalah indeks yang terdiri dari beberapa pita satelit yang
dihitung menggunakan transformasi spektral
tertentu [15].
Indeks vegetasi Normalized Difference Vegetation Index
(NDVI) dan Enhanced Vegetation Index (EVI) banyak
digunakan pada penelitian sebelumnya terkait dengan deteksi
dan klasifikasi perkebunan kelapa sawit [10], [12], [15].
NDVI merupakan transformasi indeks yang menggunakan pita
NIR dan Red. NDVI mampu untuk mengevaluasi keberadaan

3 / 8

lahan vegetasi hijau pada target amatan [16]. EVI adalah
transformasi indeks yang menggunakan pita NIR, Red, dan
Blue. EVI bekerja secara akurat terutama di wilayah vegetasi
yang rapat dengan kanopi pohon yang lebat [17]. Indeks
komposit Pan-sharpening NDVI (PANNDVI) dan Modified
Soil Adjusted Vegetation Index 2 (MSAVI2) digunakan untuk
mengestimasi produksi kelapa sawit [18]. Infrared Percentage
Vegetation Index (IPVI) dan Soil Adjusted Vegetation Index
(SAVI) juga digunakan pada penelitian sebelumnya [15].
Selain indeks vegetasi, indeks komposit lain juga diperlukan
untuk membedakan kelas tanah dan lahan terbangun. Salah
satu
untuk
mendeteksi bangunan adalah Normalized Difference Built-Up
Index (NDBI) [19].

yang banyak

digunakan

komposit

indeks

D. Metode Analisis
fitur

Analisis

level

yang

secara

deskriptif

dilakukan

digunakan

tertentu. Metode

dengan
menempatkan setiap pita multispektral dan indeks komposit
pada
untuk
tersebut adalah Jenks Natural
menentukan rentang level
Breaks. Natural Breaks adalah jenis metode klasifikasi
optimal yang akan meminimalkan varians dalam kelas dan
memaksimalkan
ini
mengelompokkan data secara alami berdasarkan informasi
yang melekat dalam data (data-driven). Rentang dibagi ke
dalam 5 level yaitu very low, low, medium, high, dan very
high [20]. Goodness of Variance Fit (GVF) menunjukkan
variansi antar kelas di mana pembagian kelas akan optimum
jika nilai GVF mendekati 1.

antarkelas.

Metode

varians

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Sedangkan algoritma yang digunakan untuk metode deep
learning adalah Fully Connected Multi Layer Perceptron
(MLP) dan Convolutional Neural Network (CNN). MLP
merupakan algoritma dasar dari arsitektur Neural Network,
untuk menentukan fitur mana yang paling berkorelasi dengan
kelas tertentu, di mana setiap layer menyatukan semua node
menjadi satu dimensi [23]. Arsitektur MLP terdiri dari 1 input
layer, 3 hidden layer, dan 1 output layer. Fungsi aktivasi
yang digunakan pada bagian input dan hidden layer adalah
Rectified Linear Unit (ReLU). Fungsi ReLU lebih efektif
aktivasi
untuk klasifikasi multikelas. Sedangkan fungsi
layer untuk mendapatkan
Softmax digunakan pada output
probabilitas di kelas
akan
terklasifikasi. CNN merupakan operasi konvolusi yang
menggabungkan beberapa lapisan pemrosesan, menggunakan
dan
beberapa
terinspirasi oleh sistem saraf biologis [24]. Arsitektur yang
digunakan adalah CNN 1-D dengan konvolusi domain
spektral. Arsitektur terdiri dari dua lapisan konvolusional, dan
masing-masing diikuti oleh lapisan Pooling dan Dropout.
Pooling layer dan dropout membantu mengurangi overfitting
dan meningkatkan efisiensi kinerja mesin [25]. Berdasarkan
hasil
dilakukan
tuning
sebelumnya, ditetapkan Max Pooling sebagai layer pooling
dan LeCun Uniform sebagai kernel
initializer. Setelah
memasuki
selanjutnya adalah fully
layer
tahap flatten,
connected dengan arsitektur MLP yang sama yang telah
dibuat sebelumnya.

hyper-parameter

target mana

beroperasi

elemen

paralel

secara

setiap

input

yang

telah

yang

Analisis citra dilakukan dengan algoritma klasifikasi
terbimbing (supervised classification). Untuk metode machine
learning, algoritma Random Forest
(RF) dipilih karena
terbaik dibandingkan algoritma ML
menghasilkan model
lainnya pada deteksi perkebunan kelapa sawit [21]. Kelebihan
lain dari algoritma ini yaitu dapat memproses data dalam
jumlah besar, dengan variabel yang banyak, baik yang sensitif
[22].
terhadap

overfitting maupun multikolinearitas

V. KERANGKA PIKIR

Penelitian ini berfokus pada pemanfaatan citra satelit dan
metode penginderaan jauh untuk mendeteksi perkebunan
kelapa sawit. Kerangka pikir menggambarkan alur logika
penelitian mulai dari permasalahan, solusi yang diusulkan,
sasaran/tujuan penelitian, hingga indikator yang digunakan.
Kerangka pikir pada penelitian ini mengacu pada [37]-[38]
dan diilustrasikan pada Gambar 3.

Gambar 3. Kerangka pikir

4 / 8

VI. HASIL DAN PEMBAHASAN

A. Identifikasi Fitur Terbaik Penciri Kelapa Sawit

Kemudian

Semua pita multispektral dan indeks komposit Sentinel-2
dan Landsat-8 distandarisasi dan dinormalisasi. Nilai GVF
setiap fitur dihitung sebelum menentukan rentang lima level.
Semua fitur menunjukkan nilai GVF mendekati 1, yang
artinya pembagian lima level rentang sudah optimal.
fitur

dan
diklasifikasikan menurut levelnya. Lima kategori level terbagi
low, medium, high, dan very high.
menjadi: very low,
Tujuannya adalah untuk mengetahui bagaimana distribusi fitur
di setiap kelas target. Gambar 4 menunjukkan rentang fitur
untuk masing-masing kelas target pada citra satelit Sentinel-2,
dan Gambar 5 pada citra Landsat-8.

dirata-ratakan

tersebut

semua

Gambar 4. Rentang fitur pada citra Sentinel-2.

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

NDBI berada pada level low, EVI pada level high, dan semua
indeks vegetasi kecuali EVI berada pada level very high.
Hanya pita NIR yang berbeda, yaitu berada pada level medium
di Sentinel-2, dan level high di Landsat-8. Namun keduanya
memiliki interpretasi yang sama: objek kelapa sawit paling
banyak memantulkan pita NIR dan memiliki nilai
indeks
vegetasi yang sangat tinggi. Begitu pula interpretasi untuk
kelas-kelas lainnya.

Selanjutnya, kami berfokus pada indeks vegetasi. Kelas
sawit, hutan, dan lahan terbuka hijau memiliki sebaran yang
sama: rentang high untuk EVI dan rentang very high untuk
semua indeks vegetasi kecuali EVI. Hal tersebut menunjukkan
bahwa seluruh indeks vegetasi
tidak dapat membedakan
antara kelas sawit dengan kelas hutan dan lahan terbuka hijau.
Oleh karena itu, diambil satu indeks NDVI yang akan
mewakili indeks vegetasi. Sedangkan indeks bangunan NDBI
dipilih karena dapat mencirikan kelas
tanah dan lahan
terbangun.

Pada Sentinel-2, pita VRE, NIR, dan Narrow NIR dapat
membedakan antara kelas sawit dan hutan dengan kelas lahan
terbuka hijau. Pada Landsat-8, pita NIR dapat membedakan
antara kelas sawit dan lahan terbuka hijau dengan kelas hutan.
Sehingga diperlukan indeks baru yang terdiri dari pita VRE,
NIR, dan Narrow NIR untuk membedakan kelas sawit
terhadap kelas non sawit

Berdasarkan hasil analisis deskriptif, didapatkan sejumlah
fitur pita multispektral maupun indeks komposit terpilih yang
akan digunakan dalam pemrosesan selanjutnya. Pita spektral
RGB, NIR, dan SWIR dipilih karena merupakan pita dasar
yang banyak digunakan pada penelitian terdahulu terkait
deteksi lahan sawit. Sementara pita VRE dan Narrow NIR
pada Sentinel-2, serta indeks komposit NDVI dan NDBI
dipilih karena kemampuannya dalam mencirikan kelas target.
Tabel 3 menunjukkan komposisi fitur terpilih pada satelit
Sentinel-2 dan Landsat-8.

TABEL III
PITA MULTISPEKTRAL DAN INDEKS KOMPOSIT TERPILIH

Komposisi fitur

Sentinel-2

Landsat-8

B2

B3

B4

-

B5

-

B6

B2

B3

B4

B7

B8

B8A

B11

NDVI

NDBI

Deskripsi

Blue

Green

Red

VRE

NIR

Narrow NIR

SWIR

Indeks vegetasi

Indeks bangunan

Gambar 5. Rentang fitur pada citra landsat-8.

Fitur pada citra Sentinel-2 dan Landsat-8 menghasilkan
distribusi yang serupa. Misalnya, kelas sawit pada kedua citra
satelit memiliki distribusi fitur sebagai berikut: RGB berada
pada level very low, pita Shortwave Infrared (SWIR) dan

B. Identifikasi Model Klasifikasi Terbaik Lahan Kelapa Sawit

Model yang telah dilatih diterapkan di seluruh wilayah
Rokan Hulu. Citra satelit Sentinel-2 memiliki tampilan yang
lebih detail karena resolusi pita spektralnya lebih tinggi (10
meter untuk RGB & NIR; 20 meter untuk SWIR) daripada
citra Landsat-8 (30 meter untuk semua pita spektral). Gambar

5 / 8

6, 7, dan 8 menunjukkan kurva akurasi pada model RF, MLP,
dan CNN untuk masing-masing citra satelit.

dan presisi sebesar 82%. Tabel 4 menunjukkan perbandingan
akurasi, presisi, recall, dan F1-score pada setiap metode untuk
masing-masing citra satelit.

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

TABEL IV
PERBANDINGAN AKURASI , PRESISI, RECALL DAN F1-SCORE
Akurasi

Metode

Presisi

Recall

Satelit

F1-score

Random Forest

92,14% 92,32% 92,14% 92,14%

Sentinel-2

MLP

CNN

90,44% 91,67% 90,55%

91,11%

91,95% 92,5% 91,66%

92,07%

(a)

(b)

Landsat-8

Gambar 6. Kurva akurasi pada model RF untuk citra satelit Sentinel-2 (a)

dan Landsat-8 (b).

Random Forest

81,64% 82,01% 81,64%

81,59%

MLP

CNN

78,89% 82,72% 76,22%

79,34%

80,22% 83,92% 77,61%

80,64%

Terlihat bahwa citra satelit Sentinel-2 menghasilkan akurasi,
presisi, recall, dan F1-score yang lebih tinggi dibandingkan
citra Landsat-8. Sehingga secara deskriptif, model klasifikasi
terbaik dihasilkan oleh metode RF pada citra Sentinel-2.
Gambar 9 menunjukkan visualisasi dari model tersebut.

(a)

(b)

Gambar 7. Kurva akurasi pada model MLP untuk citra satelit Sentinel-2 (a)

dan Landsat-8 (b).

(a)

(b)

Gambar 9. Hasil klasifikasi model RF pada citra satelit Sentinel-2.

Gambar 8. Kurva akurasi pada model CNN untuk citra satelit Sentinel-2 (a)

dan Landsat-8 (b).

Jika dilihat dari gambar di atas, citra satelit Sentinel-2
menghasilkan nilai akurasi yang lebih tinggi dari citra satelit
Landsat-8. Model yang dihasilkan dari citra Sentinel-2 lebih
baik karena memiliki persentase awan yang lebih sedikit yaitu
di bawah 20%. Sedangkan pada periode yang sama, citra
Landsat-8 mengandung persentase awan yang lebih besar 20-
30%. Selain itu, dari segi pita spektral, satelit Sentinel-2
memiliki resolusi pita spektral yang lebih tinggi daripada
satelit Landsat-8.

Berdasarkan model yang telah diuji, secara deskriptif
metode RF menghasilkan akurasi yang paling tinggi. Pada
citra Sentinel-2, RF menghasilkan rata-rata akurasi, presisi,
recall dan F1-score mencapai 92%. Pada citra Landsat-8, RF
menghasilkan rata-rata akurasi, recall, F1-score sebesar 81%

ini

dalam penelitian

Namun diperlukan uji

lanjutan untuk meyatakan bahwa
metode RF adalah yang terbaik secara statistik. Ada dua jenis
pengukuran: statistik parametrik dan non-parametrik. Tes
yang digunakan
adalah statistik
parametrik uji T. Uji T adalah salah satu teknik paling umum
yang digunakan untuk menguji apakah terdapat perbedaan
antara rata-rata dua kelompok sampel. Uji T digunakan karena
titik sampel diproses secara independen antar algoritma.
Dalam statistik parametrik,
terdapat beberapa asumsi yang
harus dipenuhi: data berdistribusi normal dan variansnya
homogen. Variabel yang akan diuji adalah akurasi dan F1-
score. Model dijalankan sebanyak 30 kali pengulangan untuk
kemudian dicatat hasilnya. Kemudian, output dari ketiga
algoritma tersebut akan diuji rata-ratanya.

Uji Shapiro-Wilk digunakan untuk memeriksa normalitas
data dengan hipotesis nol: data berdistribusi normal. Tingkat
signifikansi yang digunakan adalah 5%. Tabel 5 menunjukkan
hasil uji normalitas citra Sentinel-2 dan Landsat-8.

6 / 8

TABEL V
HASIL UJI SHAPIRO-WILK

Satelit

Metode

P-value

Akurasi

F1-score

Sentinel-2

Landsat-8

Random Forest

MLP

CNN

Random Forest

MLP

CNN

0,68

0,282

0,326

0,555

0,432

0,987

0,737

0,605

0,216

0,493

0,434

0,801

Tabel 5 menunjukkan bahwa semua kelompok memiliki p-
value di atas 0,05. Sehingga dapat disimpulkan bahwa nilai
akurasi dan F1-score untuk kedua satelit berdistribusi normal.
Selanjutnya dilakukan uji kesamaan varians dengan uji
Bartlett. Uji ini digunakan untuk mengetahui varians semua
kelompok dengan hipotesis nol: varians antara dua kelompok
homogen. Kami menggunakan tingkat signifikansi 5%. Tabel
6 menunjukkan hasil uji Bartlett masing-masing pasangan
kelompok model untuk kedua citra satelit.

TABEL VI
HASIL UJI BARTLETT
P-value
RF-CNN

RF-MLP

MLP-CNN

Akurasi

F1-
score

Akurasi

F1-
score

Akurasi

F1-
score

Satelit

Sentinel-
2
Landsat-
8

Tabel 6 menunjukkan bahwa semua kelompok memiliki p-
value di bawah 0,05 kecuali akurasi dan F1-score pasangan
MLP dan CNN pada kedua citra satelit. Dengan demikian,
dapat disimpulkan bahwa semua pasangan memiliki varians
yang heterogen kecuali MLP dan CNN memiliki varians yang
homogen.

Terakhir, uji T digunakan untuk memastikan apakah
terdapat perbedaan rata-rata pada akurasi dan F1-score antara
model RF, MLP, dan CNN. Uji T digunakan dengan hipotesis
nol:
tidak ada perbedaan rata-rata antara dua pasangan
kelompok. Kami menggunakan tingkat signifikansi 5%. Tabel
7 menunjukkan hasil uji T pada citra Sentinel-2 dan Landsat-8.

TABEL VII
HASIL UJI T

P-value
RF-CNN

RF-MLP

Satelit

Sentinel-
2
Landsat-
8

Akurasi

F1-
score

Akurasi

F1-
score

Akurasi

F1-
score

0,1062

0,0834

0,8102

0,4929

0,1804

0,0828

5,4E06

1,9E05

0,0036

0,0070

0,2746

0,3169

MLP-CNN

B. Saran

Tabel 7 menunjukkan bahwa akurasi dan F1-score dari
ketiga pasangan RF-MLP, RF-CNN, dan MLP-CNN pada

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

citra Sentinel-2 memiliki p-value di atas 0,05. Dapat
disimpulkan bahwa dengan tingkat kepercayaan 95%, pada
citra Sentinel-2 tidak ada perbedaan akurasi dan F1-score
antara ketiga model atau dengan kata lain, komposisi fitur
pada citra Sentinel-2 bekerja secara optimum baik pada
machine learning maupun deep learning. Sedangkan citra
Landsat-8 menunjukkan hanya akurasi dan F1-score dari
pasangan MLP-CNN yang memiliki p-value di atas 0,05.
Dapat disimpulkan bahwa dengan tingkat kepercayaan 95%,
tidak ada perbedaan akurasi dan F1-score yang dihasilkan
oleh model MLP dan CNN, namun terdapat perbedaan antara
metode RF dengan CNN dan MLP di mana metode RF
menghasilkan nilai akurasi dan F1-score tertinggi.

C. Estimasi luas lahan perkebunan kelapa sawit

selisih

Estimasi

luas lahan perkebunan kelapa sawit dilakukan
dengan menggunakan model terbaik yaitu metode Random
Forest dengan citra satelit Sentinel-2. Berdasarkan hasil
klasifikasi, didapatkan estimasi luas lahan sawit yaitu seluas
lahan
369.674,47 Ha. Terdapat
perkebunan kelapa sawit antara hasil klasifikasi pada citra
satelit dengan angka resmi yang dipublikasikan oleh BPS. Hal
ini terjadi karena data luas wilayah perkebunan yang diperoleh
BPS yaitu melalui Survei Perusahaan Perkebunan (SKB) yang
diisi secara mandiri oleh administratur perkebunan [36].
Sedangkan data yang digunakan pada metode penginderaan
jauh merupakan tangkapan citra permukaan bumi pada bulan
Juli (Sentinel-2) dan Maret (Landsat-8).

estimasi

luas

sawit

juga menghasilkan estimasi

Manfaat dari penelitian ini yaitu mendeteksi

lahan
termasuk wilayah yang sulit
perkebunan kelapa
dijangkau (remote area) dengan akurasi yang tinggi. Selain itu,
penelitian ini
luas perkebunan
kelapa sawit dengan waktu,
tenaga, dan biaya yang lebih
sedikit. Menjawab tujuan pertama penelitian, dari hasil
eksperimen didapatkan fitur terbaik penciri lahan perkebunan
kelapa sawit yang terdiri dari pita Red, Green, Blue,
Vegetation Red Edge, NIR, Narrow NIR, SWIR, serta indeks
komposit NDVI dan NDBI. Sedangkan untuk menjawab
tujuan kedua penelitian, dari hasil eksperimen didapatkan
model terbaik secara deskriptif yaitu metode Random Forest
dengan citra Sentinel-2 karena menghasilkan akurasi tertinggi.
Terakhir untuk menjawab tujuan ketiga penelitian, dari model
luas lahan perkebunan kelapa
terbaik didapatkan estimasi
sawit pada tahun 2019 seluas 369.674,47 Ha.

Fokus dari penelitian ini adalah untuk mendeteksi lahan
perkebunan sawit dari non sawit, yaitu hutan dan lahan
terbuka hijau. Hingga saat ini, ketersediaan indeks komposit
masih terbatas dan belum secara tegas dapat membedakan
kelapa sawit, hutan, dan lahan terbuka hijau. Dari hasil
eksperimen, terdapat potensi pembangunan indeks komposit
baru yang terdiri dari pita Vegetation Red Edge, NIR, dan
Narrow NIR sesuai temuan identifikasi fitur oleh peneliti.

7 / 8

5,5E22

1,9E21

6,3E22

2,2E21

0,9684

0,9632

VII.

PENUTUP

3,1E20

2,6E20

8,9E22

7,5E22

0,3103

0,3112

A. Kesimpulan

DAFTAR PUSTAKA
[1] L. Li, J. Dong, S. N. Tenku, and X. Xiao, “Mapping oil palm plantations
in Cameroon using PALSAR 50-m orthorectified mosaic images,”
Remote Sensing, vol. 7, pp. 1206–1224, 23 Jan 2015.

[2] P. Srestasathiern and P. Rakwatin, “Oil palm tree detection with high
resolution multi-spectral satellite imagery,” Remote Sensing, vol. 6, pp.
9749–9774, 25 Aug 2014.

[3] United States Department of Agriculture. (2021, 3) Oil palm world
production.
Available:
http://ipad.fas.usda.gov/cropexplorer/cropview/commodityView.aspx?cro
pid=4243000

[Online].

explorer

palm

Oil

In:

[4] McCarthy. (2020, 2) Which countries produce the most palm oil?
https://www.statista.com/chart/23097/amount-of-

[Online]. Available:
palm-oil-produced-in-selected-countries

[5] Statistik Indonesia 2020. Badan Pusat Statistik, 2020.
[6] N. A. Mubin, E. Nadarajoo, H. Z. M. Shafri, and A. Hamedianfar,
“Young and mature oil palm tree detection and counting using
convolutional neural network deep learning method,” International
Journal of Remote Sensing, vol. 40, no.19, pp. 7500–7515, 25 Jan 2019.
[7] H. Z. Shafri, N. Hamdan, and M. I. Saripan, “Semi-automatic detection
and counting of oil palm trees from high spatial resolution airborne
imagery,” International Journal of Remote Sensing, vol. 32, no. 8, pp.
2095–2115, 20 Apr 2011.

[8] S. Vadivelu, A. Ahmad, and Y. H. Choo, “Remote sensing techniques for
oil palm age classification using Landsat-5 TM satellite,” Sci Int (Lahore),
vol. 26, no. 4, pp. 1547–1551, 2014

[9] J. S. H. Lee, S. Wich, A. Widayati, and L. P. Koh, “Detecting industrial
oil palm plantations on Landsat images with Google Earth Engine,”
Remote Sensing Applications: Society and Environment, vol. 4, pp. 219–
224, 11 Nov 2016.

[10] G. Dwinita and S. H. Murti, “Aplikasi citra penginderaan jauh untuk
estimasi produksi kelapa sawit
jacq) berbasis
Normalized Different Vegetation Index (Perkebunan PT. Mutiara Sawit
Seluma, Kabupaten Seluma, Provinsi Bengkulu),” Jurnal Bumi Indonesia,
vol. 5, no. 4, pp. 1–12, 2016.

(Elaies guineensis

[11] A. Tridawati and S. Darmawan, “Investigation of classification algorithm
for land cover mapping in oil palm area using optical remote sensing,” in
1st
Institut
Teknologi Nasional, Bandung, 9–11 Oktober 2017.

technology international congress,

faculty of

industrial

[12] M. Freudenberg, N. Nölke, A. Agostini, F. Wörgötter, and C. Kleinn,
“Large scale palm tree detection in high resolution satellite images using
U-Net,” Remote Sensing, vol. 11, no. 312, 4 Feb 2019.

[13] D. W. Triscowati and A. W. Wijayanto, “Peluang dan tantangan dalam
pemanfaatan teknologi penginderaan jauh dan Machine Learning untuk
prediksi data tanaman pangan yang lebih akurat,” in Seminar Nasional
Official Statistics 2019, vol. 2019, no. 1, pp. 177–187.

[14] T.Hoeser, F. Bachofer, and C. Kuenzer, “Object detection and image
segmentation with deep learning on earth observation data: A review—
part II: Applications,” Remote Sensing, vol. 12, no. 3053, 18 Sep 2020.
[15] S. A. Mansor, and M. L. R. Sarker, “Remote sensing technique for
estimating the age of oil palm using high resolution image” - in 36th
Asian Conference on Remote Sensing: Fostering Resilient Growth in Asia
2015. Filipina, 24-28 Okt 2015.

[16] A. W. Wijayanto, D. W. Triscowati, and A. H. Marsuhandi, “Maize field
area detection in East Java, Indonesia: An integrated multispectral remote
sensing and machine
in 12th International
learning approach,”
Conference on Information Technology and Electrical Engineering
(ICITEE). Yogyakarta: IEEE, 6–8 Okt 2020.

[17] Solutions HG. (2013) Vegetation analysis: using vegetation indices in
ENVI
Available:
[Online].
http://www.harrisgeospatial.com/Learn/WhitepapersDetail/TabId/802/Art
MID/2627/ArticleID/13742/Vegetation-Analysis-Using-Vegetation-
Indices-in-ENVI.aspx.

[18] H. A. Setyowati and S. H. Murti, “Aplikasi citra SPOT-6 berbasis
transformasi indeks vegetasi untuk estimasi produksi kelapa sawit (Elaeis
guineensis jacq) (Kasus Perkebunan Kelapa Sawit PT. Tunggal Perkasa
Plantations, Air Molek, Kabupaten Indragiri Hulu, Propinsi Riau,
Sumatera),” Jurnal Bumi Indonesia, vol. 4, no. 4, 2015.

[19] A. H. Marsuhandi, D. W. Triscowati, A. W. Wijayanto, “Tinjauan
pemanfaatan big data penginderaan jauh dan pembelajaran mesin untuk
official statistics di wilayah perkotaan,” in Seminar Nasional Official

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Statistics 2020: Statistics in the New Normal: a Challenge of Big Data
and Official Statistics. Politeknik Statistika STIS, 23–24 Sep 2020.

[20] N. Verde, I. P. Kokkoris, C. Georgiadis, D. Kaimaris, P. Dimopoulos, I.
Mitsopoulos, and G. Mallinis, “National scale land cover classification
for ecosystem services mapping and assessment, using multitemporal
Copernicus EO data and Google Earth Engine,” Remote Sensing, vol. 12,
no. 3303, 11 Okt 2020.

[21] Y. Nurmasari and A. W. Wijayanto, “Oil palm plantation detection in
Indonesia using optical satellites imagery of Sentinel-2 and Landsat-8
(case study: Rokan Hulu Regency, Riau Province),” International Journal
of Remote Sensing and Earth Sciences, 2021.

[22] D. W. Triscowati, B. Sartono, A. Kurnia, D. D. Domiri, and A. W.
Wijayanto, “Multitemporal remote sensing data for classification of food
crops plant phase using supervised random forest,” in the Sixth
Geoinformation Science Symposium, International Society for Optics and
Photonics. Yogyakarta: SPIE, 21 Nov 2019.

[23] E. N. Arrofiqoh and Harintaka, “Implementasi metode Convolutional
Neural Network untuk klasifikasi tanaman pada citra resolusi tinggi,”
Geomatika, vol. 2, pp. 61–68, 23 Jul 2018.

[24] F. Hu, G. S. Xia, J. Hu, and L. Zhang, “Transferring deep Convolutional
Neural Networks for the scene classification of high-resolution remote
sensing imagery,” Remote Sensing, vol. 7, no. 11, pp. 14680–14707, 5
Nov 2015.

[25] A. Peryanto, A. Yudhana, and R. Umar, “Klasifikasi citra menggunakan
Convolutional Neural Network dan K Fold Cross Validation,” Journal of
Applied Informatics and Computing, vol. 4, no. 1, pp. 45–51, Juli 2020.
[26] K. Nomura and E. T. A. Mitchard, “More than meets the eye: using
landscapes,”

Sentinel-2 to map small plantations in complex forest
Remote Sensing, vol. 10, no. 1693, 26 Oct 2018.

[27] N. I. Fawzi and M. Y. Iswari, “Analisis heat island pada perkebunan
kelapa sawit: studi kasus di Kabupaten Kayong Utara, Kalimantan
Barat,” Jurnal Wilayah dan Lingkungan, vol. 8, no. 2, pp. 106–115,
Agustus 2020.

[28] I. Carolita, S. Darmawan, R. Permana, D. Dirgahayu, D. Wiratmoko, T.
Kartika, and S. Arifin, “Comparison of optic Landsat-8 and SAR
Sentinel-1 in oil palm monitoring, case study: Asahan, North Sumatera,
Indonesia,” IOP Conference Series: Earth and Environmental Science,
vol. 280, 2019.

[29] S. Agustin, H. Tjandrasa, R. V. H. Ginardhi, “Deep learning-based
method for multi-class classification of oil palm planted area on plant
ages using Ikonos Panchromatic imagery,” International Journal on
Advanced Science Engineering Information Technology, vol. 10, no. 6,
pp. 2200–2206, 2020.

[30] A. Descals, Z. Szantoi, E. Meijaard, H. Sutikno, G. Rindanata, and S.
Wich, “Oil palm (Elaeis guineensis) mapping with details: smallholder
versus industrial plantations and their extent in Riau, Sumatra,” Remote
Sensing, vol. 11, no. 2590, 5 Nov 2019.

[31] T. Sarzynski, X. Giam, L. Carrasco, J. S. H. Lee, “Combining radar and
optical imagery to map oil palm plantations in Sumatra, Indonesia, using
the Google Earth Engine,” Remote Sensing, vol. 12, no. 1220, 10 Apr
2020.

[32] N. Torbick, L. Ledoux, W. Salas, and M. Zhao, “Regonal mapping of
plantation extent using multisensor imagery,” Remote Sensing, vol. 8, no.
236, 14 Mar 2016.

[33] J. D. T. D. Alban, G. M. Connette, P. Oswald, and E. L. Webb,
“Combined Landsat and L-Band SAR data improves land cover
classification and change detection in dynamic tropical
landscapes,”
Remote Sensing, vol. 10, no. 306, 16 Feb 2018.

[34] P. Hao, F. Löw, and C. Biradar, “Annual cropland mapping using
reference Landsat time series － a case study in Central Asia,” Remote
Sensing, vol. 10, no. 2057, 18 Dec 2018.

[35] H. T. T. Nguyen, T.M. Doan, E. Tomppo, and R. E. McRoberts, “Land
use/land cover mapping using multitemporal Sentinel-2 imagery and four
classification methods －a case study from Dak Nong, Vietnam,” Remote
Sensing, vol. 12, no. 1367, 26 Apr 2020.

[36] Statistik Kelapa Sawit Indonesia 2019. Badan Pusat Statistik, 2019.
[37] G. Polancik, “Empirical Research Method Poster,” Maribor, Slovenia,

2007.

[38] M. Berndtsson, J. Hansson, B. Olsson, and B. Lundell, Thesis Projects—
A Guide for Students in Computer Science and Information System 2nd
Edition. London: Springer, 2008.

8 / 8

"
221710064,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Kajian Penerapan Data-Driven Untuk 
Pengelompokkan Rumah Tangga Miskin di D.I. 
Yogyakarta Tahun 2019 

Yanuar Krisna Pamuja (221710064, 4SD2) 
Dosen Pembimbing: Takdir, S.ST., M.T. 

Ringkasan—Pengolahan  data  secara  objektif  (Data-driven) 
dapat dimanfaatkan untuk mengukur tingkat kemiskinan rumah 
tangga dengan  berbasiskan  data Susenas Kor.  Peneliti  mencoba 
untuk  mengelompokkan  rumah  tangga  berdasarkan  keadaan 
sosial  ekonominya  dengan  melakukan  teknik  data  mining  yaitu 
metode  clustering  menggunakan  K-prototype  serta  dilakukan 
seleksi  atribut  untuk  menemukan  atribut  yang  paling  relevan 
dan  berpengaruh  terhadap  proses  clustering  menggunakan 
algoritma  Unsupervised  Spectral  Features  Selection  Method 
(USFSM). Dari proses seleksi atribut, dari 356 atribut terseleksi 
  analisis  klaster 
46  atribut  yang  paling  relevan. 
menunjukkan  terbentuknya  dua  klaster  yang  terdiri  dari  889 
rumah  tangga  miskin  dan  2845  rumah  tangga  tidak  miskin. 
Karena  jumlah  kelompok  dan  klasifikasi  kemiskinan  dari  BPS 
sama,  maka  dapat  diukur  akurasi  dari  proses  clustering  yaitu 
sama 
sebesar  77,21%  atau  2883  rumah 
pengklasifikasiannya . 

tangga  yang 

  Hasil 

Kata  Kunci—  Data-driven,  kemiskinan,  clustering,  seleksi 

atribut. 

I.  LATAR BELAKANG 

termasuk 

Salah  satu  permasalahan  mendasar  yang  masih  dihadapi 
oleh  negara  berkembang 
Indonesia  adalah 
kemiskinan.  Bahkan  penurunan  kemiskinan  sudah  menjadi 
salah  satu  program  utama  Sustainable  Development  Goal’s 
(SDG’s) yang telah disepakati negara-negara di dunia. Dalam 
pembukaan  UUD  1945  alinea  4 
tujuan 
tertera 
Indonesia  yaitu  untuk 
pembangunan  nasional  Negara 
memajukan  kesejahteraan  umum.    Kesejahteraan  umum  bisa 
digambarkan  atau  dilihat  dari  tingkat  kemiskinan  penduduk 
suatu negara.  

juga 

Menurut  publikasi  yang  dikeluarkan  Badan  Pusat  Statistik 
(BPS)  dengan  judul  “Penghitungan  dan  Analisis  Makro 
Kemiskinan  di  Indonesia  Tahun  2020”,  jumlah  penduduk 
miskin pada Maret 2020 sebesar 26,42 juta orang (9,7%), naik 
0,37%  dibanding  Maret  2019  yang  sebesar  25,14  juta  orang 
yang  ditandai  juga  naiknya  angka  garis  kemiskinan  dari 
Rp425.250,00  pada  Maret  2019  –  Maret  2020  menjadi 
Rp454.652,00  perkapita  per  bulan  atau  naik  sebesar  6,91  % 
pada  Maret  2020.  Dimana  provinsi  Bali  memiliki  angka 
kemiskinan  paling  rendah  (3,78%)  yang  tahun  sebelumnya 
diperoleh  provinsi  DKI  Jakarta,  sedangkan  provinsi  Papua 
memiliki  angka  kemiskinan  tertinggi  sebesar  26,64%  dan 
tetap  menjadi  paling  tinggi  dari  tahun  sebelumnya.  Ini 

pemerataan 

menandakan  masih 
dalam 
kurangnya 
pemberantasan  kemiskinan.  Memang  jika  berbicara  tentang 
pemberantasan  kemiskinan  terutama  di  negara  berkembang 
pasti  tidak  akan  ada  habisnya  dari  tahun  ke  tahun.  Butuh 
waktu  dan    kebijakan  yang  tepat  untuk  terus  menekan  angka 
kemiskinan.  Pengukuran  kemiskinan  yang  tepat  dan  dapat 
dipercaya  dapat  menjadi  instrumen  tangguh  bagi  pengambil 
kebijakan  dalam  memfokuskan  perhatian  pada  kondisi  hidup 
orang miskin.  

dan 

pemerintah 

Pengukuran kemiskinan juga tak luput dari data yang terus 
dikumpulkan 
yang 
diolah.  Seperti 
menggunakan garis kemiskinan berdasarkan ukuran dari BPS 
yang  dihitung  berdasarkan  data  Survei  Sosial-Ekonomi 
Nasional  (Susenas).  Pemanfaatan  data  yang  ada  harus  bisa 
dioptimalkan  dengan  baik  oleh  pemerintah  maupun 
perusahaan  atau  perorangan  sehingga  dengan  data  yang  ada 
bisa dilakukan analisis untuk menjadi landasan kebijakan bagi 
pemerintah  maupun  dalam  dunia  bisnis  dan  lainnya.  Maka 
dari  itu  dibutuhkan  peran  data-driven  yang  akan  mengambil 
keputusan  berdasarkan  data  yang  ada.  Sehingga  dalam  suatu 
tindakan  atau  analisis  data,  subjektivitas  serta  pengaruh 
pengetahuan dari luar data dalam pengambilan keputusan bisa 
diminimalisir.  Data  memberikan  gambaran  informasi  suatu 
keadaan  populasi  yang  dijadikan  subjek  penelitian,  sehingga 
jika data dipengaruhi oleh pengetahuan diluar data, entah dari 
domainnya,  perilaku  data  dari  tahun-tahun  sebelumnya,  atau 
perilaku  data  dari  subjek  penelitian  lain,  maka  tentunya 
keputusan  yang  akan  diambil  berdasarkan  hasil  pengolahan 
data  akan  berbeda  jika  dibanding  memanfaatkan  sepenuhnya 
data yang ada, terlepas dari hasil serta keputusan yang diambil 
benar  atau  salah.    Sederhananya  bagaimana  bisa  membuat 
kebijakan  atau  keputusan  berbasiskan  data,  bukan  sekedar 
intuisi. 

Penelitian  ini  mencoba  mengimplementasikan  data-driven 
seperti  yang  dilakukan  dalam  penelitian  [1]  untuk  analisis 
kemiskinan  rumah  tangga  yang  ada  di  Indonesia.  Data 
kemiskinan  yang  dipakai  di  Indonesia  seperti  bahasan 
sebelumnya  yaitu  menggunakan  garis  kemiskinan  dari  BPS 
yang  dihitung  dari  data  Susenas  Modul  Konsumsi  dan 
Pengeluaran.  Disini  peneliti  ingin  memberikan  gambaran 
informasi  lain  mengenai  kondisi  ekonomi  rumah  tangga  dari 
perspektif  yang  lebih  luas.  Kalau  pemerintah  menggunakan 
garis  kemiskinan  sebagai  tolak  ukur  tingkat  kesejahteraan 

 1 / 8 

 
 
 
 
 
 
rumah 

masyarakatnya,  disini  peneliti  menggunakan  tolak  ukur  lain 
diluar  konsep  pengeluaran  dengan  memanfaatkan  data 
Susenas Kor yang mana data Susenas Kor sendiri dibuat untuk 
mensurvei  keadaan  sosial  ekonomi  subjek  penelitiannya. 
Dengan data  yang ada akan diolah  menggunakan pendekatan 
data  mining  khususnya  dengan  teknik  clustering  untuk 
tangga  berdasarkan  kemiripan 
mengelompokkan 
keadaan  sosial  ekonominya  sehingga  bisa  dilakukan  analisis 
untuk  mengukur 
tiap  kelompoknya. 
tingkat  kemiskinan 
Masalah  utama  dalam  proses  discovering  knowledge  dari 
sebuah  data  adalah  mengidentifikasi  atribut  yang  tepat  untuk 
proses  clustering  [1].  Atribut-atribut  yang  banyak  dan 
sebenarnya  tidak  berpengaruh  dapat  mengurangi  kinerja 
algoritma  clustering,  sehingga  perlu  dilakukan  seleksi  atribut 
terlebih  dahulu  untuk  mengetahui  atribut  yang  sebenarnya 
berpengaruh dan yang tidak berpengaruh. Setiap atribut dalam 
data  Susenas  memiliki  nilai  yang  dapat  diolah.  Berbagai 
penelitian terkait analisis kemiskinan sudah banyak dilakukan 
dengan  memanfaatkan  data  Susenas,  akan  tetapi  dalam 
pemilihan  variabel  atau  atribut  yang  akan  dipakai  seringkali 
masih  berdasarkan  pengetahuan  domain.  Oleh  karena  itu, 
penelitian  ini  akan  menggunakan  algoritma  seleksi  atribut 
untuk meminimalisir subjektivitas dalam pemilihan atribut. 

Dengan  kombinasi  antara  proses  seleksi  variabel  atau 
atribut  serta  metode  clustering  yang  akan  dipakai,  peneliti 
dapat mengolah semua variabel dari data  Susenas Kor secara 
ini  akan  didapatkan 
objektif.  Dari  penelitian 
informasi 
mengenai 
yang  mampu  membedakan 
karakteristik 
sekelompok  rumah  tangga  dengan  tepat  serta  mendapatkan 
gambaran kemiskinan dari perspektif yang lebih luas. Dengan 
adanya  informasi  mengenai  karakteristik  sosial  ekonomi  dari 
tiap  klaster,  peneliti  dapat  menentukan  tingkat  kemiskinan 
untuk setiap klaster dan berharap program atau kebijakan dari 
pemerintah  untuk  mengurangi  angka  kemiskinan  dapat 
dilaksanakan sesuai target yang ingin dicapai. 

II.  TUJUAN PENELITIAN 

Tujuan yang ingin dicapai dalam penelitian ini adalah: 

1.  Mengkaji  metode  penentuan  rumah  tangga  miskin 
dari  ukuran  kemiskinan  multidimensi  dengan 
melakukan  eksplorasi  berbasiskan  data  (data-driven) 
menggunakan pendekatan data mining. 

2.  Benchmarking  terhadap  penentuan  rumah  tangga 
miskin dari ukuran kemiskinan versi pemerintah atau 
BPS menggunakan garis kemiskinan. 

III. PENELITIAN TERKAIT 

Penelitian ini mengambil beberapa penelitian rujukan yang 
berguna  sebagai  landasan  teori  dalam  melakukan  penelitian 
ini. Penelitian rujukan pertama yaitu penelitian [2] yang mana 
tingkat 
penelitian 
kemiskinan  dari 
indeks  kemiskinan  multidimensi,  yang 
artinya  variabel  yang  digunakan  itu  menggambarkan  banyak 

tersebut  bertujuan  untuk  mengukur 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

diluar 

dimensi 

dimensi 
Kemudian 
membandingkan  hasil  klasifikasi  kemiskinan  dari  hasil 
penelitian  tersebut  dengan  BPS  yang  menggunakan  garis 
kemiskinan. 

pengeluaran. 

Dibutuhkan  alat  yang  digunakan  untuk  melakukan 
klasifikasi  ataupun  pengelompokkan  rumah  tangga  agar  bisa 
diukur tingkat kemiskinannya. Maka dari itu peneliti merujuk 
pada  penelitian  [1],  [2],  [5]  dan  [11]  dimana  penelitian 
tersebut memanfaatkan teknik data mining untuk menemukan 
informasi  mendalam  dalam  data  dan  dilakukan  proses 
clustering  untuk  menemukan  klaster  hingga 
tingkat 
kemiskinan tiap klasternya. 

Dalam analisis deskriptif data, variabel dalam data Susenas 
memiliki  tipe  data  campuran,  antara  tipe  data  numerik  dan 
kategorik.  Tidak  semua  metode  clustering  dapat  mengatasi 
fenomena tersebut. Maka dari itu penelitian ini mengacu pada 
penelitian  [9]  dan  [10]  untuk  meimplementasikan  metode  K-
prototype yang bisa mengatasi tipe data campuran. 

Proses  clustering  seringkali  menghasilkan  struktur  cluster 
yang kurang baik, ini disebabkan karena adanya variabel atau 
atribut yang sebenarnya kurang signifikan dalam membedakan 
subjek  kedalam  beberapa  klaster.  Untuk  mengatasi  hasl 
tersebut,  penelitian  ini  menggunakan  metode  seleksi  atribut 
berdasarkan  penelitian  [6]-[8].  Algoritma  USFSM  dipilih 
karena  algoritma  tersebut  juga  dapat  mengatasi  data  yang 
memiliki variabel dengan tipe data campuran. 

IV. METODE PENELITIAN  

Metode  dalam  penelitian  ini  menggunakan  tahapan  umum 
dari proses data mining seperti [11]. Metode penelitian dibuat 
untuk  mempermudah  pencapaian  hasil  penelitian  sehingga 
penelitian dapat diselesaikan dengan tepat waktu dan berjalan 
sesuai  dengan  alur  yang  diharapkan.  Adapun  tahapan  dari 
metode penelitian yang digunakan dapat dilihat pada gambar 1. 

 2 / 8 

 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

dan  nilai  ACC  yang  terbesar  akan  menghasilkan 
cluster yang lebih baik. 

9.  Pembuatan Laporan 

Pembuatan  laporan  akhir  penelitian  berupa  buku 
skripsi,  dilakukan  berdasarkan  kerangka  yang  telah  
dirancang    mulai  dari  awal  penelitian  hingga  tujuan 
penelitian yang sudah tercapai. 

V.  KERANGKA PIKIR 

Kerangka  pikir  pada  penelitian  ini  dimulai  dari  melihat 
ukuran  kemiskinan  untuk  menentukan  rumah  tangga  miskin, 
kemudian  pemilihan  data  yang  sesuai  dengan  permasalahan 
tersebut.  Disini  peneliti  menggunakan  data  dari  BPS  yaitu 
Susenas  Kor  2019,  dengan  alasan  bahwa  pemerintah  juga 
menggunakan  garis  kemiskinan  dari  BPS  yang  dihitung  juga 
dari  hasil  Susenas.  Diharapkan  penggunaan  data  ini  sebagai 
dasar penerapan data-driven bisa memberikan perspektif yang 
lebih luas tentang kondisi riil kemiskinan di Indonesia.  

Metode  seleksi  fitur  atau  atribut  dikombinasi  dengan 
metode  clustering  dalam  penentuan  kelompok  rumah  tangga 
miskin ini bertujuan supaya dapat mengindentifikasi fitur atau 
atribut  yang  berpengaruh  untuk  proses  clustering.  Setelah  itu 
dilakukan  analisis  terhadap  fitur  atau  atribut  yang  terpilih 
untuk  proses  clustering  dengan  analisis  deskriptif  terhadap 
objek amatannya. 

Gambar 1. Flowchart dari metode penelitian 
Berikut ini merupakan penjelasan dari setiap bagian yang 
tercantum pada kerangka kerja penelitian dalam gambar 1. 

1. 

Identifikasi Masalah 
Peneliti  melakukan  identifikasi  dan  merumuskan 
permasalahan pada penelitian, hal ini bertujuan untuk 
melakukan  kajian  mengenai  kekurangan-kekurangan 
metode  yang  sudah  pernah  dipakai  dalam  penelitian 
lain  terkait  pengelompokkan  atau  klasifikasi  rumah 
tangga miskin. 
2.  Studi Literatur 

Pada  tahapan  ini  peneliti  melakukan  kajian  pustaka, 
dengan  mempelajari  jurnal-jurnal  referensi,  artikel-
artikel,  serta  hasil  penelitian  sejenis  yang  relevan 
dengan  permasalahan  yang  sedang  diteliti,  teknik 
untuk menyelesaikan permasalahan penelitian supaya 
dapat  memahami  dengan  benar  penelitian  yang 
sedang dilakukan. 
3.  Pengumpulan Data 

Penelitian  ini  berbasiskan  data  rumah  tangga  hasil 
survei dari Survei Sosial Ekonomi Nasional (Susenas) 
Kor  tahun  2019  di  Provinsi  D.I.  Yogyakarta  yang 
dilakukan oleh BPS dan juga memakai data uji coba 
dari  UCI  untuk  mengevaluasi  algoritma  seleksi 
atribut. 

4.  Data Selection 

Pemilihan  (Seleksi)  data  baru  dari  sekumpulan  data 
operasional 
tahap 
penggalian informasi dalam data mining dimulai. 

dilakukan 

sebelum 

perlu 

5.  Preprocessing Data 

Pertama  dalam  mengatasi  missing  value,  atribut 
numerik  diganti  dengan  nilai  mean  dan  atribut 
kategorik  diganti  nilai  modus.  Kedua  melakukan 
seleksi  atribut  dengan  algoritma  USFSM  guna 
mengidentifikasi  atribut  yang  tepat  untuk  proses 
clustering. 

6.  Data Transformation 

Standarisasi  adalah  proses  transformasi  pada  data 
yang  telah  dipilih,  sehingga  data  tersebut  sesuai 
untuk proses data mining. 

7.  Data Mining 

Proses  utama  pada  metode  yang  digunakan  untuk 
mendapatkan  pengetahuan  baru  dari  data  yang  telah 
diproses.  Pada penelitian  ini  metode  clustering  yang 
digunakan yaitu K-prototype dan Fuzzy K-prototype. 

8.  Analisis hasil 

Pada 

sebelumnya. 

Mengidentifikasi  pengetahuan  yang  didapat  dari 
ini, 
tahap 
proses-proses 
tangga  yang  di 
menghasilkan  kelompok  rumah 
evaluasi  untuk  menilai  kajian  yang  ada 
telah  
memenuhi  target  yang  diharapkan.  Evaluasi  juga 
clustering  dengan 
dilakukan  kepada  metode 
menggunakan  Clustering  Accuraccy  (ACC)  dari 
penelitian [6]. Nilai ACC berkisar antara 0 sampai 1 

Gambar 2. Kerangka pikir penelitian 

Penentuan 

tingkat  kemiskinan  dari  hasil  clustering 
dilakukan dengan analisis deskriptif terhadap karakteristik tiap 
ini  dilakukan  untuk  bisa 
klaster 
mengetahui  bagaimana  perbedaan  situasi  dari  klaster  yang 
terbentuk.  Kemudian  melakukan 
atau 
membandingkan  hasil  antara  rumah  tangga  miskin  dari 

(profiling).  Profiling 

benchmarking 

 3 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
penerapan data  mining dengan hasil versi BPS menggunakan 
garis kemiskinan. 

VI. HASIL DAN PEMBAHASAN 

4.1. Dataset Ujicoba 

dataset 

Langkah  pertama  pada  penelitian  ini  yaitu  melakukan 
evaluasi  hasil  dari  algoritma  seleksi  atribut  menggunakan 
USFSM  dan  metode  clustering  menggunakan  K-prototype 
kemudian 
ujicoba, 
untuk 
mengimplementasikannya  pada  data  studi  kasus.  Dalam 
penelitian 
ini,  5  dataset  uji  coba  diambil  dari  ruang 
penyimpanan UCI. Karakteristik dataset uji coba dapat dilihat 
pada  Tabel  1.  Dalam  implementasi  metode  K-prototype, 
peneliti  menggunakan  package  yang  tersedia  di  bahasa 
pemrograman  python.  Untuk  USFSM  sendiri,  peneliti  ingin 
menguji  apakah  kode  yang  dibuat  dari  algoritmanya  dapat 

baru 

Dataset 

Post-
operative 

Hepatitis 

Banyak 
Objek 

90 

155 

Contraception 

1473 

Thoracic-
Surgery 

Labor 

470 

57 

berjalan dengan tepat. 

Fitur 

Numerik  Kategorik  Total 

Banyak 
Kelas 

1 

7 

2 

3 

8 

8 

13 

8 

14 

8 

9 

20 

10 

17 

16 

3 

2 

3 

2 

2 

TABEL 1 
TABEL DARI DATASET BERTIPE CAMPURAN DARI UCI 

Semua  dataset  telah  distandarisasi  untuk  pengolahan  dan 
nlai  yang  hilang  akan  diisi  oleh  nilai  rata-rata  untuk  fitur 
numerik  dan  nilai  modus  untuk  fitur  kategorik.  Kemudian 
dilakukan seleksi fitur dengan mengambil 20% dari total fitur 
dataset  berdasarkan  ranking  yang  diperoleh  dari  hasil 
algoritma  USFSM.  Jika  dari  hasil  seleksi  ternyata  fitur  yang 
didapat  merupakan  satu  tipe  (numerik  semua  atau  kategorik 
semua),  maka  akan  diambil  satu  fitur  dengan  tipe  lawannya 
supaya dalam proses clustering tetap menggunakan algoritma 
K-prototype.  Gambar  3  memperlihatkan  hasil  akurasi  dari  K-
prototype  dengan  atau  tanpa  menggunakan  seleksi  fitur 
sebagai preprocessingnya. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 3. Hasil Clustering Accuracy (ACC) dengan K-prototype 

Dapat 

bahwa 

terlihat 

algoritma  USFSM 

dapat 
meningkatkan  akurasi  dari  algoritma  K-prototype.  Karena 
fitur  yang  tidak  relevan  dapat  mengurangi  kinerja  algoritma 
clustering  [1].  Walaupun  terlihat  tidak  semua  akurasi  dari 
metode  tersebut  dapat  dikatakan  cukup  akurat,  karena  dari 
gambar  tersebut  data  ke  1  dan  3  memiliki  akurasi  dibawah 
70%. Tabel 2 menunjukkan detail nilai akurasi hasil clustering 
dari  masing-masing  data  .  kemudian  dilanjutkan  dengan 
evaluasi  menggunakan  waktu  selama  pembentukan  cluster 
yang ditunjukkan oleh gambar 4. 

TABEL 2 
 TABEL AKURASI HASIL CLUSTERING 

ACC 

tanpa USFSM 

dengan USFSM 

0.389 

0.781 

0.384 

0.570 

0.625 

0.411 

0.794 

0.399 

0.826 

0.95 

Dataset 

Post-operative 

Hepatitis 

Contraception 

Thoracic-
Surgery 

Labor 

Gambar 4. Waktu tiap proses clustering dengan  K-prototype 

Untuk waktu yang dibutuhkan untuk proses clustering lebih 
bervariasi,  dilihat  bahwa  waktu  dengan  perlakuan  USFSM 
pun  kadang  tidak  lebih  cepat  dibanding  menggunakan  data 
utuh.  Menurut peneliti,  waktu dari tiap proses algoritma  juga 
dipengaruhi oleh performa dari laptop atau alat bantu lainnya. 
Penilaian hasil clustering dilakukan dengan metode lain yaitu 
Normalized Mutual Information (NMI) yang ditunjukkan oleh 
Tabel 3. 

TABEL 3 
TABEL DARI DATASET BERTIPE CAMPURAN DARI UCI 

Dataset 

NMI 

tanpa USFSM 

dengan USFSM 

Post-operative 

0.055045331 

Hepatitis 

0.162208241 

Contraception 

0.014584678 

0.055045331 

0.20965493 

0.031884758 

Thoracic-
Surgery 

Labor 

0.001753301 

0.003132113 

0.001970351 

0.697230827 

 4 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Nilai  NMI  berkisar  dari  0  sampai  1,  dimana  NMI 
merefleksikan  seberapa  identik  atau  independennya  antara 
kelas  target  asli  dengan  kelas  yang  dihasilkan  dari  proses 
clustering.  Semakin  besar  nilai  NMI,  berarti  hasil  clustering 
semakin  baik.  Dapat  dilihat  dari  Tabel  3,  bahwa  nilai  NMI 
tanpa 
naik  dengan  diterapkannya  USFSM  daripada 
Ini 
kecuali 
menggunakan  USFSM 
menunjukkan  bahwa  penerapan  seleksi  atribut  menggunakan 
algoritma USFSM dapat memperbaiki performa dan hasil dari 
proses clustering. 

kesatu. 

dataset 

4.1. Dataset Studi Kasus 

Penelitian  dilanjutkan  kepada  pengolahan  data  studi  kasus 
yaitu  data  Susenas  Kor  2019  di  Provinsi  D.I.  Yogyakarta. 
Data  tersebut  terdiri  dari  3734  observasi  serta  466  variabel 
yang bisa dilihat di [14]. Dalam proses data selection, peneliti 
memilih  data  sesuai  dengan  batasan  masalah  yaitu  unit 
analisisnya  adalah  kepala  rumah  tangga,  dan  variabel  yang 
digunakan  meliputi  indikator  selain  pengeluaran.  Kemudian 
variabel  yang  isinya  berupa  nomor  urut  serta  variabel  yang 
hanya  punya  satu  jenis  nilai  unik  akan  dihapus,  karena 
variabel  tersebut  tidak  masuk  dalam  análisis  penelitian  serta 
tidak  dapat  mempengaruhi  proses 
clustering.  Hasil 
pengamatan  terhadap  nilai  dari  tiap-tiap  variabel,  Dari  466 
variabel tersisa 356 variabel untuk masuk preprocessing data. 
 Preprocessing  data  dimulai  dengan  menentukan  tipe  dari 
tiap variabel pengamatan yang dibedakan menjadi 3 tingkatan, 
yaitu nominal, ordinal. dan rasio. Selanjutnya mencari apakah 
ada  nilai  yang  hilang  pada  data.  Dari  hasil  olah  data,  data 
Susenas  memiliki  nilai  yang  hilang,  tapi  nilai  tersebut 
sebenarnya  merupakan  nilai  yang  memang  bukan  ditujukan 
untuk  objek  tersebut.  Jadi  untuk  variabel  bertipe  kategorik, 
nilai  yang  hilang  diganti  suatu  nilai  unik  baru,  sedangkan 
variabel  numerik,  nilai  yang  hilang  diganti  0.  Tipe  variabel 
dari data susenas lebih rincinya dapat dilihat pada Tabel 4. 

TABEL 4 
TABEL TIPE VARIABEL DATA SUSENAS 

Tipe Variabel 

Jumlah 

Kategorik 

Numerik 

Nominal 

Ordinal 

Rasio 

274 

11 

71 

Setelah  tipe  tiap  variabel  dan  nilai  yang  hilang  sudah 
tertangani,  masuk  kedalam  tahapan  seleksi  atribut  guna 
mereduksi dimensi data Susenas dan menemukan atribut yang 
relevan  terhadap  struktur  klaster.  Tabel  5  memperlihatkan 
ukuran  variabel  yang 
terpilih  dari  proses  seleksi  fitur 
menggunakan USFSM. 

TABEL 5 
TABEL HASIL PROSES SELEKSI FITUR 

Iterasi ke- 

1 
2 
3 
4 
5 

375 (1) 
71 
71 
80 
77 
73 

Ukuran sampel 
750 (2) 
74 
67 
73 
66 
75 

1000 (3) 
69 
64 
79 
81 
77 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

53 

59 

55 
46 

Overall 
Irisan (4) 
Peneliti  mengambil  sampel  acak  dengan  ukuran  sesuai 
Tabel  5,  karena  algoritma  USFSM  masih  terlalu  lambat  jika 
objek amatannya terlalu besar. Menurut [7] sampel acak yang 
cukup  kecil  dapat  mempertahankan  informasi  cluster  asli 
dalam  banyak  kasus.  Maka  dari  itu  peneliti  mendasarkan 
algoritma USFSM pada pengambilan sampel acak jika dinilai 
observasinya terlalu besar. 

relevan 

Melakukan  iterasi  sebanyak  5  kali,  dan  di  tiap  iterasinya 
akan  dihasilkan  nilai  relevansi  untuk  tiap  atribut  kemudian 
diambil  atribut  yang  punya  nilai  relevansi  lebih  dari  0 
berdasarkan nilai yang diperoleh dari hasil algoritma USFSM. 
Untuk  menyimpulkan  hasil  yang  mana  saja  atribut  yang 
benar-benar 
terhadap  struktur  cluster,  peneliti 
mengambil  irisan  himpunan  atribut  relevan  yang  dihasilkan 
dari  kelima  iterasi.  Jumlah  atribut  yang  relevan  dapat  dilihat 
pada  Tabel  5  sesuai  dengan  percobaan  menggunakan  sampel 
acak.  Dari  tiga  sampel  tersebut,  ada  46  variabel  yang  selalu 
relevan  pada  ketiga 
Itu 
menandakan  dengan  mengambil  sampel  acak,  algoritma 
tersebut masih konsisten, ditandai dengan ada lebih dari 80% 
variabel relevan yang sama untuk tiap sampel yang diuji. 

jenis  pengambilan 

sampel. 

TABEL 6 
 TABEL LIMA VARIABEL PALING RELEVAN 

Sampel 
375 

R2109 

Nilai 

Sampel 
750 

Nilai 

Sampel 
1000 

Nilai 

0.00235 

R2109 

0.00195 

R2109 

0.000461 

R2110BK4 

0.00231 

R2110BK4 

0.00193 

R2110BK4 

0.000455 

R2110BK3 

0.00230 

R2110BK5 

0.00192 

R2110BK5 

0.000454 

R2110BK5 

0.00229 

R2110BK3 

0.00191 

R2110BK3 

0.000451 

R2110AK
4 

0.00222 

R2110AK
4 

0.00182 

R2110AK
4 

0.000433 

Tabel  6  memperlihatkan  lima  variabel  yang  menunjukkan 
nilai  relevansi  yang  paling  tinggi  untuk  masing-masing 
sampel.  Nilai  tersebut  merupakan  tolak  ukur  seberapa  besar 
variabel  atau  fitur  tersebut  dapat  mempengaruhi  struktur 
klaster dari data Susenas. Dari keempat percobaan dari Tabel 
5,  selanjutnya  dilakukan  evaluasi  hasil  klaster  dari  keempat 
percobaan  itu  untuk  menentukan  atribut  mana  yang  paling 
tepat  yang  sesuai  dengan  output  algoritma  USFSM.  Evaluasi 
hasil  klaster  menggunakan  Total  Cost  dan  Categorical 
Variance  Criterion  (CVC)  seperti  pada  [11].  Hasil  yang 
didapat dapat dilihat pada Tabel 7. 

TABEL 7 
TABEL HASIL EVALUASI KLASTER 

Percobaan 
375 Sampel (53 atribut) 
750 Sampel (55 atribut) 
1000 sampel (59 atribut) 
Irisan (46 atribut) 

Total Cost 
17881.7078 
26276.9810 
28939.0868 
14760.7078 

CVC 
-0.1882 
-0.0116 
-0.0375 
0.0756 

Setelah  nilai  total  cost  dan  CVC  didapat,  maka  percobaan 
paling  baik  yaitu  percobaan  yang  memiliki  nilai  total  cost 
paling  kecil  serta  nilai  CVC  yang  paling  besar.  Dari 

 5 / 8 

tanpa USFSMdengan USFSM1Post-operative5.50E-025.50E-022Hepatitis0.1622082412.10E-013Contraception1.45847E-023.19E-024Thoracic-Surgery1.75E-033.13E-035Labor1.97E-036.97E-01NoDatasetNMI 
 
 
 
 
 
 
 
 
keterangan  tersebut  didapat  percobaan  paling  baik  yang 
menunjukkan  hasil  seleksi  atribut  paling 
tepat  yaitu 
menggunakan irisan ketiga percobaan sampel acak (46 atribut 
terpilih).  Data  baru  dengan  menggunakan  46  atribut  terpilih 
dari  hasil  seleksi  atribut  tersebut  yang  akan  dipakai  untuk 
proses clustering sampai analisi hasil yang didapat. 

Metode  clustering  yang  dipakai  yaitu  K-prototype 
mengharuskan peneliti untuk menentukan jumlah klaster yang 
diinginkan  terlebih  dahulu.    Disini  peneliti  menggunakan 
metode  k-elbow  yang  sudah  diterapkan  pada  [12]  dan  [13] 
untuk  menentukan  jumlah  klaster  terbaik  yang  dilihat  dari 
total cost-nya. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

pengelompokkan  dari  gambar  6  yang  menunjukkan  warna 
biru untuk klaster 1 dan kuning untuk klaster 2. 

Cluster ke- 
Cluster 1 
Cluster 2 

TABEL 8 
TABEL JUMLAH ANGGOTA PER CLUSTER 
Jumlah Anggota (Rumah tangga) 
889 
2845 

 Persentase 
23,8 
76,2 

Setelah  didapatkan  hasil  pengelompokkan,  akan  dilakukan 
profiling  atau  analisis  deskriptif  terkait  karakteristik  dari  tiap 
klaster  yang  terbentuk.  Karakteristik  ini  akan  menentukan 
perbandingan  tingkat  kemiskinan  atau  kesejahteraan  antar 
klaster.  Hal  tersebut  dapat  diambil  karena  data  Susenas 
merupakan  data  yang  terkait  kondisi  sosial  ekonomi  rumah 
tangga,  jadi  tentunya  antar  klaster  akan  memiliki  anggota 
yang kondisi ekonomi sosialnya hampir mirip. 

TABEL 9 
TABEL PROFIL DARI VARIABEL KATEGORIK ANTAR CLUSTER 

Variabel 

Modus 

Persentase (%) 

Tingkat 
Kemiskinan 

C1 

C2 

C1 

C2 

C1 

C2 

Gambar 5. Perubahan nilai cost menurut jumlah klaster 

Pada  gambar  5  terlihat  perubahan  nilai  total  cost  dengan 
beberapa kali percobaan, mulai dari k = 2 dan sampai dengan 
k  =  9.  Semakin  besar  jumlah  klaster  yang  ditentukan  maka 
nilai  total  cost  semakin  mengecil.  Cara  untuk  menentukan 
jumlah  klaster  yang  paling  baik,  yaitu  dengan  mencari 
penurunan  nilai  cost  yang  paling  signifikan 
(mencari 
perubahan  cost  yang  menunjukkan  sudut  siku-siku  paling 
tajam).  Penurunan  paling  signifikan  terjadi  pada  saat  k=2, 
sedangkan  setelah  k=2  nilai  cost  tidak  seberapa  signifikan 
perubahannya.  

R1101_A  Punya 
Milik 
Sendiri 
Semen/ 
bata 

R1808  

R1802 

Tidak 
Milik 
Sendiri 

89.50 

63.90 

Tinggi 

Rendah 

91.70 

71.20 

Rendah  Tinggi 

Keramik 

46.50 

69 

Tinggi 

Rendah 

R2105 

Punya 

Tidak 

53.10 

94.80 

Tinggi 

Rendah 

R2106 

Punya 

Tidak 

73.70 

96.80 

Tinggi   Rendah 

R2107A 

R2107B 

Ya 

Ya 

Tidak 

55.80 

96.80 

Tinggi   Rendah 

Tidak 

71.40 

96.80 

Tinggi   Rendah 

R2107C 

ATM 

Tidak 

39.60 

96.80 

Tinggi   Rendah 

R2109 

R615 

Ya 

SD 

Tidak 

100.00 

99 

Tinggi   Rendah 

SMA 

31.40 

21.70 

Tinggi   Rendah 

R2001F  

Tidak 

Tidak 

91.50 

60.80 

Tinggi 

Rendah 

TABEL 10 
TABEL PROFIL DARI VARIABEL NUMERIK ANTAR CLUSTER 

Mean 

Tingkat Kemiskinan 

Variabel 

Cluster 1 

Cluster 2 

Cluster 1 

Cluster 2 

R2110A1K2 

99370.07874 

740.2460457  Tinggi 

Rendah 

R2110A1K3 

104435.3206 

353.602812 

Tinggi 

Rendah 

R2110A1K4 

105055.1181 

237.6098418  Tinggi 

Rendah 

Gambar 5. Scatter plot dari hasil pengelompokkan K-prototype 

R2110A1K5 

104561.3048 

248.1546573  Tinggi 

Rendah 

Setelah didapat jumlah klaster, dilakukan proses  clustering 
menggunakan  K-prototype  dengan  jumlah  klaster  adalah  2. 
Hasil  dari  proses  clustering  didapatkan  jumlah  anggota  tiap 
klaster  seperti  terlihat  pada  Tabel  8.  Dalam  tabel  diperoleh 
informasi  bahwa  jumlah  anggota  cluster  1  sebanyak  889 
rumah tangga atau sekitar 23,8 % dan sisanya masuk kedalam 
anggota  cluster  2  sebanyak  2845  rumah  tangga  atau  sekitar 
76,2%.  Dapat  dilihat  sebaran  datanya  setelah  dilakukan 

R2110C1B2 

84131.04612 

470.2987698  Tinggi 

Rendah 

R2110C1B3 

89162.26097 

238.6643234  Tinggi 

Rendah 

R2110C1B4 

89412.65467 

140.5975395  Tinggi 

Rendah 

R2110C1B5 

88976.54668 

151.1423551  Tinggi 

Rendah 

R2110C2B2 

7.967716535 

0.04516696 

Tinggi 

Rendah 

R2110C2B3 

8.450281215 

0.022495606  Tinggi 

Rendah 

R2110C2B4 

8.491338583 

0.01370826 

Tinggi 

Rendah 

R2110C2B5 

8.44791901 

0.014762742  Tinggi 

Rendah 

 6 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Dari Tabel 9 dan Tabel 10, dilakukan analisis perbandingan 
tiap  variabel  antar  klasternya.  Variabel  R1101_A  (apakah 
punya  BPJS  Kesehatan  penerima  bantuan  iuran),  cluster  1 
memiliki  persentase  jumlah  kepala  rumah  tangga  penerima 
BPJS  PBI  lebih  besar  dibanding  cluster  2,  ini  menunjukkan 
dari variabel R1101_A bahwa tingkat kemiskinan di cluster 1 
lebih  tinggi  dibanding  cluster  2,  karena  lebih  banyak  kepala 
rumah tangga yang masih menerima bantuan iuran.  

Dari variabel R1808 (bahan bangunan utama lantai terluas), 
bahan lantai untuk klaster 1 paling banyak terbuat dari semen/ 
bata  merah  dengan  jumlah  rumah  tangga  sebesar  413  atau 
46.5%,  sedangkan  untuk  klaster  2  paling  banyak  terbuat  dari 
keramik  dengan  1962 
Ini 
menunjukkan  bahwa  dari  segi  bahan  bangunan  lantai,  klaster 
1 cenderung kurang maju dibandingkan dengan klaster 2.  

rumahtangga. 

atau  69% 

Variabel  R2105  (penerima  Kartu  Keluarga  Sejahtera), 
rumah  tangga  di  klaster  1  paling  banyak  sebagai  penerima 
KKS yaitu sebesar 53.1%, sedangkan rumah tangga di klaster 
2  memiliki  94.8%  rumah  tangga  yang  tidak  menerima  KKS. 
KKS  ditujukan  untuk  membantu  menyejahterahkan  rumah 
tangga  bagi  yang  membutuhkan,  ini  artinya  rumah  tangga  di 
klaster 1 memiliki tingkat kesejahteraan yang kurang baik bila 
dibandingkan dengan rumah tangga di klaster 2.  

Jika  dilihat  dari  variabel  R615  (ijasah/sttb  tertinggi), 
sebanyak  31.4%  kepala  rumah  tangga  (krt)  di  klaster  1 
memiliki  ijasah  tertinggi  yaitu  SD,  itu  merupakan  kejadian 
terbanyak di klaster 1. Sedangkan untuk krt di klaster 2, paling 
banyak  memiliki  ijasah  tertinggi  yaitu  SMA  sebesar  21.7%. 
ini menandakan dari segi pendidikan, krt di klaster 1 memiliki 
tingkat  pendidikan  yang  kurang  tinggi  dibandingkan  dengan 
klaster 2.  

Variabel  R2110A1K2  –  R2110A1K5  membicarakan 
tentang  nilai  bantuan  BPNT  yang  diterima  dari  bulan 
November  2018  sampai  Februari  2019.  Rumah  tangga  di 
klaster  1  rata-rata  menerima  bantuan  sebesar  Rp  103.355. 
untuk  klaster  2  menerima  rata-rata  bantuan  sebesar  Rp  394. 
Rata-rata jumlah bantuan yang diterima oleh rumah tangga di 
klaster  1  lebih  besar  daripada  di  klaster  2,  ini  disebabkan 
karena  lebih  banyaknya  rumah  tangga  di  klaster  1  yang 
menerima  bantuan  BPNT.  Ini  dapat  diartikan  juga  bahwa 
tingkat kemiskinan di  klaster  1 lebih tinggi  karena  menerima 
bantuan yang lebih besar dari klaster 2. Variabel R2110C1B2 
–  R2110C1B5  membahas  tentang  besarnya  bantuan  yang 
digunakan  untuk  membeli  beras  dari  bulan  November  2018 
sampai  Februari  2019.  Rata-rata  rumah  tangga  di  klaster  1 
mengeluarkan  Rp  87.920  dari  bantuan  yang  diterima  untuk 
membeli  beras,  sedangkan  di  klaster  2  mengeluarkan  hanya 
Rp  250.  Besaran  yang  dikeluarkan  oleh  rumah  tangga  di 
klaster 1 lebih besar daripada klaster 2.Ini mengartikan bahwa 
pengaruh  bantuan  yang  diterima  dalam  memenuhi  kebutuhan 
sehari-hari  rumah  tangga  di  klaster  1  lebih  besar  daripada  di 
klaster  2.  Sedangkan  untuk  banyaknya  beras  yang  dibeli 
menggunakan bantuan dapat dilihat dari variabel R2110C2B2 
–  R2110C2B5.  Rumah  tangga  di  klaster  1  rata-rata  membeli 
beras  sebanyak  8.34  kg  dengan  bantuan  yang  diterima, 
sedangkan  klaster  2  hanya  0.024  kg.  Dapat  disimpulkan 
bahwa  tingkat  kemiskinan  di  klaster  1  lebih  tinggi  dibanding 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

klaster  2,  karena  besarnya  jumlah  bantuan  yang  digunakan 
oleh  rumah  tangga  di  klaster  1  dalam  memenuhi  kebutuhan 
sehari-hari  berupa  beras  lebih  besar  daripada  rumah  tangga 
klaster 2.  

Dari  karakteristik  tiap  variabel,  dapat  disimpulkan  bahwa 
rumah  tangga  yang  berada  di  klaster  1  memiliki  tingkat 
kemiskinan  yang  lebih  tinggi  dibandingkan  rumah  tangga  di 
klaster  2.  Kemudian  dilakukan 
atau 
membandingkan  dengan  hasil  pengelompokkan  dengan  tolak 
ukur  kemiskinan  versi  BPS  menggunakan  pendekatan 
pengeluaran.  Dari  hasil  klasifikasi  menggunakan  garis 
kemiskinan,  didapatkan  bahwa  ada  446  rumah  tangga  yang 
diklasifikasikan  sebagai  rumah  tangga  miskin,  dan  3288 
rumah tangga diklasifikasikan tidak miskin. 

benchmarking 

Gambar 6. Perbandingan pengelompokkan BPS dengan Data Mining 

 Rumah tangga miskin artinya rumah tangga yang memiliki 
besaran  pengeluaran  perkapita  perbulan  kurang  dari  garis 
kemiskinan  Provinsi  D.I.  Yogyakarta  sebesar  Rp  432.026. 
Jika  membandingkan  hasil  klasifikasi  rumah  tangga  miskin 
menggunakan  pendekatan  pengeluaran  dengan  data  mining, 
diperoleh akurasi sebesar 77.21%.  

Gambar 7. Diagram venn hasil dari BPS dan Data Mining 

rumah 

Dari  3734  sampel 

tangga  Susenas  Provinsi 
D.I.Yogyakarta,  ada  2883  rumah  tangga  yang  tepat  masuk 
kedalam kelompoknya, artinya dari hasil pengelompokkan  K-
prototype sama dengan hasil pengelompokkan atau klasifikasi 
menggunakan  garis  kemiskinan.  Sebaliknya  ada  851  rumah 
tangga  yang  masuk  kedalam  kelompok  yang  berbeda.  Ada 
647  rumah  tangga  yang  dikategorikan  miskin  dari  hasil  dari 
proses  data  mining  tetapi  masuk  kedalam  kategori  tidak 
miskin oleh BPS dan ada 204 rumah tangga tidak miskin yang 
dikategorikan menjadi miskin yang bisa dilihat pada gambar 7. 

 7 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
[5] 

[6] 

[7] 

[8] 

[9] 

[10] 

[11] 

[12] 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

F.  Astuti,  “Penerapan  Data  Mining  Untuk  Clustering  Data 
Penduduk  Miskin  Menggunakan  Algoritma  Hard  C-Means,”  Data 
Manaj. dan Teknol. Inf., vol. 18, no. 1, pp. 64–69, 2017. 

S.  Solorio-fernández,  A.  Carrasco-ochoa,  L.  E.  Erro,  and  S.  M. 
Tonantzintla,  “Unsupervised  Feature  Selection  Method  for  Mixed 
Data © Computer Science Department,” no. Ccc, 2019. 

M. Dash and H. Liu, “Feature selection for clustering,” Lect. Notes 
Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes 
Bioinformatics), vol. 1805, pp. 110–121, 2000, doi: 10.1007/3-540-
45571-x_13. 

A. Daemen and B. De Moor, “Development of a kernel function for 
clinical data,” Proc. 31st Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. 
Eng.  Futur.  Biomed.  EMBC  2009,  no.  September,  pp.  5913–5917, 
2009, doi: 10.1109/IEMBS.2009.5334847. 

M.  I.  Arsa  and  R.  Nooraeni,  “Kombinasi  Algoritma  Genetika  dan 
Fuzzy K-Prototype Untuk Pengelompokan Data Campuran,” pp. 1–
15, 2018. 

R. Nooraeni, “Metode Cluster Menggunakan Kombinasi Algoritma 
Cluster  K-Prototype  Dan  Algoritma  Genetika  Untuk  Data  Bertipe 
Campuran  Cluster  Method  Using  a  Combination  of  Cluster  K- 
Prototype  Algorithm  and  Genetic  Algorithm  for  Mixed  Data,”  pp. 
81–97, 2016. 

J.  Han,  M.  Kamber,  and  J.  Pei,  Data  Mining:  Concepts  and 
Techniques, no. June 2015. 2012. 

A.  F.  Febrianti,  A.  H.  Cabral,  and  G.  Anuraga,  “K-Means 
Clustering  Dengan  Metode  Elbow  Untuk  Pengelompokan 
Kabupaten  Dan  Kota  Di  Jawa  Timur,”  Semin.  Nas.  Has.  Ris.  dan 
Pengabdi.  -SNHRP,  pp.  863–870,  2018,  [Online].  Available: 
http://karyailmiah.unipasby.ac.id/wp-content/uploads/2019/04/K-
Means-Artikel.pdf. 

[13] 

N. Putu, E. Merliana, and A. J. Santoso, “Analisa Penentuan Jumlah 
Cluster Terbaik pada Metode K-Means,” pp. 978–979. 

[14] 

D. B. F. Data and B. File, “Kamus Data,” pp. 1–23, 2019. 

VII. 

PENUTUP 

Berdasarkan  penelitian  yang  sudah  dilakukan,  kombinasi 
dari  algoritma  USFSM  dan  metode  K-prototype  dalam 
mengelompokkan objek berdasarkan kemiripannya (clustering) 
sudah  cukup  baik.  Dataset  ujicoba  menunjukkan  ACC 
mengalami  kenaikan  nilai  jika  USFSM  diterapkan  sebagai 
preprocessing  datanya  sebelum  dilakukan  proses  clustering. 
USFSM  dengan  pengambilan  sampel  acak  diterapkan  untuk 
menemukan  fitur  relevan  dari  data  Susenas  yang  selanjutnya 
akan  dilakukan  teknik  data  mining  sesuai  dengan  metode 
penelitian yang sudah dibuat.  

Kesimpulan  yang  dapat  diambil  dari  penelitian  ini  antara 
lain  bahwa  hasil  simulasi  seleksi  atribut  menggunakan 
algoritma  USFSM  terpilih  46  atribut  relevan  dari  total  356 
atribut.  Atribut  tersebut  terpilih  juga  karena  hasil  evaluasi 
menggunakan  CVC  dan  total  cost  untuk  tiap  percobaan 
seleksi  atributnya.  Kemudian  ditentukan  jumlah  klaster  yang 
akan digunakan  yaitu sebanyak 2 klaster berdasarkan  metode 
k-elbow.  Dimana  dari  hasil  algoritma  K-prototype  untuk 
proses  clustering  dapat  terbentuk  klaster  pertama  berisi  889 
rumah tangga yang bisa dikategorikan miskin dan 2845 rumah 
tangga  di  klaster  kedua  yang  dikategorikan  tidak  miskin 
berdasarkan hasil analisis karakteristik dari tiap atribut kedua 
cluster  tersebut.  Dari  kedua  cluster  yang  terbentuk  dapat 
dilakukan  benchmarking  terhadap  penentuan  rumah  tangga 
miskin  versi  BPS  menggunakan  garis  kemiskinan.  Hasil 
perbandingan  kedua  metode  klasifikasi  menunjukkan  akurasi 
yang  didapat  dari  metode  K-prototype  sebesar  77,21%,  yang 
artinya  terdapat  2883  rumah  tangga  yang  dikelompokkan 
secara tepat atau sama terhadap klasifikasi BPS.  

Adapun saran untuk penelitian selanjutnya, diharapkan bisa 
memakai algoritma clustering yang berbeda dengan penelitian 
ini,  atau  membandingkan  algoritma  dalam  penelitian  ini 
dengan  algoritma  yang  akan  dikaji  dalam  penelitian 
selanjutnya.  Kemudian  dapat  dilakukan  kajian 
terhadap 
cakupan  yang  lebih  luas,  misalnya  tingkat  nasional  untuk 
mendapatkan informasi lebih bagus dan merata. Karena data-
driven sendiri bergantung penuh pada data yang dipakai dalam 
penelitian,  sehingga  cakupan  penelitian  yang  berbeda  akan 
menghasilkan pengelompokkan yang berbeda pula. 

[1] 

[2] 

[3] 

DAFTAR PUSTAKA 

F.  D.  Astuti,  “Seleksi  Atribut  Menggunakan  Information  Gain 
Untuk  Clustering  Penduduk  Miskin  Dengan  Validity  Index  Xie 
Beni,”  Teknika,  vol.  6,  no.  1,  pp.  61–65,  2017,  doi: 
10.34148/teknika.v6i1.58. 

W.  K.  Indonesia,  “Indeks  Kemiskinan  Multidimensi :  Memotret,” 
no. April, 2019. 

J. I. Matematika and P. Matematika, “Irtania Muthia Rizki Magister 
Statistika  ,  FMIPA  Unpad  ,  Bandung  Septiadi  Padmadisastra 
Departemen Statistika , FMIPA Universitas Padjadjaran , Bandung 
Bertho  Tantular  Departemen  Statistika 
,  FMIPA  Universitas 
Padjadjaran , Bandung Menurut Biro Pusat Stat,” vol. 9, no. 2, pp. 
63–74, 2017. 

[4] 

B. P. Statistik and E. Di, “Tingkat Kemiskinan Di Indonesia Tahun 
2007,” no. 38, pp. 1–5, 2007. 

 8 / 8 

 
 
 
 
"
221710060,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Pembangunan Aplikasi PARA
Instrumen Pengumpulan Data Pendapat dari Alumni KS, Rekan, dan
Atasan
Xenan Fathurrahman (221710060, 4SI1)

Dosen Pembimbing: Dr. Budiasih

Ringkasan— Data pendapat dari alumni suatu perguruan
tinggi, rekan sejawat, dan atasan penting untuk dikumpulkan
dan bermanfaat bagi perguruan tinggi bersangkutan atau
stakeholder lainnya. Pengelolaan data pendapat tersebut secara
digital diharapkan dapat memudahkan dalam pengoperasiannya
sehingga diperlukan suatu aplikasi yang dapat melakukan hal ini.
Tujuan penelitian ini adalah untuk membangun aplikasi PARA.
ini digunakan untuk mengumpulkan data pendapat
Aplikasi
dari alumni Prodi Komputasi Statistik di
lingkungan BPS
Kabupaten/Kota, BPS Provinsi, dan BPS Pusat; rekan sejawat;
ini
dan atasan dalam kantor yang sama. Manfaat aplikasi
memberikan: informasi mengenai seberapa sering suatu mata
kuliah yang diterima alumni KS Polstat STIS dan diterapkan
pada kegiatan BPS; informasi mata kuliah yang belum diterima
saat kuliah dan perlu diberikan kepada mahasiswa KS Polstat
STIS; informasi pendapat rekan sejawat dan atasan mengenai
kinerja alumni KS; serta informasi total sebaran alumni KS di
BPS Pusat, BPS Provinsi, dan BPS Kabupaten/Kota.

Kata Kunci— Pembangunan, Aplikasi, Alumni KS, Politeknik

Statistika STIS.

I. LATAR BELAKANG

Politeknik Statistika STIS (Polstat STIS) adalah Perguruan
Tinggi Kedinasan di bawah naungan Badan Pusat Statistik
(BPS) sehingga alumni ditempatkan sebagai pegawai BPS di
semua bidang (IPDS dan Non IPDS).
IPDS merupakan
singkatan dari Integrasi Pengolahan dan Diseminasi Statistik.
Program Studi (Prodi) yang terdapat di Polstat STIS adalah
Prodi D3 Statistika, Prodi D4 Statistika, dan Prodi D4
Komputasi Statistik (KS). Pada Prodi KS, mahasiswa
diberikan mata kuliah komputasi
tentang pengetahuan
jaringan, pemrograman komputer, dan pengolahan data
dengan mengombinasikan metode statistik dan algoritma data
mining.

Mata kuliah yang diterima oleh alumni KS diharapkan
dapat menunjang kegiatan di BPS. Salah satu cara mengetahui
seberapa sering mata kuliah komputasi diterapkan pada
kegiatan BPS,
cara mengumpulkan
pendapat dari alumni KS yang saat ini bekerja di BPS (IPDS
maupun Non IPDS).

dilakukan

dengan

Sistem Informasi Pendapat Alumni KS, Rekan, dan Atasan
(Aplikasi PARA) dibangun untuk mengumpulkan pendapat
dari alumni KS terkait seberapa sering mata kuliah diterapkan
pada kegiatan BPS dan masukan mata kuliah yang perlu
diberikan kepada mahasiswa KS; pendapat rekan dan atasan
terkait kinerja alumni dan masukan mata kuliah yang perlu
diberikan kepada mahasiswa KS. Hasil penelitian ini, Aplikasi
PARA yang jika diterapkan, diharapkan dapat memberikan:

informasi mengenai seberapa sering suatu mata kuliah yang
diterima alumni KS dan diterapkan pada kegiatan BPS;
informasi mata kuliah yang belum diterima saat kuliah dan
perlu diberikan kepada mahasiswa KS; informasi pendapat
rekan sejawat dan atasan mengenai kinerja alumni KS; serta
informasi total sebaran alumni di BPS Pusat, BPS Provinsi,
dan BPS Kabupaten/Kota.

II. TUJUAN PENELITIAN

Tujuan Penelitian ini adalah membangun Aplikasi PARA.
Sistem ini digunakan untuk mengumpulkan data pendapat dari
alumni KS di lingkungan BPS Kabupaten/Kota, BPS Provinsi,
dan BPS Pusat; rekan sejawat; dan atasan dalam kantor yang
sama.

III. PENELITIAN TERKAIT

Berikut adalah penelitian terkait yang berhubungan dengan

penelitian skripsi ini.

yang

Putri

(2018)

berjudul
dalam penelitiannya
“Implementasi Metode Rapid Application Development Pada
Website Service Guide Waterfall Tour South Sumatera” yang
diterbitkan pada jurnal SISFOKOM menjelaskan tentang
bagaimana model pengembangan sistem RAD diterapkan
pada pengembangan sebuah website [6]. Berdasarkan dari
penelitian tersebut, penulis memperoleh informasi mengenai
penjelasan umum RAD dan langkah-langkah mengembangkan
sistem dengan menggunakan RAD.

Budi (2017) dalam penelitiannya yang berjudul Tracer
Study ITB 2016 Angkatan 2009 menjelaskan tentang Sistem
Institut
Informasi Tracer Study yang diterapkan pada
Teknologi Bandung. Tujuan dari penelitian ini secara garis
besar untuk menganalisis data alumni dari ITB Angkatan 2009
tentang pekerjaan selepas lulus dari ITB beserta seberapa
ITB terhadap
besar kompetensi yang didapatkan dari
pekerjaan alumni sekarang ini [1]. Berdasarkan dari penelitian
tersebut, penulis memperoleh informasi mengenai cara dan
metode pengumpulan data alumni yang digunakan dalam
penyusunan kuesioner.

Febiharsa (2019) dalam penelitiannya yang berjudul “Uji
Fungsionalitas (Blackbox Testing) Sistem Informasi Lembaga
Sertifikasi Profesi (SILSP) Batik Dengan Appperfect Web
Test Dan Uji Pengguna” yang diterbitkan pada Joined Journal.
Penelitian ini menjelaskan mengenai penjelasan umum black
box testing, cara pengujian sistem dengan metode ini, dan
contoh analisis hasil pengujian [2]. Penelitian ini bermanfaat

1 / 8

untuk mengetahui tahapan cara atau metode pengujian sistem
dengan menggunakan black box testing dilakukan serta
mengetahui analisis dari hasil pengujian yang dilakukan.

Apriliana (2015) dalam penelitiannya

yang berjudul
“Pengujian Usability Website Menggunakan System Usability
Scale” yang diterbitkan pada jurnal IPTEK-KOM. Penelitian
ini menjelaskan mengenai penjelasan umum system usability
scale, cara pengujian sistem dengan metode ini, dan contoh
analisis hasil pengujian [5]. Penelitian ini bermanfaat untuk
mengetahui
tahapan cara atau metode pengujian sistem
dengan menggunakan system usability scale dilakukan serta
mengetahui analisis dari hasil pengujian yang dilakukan.

IV. METODE PENELITIAN

A. Metode Pengumpulan Data.

Pertama, membaca beberapa

referensi berupa buku
mengenai dasar-dasar pemrograman web. Referensi [3] dan [4]
yang berupa buku yang dimaksud menjelaskan mengenai
pemrograman web, cara penggunaan kode program, dan
contoh penerapan kode program pada aplikasi tertentu. Hal
yang dilakukan selanjutnya
adalah membaca beberapa
penelitian terkait mengenai sistem pengumpulan data alumni
dan cara untuk menguji sistem dengan metode black box
testing dan system usability scale.

Kedua, melakukan interview kepada beberapa dosen yang
memiliki keahlian (expert) di bidang sistem informasi dan
pemrograman. Hal ini untuk mengetahui cara merancang dan
mengembangkan sistem yang baik, dan membantu dalam
mencari solusi ketika menemui masalah ketika melakukan
coding program. Selanjutnya, melakukan uji coba aplikasi
PARA dengan cara interview untuk mendapatkan masukan
terkait aplikasi kepada alumni Prodi KS yang ditempatkan di
BPS.

Ketiga, melakukan evaluasi sistem dengan dengan cara
menyebarkankan kuesioner System Usability Scale (SUS)
secara online. Survei yang dilaksanakan menggunakan Google
Form untuk menyampaikan pertanyaan. Objek pengujian
sistem yang dilakukan meliputi tiap-tiap halaman kuesioner
sebagai fitur untuk pengisian data dan halaman admin sebagai
fitur untuk pengelolaan data. Tujuannya untuk mengetahui
seberapa efektif, efisien, dan memuaskan sistem yang telah
dikembangkan bagi pengguna. Target
responden untuk
pengujian sistem dengan menerapkan survei SUS adalah
beberapa orang sampel yang terdiri dari pegawai BPS dan
mahasiswa Polstat STIS.

B. Metode Pengembangan Sistem.

Penelitian ini menerapkan metode pengembangan sistem
dengan model RAD. Model RAD yang diterapkan pada saat
ini merupakan hasil
transisi dari metode waterfall yang
terapkan sebelumnya. Transisi ini dilakukan karena pada awal
pengembangan sistem ini dilakukan secara teratur dan
sistem
tahap
berkesinambungan
dilakukan setelah menyelesaikan tahap sebelumnya), namun
di
tengah pengembangan sistem terdapat masukan dari
stakeholder sehingga ada penambahan fitur pada sistem. Hal

pengembangan

(tiap

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

tersebut menyebabkan model waterfall sudah tidak dapat lagi
diterapkan sehingga dilakukan transisi ke model RAD. Tahap
pengembangan sistem dengan model RAD adalah sebagai
berikut :
1. Tahap Perencanaan Kebutuhan.

Langkah awal merupakan penentuan mengenai proses
bisnis sistem yang akan dikembangkan. Secara sederhana,
proses bisnis sistem dapat dijelaskan dalam langkah berikut:
(1)Responden melakukan pengisian data, (2)Responden
sistem merekam data dan
mengirim data
(3)Sistem melakukan
disimpan
data
pengolahan
dapat
sehingga
(4)Admin melakukan
ditampilkan hasil akumulasinya,
pemeriksaan
perlu),
(bila
data
pengeditan
data,
penghapusan data (bila perlu); sehingga sistem dapat
memproses aksi tersebut dan menampilkan feedback.

jawaban,
dalam database,
dan

penghitungan

Sistem akan dikembangkan dan dibangun dengan
menggunakan laptop beserta peralatan mendukung lainnya
milik pribadi, berikut adalah beberapa detail kebutuhan
untuk mengembangkan sistem :
a) Laptop dengan spesifikasi: Intel Core i5, RAM 12 GB,

memori penyimpanan 1 TB

b) Sistem membutuhkan koneksi internet
c) Sistem yang akan dibuat berbasis Web
d) Data yang direkam akan diatur dalam basis data

menggunakan MySQL (phpmyadmin)

e) Pengembangan sistem awalnya akan menggunakan local
server XAMPP. Setelah sistem berjalan dengan baik
akan di-hosting menggunakan domain kampus.
digunakan
yang
pengembangan sistem adalah Google Chrome.

Browser

f) Web

untuk

akan

g) Text Editor yang digunakan untuk menuliskan kode
program saat sistem dikembangkan adalah Visual Studio
Code.

2. Workshop Desain RAD.

Proses ini terdiri dari dua tahap yang berjalan secara
siklus. Tahap pertama adalah Interaksi Pengguna dan tahap
kedua adalah Pembuatan Sistem.

Tahap Interaksi Pengguna berisikan permintaan dan

masukan pengguna berikut ini.
a) Pengguna sistem terdiri dari empat pihak: alumni Polstat
STIS, rekan alumni, atasan alumni, dan admin. Alumni,
rekan, dan atasan merupakan responden; dan admin
merupakan pengelola data responden.

b) Sistem paling tidak dapat menampilkan halaman :

i. Halaman beranda untuk antarmuka awal pembukaan

web.

ii. Antarmuka untuk responden, yang berisi

form

pengisian data.

iii. Antarmuka login untuk admin.
iv. Antarmuka untuk admin, yang berisi tampilan untuk

mengelola data.

c) Admin dapat login supaya tidak sembarang orang dapat
mengakses halaman admin (halaman olah data). Admin
dapat melakukan pengubahan dan penghapusan data
jika data yang diterima dirasa tidak valid.

2 / 8

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

tersedianya beberapa framework gratis yang tersedia di
internet.

Pengembangan sistem ini dilakukan dengan menerapkan
beberapa aplikasi dan metode dengan harapan supaya sistem
dapat berjalan dengan baik dan dapat menyelesaikan masalah
yang ada. Aplikasi PARA dikembangkan dengan basis web
(supaya dapat berjalan di berbagai platform), menggunakan
bahasa pemrograman utama PHP, menggunakan aplikasi
manajemen basis data MySQL, dan setelah sistem selesai
dibuat akan dilakukan pengujian dengan metode black-box
testing dan SUS (System Usability Scale).

Hasil yang diharapkan pada penelitian ini tentunya adalah
keberhasilan dalam membangun Aplikasi PARA untuk
mengumpulkan pendapat alumni KS Polstat STIS mengenai
penerapan mata kuliah komputasi pada kegiatan BPS. Selain
tersebut
itu, hasil dari pengumpulan pendapat
diharapkan dapat menjadi informasi yang bermanfaat untuk
stakeholder yang membutuhkan.

alumni

Berikut merupakan diagram dari kerangka pikir dengan

detail yang telah dijelaskan sebelumnya.

d) Admin dapat melihat progress pengisian kuesioner yang

diisi oleh responden.

e) Admin dapat melakukan ekspor data dengan format file

CSV.

f) Responden dapat mengakses halaman beranda dan

halaman kuesioner untuk pengisian data.
Tahap Pembuatan Sistem berisikan proses coding yang

terdiri dari hal-hal berikut ini.
a) Melakukan coding dalam pembuatan halaman login

admin.

b) Melakukan coding dalam pembuatan halaman form

pengisian data responden.

c) Melakukan coding dalam pembuatan halaman admin.
d) Melakukan coding dalam pembuatan tabel-tabel dan

pengaturan dalam basis data.

e) Melakukan coding terhadap pengiriman, pengambilan,

dan pengolahan data dari basis data.

f) Melakukan coding untuk menampilkan data dengan

menggunakan grafik.

g) Melakukan coding untuk fitur export file.
h) Melakukan coding untuk menampilkan feedback sistem,

misalkan pesan berhasil dalam melakukan sesuatu.

3. Implementasi.

Pada tahap ini dilakukan pemasangan aplikasi pada
server (hosting) supaya aplikasi bisa diakses di semua
perangkat yang memiliki web browser. Selanjutnya
dilakukan juga pengujian pada aplikasi. Berikut adalah
detail pengujian secara simultan dan integrasi dari tiap-tiap
komponen yang ada pada sistem atau perangkat lunak pada
penelitian ini.
a) Melakukan uji komponen pada nomor 3 apakah transisi

antar halaman sudah bekerja dengan baik.

b) Melakukan uji pada basis data apakah terjadi kesalahan
maupun basis data tidak bisa melakukan penyimpanan
data.

c) Melakukan uji apakah halaman form pengisian data
telah terhubung dengan basis data sehingga data yang
diinput dapat disimpan.

d) Melakukan uji apakah hasil output pengolahan data

sudah sesuai yang diharapkan.
juga
Pengujian

dilakukan

dengan mengumpulkan
masukan dan pendapat dari pengguna yang mengakses
aplikasi. Pengujian ini menggunakan Metode Evaluasi
yang akan dijelaskan pada pembahasan selanjutnya.

V. KERANGKA PIKIR

Pembangunan Aplikasi PARA dilakukan karena adanya
permasalahan yang ada dan kesempatan yang mendukung.
Permasalahan secara garis besar adalah belum adanya sistem
yang mengumpulkan data tentang pendapat alumni KS
mengenai penerapan mata kuliah komputasi pada kegiatan
BPS. Kesempatan atau situasi yang mendukung supaya
penelitian ini dilakukan adalah pihak-pihak yang terlibat
dalam pengumpulan data (khususnya alumni KS) memiliki
perangkat yang memadai supaya dapat mengakses sistem
dengan mudah dan pembuatan sistem ini sangat terbantu oleh

Gambar 1. Kerangka Pikir

VI. HASIL DAN PEMBAHASAN

A. Perkembangan Aplikasi PARA.

Berikut merupakan beberapa hal yang menunjukkan

perkembangan Aplikasi PARA.

3 / 8

a) Penambahan fitur “filter” pada bagian data penerapan mata
kuliah di halaman visualisasi data. Fitur ini disediakan
untuk membantu dalam melihat data penerapan mata
kuliah menurut kategori tertentu.
“progress”
fitur

halaman
pengelolaan data (alumni, rekan, dan atasan). Fitur ini
disediakan untuk memantau kemajuan pengisian kuesioner
oleh responden.

b) Penambahan

setiap

pada

c) Penambahan

fitur

pada

setiap

“ekspor”

halaman
pengelolaan data (alumni, rekan, dan atasan). Sistem dapat
mengekspor menjadi file dengan format CSV dan Excel.
Fitur ini disediakan untuk membantu pihak stakeholder
yang ingin menganalisis data lebih lanjut dengan aplikasi
lain, misalkan dengan R atau SPSS.

B. Rincian Aplikasi PARA.

Berikut merupakan rincian dari Aplikasi PARA, meliputi

fitur-fitur yang telah dikembangkan sampai saat ini.
a) Dapat diakses secara online melalui berbagai platform
dengan memakai web browser populer (misalkan Google
Chrome, Mozilla Firefox). Namun saat ini paling optimal
diakses melalui perangkat laptop atau komputer.

b) Mampu mengumpulkan data dari pihak alumni KS Polstat

STIS, rekan alumni, dan atasan alumni.

c) Mampu melakukan validasi pada setiap butir pertanyaan
pada kusioner untuk mengumpulkan data dari pihak
alumni KS Polstat STIS, rekan alumni, dan atasan alumni.
Hal ini bertujuan untuk memperkecil kemungkinan terjadi
pengguna menjawab
pada
nya
pertanyaan, terutama karena kesalahan pengetikan.

human

error

saat

d) Terdapat fitur autentikasi

jika ingin menggunakan fitur

admin.

e) Mampu melakukan perubahan data individu untuk data
alumni KS Polstat STIS, rekan alumni, maupun atasan
alumni dengan menggunakan pada fitur admin.

f) Mampu melakukan penghapusan data individu untuk data
alumni KS Polstat STIS, rekan alumni, maupun atasan
alumni pada fitur admin.

g) Mampu melakukan pemeriksaan data individu untuk data
alumni KS Polstat STIS, rekan alumni, maupun atasan
alumni pada fitur admin.

h) Mampu melakukan pencarian data individu berdasarkan
kata kunci nama untuk data alumni KS Polstat STIS, rekan
alumni, maupun atasan alumni pada fitur admin.

i) Mampu melakukan

pembatasan menampilkan

data
individu (pagination) untuk data alumni KS Polstat STIS,
rekan alumni, maupun atasan alumni pada fitur admin. Hal
ini dilakukan dengan tujuan supaya data yang ditampilkan
tidak terlalu banyak dan mencegah terjadinya pemuatan
data yang berlebih.

j) Mampu melakukan visualisasi data secara keseluruhan

pada fitur admin.

k) Terdapat fitur “filter” pada bagian data penerapan mata

kuliah di halaman visualisasi data.

l) Terdapat fitur “progress” pengisian kuesioner pada setiap
halaman pengelolaan data (alumni, rekan, dan atasan).
m) Mampu melakukan ekspor data dengan format file berupa

file Excel dan file CSV.

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

C. Rancangan Arsitektur Sistem.

CodeIgniter

Aplikasi PARA merupakan sebuah sistem yang dibangun
dengan menggunakan
dan
framework
menggunakan bahasa pemrograman utama PHP. Sistem ini
juga menggunakan bahasa pemrograman HTML, Javascript,
dan CSS untuk mendukung beberapa fitur dan meningkatkan
tampilan antarmuka. Berkaitan dengan tampilan
kualitas
antarmuka sistem, Aplikasi PARA menggunakan framework
Bootstrap dengan bahasa pemrograman CSS supaya tampilan
sistem menjadi lebik menarik dan dapat menyesuaikan ketika
sistem diakses dari perangkat dengan berbagai ukuran layar
(responsif). Aplikasi
framework
Chart.JS untuk menampilkan beberapa jenis grafik yang
digunakan sistem untuk menampilkan visualisasi data.
Aplikasi PARA menggunakan aplikasi manajemen basis data
(DBMS) MySQL untuk mengelola data yang ada di basis data.
dengan
sistem ini
jenis perangkat yang terhubung
menggunakan berbagai
dengan koneksi
internet yang memadai dan terdapat web
browser untuk mengakses link menuju situs sistem. Perangkat
yang
smartphone,
komputer, dan tablet.

PARA menggunakan

dimaksud misalnya

dapat mengakses

Pengguna

laptop,

berupa

Berikut merupakan gambaran arsitektur

sistem dari

Aplikasi PARA.

Gambar 2. Arsitektur Sistem

D. Proses Bisnis Aplikasi PARA.

Proses bisnis sistem digambarkan secara umum melalui
diagram activity pada Gambar 3. Terdapat tiga peran pada
proses bisnis, yaitu responden (alumni KS, rekan alumni,
maupun, dan alumni), sistem (aplikasi), dan admin (pengelola
data).

Pihak responden merupakan pihak yang menjadi objek
pengumpulan data. Responden pada awalnya akan di arahkan
menuju halaman kuesioner yang sesuai dan pada halaman
tersebut
responden diharapkan mengisi setiap pertanyaan
dengan jawaban yang sesuai. Setelah responden mengisi
setiap pertanyaan dan mengirim data tersebut, dilakukan
validasi untuk setiap butir jawaban dari pertanyaan yang ada.
Jika terdapat
jawaban yang tidak sesuai ketentuan, maka
sistem akan mengembalikan setiap jawaban tersebut (jawaban

4 / 8

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

tidak akan hilang) ke halaman kuesioner dan sistem akan
menampilkan pesan kesalahan kepada responden, untuk
jawaban supaya sesuai
kemudian responden memperbaiki
dengan yang diharapkan. Jika semua jawaban telah tervalidasi
(sudah sesuai ketentuan) maka sistem akan menyimpan data
dari jawaban tersebut ke basis data.

fitur

pihak

admin

proses

autentikasi

Pihak admin merupakan pihak yang bertugas mengelola
dan memonitor data yang terdapat pada basis data melalui
sistem. Admin melakukan hal tersebut pada sebuah halaman
admin atau halaman pengelolaan data, namun pihak admin
login terlebih
harus melakukan autentikasi melalui
dahulu melalui halaman login admin. Sistem yang akan
melakukan
dengan
mencocokan username dan password yang diinput dengan
yang terdapat di basis data. Jika proses autentikasi gagal maka
pihak yang mencoba login tersebut akan tetap di halaman
tersebut dan sistem akan menampilkan pesan bahwa terdapat
kesalahan. Jika proses autentikasi berhasil maka pihak admin
akan diarahkan ke halaman utama admin. Pihak admin dapat
melakukan pengelolaan data dengan cara mengubah data
individu responden, menghapus data individu responden,
melihat detail data individu responden, dan melihat visualisasi
data secara umum. Jika pihak admin memutuskan akan
mengubah data individu responden, maka sistem akan
melakukan validasi terlebih dahulu. Jika gagal maka sistem
akan menampilkan pesan bahwa data terkait gagal untuk
diubah. Jika berhasil maka sistem akan menampilkan pesan
bahwa data telah berhasil diubah, data yang berhasil diubah
ini maksudnya adalah data terkait yang terdapat pada basis
data berhasil diperbarui. Hal yang mirip juga terjadi apabila
admin memutuskan untuk menghapus data individu responden.
Jika gagal maka sistem akan menampilkan pesan bahwa data
terkait gagal untuk dihapus. Jika berhasil maka sistem akan
menampilkan pesan bahwa data telah berhasil dihapus, data
yang berhasil dihapus ini maksudnya adalah data terkait yang
terdapat pada basis data berhasil untuk dihilangkan.

Hal yang selanjutnya dapat dilakukan oleh pihak admin
adalah melihat detail data individu responden. Maksud dari
hal
tersebut adalah pihak admin dapat melihat detail dari
setiap jawaban yang diisi oleh responden pada kuesioner.
Pihak admin juga dapat melihat visualisasi data secara umum
melalui halaman visualisasi data. Visualisasi data secara
umum adalah pengolahan data yang terdapat pada basis data
secara simultan sehingga diperoleh ringkasan data. Visualisasi
data ini memiliki tujuan utama untuk menggambarkan data
yang ada secara umum dan membandingkan antara data
dengan karakteristik tertentu dengan data yang lain. Jika pihak
admin ingin keluar dari halaman admin, dapat mengakses
menu logout dan sistem akan mengarahkan ke halaman login
admin. Selain itu, pihak admin juga dapat melakukan eksport
data yang ada pada basis data dengan format file Excel (.xls)
dan format file CSV (.csv). Tujuan disediakan fitur export ini
untuk memudahkan
hendak
pemindahan
dilakukan pengolahan data lanjutan dengan aplikasi lain.

apabila

data

Gambar 3. Activity Diagram Proses Bisnis Sistem.

E. Desain Logo Aplikasi PARA.

Gambar di bawah ini merupakan desain logo sebagai
identitas yang unik aplikasi PARA. Logo ini merupakan hasil
karya pribadi penulis sekaligus sebagai pembuat aplikasi
PARA.

Gambar 4. Logo Aplikasi PARA.

5 / 8

F. Implementasi Antarmuka Aplikasi PARA.

Berikut merupakan implementasi antarmuka pada Aplikasi

PARA.

Gambar di bawah ini merupakan hasil tangkapan layar dari
halaman beranda Aplikasi PARA. Halaman ini berisikan
penjelasan singkat mengenai aplikasi, dan penjelasan singkat
tiap-tiap
tiga kuesioner beserta
kuesioner.

link untuk mengakses

Gambar 5. Halaman Beranda Aplikasi PARA.

Gambar di bawah ini merupakan hasil tangkapan layar dari
salah satu halaman kuesioner Aplikasi PARA. Halaman ini
berisikan pertanyaan-pertanyaan yang akan dijawab oleh
responden dan tombol untuk mengirimkan data jawaban
tersebut. Pertanyaan pada kuesioner ada yang sifatnya wajib
diisi (ditandai dengan tanda bintang berwarna merah) dan ada
yang sifatnya tidak wajib diisi
(ditandai dengan kalimat
“boleh dikosongkan”).

Gambar 6. Halaman Kuesioner Aplikasi PARA.

Gambar di bawah ini merupakan hasil tangkapan layar dari
halaman login admin Aplikasi PARA. Halaman ini berisikan
kolom username, kolom password, dan tombol login; ketiga
hal tersebut digunakan untuk proses autentikasi supaya bisa
mengakses fitur pengelolaan data. Syarat supaya pengguna
berhasil melewati proses autentikasi: kolom username dan
kolom password harus diisi, dan isiian username dan
password harus sama dengan yang ada di basis data.

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Gambar 7. Halaman Login Admin Aplikasi PARA.

Gambar di bawah ini merupakan hasil tangkapan layar dari
halaman admin Aplikasi PARA. Halaman ini berisikan fitur-
fitur yang digunakan untuk mengelola data responden, seperti:
ubah data, hapus data, pencarian data, dan ekspor data.
Halaman pengelolaan data ini ada tiga macam, masing-masing
untuk mengelola data alumni Polstat STIS, mengelola data
atasan, dan mengelola data rekan.

Gambar 8. Halaman Admin Aplikasi PARA.

Gambar di bawah ini merupakan hasil tangkapan layar dari
halaman visualisasi data Aplikasi PARA. Halaman ini
berisikan grafik-grafik yang menunjukkan gambaran secara
umum dari data-data yang telah tersimpan di dalam basis data.

Gambar 9. Halaman Visualisasi Data Aplikasi PARA.

6 / 8

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

pendapat responden layak untuk digunakan namun belum
sempurna.

B. Rencana Pengembangan Aplikasi PARA.
Aplikasi PARA yang dihasilkan saat

ini masih belum
sempurna sehingga bisa dikembangkan lebih lanjut. Berikut
merupakan hal-hal yang masih kurang dari sistem ini menurut
penulis dan pihak lain sehingga kedepannya dapat dilakukan
pengembangan.
1. Pengembangan tampilan antarmuka agar sesuai dengan

perkembangan teknologi.

2. Pengembangan fitur admin seperti: pihak pengelola dapat
mengubah passsword dan terdapat level pihak pengelola
data.

3. Penggunaan dua basis data dan cron job pada basis data.
4. Pembuatan panduan penggunaan sistem pada bagian

admin.

5. Fitur yang memungkinkan pihak responden (Alumni KS,
rekan kerja, dan atasan langsung) mengubah data nya
setelah data dikirimkan. Saat ini, perubahan data hanya
dapat dilakukan oleh pihak admin.

6. Pengembangan pada fitur analisis data. Sistem terdapat
analisis data sederhana pada bagian visualisasi data,
namun sebaiknya fitur ini dikembangkan supaya pengguna
tidak perlu berfikir lebih.

DAFTAR PUSTAKA
[1] B. S. Budi, Tracer Study ITB 2016 Angkatan 2009. Bandung: ITB, 2017.
I M. Sudana, dan N. Hudallah, Uji Fungsionalitas
[2] D. Febiharsa,
(Blackbox Testing) Sistem Informasi Lembaga Sertifikasi Profesi (SILSP)
Batik Dengan Appperfect Web Test Dan Uji Pengguna. Jurnal Vol. 1, No.
2, hal. 117-126, Januari 2019.

[3] D. Setiawan, Buku Sakti Pemrograman Web. Yogyakarta : START UP.
[4] H. Priyanto, dan J. K. Kawistara, Pemrograman Web Edisi Revisi.

Bandung : INFORMATIKA, 2017.

[5] I. Apriliana H. N., P. I. Santoso, dan R. Ferdiana, Pengujian Usability
Website Menggunakan System Usability Scale. Jurnal Vol. 17, No. 1, hal.
31-38, Juni 2015.

[6] M. P. Putri, dan H. Effendi, Implementasi Metode Rapid Application
Development Pada Website Service Guide “Waterfall Tour South
Sumatera”.

G. Hasil Pengujian Aplikasi PARA.

Pengujian Aplikasi PARA dilakukan dengan dua uji yaitu
dengan black box testing dan uji SUS (system usability scale).
Black box testing dilakukan dengan menguji apakah sistem
berhasil menjalankan fungsi dan menampilkan antarmuka
yang sesuai, dan uji ini dilakukan secara multiplatform yaitu
dengan menggunakan sistem operasi Windows 10 dan
Android, serta menggunakan web browser Google Chrome,
Microsoft Edge, dan Browser bawaan. Hasil yang didapatkan
adalah Aplikasi PARA secara
umum telah mampu
fungsi dan menampilkan beberapa
menjalankan semua
antarmuka yang diharapkan. Satu-satunya masalah yang
ditemui ketika pengujian sistem ini adalah tidak sempurna nya
tampilan sistem ketika menampilkan grafik yang ada pada
halaman visualisasi data ketika halaman tersebut dibuka
baik
dengan menggunakan
menggunakan web browser Google Chrome maupun Browser
bawaan.

sistem operasi Android,

Uji SUS dilakukan dengan mengumpulkan jawaban
responden mengenai sepuluh pertanyaan pada kuesioner SUS,
mengolah hasil jawaban tersebut menjadi skor, dan melakukan
penarikan kesimpulan dari hasil skor akhir. Hasil yang
didapatkan adalah skor akhir yang diperoleh 81,00 yang
berada di interval 80,00 sampai 90,00. Jadi, Apikasi PARA
berdasarkan pendapat dari responden mendapatkan predikat B
dan bisa dikatakan secara umum Aplikasi PARA sudah layak
digunakan dan diterima oleh responden. Berikut merupakan
kriteria yang digunakan sebagai penarikan kesimpulan
berdasarkan skor hasil uji SUS.

TABEL I
INTERPRETASI SKOR SUS

Skor SUS

Predikat (Grade)

Lebih dari 90,00
80 - 90,00
70 - 79,99
60 - 69,99
Kurang dari 60

A
B
C
D
F

VII.

PENUTUP

A. Kesimpulan.
Berikut

ini beberapa kesimpulan yang dapat diambil

setelah melakukan penelitian.
1. Aplikasi PARA dibangun untuk mengumpulkan data
pendapat alumni KS Polstat STIS, rekan sejawat, dan
telah disediakan fitur
atasan langsung. Pada sistem ini
pengelolaan data tersebut.

2. Berdasarkan hasil uji Blackbox yang telah dilakukan
disimpulkan bahwa Aplikasi PARA mampu menjalankan
setiap fungsi dan mampu menampilkan antarmuka, namun
tampilan grafik belum sempurna.

3. Berdasarkan hasil uji SUS yang telah dilakukan dengan
skor 81,00 disimpulkan bahwa Aplikasi PARA menurut

7 / 8

LAMPIRAN

2. Kuesioner Rekan beserta contoh pengisiannya.

1. Kuesioner Alumni beserta contoh pengisiannya.

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

3. Kuesioner Atasan beserta contoh pengisiannya.

8 / 8

"
221710059,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pembangunan Algoritma Advanced Preprocessing 
untuk Data Marketplace 

Wiwin Srimulyani (221710059, 4SD1) 
Dosen Pembimbing: Setia Pramana, Ph.D. 

Ringkasan— Karakteristik dari Big Data yaitu volume, variety, 
dan  velocity  menyebabkan  masalah  pada  kualitas  data  dalam 
proses  analisis,  khususnya  data  marketplace.  Hal  ini  membuat 
tidak semua data hasil proses scraping dapat digunakan sebagai 
data penunjang official statistics. Oleh karena itu, perlu dilakukan 
preprocessing  lebih  lanjut  untuk  mendapatkan  data  yang  lebih 
berkualitas.  Penelitian 
ini  bertujuan  untuk  membangun 
algoritma  preprocessing,  menghasilkan  direktori  toko  aktif  dari 
platform  marketplace,  dan  melakukan  pengecekan  performa 
algoritma.  Algoritma  utama  dibangun  menggunakan  prinsip 
Divide  and  Conquer.  Dari  hasil  penelitian,  dihasilkan  algoritma 
preprocessing lanjutan dengan 3 tahapan, yaitu pemrosesan data 
barang,  pembentukan  toko  aktif  berdasarkan  data  barang,  dan 
pemrosesan  toko.  Dari  algoritma  tersebut  dihasilkan  direktori 
dengan 10 atribut. Setelah pengecekan, algoritma yang dibangun 
tergolong sangat efisien dengan kelas fungsi notasi Big O adalah 
logaritma. 

Kata Kunci— preprocessing, marketplace, Big Data, algoritma, 

data cleaning. 

Perkembangan 

I.  LATAR BELAKANG 
teknologi  dan  masifnya  pergerakan 
masyarakat dalam bidang informasi membuat data bertambah 
secara  signifikan  dari  waktu  ke  waktu.  Berbagai  jenis  data 
dapat  kita  temui  di  belahan  dunia  dalam  bentuk,  tipe,  dan 
variasi yang beragam. Teknologi yang sering kita sebut sebagai 
internet  menjadi  kunci  utama  dalam  pertukaran  data  antar 
masyarakat dunia.  

fenomena  pergerakan  data 

Istilah  Big  Data  menjadi  representasi  yang  paling  sesuai 
untuk  menjelaskan 
secara 
eksponensial. Secara teori, Big Data mempunyai karakteristik 
3V, yaitu volume, variety (variasi), dan velocity (kecepatan) [1]. 
Beberapa  studi  juga  menambahkan  value  (nilai)  dan  veracity 
(kebenaran) sebagai salah satu karakteristik dari Big Data [2]. 
Big  Data  dapat  menjadi  salah  satu  peluang  yang  dapat 
dimanfaatkan  oleh  lembaga  statistik  pemerintah  di  berbagai 
telah 
negara.  Beberapa 
meletakkan  Big  Data  menjadi  salah  satu  sumber  untuk 
melengkapi  data  yang  dihasilkan  dari  metode  statistik 
konvensional  seperti  sensus  dan  survei  [3].  Data  yang 
dihasilkan  perlu  dikaji  lebih  dalam  lagi  apakah  data  tersebut 
dapat  berperan  sebagai  pengganti  atau  pelengkap  [4].  Seperti 
peran media sosial untuk berbagai jenis indikator, harga yang 
diperoleh dari informasi web, dan masih banyak lagi. 

statistik  pemerintah 

lembaga 

Dalam penerapannya, berbagai masalah dapat timbul akibat 
kelima  karakteristik  Big  Data  tersebut.  Volume  yang  sangat 
besar, data yang terus menerus bertambah setiap detiknya, dan 
juga variasi data yang sangat beragam membuat data tersebut 

tidak  bisa  dilakukan  penyimpanan  atau  pemrosesan  secara 
tradisional.  Terlebih  lagi  jika  data  yang  disimpan  bersifat 
unstructured.  Terdapat  sekitar  80%  data  yang  disimpan  oleh 
organisasi bersifat unstructured [5]. Data tersebut dapat terdiri 
dari  informasi  dari  media  sosial,  e-mail,  customer  call, 
komentar,  dan  masih  banyak  lagi.  Jenis  data  yang  beragam 
tersebut  dapat  menyebabkan  berbagai  masalah  dalam 
penyimpanan, pengambilan, dan analisis data [6]. 

oleh 

data 

ditunjukkan 

Salah  satu  hal  yang  harus  menjadi  perhatian  dalam 
pemrosesan  Big  Data  adalah  veracity  [7].  Data  yang 
dikumpulkan atau disimpan dapat mempunyai struktur berbeda 
yang akan berpengaruh pada kualitas data. Perbedaan struktur 
data ini dapat disebabkan oleh sumber data, teknik pemrosesan, 
dan  metodologi  pengambilan  data  yang  dipakai  [8].  Kualitas 
data  juga  dapat  bergantung  pada  seberapa  baik  data  tersebut 
terorganisasi.  Istilah  ini  sering  disebut  sebagai  messy  data. 
messy 
ketidaklengkapan, 
ketidakkonsistenan, dan ketidaktepatan waktu [1]. Oleh sebab 
itu,  Big  Data  dapat  mengandung  bias,  ambiguitas,  dan 
ketidakakuratan  yang  mana  perlu  diidentifikasi  dan 
dihilangkan  untuk  mengurangi  kesalahan 
inferensi  dan 
meningkatkan akurasi dari insight yang didapatkan [8]. Untuk 
mengatasi  hal  ini,  preprocessing  dapat  menjadi  solusi  yang 
tepat  dalam  mengatasi  berbagai  masalah  Big  Data  dalam 
analisis  [9].  Pada  saat  memperbaiki  data,  dapat  dilakukan 
beberapa 
teknik  preprocessing  diantaranya  yaitu  data 
integration,  data  cleaning,  data  transformation,  dan  data 
reduction. Tetapi, hal yang perlu diperhatikan lebih mendalam 
adalah  bagaimana  algoritma  preprocessing  yang  dibangun 
dapat  berjalan  secara  efisien  dan  efektif  dalam  menangani 
masalah-masalah  yang  mungkin  akan  timbul  akibat  dari 
karakteristik Big Data. 

Di  sisi  lain,  pemanfaatan  teknologi  dalam  segi  pemasaran 
dan  industri  retail  berkembang  sangat  pesat  di  Indonesia.  Di 
antara banyaknya jenis platform online yang digunakan untuk 
berjualan,  marketplace  menjadi  salah  satu  alternatif  utama 
masyarakat dalam berbelanja secara daring. Salah satu alasan 
menjamurnya platform pemasaran online atau biasa kita sebut 
dengan marketplace ini adalah servis yang baik dan akhirnya 
berdampak  pada  loyalitas  masyarakat  dalam  membelanjakan 
uangnya  [10].  Selain  itu,  marketplace  juga  menyediakan 
berbagai  macam  variasi  barang  dengan  harga  yang  beragam 
sehingga  memudahkan  masyarakat  dalam  membeli  barang 
yang diinginkan. 

Tak  terlepas  dari  pandemi,  covid-19  juga  mempengaruhi 
pilihan  masyarakat  dalam  menentukan  tempat  berbelanja. 
Tercatat  bahwa  aktivitas  belanja  online  melonjak  dari  4,7 
persen menjadi 28,9 persen [11]. Hal ini tentunya dimanfaatkan 

 1 / 8 

 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

IV. METODE PENELITIAN  

A.  Data dan Tools yang Digunakan 

Data  yang  digunakan  adalah  data  marketplace  hasil 
scraping.  Namun,  dalam  penelitian  ini  hanya  menggunakan 
provinsi DKI Jakarta saja dengan pertimbangan cost efficiency 
dan  kompleksitas  data.  Secara  keseluruhan,  data  yang 
digunakan yaitu Januari hingga Desember 2020. Namun, dalam 
penghitungan jumlah barang terjual setiap bulannya digunakan 
data mulai dari Desember 2019 hingga Januari 2021. 

Dalam  penelitian 

ini  menggunakan 

severless  data 
warehouse,  yaitu  bigquery  sebagai  media  untuk  menjalankan 
kode. Kode dituliskan dengan menggunakan Bahasa Structural 
Query Language (SQL). 

B.  Metode Penelitian 

Berikut merupakan rancangan metode yang dipakai untuk 

membangun algoritma preprocessing: 

oleh  berbagai  macam  platform  marketplace  untuk  berlomba-
lomba memberikan kebijakan yang akan mempengaruhi minat 
masyarakat  dalam  berbelanja,  seperti  menurunkan  harga, 
memberi  diskon,  dan  masih  banyak  lagi.  Hal  ini  akan 
berdampak terhadap struktur data yang diambil. 

beberapa 

telah  melakukan 

Badan Pusat Statistik Selaku Lembaga Statistik Pemerintah 
untuk 
Indonesia 
memanfaatkan  Big  Data  selaku  sumber  data  baru  untuk 
melengkapi statistik resmi baik dalam bidang ekonomi ataupun 
sosial.  Pemanfaatan  data  marketplace  sendiri  sudah  banyak 
dilakukan  oleh  BPS  diantaranya  adalah  tinjauan  Big  Data 
terhadap  covid-19  dan  pemanfaatan  data  marketplace  untuk 
melengkapi frame pada survei e-commerce tahun 2020. 

upaya 

Pengumpulan data toko melalui marketplace dapat menjadi 
peluang  yang  besar  bagi  lembaga  statistik  pemerintah  untuk 
jenis  survei. 
melengkapi  direktori 
Khususnya untuk direktori toko yang berkaitan dengan UMKM 
di  Indonesia,  direktori  toko  aktif  hasil  preprocessing  data 
marketplace dapat digunakan sebagai opsi pelengkap data. 

toko  pada  berbagai 

Namun, hal yang perlu diperhatikan disini adalah kebebasan 
setiap  orang  untuk  membuat  toko  dan  menjual  barang  di 
platform  marketplace  menciptakan  keberagaman  data  yang 
dihasilkan.  Penjual  dan  pembeli  dengan  bebas  melakukan 
transaksi kapan pun saja dalam 24 jam. Oleh sebab itu, terdapat 
sebagian data dan toko yang mengalami anomali. Anomali di 
sini justru timbul akibat kepraktisan dalam melakukan transaksi 
di platform online. Berbeda dengan toko yang secara fisik ada, 
penjual di platform online sering kali hanya laku di beberapa 
bulan saja, atau karena strategi marketing barang-barang yang 
dijual akan berganti-ganti sesuai tren yang terjadi di masyarakat. 
Oleh sebab itu, dalam penelitian ini akan dilakukan pemrosesan 
data  marketplace  dan  pencarian  anomali  data  dengan  cara 
membangun algoritma untuk menghasilkan direktori toko aktif. 

II.  TUJUAN PENELITIAN 

Berikut adalah tujuan utama dalam penelitian ini: 
1.  Membangun  algoritma  preprocessing  lanjutan  untuk 
menghasilkan  toko  aktif  berdasarkan  data  barang  dan 
data toko dari platform marketplace 

2.  Menghasilkan  direktori 

toko  aktif  dari  platform 

marketplace 

3.  Melihat  performa  algoritma  preprocessing  lanjutan 

yang dibangun. 

III. PENELITIAN TERKAIT 

Gambar 2. Flow Chart Metode Penelitian 

V.  KERANGKA PIKIR 

Gambar 1. peta literatur (literature map) 

Gambar 3. Kerangka Pikir Penelitian 

 2 / 8 

 
 
 
 
 
 
 
VI. HASIL DAN PEMBAHASAN 

A.  Pendefinisian Sub-Masalah atau Parameter Toko Aktif 

Proses penentuan parameter/sub-masalah dilakukan dengan 
mengidentifikasi dan menelusuri data barang marketplace yang 
sudah dilakukan pemeriksaan validitas dan konsistensi variabel 
jumlah  barang  terjual  bulanan,  penghilangan  outlier,  dan 
imputasi  bulan  Agustus  2020.  Data  barang  digunakan  untuk 
menentukan  aktif  atau  tidaknya  sebuah  toko.  Temuan  yang 
didapatkan  akan  dijadikan  sebagai  calon  parameter/sub-
permasalahan  pada  pendefinisian  toko  aktif  berdasarkan  data 
barang. 

Berikut merupakan temuan yang didapatkan: 

1)  Barang Tidak Pernah Terjual dalam Satu Tahun 

Dalam  satu  tahun,  ada  kemungkinan  bahwa  barang  tidak 
pernah  laku  terjual.  Barang  tidak  pernah  laku  terjual  ditandai 
dengan variabel jumlah barang yang sama dengan nol selama 
setahun. Jika dipertahankan, hal ini tentunya akan berdampak 
pada efisiensi penyimpanan data. 

2)  Barang Musiman 

Temuan  lainnya  adalah  barang  hanya  laku  terjual  di 
beberapa  bulan  saja  atau  terdapat  perbedaan  yang  signifikan 
jumlah barang terjual pada bulan tertentu dengan bulan lainnya. 

a)  Barang yang laku terjual hanya di bulan tertentu 
Beberapa  barang  yang  dijual  hanya  laku  di  bulan  tertentu 
saja.  Contohnya,  Barang  dengan  Item  ID  6103409941,  yaitu 
kalender  tahun  2021  yang  hanya  laku  terjual  pada  bulan 
November  dan  Desember  2020.  Hal  ini  dapat  terjadi  akibat 
event  tertentu,  ataupun  barang  tersebut  belum  dikenal  oleh 
pembeli. 

b)  Terdapat  Perbedaan  Penjualan  yang  Signifikan  di 

Beberapa Bulan 

Shop ID 
208558848 

Item ID 
4911154238 

TABEL I 
Contoh Barang Musiman 
Bulan 
Januari 
Februari 
Maret 
Mei 
Juni 
Juli 
Agustus 
September 
Oktober 
November 
Desember 

Terjual_b 
27 
27 
29 
2.090 
178 
94 
29 
13 
2 
4 
4 

Barang  dengan  Item  ID  tersebut  adalah  produk  olahan 
teripang  segar.  Musim  puncak  penangkapan  teripang  terjadi 
bulan Mei sampai Juli. Hal inilah yang menyebabkan produk 
tersebut mengalami puncak penjualan pada bulan Mei hingga 
Juli 2020.  

c)  Barang yang hanya ada beberapa bulan saja. 
Dalam satu tahun, terdapat barang yang hanya memiliki data 
beberapa bulan saja. Barang yang laku terjual di bulan tertentu 
tetap  memiliki  data  yang  disimpan  meskipun  nilai  barang 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

terjual bulanan adalah nol, sedangkan barang yang hanya ada 
beberapa  bulan  saja  muncul  di  sebagian  bulan.  Misalnya, 
barang A muncul hanya 2 bulan sepanjang tahun 2020. 

Perlu  diketahui  bahwa  tidak  semua  barang  memiliki  data 
lengkap di semua bulan. Hal ini dapat diakibatkan item tersebut 
hanya  dimunculkan  pada  bulan-bulan  tertentu  oleh  penjual, 
baru  dibuat,  dihapus,  atau  variabel  jumlah  barang  terjual 
kumulatif tidak konsisten. 

3)  Barang yang Dijual Tidak Konsisten 

Pada  prinsipnya  Satu  Item  ID  ditujukan  unik  untuk  setiap 
barang. Namun, pada kenyataannya satu Item ID dapat terdiri 
dari  beberapa  barang.  Beberapa  penjual  memilih  untuk 
mengubah  barang  daripada  membuat  Item  ID  baru.  Hal  ini 
mengakibatkan  ketidakkonsistenan  data  barang.  Berikut 
merupakan contoh ketidakunikan Item ID: 

Kategori 

Otomotif 

TABEL II 
Contoh Barang Tidak Konsisten dalam Satu Item ID 
Sub-Sub 
Kategori 
Masker 

Sub Kategori 

Nama 

Masker scuba  

Aksesoris 
Pengendara 
Motor 

Serba Serbi  Lain-lain 
Serba Serbi  Lain-lain 
Fashion 
Bayi 
Anak 

& 

Pakaian  Anak 
Perempuan 

Lain-lain 
Lain-lain 
Kaos 
Perempuan 

Anak 

Mesin press  
Masker scuba isi 12 pcs 
KAOS ANAK  

Dari  tabel  diatas  satu  Item  ID  dapat  diubah-ubah  kategori, 
sub-kategori,  sub-sub  kategori,  dan  namanya  sehingga 
menyebabkan Item ID tidak unik lagi. 

B.  Pendefinisian Toko Aktif berdasarkan Data Barang: 
Penentuan Parameter 

Pada  tahap  awal  pemrosesan,  dilakukan  filterisasi  data. 
Filterisasi data dilakukan untuk mendapatkan data barang dan 
toko yang berada di Provinsi DKI Jakarta saja. Setelah itu, data 
tersebut digunakan sebagai input pada proses pembersihan data, 
yaitu,  pengecekan  validitas  dan  konsistensi,  penghilangan 
outlier,  dan  imputasi  Bulan  Agustus  2020.  Proses  tersebut 
menggunakan algoritma yang dirancang oleh Bustaman et al. 
(2020). 

Gambar 4. Algoritma Keseluruhan Penentuan Toko Aktif 

 3 / 8 

 
 
 
 
 
Gambar diatas adalah sub-algoritma yang digunakan untuk 
mendefinisikan  toko  aktif  berdasarkan  data  barang.  Prinsip 
Pembangunan  algoritma  menggunakan  Divide  and  Conquer 
dimana  permasalahan  ‘toko  aktif’  dipecah-pecah  menjadi 
permasalahan 
sebelumnya. 
Kemudian,  Permasalahan  tersebut  dijadikan  parameter  untuk 
menentukan aktif atau tidaknya suatu toko. 

didefinisikan 

sudah 

yang 

Sebelum  proses  pembentukan  toko  aktif,  barang-barang 
yang  tidak  pernah  terjual  dalam  satu  tahun  dihilangkan. 
Barang-barang  yang  tidak  pernah  terjual  dalam  satu  tahun 
ditandai dengan rata-rata penjualan dalam satu tahun yang sama 
dengan  nol.  Proses  filterisasi  ini  perlu  dilakukan  untuk 
mengurangi  data-data  yang  akan  membebani  performa 
algoritma. 

Proses  Divide  and  Conquer  dilakukan  untuk  mendapatkan 
direktori  toko  aktif  sementara  berdasarkan  data  barang.  Pada 
Dasarnya, prinsip penyelesaian masalah ini menggunakan data 
cleaning,  dimana 
langkah  pertama  adalah  pendeteksian 
diskrepansi  atau  discrepancy  detection.  Diskrepansi  yang 
muncul  dapat  disebabkan  oleh  penjual,  karakteristik  barang, 
marketing produk, dsb. Setelah diskrepansi barang ditemukan 
dan  diselesaikan,  Data  Flags  dari  setiap  parameter,  Item  ID, 
dan Shop ID digunakan untuk menentukan aktif atau tidaknya 
sebuah  toko  dengan  memodifikasi  algoritma  Boyer-Moore 
Majority Vote. 

1)  Barang yang Tidak Pernah Terjual selama Setahun 

Barang  yang  tidak  pernah  terjual  selama  setahun  diwakili 
dengan  rata-rata  penjualan    bulanan  dalam  satu  tahun  yang 
sama  dengan  nol.  Proses  ini  dilakukan  dengan  menyaring 
barang dengan rata-rata penjualan tahunan lebih dari nol. 

Berikut  merupakan  kondisi  sebelum  dan  sesudah  proses 

filterisasi 

TABEL III 
Kondisi Sebelum dan Sesudah Proses Filterisasi 
Kondisi 

Perubahan (%) 

Jumlah Item ID 
Jumlah Baris 

Sebelum 
6.957.459 
24.580.029 

Sesudah 
4.661.132 
16.504.949 

-33,01 
-32,85 

2)  Parameter 1: Barang Musiman 

Penentuan  barang  musiman  dilakukan  dengan  menghitung 
rasio jumlah barang terjual perbulan dengan rata-rata penjualan 
barang dalam setahun. Setelah itu, setiap barang akan dihitung 
varians dari rasio tersebut untuk digunakan sebagai parameter 
pertama, yaitu variance cut off. 

1  Rasio 

Rasio  dihitung  berdasarkan  perbandingan  jumlah  barang 
terjual setiap bulan dengan rata-rata penjualan dalam satu tahun. 
Setiap Item ID akan memiliki rasio tiap bulannya. Jika terjadi 
perbedaan  penjualan  yang  signifikan  di  bulan  tertentu  maka 
rasio pada bulan tersebut akan berbeda signifikan pula dengan 
bulan lainnya. 
Berikut merupakan rumusan rasio yang digunakan: 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Varians  rasio  digunakan  untuk  melihat  seberapa  tinggi 
keragaman  yang  terjadi  antar  Rasio tiap  bulannya  dalam  satu 
Item  ID.  Hal  ini  bertujuan  untuk  melihat  jika  ada  perbedaan 
yang  signifikan  antara  penjualan  di  bulan  tertentu  dengan 
penjualan  keseluruhan.  Jika  terdapat  penjualan  bulanan  yang 
terlalu tinggi/rendah diantara bulan lainnya maka varians akan 
tinggi. 

3  Penentuan Cut-off Variance 

Penentuan  cut-off  dari  varians  menggunakan  persentase 
kumulatif data. Jika batasan varians yang ditentukan kecil maka 
akan  banyak  data  yang  hilang.  Oleh  karena  itu,  dipakai  nilai 
cut-off persentil 95 Kemudian, didapatkan nilai Variance Cut-
off  sebesar 3.25. 

3)  Parameter 2: Minimum jumlah Bulan setiap Item dalam 

Satu Tahun 

Untuk menghindari kehilangan data penting penentuan cut-

off dari jumlah bulan memperhatikan logika sebagai berikut 
- 

Jika jumlah bulan 4 dan terdapat A jumlah barang terjual 
di  satu  bulan,  yaitu  bulan  ke-n  maka  rasio  bulan  ke-n 
adalah 4 dan varians adalah 3 (varians memenuhi  cut-off 
tetapi barang tidak layak masuk) 
Jika jumlah bulan 5 dan terdapat A jumlah barang terjual 
di  satu  bulan,  yaitu  bulan  ke-n  maka  rasio  bulan  ke-n 
adalah  5  dan  varians  adalah  4  (varians  tidak  memenuhi 
Cut-off dan barang tidak layak masuk) 

- 

Berdasarkan  logika  diatas  maka  didapatkan  nilai  cut-off 
jumlah  bulan  yaitu  lebih  dari  sama  dengan  5.  Tentunya  nilai 
cut-off  jumlah  bulan  akan  berbeda  jika  nilai  persentil  95  dari 
varians berbeda. 

4)  Parameter 3: Item yang tidak unik 

Metode  yang  digunakan  adalah  dengan  cara  menghitung 
jumlah unik (count distinct) dari setiap kategori, sub-kategori, 
sub-sub-kategori, dan nama barang dari setiap item. Nilai Cut-
off  digunakan  untuk  menentukan  minimum  jumlah  hitung 
(count) dari setiap kategori sampai nama barang. 

Secara umum terdapat 2 kasus yang menyebabkan Item ID 

tidak unik, yaitu 
1.  Kategori/sub-kategori/sub-sub  kategori/  nama  barang 

berbeda, tetapi barang yang dijual sama. 

2.  Kategori/sub-kategori/sub-sub 

kategori/nama 

barang 

berbeda dan barang yang dijual berbeda. 

Hanya  segelintir  data  barang  saja  yang  mempunyai  kasus 
nomor  2.  Oleh  karena  itu,  penelitian  ini  memutuskan  untuk 
menggunakan  persentil  99  sebagai  nilai  cut-off.  Hal  ini 
bertujuan untuk menghindari kehilangan banyak data. Berikut 
adalah nilai Cut-off jumlah perbedaan kategori yang diberikan 
penjual dalam satu Item ID: 

TABEL IV 
Nilai cut off hingga Kategori Terkecil 
Sub-Kategori 
3 

3 

Kategori 
2 

Sub-Sub-Kategori  Nama Barang 

4 

𝑅𝑎𝑠𝑖𝑜 = 𝑇𝑒𝑟𝑗𝑢𝑎𝑙_𝑏𝑖

𝑥̅𝑡

2  Varians dari Rasio 

(1) 

Terdapat  penyesuaian  nilai  persentil  99  pada  bagian  sub-
kategori dan sub-sub-kategori dari nilai 2 digeser ke nilai 3, hal 
ini disebabkan oleh masih banyak barang dengan kasus 1 jika 
sub-kategori dan sub-sub kategori menggunakan nilai 2. 

 4 / 8 

 
 
 
 
   
 
 
Item  yang  melewati  nilai  Cut-off  selanjutnya  ditandai  dan 
digunakan  untuk  penentuan  toko  aktif.  Namun,  setelah 
penentuan toko aktif dilakukan barang dengan Item ID tersebut 
akan dihilangkan. 

Setelah  digunakan  untuk  menentukan  toko  aktif,  barang 
yang  tidak  memenuhi  kondisi  parameter  di  toko  aktif  yang 
terpilih akan dihilangkan. 

Berikut  merupakan  perubahan  jumlah  barang  dari  proses 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

C. Pendefinisian Toko Aktif berdasarkan Data Barang: 
Penentuan Toko Aktif berdasarkan Parameter 

Pembangunan  algoritma  penentuan  toko  aktif  berdasarkan 
parameter  dengan  cara  memodifikasi  algoritma  Boyer-Moore 
Majority  Vote.  Hal  ini  beralasan  jika  hanya  dengan  prinsip 
voting  akan  menghilangkan  informasi  penting  pada  item 
lainnya yang dapat melewati nilai cut-off. 

Berikut  merupakan  algoritma  yang  dibangun  untuk 
menentukan toko aktif berdasarkan parameter yang ditentukan: 

Gambar 5. Algoritma Penentuan Toko Aktif Berdasarkan Nilai Parameter 

Berikut  merupakan  logika  dari  algoritma  penentuan  toko 

aktif yang dibangun: 
1.  Toko  aktif  akan  ditentukan  berdasarkan  karakteristik 

barang yang dijual 

2.  Parameter 1 dan Parameter 2 akan digabung di tingkat data 
barang.  Kondisi  gabungan  parameter  akan  terpenuhi  jika 
kedua parameter terpenuhi 

3.  Jika  dalam  satu  toko  tidak  ada  satupun  item  yang 
memenuhi kondisi gabungan parameter 1 dan 2 maka toko 
tersebut akan dihilangkan 

4.  Parameter gabungan 1 dan 2 dan parameter 3 akan dihitung 

score di tingkat toko 

5.  Untuk parameter 3 akan memakai prinsip algoritma Boyer-
Moore Majority Vote. Dimana parameter 3 akan terpenuhi 
di tingkat toko jika parameter 3 dengan label ‘terpenuhi’ 
menang oleh voting barang-barang di toko tersebut. 
6.  Jika Gabungan Parameter 1 dan 2 terpenuhi dan Parameter 
3  terpenuhi  maka  toko  tersebut  dikatakan  toko  aktif 
sementara. 
Berikut  merupakan  perubahan  jumlah  toko  yang  terjadi 
pada pembentukan direktori toko aktif berdasarkan data barang: 

kondisi 
awal 
gabungan parameter 1 dan 2 
parameter 3 

TABEL V 
Perubahan Jumlah Toko 
jumlah toko 
269.358 
113.498 
113.488 

% perubahan 

-57,864 
-0,009 

pembentukan direktori toko aktif keseluruhan: 

TABEL VI 
Perubahan Jumlah Item ID dan Jumlah Baris 
Kondisi 

% Perubahan 

Sebelum 
4.661.132 
16.504.949 

Sesudah 
1.255.947 
8.542.654 

Jumlah Item ID 
Jumlah baris 
Pada tabel diatas dapat dilihat bahwa persentase perubahan 
jumlah baris jauh lebih kecil dibandingkan dengan perubahan 
jumlah  Item  ID.  Hal  ini  berarti  bahwa  barang-barang  yang 
jarang muncul dalam satu tahun berhasil dihilangkan. 

-73,05 
-48,24 

D. Pendefinisian Permasalahan di Tingkat Data Toko 

Pengecekan konsistensi omzet dilakukan guna untuk melihat 
anomali  data  yang  masih  tersisa,  anomali  dapat  disebabkan 
oleh  variabel  harga  barang  yang  berbeda  drastis  walaupun 
mempunyai Item ID yang sama. 

 Metode yang digunakan untuk mengecek konsistensi omzet 
dalam  satu  tahun  menggunakan  rasio  antara  omzet  bulan  i 
dengan  rata  rata  omzet  dalam  satu  tahun.  Berikut  merupakan 
formula rasio yang digunakan: 

𝑅𝑎𝑠𝑖𝑜 = 𝑂𝑚𝑠𝑒𝑡𝑖

𝑥̅𝑡

(2) 

Setelah  dihitung  rasio,  dilakukan  penghitungan  terhadap 
varians rasio dari setiap Shop ID. Kemudian, varians diurutkan 
dari yang terbesar hingga terkecil. Varians terbesar dari omzet 
bulanan  adalah  8,9.  berikut  merupakan  distribusi  omzet 
bulanan  toko  dengan  varians  omzet  antar  bulannya  terbesar, 
yaitu 8,9: 

TABEL VII 
Distribusi Omzet Per Bulan Toko dengan Varians Omzet antar Bulan 
Terbesar 

Bulan 
Februari 
Maret 
April 
Mei 
Juni 
Juli 
Agustus 
September 
Oktober 
November 
Desember 

Omzet 

155.000 
355.000 
290.000 
1.025.000 
892.200 
1.214.250 
940.200 
459.250 
1.239.500 
15.905.750 
407.623.450 

Dari tabel tersebut terlihat terjadi ledakan omzet pada bulan 
November. Lalu di bulan Desember, omzet mengalami anomali 
yang  cukup  parah.  Setelah  ditelusuri,  permasalahan  terdapat 
pada variasi harga yang tinggi antar bulan di Item ID yang sama. 
Berikut  merupakan  data  dari  Item  ID  3726806370  yang 
merupakan salah satu penyebab omzet melambung tinggi : 

TABEL VIII 
Contoh Harga yang Mengakibatkan Omzet Jauh Berbeda 

 5 / 8 

 
 
 
 
 
 
 
 
 
 
     
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Bulan 
April 
Mei 
Juni 
Juli 
Agustus 
September 
Oktober 
Desember 

Harga 

500 
500 
500 
500 
500 
500 
500 
150.000 

Terjual_b 
0 
0 
182 
324 
430 
267 
654 
2.712 

Omzet 

0 
0 
91.000 
162.000 
215.000 
133.500 
327.000 
406.800.000 

%𝑗𝑎𝑛𝑔𝑘𝑎𝑢𝑎𝑛 ℎ𝑎𝑟𝑔𝑎 = 𝑚𝑎𝑥(ℎ𝑎𝑟𝑔𝑎)−𝑚𝑖𝑛(ℎ𝑎𝑟𝑔𝑎)

𝑚𝑎𝑥 (ℎ𝑎𝑟𝑔𝑎)

× 100     (3) 

Dari hasil perhitungan yang didapatkan, persentase jangkauan 
harga  diurutkan  dari  yang  terbesar  hingga  yang  terkecil. 
Kemudian, Item ID yang memiliki persentase perubahan harga 
yang besar diteliti lebih lanjut untuk mengidentifikasi penyebab, 
berikut merupakan hasil dari identifikasi masalah: 

Barang tersebut adalah produk sunscreen. Setelah diselidiki 
harga pasaran produk tersebut memang berada pada sekitar 150 
ribu.  Namun  harga-harga  di  bulan  sebelumnya  sangat  jauh 
berbeda dan tidak konsisten. 

Setelah dirangkum, terdapat beberapa kemungkinan hal ini 

dapat terjadi: 

1  Barang yang dijual tidak sama 

TABEL IX 
Contoh Barang yang Dijual Tidak Sama dalam Satu Item ID 

Sub-Sub-
Kategori 
Mouse 
Mouse 
Mouse 
Mouse 
Laptop Konsumer 

Bulan 

Harga 

Terjual_b  Omzet 

Agustus 
September 
Oktober 
November 
Desember 

165.000 
165.000 
165.000 
165.000 
13.150.000 

1 
0 
2 
0 
0 

165.000 
0 
330.000 
0 
0 

Pada  tabel  diatas  dapat  telihat  bahwa  Item  ID  yang  sama 
dapat menjual produk yang berbeda. Sebenernya, kasus ekstrim 
dari penjualan barang yang berbeda sudah dihilangkan dengan 
metode pembentukan toko aktif di tingkat data barang. Namun, 
tidak  dapat  menyaring  kasus  seperti  diatas.  Jika  ditinjau  dari 
segi omzet, Item ID tersebut tidak akan menimbulkan anomali 
di  tingkat  toko  karena  omzet  masih  terlihat  wajar.  Hal  ini 
disebabkan oleh barang yang berbeda, yaitu laptop tidak terjual 
secara  masif 
sunscreen  diatas  yang 
menyebabkan anomali di tingkat data toko. 

seperti  produk 

2  Kemungkinan lainnya 

Setelah di cek sampai ke nama barang, barang yang dijual di 
Item  ID  3726806370  adalah  sama.  Oleh  sebab  itu,  dapat 
disimpulkan  bahwa 
lainnya  yang 
menyebabkan  harga  barang  dapat  berbeda  signifikan, 
diantaranya adalah metode scraping atau API dari marketplace 
yang bersangkutan. Tentunya, hal ini perlu didiskusikan lebih 
lanjut kepada pemangku kepentingan yang terkait. 

terdapat  kemungkinan 

E. Shop Processing: Deteksi Anomali dan Konsistensi 
Variabel Harga dan Omzet di Tingkat Data Toko 

1) Metode Pendeteksian Anomali pada Variabel Harga 
Perlu untuk mengecek anomali pada variabel harga di setiap 
Item  ID  yang  sama.  Hal  ini  berguna  untuk  mengantisipasi 
perbedaan  besar  pada  omzet  yang  dihasilkan  setiap  bulannya 
akibat  variabel  harga.  Pengecekan  anomali  harga  dilakukan 
dengan  membagi  jangkauan  dengan  nilai  maksimum  harga 
pada  Item  ID  yang  sama.  Berikut  merupakan  formula  yang 
digunakan: 

Gambar 6. Ishikawa Diagram Omzet Bulanan Tinggi 

Jika  masalah  harga  disebabkan  oleh  proses  scraping  atau 
API  yang  disediakan  maka  omzet  akan  tidak  representatif. 
Sebaliknya,  jika  masalah  harga  disebabkan  oleh  barang  yang 
dijual berbeda maka omzet tetap representative. Namun, perlu 
dilakukan  pengkodean  kembali  terhadap  Item  ID setiap  toko. 
Masalah  lainnya  adalah  jika  perbedaan  signifikan  harga 
disebabkan oleh flash sale/diskon, tetapi dalam kasus ini omzet 
tetap representatif. 

Solusi  yang  dapat  dilakukan  untuk  mengatasi  konsistensi 

harga adalah sebagai berikut: 
1.  Menghapus  barang  yang  menyebabkan  omzet  di  bulan 
tertentu  melambung  tinggi,  dengan  catatan  tidak  banyak 
barang yang terlibat untuk menghindari kehilangan banyak 
data. 

2.  Mengganti  harga  terendah  yang  mempunyai  persentase 

jangkauan harga yang tinggi. 

jangkauan 

1)  Menghapus barang yang menyebabkan omzet tinggi 
Item yang menyebabkan omzet tinggi diidentifikasi dengan 
persentase 
harga.  Dengan  memperhatikan 
karakteristik harga dan kemungkinan perubahan harga karena 
efek  barang  yang  dijual  berbeda,  ditentukan  nilai  cut-off  dari 
persentase  jangkauan  harga  sebesar  95%.  Berikut  merupakan 
ringkasan data setelah barang yang menyebabkan omzet tinggi 
dihapus: 

TABEL X 
Kondisi Sebelum dan Sesudah Penghapusan Barang yang Menyebabkan 
Omzet Bulanan Tinggi 

Kondisi 

 6 / 8 

 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Sebelum 

Sesudah 

Item ID 
Shop ID 

1.255.557 
113.458 

1.253.774 
113.438 

Perubahan 
(%) 
-0,14 
-0,02 

toko  yang  sudah  diproses  kemudian  diberikan  atribut-atribut 
yang  dibutuhkan  untuk  pembentukan  direktori.  Berikut 
merupakan sebagian source system metadata yang digunakan, 
yaitu pada tabel direktori toko aktif: 

Dapat dilihat dari tabel diatas bahwa tidak ada perubahan yang 
berarti  pada  jumlah  Shop  ID.  Sehingga,  metode  ini  dapat 
dipertimbangkan  untuk  menghindari  omzet  yang  tinggi  di 
bulan  tertentu.  Setelah  dilakukan  penghapusan,  varians  dari 
omzet bulanan tertinggi pun menurun dari 8,90 ke 6,74. 

2)  Mengganti anomali harga 
Solusi  ini  hanya  bisa  diaplikasikan  jika  terdapat  proses 
pengelompokan  terhadap  anomali  harga  terlebih  dahulu. 
Pengelompokkan  anomali  harga  perlu  dilakukan  untuk 
jika 
menentukan  perlakuan  dari  setiap  data.  Tentunya 
perbedaan  harga  disebabkan  oleh  barang  yang  dijual  berbeda 
dalam  satu  item  ID  yang  sama  atau  karena  flash  sale/diskon 
maka penggantian anomali harga tidak dapat dilakukan karena 
hal 
tidak 
tersebut  akan  menyebabkan  omzet  penjual 
representatif.  Oleh  karena  itu,  penggantian  anomali  harga 
memerlukan  perhatian  khusus  dan  penelusuran  lebih  lanjut 
dengan  melakukan  konfirmasi  terhadap  setiap  pemangku 
kepentingan  baik  dari  segi  subject  matter  ataupun  pihak 
marketplace. 

Tentunya  proses  penggantian  anomali  harga  akan 
memerlukan  usaha  yang  ekstra.  Untuk  daerah  DKI  Jakarta, 
jumlah item ID yang mengalami anomali harga hanya sekitar 
0,14%.  Penting  atau  tidaknya  dampak  yang  ditimbulkan  oleh 
anomali harga tergantung kepada tujuan analisis. Jika anomali 
tersebut  akan  menimbulkan  masalah  pada  proses analisis  dan 
mengakibatkan berubahnya kesimpulan dari sebuah penelitian 
maka anomali tersebut perlu diperhatikan lebih lanjut. Secara 
teori,  kondisi  anomali  harga  0,14%  dari  jumlah  keseluruhan 
item  ID  terbilang  sangat  kecil.  Namun,  jika  0,14%  item  ID 
tersebut  akan  menimbulkan  perbedaan  penarikan  kesimpulan 
maka anomali tersebut perlu diperlakukan khusus. 

Pada  pembentukan  direktori  toko  aktif  terdapat  atribut 
omzet  bulanan.  Omzet  bulanan  yang  dihitung  dengan 
mengalikan  jumlah  barang  terjual  dengan  harga  akan  tidak 
representatif jika harga yang digunakan tidak representatif pula. 
Oleh  karena  itu,  fokus  utama  adalah  anomali  harga  yang 
menimbulkan anomali omzet sehingga menyebabkan omzet di 
bulan 
tertentu  akan  berbeda  secara  signifikan.  Perlu 
diperhatikan bahwa penyesuaian terhadap variabel harga yang 
menyebabkan  anomali  akan  menimbulkan  anomali  lainnya 
atau  tidak.  Jika  langkah  tersebut  akan  menimbulkan  anomali 
lainnya yang akan berpengaruh terhadap penarikan kesimpulan 
maka upaya tersebut tidak bisa dilakukan. Tentunya, upaya ini 
memerlukan  penelitian 
lanjut  yang  menguraikan 
mekanisme  rinci  tentang  prosedur  beserta  dampak  yang 
ditimbulkan.  Oleh  karena  hal  tersebut  dan  jumlah  anomali 
harga terbilang relatif kecil, upaya penghapusan menjadi opsi 
yang baik untuk mengatasi anomali omzet pada penelitian ini. 

lebih 

F. Pembentukan Direktori Toko Aktif 

Penelitian  ini  menghasilkan  hasil  2  tabel,  yaitu  tabel  toko 
aktif dan tabel barang dengan foreign key adalah shop ID yang 
digunakan  untuk  menghubungkan  kedua  tabel  tersebut.  Data 

TABEL XI 
Metadata Direktori Toko Aktif 

Atribut 
shop_id 
num_item 

Tipe 
String 
Integer 

annual_turnover 
umkm 

Float 
String 

username 
name 
tipe_toko 
provinsi 
kabupaten 
kecamatan 

String 
String 
String 
String 
String 
String 

Deskripsi 
ID unik dari setiap toko aktif 
jumlah barang pada toko aktif yang lolos 
dari preprocessing 
omzet tahunan toko aktif 
klasifikasi umkm. Mikro <= 300 juta, kecil 
=> 300 juta dan <= 2,5 milyar, menengah 
=> 2,5 milyar dan <= 50 milyar, besar > 50 
milyar 
username yang digunakan untuk mendaftar 
nama took 
tipe toko: regular, star seller 
provinsi toko aktif berada 
kabupaten toko aktif berada 
kecamatan toko aktif berada 

Perlu digaris bawahi bahwa pada direktori yang dihasilkan 
tidak terdapat atribut alamat secara detail, tetapi hanya berupa 
ini 
nama  provinsi,  kabupaten,  hingga  kecamatan.  Hal 
dikarenakan  bahwa  marketplace  yang  dipakai 
tidak 
memberikan  informasi  alamat  toko  tersebut  di  dalam  public 
API  yang  disediakan.  Hal  ini  dapat  dikatakan  wajar  karena 
alamat  merupakan  bagian  dari  Personal 
Identifiable 
Information  (PII),  dimana  terdapat  kewajiban  dari  setiap 
perusahaan  untuk  melindungi  informasi  tersebut.  tetapi  tidak 
menutup  kemungkinan  bahwa  marketplace  lain  menyediakan 
informasi  alamat  secara  lebih  rinci.  Kedepannya,  jika  akan 
to  machine  dengan  pihak 
dilakukan  proses  machine 
marketplace  alamat  menjadi  atribut  yang  penting  untuk 
dipertimbangkan. 

G. Analisis Deskriptif 

Gambar 7. Distribusi Jumlah Toko Aktif Berdasarkan Klasifikasi UMKM dan 
Kabupaten/Kota di Provinsi DKI Jakarta 

Dapat  dilihat  pada  gambar  diatas  bahwa  mayoritas  toko 
aktif  tergolong  kedalam  usaha  mikro  yaitu  sebesar  92.1%, 
kemudian sisanya berturut-turut dari persentase terbesar adalah 
usaha  kecil,  menengah,  dan  besar.  Hal  ini  disebabkan  oleh 
marketplace  yang  digunakan  dalam  penelitian  ini  adalah 
marketplace  berjenis  customer-to-customer  (C-to-C),  dimana 

 7 / 8 

 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

konsumen  individu  dapat  menjual  atau  membeli  produk  dari 
konsumen lainnya. Sehingga, hal tersebut akan menyebabkan 
banyak usaha yang berjenis mikro dan kecil. 

G. Performa Algoritma 

Algoritma dianggap efisien jika konsumsi sumber dayanya 
(computational  cost)  berada  pada  atau  di  bawah  beberapa 
tingkat  yang  dapat  diterima.  Secara  umum,  ‘dapat  diterima’ 
berarti  bahwa  algoritma  tersebut  akan  berjalan  dalam  jumlah 
waktu  atau  ruang  yang  wajar  pada  komputer  yang  tersedia 
biasanya fungsi dari ukuran input. 

Ada  banyak  cara  untuk  mengukur  sumber  daya  yang 
dikonsumsi  oleh  algoritma.  Dua  ukuran  yang  paling  umum 
adalah  kecepatan  dan  penggunaan  memori.  Beberapa  pilihan 
juga  bergantung  kepada  ukuran  input  ke  algoritma,  yaitu 
jumlah data yang akan diproses. 

Dalam penelitian ini Notasi Big O Donald Knuth digunakan 
untuk  merepresentasikan  kompleksitas  algoritma  sebagai 
fungsi  dari  ukuran  input  n.  dilakukan  eksperimen  dengan 
jumlah  data  untuk  mengetahui  kompleksitas 
menambah 
algoritma yang diwakili oleh Notasi Asimtotik big O. 

Berikut  merupakan  grafik  dari  jumlah  data  dibandingkan 

dengan runtime dalam detik dari algoritma: 

pembentukan  toko  aktif  berdasarkan  data  barang,  dan 
pemrosesan data toko. 

2.  Direktori toko aktif telah dihasilkan dari 10 atribut. 
3.  Kompleksitas  algoritma  yang  dibangun  terklasifikasi  ke 
dalam fungsi logaritma, dimana dapat dikatakan algoritma 
yang dibangun sangat efisien. 

Adapun saran dari penelitian ini adalah sebagai berikut: 
1.  Diperlukan  penelitian  lebih  lanjut  tentang  metode  dalam 
menangani anomali harga selain dengan cara penghapusan. 
2.  Diperlukan  pengembangan  metode  pada  platform 

marketplace lain. 

3.  Diperlukan  penelitian  lebih  lanjut  untuk  mendapatkan 
atribut alamat lengkap, seperti peninjauan sumber data dari 
marketplace lain, machine to machine, atau menggunakan 
Teknik Matching dengan sumber data lain. 

DAFTAR PUSTAKA 
[1]  A. Sapountzi and K. E. Psannis, Big Data Preprocessing: An Application 

on Online Social Networks, no. July. 2020. 

[2]  H. J. Hadi, A. H. Shnain, S. Hadishaheed, and A. H. Ahmad, “Big Data 
and Five V ’ S Characteristics Big Data and Five V ’ S Characteristics,” 
Int. J. Adv. Electron. Comput. Sci., vol. 2, no. January 2015, pp. 16–23, 
2019. 

[3]  U. Bustaman, D. N. Larasati, Z. H. S. Putri, S. Mariyah, Takdir, and S. 
Pramana, “Building Effective and Efficient Procedure for  Preprocessing 
Marketplace  Data,”  ICITEE  2020  -  Proc.  12th  Int.  Conf.  Inf.  Technol. 
Electr. 
doi: 
10.1109/ICITEE49829.2020.9271717. 

186–191, 

2020, 

Eng., 

pp. 

[4]  P. Struijs, B. Braaksma, and P. J. H. Daas, “Official statistics and Big Data,” 
doi: 

2014, 

1–6, 

vol. 

no. 

pp. 

1, 

1, 

Big  Data  Soc., 
10.1177/2053951714538417. 

Gambar 8. Fungsi Jumlah Input Dibandingkan dengan Runtime dari 
Algoritma 

Dapat dilihat dari grafik diatas bahwa seiring bertambahnya 
jumlah data maka pertambahan jumlah runtime akan cenderung 
lebih  sedikit.  Maka  dapat  disimpulkan  bahwa  kompleksitas 
algoritma  terklasifikasi  sebagai  fungsi  logaritma.  Berikut 
merupakan Notasi Big O: 

𝑂(𝑙𝑜𝑔 𝑛) 

Dapat dikatakan bahwa tingkat kompleksitas dari algoritma 
akan  berbanding  lurus  dengan  log  dari  banyaknya  data.  Oleh 
karena  itu,  dapat  dikatakan  bahwa  algoritma  yang  dibangun 
sangat efisien. 

VII. 

PENUTUP 

Berdasarkan pemaparan yang sudah dijelaskan sebelumnya, 

berikut merupakan kesimpulan akhir dari penelitian ini: 
1.  Algoritma  yang  dibangun  telah  mampu  menghasilkan 
direktori  toko  aktif  dari  platform  marketplace  dengan  3 
tahapan  pemrosesan  yaitu  pemrosesan  data  barang, 

[5]  A. C. Eberendu, “Unstructured Data: an overview of the data of Big Data,” 
Int.  J.  Comput.  Trends  Technol.,  vol.  38,  no.  1,  pp.  46–50,  2016,  doi: 
10.14445/22312803/ijctt-v38p109. 

[6]  S. H. Saleh, R. Ismail, Z. Ibrahim, and N. Hussin, “Issues, Challenges and 
Solutions of Big Data in Information Management: An Overview,” Int. J. 
Acad. Res. Bus. Soc. Sci., vol. 8, no. 12, 2019, doi: 10.6007/ijarbss/v8-
i12/5240. 

[7]  A. P. Reimer and E. A. Madigan, “Veracity in big data: How good is good 
enough,” Health Informatics J., vol. 25, no. 4, pp. 1290–1298, 2019, doi: 
10.1177/1460458217744369. 

[8]  V.  L.  Rubin,  “Veracity  roadmap:  Is  big  data  objective,  truthful  and 
credible?,” Adv. Classif. Res. Online, vol. 24, no. July, pp. 4–15, 2014, doi: 
10.7152/acro.v24i1.14671. 

[9]  S. García, S. Ramírez-Gallego, J. Luengo, J. M. Benítez, and F. Herrera, 
“Big data preprocessing: methods and prospects,” Big Data Anal., vol. 1, 
no. 1, pp. 1–22, 2016, doi: 10.1186/s41044-016-0014-0. 

[10] Sfenrianto, I. Oliver, A. Christiano, and M. P. Mulani, “Impact of E-service 
on  customer  loyalty  in  marketplace  in  Indonesia,”  J.  Theor.  Appl.  Inf. 
Technol., vol. 96, no. 20, pp. 6795–6805, 2018. 

[11] Data 

Reportal, 

2020. 
https://datareportal.com/reports/digital-2020-indonesia (accessed Mar. 06, 
2021). 

INDONESIA,” 

“DIGITAL 

2020: 

 8 / 8 

 
 
 
 
 
 
 
"
221710052,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pembangunan Package R untuk Small Area 
Estimation Pendekatan Nonparametrik Berbasis 
Kernel Nadaraya-Watson 

Wicak Surya Hasani (221710052, 4SD1) 
Dosen Pembimbing: Dr. Azka Ubaidillah, SST., M.Si 

Ringkasan—  Keterbatasan  sampel  pada  kegiatan  survei 
menjadi  kendala  untuk  menyediakan  data  pada  domain  dan 
wilayah  yang  lebih  kecil.  Small  Area  Estimation  (SAE)  dapat 
dilakukan  untuk  mengatasi  permasalahan  ini.  Tetapi  teknik 
estimasi  tak  langsung  ini  memerlukan  asumsi  hubungan  linier 
antara  rataan  area  kecil  dengan  variabel  penyerta.  Masalah 
tersebut  dapat  diatasi dengan  pendekatan nonparametrik, salah 
satu  pendekatan  nonparametrik  yang  dapat  digunakan  yaitu 
dengan  menggunakan  basis  Kernel  Nadaraya-Watson.  Untuk 
memudahkan  dalam  pengimplementasian,  peneliti  membangun 
suatu  Package  R  untuk    Small  Area  Estimation  pendekatan 
nonparametrik berbasis Kernel Nadaraya-Watson dengan nama 
package  “saekernel”.  Dimana  hasilnya  menunjukkan  bahwa 
package “saekernel” yang telah dibangun sudah sesuai dan layak 
untuk digunakan. Package yang telah dibangun juga diterapkan 
pada  survei  BPS,  yaitu  untuk  menduga  pengeluaran  perkapita 
pada tingkat kecamatan di Provinsi D.I.Yogyakarta berdasarkan 
data Survei Sosial Ekonomi Nasional (SUSENAS) Maret 2019.  

Kata  Kunci—  Small  Area  Estimation  (SAE),  Nonparametrik, 

Kernel Nadaraya-Watson, Package R, saekernel. 

I.  LATAR BELAKANG 

Informasi  yang  lebih  detail  mengenai  keadaan  di  suatu 
daerah  menjadi  sangat  penting  dalam  acuan  untuk  menyusun 
kebijakan  pembangunan  oleh  pemerintah,  apalagi  dengan 
berkembangnya  era  otonomi  daerah  di  Indonesia  saat  ini.  
Setiap  daerah  memiliki  karakteristik  pembanguan  yang 
berbeda 
informasi  mengenai 
karakteristik pembangunan menjadi kunci utama keberhasilan 
rumusan  kebijakan  pembangunan  yang  direncanakan.  Oleh 
karena  itu,  statistik  yang  lengkap  hingga  level  terkecil  akan 
sangat dibutuhkan dan berguna sebagai dasar penyusunan serta 
pengambilan kebijakan yang tepat. 

ketersediaan 

sehingga 

Data 

terkait 

indikator-indikator  pembangunan  daerah 
bersumber  dari  Badan Pusat  Statistik  (BPS). BPS  melakukan 
pengumpulan data menggunakan cara sensus dan survei. Survei 
menjadi kegiatan yang sering dilakukan oleh BPS dikarenakan 
memiliki banyak kelebihan, tetapi survei yang dihasilkan BPS 
hanya bisa memberikan estimasi yang baik ditingkat area besar 
sehingga belum mampu memberikan estimasi yang baik untuk 
area kecil. 

Pendugaan  yang  didasarkan  pada  penerapan  model  desain 
penarikan  contoh  (design-based)  disebut  sebagai  pendugaan 
langsung 
tidak 
mampu memberikan ketelitian yang cukup bila ukuran sampel 
pada area kecil yang menjadi perhatian sedikit/ berukuran kecil, 

(direct-estimation).  Pendugaan 

langsung 

sehingga  statistik  yang dihasilkan akan  memiliki  varian  yang 
besar  atau  bahkan  pendugaan  tidak  dapat  dilakukan  karena 
tidak terwakili dalam survei [1]. 

tak 

sebagai  alternatif 

Teknik  pendugaan 

langsung  (indirect  estimation) 
digunakan 
teknik  pendugaan  untuk 
meningkatkan efektivitas ukuran sampel dan menurunkan error 
sehingga  dapat  menghasilkan  pendugaan  pada  area  kecil 
dengan ketelitian yang cukup. Teknik pendugaan tak langsung 
ini dilakukan melalui suatu model yang menghubungkan area 
terkait dengan menggunakan informasi tambahan atau variabel 
penyerta  yang  nantinya  ini  akan  menjadi  konsep  small  area 
estimation.  Small  Area  Estimation  (SAE)  merupakan  suatu 
metode  alternatif  yang  bisa  digunakan  untuk  mengestimasi 
parameter suatu area ketika ukuran sampel pada area tersebut 
cukup  kecil  untuk  mendapatkan  tingkat  ketelitian  yang 
memadai ketika dilakukan estimasi langsung. Model SAE pada 
dasarnya  meminjam  informasi  dari  sampel  observasi  melalui 
data  variabel penyerta untuk  meningkatkan efektifitas ukuran 
sampel [2].  
Pada 

dimana 
mengasumsikan  adanya  hubungan  linier  antara  rataan  area 
kecil  dengan  variabel  penyerta  yang  digunakan  sebagai 
informasi tambahan dalam pendugaan tersebut. Jika hubungan 
antara rataan area kecil dan variabel penyerta tidak linier maka 
kurang tepat untuk meminjam kekuatan dari area lain dengan 
menggunakan  model  linier  dalam  pendugaan  tak  langsung. 
Berbagai  teknik  pendugaan  area  kecil  yang  sering  digunakan 
seperti  Bayes  Empiris,  Hirarical  Bayes,  Pendekatan  Sintetik, 
Penduga  Komposit,  dan  EBLUP,  semuanya  menggunakan 
prosedur  parametrik  [2].  Metode  pendugaan  area  kecil  dapat 
dilakukan dengan cara memodelkan penduga langsung dengan 
peubah  penjelas.  Berdasarkan  peubah  penjelas  yang  tersedia, 
terdapat dua model area kecil, yaitu basic area level model dan 
basic unit level model. Kedua model tersebut mengasumsikan 
bahwa  penduga  langsung  memiliki  hubungan  yang  linier 
dengan peubah penjelas [3].  

pendugaan 

langsung 

teknik 

tak 

Pendekatan nonparametrik dapat dilakukan untuk mengatasi 
masalah  tersebut.  Salah  satu  pendekatan  nonparametrik  yang 
dapat  digunakan  yaitu  dengan  menggunakan  basis  kernel 
sebagai  solusi  pendugaan  area  kecil.  Karena  pendekatan 
nonparametrik  berbasis  kernel  memberikan  prosedur  yang 
fleksibel dalam pendugaan area kecil. Adapun salah satu basis 
kernel  yang  sering  digunakan  yaitu  Kernel  Nadaraya-Watson 
sehingga  basis  tersebut  juga  yang  akan  digunakan  dalam 

 1 / 8 

 
 
 
 
 
penelitian  ini  sebagai  basis  pada  pendekatan  nonparametrik 
dalam small area estimation. 

estimation  pendekatan  nonparametrik  dengan  basis  penalized 
spline. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

tools 

Penggunaan 

sangat  membantu  dalam  proses 
melakukan  analisis  statistik.  Tools  yang  sering  digunakan 
dalam analisis statistik salah satunya yaitu aplikasi R. Aplikasi 
R tersebut banyak diminati karena  bersifat  free, open source, 
dan  memiliki  fitur statistik  yang cukup lengkap. Hal  tersebut 
yang  membuat  pemodelan  SAE  lebih  sering  menggunakan 
aplikasi R, apalagi dengan adanya penggunaan package yang 
disediakan  oleh  R.  Dalam  pemodelan  SAE  ada  beberapa 
package  yang  bisa  digunakan,  seperti  “sae”,  “sae2”,  dan 
sebagainya.  Namun,  package  untuk  pemodelan  SAE  dengan 
pendekatan nonparametrik masih belum tersedia. Oleh karena 
itu,  diperlukan  pembangunan  suatu  package  R  yang 
didalamnya dapat dilakukan pemodelan Small Area Estimatin 
(SAE)  dengan  pendekatan  nonparametrik,  dimana  basis  yang 
digunakan dalam pendekatan nonparametrik dalam package ini 
yaitu Kernel Nadaraya-Watson.  

Model  yang  dikembangkan  dalam  package  R  ini  akan 
diterapkan dalam data BPS. Salah satu data BPS yang penting 
sebagai acuan kebijakan pemerintah tetapi  hanya  cukup pada 
area  besar  sehingga  kurang  baik  dalam  menduga  area  kecil 
yaitu  data  pengeluaran  per  kapita.  Oleh  karena  itu  pada 
penelitian  ini  akan  menduga  pengeluaran  perkapita  pada 
tingkat  kecamatan  di  Provinsi  D.I.Yogyakarta  tahun  2019 
berdasarkan data Survei Sosial Ekonomi Nasional (SUSENAS) 
Maret  2019,  dimana  variabel  penyerta  yang  digunakan  yaitu 
data Potensi Desa (Podes) 2018. 

II.  TUJUAN PENELITIAN  
Tujuan  yang  ingin  dicapai  dalam  penelitian  ini  adalah 

sebagai berikut: 
1.  Mengkaji dan membangun  model  Small Area Estimation 
pendekatan  nonparametrik  berbasis  Kernel  Nadaraya-
Watson 

2.  Mengkaji estimasi Mean Square Error (MSE) pada Small 
Area  Estimation  pendekatan  nonparametrik  berbasis 
Kernel Nadaraya-Watson 

3.  Membangun  package  R  untuk  Small  Area  Estimation 
pendekatan  nonparametrik  berbasis  Kernel  Nadaraya-
Watson 

4.  Menerapkan  model  Small  Area  Estimation  pendekatan 
nonparametrik  berbasis  Kernel  Nadaraya-Watson  pada 
data  survei BPS (studi kasus  pengeluaran perkapita pada 
level kecamatan di provinsi D.I. Yogyakarta tahun 2019). 

III. PENELITIAN TERKAIT 

Berbagai  penelitian  yang  berkaitan  dengan  small  area 
estimation  dengan  pendekatan  nonparametrik  telah  banyak 
dilakukan.  Diantara  peneliti  tersebut,  [3]  Mukhopadhay  dan 
Maiti  (2004)  dengan 
judul  “Two  stage  non-parametric 
approach for small area estimation” sebagai pelopor penggunaa 
small area estimation pendekatan nonparametrik yang berbasis 
pada Kernel Nadaraya-Watson. Selanjutnya [4] Opsomer (2005)  
dengan  judul  “Nonparametric  Small  Area  Estimation  Using 
Penalized  Spline  Regression”  sebagai  pelopor  small  area 

Selain penelitian tersebut, di Indonesia sudah ada beberapa 
penelitian  yang  menggunakan  pendekatan  nonparametrik 
dalam  small  area  estimation  diantaranya  yaitu    [5]  Darsyah 
(2012) menggunakan SAE Kernel-Bootstrap, [6] Farida (2017) 
menggunakan  pendekatan  semiparametrik  penalized  spline, 
serta  [7]  Indahwati,  Sadik,  dan  Nurmasari  (2008)  dengan 
metode pendekatan pemulusan kernel. 

IV. METODE PENELITIAN  

4.1  Metode Analisis 
4.1.1  Estimasi Langsung (Direct Estimation) 

 Metode  estimasi  langsung  (direct  estimation)  merupakan 
cara  pendugaan  parameter  populasi  di  suatu  domain  dengan 
hanya didasarkan pada data sampel yang diperoleh dari domain 
tersebut  [2].  Pendekatan  dalam  metode  estimasi  ini  yaitu 
berbasis  rancangan  (design-based).  Teknik  estimasi  design-
based  merupakan  teknik  estimasi  yang  sesuai  dengan  desain 
penarikan  sampel  (sampling  design)  yang  digunakan  dalam 
pengumpulan data. 

 Nilai  hasil  estimasi  langsung  pada  suatu  area  kecil 
merupakan estimator tak bias meskipun memiliki varian yang 
besar  dikarenakan  dugaannya  diperoleh  dari  ukuran  sampel 
yang kecil [8]. Estimasi langsung membutuhkan ukuran sampel 
yang  cukup  besar  untuk  memenuhi  syarat  kecukupan  sampel 
sehingga  dapat  menghasilkan  estimasi  dengan  presisi  yang 
baik.  Ketika  ukuran  sampel  terlalu  kecil,  estimasi  langsung 
tidak dapat dilakukan karena akan menghasilkan presisi yang 
buruk.  Suatu  estimasi  dengan  presisi  yang  buruk  tidak  dapat 
dijadikan  bahan  pertimbangan  dalam  proses  pengambilan 
kebijakan.  Sehingga  pada  kondisi  seperti  ini  perlu  dilakukan 
estimasi tidak langsung untuk mengatasi hal tersebut. 

4.1.2  Small Area Estimation (SAE) 

Menurut  penelitian  [2],  Small  Area  Estimation  merupakan 
suatu  teknik  statistik  untuk  menduga  parameter-parameter 
subpopulasi  yang  ukuran  samplenya  kecil.  Estimasi  tidak 
langsung  merupakan  proses  estimasi  dimana  memanfaatkan 
informasi  lain  untuk  mengestimasi  nilai  parameter  dari  suatu 
area  tertentu.  Terdapat  dua  masalah  pokok  dalam  SAE. 
Masalah  pertama  adalah  bagaimana  menghasilkan  suatu 
dugaan parameter yang cukup baik dengan ukuran sampel kecil 
pada  suatu  domain  atau  area  kecil.  Masalah  kedua  yaitu 
bagaimana menduga Mean Square Error (MSE). Solusi untuk 
masalah  tersebut  adalah  dengan  “meminjam  informasi”  dari 
dalam area, luar area, maupun luar survei [8]. 

 Terdapat  dua 

ide  utama  yang  digunakan  untuk 
mengembangkan  model  pendugaan  parameter  small  area, 
yaitu:  
1.  Model pengaruh tetap (fixed effect model)  

yaitu dimana asumsi bahwa keragaman didalam small area 
variabel  respon  dapat  diterangkan  seluruhnya  oleh 
hubungan  keragaman  yang  bersesuaian  pada  informasi 
tambahan.  

2.  Pengaruh acak small area (random effect)  

 2 / 8 

 
 
 
 
 
 
yaitu dimana asumsi keragaman spesifik small area tidak 
dapat diterangkan oleh informasi tambahan. 

 Gabungan  antara  kedua  model  diatas  akan  membentuk 
model  campuran  (mixed  model).  Karena  variabel  respon 
diasumsikan  berdistribusi  normal  maka  pendugaan  area  kecil 
yang  dikembangkan  merupakan  bentuk  khusus  dari  General 
Linear Mixed Model (GLMM). 

 Model  area  kecil  biasanya  menggunakan  model  linier 

campuran dalam bentuk 

  (1) 
𝑦 = 𝑋𝛽 + 𝑍𝑢 + 𝑒 
dimana  𝑋  adalah  matriks  peubah  penyerta,  𝑍  adalah  vektor 
acak  yang  biasa  dikenal  sebagai  pengaruh  area  kecil,  dan  𝑒 
adalah vektor dari galat sampel [2].  

 Berdasarkan  ketersediaan  data,  pada  penelitian 

[2] 
menyebutkan  bahwa  estimasi  tidak  langsung  pada  area  kecil 
dikelompokkan menjadi dua jenis yaitu : 
1.  Basic Area Level Model 

Pada model dasar level area (basic area level model), data 
pendukung  yang  tersedia  hanya  sampai  pada  tingkat  area. 
Model  dasar  level  area  menghubungkan  penduga  langsung 
dengan data pendukung dari domain lain untuk setiap area yaitu 
𝑇 = (𝑥1𝑖, … , 𝑥𝑝𝑖). Parameter small area yang akan diestimasi 
𝑥𝑖
yaitu  𝜃𝑖 .  Model  linier  yang  menjelaskan  hubungan  tersebut 
adalah : 

𝜃𝑖 = 𝑥𝑖

𝑇𝛽 + 𝑏𝑖𝑢𝑖   𝑖 = 1,2, … , 𝑚 

  (2) 

𝑇
: (𝛽0, 𝛽1, … , 𝛽𝑝)

 adalah  koefisien  regresi  berukuran 

: konstanta positif yang diketahui 
:random  effect  dari 

small  area,  diasumsikan 

dimana : 
𝛽 
(𝑝 + 1) × 1 
𝑏𝑖 
𝑢𝑖 
𝑢𝑖~𝑖𝑖𝑑 𝑁(0, 𝜎𝑢
𝑚 

2) 

: jumlah observasi 

Dalam  membuat  kesimpulan  tentang  populasi  dibawah 
persamaan (2), diasumsikan bahwa  nilai pendugaan langsung 
𝜃̂𝑖 diketahui dan dapat dituliskan sebagai berikut : 

𝜃̂𝑖 = 𝜃𝑖 + 𝑒𝑖   𝑖 = 1,2, … , 𝑚 
 adalah 

  (3) 
sampling  error  yang  diasumsikan 

dimana  𝑒𝑖
𝑒𝑖~𝑁(0, 𝜎𝑒𝑖

2). 

Apabila  model  pada  persamaan  (2)  dan  (3)  digabungkan 
maka akan membentuk Fay-Herriot Model dengan persamaan 
sebagai berikut : 

𝜃̂𝑖 = 𝑥𝑖

𝑇𝛽 + 𝑏𝑖𝑢𝑖 + 𝑒𝑖   𝑖 = 1,2, … , 𝑚 

  (4) 

2.  Basic Unit Level Model 

Pada  model  dasar  unit  level  (Basic  Unit  Level  Model), 
𝑇 = (𝑥𝑖𝑗1, … , 𝑥𝑖𝑗𝑝) 
diasumsikan  bahwa  data  pendukung  𝑥𝑖𝑗
tersedia  untuk  setiap  elemen  ke-j  pada  area  ke-i.  Selanjutnya 
variabel 𝑦𝑖𝑗 diasumsikan berkaitan dengan 𝑥𝑖𝑗 sehingga bentuk 
persamaan modelnya yaitu sebagai berikut : 

𝜃̂𝑖𝑗 = 𝑥𝑖𝑗

𝑇𝛽 + 𝑢𝑖𝑗 + 𝑒𝑖   𝑗 = 1,2, … , 𝑛 ;  𝑖 = 1,2, … , 𝑚   (5)   

4.1.3  Regresi Kernel 

Secara  sederhana,  model  regresi  nonparametrik  dapat 

dituliskan sebagai: 

𝑦 = 𝑚(𝑥) + 𝜀 
(6) 
dimana  𝑦  merupakan  peubah  respon  yang  diamati,  𝑚(𝑥) 
merupakan  fungsi  regresi  yang  ingin  diduga  dan  tidak  dapat 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

didekati  dengan  model  parametrik,  serta  𝜀  merupakan  error 
pengamatan  yang  tidak  dapat  dijelaskan  oleh  fungsi  regresi 
𝑚(𝑥). 

 Beberapa  metode  pemulusan  yang  biasa  digunakan  untuk 
menduga  pada  persamaan  (6)  adalah 
local  polynomial 
smoothers,  regression  splines,  smoothing  splines,  penalized 
splines, dan Kernel smoothers. 

 Salah  satu  model  pendekatan  nonparametrik  yang  umum 
digunakan  adalah  estimator  kernel.  Estimator  densitas  kernel 
adalah suatu metode pendekatan dengan fungsi kernel terhadap 
fungsi densitas yang belum diketahui. Hal tersebut disebabkan 
estimator densitas kernel mempunyai beberapa kelebihan, yaitu 
[9] : 
1.  Estimator  kernel  mempunyai  bentuk  yang  fleksibel  dan 

secara matematis mudah dikerjakan. 

2.  Estimator  kernel  mempunyai  rata-rata  kekonvergenan 

yang relatif cepat. 

 Estimator  kernel  merupakan  pengembangan  dari  estimator 
histogram. Estimator kernel ini disebut juga estimator densitas 
kernel Rosenblatt-Parzen karena dikenalkan pertama kali oleh 
Parzen  (1962)  dan  Rosenblatt  (1956)  [9].  Menurut  penelitian 
[10]  pada  dasarnya  estimator  kernel  sama  dengan  estimator 
linier  lainnya  hanya  saja  metode  kernel  lebih  khusus  dalam 
penggunaan metode bandwidth. 

 Terdapat tiga macam estimator kernel, yaitu : 

1.  Nadaraya-Watson Estimate  
2.  Pritley-Chao Estimate 
3.  Gasser-Muller Estimate 

 Menurut  penelitian  [9],  fungsi  regresi  𝑚(𝑥𝑖) pada  model 
regresi  nonparametrik  dapat  diestimasi  dengan  pendekatan 
kernel  yang  didasarkan  pada  fungsi  densitas  kernel.  Estimasi 
densitas kernel didefinisikan dengan: 

1

𝑥

ℎ

)

𝑛
∑ 𝐾 (
𝑖=1

𝑚̂ ℎ =

(7) 
ℎ
dimana  𝐾(. )  disebut  dengan  fungsi  kernel  dan  ℎ  adalah 
bandwidth  atau  parameter  penghalus  yang  berfungsi  untuk 
mengatur  kehalusan  dari  kurva  yang  diestimasi.  serta  fungsi 
kernel diatas harus memenuhi : 
(i)  𝐾(𝑥) ≥ 0, untuk semua 𝑥 
(ii)  ∫ 𝐾(𝑥)𝑑𝑥
(iii) ∫ 𝑥2𝐾(𝑥)𝑑𝑥
(iv) ∫ 𝑥𝐾(𝑥)𝑑𝑥

= 𝜎2 > 0 
= 0 

= 1 

∞
−∞
∞
−∞
∞
−∞

 Beberapa fungsi Kernel yang umum digunakan adalah Box, 
Parzen, Triangel, dan Gaussian (Normal). Fungsi Kernel yang 
digunakan pada penelitian ini adalah fungsi Kernel Gaussian. 
Fungsi  Kernel  Gaussian  memiliki  persamaan  matematis 
sebagai berikut : 
𝐾(𝑥) =

(𝑥)2) , −∞ < 𝑥 < ∞ 

       (8) 

exp (−

1

1

√2𝜋

2

4.1.4  Pemilihan Bandwidth Optimum 

Bandwidth adalah parameter pemulus yang berfungsi untuk 
mengontrol kemulusan dari kurva yang diestimasi dan sebagai 
ukuran kesesuaian fungsi pada data, sehingga dalam memilih 
nilai bandiwdth diharapkan nilai yang optimal. Bandwidth yang 
optimal pada penelitian ini diperoleh dengan menghitung nilai 
cross  validation  (CV),  dimana  pemilihan  bandwidth  yang 

 3 / 8 

 
 
 
 
 
 
 
 
  
 
 
optimal diperoleh berdasarkan nilai CV minimum. [11] Metode 
CV dapat dinyatakan sebagai berikut:  
𝐶𝑉(ℎ) =

(9)  
dimana  𝐶𝑉 adalah  cross  validation,  ℎ  adalah  bandwidth,  𝑌𝑖 
adalah  variabel  dependen  ke- 𝑖  ,  dan  𝑚̂ (𝑋≠𝑖)  adalah  nilai 
estimasi. 

2
∑ (𝑌𝑖 − 𝑚̂ (𝑋≠𝑖))

𝑛
𝑖=1

𝑛

1

4.1.5  Small  Area  Estimation  Pendekatan  Nonparametrik 

Berbasis Kernel Nadaraya-Watson 

 Pada penelitian ini fungsi 𝑚(𝑥𝑖) diduga dengan persamaan 

Nadaraya-Watson Kernel, [3] merumuskannya sebagai : 

𝑚ℎ̂ (𝑥𝑖) =

∑ 𝐾ℎ(𝑥−𝑥𝑖)𝑦𝑖
𝑖
∑ 𝐾ℎ(𝑥−𝑥𝑖)

     (10) 

𝑖
dimana 𝐾ℎ(. ) adalah  fungsi  Kernel  dengan  Bandwidth ℎ dan 
𝐾ℎ(𝑥) =
) dengan 𝐾ℎ(. ) memenuhi 𝐾(. ) simetris, 𝐾(. ) 
terbatas dan kontinu pada daerah hasil 𝑥, dan ∫ 𝐾(𝑥) 𝜕𝑥 = 1. 

𝐾 (

ℎ

ℎ

𝑥

1

 Fungsi Kernel yang sering digunakan adalah fungsi normal 
[12].  Sehingga  dalam  penelitian  ini  fungsi  kernel  yang  akan 
digunakan  adalah  fungsi  kernel  normal  (Gaussian)  yaitu 
𝑥2) , −∞ < 𝑥 < ∞ 
𝐾(𝑥) =
 Maka  penduga 𝑚̂ ℎ(𝑥) berhubungan  terhadap 𝑦𝑖,  dan  dapat 

exp (−

√2𝜋

1

1

2

dituliskan sebagai : 
𝑚̂ ℎ(𝑥𝑖) =

𝑚
𝑖=1

1

∑ 𝑊ℎ𝑖(𝑥)𝑦𝑖

𝑚

 dimana 𝑊ℎ𝑖(𝑥) =

𝐾ℎ(𝑥−𝑥𝑖)
∑ 𝐾ℎ(𝑥−𝑥𝑖)

𝑖

1
𝑚

 (11) 

 Dengan  menggunakan  kernel  Nadaraya-Watson  diatas, 
maka  didapatkan  penduga  terbaik  dari  rataan  area  kecil  𝜃𝑖 
yaitu : 

𝐸(𝜃𝑖|𝑦𝑖) = 𝜃𝑖̃ = 𝛾𝑖𝑦𝑖 + (1 − 𝛾𝑖)𝑚ℎ̂ (𝑥𝑖) 
2 diketahui.  
 dengan asumsi 𝜎𝑢

2

𝜎𝑢
2+𝐷𝑖

𝜎𝑢

dimana 𝛾𝑖 =

 Bila  𝜎̂𝑢

2  merupakan  penduga  dari  𝜎𝑢
sehingga persamaan (12) akan menjadi :     

2  maka  𝛾𝑖̂ =

2
𝜎̂𝑢
2+𝐷𝑖
𝜎̂𝑢

, 

𝜃̂𝑖 = 𝛾𝑖̂𝑦𝑖 + (1 − 𝛾𝑖̂)𝑚ℎ̂ (𝑥𝑖) 

        (13) 

4.1.6  Mean Square Error (MSE) Bootstrap 

 Metode Bootstrap pertama kali diperkenalkan oleh Bradley 
Efron  pada  tahun  1979.  Metode  Bootstrap  merupakan  suatu 
metode  pendekatan  nonparametrik  untuk  menaksir  berbagai 
kuantitas  statistik  seperti  mean,  standar  error,  dan  bias  suatu 
estimator  atau  untuk  membentuk  interval  konfidensi  dengan 
memanfaatkan kecanggihan teknologi komputer. 

 [3]  Merumuskan  Mean  Square  Error  (MSE)  dari  𝜃̂  pada 

persamaan (13) sebagai : 

2

𝐷𝜎̂𝑢
2 + 𝐷
𝜎̂𝑢
+2𝐷2(𝜎̂𝑢

𝑚𝑠𝑒(𝜃𝑖̂ ) =

(1 − 𝛾)2𝑚𝑠𝑒[𝑚̃ ℎ(𝑥𝑖)] 

−3

2) 

𝑚𝑠𝑒(𝜎̂𝑢

2 + 𝐷)
   (15) 
Tetapi  nilai  𝑚𝑠𝑒(𝜃𝑖̂ )  pada  persamaan  diatas  tidak  dapat 
diselesaikan  karena 
tidak  dapat  dihitung  nilai  dari 
2). Sehingga pada peelitian ini akan 
𝑚𝑠𝑒[𝑚̃ ℎ(𝑥𝑖)] dan 𝑚𝑠𝑒(𝜎̂𝑢
menggunakan pendekatan bootstrap untuk memperoleh dugaan 
MSE  dari  𝜃̂ .    [5]  merumuskan  pendugaan  MSE  bootstrap 
sebagai berikut : 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

𝑚𝑠𝑒∗(𝜃𝑖̂ ) =

1

𝐵

∑

𝐵
𝐽=1

(𝜃𝑖̂ ∗(𝑗)

− 𝜃𝑖

∗(𝑗))

2

   (16) 

dimana  𝜃𝑖̂ ∗(𝑗)
 adalah  penduga  rataan  area  kecil  ke-i  dari 
∗(𝑗)  adalah  nilai  sebenarnya  rataan  area 
populasi  ke-j,  dan 𝜃𝑖
kecil ke-i dari populasi ke-j. Penentuan besaran nilai 𝐵 sangat 
variatif, tetapi dalam penelitian ini penulis menggunakan nilai 
𝐵 = 1000 karena dianggap sudah cukup mewakili. 

4.2  Metode Pengumpulan Data 

 Data  yang  digunakan  dalam  penelitian  ini  terdiri  dari  dua 
jenis data yaitu data studi kasus dan data simulasi. Data studi 
kasus  yang  digunakan  adalah  data  Survei  Sosial  Ekonomi 
Nasional 
(Susenas)  Maret  2019  untuk  mengestimasi 
pengeluaran  per  kapita  pada  tingkat  kecamatan  di  Provinsi 
D.I.Yogyakarta,  dimana  variabel  penyerta  yang  digunakan 
bersumber dari data Potensi Desa (Podes) 2018 dan publikasi 
yang dihasilkan oleh BPS.  

 Data  simulasi  diperoleh  dengan  membangkitkan  data. 
Dimana  banyaknya  area (𝑚) yang  digunakan  adalah  25,  50, 
dan  100.  Langkah  langkah  dalam  memperoleh  data  simulasi 
yaitu sebagai berikut : 
1.  Masukkan jumlah banyaknya area (𝑚) 
2.  Membangkitkan  variabel  penyerta  (𝑥𝑖)  sebanyak  𝑚 , 

dimana 𝑥𝑖~𝑈𝑛𝑖𝑓(0,1) 

3.  Memetakan  𝑥𝑖  melalui  fungsi  matematis  tertentu  untuk 
memperoleh 𝑚(𝑥𝑖),  dimana  𝑚(𝑥𝑖) = sin(2 × 𝜋 × 𝑥3) +
5 

4.  Membangkitkan  pengaruh  acak  area  (𝑢𝑖)  sebanyak  𝑚 , 

5.  Menghitung  parameter  area  kecil  𝜃𝑖 ,  dimana  𝜃𝑖 =

𝑚(𝑥𝑖) + 𝑢𝑖 

6.  Membangkitkan  error  (𝑒𝑖)  sebanyak  𝑚 ,  dimana 

𝑒𝑖~𝑁(0, 0.02) 

7.  Menghitung penduga langsung (𝑦𝑖), dimana 𝑦𝑖 = 𝜃𝑖 + 𝑒𝑖  
8.  Menghitung  bandwidth  optimum  dengan  menggunakan 

9.  Menghitung nilai pendugaan area kecil. 
10.  Menghitung nilai MSE  
11.  Langkah  4,5,6,7,9,  dan  10  dilakukan  berulang  sebanyak 
100  kali  perulangan.  Kemudian  mengukur  performa 
simulasi pendugaan yang dilakukan dengan menghitung : 
a.  Relative Bias untuk area kecil ke−𝑖 : 

(𝑟)

(𝑟)

𝑅
𝑟=1

∑

|

𝑅𝐵(𝜃𝑖̂ ) =

1
𝑅
b.  Rata-rata Mean Square Error (MSE) area ke−𝑖 : 
1
𝑅

∑ 𝑚𝑠𝑒𝑏𝑠

∗ (𝜃𝑖̂ ) =

∗(𝑟)(𝜃𝑖̂ )

𝑚𝑠𝑒̅̅̅̅̅̅𝑏𝑠

− 𝜃𝑖

100

(𝑟)

| 

𝜃̂𝑖
𝜃𝑖

𝑟=1

4.3  Tahapan Penelitian 

Tahapan penelitian yang dilakukan dalam penelitian ini yaitu 
menggunakan  metode  Desaign  Science  Research  yang 
diajukan[13], dimana metode ini memiliki lima tahapan antara 
lain : 
1.  Awareness Of Problem 

Tahapan  ini  dilakukan  dengan  identifikasi  permasalahan 
yang  berkaitan  dengan  Small  Area  Estimation  (SAE). 

 4 / 8 

        (12) 

dimana 𝑢𝑖~𝑁(0, 0.02) 

dimana 
 𝜎̂𝑢

2 = max (0,

1

𝑚−1

∑ 𝑊ℎ𝑖(𝑥)

𝑚
𝑖=1

[𝑦𝑖 − 𝑚ℎ̂ (𝑥𝑖)]2 − 𝐷)   (14)     

cross validation. 

 
 
  
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Proses ini dilakukan dengan cara membaca beberapa jurnal 
terkait SAE dan melihat masalah apa yang masih dihadapi 
dalam  penggunaan  SAE.  Melakukan  wawancara  dengan 
ahli  dibidangnya  juga  dapat  digunakan  sebagai  sumber 
informasi mengenai masalah yang ada pada SAE. 

2.  Suggestion 

berbasis  Kernel  belum  terimplementasikan.  Sehingga  hal  itu 
yang  melandasi  penulis  untuk  membangun  suatu  package  R 
pendekatan 
yang  mengimplementasikan  model  SAE 
Nonparametrik  dimana  basis  yang  digunakan  yaitu  Kernel 
Nadaraya-Watson  yang  nantinya  akan  mudah  untuk  diakses 
dan digunakan. 

Tahapan ini  merupakan  upaya dalam  mencari solusi dari 
permasalahan  yang  telah  diidentifikasikan  sebelumnya. 
Pencarian  solusi  dilakukan  dengan  studi  literatur  dan 
wawancara.  Studi  literatur  dilakukan  dengan  membaca 
buku atau jurnal terkait penelitian SAE pada bagian saran 
dan  kesimpulan  untuk  melihat  kemungkinan  solusi  yang 
dapat  digunakan  untuk  mengatasi  permasalahan. 
Wawancara dilakukan dengan meminta pendapat ataupun 
saran dari para ahli dibidang SAE.  

3.  Development 
tahap 

ini 

dilakukan 

development 

Pada 
proses 
pembangunan  package  yang  disusun  berdasarkan  saran 
dan  masukan 
telah 
diidentifikasikan  sebelumnya.  Tahapan  ini  terdiri  dari 
beberapa  bagian  yaitu  pemodelan,  perancangan,  dan 
implementasi. 

permasalahan 

terkait 

yang 

4.  Evaluation 

Evalusi dilakukan untuk memastikan bahwa package yang 
dibangun telah sesuai. Pada tahapan ini dilakukan dengan 
beberapa uji yaitu uji validitas, uji SUS, dan studi kasus.  

5.  Conclusion 

Pada  tahap  ini  dilakukan  dengan  penarikan  kesimpulan 
berdasarkan hasil dari seluruh proses penelitian yang telah 
dilakukan.  Salain  itu,  dalam  tahapan  ini  juga  mencakup 
saran penelitian terkait proses pengerjaan penelitian. 

VI. HASIL DAN PEMBAHASAN 

6.1 Awareness of Problem 

 Ketersediaan  implementasi  SAE  masih  cukup  terbatas. 
Implementasi  SAE  yang  bisa  digunakan  hanya  terdapat  pada 
package R yaitu sae dan sae2, model yang tercakup pun masih 
terbatas. Memahami algoritma dari SAE yang akan digunakan 
kemudian menerjemahkannya ke dalam script memakan waktu 
lama dan usaha yang besar. Dan script yang dibuat pun hanya 
bisa digunakan sendiri. 

 Selain  itu,  semua  implementasi  tersebut  mengasumsikan 
hubungan yang linier antara rataan area kecil dengan variabel 
penyertanya. Ada kalanya hubungan data tidak linier, akibatnya 
kurang 
yang 
tepat  mengimplementasikan  model 
mengasumsikan  hubungan 
linier.  Sehingga  diperlukan 
implementasi dari model dengan pendekatan nonparametrik. 

 Dari  uraian  tersebut,  didapatkan  diagaram  tulang  ikan 

(fishbone diagram) sebagai berikut : 

Waktu 

Knowledge  
Management 

Metode 

Memahami 
algoritma 

Script digunakan 
sendiri 

Data  
tidak linier 

Dibutuhkan 
Implementasi 
SAE Kernel 

V.  KERANGKA PIKIR 

Kerangka  pikir  dalam  penelitian  ini  dilakukan  sebagai 

gambar 1 berikut : 

Small Area  
Estimation 

Hanya metode 
lama 

Membuat 
script sendiri 

Implementasi 

Penggunaan 

Gambar 2. Fishbone Diagram 

SAE Kernel 

Tidak 

Linier 

Ya 

SAE FH 

Implementasi 

? 

Implementasi 

Package sae 

Gambar 1. Kerangka Pikir 

 Berdasarkan uraian sebelumnya, dijelaskan bahwa terdapat 
permasalahan  pada  metode  SAE  Fay-Herriot,  dimana  ketika 
hubungan  datanya  tidak  linier  maka  meminjam  kekuatan 
menjadi kurang tepat. Oleh karena itu, diperlukan model yang 
mampu  mengatasi  masalah  tersebut.  Model  SAE  Fay-Herriot 
telah  diimplementasikan  dalam  bentuk  package  di  R.  Tetapi, 
pengimplementasian  model  SAE  pendekatan  nonparametrik 

6.2 Suggestion 

 Dalam  penelitian 

ini  solusi  yang  ditawarkan  untuk 
menjawab  permasalahan  tersebut  adalah  dengan  membangun 
sebuah  package  R  untuk  “Small  Area  Estimatin  Non-
Parametric  Based  Nadaraya-Watson  Kernel”  dengan  nama 
package  “saekernel”.  Pembangunan  package 
ini  harus 
mengikuti  kaidah  yang  ada  dalam  Comprehensive  R  Archive 
Network  (CRAN)  sehingga  nantinya  dapat  digunakan  secara 
umum.  Pembangunan  Package  R  menjadi  pilihan  karena 
package R memiliki kelebihan sebagai berikut : 
1.  Modular 

Package R bersifat modular yang sangat extensible. Hal itu 
memberikan  kemudahan  bagi  para  developer  atau 
pengembang  untuk  membuat  package  baru  atau 
memanfaatkan  package  yang  ada  untuk  diterapkan  pada 
aplikasi lainnya. 

2.  Repository 

Karena sifatnya yang terbuka, sistem repository yang ada 
pada R memberikan kemudahan akses bagi para pengguna 

 5 / 8 

 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

sehingga  package  yang  ada  dalam  repository  ini  dapat 
digunakan  oleh  siapapun.  Package 
tersebut  dapat 
digunakan  secara  langsung  maupun  diterapkan  untuk 
menjadi bagian dari aplikasi lain. 

3.  Kekhasan R 

Bahasa  R  didesain  khusus  untuk  keperluan  statistik  dan 
grafis.  Sehingga  berbagai  fungsi  dasar  untuk  analisis 
statistik  maupun pembuatan grafis telah tersedia didalam 
R.  Hal 
ini  akan  memudahkan  developer  dalam 
mengimplementasikan metode-metode statistik baru di R.  

File  ini  berisikan  informasi  bagaimana  package  ini 
berinteraksi  dengan  package  lain  maupun  pengguna. 
Didalamnya mencakup : 
a.  Ekspor 

b. 

Fungsi atau objek yang bisa diakses dari luar baik oleh 
pengguna ataupun package lain. 
Impor 
Fungsi atau objek dari  package lain  yang digunakan 
oleh package yang dibangun. 

3.  Folder data 

Folder  ini  berisikan  data  yang  bisa  digunakan  dengan 

6.3 Development 
6.3.1 Pemodelan 

ekstensi .rda. 

4.  Folder R 

 Pemodelan  dalam  pembangunan  package  R  ini  yaitu 
menggunakan  use  case  diagram.  Diagram  use  case 
menggambarkan bagaimana interaksi antara actor (pengguna) 
dengan  sistem,  dan  juga  apa  saja  yang  dapat  dilakukan  pada 
sistem  yang  telah  dibangun.  Diagram  use  case  berikut 
menggambarkan 
interaksi  pengguna  dengan  package  R 
“saekernel” melalui aplikasi R. 

Package R saekernel 

UC01, Estimasi Area  

UC02, Estimasi MSE 

UC03,  load dataset 

<<include>> 

<<include>> 

<<include>> 

Pengguna 

Folder  ini  berisiskan  semua  source  code  dari  metode 
yang  diimplementasikan.  Source  code  bisa  dalam  1  atau 
beberapa file. 

5.  Folder man  

Folder ini berisikan dokumentasi dari fungsi atau objek 
dalam package dengan ekstensi .Rd. Dokumentasi ini bisa 
diakses  oleh  pengguna  dengan  memanggil  help. 
Dokumentasi  ini  dibuat  secara  otomatis  menggunakan 
“roxygen2”. 

saekernel/ 

UC04, load package saekernel 

DESCRIPTION 

NAMESPACE 

data/ 

R/ 

man/ 

Gambar 3. Use Case Diagram 

6.3.2 Perancangan 

Kernel 

berbasis 

 Algoritma  dari  Small  Area  Estimation  pendekatan 
nonparametrik 
Nadaraya-Watson 
diimplementasikan  pada  sebuah  package  baru  yang  bernama 
“saekernel”.  Package  “saekernel”  dirancang  dengan  struktur 
yang  telah  ditetapkan  oleh  R.  Struktur  tersebut  terdiri  dari 
beberapa komponen yaitu : 
1.  File DESCRIPTON 

File  ini  mencakup  informasi  umum  mengenai  package 

yang dirancang. Informasi didalamnya terdiri dari : 
a.  Nama 

Nama dari package yang dibangun. Nama tidak boleh 
mengandung spasi, dan harus unik. 

b.  Tipe 
c. 
Judul 
Judul dari package yang dibangun. 

d.  Versi 

Versi dari package yang dibangun. 

e.  Author dan Maintainer 

Nama developer yang telah mengembangkan package. 

f.  Deskripsi 

Penjelsan  tentang  package  secara  detail  dalam  1 
paragraf. 

2.  File NAMESPACE 

Data_saekernel.

rda 

Data_saekernel.

R 

Data_saekernel.

Rd 

saekernel.R 

Saekernel.Rd 

mse_saekernel.

R 

mse_saekernel.

Rd 

Gambar 4. Struktur Perancangan Package “saekernel” 

6.3.3 Implementasi 

Area 

Small 

Kernel 

berbasis 

Estimation 

pendekatan 
 Algoritma 
nonparametrik 
Nadaraya-Watson 
diimplementasikan  di  R  dalam  bentuk  function.  Pengguna 
dapat mengakses kedua function yang terdapat dalam package, 
yaitu  saekernel  dan  mse_saekernel.  Pengguna  juga  dapat 
menggunakan  dataset  yang  tersedia  dengan  cara  me-load 
dataset  Data_saekernel.  Kedua  function  dan  dataset  yang 
tersedia  dapat  digunakan  setelah  me-load  package  saekernel 
pada aplikasi R yang digunakan. 

 Package  dibangun  dengan  membuat  R-project  package. 
Dokumentasi  dari  package  “saekernel”  dibuat  menggunakan 
lain  yaitu  package  “roxygen2”  yang  dapat 
package 
mengenerate file .rd dimana outputnya dapat dilihat pada fitur 
help  package.  Dokumentasi  yang  dirancang  pada  package 
“saekernel”  berfungsi  sebagai  bantuan  untuk  penguna  dalam 
menggunakan setiap algoritma pada package tersebut.  

6.4 Evaluasi 
6.4.1 Uji Validitas 

 6 / 8 

 
 
 
 
 
 
 
 
 
 Uji validitas dilakukan dengan menggunakan data simulasi 
yang telah telah disebutkan sebelumnya. Dimana uji validitas 
bertujuan  untuk  melihat  kesesuaian  output  yang  dihasilakan 
dari algoritma yang telah dibangun. Bandwidth yang digunakan 
dalam  SAE  Kernel  dihitung  menggunakan  Cross  Validation 
dimana  untuk  jumlah  domain  25,  50,  dan  100  berturut-turut 
menghasilkan  nilai  bandwidth  optimum  yaitu  0.11,  0.08,  dan 
0.05. Hasil simulasi dan performa dari Small Area Estimation 
pendekatan  nonparametrik  berbasis  Kernel  Nadaraya-Watson 
dengan jumlah domain 25, 50, dan 100 ditunjukkan pada table 
berikut : 

TABEL I 
NILAI UJI VALIDITAS 

Domain 

25 

50 

100 

RB 

Kernel 

0.004740 

0.004754 

0.005679 

FH 

0.007225 

0.006430 

0.006759 

Rata 

Kernel 

0.000884 

0.000879 

0.001043 

MSE 

FH 

0.01811 

0.01830 

0.01803 

 Dengan  menggunakan  data  simulasi  yang  tidak  memiliki 
hubungan  linier  memberikan  hasil  estimasi  dari  SAE  Kernel 
yang memiliki nilai Relative Bias (RB) dan Mean Square Error 
(MSE)  yang  lebih  kecil  dibandingkan  SAE  Fay-Herriot. 
Sehingga  dapat  disimpulkan  bahwa  algoritma  yang  dibangun 
sudah sesuai karena menunjukkan hasil yang diharapkan. 

6.4.2 System Usability Scale (SUS) 

 Uji System Usability Scale (SUS) digunakan untuk melihat 
apakah  program  yang  dibuat  dapat  diterima  atau  tidak  oleh 
pengguna. Pada penelitian ini uji SUS dilakukan terhadap 11 
responden  yang  merupakan  mahasiswa  tingkat  IV  Politeknik 
Statistika  STIS  yang  skripsinya  menggunakan  topik  SAE. 
Berdasarkan  hasil  uji  SUS,  didapatkan  nilai  rata-rata  secara 
keseluruhan  sebesar  75.45.  Artinya  package  dapat  digunakan 
dengan baik oleh pengguna. Nilai SUS terkecil terdapat pada 
pertanyaan SUS nomor 10, yakni Saya perlu membiasakan diri 
terlebih  dahulu  sebelum  menggunakan  sistem  ini.  Nilai  SUS 
terbesar  terdapat  pada  pertanyaan  SUS  nomor  5,  yakni  Saya 
merasa fitur-fitur sistem ini berjalan dengan semestinya. 

6.4.3 Studi Kasus 
6.4.3.1 Bandwidth Optimum 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 5. Bandwidth Optimum Studi Kasus 

 Dari gambar  5 terlihat bahwa  dengan  menggunakan  cross 
validation diperoleh dari nilai 0 sampai 25 menghasilakan nilai 
bandwidth optimum yaitu sebesar ℎ = 2.  
6.4.3.2 Regresi Kernel 

Gambar 6. Regresi Kernel 

 Gambar  6  menunjukkan  Regresi  Kernel  yang  dihasilkan, 
dimana  terlihat  bahwa  Regresi  Kernel  yang  dihasilkan  lebih 
fleksibel sehingga mengikuti pola data. Dimana regresi Kernel 
memiliki nilai rata-rata sebesar 11.601 yang tidak jauh berbeda 
dengan nilai Y sesungguhnya yaitu 11.844.  
6.4.3.3 Perbandingan Hasil Estimasi 

Gambar 7. Perbandingan Nilai Estimasi 

 Perbandingan  nilai  pendugaan  dari  model  yaitu  pada 
gambar 7 dapat dilihat bahwa penduga langsung, penduga SAE 
kernel,  dan  pendugaan  SAE  Fay-Herriot  ketiganya  memiliki 
pola data yang serupa, dengan nilai mean ketiganya berturut-
turut yaitu 11.844, 11.586, dan 11.338 yang artinya nilai SAE 

 7 / 8 

 
 
 
 
 
 
 
 
Kernel lebih mendekati nilai penduga langsung. Dimana ketiga 
model nilai minimum yaitu pada Kecamatan Purwosari. Tetapi 
nilai maksimal penduga langsung berbeda dengan kedua model 
lainnya, dimana pada penduga langsung yaitu Kecamatan Mlati 
sedangkan pada SAE Kernel dan SAE Fay-Herriot yaitu pada 
Kecamatan Pakualaman.  
6.4.3.4 Perbandingan Nilai MSE 

Gambar 8. Perbandingan nilai MSE 

 Perbandingan  nilai  MSE  ketiga  model  dapat  dilihat  pada 
gambar  8.  Berdasarkan  gambar  8  MSE penduga  SAE  Kernel 
memiliki  mean  sebesar  0.491615  dimana  jauh  lebih  baik 
dibandingkan MSE SAE Fay-Herriot yaitu sebesar 2.0127 dan 
MSE Penduga Langsung yaitu sebesar 3.451.  
6.4.3.5 Perbandingan Nilai RSE 

Gambar 9. Perbandingan nilai RSE 

 Perbandingan  nilai  RSE  ketiga  model  dapat  dilihat  pada 
gambar  8.  Berdasarkan  gambar  8  RSE  penduga  SAE  Kernel 
lebih  baik 
memiliki  mean  sebesar  2.774  dimana 
dibandingkan RSE SAE Fay-Herriot yaitu sebesar 10.597 dan 
RSE Penduga Langsung yaitu sebesar 11.698.  

jauh 

VII. 

PENUTUP 

7.1 Kesimpulan 

Berdasarkan hasil dan pembahasan pada bagian sebelumnya, 

diperoleh beberapa kesimpulan yaitu : 
1.  Model SAE Kernel merupakan suatu penerapan dari model 
SAE  Fay-Herriot 
pendekatan 
dimana 
nonparametrik  dan  menggunakan  Kernel  Nadaraya-
Watson sebagai basis pendekatannya.  

dilakukan 

2.  Estimasi  MSE  pada  SAE  Kernel  tidak  dapat  dilakukan 
perhitungannya sehingga estimasi MSE pada SAE Kernel 
dilakukan dengan pendekatan Bootstrap. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

3.  Package 

Area 

Small 

pendekatan 
nonparametrik berbasis Kernel Nadaraya-Watson berhasil 
dibangun  dan  dapat  diakses  pada  CRAN  dengan  nama 
“saekernel”. 

Estimation 

4.  Small Area Estimation pendekatan nonparametrik berbasis 
Kernel Nadaraya-Watson dapat diterapkan pada data BPS, 
yaitu  untuk  mengestimasi  pengeluaran  per  kapita  level 
kecamatan  di  Provinsi  D.I.Yogyakarta.  Dimana  SAE 
Kernel  memberikan  hasil  yang  lebih  baik  dibandingkan 
model  parametrik  SAE  Fay-Herriot  dan  Estimasi 
Langsung pada pola hubungan data yang tidak linier. Hal 
ini  ditunjukkan  dengan  nilai  MSE  SAE  Kernel  dengan 
pendekatan  bootstrap  memiliki  nilai  yang  lebih  kecil 
dibandingakan MSE SAE Fay-Herriot dan MSE estimasi 
langsung. 

7.2 Saran 

Berdasarkan  penelitian  yang 

telah  dilakukan,  penulis 

memberikan saran yaitu sebagai berikut : 
1.  Mengembangkan 

package 

“saekernel” 

dengan 

menggunakan model Multivariat. 
lainnya 

2.  Menggunakan 

basis 

dalam 

pendekatan 

nonparametrik pada Small Area Estimation (SAE). 

DAFTAR PUSTAKA 
[1]  Prasad. NG Narasimha, Rao. Jon NK, The estimation of the mean squared 
error  of  small-area  estimators, Journal  of  the  American  statistical 
association, 85.409, 163-171, 1990. 

[2]  J. N. K. Rao and I. Molins, Small Area Estimation. New Jersey:Jhon Wiley 

& Sons, 2003. 

[3]  Mukhopadhyay.  Pushpal,  Maiti.  Tapabrata,  Two  stage  non-parametric 
approach for small area estimation, Proceedings of ASA Section on Survey 
Research Methods, 4058-4065, 2004. 

[4]  Opsomer,  J.  D.,  et  al.  Non‐parametric  small  area  estimation  using 
penalized spline regression. Journal of the Royal Statistical Society, Series 
B (Statistical Methodology), 2008, 70.1: 265-286. 

[5]  Darsyah,  Moh.Yamin.  Small  Area  Estimation  terhadap  Pengeluaran  Per 
Kapita di Kabupaten Sumenep Dengan Pendekatan Nonparametrik. Jurnal 
Statistika Universitas Muhammadiyah Semarang, 2013, 1.2. 

[6]  Apriani,  Farida. Pemodelan  pengeluaran  per  kapita  menggunakan  small 
area  estimation  dengan  pendekatan  semiparametrik  penalized  spline: 
Institut Teknologi Sepuluh Nopember, 2017. 

[7]  Indahwati, Sadik K., Nurmasari. R. Pendekatan Metode Pemulusan Kernel 

Pada Pendugaan Area Kecil, 2008. 

[8]  Pfefferman,  D.  Small  area  estimation‐new  developments  and 

directions. International Statistical Review, 2002, 70.1: 125-143. 

[9]  Hardle, W. Applied nonparametric regression: Cambridge university press, 

1990. 

[10] Eubank, R. L. Spline smoothing and nonparametric regression. New York: 

M. Dekker, 1988. 

[11] Anisa. N, Debataraja. N. N., Martha. S, ESTIMASI MODEL REGRESI 
NONPARAMETRIK  KERNEL  MENGGUNAKAN  ESTIMATOR 
NADARAYA-WATSON. BIMASTER, 8.4. 

[12] Silverman,  B.  W. Density  estimation  for  statistics  and  data  analysis. 

Routledge, 1986. 

[13] Vaishnavi. V. K., Kechler.  W,  Improving and innovating information & 
communication  technology:  Design  science  research  methods  and 
patterns. Taylor Francis, 2008. 

 8 / 8 

 
 
 
 
 
 
 
"
221710038,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Kajian Penerapan Safe Exam Browser dalam Ujian 
dengan  Sistem Ujian BYOD Di Politeknik Statistika 
STIS  

Verry Fitra Nata Rahman (221710038, 4SI1) 
Dosen Pembimbing: Lutfi Rahmatuti Maghfiroh SST, MT 

Ringkasan—  Evaluasi  merupakan  hal  yang  penting  dalam 
proses pembelajaran, salah satu bentuk evaluasi tersebut adalah 
pelaksanaan ujian. Dalam keadaan pembelajaran daring selama 
pandemi  yang  dilakukan  oleh  Politeknik  Statistika  STIS 
menggunakan  bantuan    Learning  Management  System  (LMS)  . 
Namun  diperlukan suatu sistem  ujian  yang  pada  penerapannya 
mampu    memberi  evaluasi  pembelajaran  yang  baik  dengan 
menjamin keamanan ujian untuk memberikan hasil yang sesuai 
seperti  pada  keadaan  pembelajaran  luring.  Safe  Exam  Browser 
merupakan  aplikasi  bantu  untuk  mengamankan  ujian  daring 
yang  dapat  mengakses  web  dengan  tampilan  layar  penuh.  Safe 
Exam  Browser  bisa  dipadukan  dengan  LMS  untuk 
ingin 
menyelenggarakan  ujian  daring.  Untuk 
mengkaji  tentang  penerapan  Safe  Exam  Browser  dalam  ujian 
dengan sistem ujian Bring Your Own Device (BYOD) di Politeknik 
Statistika STIS. Pada penelitian ini, model ujian akan dibangun 
berdasarkan  data  observasi  yang  diperoleh  dan  hasil  evaluasi 
yang  didapat  dari  penetration  testing.  Hasil  dari  penelitian  ini 
diharapkan  dapat  menjadi 
rekomendasi  dalam  model 
pelakasanaan ujian secara daring dan selanjutnya bisa digunakan 
sebagai  pengganti  ujian  menggunakan  kertas  pada  ujian  luring 
Politeknik Statistika STIS. 

itu  peneliti 

Kata  Kunci—  Safe  Exam  Browser,  Learning  Management 

System, Penetration testing, BYOD, Ujian Daring,  

I.  LATAR BELAKANG 

Pandemi  Covid-19  telah  mempengaruhi  sistem  pendidikan 
di  seluruh  dunia  yang  berujung  pada  penutupan  lembaga 
pendidikan  yaitu  sekolah,  universitas  serta  perguruan  tinggi. 
Hal  ini  membuat  pengaruh  yang  besar  karena  banyak  negara 
termasuk  Indonesia  meliburkan  aktivitas  pendidikan  yang 
membuat  lembaga  pendidikan  harus  menghadirkan  alternatif 
dalam  proses  pendidikan  baik  bagi  siswa maupun mahasiswa 
yang  tidak  bisa  melaksanakan  Pendidikan  di  lembaga  [1]. 
Melalui  kebijakan  pemerintah,  seluruh  kegiatan  belajar  tatap 
muka  ditiadakan  dan  diganti  dengan  kegiatan  pembelajaran 
jarak jauh guna menekan transmisi virus di tengah masyarakat 
[2]. 

Politeknik  Statistika  STIS  (yang  selanjutnya  akan  disebut 
STIS)  merupakan  salah  satu  lembaga  pendidikan  yang  telah 
menerapkan  pembelajaran  daring  selama  masa  pandemi. 
Segala sesuatu dalam proses pendidikan diusahakan memiliki 
kapasitas yang setara dengan kondisi normal saat pembelajaran 
luring.  Saat  pembelajaran  daring  beberapa  Learning 
Management  System  (LMS)  digunakan  untuk  memudahkan 
kegiatan pembelajaran.  

LMS  adalah  sebuah  perangkat  lunak  atau  software  untuk 
kebutuhan administrasi, dokumentasi, laporan sebuah kegiatan, 
kegiatan  belajar  mengajar,  kegiatan  secara  daring  (terhubung 
ke internet), E-learning, materi-materi pelatihan dan semua itu 
dilakukan dengan daring.”[3]. LMS juga bisa digunakan untuk 
melakukan  evaluasi  pembelajaran  dimana  LMS  dijadikan 
sarana untuk melaksanakan penilaian berupa ujian. LMS yang 
umumnya diketahui dan digunakan banyak orang diantaranya 
Moodle [4], Google Classroom [5], Kahoot [6], dan, Quiziz [7]. 

Evaluasi merupakan  bagian  dari  proses  pembelajaran  yang 
secara  keseluruhan  tidak  dapat  dipisahkan  dari  kegiatan 
mengajar,  melaksanakan  evaluasi  yang  dilakukan  dalam 
kegiatan pendidikan mempunyai arti yang sangat utama, karena 
evaluasi  merupakan  alat  ukur  atau  proses  untuk  mengetahui 
tingkat  pencapaian  keberhasilan  yang  telah  dicapai  peserta 
didik atas bahan ajar atau materi-materi yang telah disampaikan, 
sehingga  dengan  adanya  evaluasi  maka 
tujuan  dari 
pembelajaran akan terlihat secara akurat dan meyakinkan [8] 

Salah satu cara untuk melakukan evaluasi dan melihat hasil 
pembelajaran di STIS seperti yang tercantum dalam Peraturan 
Direktur  Politeknik  Statistika  STIS  nomor  002  tahun  2020 
tentang  pedoman  penyelenggaraan  pendidikan  di  Politeknik 
Statistika  STIS  adalah  dengan  melaksanakan  ujian  yang 
dilakukan  pada  tengah  semester  (UTS)  dan  akhir  semester 
(UAS).  Ujian  dilakukan  untuk  menilai  sejauh  mana 
pemahaman yang didapat oleh mahasiswa selama pembelajaran.  

Berdasarkan  dari  hasil  observasi,  dalam  keadaan  luring  di 
STIS, UTS dan UAS dilaksanakan dengan aturan seperti ujian 
pada umumnya seperti close book ( terutama untuk mata kuliah 
yang bersifat teoritis) , tidak boleh mencontek dan tidak boleh 
bekerja sama serta diawasi oleh 2 pengawas ujian selama batas 
waktu yang ditentukan. Ujian juga berbentuk Paper based atau 
masih menggunakan kertas. 

Sementara  pada  keadaan  ujian  secara  daring,  aturannya 
sedikit  berbeda  yaitu  sebagian  besar  open  book,  tidak  boleh 
bekerja sama, namun ujian tersebut hanya dibatasi oleh waktu 
dan peserta ujian hanya menuliskan sebuah “Pakta Integritas” 
bahwa  mereka  tidak  melakukan  kecurangan  tanpa  ada  yang 
mengawasi selama ujian   [9].  Sama  seperti  perguruan  tinggi 
lainnya ujian online merupakan kondisi baru untuk dosen dan 
mahasiswa,  penilaian  bagi  mahasiswa  bisa  saja  memiliki 
kesalahan  pengukuran,  tidak  seperti  pengukuran  seperti  biasa 
dilakukan [1]. 

 1 / 8 

 
 
 
 
 
 
 
hal  ini  bisa  menjadi  sebuah  kelemahan  dalam  evaluasi 
pembelajaran  selama  pandemi  dan  terlebih  lagi  tidak  ada 
jaminan  jika  peserta  benar  benar  mengerjakan  ujian  dengan 
kemampuannya  sendiri  tanpa  bantuan  orang  lain  seperti  di 
keadaan pembelajaran normal. Hal ini pernah diteliti Danielle 
Truszkowski  pada  tahun  2019  tentang  bagaimana  perbedaan 
ujian  daring  yang  diawasi  dengan  ujian  daring  yang  tidak 
diawasi  memiliki  perbedaan  hasil  karena  kondisi  keamanan 
tersebut memungkinkan terjadi beberapa jenis kecurangan [10]. 
Di  STIS  ujian  juga  memang  dilakukan  secara  daring  namun 
jawaban  yang  dikumpulkan  masih  ditulis  di  kertas  dan 
kemudian  di  foto/dipindai  untuk  dijadikan  format  pdf.  Untuk 
mengetahui hal  ini  lebih  lanjut  perlu dilakukan pengumpulan 
data terhadap pelaksana dan juga peserta ujian. 

Dari uraian di atas perlu suatu sistem yang dapat mengawasi 
evaluasi  pembelajaran  atau  ujian  agar  berjalan  dan 
menghasilkan  output  yang  sesuai  seperti  pada  keadaan 
pembelajaran  luring.  Safe  Exam  Browser  (SEB)  adalah 
lingkungan browser web untuk melakukan penilaian elektronik 
dengan  aman.  Perangkat  lunak  ini  mengubah  komputer 
manapun untuk sementara menjadi workstation yang aman. Hal 
ini mengontrol akses ke sumber daya seperti fungsi sistem, situs 
web  dan  aplikasi lain  dan mencegah  sumber  daya  yang  tidak 
sah  digunakan  selama  ujian  [8].  Singkatnya  SEB  membatasi 
akses kontrol terhadap apa saja yang bisa dilakukan oleh user 
selama melaksanakan ujian. SEB membantu mencegah segala 
bentuk  kecurangan  dengan  cara  mencegah  akses  ke  sumber 
daya  dan  utilitas  yang  tidak  diinginkan  seperti  fungsi  sistem, 
situs  web,  aplikasi  dan  file  selama  ujian  berlangsung  [11]. 
Untuk memperkuat keamanan SEB juga bisa dipadukan dengan 
aplikasi meeting online seperti Zoom [12] dan  Google Meet [13] 
untuk memantau  peserta  ujian.SEB  juga  bisa  dikombinasikan 
dengan  berbagai  Learning  Management  System  (LMS)  yang 
sudah  disebutkan  di  uraian  sebelumnya.  SEB  bisa  menjadi 
solusi  untuk  dalam  pelaksanaan  ujian  daring  di  STIS  dan 
selanjutnya  bisa menjadi  solusi  untuk mengganti  sistem  ujian 
paper based menjadi sistem Bring your own Device (BYOD). 
“Bawa perangkat anda sendiri” (BYOD) mengacu pada tren 
karyawan  yang  menggunakan  perangkat  pribadi  untuk 
terhubung ke jaringan organisasi mereka dan mengakses sistem 
terkait pekerjaan dan data yang berpotensi sensitif atau rahasia. 
Perangkat  pribadi  dapat  mencakup  ponsel  cerdas,  komputer 
pribadi, tablet, atau drive USB [14]. salah satu contoh BYOD 
yang  telah  diterapkan  di  Indonesia  terdapat  pada  penelitian 
Ambiya  dkk  pada  tahun  2019  yang  dilakukan  kepada  siswa 
SMKN  1  Lahat  dengan  perangkat  android  [15]    Secara  garis 
besar  metode  BYOD  ini  menuntut  peserta  ujian  membawa 
perangkatnya  sendiri  untuk  melakukan  ujian.  Maka  dari  itu 
peneliti  melakukan  kajian  lebih  lanjut  mengenai  bagaimana 
penerapan  Safe  Exam  Browser  dalam  ujian  dengan    sistem 
ujian BYOD di STIS bisa direkomendasikan.  

II.  TUJUAN PENELITIAN 

Tujuan  dilakukannya  penelitian  ini  adalah  melakukan 
pengkajian  agar  penerapan  SEB  bisa  dijadikan  rekomendasi 
dan digunakan dalam sistem ujian BYOD di STIS.  

III. PENELITIAN TERKAIT 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Terdapat  beberapa  penelitian 

terkait  yang  dilakukan 
mengenai  penerapan  BYOD  dan  SEB  pada  sistem  ujian. 
Penelitian yang  dilakukan oleh Ambiyar dkk (2019) [15]  telah 
memadukan SEB pada klien android untuk ujian berbasis web 
untuk mengoptimasi ujian dengan metode BYOD untuk siswa 
SMKN  1  Lahat.  Tujuan  penelitian  ini  untuk  menghasilkan 
sebuah  browser  yang  valid  untuk  membatasi  akses  terhadap 
fitur-fitur  di  perangkat  handphone  selama  ujian.  Hasil  dari 
penelitian  menunjukan  bahwa  SEB  bisa  bekerja  dengan  baik 
dalam mengawasi ujian. 

Penelitian Berikutnya adalah penelitian yang dilakukan oleh 
Søgaard,T.  M  pada  tahun  2016  [16].  Penelitian  ini  bertujuan 
untuk  memitigasi  kecurangan  yang  terjadi  pada  ujian  BYOD 
dengan  SEB.  Metode  yang  digunakan  dalam  penelitian  ini 
dengan    analisis  kerentanan  dan  resiko  kemudian  dilanjutkan 
dengan  penetrasi  testing  dengan  metode  HARM.  Hasil  yang 
didapatkan  beberapa  hasil  kerentanan  dan  juga  solusi  terkait 
mitigasi ujian BYOD dengan SEB. 

Penelitian selanjutnya dilakukan oleh Danielle Truszkowski 
pada  tahun  2019  yang  berjudul  “Proctored  Versus  Non-
Proctored  Testing:  A  Study  for  Online  Classes”  membahas 
tentang perbedaan antara ujian daring yang diawasi dan ujian 
daring  yang  tidak  diawasi  [10].  Dalam  disertasinya  ia 
menemukan  bahwa  nilai  akhir  dari  ujian  yang  tidak  diawasi 
memiliki  nilai  yang  lebih  tinggi  daripada  ujian  yang  diawasi 
karena penggunaan bantuan yang tidak sah /tidak diizinkan dan 
beberapa jenis kecurangan. 

Untuk 

referensi  dalam  melakukan  penetration 

test 
menggunakan  metode  HARM  (Hacker  Attack  Refresentation 
Method),  Penelitian  Aparna  Vegendla  pada  tahun  2016 
mencontohkan  bagaimana  penetration  test  dilakukan  dengan 
mengekstensi  metode  HARM  dan  menerjemahkannya  untuk 
mencapai  pendekatan  yang  sistematik    terhadap  serangan-
serangan  yang  mungkin  terjadi  untuk  disimulasikan  terhadap 
sistem ujian BYOD [17] 

Terakhir  ada  juga  penelitian  terbaru  yang  dilakukan  oleh  
Goffredo Haus dkk di Universitas of Milan, Italia, pada tahun 
2020,  penelitian  ini  membahas  tentang  masalah  menjalankan 
ujian tertulis  daring  selama  masa  darurat  covid-19  dan  solusi 
berupa beberapa skenario ujian dan pengawasan dari kombinasi 
LMS  dan  aplikasi  pengawas  ujian  seperti  SEB,  Exam.net, 
Proctorio  dan  Respoundus  [18].  Yang  pada  kesimpulannya 
skenario dengan aplikasi pengawas ujian tersebut bisa menjadi 
solusi dalam menangani ujian online tertulis. 

IV. METODE PENELITIAN 

Pendekatan  yang  digunakan  pada  penelitian  ini  adalah 
pengumpulan  data  dengan  melakukan  wawancara  dan  survei 
serta  observasi.  Wawancara  dilakukan  kepada  subject  matter 
yaitu  pelaksana  ujian  yang  terdiri  dari  dosen  dan  BAAK, 
sedangkan survei dan observasi akan dilakukan kepada peserta 
ujian  yang  terdiri  dari  mahasiswa  STIS.  Metode  yang 
digunakan untuk  pengujian  sistem adalah metode  penetration 
testing dengan metode HARM [12]. 
Adapun langkah-langkah penelitian meliputi : 
1.  Memilih dan merumuskan masalah.  
2.  Menelusuri sumber-sumber kepustakaan.  
3.  Melakukan observasi dan wawancara. 

 2 / 8 

 
 
4.  Menginterpretasikan  kondisi  penelitian  dengan  data 

yang diperoleh. 

5.  Melakukan analisis data.  
6.  Membangun model prototype sesuai analisis data.  
7.  Melakukan  Penetration 

testing 
prototype  dan melakukan evaluasi 

terhadap  model 

8.  Membuat  model  akhir  yang  akan  direkomendasi 

sesuai hasil evaluasi prototype 

9.  Membuat  laporan  penelitian  ilmiah  menggunakan 
grafik,gambar,  flowchart,  atau  alat  bantu  visual 
lainnya  untuk memudahkan. 

V.  KERANGKA PIKIR 

Evaluasi  pembelajaran  di  STIS  melalui  ujian  jarak  jauh 
(UJJ)    masih  belum  dikatakan maksimal  karena  masih  belum 
terdapat  pengaman  ujian  yang  memantau  kegiatan  tersebut. 
Model  ujian  yang  sesuai  pun 
juga  dibutuhkan  untuk 
memaksimalkan output UJJ terutama di bagian keamanan. Dari 
latar  belakang 
tersebut  peneliti  melakukan  beberapa 
pengumpulan  data  yang  selanjutnya  digunakan  untuk 
menentukan model ujian yang sesuai untuk dipadukan dengan 
SEB sebagai pengaman ujian. 

Gambar 1. Kerangka pikir penelitian 

VI. HASIL DAN PEMBAHASAN 
A.  Hasil Observasi dan Pengumpulan Data 

Berdasarkan  survei  pendahuluan  yang  dilakukan  kepada 
seluruh  mahasiswa  STIS  didapat  informasi  kelengkapan 
mahasiswa diantaranya adalah jenis dan OS perangkat yang 
digunakan sampai kualitas jaringan dalam menghadapi PJJ. 
Didapat  juga  informasi  mengenai  jenis  ujian,LMS  yang 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

digunakan sampai kondisi ujian dalam PJJ. Berikut adalah 
hasil dari survei tersebut. 
a.  Perangkat digunakan untuk PJJ 

Dari survei diketahui dalam melaksanakan PJJ 74,2% 
mahasiswa  memanfaatkan  Laptop/Pc  bersamaan 
dengan  smartphone,  24,5%  hanya  memanfaatkan 
1.3% 
Laptop/Pc 
menggunakan Smartphone 

sedangkan 

sisanya 

yaitu 

b.  OS Laptop/PC 

Dari survei diketahui 90,1% mahasiswa menggunakan 
windows 8 dan windows 10 sementara sisanya masih 
menggunakan windows 7. 

c.  Kualitas Jaringan  Internet Penunjang PJJ 

Dari  survei  diketahui  bahwa  90.7%  mahasiswa 
memiliki  jaringan  internet  yang  baik    sementara 
sisanya memiliki jaringan internet yang buruk 

d.  LMS  Yang  Digunakan  oleh  Mahasiswa  pada  Saat 

e. 

Ujian 
Dari  survei  98.7  %  mahasiswa  melaksanakan  ujian 
lewat  google  classroom,  sedangkan  sisanya  1.3% 
menggunakan google classroom dan TCExam 
Jenis Ujian Terstruktur yang paling sering dihadapi 
Dari Survei 49% mahasiswa menghadapi lebih banyak 
ujian  dengan  sistem  closed  book  sementara  21.9% 
menghadapi  lebih  banyak  ujian  dengan  sistem  open 
book sedangkan sisanya di 29.1% menghadapi jumlah 
ujian yang seimbang di antara keduanya 

f.  Output Jawaban Ujian TerStruktur yang dikumpulkan 
Dari  survei  97.4%  mahasiswa  masih  mengumpulkan 
hasil  ujian  berupa  pdf  dari  hasil  scan  tulisan  tangan 
sementara sisanya 2.6% langsung menginput jawaban 
pada LMS, 
g.  Kondisi Ujian 

pada  blok  pertanyaan  ini  terdapat  beberapa  hasil 
diantaranya  pada  saat  ujian  closed  book  berlangsung 
91,4%  mahasiswa  mengaku  bisa  membuka  dokumen 
dalam  bentuk  PDF/Pptx  sedangkan  sisanya  merasa 
  search 
tidak  bisa.  kemudian  untuk  mengakses 
engine/forum (google, reddit, dll)/website lainnya 88,1% 
mahasiswa  yang  disurvei  merasa  bisa  sedangkan 
sisanya  11.9%  merasa 
tidak  bisa  mengakses. 
Sementara  untuk  keadaan  Ujian  dengan  sistem  open 
book 90.7% mahasiswa merasa bisa untuk mengakses 
sosial  media(WA,  FB,IG)  selama  ujian  sementara 
sisanya tidak bisa 
h.  Potensi Kecurangan 

kecil 

sangat 

sedangkan 

Pada  blok  ini  terdapat  skala  likert  untuk  mengukur 
potensi kecurangan di ujian PJJ pada sistem open book 
maupun  closed  book  dengan  skala  1  –  5.  1 
mereferensikan 
5 
mereferensikan sangat besar. 
Untuk ujian bersistem open book didapat hasil sebagai 
berikut  
11 mahasiswa menjawab sangat kecil (1) = 11 x 1 = 11  
20 mahasiswa menjawab kecil (2) = 20 x 2 = 40 
42 mahasiswa menjawab netral (3) =  42 x 3 = 126 
42 mahasiswa menjawab besar (4) =  42 x 4 = 168 
36 mahasiswa menjawab sangat besar (5) = 36 x 5 =180 

 3 / 8 

 
 
 
 
 
Total skor = 525 
Y = skor tertinggi likert x jumlah panelis  
    = 5 x 151 = 755                  
Index% = total skor/ Y x 100 
            = (525/755) x 100 
             = 69.53%, kategori besar 
Dari  indeks  tersebut  diketahui  bahwa  untuk  sistem 
ujian selama PJJ bersistem open book terdapat potensi 
kecurangan yang besar. 

Untuk  ujian  bersistem  Closed  book  didapat  hasil 
sebagai berikut  
1 mahasiswa menjawab sangat kecil (1) = 1 x 1 = 1  
2 mahasiswa menjawab kecil (2) = 2 x 2 = 4 
36 mahasiswa menjawab netral (3) =  36 x 3 = 108 
60 mahasiswa menjawab besar (4) =  60 x 4 = 240 
39 mahasiswa menjawab sangat besar (5) = 39 x 5 =195 
Total skor = 548 
Y = skor tertinggi likert x jumlah panelis  
    = 5 x 151 = 755 

Index% = total skor/ Y x 100 
            = (548/755) x 100 
             = 72.58%, kategori besar 
Dari  indeks  tersebut  diketahui  bahwa  untuk  sistem 
ujian  selama  PJJ  bersistem  closed  book  terdapat 
potensi kecurangan yang besar. 

B. Pemilihan SEB (Safe Exam Browser) 

SEB  dipilih  karena  merupakan aplikasi  open  source/ 
gratis untuk dipakai, kemudian adapun Minimum Requirement 
dari SEB : 

  Os. Windows 7.1 ( menggunakan Safe exam 2.4.1) 
  Os. Windows 8.1 dan 10 ( menggunakan Safe Exam 

Browser 3.1) 

  Mac Os. ( Menggunakan Safe Exam Browser 2.3) 
Menilik  dari  data  OS  pc/laptop  mahasiswa  hasil  observasi, 
spesifikasi  yang  dibutuh  SEB  bisa  mencakup  sebagian  besar 
dari mahasiswa STIS 

C. Model Awal    

PJJ 

yang 

ujian 

besar 

ketika 

pada model ini peneliti membangun model rancangan 
ujian awal yang dibangun berdasarkan data yang didapat dari 
observasi  lapangan  melalui  survei  dimana  terdapat  potensi 
kecurangan 
tidak 
diamankan/diawasi  oleh  suatu  sistem.  Adapun  potensi 
kecurangan  tersebut  adalah  membuka  referensi  di  internet, 
membuka  referensi  materi  yang  diberikan  sebelumnya  pada 
ujian PJJ bersistem Closed book, sementara untuk Ujian open 
book  jenis kecurangan  yang  berpotensi  terjadi adalah  bekerja 
sama  antar  mahasiswa  dengan  fasilitas  social  media  seperti 
WhatsApp. Maka dari itu model awal akan mencoba mengatasi 
potensi kecurangan yang umum yaitu membuka referensi dari 
berkas materi atau website lainnya.  Model ini dan seterusnya 
akan  menggunakan  Googleclassroom 
sebagai  LMS 
collaborator  karena  berdasarkan  data  observasi  di  lapangan 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

LMS  ini  yang  paling  sering  dipakai.  Model  awal  kemudian 
diuji  cobakan  kepada  4 mahasiswa  yang  dari  jurusan  statistik 
yang  dianggap  sebagai  mahasiswa  awam  dan  4  mahasiswa 
jurusan KS yang dianggap sebagai ahli. Berikut adalah model 
rancangan awal :  

Gambar 2. Rancangan model prototype Ujian dengan SEB 

Hasil dari uji coba model prototype ini mendapat kesimpulan 
bahwa  mahasiswa  yang  masuk  ke  lingkungan  SEB  akan 
terkunci  dan  tidak  bisa  mengakses  desktop  selain  yang 
disediakan  di  dalam  lingkungan  SEB.  Mahasiswa  juga  tidak 
bisa  berpindah tab  ke  desktop  lainnya  menggunakan  shortcut 
(alt+tab)  maupun  (alt+f4)  untuk  memaksa  keluar  dari 
lingkungan SEB. Adapun beberapa kekurangan yang menjadi 
evaluasi dari mahasiswa yang melaksanakan uji coba yaitu :  

1.  Mahasiswa di lingkungan SEB masih bisa mengakses 
website  lain,  selain  LMS  tempat  soal  ujian  berada 
walaupun  tidak  bisa  mengetik/memasukan  URL 
tertentu dengan cara mengklik shortcut yang ada pada 
LMS. 

2.  Aplikasi  yang  dibuka  sebelum  membuka  file.SEB 
untuk  masuk  ke  lingkup  SEB  akan  tetap  hidup 
walaupun  tidak  bisa  diakses  dari  dalam  lingkungan 
SEB, pada kasus ini salah satu mahasiswa masih bisa 
mendengar  musik  dari  youtube  walaupun  tidak  bisa 
mengakses browser yang menyalakan youtube di luar 
lingkungan  SEB,  solusinya  pada  model  berikutnya 
pengaturan  keamanan  SEB  akan  ditambah 
lagi 
terutama  soal  pembatasan  website  dan  background 
process 

3.  Masih  terdapat  potensi  kecurangan  yaitu  dengan 
menggunakan  perangkat  lainnya  seperti  smartphone 
atau secondary pc/laptop maka dari itu solusinya akan 
ditambahkan  zoom  di  dalam  lingkungan  SEB  agar 
terdapat pengawas yang akan mengawasi mahasiswa.  

 4 / 8 

 
 
 
                   
 
 
 
 
 
 
 
 
D.  Model Lanjutan  

Pada  model  ini  peneliti  peneliti  membangun  model  yang 
disesuaikan  dengan  evaluasi  model  prototype  awal  dan  data 
observasi yang telah ada. Hasilnya adalah model berikut :  

Gambar 3. Rancangan Model Lanjutan Ujian dengan SEB 

Hasil  dari  model  lanjutan  ini  sudah  cukup  baik  dimana 
dilakukan penetrasi masalah yang dihadapi di model prototype 
seperti website lain yang masih bisa diakses, kini sudah diatasi 
dengan melakukan pengaturan pembatasan website/url apa saja 
yang  bisa  dibuka  selama  SEB terbuka  dan  juga  aplikasi  SEB 
akan  meminta  izin  untuk  mematikan  aplikasi  yang  terbuka 
sebelum memasuki lingkungan SEB agar aplikasi tersebut tidak 
berjalan  di  background.  Hal  ini  dilakukan  dengan  mengatur 
“prohibited  and  permitted  application”  pada  setting  SEB. 
Dalam  hasil  ini  mahasiswa  tidak  memiliki  kesempatan  untuk 
membuka aplikasi lain ataupun URL lain yang tidak diizinkan 
oleh SEB. Selanjutnya model ini juga sudah mengatasi potensi 
kecurangan apabila mahasiswa menggunakan smartphone atau 
saat  ujian,  yaitu  dengan 
secondary  devicenya  pada 
menambahkan 
aplikasi  SEB  yang 
memungkinkan  pengawas  ujian  bisa  difungsikan  layaknya 
ujian pada kondisi luring.  

zoom  di  dalam 

E. Penetration Testing 

Penetration 

testing  adalah  serangkaian  kegiatan  yang 
dilakukan  untuk  menguji  keamanan  dari  sebuah  sistem  atau 
model dengan menirukan serangkaian serangan yang mungkin 
terjadi. Penetration testing yang digunakan pada penelitian ini 
adalah metode HARM 

HARM  adalah  metode  untuk  memodelkan  ancaman  dan 
serangan  keamanan  yang  dikombinasikan  dengan  arsitektur 
sistem,  sehingga  dapat  lebih  memahami  potensi  serangan. 
HARM  menggabungkan  beberapa  format  spesifikasi  yang 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

berbeda  untuk  memberikan  pandangan  yang  komprehensif 
tentang kemungkinan serangan.  

Attack  Sequence  Descriptions  (ASD):  Ini  adalah  deskripsi 
bahasa  alami  sederhana  dari  serangan,  membentuk  urutan 
tindakan. Contoh ASD dapat berupa sesuatu seperti “(1) Mulai 
panggilan  komunikasi  via  skype/discord  dengan  kaki  tangan 
luar, dan jalankan di latar belakang. (2) Masuk ke tempat ujian 
dan  mulai  ujian  dengan  cara  biasa.  (3)  Komunikasikan 
pertanyaan kepada kaki tangan dan dapatkan jawaban kembali , 
menggunakan  lubang  suara  nirkabel  tersembunyi.  (4)  Ketik 
jawaban ke dalam sistem ujian elektronik dan kirimkan  

Misuse  Sequence  Diagrams  (MUSD):  Jika  lebih  memilih 
bentuk ekspresi yang lebih formal daripada bahasa alami ASD, 
urutan serupa dapat digambarkan sebagai MUSD. Diagram ini 
mirip dengan diagram urutan UML, tetapi selain objek yang sah 
dan  panggilan  pesan,  diagram  ini  juga  berisi  objek  yang 
menyerang  dan  panggilan  pesan  (memiliki  kotak  merah  dan 
panah merah). Diagram pada Gambar. 1 menunjukkan peserta 
ujian  yang  curang  menyiapkan  panggilan  Skype  dengan 
tangan/pembantu  sebelum  ujian  dimulai. 
seorang  kaki 
Kemudian  peserta  ujian  memulai  browser  penguncian  dan 
mengautentikasi dengan untuk mendapatkan kode akses untuk 
terhubung dengan server ujian. Melalui koneksi Skype, peserta 
ujian mengkomunikasikan pertanyaan kepada kaki tangan, dan 
kaki  tangan  menjawab  dengan  jawaban.  Oval  merah  putus-
putus  menunjukkan  kerentanan  yang  digunakan  untuk 
membuat serangan berhasil, dan labelnya dijelaskan di bawah 
diagram. 

Gambar 4. MUSD untuk panggilan yang terhubung di latar belakang 

Kerentanan (Vulnerabilities)  
V1  :  Browser  tidak  mendeteksi  aplikasi  komunikasi  di  latar   
belakang dan tidak mematikannya  
V2  :  Pengawas  ujian tidak memperhatikan/  tidak mengetahui 
terdapat  aplikasi  komunikasi  di 
latar  belakang  desktop 
mahasiswa  
V3  :  Aplikasi  ujian  tidak  mendeteksi  aplikasi  komunikasi 
belakang layar  
V4 : Browser tidak mendeteksi audio mahasiswa dalam ujian 
selama berkomunikasi untuk mencari jawaban  
V5 : Pengawas ujian tidak menemukan kecurangan  
V6 : Aplikasi ujian tidak menemukan kecurangan 

 5 / 8 

 
 
 
 
 
 
 
 
 
 
 
""Misuse Case Diagrams"" (MUD): MUD memperluas 
diagram  kasus  penggunaan  UML  untuk  menunjukkan 
seorang  yang  melakukan  penyalahgunaan 
bagaimana 
melakukan aktivitas rutin maupun tidak teratur dengan sistem. 
Gambar  5.  menunjukkan  MUD  untuk  ancaman  kecurangan 
yang  dipelajari  dalam  penelitian  ini.  Dibandingkan  dengan 
MUSD  dan  MUCM,  yang  menunjukkan  detail dari  satu  jenis 
serangan 
penyalahgunaan 
menunjukkan  gambaran  yang  lebih  luas.  Dalam  diagram 
tertentu  di  Gambar  5,  gambaran  umum  ini  bisa  dibuat  ekstra 
luas  dengan  menunjukkan  fungsi  dan  ancaman  khusus  untuk 
aplikasi  ujian  elektronik  (batas  sistem  dalam)  dan  ancaman 
curang  di  luar  (misalnya,  cara  curang  yang  lebih  tradisional 
dalam ujian kamar). 

tertentu, 

diagram 

kasus 

Gambar 5. Misuse case diagram pada ujian BYOD 

Attack Trees  (AT):  Ini  juga  menunjukkan  gambaran 
umum  dari  beberapa  ancaman.  Tidak  seperti  diagram  kasus 
penyalahgunaan,  yang  berfokus  pada  hubungan  antara 
ancaman  dan  perilaku  yang  sah,  pohon  penyerang  hanya 
berfokus  pada  perilaku  tidak  sah,  memecah  ancaman  tingkat 
tinggi  menjadi  yang 
lebih  detail.  Simpul  non-leaf 
didekomposisi  menjadi  pohon  simpul  konjungtif  (“cabang 
AND”)  dan  simpul  disjungtif  (“cabang  OR”).  Node  OR 
mewakili alternatif, sedangkan node AND mewakili sub tujuan 
di mana semua harus dipenuhi untuk mencapai tujuan. Dalam 
Gambar  6.  semua  cabang  adalah  cabang  OR,  menunjukkan 
berbagai  cara  untuk  melakukan  serangan  tingkat  tinggi 
""kecurangan selama ujian BYOD"". 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 6. Attack trees 

Kemudian dari attack trees dilakukanlah penetration 
test scenarios dari masing masing ancaman untuk mengetahui 
keamanan dari sistem BYOD  yang didesain dalam penelitian 
ini.  
       Tabel 1. Hasil dari  penetration test scenarios 

Serangan  

Hasil  

Deskripsi  

Menjalankan SEB di 
virtual machine  

Menjalankan SEB di 
remote computer  

Menggunakan 
Clipboard untuk 
mengkopi dan 
menyalin catatan ke 
web ujian  
Mendapat bantuan  
SEB di remote 
computer  
Mendapat 
bantuan 
melalui 
aplikasi 
komunikasi 
audio maupun 
video  
Mendapat bantuan 
melalui aplikasi share 
screen  
Menggunakan 
Secondary 
device untuk 
mendapat 
bantuan  

Gagal    Saat menjalankan SEB, muncul 

window berisi pernyataan bahwa 
SEB telah mendeteksi virtual 
machine dan SEB tidak akan 
terbuka  

Gagal    Saat Menjalankan SEB, Muncul window 
yang berisi pernyataan SEB telah 
mendeteksi aplikasi remote  
computer, dan aplikasi akan dimatikan 
paksa jika ingin membuka SEB  
Gagal    Tidak bisa menggunakan fitur CTRL+P  

dan klik kanan untuk menyalin catatan 
dan  

Gagal    Remote computer tidak bisa berjalan di 

background saat SEB dijalankan  

Gagal    Aplikasi komunikasi terdeteksi oleh SEB 
dan tidak bisa berjalan di background saat 
SEB dijalankan  

?  

Gagal   Aplikasi share screen terdeteksi oleh SEB 
dan tidak bisa berjalan di background saat 
SEB dijalankan  
Hasil dari penetrasi yang satu ini bias karena 
tergantung dari pengawas yang mengawasi 
dan pelaku kecurangan, jika pengawas 
kurang teliti dan pelaku kecurangan cerdik , 
serangan ini bisa berhasil, di lain sisi jika 
pelaku kecurangan kurang cerdik dan 
pengawasnya teliti maka serangan ini bisa 
gagal  

 6 / 8 

 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Hasilnya  untuk  skenario  yang  berhubungan  dengan 
sistem  dan  aplikasi  di  desktop  semuanya  gagal  karena  pada 
desain  ujian  ini  SEB  sebagai  lockdown  browser  telah  diatur 
sedemikian  rupa  untuk  mengantisipasi  kecurangan,  adapun 
kecurangan  dalam  sistem  bisa 
terjadi  apabila  pelaku 
kecurangan menemukan aplikasi yang belum di block oleh SEB. 
Sedangkan  untuk  skenario  di  luar  sistem  yaitu  menggunakan 
secondary  device  hasilnya  bias  tergantung  pengawas  dan 
pelaku kecurangan.  

mahasiswa  akan  melakukan  pemeriksaan,  absensi 
dan  diawasi  oleh  pengawas  ujian  selama  ujian 
berlangsung 

5.  Untuk  masuk  ke  zoom  mahasiswa  cukup  menekan 
ikon zoom yang terdapat di bagian pojok bawah kiri 
pada halaman utama di dalam lingkungan safe exam 
browser,  dengan  mengklik  ikon  tersebut  mahasiswa 
akan  langsung  diarahkan  ke  ruang  zoom  sesuai 
dengan kelas dan mata kuliah ujian 

F. Cara menggunakan Safe Exam browser untuk ujian 

Untuk  menggunakan  SEB  sebagai  media  ujian, 
Mahasiswa  terlebih  dahulu  harus  mendapatkan  file  dengan 
extensi  .seb  seperti  di  gambar  yang  kemudian  hanya  bisa 
dibuka  menggunakan  aplikasi  SEB  dan  hanya  apabila 
mahasiswa memiliki password yang diberikan oleh admin 

Gambar 7. contoh file dengan extensi .seb 

Gambar 8. Jendela memasukan password ke SEB  

Kemudian  setelah  itu  mahasiswa  akan  memasuki  lingkungan 
SEB  untuk  memulai  ujian  sesuai  dengan  instruksi  dari 
pengawas ujian 

G. Skema Ujian                                                                 

1.  Mahasiswa mendownload File (.seb) yang diberikan 

admin ujian 

2.  Pada  waktu  ujian  berlangsung mahasiswa membuka 
file tersebut dengan memasukan password yang akan 
diberikan admin 15 menit sebelum ujian berlangsung 
3.  Di dalam lingkungan SEB Mahasiswa akan diarahkan 
untuk  login  ke  Google  classroom  dan  kemudian 
masuk ke dalam soal ujian yang akan di post dalam 
assignment google classroom oleh admin 

4.  Untuk  masuk  ke  dalam  soal  ujian  Mahasiswa 
memerlukan  kode  yang  akan  diberikan  oleh 
pengawas  ujian  di  dalam  zoom,  maka  dari  itu 
mahasiswa  harus  masuk  ke  zoom,  di  dalam  zoom 

Gambar 9. Halaman utama Safe Exam Browser dengan zoom 

6.  Setelah  mendapat  kode  ujian  ,  mahasiswa  bisa 
langsung  mengerjakan  ujian  sampai  waktu  yang 
berlaku  setelah  ujian  selesai  mahasiswa  bisa  keluar 
dari zoom setelah memasukan password keluar yang 
akan diberikan oleh pengawas di akhir ujian.    

H. Evaluasi Pengujian Model Ujian SEB pada Ujian Jarak 
Jauh 

Pengujian  model  ini  dilakukan  pada  saat  ujian 
tambahan mata kuliah Interaksi Manusia dan Komputer kepada 
66 Mahasiswa. Ujian ini menggunakan TCExam sebagai LMS 
ujian.  Sebelum  ujian  dimulai  sebelumnya  peserta 
telah 
diberikan  quick  guide  untuk  menggunakan  SEB.  Kemudian 
Mahasiswa diberikan angket untuk mengevaluasi model ujian. 
Berikut adalah hasil dari angket : 

a.  Kondisi Ujian 

Pada  blok  pertanyaan  ini  terdapat  beberapa  hasil 
diantaranya  90%  Mahasiswa  tidak  bisa  mengakses 
file  materi  berupa 
referensi  dengan  membuka 
pdf/word/pptx  pada  saat  ujian  sementara  sisanya 
menjawab tidak tau karena tidak mencoba hal tersebut, 
kemudian  untuk  mengakses  search  engine  atau  web 

 7 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
lainnya  pada  saat  ujian  92.5%  mahasiswa  tidak  bisa 
sedangkan  sisanya  tidak  tau  karena  tidak  mencoba  hal 
tersebut.kemudian  untuk  mengakses  perangkat  lainnya 
untuk  mencari  referensi  80%  mahasiswa  merasa  tidak 
bisa sedangkan sisanya merasa bisa mengakses dengan 
alasan pengawasan dari zoom yang kurang ketat 
 Potensi Kecurangan 

Pada  blok  ini  terdapat  skala  likert  untuk  mengukur 
potensi kecurangan di ujian PJJ pada sistem open book 
maupun closed book dengan skala 1 – 5. 1 merefrensikan 
sangat kecil sedangkan 5 mereferensikan sangat besar. 

Untuk  ujian  bersistem  open  book  didapat  hasil 

b. 

sebagai berikut  
4 mahasiswa menjawab sangat kecil (1) = 11 x 1 = 4  
18 mahasiswa menjawab kecil (2) = 18 x 2 = 36 
      16 mahasiswa menjawab netral (3) =  19 x 3 = 48 
       1 mahasiswa menjawab besar (4) =  1 x 4 = 4 

1 mahasiswa menjawab sangat besar (5) = 1 x 5 = 5 

       Total skor = 97 

Y = skor tertinggi likert x jumlah panelis  = 5 x 40 = 200                   

Index% = total skor/ Y x 100 
            = (97/200) x 100 
             = 48.5%, kategori netral 

Dari  indeks  tersebut  diketahui  bahwa  untuk  sistem 
ujian  selama  PJJ  bersistem  open  book  terdapat 
potensi kecurangan yang netral/sedang. 

c.  Evaluasi Proses  

Pada  blok  ini  kembali  terdapat  beberapa  pertanyaan 
untuk  mengevaluasi  model  ujian  dengan  SEB  dengan 
keterangan STS : Sangat Tidak Setuju, TS : Tidak Setuju 
RG : Ragu Ragu, S : Setuju , SS : Sangat Setuju 

STS 
2.5% 

TS 
0% 

S 
37,5% 

SS 
40% 

RG 
7.5
% 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Tabel 2. Persentase jawaban Mahasiswa pada Evaluasi Model Ujian 

SEB Dengan Indikator Hasil Yang Diharapkan 

Hasilnya  sebagian  besar  mahasiswa  setuju  atas  indikator 
indikator yang dipaparkan pada pertanyaan 

VII. 

PENUTUP 

Dari hasil  penelitian  ini menghasilkan  model  ujian  BYOD 
yang aman dan bisa direkomendasikan untuk ujian BYOD di 
STIS  dengan  beberapa  catatan  diantaranya  adalah  sebagai 
berikut : 
a. 

Jenis  Ujian  yang  digunakan  adalah  ujian  yang 
sifatnya  teoritis/  hanya  berupa  essay/  pilihan 
ganda sehingga memudahkan input jawaban 
b.  Untuk ujian BYOD yang sifatnya jarak jauh(UJJ) 
perlu  merancang  regulasi  terkait  pengawasan 
lewat 
seperti  posisi  kamera  dan 
pemeriksaan sebelum ujian  

zoom 

c.  Model Ujian BYOD bisa dikombinasikan dengan 
LMS  apapun  termasuk  LMS  dari  STIS  sendiri 
yaitu  TCEXAM  maupun  ujj.stis.ac.id,  namun 
untuk  BYOD  yang  sifatnya  jarak  jauh  lebih 
direkomendasikan  menggunakan  LMS  moodle 
karena  terdapat  plugin  SEB  yang  memudahkan 
dan  memitigasi  kecurangan  yang  kurang  bisa 
dipantau  karena  diawasi  lewat  zoom,  sehingga 
lebih efektif. 

DAFTAR PUSTAKA 

[1]  Aji, R. H. , “Dampak Covid-19 Pada Pendidikan Di Indonesia:  Sekolah, 

Keterampilan, Dan Proses Pembelajaran”, Vol 7. 2020  

[2]  Perdana, N. R. “Memberdayakan Pendidikan Di Masa Pandemi COVID-
19 (Studi Kasus Refleksi Pembelajaran Jarak Jauh Oleh Para Pelajar)”. 
2020  

[3]  Ellis, Ryann K.  “Field Guide to Learning Management Systems”. 2009 
[4]  Moodle [Online]. Available : Https://Moodle.org 
[5]  Google Classroom [Online] : Available : Https://classroom.google.com 
[6]  Kahoot [Online]. Available : Https://Kahoot.it 
[7]  Quiziz [Online]. Available : Https://Quiziz.com 
[8]  Safe 

Available 

[Online]. 

Browser 

Exam 

: 

No 
1 

2 

3 

4 

Pertanyaan  

Model Ujian 
Dengan SEB 
meningkatkan 
keamanan ujian 
online 
Model Ujian 
Online Dengan 
SEB 
memudahkan 
input ujian online 
dibandingkan 
dengan ujian 
online tulis 
tangan yang 
discan dengan 
PDF 
Model Ujian 
Online Dengan 
SEB memitigasi 
kecurangan 
dalam ujian 
Online 
Model Ujian 
Online Dengan 
SEB Cocok 
digunakan 
sebagai ujian 
yang aman di 
STIS 

2.5% 

12.5
% 

17,
5% 

47,5% 

30% 

Https://safeexambrowser.org/about_overview_en.htm. 

[9]  Politeknik Statistika STIS. “Panduan Ujian Jarak Jauh Politeknik Statistika 

STIS”,  2020 

[10]  Truszkowski,  D.,  “Proctored  Versus  Non-Proctored  Testing:  A   

 Study  for Online Classes”, Dissertation of the Doctoral Program of the 
American College of Education . 2019 

0 

2.5
% 

20
% 

47,5% 

30% 

7.5% 

7.5
% 

22.
5% 

45% 

17, 
5 % 

[11]  Halbher, T., Reuter, K., Schneider, D., Schlienger, C., &amp; Piendl, T. 
“Making  Examinations  More  Valid,  Meaningful  And  Motivating:  The 
Online Exams Service At Eth Zurich”. 2016 
[12] Zoom [Online]. Available : Https://Zoom.us 
[13]  Google Meet [Online]. Available : Https://meet.google.com 
[14] Force 

Available 

[Online]. 

Point 

:  

Https://www.forcepoint.com/cyberedu/bring-your-own-device-byod 
[15]  Panyahuti,  Ganefri,  Ambiyar,  &  Suryani,  K.  “SAFE  EXAM  BROWSER 

UNTUK KLIEN ANDROID PADA UJIAN BERBASIS WEB”, 2019 
[16]  Søgaard,T. M. “Mitigation of Cheating Threats in Digital BYOD exams”, 

June 2016 

[17] Vegendla,  A.,  Sindre, G.,  & Søgaard, T. M., “Extending Harm to  make 

Test Casesfor Penetration Testing”., 2016 

[18] Haus,  G.,  Pasquinelli,  Y.,  Scaccia,  D.  and  Scarabottolo,  N.,  “ONLINE 

WRITTEN EXAMS DURING COVID-19 CRISIS”., 2020 

 8 / 8 

 
 
 
 
 
 
 
 
"
221710035," Analisis Perbandingan Metode Imputasi untuk 
Penanganan Missing Value 

Tsasya Raudhatunnisa1, Nori Wilantika, S.S.T., M.T.I.2 

1IVSD2/221710035 
Jurusan Komputasi Statistik 
Peminatan Sains Data  
e-mail: *1221710035@stis.ac.id, 2wilantika@stis.ac.id  

Abstrak 

Penelitian  ini  membandingkan  tiga  buah  metode  imputasi  untuk  penanganan  missing 
value,  yaitu  Hot-Deck      Imputation,  K-Nearest  Neighbor  Imputation  (KNNI),  dan  Predictive 
Mean  Matching  (PMM).  Perbedaan  cara  kerja  ketiga  metode  yang  menyebabkan  hasil 
estimasinya  pun 
akan  berbeda.  Perbandingan  ketiga  metode  dilakukan  dengan 
mempertimbangkan  Root  Mean  Squared  Error  (RMSE),  Unsupervised  Classification  Error 
(UCE), Supervised Classification Error (SCE), dan waktu yang digunakan untuk menajalankan 
algoritma. Penelitian ini menggunakan dua buah analisis, yaitu analisis perbandingan dan analisis 
skoring.  Analisis  perbandingan  dilakukan  dengan  menerapkan  simulasi  yang  memperhatikan 
mekanisme  missing  value.  Mekanisme  missing  value  yang  digunakan  dalam  simulasi  adalah 
Missing  Completely  at  Random  (MCAR),  Missing  at  Random  (MAR),  dan  Missing  Not  at 
Random (MNAR). Lalu untuk analisis skoring bertujuan untuk mengerucutkan hasil dari analisis 
perbandingan  dengan  memberikan  skor  pada  hasil  imputasi  dari  ketiga  metode.  Berdasarkan 
tahapan penelitian yang telah dilakukan didapatkan bahwa metode Hot-Deck merupakan metode 
yang  memiliki  skor  tertinggi  di  setiap  mekanisme,  sehingga  metode  Hot-Deck  Imputation 
merupakan metode yang paling baik dalam menangani missing value. 

Kata Kunci— Missing value, Hot-Deck Imputation, KNNI, PMM, perbandingan 

Abstract 

 This study compares three imputation methods for handling missing values, Hot-Deck 
Imputation, K-Nearest Neighbor Imputation (KNNI), and Predictive Mean Matching (PMM). The 
difference  in  the  way  the  methods  work  will  cause  the  estimation  results  to  be  different.  The 
comparison of the  three  methods  is  carried out  by considering the  Root  Mean Squared  Error 
(RMSE), Unsupervised Classification Error (UCE), Supervised Classification Error (SCE), and 
the time used to run the algorithm. This study uses two analyzes, comparative analysis and scoring 
analysis. Comparative analysis is carried out by applying simulations that pay attention to the 
missing  value  mechanism.  Missing  value  mechanisms  used  in  the  simulation  are  Missing 
Completely  at  Random  (MCAR),  Missing  at  Random  (MAR),  and  Missing  Not  at  Random 
(MNAR). Then for the scoring analysis, it aims to narrow the results of the comparative analysis 
by giving a score to the imputation results of the three methods. Based on the research stages that 
have been carried out, it was found that the Hot-Deck method is the method that has the highest 
score in each mechanism, so the Hot-Deck Imputation method is the best method in dealing with 
missing values. 

Keywords— Comparison, Missing value, Hot-Deck Imputation, KNNI, PMM 

1 

 
 
 
    
 
 
 
 
 
 
 
 
 
 
 
 
   ◼ 

       ISSN: 1978-1520 

1. PENDAHULUAN 

Dalam  proses  produksi  data,  sebelum  melakukan  analisis  lanjutan,  sebuah  dataset  perlu 
melewati  preprocessing  terlebih  dahulu.  Salah  satu  proses  yang  cukup  penting  dalam 
preprocessing  data  adalah  pembersihan  data.  Proses  pembersihan  data  dilakukan  untuk 
mengurangi dampak dari data tidak memenuhi syarat kenormalan data, seperti ketidakkonsistenan 
nilai,  noise,  dan  ketidaklengkapan  data.  Salah  satu  topik  yang  cukup  banyak  diangkat  dalam 
pembersihan data adalah missing value [1]. Missing value dapat dikatakan sebagai nilai yang tidak 
tersedia untuk sebuah obyek [2]. Missing value terbagi berdasarkan pola dan tipe missing-nya [3]. 
Adapun missing value berdasarkan tipenya terbagi menjadi tiga, yaitu: 
1.  Missing at Random (MAR)  

MAR    adalah  mekanisme data  hilang terdistribusi  secara  acak  untuk  sebagian  unit 
observasi.  Dengan  kata  lain, MAR  berarti  terjadinya missing  data hanya  berkaitan  dengan 
variabel respon/pengamatan.  

2.  Missing Completely at Random (MCAR)  

MCAR  adalah  mekanisme data  hilang yang  terdistribusi  secara  acak  untuk  seluruh  unit 
observasi.  Dengan  kata  lain  MCAR  berarti  bahwa  terjadinya missing  data tidak  berkaitan 
dengan nilai semua variabel, apakah itu variabel dengan missing values atau dengan variabel 
pengamatan. Hal ini berarti missing data terjadi secara acak. 

3.  Missing Not at Random (MNAR) 

MNAR  mekanisme data  hilang yang  tidak  terdistribusi  secara  random.  Dengan  kata 
lain, Missingness  Is  Non-Ignorable bahwa  terjadinya missing  data pada  suatu  variabel 
berkaitan dengan variabel itu sendiri, sehingga ini tidak bisa diprediksi dari variabel lain pada 
suatu dataset. 
Missing  value pada  dasarnya  tidak  akan  bermasalah  bagi  keseluruhan  data,  apalagi  jika 
jumlahnya hanya sedikit, misalnya hanya 1 % dari seluruh data. Akan tetapi, dalam kenyataan 
yang terjadi di lapangan adalah missing value pada data memiliki persentase yang cukup besar. 
Fenomena  missing  value  banyak  dijumpai  dalam  survei.  Non-response  merupakan  salah  satu 
alasan terbesar munculnya missing value pada dataset [4]. Akibat yang ditimbulkan dari missing 
value adalah kesimpulan penelitian akan menjadi bias dan tidak merepresentasikan keadaan yang 
sebenarnya.  Banyak  sekali  faktor  yang  mendukung  adanya  missing  value  pada  suatu  dataset 
seperti sulitnya bertemu dengan responden, data yang tidak tercatat oleh petugas, kesalahan pada 
aplikasi  atau  peralatan  yang  digunakan,  dan  lain  sebagainya  [5].  Missing  value  dapat  muncul 
sebagai outlier atau nilai yang tidak konsisten dari nilai sebelumnya, ataupun isian yang tidak 
wajar pada data [1]. Beberapa masalah terkait dengan adanya missing values, mulai dari hilangnya 
efisiensi, komplikasi dalam menangani dan menganalisis data, ataupun masalah adanya bias yang 
dihasilkan antara data yang mengandung missing value dengan data lengkap [6]. 

Untuk  menghindari  masalah  yang  ditimbulkan  dari  missing  value,  maka  peneliti  biasanya 
menggunakan  beberapa  metode  untuk  menangani  missing  value,  antara  lain  listwise  deletion, 
pairwise  deletion  dan  imputasi  [7].  Listwise  deletion  adalah  menghapus  kasus  (obyek)  yang 
mengandung  missing  data  secara  keseluruhan  [5].  Gambaran  mudahnya  adalah  jika  pada  unit 
observasi A ada variabel yang mengalami missing value, maka unit observasi A akan dihapus dari 
dataset. Pairwise deletion adalah menghapus missing data, sehingga yang dianalisis hanya nilai-
nilai  yang  tersedia  saja  [5].  Berbeda  dengan  Listwise  deletion,  Pairwise  deletion  hanya  akan 
menghapus variabel yang mengalami missing value pada satu unit observasi sehingga yang tersisa 
dari  unit  tersebut  hanyalah  variabel  yang  memiliki  nilai  saja.  Lalu  Imputasi  adalah  mengisi 
missing value dengan nilai yang mungkin berdasarkan informasi yang tersedia pada data.  Dari 
ketiga metode tersebut, metode imputasi merupakan metode paling baik yang dapat digunakan 
untuk  mengatasi  missing  value  dibandingkan  dengan  kedua  metode  lainnya  [8].  Penggunaan 
metode  listwise  deletion  dan  metode  pairwise  deletion  memungkinkan  untuk  menghilangkan 
banyak unit variabel. Jika unit observasi tidak memnuhi jumlah sampel yang diperlukan, maka 
yang  bisa  dilakukan  adalah  sengan  mealkukan  sampling  ulang  agar  jumlah  unit  observasi 
mencukupi  jumlah  sampel  yang  diperlukan.  Tentunya  hal  ini  akan  membutuhkan  waktu  yang 

2 

  
 
 
lama  serta  biaya  yang  yang  tidak  sedikit.  Maka  dari  itu  sangat  tidak  dianjurkan  untuk 
menggunakan  metode  ini  apalagi  untuk  survei  yang  cukup  besar  jumlah  sampelnya.  Metode 
imputasi mampu untuk mengatasi permasalahan pada kedua metode karena mengurangi resiko 
resampling dengan mengestimasi nilai missing value dengan menggunakan unit observasi yang 
tidak mengalami missing value [9]. 

Metode imputasi dibagi kedalam dua jenis, yaitu metode imputasi berbasis statistik dan metode 
imputasi berbasis machine learning [10]. Teknik Imputasi statistikal merupakan Teknik imputasi 
dengan menggunakan kaidah – kaidah statistik dalam imputasinya. Sedangkan teknik imputasi 
machine learning adalah teknik imputasi yang memanfaatkan training pada data yang nantinya 
akan digunakan untuk memprediksi nilai yang akan diimputasi. [5] menyatakan bahwa metode 
imputasi  secara  garis  besar  dibagi  menjadi  dua  jenis,  yaitu  metode  imputasi  tunggal  (single 
imputation  method)  dan metode imputasi  ganda  (multiple  imputation  method).  Perbedaan  dari 
kedua  metode  imputasi  tersebut  terletak  pada  pemanfaatan  data  training  untuk  mengimputasi 
missing  value.  Pemilihan  metode  untuk  penanganan  missing  value  menjadi  hal  yang  cukup 
penting karena akan mempengaruhi nilai estimasi yang dihasilkan. Selain itu, seringkali pengguna 
mengalami kesulitan dalam menentukan metode penanganan missing value yang tepat untuk data 
mereka  [3].  Maka  dari  itu,  pada  penelitian  ini  akan  dibandingkan  beberapa  metode  imputasi, 
antara  lain  Hot-Deck  Imputation  dan  K-Nearest  Neighbor  Imputation  (KNNI),  dan  Predictive 
Mean  Matching  (PMM).  Metode  Hot-Deck  Imputation  adalah  metode  yang  digunakan  untuk 
mengatasi  missing  value  dengan  mengimputasi  nilai  yang  diambil  dari  unit  observasi  yang 
lengkap yang dianggap memiliki kesamaan dengan unit observasi yang mengalami missing value 
[11]. Pemilihan metode Hot-Deck Imputation karena metode ini cocok digunakan pada banyak 
jenis data dan dapat dilakukan unttuk imputasi berbagai tipe data [4]. Sedangkan metode KNNI 
menyediakan metode yang lebih kuat dan sensitif [4]. Metode KNNI sendiri merupakan metode 
imputasi  yang  memanfaatkan  algoritma  K-Nearest  Neighbor  (KNN).    Algoritma  KNN 
merupakan  algoritma  pengelompokan  data  non  hirarki,  jumlah  kelompok  yang  akan  dibentuk 
sudah  terlebih  dahulu  diketahui  dan  ditentukan  jumlahnya  [12].  Selain  itu,  metode  ini  tidak 
menggunakan  asumsi  apapun,  tidak  diperlukan  pembentukan  model  prediksi,  serta  dapat 
mengatasi missing value baik pada data numerik maupun kategorik. Pemilihan kedua metode ini 
didasari  oleh  penelitian  mengatakan  bahwa  metode  Hot-Deck  Imputation  merupakan  metode 
terbaik dalam metode imputasi berbasis statistik karena metode ini cocok digunakan pada banyak 
jenis data dan dapat dilakukan unttuk imputasi berbagai tipe data [4]. Sedangkan untuk metode 
imputasi berbasis machine learning terbaik, dipegang oleh metode KNNI [12]. 

Meskipun kedua metode tersebut merupakan metode yang dinilai sangat baik dalam menangani 
missing value, akan tetapi kedua metode tersebut memiliki algoritma yang cukup rumit. Hal ini 
yang menjadi alasan kedua metode ini cukup jarang digunakan [13]. Dalam pencarian sederhana 
di Google dengan kata kunci “missing data” ditemukan bahwa metode Predictive Mean Matching 
(PMM)  memiliki  lebih  dari  21.000  hasil  dari  keseluruhan  hasil  yang  ada  [13].  Angka  ini 
merupakan  angka  tertinggi  jika  dibandingkan  dengan  KNNI  dan  Hot-Deck  Imputation  yang 
hanya  memiliki  13.000  hasil  dan  4.300  hasil.  Hal  ini  menunjukkan  bahwa  metode  PMM 
merupakan metode yang cukup sering digunakan meskipun metode ini menunjukkan performa 
yang lebih rendah dibandingkan dengan kedua metode lainnya. 

Untuk  itu  peneliti  tertarik  untuk  membandingkan  ketiga  metode  imputasi  tersebut  dengan 
mengaplikasikannya pada beberapa data. Ketiga metode tersebut akan dibandingkan berdasarkan 
skor yang didapatkan dari beberapa kriteria utama yang telah ditentukan. Secara khusus penelitian 
ini dilakukan untuk mengetahui kelebihan dan kekurangan pada metode  Hot-Deck Imputation, 
KNNI,  dan  PMM  serta  menentukan  metode  yang  terbaik  berdasarkan  skor  tertinggi  dalam 
menangani missing value 

3 

 
 
 
 
 
 
 
   ◼ 

       ISSN: 1978-1520 

2. METODOLOGI 

2.1  Simulasi Data 

Dalam penelitian ini, tahap analisis data diawali dengan menerapkan simulasi pada dataset. 
Dataset awal yang masih lengkap akan dibentuk menjadi dataset yang memiliki missing value. 
Adapun dataset yang digunakan dalam penelitian ini, adalah: 
1.  Subset dataset kecil dari data SUSENAS KOR Maret Tahun 2019. Dataset ini merupakan 
subset dari dataset SUSENAS KOR Maret 2019 yang telah disampling sebanyak 600 unit 
observasi. 

2.  Subset dataset besar dari data SUSENAS KOR Maret Tahun 2019. Dataset ini merupakan 
subset dari dataset SUSENAS KOR Maret 2019 yang telah disampling sebanyak 9000 unit 
observasi. 

3.  Dataset Iris. Dataset ini tidak mengalami proses sampling dan merupakan dataset kecil. 
4.  Dataset E.Coli. Dataset ini tidak mengalami proses sampling dan merupakan dataset kecil. 
5.  Dataset Breast Cancer I. Dataset ini tidak mengalami proses sampling dan merupakan 

dataset besar. 

6.  Dataset Breast Cancer II. Dataset ini tidak mengalami proses sampling dan merupakan 

dataset besar. 
Pembentukan missing value akan menerapkan semua mekanisme missing value, yaitu Missing 
Completely  At  Random  (MCAR),  Missing  At  Random  (MAR),  dan  Missing  Not  At  Random 
(MNAR) pada sepuluh tingkatan missing value dengan interval 5% dimulai dari missing value 
sebanyak 5% hingga missing value sebanyak 50%. Setelah dataset ini terbentuk tahap selanjutnya 
adalah mengimputasi dataset ini dengan ketiga metode yang terpilih, yaitu Hot-Deck   Imputation, 
KNNI, dan PMM. Setalah proses imputasi selesai, setiap metode pada setiap tingkatan missing 
value  dan  setiap  dataset  akan  dihitung  nilai  dari  kriteria  –  kriteria  yang  ada  pada  analisis 
perbandingan.  Nilai  –  nilai  kriteria  yang  didapatkan  dari  proses  simulasi  ini  akan  dicatat  dan 
diinout kedalam tabel pencatatan. 

2.2  Analisis Perbandingan 

Pada proses ini akan dihitung nilai rata – rata dari keempat kriteria yang akan digunakan dalam 
analisis perbandingan. Kriteria yang digunakan dalam analisis perbandingan pada penelitian ini, 
adalajh: 
1.  Root Mean Squared Error (RMSE) 

Pengertian Root Mean Square Error (RMSE) adalah metode pengukuran dengan mengukur 

perbedaan nilai dari prediksi sebuah model sebagai estimasi atas nilai yang diobservasi.  
2.  Unsupervised Classification Error (UCE) 

Unsupervised Classification Error (UCE) mengukur seberapa baik pengelompokan data 
yang telah diimputasi dengan data yang lengkap. Pendekatan yang digunakan untuk UCE adalah 
Hierarchical Clustering dengan korelasi 𝑑 = 1 − 𝑝𝑒𝑎𝑟𝑠𝑜𝑛 sebagai jarak dan agregasi Ward.  
3.  Supervised Classification Error (SCE) 

Supervised Classification Error (SCE) menilai kekuatan diskriminatif atau daya prediksi 

metode imputasi dengan mengukur perbedaan antara subkelompok yang diprediksi setelah 
imputasi data yang hilang dan subkelompok yang sebenarnya. Pendekatan yang digunakan 
untuk SCE adalah Linear Discriminant Analysis (LDA) pada sekumpulan variabel yang dipilih 
secara apriori pada setiap kumpulan data referensi tanpa nilai yang hilang.  
4.  Waktu yang Digunakan untuk Mengeksekusi Algortima 

Waktu yang digunakan untuk mengeksekusi algoritma adalah perhitungan waktu yang 
digunakan  dari  mulai  dimasukkan  data  ke  dalam  aplikasi  RStudio  hingga  algoritma  tersebut 
selesai dijalankan. 

4 

  
 
 
 
 
 
 
2.3 Tahapan Penelitian 

Tahapan  diawali  dengan  proses  pengolahan  dataset  SUSENAS  KOR  Maret  2019.  Dataset 
SUSENAS terlebih dahulu akan melalui proses manipulasi sebelum masuk pada proses sampling. 
Nantinya  dataset  SUSENAS  yang  sudah  melalui  proses  manipulasi  akan  diambil  sampelnya 
sebanyak 600 dan 9000 unit observasi.  

Gambar 1. Tahapan Penelitian 

Selanjutnya, seperti pada Gambar 1, dilakukan proses pembentukan dataset dengan missing 
value.  Pada  proses ini,  keenam dataset  dibentuk  missing  value-nya  sebanyak  5%  hingga  nanti 
berakhir pada missing value sebanyak 50%. Pembentukan missing value ini akan memperhatikan 
mekanisme missing value-nya. Sehingga, hasil yang didapatkan dari proses generalisasi missing 
value  adalah  dataset  yang  memiliki  missing  value  yang  ada  pada  rentang  5%  -  50%  dengan 
mekanisme MCAR, MNAR, dan MNAR. 

Dataset yang telah memiliki missing value ini diimputasi dengan menggunakan ketiga metode 
imputasi,  yaitu  Hot-Deck      Imputation,  KNNI,  dan  PMM.  Selanjutnya  akan  dinilai  performa 
ketiga metode imputasi tersebut dengan menggunakan nilai RMSE, UCE, SCE, dan waktu yang 
digunakan. Nilai – nilai keriteria yang didapatkan akan dimasukkan ke dalam tabel pencatatan 
dan diringkas berdasarkan rata – ratanya dengan menggunakan tabel perbandingan. Penggunaan 
tabel perbandingan bertujuan agar proses analisis menjadi lebih mudah. Setelah tabel rata – rata 
hasil  imputasi  didapatkan,  maka  yang  terakhir  adalah  proses  skoring.  Pada  proses  ini  akan 
diwujudkan  tujuan  dari  penelitian  ini,  yaitu  mendapatkan  metode  terbaik.  Skoring  dilakukan 
berdasarkan nilai kuantil dari tabel rata – rata hasil imputasi. 

5 

 
 
 
 
 
 
 
   ◼ 

       ISSN: 1978-1520 

3. HASIL DAN PEMBAHASAN 

Analisis  perbandingan  dilakukan  untuk  membandingkan  metode  Hot-Deck      Imputation, 
metode KNNI, dan metode PMM. Untuk mempermudah proses analisis, maka nilai – nilai yang 
didapatkan dari simulasi yang dilakukan pada masing – masing dataset akan dirata – ratakan dan 
digabungkan ke dalam tabel perbandingan seperti pada Tabel I, Tabel II, dan Tabel III. Tabel - 
tabel  tersebut  berisikan  nilai  yang  didapatkan  dari  kriteria  –  kriteria  perbandingan  yang  telah 
ditentukan dengan menggunakan mekanisme MCAR, mekanisme MAR, dan mekanisme MNAR. 
Tabel I merupakan tabel rata  – rata hasil imputasi dengan mekanisme MCAR. Mekanisme 
MCAR adalah keadaan dimana missing value terjadi secara acak. Sehingga pengguna tidak dapat 
memprediksi di variabel apa atau bahkan di unit observasi mana missing value akan terjadi.  

TABEL I 
RINGKASAN RATA – RATA HASIL IMPUTASI DENGAN MEKANISME MCAR 

Metode 

Hot-Deck   Imputation 

KNNI 

PMM 

RMSE 

33.098 
33.199 

UCE 
2.710 
2.789 

3.535 
3.564 
3.812 
3.766 

32.249 
33.298 
15.233 
15.984 

SCE  Waktu 
Dataset 
0.016 
2.222 
1 
0.015 
2 
2.019 
0.270 
3  1479681.356  36.246  20.344 
0.450 
4  1456234.198  36.330  24.665 
0.019 
2.361 
5 
0.019 
2.431 
6 
0.033 
2.430 
1 
0.034 
2 
2.653 
0.222 
3  1278961.190  27.634  21.444 
9.360 
926501.660  21.093  23.987 
4 
0.168 
1.798 
5 
0.210 
2.356 
6 
0.008 
3.221 
1 
0.010 
2 
3.199 
0.156 
3  1567256.146  29.345  25.667 
0.256 
4  1432115.800  30.998  24.109 
0.018 
2.567 
5 
0.020 
2.439 
6 

9.667 
9.699 
45.120 
45.361 

3.210 
3.246 
4.015 
4.015 

39.723 
39.500 

3.850 
3.856 

Selanjutnya adalah mekanisme missing value yang cukup sering kita temukan di kehidupan 
sehari – hari. Mekanisme MAR biasanya terjadi karena non-response pada suatu survei. Untuk 
hasil  perbandingan  metode  Hot-Deck  Imputation,  metode  KNNI,  dan  metode  PMM  pada 
mekanisme ini terlampir pada Tabel II berikut. 

TABEL II 
RINGKASAN RATA – RATA HASIL IMPUTASI DENGAN MEKANISME MAR 

Metode 

Hot-Deck   
Imputation 

KNNI 

Dataset  RMSE 

32.910 
33.017 

UCE 
3.583 
3.577 

SCE  Waktu 
0.016 
1 
2.094 
0.017 
2.142 
2 
0.290 
3  1476738.300  27.099  19.868 
0.490 
4  1447584.899  27.492  23.531 
0.019 
1.843 
5 
0.019 
2.574 
6 
0.051 
2.130 
1 
0.054 
2 
2.901 
0.250 
3  1257838.932  26.238  20.530 
9.950 
924856.834  20.935  23.122 
4 
0.199 
1.209 
5 
0.300 
2.356 
6 

32.141 
32.290 
14.763 
14.709 

3.512 
3.528 
3.620 
3.637 

8.132 
8.156 

3.200 
3.227 

6 

  
 
 
 
 
 
PMM 

3.905 
3.933 

44.654 
44.78 

2.168 
1 
2 
2.382 
3  1562372.820  28.111  25.100 
4  1428484.378  30.149  23.789 
2.943 
5 
3.146 
6 

39.296 
39.381 

3.732 
3.755 

0.009 
0.014 
0.210 
0.277 
0.020 
0.034 

Mekanisme terakhir yang digunakan dalam analisis perbandingan pada penelitian ini, adalah 
mekanisme MNAR. Mekanisme ini terjadi dikarenakan adanya standar untuk suatu pertanyaan 
dalam survei. Sehingga saat responden tidak memenuhi syarat yang sitentukan untuk pertanyaan 
tersebut,  maka  secara  otomatis  pertanyaan  ini  akan  bernilai  kosong  atau  missing.  Untuk  itu, 
biasanya  missing  value  dengan  mekanisme  ini  memiliki  pola  tersendiri  dan  mudah  tertebak 
dimana  data  tersebut  akan  kosong.  Untuk  melihat  hasil  imputasi  dari  metode  Hot-Deck 
Impitation, metode KNNI, dan metode PMM dalam menangani missing value dengan mekanisme 
ini, terlampir pada Tabel III berikut. 

TABEL III 
RINGKASAN RATA – RATA HASIL IMPUTASI DENGAN MEKANISME MNAR 

Metode 

Hot-Deck   
Imputation 

KNNI 

PMM 

Dataset  RMSE 

31.654 
31.93 

UCE 
3.439 
3.457 

3.470 
3.488 
3.553 
3.556 

30.980 
31.123 
14.783 
14.99 

SCE  Waktu 
0.017 
1 
1.846 
0.016 
1.835 
2 
0.300 
3  1467939.508  26.260  18.674 
0.512 
4  1467890.642  26.601  22.957 
0.021 
1.700 
5 
0.024 
2.328 
6 
0.065 
1.860 
1 
0.066 
2 
1.875 
0.467 
3  1256383.930  24.910  19.983 
13.792 
918348.493  20.103  22.742 
4 
0.256 
1.079 
5 
0.333 
2.245 
6 
0.010 
2.701 
1 
0.012 
2 
2.771 
0.273 
3  1557934.309  27.827  24.742 
0.310 
4  1427545.489  28.999  22.999 
0.036 
1.951 
5 
0.045 
2.300 
6 

7.997 
8.012 
43.880 
43.096 

3.286 
3.289 
3.868 
3.888 

38.991 
39.091 

3.694 
3.688 

Setelah didapatkan tabel rata – rata hasil imPutasi dari metode Hot-Deck Imputation, metode 
KNNI, dan metode PMM, maka langkah selanjutnya adalah dengan melakukan analisis skoring. 
Analisis skoring dilakukan untuk mengerucutkan hasil perbandingan yang tealh didapatkan dari 
analisis  perbandingan  sehingga  didapatkan  kesimpulan  mengenai  metode  terbaik  dalam 
penanganan missing value. Skoring dilakukan dengan menggunakan nilai kuantil dari tabel rata – 
rata hasil imputasi yang ada pada Tabel I, Tabel II, dan tabel III. Setelah dilakukan skoring, maka 
dihitung  total  skor  yang  didapatkan  oleh  masing  –  masing  metode  berdasarkan  mekanisme 
missing value-nya. Pemberian skor kepada tiap metode berdasarkan penilaian yang dilampirkan 
pada Tabel IV berikut. 

TABEL IV 
TABELL SKORING 

Kriteria  Mekanisme 

Rentang 

RMSE 

MCAR 

0 - 3.246 
3.247 - 3.961 

7 

Skor 
**** 
*** 

 
 
 
 
 
 
   ◼ 

       ISSN: 1978-1520 

3.961 - 1190846.000 
1190846.001 - 
1567256.146 
0 - 3.217 
3.218 - 3.933 
3.934 - 1174593.000 
1174593.001 - 
1562372.280 
0 - 3.101 
3.102 - 3.904 
3.905 - 1171875.000 
1171875.001 - 
1557934.309 
0 - 3.542 
3.543 - 3.853 
3.854 - 25.998 
25.999 - 36.330 
0 - 3.578 
3.579 - 3.743 
3.744 - 24.912 
24.913 - 30.149 
0 - 3.474 
3.475 - 3.691 
3.692 - 23.708 
23.709 - 28.999 
0 - 2.378 
2.379 - 2.610 
2.611 - 21.169 
21.170 - 25.667 
0 - 2.148 
2.149 - 2.737 
2.738 - 20.364 
20.365 - 25.100 
0 - 1.863 
1.864 - 2.314 
2.315 - 19.655 
19.655 - 24.742 
0 - 0.018 
0.019 -0.033 
0.034 - 0.219 
0.219 - 9.360 
0 - 0.019 
0.020 - 0.052 
0.053 - 0.270 
0.271 - 9.950 
0 - 0.021 
0.022 - 0.065 
0.066 - 0.307 
0.308 - 13.792 

** 

* 

**** 
*** 
** 

* 

**** 
*** 
** 

* 

**** 
*** 
** 
* 
**** 
*** 
** 
* 
**** 
*** 
** 
* 
**** 
*** 
** 
* 
**** 
*** 
** 
* 
**** 
*** 
** 
* 
**** 
*** 
** 
* 
**** 
*** 
** 
* 
**** 
*** 
** 
* 

MAR 

MNAR 

MCAR 

UCE 

MAR 

MNAR 

MCAR 

SCE 

MAR 

MNAR 

MCAR 

Waktu 

MAR 

MNAR 

Berdasarkan tabel skoring yang telah terbentuk pada Tabel IV, maka akan didapatkan hasil 
skoring seperti pada Tabel V, Tabel VI, dan Tabel VII berikut. Pada mekanisme MCAR, metode 
Hot-Deck Imputation merupakan metode yang memiliki skor paling tinggi diantara kedua metode 
lainnya dengan total skor yang didapatkan sebesar 61. Yang berarti metode Hot-Deck Imputation 
lebih unggul 2 poin dari metode KNNI dan lebih unggul 9 poin dari metode PMM. Dengan skor 

8 

  
 
 
ini  maka  metode  Hot-Deck  Imputation  dapat  dikatakan  sebagai  metode  imputasi  yang  terbaik 
untuk menangani missing value dengan mekanisme MCAR dibandingkan dengan metode KNNI 
dan PMM. 

TABEL V 
TABEL HASIL SKORING DENGAN MEKANISME MCAR 

Metode 

Hot-Deck   
Imputation 

KNNI 

PMM 

Simulasi  RMSE  UCE 
**** 
**** 
* 
* 
**** 
*** 
** 
*** 
* 
* 
**** 
**** 
** 
** 
* 
* 
** 
** 

1  *** 
2  *** 
3  * 
4  * 
5  *** 
6  *** 
1  **** 
2  **** 
3  * 
4  ** 
5  * 
6  * 
1  ** 
2  ** 
3  * 
4  * 
5  *** 
6  *** 

SCE  Waktu 
**** 
**** 
* 
* 
** 
** 
*** 
** 
* 
* 
**** 
**** 
** 
** 
* 
* 
*** 
*** 

**** 
**** 
* 
* 
*** 
*** 
** 
** 
*** 
* 
**** 
*** 
**** 
**** 
** 
* 
**** 
*** 

Total 

Total 
Skor 

15 
15 
4 
4 
12 
11 
11 
11 
7 
5 
13 
12 
10 
10 
5 
4 
12 
11 

61 

59 

52 

Berdasarkan Tabel VI, maka dapat disimpulkan bahwa metode Hot-Deck merupakan metode 
yang paling baik dalam menangani missing value dengan mekanisme MAR. Hal ini berdasarkan 
poin yang didapatkan oleh metode Hot-Deck Imputation yang berada diatas metode KNNI dan 
metode PMM dengan poin sebesar 65. Poin ini lebih tinggi 3 poin dibandingkan dengan poin 
yang didapatkan oleh KNNI dan lebih tinggi 7 poin dibandingkan dengan poin milik PMM. 

TABEL VI 
TABEL HASIL SKORING DENGAN MEKANISME MAR 

Metode 

Simulasi 

RMSE 

Hot-
Deck   
Imputat
ion 

KNNI 

PMM 

1  *** 
2  *** 
3  * 
4  * 
5  **** 
6  *** 
1  **** 
2  **** 
3  * 
4  ** 
5  **** 
6  **** 
1  ** 
2  ** 
3  **** 
4  **** 
5  *** 
6  ** 

UCE  SCE  Waktu 
*** 
**** 
* 
* 
**** 
**** 
*** 
*** 
* 
* 
**** 
**** 
** 
** 
* 
* 
*** 
** 

**** 
**** 
* 
* 
**** 
*** 
**** 
** 
* 
* 
**** 
*** 
*** 
*** 
* 
* 
** 
** 

**** 
**** 
* 
* 
*** 
*** 
** 
** 
* 
* 
*** 
*** 
**** 
**** 
** 
*** 
*** 
** 

9 

Total Skor 

65 

62 

58 

Total 
14 
15 
4 
4 
15 
13 
13 
11 
4 
5 
15 
14 
11 
11 
8 
9 
11 
8 

 
 
 
 
 
   ◼ 

       ISSN: 1978-1520 

Dengan  mekanisme  MNAR,  metode  Hot-Deck  Imputation  kembali  unggul  dibandingkan 
dengan metode KNNI dan metode PMM. Metode ini mendapatkan poin 65 yang merupakan poin 
tertinggi dalam mekanisme MNAR.  Dengan poin ini, Hot-Deck Imputation berada jauh diatas 
metode PMM dan metode KNNI yang mendapatkan sama – sama mendapatkan poin sebesar 52. 

Metode 

Hot-Deck   
Imputatio
n 

KNNI 

PMM 

TABEL VII 
TABEL HASIL SKORING DENGAN MEKANISME MNAR 
Simulasi 

SCE  Waktu 

RMSE  UCE 

Total 

Total Skor 

1  *** 
2  *** 
3  * 
4  * 
5  **** 
6  ** 
1  **** 
2  **** 
3  * 
4  ** 
5  * 
6  * 
1  ** 
2  ** 
3  * 
4  * 
5  *** 
6  ** 

**** 
**** 
* 
* 
**** 
*** 
*** 
*** 
* 
* 
**** 
**** 
** 
** 
* 
* 
** 
*** 

**** 
**** 
* 
* 
**** 
** 
**** 
*** 
* 
* 
**** 
*** 
** 
** 
* 
* 
*** 
*** 

**** 
**** 
** 
* 
**** 
*** 
** 
** 
* 
* 
** 
* 
**** 
**** 
*** 
* 
*** 
*** 

15 
15 
5 
4 
16 
10 
11 
12 
4 
5 
11 
9 
10 
10 
6 
4 
11 
11 

65 

52 

52 

Gambar 2. Tahapan Penelitian 

Berdasarkan  Gambar  2,  maka  dapat  disimpulkan  bahwa  metode  Hot-Deck  Imputation 
merupakan metode imputasi terbaik dalam menangani missing value dengan mekanisme MCAR, 
MAR,  dan  MNAR.  Hal  ini  dapat  disimpulkan  berdasarkan  hasil  skoring  yang  secara  mutlak 
menyatakan  Hot-Deck  Imputation  sebagai  metode  terbaik  untuk  menangani  missing  value. 
Meskipun secara keseluruhan metode ini mendapatkan poin yang cukup tinggi, akan tetapi jika 
diteliti  lagi  metode  ini  cukup  lemah  dalam  menghadapi  missing  value  yang  ada  pada  dataset 
official statistics. Seperti yang tertera pada tabel VII, tabel VIII, dan tabel IX, metode Hot-Deck 
Imputation  mendapatkan  poin  yang  sangat  minimal  pada  simulasi  3  dan  simulasi  4  yang 
merupakan simulasi untuk dataset official statistics, yaitu dataset SUSENAS KOR Maret Tahun 
2019. Berdasarkan [4] hal ini bisa terjadi dikarenakan adanya pernyataan bahwa metode imputasi 
akan  lebih  baik  diterapkan  pada  data  berdistribusi  normal.  Sedangkan  data  mentah  yang 

10 

  
 
 
 
 
digunakan  pada  simulasi  3  dan  simulasi  4  tidak  memenuhi  asumsi  ini.  Oleh  karena  itu,  ada 
kemungkinan  apabila  dilakukan  transformasi  data  menjadi  data  normal  poin  yang  akan 
didapatkan  oleh  metode  Hot-Deck  Imputation  pada  analisis  skoring  akan  menjadi  lebih  baik. 
Selain itu, Jumlah variable juga mempengaruhi performa dari metode Hot-Deck Imputation [18]. 
Hot-Deck Imputation lebih cocok apabila diterapkan pada dataset yang memiliki variabel banyak. 
Hal  ini  dapat  dibuktikan  pada  simulasi  5  dengan  mekanisme  MNAR.  Metode  Hot-Deck 
Imputation mendapatkan poin yang sempurna yaitu 16. Pada simulasi 5 digunakan dataset Breast 
Cancer I yang memiliki 65 variabel yang merupakan dataset dengan variabel terbanyak dalam 
penelitian ini. sehingga dapat dismpulkan bahwa metode Hot-Deck Imputation memiliki performa 
komputasi yang lebih baik dikarenakan total poin yang didapatkan pada analisis skoring yang 
selalu berada di urutan paling atas. Selain itu, metode Hot-Deck Immputation akan lebih efektif 
dijalankan  pada  dataset  yang  memiliki  variabel  banyak.  Akan  tetapi,  metode  ini  cukup  lemah 
dalam kekonsistenan estimatornya. Hal ini dibuktikan dengan nilai RMSE dari metode ini cukup 
rendah dibanndingkan dengan kedua metode lainnya. 

Berhubungan dengan ketepatan estimator, maka metode KNNI memiliki ketepatan estimator 
yang lebih baik dibandingkan dengan metode Hot-Deck Imputation dan metode PMM. Hal ini 
dibuktikan  dengan  perolehan  nilai  RMSE  metode  KNNI  yang  cukup  baik.  Nilai  RMSE  dari 
metode KNNI dapat lebih tinggi lagi apabila adanya proses standarisasi data pada keenam dataset. 
salah satu faktor utama yang harus diperhatikan pada metode KNNI adalah satuan datanya. Maka 
penggunaan data mentah dalam penelitian ini tentunya akan mempengaruhi peforma dari metode 
KNNI [12]. 

Berbanding terbalik dengan metode Hot-Deck Imputation yang cukup baik dalam menangani 
dataset dengan jumlah yang besar dan variabel yang banyak, justru metode KNNI ini kurang baik 
diterapkan  dalam  dataset  yang  memiliki  variabel  yang  banyak.  Hal  ini  terbukti  dengan  hasil 
skoring pada simulasi 4, simulasi 5, dan simulasi 6 metode KNNI mengalami penurunan performa 
komputasi. Selain itu, waktu yang dibutuhkan untuk menjalankan algortima pada metode KNNI 
cukup  memakan  waktu  yang  lebih  panjang  jika  dibandingkan  dengan  metode  Hot-Deck 
Imputation dan metode PMM. Hal ini terlihat dari hasil skoring waktu yang digunakan metode 
KNNI rata – rata hanya berada direntang 1 hingga 3. Penggunaan waktu yang cukup lama dalam 
metode KNNI ini didorong oleh algortima yang dimiliki oleh metode KNNI ini cukup panjang, 
sehingga  waktu  yang  dibutuhkan  untuk  mengeksekusi  algortima  metode  KNNI  juga  menjadi 
lebih  panjang.  Sehingga  metode  KNNI  ini  akan  lebih  baik  jika  digunakan  untuk  data  yang 
memiliki unit observasi yang kecil dan variabel yang sedikit.  

Permasalahan penggunaan waktu yang ada pada metode KNNI ini nyatanya tidak berlaku pada 
metode PMM. Untuk kriteria waktu, metode PMM memiliki performa yang cukup baik karena 
hasil  skoringnya  menunjukkan  bahwa  metode  ini  memiliki  waktu  yang  cukup  singkat  dalam 
mengeksekusi algoritmanya. Hal ini dipengaruh oleh algoritma yang dimiliki oleh metode PMM 
cukup sederhana. Hal ini juga yang menjadikan metode PMM menjadi salah satu metode imputasi 
yang  cukup  sering  digunakan.  Metode  PMM  bekerja  lebih  baik  pada  data  berdistribusi  tidak 
normal  dibandingkan  dengan  dataset  yang  berdistribusi  normal  [16].  Jika  dilihat  dilihat 
berdasarkan poin yang didapatkan dari analisis skoring, metode PMM memiliki poin yang lebih 
tinggi dibandingkan metode Hot-Deck Imputation dan metode KNNI pada simulasi 3 dan simulasi 
4. Jika silihat dari poin yang didapatkan oleh metode PMM, metode PMM bekerja lebih baik saat 
menangani dataset SUSENAS KOR Maret Tahun 2019 yang merupakan data  official statistics 
yang tidak memenuhi asumsi kenormalan. Sehingga metode PMM dapat menjadi pilihan yang 
paling baik diabandingkan dengan metode Hot-Deck Imputaion dan metode KNNI disaat dataset 
yang dimiliki tidak memenuhi asumsi kenormalan. 

4. KESIMPULAN 

Berdasarkan hasil dan pembahasan pada bab sebelumnya, maka didapatkan kesimpulan dari 
penelitian ini, yaitu secara umum metode Hot-Deck Imputation merupakan metode yang paling 

11 

 
 
 
 
 
   ◼ 

       ISSN: 1978-1520 

baik  dalam menangani  missing value, metode ini  sangat  baik dalam menangani  missing  value 
pada dataset dengan jumlah variabel yang banyak akan tetapi kurang mampu menangani missing 
value pada dataset kecil. Kekurangan yang dimiliki oleh metode Hot-Deck Imputation nyatanya 
tidak berlaku pada metode KNNI yang justru sangat baik dalam menangani dataset kecil. Namun, 
metode KNNI kurang baik dalam segi waktu. Algoritma yang cukup rumit membuat metode ini 
memakan waktu yang cukup banyak dalam eksekusinya. Permasalahan waktu yang dialami oleh 
metode KNNI tidak berlaku pada metode PMM yang justru memiiki waktu yang singkat dalam 
mengimputasi  missing  value  pada  data.  Ketiga  metode  sama  –  sama  kurang  baik  dalam 
menangani missing value pada data SUSENAS. Hal ini dapat dilihat dari penurunan performa 
yang dialami ketiga metode pada simulasi ketiga dan keempat yang merupakan simulasi dengan 
menggunakan  data  SUSENAS.  Penurunan  peforma  dapat  disebabkan  karena  data  SUSENAS 
yang digunakan dalam simulasi merupakan data mentah sehingga data tidak berdistribusi normal. 
Kemungkinan jika dilakukan normalisasi maka hasil imputasi dapat lebih baik. 

5. SARAN 

Berdasarkan hasil dan kesimpulan yang didapatkan dari penelitiaan ini, maka beberapa saran 
yang  dapat  peneliti  berikan  untuk  penelitian  yang  selanjutnya,  adalah  memperhatikan  pola 
missing value dan mengembangkan metode skoring yang lebih baik. 

DAFTAR PUSTAKA 

[1] 
[2] 
[3] 

[4] 

[5] 
[6] 

[7] 
[8] 

[9] 

[10] 

[11] 

[12] 

[13] 

[14] 

[15] 

[16] 

[17] 

[18] 

J. Han, M. Kamber, dan J. Pei, Data Mining: Concepts and Techniques. 2012. 
R. E. A. Joseph F. Hair Jr, William C. Black, Barry J.Babin, “Multivariate Data Analysis.pdf.” hal. 758, 2010. 
T. Hendrawati, “Kajian Metode Imputasi Dalam Menangani Missing Data,” Pros. Semin. Nas. Stat. | Dapertemen Stat. 
FMIPA Univ. Padjadjaran, hal. 637–642, 2015. 
I. J. Fadillah dan S. Muchlisoh, “Perbandingan Metode Hot-Deck Imputation Dan Metode Knni Dalam Mengatasi Missing 
Values,” Semin. Nas. Off. Stat., vol. 2019, no. 1, hal. 275–285, 2020, doi: 10.34123/semnasoffstat.v2019i1.101. 
S. Van Buuren, Flexible Imputation of Missing Data, Second Edi. London, New York: CRC Press, 2018. 
J. Luengo, S. García, dan F. Herrera, “A study on the use of imputation methods for experimentation with Radial Basis 
Function Network classifiers handling missing attribute values: The good synergy between RBFNs and EventCovering 
method,” Neural Networks, vol. 23, no. 3, hal. 406–418, 2010, doi: 10.1016/j.neunet.2009.11.014. 
C. Chatfield, R. J. A. Little, dan D. B. Rubin, Statistical Analysis with Missing Data., vol. 151, no. 2. 1988. 
H. Migdady dan M. Al-Talib, “An enhanced fuzzy K-means clustering with application to missing data imputation,” 
Electron. J. Appl. Stat. Anal., vol. 11, no. 2, hal. 674–686, 2018, doi: 10.1285/i20705948v11n2p674. 
M. R. Stavseth, T. Clausen, dan J. Røislien, “How handling missing data may impact conclusions: A comparison of six 
different imputation methods for categorical questionnaire data,” SAGE Open Med., vol. 7, hal. 205031211882291, 2019, 
doi: 10.1177/2050312118822912. 
J. M. Jerez et al., “Missing data imputation using statistical and machine learning methods in a real breast cancer 
problem,” Artif. Intell. Med., vol. 50, no. 2, hal. 105–115, 2010, doi: 10.1016/j.artmed.2010.05.002. 
R. R. Andridge dan R. J. A. Little, “A review of hot deck imputation for survey non-response,” Int. Stat. Rev., vol. 78, no. 
1, hal. 40–64, 2010, doi: 10.1111/j.1751-5823.2010.00103.x. 
M. Mukarromah, S. Martha, dan I. Ilhamsyah, “Perbandingan Imputasi Missing Data Menggunakan Metode Mean Dan 
Metode Algoritma K-Means,” Bimaster, vol. 4, no. 3, hal. 305–312, 2015, [Daring]. Tersedia pada: 
http://jurnal.untan.ac.id/index.php/jbmstr/article/view/12425/. 
S. P. Mandel J, “A Comparison of Six Methods for Missing Data Imputation,” J. Biom. Biostat., vol. 06, no. 01, hal. 1–6, 
2015, doi: 10.4172/2155-6180.1000224. 
S. Y. Siregar, S. St, T. Toharudin, B. Tantular, S. Si, dan M. Si, “Performa Metode K Nearest Neighbor Imputation ( Knni 
) Untuk Menangani Multivariate Missing Data,” hal. 1–7, 2013. 
I. Aprillani dan A. A. Suryani, “Analisis Penanganan Missing Value Dengan Metode Predictive Mean Matching (Pmm),” 
2012. 
H. Anisa, Ladpoje, “Imputasi Ganda Dengan Metode Predictive Mean Matching Untuk Estimasi Data Hilang pada Item 
Nonrespon,” Digilib.Unhas.Ac.Id, no. Matematika, hal. 68–70, 2015. 
W. Wilsen, W. Rahayu, dan V. M. Santi, “Penerapan Imputasi Ganda dengan Metode Predictive Mean Matching (PMM) 
untuk Pendugaan Data Hilang pada Model Regresi Linear,” J. Stat. dan Apl., vol. 2, no. 1, hal. 12–20, 2018, doi: 
10.21009/jsa.02102. 
D. W. Joenssen dan U. Bankhofer, “Hot deck methods for imputing missing data: The effects of limiting donor usage,” 
Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics), vol. 7376 LNAI, hal. 63–
75, 2012, doi: 10.1007/978-3-642-31537-4_6. 

12 

  
 
 
 
 
 
 
"
221710028,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Estimasi Luas Area Tanam Padi dengan Deep 
Learning pada Citra Satelit Resolusi Menengah 
Sentinel-2 dan Landsat-8,  
Studi Kasus di Kabupaten Nganjuk, Jawa Timur   

Terry Devara Tri Saadi (221710028, 4SD2) 

Dosen Pembimbing: Dr. Eng. Arie Wahyu Wijayanto, SST, MT. 

Ringkasan—  Pada  tahun  2018,  Badan  Pusat  Statistik  (BPS) 
telah  memperkenalkan  metode  Kerangka  Sampel  Area  (KSA) 
untuk melakukan estimasi area produksi padi, di mana prosesnya 
masih  membutuhkan  sumber  daya  manusia.  Penelitian  ini 
membahas  mengenai  pemanfaatan  data  penginderaan  jauh, 
khususnya  citra  satelit  Sentinel-2  dan  Landsat-8,  sebagai 
pendekatan  alternatif  yang  lebih  murah  dari  sisi  tenaga. 
Penelitian  ini  berfokus  pada  penentuan  fitur  pita  spektral  dan 
indeks komposit citra dengan pemanfaatan metode deep learning 
untuk mendeteksi area tanam padi di Kabupaten Nganjuk, Jawa 
Timur. Selain itu penelitian ini bertujuan untuk membuat model 
pengklasifikasian  terbaik,  sekaligus  melakukan  estimasi  area 
tanam  padi.  Hasil  penelitian  menunjukan  model  CNN 
memberikan hasil terbaik dengan nilai f1-score sebesar 97,16%. 
Kata Kunci— penginderaan jauh, deteksi padi, deep learning 

I.  LATAR BELAKANG 

Beras  telah  lama  menjadi  sumber  makanan  utama  bagi 
rakyat Indonesia. Meskipun begitu, berdasarkan publikasi Luas 
Panen  dan  Produksi  Panen  Padi  dari  Badan  Pusat  Statistik 
(BPS), luas panen padi pada 2019 diperkirakan sebesar 10,68 
juta  hektar  atau  mengalami  penurunan  sebanyak  700,05  ribu 
hektar atau 6,15 persen dibandingkan tahun 2018 [1]. Selain itu, 
produksi  padi  pada  2019  diperkirakan  sebesar  54,60  juta  ton 
GKG atau mengalami penurunan sebanyak 4,60 juta ton atau 
7,76 persen dibandingkan tahun 2018  [1]. Hal ini mendorong 
perlunya  dilakukan  monitoring  area  tanam  dan  produksi 
pangan  untuk  mewujudkan  goals  ke  dua  dari  Sustainable 
Development Goals (SDGs). 

Memonitoring  tanaman  pangan  telah  dilakukan  oleh  BPS 
melalui Kerangka Samplel Area (KSA) yang dilakukan dengan 
survei lapangan terhadap titik-titik pengamatan dalam sampel 
segmen  [2].  Meskipun  merupakan  metode  estimasi  dengan 
akurasi yang baik, KSA memerlukan sumber daya yang cukup 
besar.  Selain  itu,  penggunaan  KSA  juga  masih  rentan  pada 
kondisi  kahar  dan  remote  area.  Hal 
ini  memberikan 
keterbatasan untuk dapat mencakup seluruh wilayah Indonesia 
dalam periode update yang cepat sesuai kebutuhan monitoring 
waktu tanam padi. 

Di sisi lain, data penginderaan jauh dapat diperoleh dengan 
mudah  dan  dapat  digunakan  di  berbagai  bidang.  Beberapa 
bidang  yang  telah  memanfaatkan  data  penginderaan  jauh 
sendiri  di  antaranya  adalah  klasifikasi  tutupan  lahan  [3], 
prediksi kemiskinan [4], pemetaan area terbakar [5], klasifikasi 
tanaman  [6].  Penelitian  ini  berfokus  pada  estimasi  luas  area 

tanam padi dengan deep learning di kabupaten Nganjuk sebagai 
salah  satu  kabupaten  utama  penghasil  padi  di  Jawa  Timur  di 
mana separuh wilayahnya digunakan untuk menanam padi.  

II.  TUJUAN DAN BATASAN PENELITIAN 

A. Tujuan Penelitian 

Tujuan dari penelitian ini adalah untuk  

1.  Mengidentifikasi fitur terbaik untuk mengenali citra lahan 

tanam padi. 

2.  Mengidentifikasi model terbaik untuk pengenalan citra dan 

estimasi luas lahan tanam padi. 

3.  Melakukan estimasi luas lahan tanam padi. 

B. Batasan Penelitian 

Dalam proses identifikasi model klasifikasi area tanam padi, 
penentuan  label  pada  penelitian  terkendala  oleh  keterbatasan 
data  administrasi  yang  ada  sehingga  dilakukan  pemeriksaan 
lapangan  (ground  check)  untuk  memastikan  keabsahan  label 
yang digunakan. Semua model yang digunakan kemudian diuji 
dengan data label yang sama untuk mencapai prinsip  fairness 
and equality. 

Penelitian  ini  berfokus  pada  pemanfaatan  teknologi  deep 
learning  dalam  melakukan  segmentasi  citra  satelit  dengan 
mempelajari  fitur  spektral  dan  melakukan  estimasi  luas  area 
tanam  padi  dari  hasil  segmentasi  tersebut.  Maka  dari  itu, 
beberapa  metode  seperti  pemanfaatan  data  multi  temporal 
hingga sampai ke fase tanam tidak digunakan karena sudah ada 
penelitian lain yang fokus pada topik tersebut. Hal ini berbeda 
dari produksi data untuk statistik  official di mana semua opsi 
harus  diperhitungkan  agar  dapat  menghasilkan  metode  yang 
feasible dengan risiko sekecil mungkin.  

III. PENELITIAN TERKAIT 

Terdapat  beberapa  penelitian  berkaitan  dengan  tujuan 
klasifikasi  penggunaan  lahan.  Sebagaimana  bisa  dilihat  dari 
tabel literatur di Tabel 1. Masih terbatasnya jumlah penelitian 
Indonesia  yang 
pada  bidang  penginderaan 
memanfaatkan  teknologi  machine  learning  untuk  melakukan 
klasifikasi lahan padi. Sementara itu, hingga saat ini, sepanjang 
pengetahuan dan pencarian penulis, pemanfaatan metode deep 
learning  yang  secara  luas  terbukti  lebih  unggul  di  bidang 
computer  vision  belum  dipergunakan  untuk  klasifikasi  lahan 
padi di Indonesia. 

jauh  di 

 1 / 8 

 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

TABEL I 
TABEL LITERATUR 

Tujuan 

Fitur dan Metode 

Sumber Data 

Lokasi Studi 

No 

1 

Pengarang, 
Tahun 
I. M. Parsa  et al., 
2020 [7] 

Identifikasi konversi lahan sawah akibat 
pembangunan jalan bebas hambatan 

2  M. Parsa et al., 
2020 [8] 

3 

Rokhmatuloh et 
al., 2019 [9] 

Memperoleh parameter optimal untuk 
model klasifikasi lahan tanam padi 
berdasar data multi temporal 
Pemetaan area tanam padi dengan citra 
multi spectral  

4  Widiatmaka et 
al., 2016 [10] 
D. W. Triscowati 
et al., 2020 [11] 

5 

Pemetaan lahan untuk mengevaluasi 
kecocokan lahan untuk tanaman padi 
Mengembangkan model klasifikasi 
tanaman padi untuk data multi temporal 

6 

D. W. Triscowati 
et al., 2019 [12] 

Mencari fitur multi temporal terbaik untuk 
klasifikasi fase tanaman padi 

7 

Penelitian ini 

Mengidentifikasikan fitur dan model 
terbaik untuk mengenali citra padi dan 
mengestimasikan luas lahan tanam. 

Transformasi indeks vegetasi 
dan overlay (tumpang-susun) 
data 

Transformasi indeks vegetasi 

Landsat 7 (Terbuka), 
Landsat 8 (Terbuka), 
SPOT 6 (Tertutup), SPOT 
7 (Tertutup) 
Landsat 8 (Terbuka) 

Transformasi indeks vegetasi 

Unmanned Aerial Vehicle 
(Tertutup) 

SPOT 6 (Tertutup) 

Landsat 8 (Terbuka) 

Subang, Jawa 
Barat 

Jawa Barat dan 
Sulawesi Selatan 

Parakansalak , 
Sukabumi, Jawa 
Barat 
Subang, Jawa 
Barat 
Banyuwangi, Jawa 
Timur 

Supervised Classification 
menggunakan software ERDAS 
Transformasi indeks vegetasi, 
transformasi indeks air, dan 
Supervised Random Forest 
Clasification 
Transformasi indeks vegetasi, 
transformasi indeks air, dan 
Supervised Random Forest 
Clasification 
Transformasi indeks vegetasi, 
transformasi indeks air, 
machine learning dan deep 
dearning 

Landsat 8 (Terbuka) 

Banyuwangi, Jawa 
Timur 

Sentinel 2 (Terbuka), 
Landsat 8 (Terbuka)  

Nganjuk, Jawa 
Jawa Timur 

IV. METODE PENELITIAN  

A. Lokasi 

Penelitian ini dijalankan di kabupaten Nganjuk, Jawa Timur 
yang  terletak  pada  latitude  7.6°S  dan  latitude  111.9333°E 
seperti yang dapat dilihat di gambar. Nganjuk memiliki wilayah 
seluas 1.182,64 km2, di mana menurut BPS, pada tahun 2018, 
65.539,42 hektar (55.42% wilayah) digunakan untuk bertanam 
padi dengan hasil produksi sebesar 395.385,12 ton GKG[13]. 
Sektor Pertanian, Kehutanan, dan Perikanan sendiri merupakan 
penyumbang  PDRB  terbesar  di  kabupaten  Nganjuk,  yaitu 
sebesar  7.232  miliar  rupiah,  atau  setara  29,18%  dari  PDRB 
kabupaten Nganjuk [14]. 

Gambar 1. Citra Sentinel-2 kabupaten Nganjuk yang digunakan 

B. Metode Pengumpulan Data 

Pada  penelitian  ini,  citra  Sentinel-2  yang  telah  diproses 
menjadi level 2A yang diambil pada 14 Maret 2020 digunakan 
untuk menjalankan proses analisis dan evaluasi. Sampel yang 
digunakan dapat dilihat pada Tabel II. 

TABEL II 
JUMLAH SAMPEL PIKSEL SENTINEL-2 MSI DAN LANDSAT 8 

Sentinel 2 

Landsat 8 

Terbangun 
Air 
Sawah 
Bera 
Hutan 
Jalan Tol 
Awan 

16.276  
4.904 
13.364 
18.335 
17.891 
3.138 
14.847 

2.663  
825 
1.826 
2.653 
2.302 
796 
1.639 

TABEL III 
LIST INDEKS KOMPOSIT YANG DIGUNAKAN DAN APLIKASI PADA SENTINEL-2 

Indeks 
Komposit 
NDVI 
NDWI 
NDBI 
NDTI 
EVI2 
BSI 

Persamaan Band yang digunakan a 

(NIR - Red) / (NIR + Red) 
(Green - NIR) / (Green + NIR) 
(SWIR1 - NIR) / (SWIR1 + NIR) 
(SWIR1 - SWIR2) / (SWIR1 + SWIR2) 
2.5 * (NIR - Red) / (NIR + 2.4 * Red + 1)  
2.5 * ((SWIR1 + Red) - (NIR + Green)) / ((SWIR1 + Red) + (NIR 
+ Green)) 

a Keterangan mengenai Band pada masing-masing satelit dapat dilihat di 
Tabel IV. 

 2 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
Fitur dasar yang digunakan dalam penelitian ini adalah band 
biru,  hijau,  merah,  near-infrared  (NIR),  serta  short  wave 
infrared (SWIR) 1 dan 2. Selain itu, beberapa indeks komposit 
juga digunakan, diantaranya Normalized Difference Vegetation 
Index  (NDVI),  dan  Enhanced  Vegetation  Index  (EVI)  yang 
akan  digunakan  untuk  membedakan  antara  sawah  dan  hutan. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Normalized  Difference  Water 
(NDWI)  untuk 
menunjukan daerah genangan air. Normalized Difference Built-
up Index (NDBI), Normalized Difference Tillage Index (NDTI), 
dan  Bare  Soil  Index  (BSI)  untuk  membedakan  antara  area 
terbangun, bera, dan jalan tol.  

Index 

TABEL IV 
SPECTRAL BAND SENTINEL-2 MSI DAN LANDSAT 8 

Sentinel 2 Band 

1 
(Aerosols) 

2 
(Blue) 

3 
(Green) 

4  
(Red) 

Wavelength (nm) 
Bandwidth (nm) 
Resolution (m) 

Landsat 8 Bands 

Wavelength (nm) 

Resolution (m) 

443 
20 
60 

1 
(Ultra 
Blue) 
0.435-
0.451 
30 

490 
65 
10 

560 
35 
10 

665 
30 
10 

2  
(Blue) 

3  
(Green) 

4  
(Red) 

0.452-
0.512 
30 

0.533-
0.590 
30 

0.636-
0.673 
30 

5 
(Red 
Edge 
1) 
705 
15 
20 

5 
(NIR) 

0.851-
0.879 
30 

6 
(Red 
Edge 
2) 
740 
15 
20 

7 
(Red 
Edge 
3) 
783 
20 
20 

8 
(NIR) 

842 
11 
10 

8A 
(Red 
Edge 
4) 
865 
20 
20 

10 
(Cirrus) 

11 
(SWIR 
1) 

12 
(SWIR 
2) 

945 
20 
60 

1375 
30 
60 

1610 
90 
20 

6  
(Shortwave 
Infrared 1) 

7  
(Shortwave 
Infrared 2) 

10  
(Brightness 
Temperature) 

11  
(Brightness 
Temperature) 

1.566-1.651 

2.107-2.294 

10.60-11.19 

11.50-12.51 

30 

30 

30 

30 

C. Metode Pemberian Label 

Pelabelan manual dilakukan dengan mengikuti tahapan yang 
dilakukan  dalam  penelitian  penginderaan  jauh.  Berdasarkan 
P.Helber  dkk.  [15],  proses  pelabelan  pada  data  penginderaan 
jauh  dilakukan  dengan  membandingkan  citra  satelit  optik 
dengan  official  label  dari  data  yang  bersangkutan.  Sebagai 
tambahan  validasi,  maka  untuk  wilayah  sawah  dilakukan 
identifikasi 
lapangan  oleh  peneliti  untuk 
memastikan bahwa yang dilabelkan memang sawah secara riil 
pada periode Desember 2020. 

langsung  ke 

D. Random Forest 

Random  forest  [16]  merupakan  metode  perubahan  dari 
teknik  bagging  yang  mengumpulkan  library  trees  yang  ber-
dekorelasi  lalu  dirata-ratakan.  Walaupun  secara  umum  mirip 
dengan bagging dari sisi performa, random forest lebih mudah 
dari segi training dan tuning. Maka dari random forest menjadi 
popular  dan  banyak  diimplementasikan  dalam  berbagai 
package. 

E. Multi-Layer Perceptron (MLP) 

Multilayer  perceptron  merupakan  model  neural  network 
dengan  beberapa  hidden  layer,  dan  neuron  antar  layer  yang 
bersebelahan saling terhubung [17]. Penggunaan hidden layer 
rendah. 
yang 

sedikit  mengakibatkan  akurasi  menjadi 

Penambahan  jumlah  layer  akan  menyebabkan  peningkatan 
waktu pemrosesan sementara akurasi belum tentu meningkat. 

F. Convolutional Neural Network (CNN) 

CNN  merupakan  model  deep  learning  multilayer  dengan 
menggunakan  sebuah  neural network  melalui  proses  training 
end to end dari nilai pixel mentah menjadi output terklasifikasi. 
Dengan  memanfaatkan  training  data  berukuran  besar  dan 
pengimpelentasian  GPU  yang  efisien,  CNN  telah  melampaui 
performa  dari  motode  konvensional  dalam  computer  vision, 
termasuk image classification, object detection, scene labelling, 
dan face recognition [18]. 

V.  KERANGKA PIKIR 

Penelitian ini didasari dari mahalnya metode yang digunakan 
saat ini dari segi tenaga, serta rentannya terhadap kondisi kahar 
seperti  pandemi,  dan  remote  area.  Maka  diajukan  metode 
dengan  memanfaatkan  citra  penginderaan  jauh  dan  analisis 
menggunakan  machine  learning  dan  deep  learning.  Sebagai 
evaluasi,  digunakan  matriks  precision,  recall,  dan  f1-score 
dengan  mempertimbangkan  kondisi  data  yang  digunakan 
memiliki  class  imbalance.  Gambar  2  menunjukan  ilustrasi 
kerangka pikir dengan mengikuti desain dari Polančič [19] dan 
Berndtsson dkk. [20]

 3 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 2. Kerangka Pikir 

VI. HASIL DAN PEMBAHASAN 

A. Identifikasi Fitur Terbaik Penciri Area Tanam Padi 

Berdasarkan heatmap (Gambar 3), dan tabel masing-masing 
band  terhadap  tiap-tiap  kelas  (Tabel  V),  informasi  mengenai 
perbedaan  nilai  kelas  dalam  tiap  band  dapat  diperoleh.  Dari 
boxplot dan heatmap, dapat dilihat bahwa membedakan kelas-
kelas yang ada hanya dengan menggunakan band bawaan dari 
Sentinel-2 saja  tidak mungkin dilakukan karena  hampir tidak 
ada perbedaan nilai atar kelas. Maka dari itu indeks komposit 
yang  merupakan  kombinasi  dari  beberapa  band  yang  ada 
digunakan.  Sementara  saat  menggunakan  komposit  indeks 

seperti NDVI atau NDWI, perbedaan antar kelas mulai terlihat 
walaupun belum bisa dipisahkan secara keseluruhan. 
Dengan  memperhatikan  perbedaan  nilai-nilai 

indeks 
komposit  pada  masing-masing  kelas,  akan  terlihat  bahwa 
indeks  tertentu  dapat  membedakan  satu  atau  dua  kelas  dari 
kelas lain, tetapi gagal dalam membedakan kelasi lain tersebut. 
Sebagai  contoh  adalah  NDWI  yang  mampu  membedakan  air 
dari  kelas  lain  tapi  tidak  dapat  membedakan  antara  area 
terbangun  dengan  bera,  di  mana  satu-satunya    indeks  yang 
dapat  membedakan  kedua  kelas  tersebut  adalah  NDTI.  Hal 
yang sama juga dapat diaplikasikan pada band dan kelas lain, 
sehingga  masing-masing  kelas  memiliki  fitur  tertentu  baik 
dalam satu indeks maupun lebih, yang dapat digunakan untuk 
membedakannya dengan kelas lain.  

TABEL V 
DISTRIBUSI BANDS DAN INDEKS PADA KELAS TUTUPAN LAHAN. 

Sentinel 2 

Bands 

B2 

B3 

B4 

B8 

B11 

B12 

Terbangun 
1489.99 ± 
1692.14 
1672.95 ± 
1711.94 
1757.26 ± 
1704.13 
2440.34 ± 
1315.21 
3276.63 ± 
1867.00 
3032.46 ± 
2145.41 

Genangan Air 
523.770 ± 
185.70 
732.28 ± 
306.96 
732.48 ± 
485.60 
555.97 ± 
429.70 
216.14 ± 
163.25 
126.81 ± 
82.09 

Sawah 

374.67 ± 96.81 
645.62 ± 
136.34 
397.47 ± 
143.74 
3088.82 ± 
426.98 
1520.34 ± 
304.67 
773.63 ± 
210.97 

Bera 
677.93 ± 
152.28 
897.43 ± 
180.91 
993.18 ± 
231.59 
1778.34 ± 
436.00 
2022.91 ± 
589.18 
1426.89 ± 
434.62 

Hutan 

327.14 ± 87.54 

592.76 ± 96.20 

296.50 ± 83.75 
3692.94 ± 
435.02 
1773.96 ± 
137.51 

769.46 ± 85.64 

Jalan Tol 
1928.40 ± 
503.28 
2167.08 ± 
542.26 
2185.40 ± 
560.78 
2622.01 ± 
467.50 
2519.27 ± 
393.86 
2194.21 ± 
451.78 

Awan 
5562.28 ± 
3701.97 
5499.88 ± 
3470.64 
5284.84 ± 
3355.60 
5984.31 ± 
2491.87 
4889.39 ± 
1408.91 
3854.86 ± 
1273.65 

NDVI 

0.24 ± 0.19 

-0.19 ± 0.12 

0.77 ± 0.10 

0.28 ± 0.11 

0.85 ± 0.04 

0.10 ± 0.10 

0.11 ± 0.12 

NDWI 

-0.27 ± 0.18 

0.26 ± 0.24 

-0.65 ± 0.08 

-0.32 ± 0.10 

-0.72 ± 0.04 

-0.10 ± 0.09 

-0.09 ± 0.11 

NDBI 

0.14 ± 0.13 

-0.33 ± 0.25 

-0.34 ± 0.10 

0.06 ± 0.10 

-0.35 ± 0.04 

-0.02 ± 0.05 

-0.09 ± 0.11 

NDTI 

0.07 ± 0.07 

0.24 ± 0.07 

0.33 ± 0.04 

0.18 ± 0.04 

0.40 ± 0.03 

0.08 ± 0.04 

0.13 ± 0.05 

EVI 

BSI 

0.42 ± 0.37 

-0.25 ± 0.16 

1.67 ± 0.29 

0.47 ± 0.22 

1.93 ± 0.15 

0.16 ± 0.17 

0.19 ± 0.21 

0.25 ± 0.26 

-0.44 ± 0.15 

-0.81 ± 0.24 

0.14 ± 0.18 

-0.87 ± 0.11 

-0.09 ± 0.09 

-0.13 ± 0.14 

 4 / 8 

 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Landsat 8 

Bands 

B2 

B3 

B4 

B5 

B6 

B7 

Terbangun 
987.07 ± 
1121.24 
1262.32 ± 
1195.80 
1344.47 ± 
1249.32 
2558.76 ± 
1049.07 
2682.47 ± 
1579.35 
2157.47 ± 
1623.82 

Genangan Air 
342.50 ± 
138.56 
572.91 ± 
199.02 
515.92 ± 
255.37 
873.78 ± 
643.12 
376.22 ± 
317.99 
202.80 ± 
161.04 

Sawah 

261.36 ± 76.38 
560.49 ± 
101.91 
406.74 ± 
130.41 
3152.31 ± 
475.66 
1373.53 ± 
250.24 
641.12 ± 
165.60 

Bera 
412.64 ± 
102.59 
689.66 ± 
131.05 
765.36 ± 
178.79 
1890.52 ± 
508.43 
1633.15 ± 
559.74 
1005.27 ± 
364.44 

Hutan 

213.09 ± 35.90 

478.53 ± 59.26 

262.86 ± 60.34 
3841.71 ± 
405.17 
1548.44 ± 
142.14 

595.77 ± 93.38 

Jalan Tol 
825.36 ± 
344.25 
1223.07 ± 
367.65 
1223.54 ± 
415.93 
2803.80 ± 
427.69 
2066.99 ± 
374.04 
1485.32 ± 
396.61 

Awan 
4297.08 ± 
1236.48 
4542.79 ± 
1264.13 
4623.30 ± 
1335.70 
5988.62 ± 
1145.48 
4917.95 ± 
1184.93 
3884.34 ± 
1017.31 

NDVI 

0.37 ± 0.19 

0.21 ± 0.25 

0.76 ± 0.10 

0.41 ± 0.11 

0.87 ± 0.04 

0.40 ± 0.14 

0.14 ± 0.08 

NDWI 

-0.40 ± 0.17 

-0.13 ± 0.24 

-0.69 ± 0.08 

-0.45 ± 0.09 

-0.78 ± 0.03 

-0.40 ± 0.12 

-0.15 ± 0.08 

NDBI 

0.00 ± 0.14 

-0.40 ± 0.11 

-0.39 ± 0.10 

-0.09 ± 0.12 

-0.42 ± 0.05 

-0.15 ± 0.07 

-0.11 ± 0.05 

NDTI 

0.14 ± 0.08 

0.27 ± 0.07 

0.37 ± 0.04 

0.24 ± 0.04 

0.45 ± 0.03 

0.17 ± 0.06 

0.12 ± 0.03 

EVI 

BSI 

0.69 ± 0.41 

0.39 ± 0.46 

1.66 ± 0.29 

0.74 ± 0.25 

2.00 ± 0.12 

0.73 ± 0.31 

0.23 ± 0.15 

0.01 ± 0.29 

-0.59 ± 0.14 

-0.88 ± 0.26 

-0.11 ± 0.23 

-1.02 ± 0.12 

-0.27 ± 0.17 

-0.14 ± 0.07 

Sentinel 2 

Landsat 8 

Gambar 3. Heatmap masing-masing band dan indeks dari Sentinel 2 (kiri) dan Landsat 8 (kanan) terhadap kelas 

Akan  tetapi,  bisa  dilihat  bahwa  sawah  dan  hutan  tidak 
mengikuti  aturan  tersebut  di  mana  keduanya  memiliki  nilai 
yang cukup mirip pada setiap band dan indeks komposit. Pada 
Sentinel  2,  walaupun  NDVI,  EVI  dan  NDTI  terlihat  mampu 
membedakan  sawah  dan  hutan  dari  kelas  lain  di  mana  nilai 
ketiga  indeks  tersebut  pada  kelas  sawah,  nilai  antara  kedua 
kelas  itu  sendiri  terlihat  tidak  berbeda  jauh.  Namun  bila 
diperhatikan  lebih  lanjut,  dapat  dilihat  bahwa  keduanya 
memiliki pola di mana hutan memiliki nilai yang lebih tinggi 
daripada sawah di hampir tiap band dan indeks selain NDWI. 
Meski perbedaan nilai antara sawah dan hutan, baik pada NDVI, 
EVI  maupun  NDWI  terlihat  tidak  berbeda  jauh,  dengan 
menggabungkan  informasi  ini,  mesin  mungkin  akan  mampu 
membedakan antara kedua kelas tersebut. Dengan begitu, bila 
nilai indeks suatu pixel lebih tinggi di EVI, tetapi lebih rendah 
di NDWI, kemungkinan besar pixel tersebut mewakili sawah. 

Sementara pada Landsat 8, antara sawah dan hutan terlihat jelas 
berbeda pada nilai NDTI. 

B. Identifikasi Model Terbaik Pengenalan Lahan Padi. 

Performa dari kedua model deep learning yang digunakan, 
serta  perbandingannya  dengan  model  random  forest  dapat 
dilihat  pada  Tabel  VI.  Tabel  tersebut  menunjukkan  hasil  10-
fold cross validation masing-masing model pada seluruh kelas. 
Dalam penelitian ini, model random forest digunakan sebagai 
pembanding  dari  model  deep  learning  yang  digunakan 
terhadap  model  machine  learning.  Tabel  VII  menunjukkan 
nilai  f1-score  dari  masing-masing  kelas  untuk  setiap  model 
yang  digunakan.  Jika  diperhatikan,  baik  dari  f1-score 
keseluruhan, maupun dari f1-score kelas sawah, CNN1D dan 
random 
lebih  baik 
dibandingkan  model  MLP,  namun  tidak  terdapat  perbedaan 
yang signifikan antara model CNN1D dan random forest.  

forest  menunjukkan  performa  yang 

 5 / 8 

 
 
 
 
 
 
 
 
 
 
TABEL VI 
PERBANDINGAN PERFORMA MODEL MENGGUNAKAN 10-FOLD 
CROSS VALIDATION. 

F1 Score 

Precision 

Recall 

S2 
94.68% 
97.03% 

L8 
91.13% 
93.21% 

S2 
95.19% 
97.16% 

L8 
92.95% 
94.28% 

S2 
94.18% 
96.89% 

L8 
89.19% 
92.17% 

96.64% 

92.23% 

96.64% 

92.54% 

96.64% 

92.21% 

MLP 
CNN 
1D 
Random 
Forest 

Dapat  dilihat  pula  dari  kedua  tabel  tersebut  bahwa  model 
yang dijalankan pada Sentinel 2 menunjukkan performa yang 
lebih  baik  bila  dibandingkan  dengan  model  yang  dijalankan 
pada  Landsat  8.    Hal  yang  bisa  menyebabkan  hal  ini  adalah 
resolusi yang digunakan, di mana Sentinel 2 memiliki resolusi 
10  meter,  sementara  Landsat  8  memiliki  resolusi  30  meter.  
Satu hal yang perlu diperhatikan adalah bahwa nilai band yang 
diterima Sentinel 2 dan Landsat 8 berbeda walaupun keduanya 
menggunakan wavelength yang hampir sama, meskipun begitu, 
setelah data di normalisasi, nilai keduanya akan sangat mirip, 
selain  pada  SWIR  1  dan  2.  Landsat  8  dan  Sentinel  2 
menggunakan wavelength yang berbeda untuk SWIR 1 dan 2 
yang  mengakibatkan  perbedaan  nilai  pada  indeks  komposit 
yang memanfaatkan band tersebut, dalam kasus ini NDTI 
itu,  kesalahan  klasifikasi  yang 

terjadi  dapat 
diperhatikan dari Gambar 4. Kesalahan klasifikasi kelas sawah 

Selain 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

dapat berupa kelas sawah yang diklasifikasikan sebagai kelas 
lain,  maupun  sebaliknya.  Kelas  lain  yang  paling  banyak 
diklasifikasikan sebagai sawah adalah kelas hutan, sementara 
kelas sawah paling banyak juga salah diklasifikasikan sebagai 
hutan.  Nilai f1-score kelas sawah sendiri paling rendah berada 
di  84.15%  pada  model  MLP  Landsat  8  dan  paling  tinggi  
sebesar  95.43%  pada  model  CNN1D  Sentinel  2.  Meski 
demikian, nilai tersebut tidak berbeda  jauh dari hasil  random 
forest  pada  Sentinel  2.  Ini  menunjukan  bahwa  performa 
CNN1D  tidak  hanya  lebih  baik  pada  keseluruhan  kelas,  tapi 
juga  khusus  pada  kelas  sawah  yang  merupakan  fokus  dari 
penelitian ini. 

TABEL VII 
PERBANDINGAN F1-SCORE MASING-MASING KELAS UNTUK TIAP 
MODEL 

MLP 

CNN1D 

Random Forest 

S2 

L8 

S2 

L8 

S2 

L8 

Terbangun 

93.05% 

91.59% 

94.72% 

92.31% 

94.18% 

87.17% 

Air 

Sawah 

Bera 

Hutan 

99.79% 

93.99% 

100.00% 

96.17% 

99.90% 

99.38% 

90.98% 

84.15% 

95.43% 

87.96% 

95.16% 

92.31% 

93.95% 

89.74% 

96.54% 

91.92% 

96.74% 

92.83% 

96.00% 

96.15% 

98.23% 

97.00% 

98.46% 

96.45% 

Jalan Tol 

79.45% 

80.29% 

89.22% 

86.76% 

89.82% 

84.13% 

Awan 

97.71% 

97.72% 

98.75% 

98.06% 

98.78% 

97.38% 

Sentinel 2 

Landsat 8 

Random Forest 

 6 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Multi-Layer Perceptron  

CNN1D  

C. Estimasi Luas Lahan Tanam Padi 

Gambar 4. Confusion Matrix dari masing-masing model 

Berdasarkan  hasil  estimasi  menggunakan  CNN1D,  luas 
lahan  tanam  berdasarkan  citra  satelit  Sentinel  2  seluas 
66.230,51 hektar atau lebih luas 1.164,47 hektar (1,758%) bila 
dibandingkan  dengan  hasil  estimasi  KSA.  Nilai  ini  diperoleh 
dengan membandingkan proporsi pixel yang diidentifikasikan 
sebagai sawah terhadap luas kabupaten Nganjuk. Selain karena 
terdapat perbedaan resolusi spasial di mana KSA menggunakan 
sub  segmen  seluas  100 meter.  Faktor  yang  mungkin  menjadi 
penyebab perbedaan hasil estimasi ini adalah terdapatnya awan 
pada  citra  satelit.  Meskipun  telah  disediakan  kelas  tersendiri 
untuk awan agar dapat dilakukan estimasi berdasarkan proporsi 
lahan.  Ditemukan  bahwa  bayangan  dari  awan  dapat  cukup 
berpengaruh dalam proses klasifikasi oleh model. Sebagaimana 
diperlihatkan  pada  Gambar  5,  yang  menunjukkan  sebagian 
wilayah  dengan  awan  dan  bayang-bayang  awan,  model 
CNN1D  dan  random 
tampak  kesulitan  dalam 
memprediksikan daerah yang tertutup bayangan awan di mana 
random 
sebagai  daerah 
terbangun  sementara  CNN1D  mengklasifikasikannya  sebagai 
genangan air. 

forest  mengklasifikasikannya 

forest 

Real Color 

Random Forest 

MLP 

CNN1D 

Gambar 5. Visualisasi hasil segmentasi citra satelit sebagian wilayah 
Kabupaten Nganjuk. 

 7 / 8 

 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Landsat-8 Data and Sentinel-1A Data,” IEEE J. Sel. Top. Appl. Earth Obs. 
Remote  Sens.,  vol.  9,  no.  6,  pp.  2500–2508,  2016,  doi: 
10.1109/JSTARS.2016.2560141. 

[7]  I.  M.  Parsa,  D.  Dirgahayu,  S.  Harini,  and  K.  T.  S,  “Analisis  Citra 
Multiresolusi  Untuk 
Identifikasi  Konversi  Lahan  Sawah  Akibat 
Pembangunan Jalan Bebas Hambatan Di Kabupaten Subang , Jawa Barat 
(  Multiresolution  Image  Analysis  for  Identification  of  Paddy  Field 
Conversion Due To the Highway Development in S,” vol. 17, no. 1, pp. 
21–32, 2020. 

[8] M. Parsa, D. Dirgahayu, S. Harini, D. Kushardono, R. Sensing, and P. Rebo, 
“OPTIMIZATION  OF  A  RICE  FIELD  CLASSIFICATION  MODEL 
BASED  ON  THE  THRESHOLD  INDEX  OF  MULTI-TEMPORAL 
LANDSAT,” vol. 17, no. 1, pp. 75–84, 2020. 

[9]  Rokhmatuloh  et  al.,  “Paddy  field  mapping  using  UAV  multi-spectral 
imagery,”  Int.  J.  GEOMATE,  vol.  17,  no.  61,  pp.  242–247,  2019,  doi: 
10.21660/2019.61.icee408. 

[10] Widiatmaka, W. Ambarwulan, P. B. K. Santoso, S. Sabiham, Machfud, and 
M. Hikmat, “Remote Sensing and Land Suitability Analysis to Establish 
Local Specific Inputs for Paddy Fields in Subang, West Java,”  Procedia 
doi: 
Sci., 
Environ. 
10.1016/j.proenv.2016.03.061. 

94–107, 

2016, 

vol. 

pp. 

33, 

[11] D.  W.  Triscowati,  B.  Sartono,  A.  Kurnia,  D.  Dirgahayu,  and  A.  W. 
Wijayanto, “Classification of Rice-Plant Growth Phase Using Supervised 
Random Forest Method Based on Landsat-8 Multitemporal Data,” Int. J. 
Remote  Sens.  Earth  Sci.,  vol.  16,  no.  2,  p.  187,  2020,  doi: 
10.30536/j.ijreses.2019.v16.a3217. 

[12] D.  W.  Triscowati,  B.  Sartono,  A.  Kurnia,  D.  D.  Domiri,  and  A.  W. 
Wijayanto, “Multitemporal remote sensing data for classification of food 
crops  plant  phase  using  supervised  random  forest,”  vol.  1131102,  no. 
November 2019, p. 10, 2019, doi: 10.1117/12.2547216. 

[13] Badan  Pusat  Statistik,  “Luas  Panen  dan  Produksi  Padi  di  Provinsi  Jawa 
Timur  2019,”  Badan  Pus.  Statisik,  vol.  XVIII,  no.  21/03,  p.  Diakses  2 
Maret 2020, 2019. 

Nganjuk 

[14] BPS-Statistics  Indonesia,  “PDRB  ADHB  Menurut  Lapangan  Usaha 
2010─2020.” 
(Juta 

Kabupaten 
https://nganjukkab.bps.go.id/statictable/2021/02/15/268/pdrb-adhb-
menurut-lapangan-usaha-2010-2020.html (accessed Mar. 26, 2021). 
[15] P. Helber, B. Bischke, A. Dengel, and D. Borth, “Eurosat: A novel dataset 
and deep learning benchmark for land use and land cover classification,” 
IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., vol. 12, no. 7, pp. 2217–
2226, 2019, doi: 10.1109/JSTARS.2019.2918242. 

Rupiah), 

[16] L.  Breiman,  “Random  forests,”  Random  For.,  pp.  1–122,  2001,  doi: 

10.1201/9780429469275-8. 

[17] W. Jiang et al., “Multilayer perceptron neural network for surface water 
extraction in landsat 8 OLI satellite images,” Remote Sens., vol. 10, no. 5, 
pp. 1–22, 2018, doi: 10.3390/rs10050755. 

[18] W. Hu, Y. Huang, L. Wei, F. Zhang, and H. Li, “Deep convolutional neural 
networks  for  hyperspectral  image  classification,”  J.  Sensors,  vol.  2015, 
2015, doi: 10.1155/2015/258619. 

[19] G.  Polančič,  “Empirical  Research  Method  Poster.”  Maribor,  Slovenia, 

2007. 

[20] M. Berndtsson, J. Hansson, B. Olsson, and B. Lundell, Thesis projects: A 
guide for students in computer science and information systems: Second 
edition, 2nd ed. London: Springer, 2008. 

[21] G.  Astaoui,  J.  E.  Dadaiss,  I.  Sebari,  S.  Benmansour,  and  E.  Mohamed, 
“Mapping  Wheat  Dry  Matter  and  Nitrogen  Content  Dynamics  and 
Estimation  of  Wheat  Yield  Using  UAV  Multispectral  Imagery  Machine 
Learning  and  a  Variety-Based  Approach:  Case  Study  of  Morocco,” 
AgriEngineering, 
doi: 
10.3390/agriengineering3010003. 

29–49, 

2021, 

vol. 

pp. 

no. 

1, 

3, 

A. Kesimpulan 

VII. 

PENUTUP 

Dalam  penelitian  ini,  simulasi  dan  eksperimen  mengenai 
pemanfaatan  deep  learning  dalam  melakukan  pendeteksian 
area  tanam  padi  pada  citra  satelit  resolusi  menengah.  Dari 
simulasi  yang  dijalankan,  dapat  disarikan  beberapa  poin 
penting  yakni:  (1)  fitur  yang  digunakan  sudah  dapat 
membedakan  kelas  dengan  cukup  baik,  terutama  Normalized 
Difference  Vegetation  Index  dan  Enhanced  Vegetation  Index 
dalam  membedakan  kelas  sawah;  (2)  model  Convolution 
Neural  Network  1  Dimension  memberikan  performa  terbaik, 
baik pada seluruh kelas yang digunakan maupun khusus pada 
kelas  sawah  dengan  nilai  f1-score  sebesar  97,16%,  dan  hasil 
klasifikasi  pada  citra  satelit  Sentinel  2  yang  memiliki  spatial 
resolution lebih tinggi menunjukkan performa yang lebih baik; 
(3)  dengan  mengasumsikan  bahwa  hasil  Kerangka  Sampel 
Area merupakan representasi dari  ground truth, estimasi luas 
lahan  tanam  padi  pada  citra  satelit  resolusi  menengah 
menggunakan deep learning menunjukkan akurasi yang cukup 
baik yaitu seluas 66.230,51 hektar. 
B. Saran 

Hal  yang  perlu  dipertimbangkan  adalah,  karena  pada 
penelitian ini menggunakan citra satelit Sentinel 2 dan Landsat 
8,  band  yang  digunakan  hanyalah  band  yang  terdapat  pada 
kedua  satelit.  Sentinel  2  masih  memiliki  beberapa  band yang 
tidak  digunakan  dalam  penelitian  ini.  Band  red-edge  yang 
terdapat  pada  Sentinel  2  menunjukkan  korelasi  antara  indeks 
sangat 
komposit  dengan  karakteristik  vegetasi  yang 
dipengaruhi  dengan  band 
lain  yang  digunakan  sebagai 
kombinasi  [21].  Selain  itu,  penelitian  ini  berfokus  pada 
mempelajari 
pixel, 
direkomendasikan  juga  untuk  mencoba  model  deep  learning 
lain  seperti  Convolution  Neural  Network  2  Dimension  yang 
dapat  menggali  fitur  spasial  seperti  distribusi  warna  yang 
membuat model ini lebih robust pada data dengan fitur spasial. 
Untuk  penelitian  berikutnya  dapat  berfokus  pada  usia  tanam 
dan  produktivitas  tanaman  padi,  sesuai  kebutuhan  publikasi 
Statistik Pertanian dan perlu dipertimbangkan juga penggunaan 
data multi temporal. 

tingkatan 

spektral 

pada 

fitur 

DAFTAR PUSTAKA 
[1]  BPS-Statistics Indonesia, Executive summary of paddy harvested area and 

production in Indonesia 2019. 2020. 

[2]  BPS-Statistics  Indonesia,  “Pedoman  Pelaksanaan  Uji  Coba  Sistem 
Available: 

Kerangka 
https://sirusa.bps.go.id/webadmin/pedoman/2016_3431_ped_Pedoman 
Pelaksanaan Uji Coba Kerangka Sampel Area (KSA)2015.pdf. 

[Online]. 

Sampel 

Area,” 

2015, 

[3]  Di.  Ienco,  R.  Gaetano,  C.  Dupaquier,  and  P.  Maurel,  “Land  Cover 
Classification via Multitemporal Spatial Data by Deep Recurrent Neural 
Networks,”  IEEE Geosci. Remote Sens. Lett., vol. 14, no. 10, pp. 1685–
1689, 2017, doi: 10.1109/LGRS.2017.2728698. 

[4]  N.  Jean,  M.  Burke,  M.  Xie,  W.  M.  Davis,  D.  B.  Lobell,  and  S.  Ermon, 
“Combining  satellite  imagery  and  machine  learning  to  predict  poverty,” 
Science  (80-.  ).,  vol.  353,  no.  6301,  pp.  790–794,  2016,  doi: 
10.1126/science.aaf7894. 

[5]  H. L. Fitriana, S. Suwarsono, E. Kusratmoko, and S. Supriatna, “Mapping 
Burnt  Areas  Using  the  Semi-Automatic  Object-Based  Image  Analysis 
Method,” Int. J. Remote Sens. Earth Sci., vol. 17, no. 1, p. 57, 2020, doi: 
10.30536/j.ijreses.2020.v17.a3281. 

[6]  N. Kussul, G. Lemoine, F. J. Gallego, S. V. Skakun, M. Lavreniuk, and A. 
Y.  Shelestov,  “Parcel-Based  Crop  Classification  in  Ukraine  Using 

 8 / 8 

 
 
 
 
 
"
221710026,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Implementasi Algoritma Support Vector Machine 
dalam Penentuan Tingkat Kemacetan pada Data 
Jejaring Sosial Twitter 
Studi Kasus: Provinsi DKI Jakarta 

Syifa Rizqi Qatrunnada (221710026, 4 SD 2) 
Dosen Pembimbing: Budi Yuniarto, SST, M.Si 

zaman 

sekarang 

Ringkasan—  Masyarakat 

sering 
mencurahkan  keluhan  mengenai  permasalahan  yang  mereka 
hadapi  melalui  sosial  media,  salah  satunya  adalah  sosial  media 
Twitter  .  Setiap  tweet  publik  yang  diekspos  dapat  direkam  oleh 
Twitter,  sehingga  datanya  dapat  digunakan  untuk  dianalisis. 
Twitter dapat memberi kita berbagai jenis informasi. Salah satu 
informasi  yang  dapat  diperoleh  dari  Twitter  adalah  informasi 
tentang  kondisi  lalu  lintas.  Di  kota  besar  seperti  Jakarta, 
kemacetan lalu lintas menjadi persoalan yang kerap dibicarakan. 
Kemacetan  sangat  menghambat  masyarakat  dalam  melakukan 
mobilitas.  Tersendatnya  arus  lalu  lintas  dapat  memberikan 
dampak buruk terhadap perekonomian, terutama dari segi bahan 
bakar  yang  digunakan.  Warga  DKI  Jakarta  menggunakan 
Twitter  sebagai  media  sosial  untuk  bertukar  informasi  tentang 
kondisi  lalu lintas.  Biasanya  warga  Jakarta menggunakan akun 
dan 
@TMCPoldaMetro,  @RadioElshinta,  @lewatmana, 
@SonoraFM92 untuk memberikan informasi tentang kondisi lalu 
lintas di sekitar mereka. 

Kata Kunci— Twitter, Kemacetan, Klasifikasi, SVM, Support 

Vector Machine 

I.  LATAR BELAKANG 

Infrastruktur  merupakan  salah  satu  bagian  penting  untuk 
mendukung  pertumbuhan  dan  perkembangan  ekonomi  pada 
suatu  negara.  Infrastruktur  yang 
tidak  memadai,  akan 
menghambat kemajuan dan kompetisi negara dibanding negara 
lainnya [1]. Dengan infrastruktur dan prasarana yang memadai, 
pemerintah  berharap  perekonomian  dan  kesejahteraan 
Indonesia pun menjadi semakin membaik.  

tentu 

tersebut 

Pembangunan infrastruktur, khususnya sarana dan prasarana 
topik  yang  menarik  perhatian 
transportasi,  merupakan 
masyarakat.  Pembangunan 
telah  banyak 
memberikan  dampak  pada  masyarakat,  baik  dari  sisi  positif 
maupun  negatif.  Dampak  positif 
terkait  pembangunan 
infrastruktur diantaranya berupa kemudahan dalam mobilisasi 
masyarakat untuk memenuhi kebutuhan hidupnya, terbukanya 
jalur  alternatif  dan  terbukanya  lapangan  pekerjaan  baru. 
Adapun  dampak  negatif  dari  pembangunan,  diantaranya 
berkurangnya  lahan  pertanian  dan  hutan,  retribusi  yang  tidak 
dipergunakan dengan baik, dan terjadinya kemacetan. [2] 

Dengan  adanya  pembangunan,  maka  sejatinya  masyarakat 
akan  berpindah  ke  daerah  yang  lebih  layak.  Kota-kota  besar 
yang memiliki pembangunan yang baik akan menjadi magnet 
untuk  masyarakat  dalam  mencari  pekerjaan.  Jika  terjadi 

urbanisasi  yang  semakin 
terkendali,  maka  akan 
menimbulkan  berbagai  masalah  baru,  salah  satunya  adalah 
kemacetan. [3] 

tidak 

Di kota besar seperti Jakarta, kemacetan lalu lintas menjadi 
persoalan  yang  kerap  dibicarakan.  Kemacetan 
sangat 
menghambat  masyarakat  dalam  melakukan  mobilitas. 
Tersendatnya arus lalu lintas dapat memberikan dampak buruk 
terhadap  perekonomian,  terutama  dari  segi  bahan  bakar yang 
digunakan. Tak lupa juga bahwa kemacetan berdampak buruk 
bagi  lingkungan  dan  kesehatan  karena  dapat  menyebabkan 
masalah  polusi.  Jika  hal  ini  tidak  bisa  diminimalisasi, 
kemacetan lalu lintas bisa menjadi masalah serius karena dapat 
menimbulkan  berbagai  dampak  negatif,  seperti  pemborosan 
bahan bakar, pemborosan waktu, dan polusi udara.  

Berdasarkan  penelitian  yang  dilakukan  oleh  Rohana 
Sitanggang dan  Euis  Saribanon  (2018),  kemacetan  lalu  lintas 
dapat  disebabkan  oleh  beberapa  faktor.  Faktor-faktor  yang 
menyebabkan  kemacetan  lalu  lintas  dapat  berasal  dari  segi 
penggunaan kendaraan pribadi, kapasitas jalan, pengguna jalan, 
terdapat  pembangunan  infrastruktur.  Kemacetan  yang  terjadi 
dari  segi  penggunaan  kendaraan  pribadi  sering 
terjadi 
diakibatkan  jumlah  penumpang  yang  kurang  dari  tiga  orang, 
bahkan  ada  yang  diisi  oleh  pengemudinya  saja.  Hal  tersebut 
tentunya termasuk ke dalam pemborosan ruas jalan. Kapasitas 
jalan  yang  tidak  proporsional  dengan  jumlah  kendaraan 
biasanya  menjadi  penyebab  kemacetan  lalu  lintas,  karena 
kapasitas jalan raya tidak dapat menampung semua kendaraan 
yang ada, sehingga kemacetan tidak dapat dihindari. [4] 

lalu 

jalan,  baik 

lintas.  Perilaku  pengemudi  yang 

Menurut  Rohana  Sitanggang  dan  Euis  Saribanon  (2018), 
manusia  sebagai  pengguna 
itu  pengemudi, 
penumpang atau pejalan kaki, masih belum disiplin dan tidak 
mematuhi  aturan  lalu  lintas,  ini  salah  satu  faktor  penyebab 
kemacetan 
tidak 
mematuhi  peraturan  rambu  lalu  lintas  yang  ada,  seperti 
melampaui  batas  kecepatan  yang  diizinkan,  menaikkan  dan 
menurunkan  penumpang  tidak  pada  tempatnya,  melanggar 
rambu jalan, menunggu penumpang di sembarang tempat, salah 
dalam  menggunakan  lampu  belok  kanan  atau  belok  kiri, 
melawan  arus,  dan 
  mengabaikan 
keselamatan  diri  dan  penumpang.  Adanya  pembangunan 
infrastruktur  seperti  MRT,  LRT,  Flyover,  Underpass,  dan 
Trotoar juga menjadi penyebab kemacetan di Jakarta. [4] 

lainnya  seringkali 

Berdasarkan  data  publikasi  BPS  yang  bertajuk  ’Jakarta 
Dalam Angka 2017’ [5], jumlah penduduk di jakarta berada di 

 1 / 7 

 
 
 
 
 
angka  10.277.628  sedangkan  jumlah  kendaraan  bermotor  di 
Provinsi DKI Jakarta telah mencapai 18.006.404 juta. Menurut 
BPS, pertumbuhan kendaraan per tahunnya sebesar 5,35%. Hal 
tersebut memberikan dampak kepadatan lalu lintas akibat tidak 
selarasnya  dengan  perkembangan  ruas  jalan  raya  di  DKI 
Jakarta yang mana pertumbuhan jalannya hanya sebesar 0,01%. 
[6] 

Saat  ini,  internet  telah  menjadi  kebutuhan  bagi  sebagian 
besar  masyarakat.  Pengguna 
internet  didominasi  oleh 
masyarakat  yang  tinggal  di  kota-kota  besar,  seperti  Jakarta. 
Kehadiran 
internet  sendiri  memberikan  pengaruh  bagi 
penggunanya,  baik  positif  maupun  negatif.  Internet  mampu 
menciptakan ruang lingkup baru dengan jangkauan yang sangat 
jenis  media  sosial  yang  mampu 
luas  dengan  berbagai 
mempercepat  penyebaran  berita  atau  kondisi 
terkini. 
Masyarakat kerap kali menggunakan internet, khususnya media 
sosial,  sebagai  wadah  dalam  penyampaian  aspirasi  bahkan 
keluhan yang dialaminya. Tak heran jika masyarakat sering kali 
menyampaikan  pendapat  mereka  terkait  hal  yang  dialaminya 
melalui  berbagai  jejaring  media  sosial,  salah  satunya  melalui 
Twitter. [7] 

Twitter  merupakan  salah  satu  media  sosial  yang  dapat 
dijadikan  wadah  bagi  pengguna  untuk  menuangkan  isi 
pikirannya  terkait  topik  tersebut.  Media  sosial  ini  merupakan 
salah satu media sosial yang sering digunakan oleh masyarakat 
Indonesia.  Indonesia  sendiri  menduduki  peringkat  5  sebagai 
negara  pengguna  twitter  terbanyak  [8].  Twitter  memberikan 
suatu  fitur  bagi  penggunanya  dalam  melakukan  penggalian 
informasi  terhadap  topik  tertentu.  Salah  satu  fitur  yang 
diberikan  adalah  fitur  ’pencarian’  atau  yang  biasa  kita  kenal 
sebagai fitur ’search’. 

Text  mining  merupakan  pengambilan  informasi  dari  teks 
antara  lain  dapat  meliputi  kategorisasi  teks  atau  dokumen, 
analisis  sentimen  (sentiment  analysis),  pencarian  topik  yang 
lebih spesifik (search engine), serta spam filtering [9]. Dengan 
menggunakan twitter API, kita dapat melakukan analisis data 
dari tweet pengguna, salah satunya yaitu dengan text mining. 

Berdasarkan  tweet  di  media  sosial  (dalam  hal  ini  Twitter), 
kita dapat menggunakan analisis sentimen untuk menganalisis 
pandangan  orang  terhadap  suatu  topik.  Analisis  sentimen 
adalah  proses  memahami,  mengekstrak,  dan  mengolah  data 
teks secara otomatis untuk mendapatkan informasi emosional 
yang  terkandung  dalam  kalimat.  Pendapat dan konsep  terkait 
seperti sentimen, evaluasi, sikap, dan emosi adalah subjek studi 
analisis  sentimen  (sentiment  analysis)  dan  penggalian  opini 
(opinion  mining)  yang  mana  kedua  subjek  studi  tersebut 
berfokus  pada  opini yang  mengekspresikan  atau menyiratkan 
sentimen positif atau negatif [10]. Dalam penelitian ini, emosi 
masyarakat yang tertuang dalam Twitter akan diklasifikasikan 
untuk menentukan apakah terdapat kemacetan atau tidak. Salah 
satu penelitian yang dijadikan sebagai acuan yaitu jurnal yang 
berjudul  Klasifikasi  Kemacetan  Lalu  Lintas  Kota  Malang 
Melalui  Media  Twitter  Menggunakan  Metode  Neighbor 
Weighted  K-Nearest  Neighbor  (NW-KNN)  yang  ditulis  oleh 
Putu  Amelia  Vennanda  Widyaswari,  dkk  [11].  Penelitian 
tersebut  bertujuan  untuk  mengklasifikasikan  kemacetan  lalu 
lintas  berdasarkan  tweet  yang  ada  di  media  sosial  Twitter 
dengan menggunakan metode K-Nearest Neighbor.  

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

II.  TUJUAN PENELITIAN 

Tujuan dari penelitian ini adalah sebagai berikut: 
1.  Mengklasifikasikan  kemacetan  lalu  lintas  di  Jakarta 

berdasarkan tweet pengguna Twitter. 

2.  Melihat pola waktu terjadinya kemacetan lalu lintas di 
DKI  Jakarta  berdasarkan  data  kemacetan  dari  media 
sosial twitter. 

III. PENELITIAN TERKAIT 

Bagian  ini  berisi  uraian  hasil  penelitian  sebelumnya  yang 

berkaitan serta uraian relevansinya dengan topik skripsi. 

TABEL I 
TABEL LITERATUR 
Tertulis 

No 

Judul 

1  Studi 

Literatur 
Tentang 
Perbandingan 
Metode Untuk 
Proses 
Analisis 
Sentimen di 
Twitter 

2  Klasifikasi 
Kemacetan 
Lalu Lintas di 
Kota Malang 
Pada Sosial 
Media Twitter 
Menggunakan 
Metode 
Improved K-
Nearest 
Neighbor 

Penulis, 
Publikasi 
Nurrun 
Muchammad 
Shiddieqy 
Hadna dkk., 
Seminar 
Nasional 
Teknologi 
Informasi dan 
Komunikasi 
2016 
(SENTIKA 
2016) 
Yogyakarta, 
18-19 Maret 
2016 
Andjar 
Prasetyo, 
Jurnal 
Pengembangan 
Teknologi 
Informasi dan 
Ilmu 
Komputer, 
Vol. 3, No. 2, 
Februari 2019 

3  Klasifikasi 
Kemacetan 
Lalu Lintas 
Kota Malang 
Melalui 
Media Twitter 
Menggunakan 
Metode 
Neighbor 
Weighted K-
Nearest 

Putu Amelia 
Vennanda 
Widyaswari, 
dkk., Jurnal 
Pengembangan 
Teknologi 
Informasi dan 
Ilmu 
Komputer, 
Vol. 3, No. 2, 
Februari 2019 

Kata-kata yang 
kurang penting 
(Seperti “di-”, 
“ke-”, “dan”) 
tidak perlu 
diperhitungkan. 

Data diambil 
melalui akun 
twitter 
@PuspitaFM, 
karena 
masyarakat Kota 
Malang saling 
berbagi 
informasi 
tentang keadaan 
lalu lintas 
disana. Peneliti 
menyebutkan 
bahwa improved 
KNN memiliki 
akurasi yang 
lebih tinggi 
disbanding KNN 
biasa, namun 
perbedaannya 
tidak begitu 
besar. 
Tidak semua 
tweet pada 
Twitter memiliki 
arti yang jelas, 
masih terdapat 
beberapa tweet 
yang memiliki 
kata ambigu 
seperti 
penggunaan kata 
‘ramai’. Kata 

Komentar 

Penulis 
memberikan 
contoh situs 
yang 
menggunakan 
analisis 
sentiment 
secara 
konkret.  

Penulis 
mencantumka
n diagram alir 
yang dapat 
membantu 
pembaca lebih 
memahami 
tahapan 
penelitian.  

Peneliti 
menjelaskan 
secara rinci 
proses 
stemming. 
Penelitian ini 
menggunakan 
metode 
klasifikasi 
NW-KNN dan 
pembobotan 

 2 / 7 

 
 
 
 
 
Neighbor 
(NW-KNN) 

Sandi Fajar 
Rodiyansyah 
dan Edi 
Winarko, 
IJCCS, Vol. 6, 
No. 1, January 
2012 

4  Klasifikasi 
Posting 
Twitter 
Kemacetan 
Lalu Lintas 
Kota Bandung 
Menggunakan 
Naive 
Bayesian 
Classification 

‘ramai’ tidak 
menunjukkan 
apakah ruas 
jalan tersebut 
masuk ke dalam 
kategori macet 
atau tidak macet. 
Sehingga 
klasifikasi 
kemacetan pada 
tweet ini perlu 
dilakukan guna 
memudahkan 
pengguna 
menentukan 
kategori atau 
kondisi yang 
sebenarnya dari 
tweet tersebut. 
Dari data tweet 
yang telah 
terklasifikasikan
, kemudian data 
tersebut 
digunakan untuk 
divisualisasikan 
di peta kota 
Bandung. Data 
yang 
ditampilkan di 
peta adalah data 
jumlah tweet 
yang 
mengandung 
informasi 
kemacetan di 
suatu jalan.  

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

dengan 
metode TF-
IDF. 

Machine  (SVM)”  yang  menyebutkan  bahwa  akurasi  yang 
dimiliki oleh model ini sebesar 98.11% [13]. 

IV. METODE PENELITIAN  

4.1  Metode Pengumpulan Data 

ini 
Data  yang  digunakan  dalam  penelitian 
merupakan  data  yang  diambil  dari  tweet  akun  twitter 
@TMCPoldaMetro,  @RadioElshinta,  @lewatmana, 
dan  @SonoraFM92  serta  tweet  yang  menyebutkan 
akun-akun  tersebut.  Proses  scraping  menggunakan 
package  python  yang  bernama  Twint.  Periode  tweet 
yang  diambil  menggunakan  library  Twint  yaitu  dari 
tanggal  1  Maret  2020  hingga  31  Maret  2020.  Hasil 
scraping  menggunakan 
library  Twint  memiliki 
beberapa  atribut,  antara  lain  berupa  tanggal,  waktu, 
username, dan tweet itu sendiri. Data hasil scraping itu 
kemudian disimpan dengan format file csv. 

4.2  Data Selection 

kata 

Tweet hasil scraping dari keempat akun yang telah 
disebutkan di atas tidak semuanya mengandung kondisi 
lalu lintas. Maka, dilakukan proses seleksi data dengan 
yang 
menggunakan 
menggambarkan kondisi lalu lintas. Adapun kata kunci 
yang  mengandung  kondisi 
jalan  adalah  macet, 
tersendat  (kata  dasar  sendat),  terhambat  (kata  dasar 
hambat), padat, pamer (padat merayap), ramai, ramlan 
(ramai  lancar),  lancar,  dan  lengang.  Jumlah  data  dari 
hasil seleksi tersebut berjumlah 11.155 tweet. 

(keyword) 

kunci 

4.3  Preprocessing 

Preprocessing digunakan untuk memastikan bahwa 
data yang akan diolah merupakan data yang konsisten 
dan  tidak  ada  nilai  yang  hilang.  Tahapan  praproses 
dalam penelitian ini adalah sebagai berikut [14]:  
1.  Cleansing 

Tahap  cleansing  dilakukan  untuk  mengurangi 
noise.  Pada  proses  ini  dilakukan  penghilangan 
username  (@username),  simbol  tanda  baca,  link 
(HTML dan URL), simbol hashtag (#hastag), dan 
angka. 
2.  Case folding 

Penulis 
membuat alur 
metode 
penelitian 
yang jelas. 
Saran peneliti, 
salah satunya 
dikatakan 
bahwa 
diperlukan 
pengaturan 
ulang  untuk 
menyesuaikan 
nama jalan 
kota dan 
koordinat 
jalan di 
Google Maps 
sehingga nama 
jalan dan 
koordinat 
sesuai dengan 
kota. 

tweet, 

Dalam 

terkadang 

yang 
menggunakan  huruf  besar,  huruf  kecil,  dan 
folding 
kombinasi  keduanya.  Tahap 
merupakan  tahapan  penyetaraan  huruf  menjadi 
lower case (huruf kecil) semua.  

case 

ada 

Gambar 1. Literature Map Penelitian 

3.  Tokenizing 

Pada  penelitian  yang  telah  dilakukan  oleh  Rustam,  dkk. 
(2012)  dengan  judul  “Pendeteksian  Jenis  dan  Kelas  Aroma 
dengan Menggunakan Metode One-Vs-One dan Metode One-
Vs-Rest”,  pengklasifikasian  menggunakan  SVM  memiliki 
akurasi sebesar 100% [12]. Rachman dan Purnami (2012) juga 
telah  melakukan  penelitian  dengan  judul  “Perbandingan 
Klasifikasi  Tingkat  Keganasan  Breast  Cancer  Dengan 
Menggunakan  Regresi  Logistik  Ordinal  Dan  Support  Vector 

Tokenisasi membuat daftar token / istilah / kata 
yang  akan  digunakan  sebagai  masukan  untuk 
metode  selanjutnya  [15].  Cara  termudah  adalah 
dengan  memisahkan  setiap  kata  dalam  dokumen 
dengan spasi. 

4.  Filtering 

 3 / 7 

 
 
 
 
 
 
 
 
 
 
 
Tahap 

ini  merupakan 

dimana 
penghilangan  kata  yang  termasuk  dalam  daftar 
stoplist dihilangkan. Stoplist berisi kata-kata yang 
tidak bermakna dan dianggap tidak penting. 

tahap 

5.  Stemming 

Stemming merupakan tahapan dilakukan proses 
pengubahan suatu kata menjadi kata dasar. Contoh: 
berjalan, menjadi ‘jalan’. 

4.4  Algoritma SVM 

Berdasarkan referensi proses pengklasifikasian dari 
penelitian sebelumnya, metode pengklasifikasian yang 
dapat digunakan dalam mengklasifikasikan kemacetan 
lalu  lintas  di  Jakarta  berdasarkan  data  jejaring  sosial 
twitter  adalah  dengan  menggunakan  metode  Support 
Vector  Machine.  Support  Vector  Machine  atau  SVM 
adalah 
untuk 
yang 
mengklasifikasikan  suatu  data.  Algoritma  tersebut 
melakukan klasifikasi objek dengan suatu teknik untuk 
menemukan hyperplane yang bisa memisahkan dua set 
data  dari  dua  kelas  yang  berbeda.  Adapun  proses 
pengklasifikasiannya  adalah  seperti  yang  disajikan 
pada gambar 2. Diagram Alir Pengklasifikasian Tweet. 

digunakan 

algoritma 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

4.5  Evaluasi Model 

Pengujian model dilakukan untuk mengetahui 
performa dari model klasifikasi SVM yang diukur 
dengan menghitung akurasi, presisi, recall, dan F1-
score. 

V.  KERANGKA PIKIR 

ramainya  pengguna 

Penelitian  bermula  dari 

twitter 
mencurahkan  keluh  kesah  mereka  di  media  sosial.  Namun, 
tidak  data  twitter  (tweet)  tidak  dapat  langsung  diolah  karena 
terdapat  tweet yang masih ambigu untuk menentukan apakah 
pengguna  twitter  menyebutkan  lokasi  ini  macet  atau  tidak. 
Pendekatan yang dilakukan dapat berupa pengklasifikasi tweet 
tentang  kemacetan  lalu  lintas  dengan  menggunakan  Support 
Vector Machine, serta membuat tingkat kemacetan berdasarkan 
perbandingan jumlah tweet yang dikategorikan macet dan tidak. 
Untuk tools, yang dibutuhkan adalah python, dengan package 
twint,  Pandas,  nltk,  Sastrawi,  pySastrawi,  dan  numpy  yang 
telah ter-install.  

Gambar 3. Kerangka Pikir Pengklasifikasian Tweet Kemacetan Lalu Lintas 

VI. HASIL DAN PEMBAHASAN 

6.1. Hasil Scraping dan Filtering Data 

Data  lalu  lintas  yang  dikumpulkan  berasal  dari 
akun 
twitter  @TMCPoldaMetro,  @RadioElshinta, 
@lewatmana,  dan  @SonoraFM92  serta  tweet  yang 
menyebutkan  akun-akun  tersebut.  Data  yang  diambil 
merupakan data pada bulan Maret 2020 yang berjumlah 
56.488  tweet.  Selanjutnya  data  tersebut  dilakukan 
proses  filtering,  sehingga  didapat  data  berjumlah 
11.155 tweet. 

6.2. Preprocessing 

Preprocessing yang dilakukan yaitu case folding, 

cleansing, tokenizing, filtering, dan stemming. 

Gambar 4. Proses Preprocessing 

 4 / 7 

Gambar 2. Diagram Alir Pengklasifikasian Tweet 

Arah Depok dari pasar Minggu padat, kemacetan dimulai dari jalan layang TB.simatupang sampai puteran Ranco lalu ke arah depok. Cc: @ElshintaLive @lewatmanaData dari TwitterData setelah preprocessingarah depok pasar minggu padat macet jalan layang tb simatupang puter ranco arah depok cc 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 8. Pola Kasus Kemacetan Berdasarkan Jam 

Berdasarkan  grafik 

tersebut,  dapat  dinyatakan 
bahwa sebagian besar kasus kemacetan di DKI Jakarta 
terjadi  pada  hari-hari  kerja  (Senin-Jumat),  terutama 
pada hari Senin. Kemacetan terjadi pada jam berangkat 
dan  pulang  kerja.  Kasus  kemacetan  cenderung 
meningkat  pada  pukul  05.00-08.00  pagi,  dan  pukul 
15.00-16.00. 

6.6. Proporsi Tweet Kemacetan (Harian) 

Proporsi 

tweet 

kemacetan 

dapat 

dihitung 

menggunakan rumus berikut: 

𝑃𝑟𝑜𝑝𝑜𝑟𝑠𝑖 𝐾𝑒𝑚𝑎𝑐𝑒𝑡𝑎𝑛 =  

𝐽𝑢𝑚𝑙𝑎ℎ 𝑡𝑤𝑒𝑒𝑡 
𝑦𝑎𝑛𝑔 𝑡𝑒𝑟𝑘𝑙𝑎𝑠𝑖𝑓𝑖𝑘𝑎𝑠𝑖 𝑚𝑎𝑐𝑒𝑡
𝐽𝑢𝑚𝑙𝑎ℎ 𝑡𝑤𝑒𝑒𝑡 𝑠𝑒𝑙𝑢𝑟𝑢ℎ𝑛𝑦𝑎

      (1) 

Dari  data  twitter,  dilakukan  proses  penyetaraan 
huruf menjadi huruf kecil, pengeluaran kata yang tidak 
bermakna,  pembentukan  menjadi  kata  dasar,  serta 
penghapusan simbol, tanda baca dan angka. 

6.3. Klasifikasi 

Gambar 5. Data yang Telah Diberi Label 

Data dari tweet dilabeli manual, dengan label tidak 
macet  sebanyak  8.604  tweet dan 2.551  tweet  berlabel 
macet. Kemudian dilakukan pembuatan model dengan 
proporsi 80% training dan 20% untuk testing. 

6.4. Evaluasi Model 

Pengujian  model  dilakukan  untuk  mengetahui 
performa  dari  model  klasifikasi  SVM  yang  diukur 
dengan  menghitung  akurasi,  presisi,  recall,  dan  F1-
score. 

Gambar 6. Data yang Telah Diberi Label 

Berdasarkan  evaluasi  yang  didapat,  model  dari 
SVM  ini  memiliki  akurasi  sebesar  99,24%,  recall 
sebesar 99,19%, presisi sebesar 97,40%, dan F1-score 
sebesar 98,29%. 

6.5. Pola Terjadinya Kemacetan 

Gambar 9. Grafik Tingkat Kemacetan Harian Bulan Maret 2020 

Gambar 7. Pola Kasus Kemacetan Berdasarkan Hari 

fluktuasi 

terkait  proporsi 

Proporsi  tweet  kemacetan  harian  tertinggi  pada 
bulan  Maret  2020  terjadi  pada  hari  Selasa,  3  Maret 
2020 dengan proporsi tweet sebesar 43,90. Sedangkan 
proporsi  tweet  kemacetan harian  terendah  pada  bulan 
Maret  2020  terjadi  pada  hari  Sabtu,  28  Maret  2020 
dengan  proporsi  tweet  sebesar  1,52.  Grafik  di  atas 
menggambarkan 
tweet 
kemacetan  lalu  lintas  berdasarkan  data  dari  Twitter. 
Pada  pertengahan  hingga  akhir  Bulan  Maret  2020, 
proporsi 
lintas  cenderung 
menurun.  Hal  ini  terjadi  karena  pada  tanggal-tanggal 
tersebut sudah ada beberapa peraturan untuk mencegah 
penyebaran  virus  corona,  salah  satunya  adalah 
meniadakan kegiatan belajar di Sekolah. Walau PSBB 
diadakan  pada  awal  Bulan  April,  namun  sudah  ada 
penurunan  proporsi  tweet  terkait  kemacetan  karena 
beberapa kebijakan di daerah DKI Jakarta. 

tweet  kemacetan 

lalu 

 5 / 7 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Gambar 10. Pola Kasus Kemacetan Berdasarkan Hari 

Berdasarkan  proporsi 

tweet  kemacetan  harian 
tertinggi,  Jakarta  Selatan  merupakan  kota  yang 
memiliki jumlah tweet terbanyak terkait informasi lalu 
lintas,  yaitu  sebesar  89  tweet  di  hari  tersebut.  Jakarta 
Selatan  juga  memiliki  proporsi  tweet  kemacetan 
tertinggi, yaitu sebesar 53,93. 

Sedangkan jumlah tweet terendah terkait lalu lintas 
dimiliki oleh kota Jakarta Barat, yaitu sebesar 24 tweet. 
Namun  untuk  proporsi  tweet  kemacetan  terendah 
dimiliki oleh Jakarta Utara, yaitu sebesar 18,75.. 

VII. 

PENUTUP 

Berdasarkan pemaparan hasil dan pembahasan sebelumnya, 

dapat disimpulkan bahwa: 

1.  Klasifikasi  menggunakan  Support  Vector  Machine 
dapat  bekerja  dengan  baik  untuk  mengklasifikasikan 
kemacetan di DKI Jakarta. 

2.  Tweet 

terkait  kemacetan  menunjukkan  bahwa 

kemacetan terjadi pada hari dan jam kerja. 

3.  Proporsi tweet terkait kemacetan harian tertinggi terjadi 
di  hari  Selasa,  3  Maret  2020  dengan  proporsi  tweet 
sebesar  43,90.  Sedangkan  proporsi  tweet  kemacetan 
harian terendah pada bulan Maret 2020 terjadi pada hari 
Sabtu,  28  Maret  2020  dengan  proporsi  tweet  sebesar 
1,52. 

4.  Berdasarkan proporsi tweet kemacetan harian tertinggi  
(3 Maret 2020), Jakarta Selatan merupakan kota yang 
memiliki jumlah tweet terbanyak terkait informasi lalu 
lintas  dan  juga  memiliki  proporsi  tweet  kemacetan 
tertinggi.  Di  hari  yang  sama,  tweet  terendah  dimiliki 
oleh kota Jakarta Barat, yaitu sebesar 24 tweet. Namun 
untuk proporsi tweet kemacetan terendah dimiliki oleh 
Jakarta Utara, yaitu sebesar 18,75. 

Adapun  saran  untuk  pengembangan  dari  penelitian  ini 

adalah sebagai berikut: 

1.  Akurasi  perwilayah  perlu  diperhitungkan. 

Jika 
langsung  diambil 

memungkinkan,  dalam  scraping 
longitude dan latitude-nya. 

2.  Penambahan  data  1  bulan  maju  dan  1  bulan  mundur 
untuk  melihat  perubahan  pola  kemacetan  lalu  lintas 
karena pandemi Covid-19. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

DAFTAR PUSTAKA 
[1]  S.  Fauziah  and  Nurwahidin,  “Pembiayaan  Infrastruktur  Dengan  Sukuk 
Negara  Di  Indonesia:  Prosedur  Dan  Struktur,”  Jurnal  Ilmiah  MEA 
(Manajemen, Ekonomi, dan Akuntansi), vol. 4, no. 1, pp. 30, 2020. 
[2]  V.  M.  Ompusunggu,  “Dampak  Pembangunan  Infrastruktur  Jalan 
Terhadap Pertumbuhan Ekonomi Masyarakat di Desa Semangat Gunung, 
Kabupaten  Karo,  Sumatera  Utara,”  JUPEKO  (Jurnal  Pendidikan 
Ekonomi), vol. 3, no. 2, 2018. 

[3]  F.  R.  Harahap,  “Dampak  Urbanisasi  bagi  Perkembangan  Kota  di 

Indonesia,” Jurnal Society, vol. 1, no. 1, Juni 2013. 

[4]  R. Sitanggang and E. Saribanon, “Faktor-Faktor Penyebab Kemaceran di 
DKI  Jakarta,”  Jurnal  Manajemen  Bisnis  Transportasi  dan  Logistik 
(JMBTI), col. 4, no. 3, Mei 2018. 

[5]  BPS  Provinsi  DKI  Jakarta,  Jakarta  dalam  Angka  2017,  Jakarta:  BPS 

Provinsi DKI Jakarta, 2017. 

[6]  A.  D.  Limantara,  L.  D.  Krisnawati,  S.  Winardi,  S.  W.  Mudjanarko, 
“Solusi  Pengawasan  Kebijakan  Mengatasi  Kemacetan  Jalan  dan  Parkir 
Kota Berbasis Internet Cerdas,” in Prosiding Seminar Nasional Teknologi 
dan Rekayasa Informasi Tahun 2017, pp. 1. 

[7]  Bimananda  W.,  I.  Riski,  K.  Dwi,  R.  Nooraeni,  T.,  and  S.  Y.  Dhea, 
“Analisis  Text  Mining  dari  Cuitan  Twitter  Mengenai  Infrastruktur  di 
Indonesia dengan Metode Klasifikasi Naïve Bayes,” Eigen Mathematics 
Journal, vol. 2, no. 2, pp. 92, Desember 2019. 

[8]  Admin Kominfo. (2012,11). Indonesia Peringkat Lima Pengguna Twitter. 
[Online]. Available: https://kominfo.go.id/content/detail/2366/indonesia-
peringkat-lima-pengguna-twitter/0/sorotan_media 

[9]  A. Z. Amrullah, A. S. Anas, and M. A. J. Hidayat,  “Analisis Sentimen 
Movie  Review  Menggunakan  Naive  Bayes  Classifier  Dengan  Seleksi 
Fitur Chi Square,” Jurnal Bumigora Information Technology, vol.2, no.1, 
pp. 40, Juni 2020. 

[10] B.  Liu,  Sentiment  Analysis  and  Opinion  Mining,  Morgan  &  Claypool 

Publishers, 2012. 

[11] P.  A.  V.  Widyaswari,  Indriati,  R.  S.  Perdana,  “Klasifikasi  Kemacetan 
Lalu Lintas Kota Malang Melalui Media Twitter Menggunakan Metode 
(NW-KNN),”  Jurnal 
Neighbor  Weighted  K-Nearest  Neighbor 
Pengembangan  Teknologi  Informasi dan  Ilmu  Komputer, vol.  3, no. 2, 
Februari 2019. 

[12] I.  F.  Yuliati,  S.  Wulandary,  and  P.  R.  Sihombing,  “Penerapan  Metode 
Support Vector Machine (SVM) dan Backpropagation Neural Network 
(BPNN) dalam Pengklasifikasian Pasangan Usia Subur di Jawa Barat,” 
Jurnal Statistika dan Aplikasinya (JSA), vol. 4, no. 1, Juni 2020. 

[13] F.  Rachman  and  S.  W.  Purnami,  “Perbandingan  Klasifikasi  Tingkat 
Keganasan Breast Cancer dengan Menggunakan Regresi Logistik Ordinal 
dan Support Vector Machine (SVM),” Jurnal Sains dan Seni ITS, vol. 1, 
no. 1, September 2012. 

[14] D.  Z.  Nathania,  Indriati,and  F.  A.  Bachtiar.  “Klasifikasi  Spam  Pada 
Twitter  Menggunakan  Metode  Improved  K-Nearest  Neighbor,”  Jurnal 
Pengembangan Teknologi Informasi dan Ilmu Komputer, vol. 2, no. 10, 
Oktober 2018. 

[15] A.  S.  Nayak  et  al.,  “Survey  on  Pre-Processing  Techniques  for  Text 
Mining,” International Journal of Advanced Trends in Computer Science 
and Engineering, vol. 5, no. 6, Juni 2016. 

[16] A. Perdana, M. T. Furqon, and Indriati, “Penerapan Algoritma Support 
Vector  Machine  (SVM)  Pada  Pengklasifikasian  Penyakit  Kejiwaan 
Skizofrenia  (Studi  Kasus:  RSJ.  Radjiman  Wediodiningrat,  Lawang),” 
Jurnal Pengembangan Teknologi Informasi dan Ilmu Komputer, vol. 2, 
no. 9, September 2018. 

[17] Bidang  Statistik  Distribusi  BPS  Provinsi  DKI  Jakarta,  “Transportasi 
Darat”,  Statistik  Transportasi  DKI  Jakarta  2017,  Ed.  Bidang  Statistik 
Distribusi  BPS  Provinsi  DKI  Jakarta,  Jakarta:  Badan  Pusat  Statistik 
Provinsi DKI Jakarta, 2018, pp. 14. 

[18]  P.  A.  Octaviani,  Y.  Wilandari,  and  D.  Inspriyanti,  “Penerapan  Metode 
Klasifikasi  Support  Vector  Machine  (SVM)  pada  Data  Akreditasi 
Sekolah Dasar (SD) di Kabupaten Magelang,” Jurnal Gaussian, vol. 3, 
no. 4, tahun 2014. 

[19]  R. D. Nurfarida, Indriati, and R. S. Perdana, “Klasifikasi Kemacetan Lalu 
Lintas di Kota Malang Pada Sosial Media Twitter Menggunakan Metode 
Improved  K-Nearest  Neighbor,”  Jurnal  Pengembangan  Teknologi 
Informasi dan Ilmu Komputer, vol. 3, no. 2, Februari 2019. 

[20]  S.  F.  Rodiyansyah  and  E.  Winarko,  “Klasifikasi  Posting  Twitter 
Kemacetan  Lalu  Lintas  Kota  Bandung  Menggunakan  Naive  Bayesian 
Classification,” IJCCS, vol. 6, no. 1, Januari 2012. 

 6 / 7 

KotaTweetProporsiJakarta Barat2429.17Jakarta Pusat8633.72Jakarta Selatan8953.93Jakarta Timur4837.50Jakarta Utara6418.75 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

[21]  S.  Prayoningsih  and  R.  P.  Kusumawardani,  “Klasifikasi  Data  Twitter 
Pelanggan  Berdasarkan  Kategori  myTelkomsel  Menggunakan  Metode 
Support Vector Machine (SVM) Studi Kasus: Telekomunikasi Selular,” 
Jurnal Sisfo, vol. 07, no. 02, Januari 2018. 

[22]  S.  Widiantoro,  “Optimalisasi  Alokasi  Dana  Desa  Pada  Pembangunan 
Infrastruktur  Sebagai  Upaya  Peningkatan  Kesejahteraan  Masyarakat 
Desa  Sumberdadap,”  Jurnal  Meta  Yuridis,  vol.  3,  no  2,  pp.  114, 
September 2020. 

[23]  V.N. Vapnik, The Nature of Statistical Learning, Berlin: Springer-Verlag, 

1999.  

[24]  Z.  A.  Syafani,  M.  Nasrun,  and  C.  Setianingsih,  “Klasifikasi  Tweet 
Kondisi Lalu Lintas Kota Jakarta dengan Penerapan Metode K-Nearest 
Neighbor,” Jurnal TEKTRIKA, vol. 3, no. 1, Januari 2018. 

[25] Z.  Rustam,  B.  Kusumoputro,  and  B.  Widjaja,  “Pendeteksian  Jenis  dan 
Kelas  Aroma  dengan  Menggunakan  Metode  One-Vs-One  dan  Metode 
One-Vs-Rest,” MAKARA SAINS, vol. 7, no. 3, Desember 2003. 

 7 / 7 

 
 
"
221710023,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Pembangunan Sistem Informasi Surat Perintah
Perjalanan Dinas Berbasis Web
(Studi kasus: BPS Kabupaten Jember)
Sultan Nabila Ravani (221710023, 4SD1)
Dosen Pembimbing: Ibnu Santoso, S.S.T., M.T.

tersebut

sehingga

kesalahan

input
dibutuhkan

rentan
permasalahan

Ringkasan— Surat Perintah Perjalanan Dinas

(SPPD)
merupakan surat pengantar yang dibuat ketika seorang pegawai
instansi pemerintah/swasta akan melakukan perjalanan dinas
yang dilakukan dalam kota atau luar kota. Pembuatan SPPD di
Badan Pusat Statistik (BPS) Kabupaten Jember masih dilakukan
dengan cara konvensional yaitu menggunakan Microsoft Word
dan Microsoft Excel. Proses ini dilakukan dengan mengambil
data-data yang sudah disiapkan pada Microsoft Excel,
lalu
data-data tersebut dilakukan mail merge ke dalam Microsoft
Word,
data.
terjadi
Berdasarkan
suatu
mekanisme yang dapat meningkatkan peluang efisiensi dan
efektivitas dalam pembuatan SPPD, yaitu berupa sebuah sistem
informasi yang terkomputerisasi dan terintegrasi dengan sistem
basis data yang dapat membantu kinerja pegawai di BPS
Kabupaten Jember agar lebih efektif dan efisien. Berdasarkan
Software Requirements Specification (SRS) yang disepakati,
sistem berhasil dibuat dengan lima tampilan pengguna sistem
yaitu admin, kepala BPS, pegawai, Pejabat Pembuat Komitmen,
dan bendahara. Sistem telah diuji oleh lima pegawai BPS
Kabupaten Jember yang pengujiannya dilakukan dengan
metode black box dan kuesioner System Usability Scale (SUS).
Dalam pengujian tersebut, diperoleh skor SUS 78,5 yang
menandakan bahwa sistem telah berfungsi dengan baik. Selain
itu, dengan ditandatanganinya surat pernyataan penerimaan
sistem oleh stakeholder, menunjukkan bahwa sistem sudah
diterima dan disetujui.

Kata Kunci— sistem informasi, web, SPPD.

I. LATAR BELAKANG

tidak terlepas dari

Badan Pusat Statistik adalah Lembaga Pemerintah Non
Kementerian yang bertanggung jawab langsung kepada
Presiden. Selain itu BPS memiliki perwakilan di setiap daerah
level kabupaten. Dalam menjalankan
di Indonesia sampai
kegiatannya
suatu proses
tentunya
administrasi, salah satu proses administrasi di BPS yaitu
proses pembuatan SPPD, pembuatan SPPD ini dilakukan
ketika ada seorang pegawai yang mendapatkan tugas tertentu
yang mengharuskan melakukan perjalanan dinas yang
melewati batas kota/kabupaten atau perjalanan dinas yang
dilaksanakan dalam kota/kabupaten. Pembuatan SPPD di BPS
dimulai dari pembuatan surat
tugas yang dilakukan oleh
satuan kerja dan diserahkan kepada pegawai yang telah
ditentukan, setelah pegawai tersebut menerima surat tersebut
pegawai
Setelah
pelaksanaan kegiatan pegawai membuat suatu laporan hasil
perjalanan dinas tersebut dan juga meminta penggantian biaya
yang telah digunakan selama perjalanan dinas lalu diserahkan
kepada bendahara untuk penggantian dana.

harus melaksanakan

kegiatannya.

Proses pembuatan SPPD ini masih dilakukan secara
konvensional dengan menggunakan Microsoft Word dan
Microsoft Excel sebagai dasar pembuatannya. Proses yang
dilakukan yaitu dengan mengambil data-data yang sudah
disiapkan pada Microsoft Excel lalu data-data tersebut di-mail
merge ke dalam Microsoft Word.

Dalam proses pembuatan SPPD tersebut, rentan terjadi
kesalahan input data. Berdasarkan hasil wawancara dengan
subject matter persentase kesalahan yang dapat terjadi yaitu
sekitar 5 persen. Kesalahan input ini dapat mengakibatkan
pembuatan SPPD membutuhkan waktu yang lebih lama,
selain itu SPPD dan informasi anggaran tidak diintegrasikan
dengan basis data
terdapat
informasi penting yang ada dalam surat-surat tersebut.

sistem manapun. Padahal,

Berdasarkan permasalahan tersebut, dibutuhkan suatu
mekanisme yang dapat meningkatkan peluang efisiensi dan
efektivitas dalam pembuatan SPPD yaitu berupa sebuah
sistem informasi yang terkomputerisasi dan terintegrasi
dengan sistem basis data sehingga dapat membantu kinerja
pegawai di BPS Kabupaten Jember agar lebih efektif dan
efisien

II. TUJUAN PENELITIAN
Tujuan dari penulisan skripsi ini adalah sebagai berikut:
1. Merancang sistem informasi manajemen SPPD di BPS

Kabupaten Jember.
2. Melakukan implementasi

rancangan
sistem informasi surat perintah perjalanan dinas di BPS
Kabupaten Jember.

terhadap hasil

3. Melakukan evaluasi dan uji coba sistem yang telah

dibangun.

III. PENELITIAN TERKAIT
TABEL I
TABEL LITERATUR

Penulis,
Publikasi
Saprina
Mamase. 2016.
Vol. 6 No.2

No

Judul

1 Rancang

Bangun Sistem
Informasi Surat
Perintah
Perjalanan
Dinas

Tertulis

Komentar

Dalam jurnal
tersebut di
buat suatu
sistem yang
menyimpan
data dan
mencetak
suatu surat
dalam format
pdf.

Persamaan
dengan
penelitian
penulis yaitu
dalam surat
yang ditelah
dibuat lalu
dicetak dalam
format pdf,
menggunakan
data-data

1 / 8

Dani Saepuloh.
2017. Vol. 3
No.2

2 Pembuatan
Sistem
Informasi Surat
Perintah
Perjalanan
Dinas Pada
Pusat Riset
Kelautan

Muh. Shamad.
2017

3 Pembangunan

Sistem
Informasi
Manajemen
Administrasi,
Monitoring,
Dan Evaluasi
Anggaran STIS

pada surat
yang
bersumber
dari
database.

Persamaan
dengan
penelitian
penulis yaitu
sistem untuk
mencetak
surat berbasis
web. Namun
metode
waterfall
yang
digunakan
dirasa sangat
kuno, maka
dari itu
penulis
menggunakan
metode yang
lebih baru
yaitu metode
prototype.

Persamaan
dengan
penelitian
penulis yaitu
sistem yang
dibangun
berbasis web,
dalam sistem
tersebut
terdapat
proses
pembuatan
SPD.

Dalam jurnal
tersebut telah
dibuat sistem
yang
berfungsi
membuat
surat berbasis
web, metode
yang
digunakan
dalam
penelitian ini
menggunakan
metode
waterfall.

Dalam skripsi
tersebut telah
dihasilkan
sistem
informasi
manajemen
administrasi,
monitoring,
dan evaluasi
anggaran.

IV. METODE PENELITIAN

ini

Dalam penelitian

digunakan metode

Systems
Development Life Cycle (SDLC) Model Prototype, prototype
adalah salah satu metode pengembangan software yang
mengizinkan pengguna sistem memiliki gambaran awal
tentang progam yang akan dikembangkan serta melakukan
pengujian awal. Alur metode ini dapat dilihat pada Gambar 1.

Gambar 1. Model Prototype

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Berikut merupakan tahapan-tahapan dalam pengembangan
tahapan tahapan

sistem yang dibuat dengan model SDLC,
tersebut adalah :
1.

Pengumpulan kebutuhan

Pengumpulan kebutuhan merupakan tahapan paling
awal dalam metode ini, pengembang dan subject matter
bersama-sama mendefinisikan sistem yang akan dibuat.
Selain itu pada tahap ini dikumpulkan juga data-data
baik hasil dari wawancara ataupun dokumen-dokumen,
data-data tersebut nantinya akan digunakan pada tahap
pengembangan selanjutnya.

2. Membuat prototype

pada

penyajian

Pada tahap ini dibuat perancangan sementara yang
berfokus
subject matter,
Prototype yang dibuat disesuaikan dengan kebutuhan
dari
sistem yang
kebutuhan subject matter.
Evaluasi prototype

didefinisikan

sebelumnya

kepada

telah

3.

Pada tahap ini dilakukan evaluasi oleh subject
matter, apakah prototyping yang dibangun sudah sesuai
dengan keinginan atau belum. Jika sudah sesuai, langkah
selanjutnya akan diambil. Namun jika tidak, prototyping
direvisi
langkah-langkah
sebelumnya.

mengulang

dengan

4. Mengkodekan sistem

5.

Dalam tahap ini prototype yang sudah disepakati
oleh subject matter selanjutnya dilakukan pengkodean
sistem ke dalam bahasa pemrograman yang sesuai.
Evaluasi sistem

Pada tahap ini subject matter akan menguji dan
mengevaluasi apakah perangkat lunak yang sudah jadi
sudah sesuai dengan yang diharapkan. Jika ya, proses
akan dilanjutkan ke tahap selanjutnya, namun jika
perangkat
tidak/belum sesuai
dengan apa yang diharapkan, tahapan sebelumnya akan
diulang.

lunak yang sudah jadi

6. Menggunakan sistem

Perangkat lunak yang telah diuji dan diterima siap

untuk digunakan.

V. KERANGKA PIKIR

Dalam penelitian ini sistem dikembangkan untuk mengatasi
masalah-masalah yang terdapat di bagian administrasi
pembuatan surat perintah perjalanan dinas. Masalah-masalah
yang menjadi fokus penelitian ini secara umum disebabkan
oleh tiga hal yaitu proses pembuatan SPPD masih sering
terjadi kesalahan input, pembuatan surat perintah perjalanan
dinas masih dilakukan secara konvensional, berkas-berkas
pembuatan surat perjalanan dinas tidak teintegrasi dengan
sistem basis data. Ketiga hal
tersebut di atasi dengan
membangun sistem informasi surat perintah perjalanan dinas
yang didalamnya terdiri dari proses pembuatan surat-surat
perjalanan dinas dan data yang dibutuhkan ataupun yang akan
tersimpan sudah terintegrasi dengan database.

yang

Sistem

dapat
dikembangkan
meningkatkan peluang efisiensi pembuatan surat perintah
perjalanan dinas, ketelitian pembuatan surat perjalanan dinas,
fleksibilitas data yang dikelola, dan kemudahan dalam

diharapkan

2 / 8

menyelesaikan tugas. Kerangka pikir penelitian dapat dilihat
pada Gambar 2.

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Dalam flowmap di atas terdapat lima aktor dalam proses
pembuatan SPPD terdiri dari Admin, Kepala BPS, Pegawai,
PPK, dan Bendahara. Selain itu, pembuatan SPPD masih
dilakukan dengan Microsoft Word dan Microsoft Excel dan
juga surat-surat perjalanan dinas dan laporan perjalanan dinas
tidak diintegrasikan dengan database.

2. Analisis Kebutuhan Sistem

Analisis

kebutuhan sistem sangat

diperlukan dalam
mendukung kinerja aplikasi, apakah aplikasi yang dibuat telah
sesuai dengan kebutuhan atau belum. Sebab kebutuhan sistem
akan mendukung tercapainya tujuan suatu aplikasi. Kebutuhan
tersebut terbagi menjadi dua, meliputi kebutuhan fungsional
dan non-fungsional.

A. Kebutuhan Fungsional

Analisis kebutuhan berfokus pada perangkat

lunak yang
akan dibuat oleh penulis. Sistem informasi SPPD web
mempunyai 5 level yang berbeda-beda, yaitu :

Gambar 2. Kerangka pikir

VI. HASIL DAN PEMBAHASAN

Admin
Admin dapat melakukan sebagai berikut :
1. Admin dapat menambah, mengubah, dan menghapus

1. Analisis Sistem Berjalan

data pegawai.

Analisis sistem berjalan dalam penelitian ini dilakukan
dengan wawancara terhadap subject matter terkait di BPS
Kabupaten Jember. Gambar 3 adalah flowmap dari proses
bisnis pembuatan SPPD di BPS Kabupaten Jember.

2. Admin dapat menambah, mengubah, mencetak, dan

menghapus Surat Perintah Tugas (SPT).
3. Admin dapat melihat dan mencetak laporan.
4. Admin dapat menambah, mengubah, dan mencetak Surat

Perjalanan Dinas (SPD).

5. Admin dapat menambah, mengubah, dan mencetak

Rincian Biaya.

6. Admin dapat melihat data Surat Perintah Bayar (SPB).
7. Admin dapat mengubah profil.

Kepala BPS
Kepala BPS dapat melakukan sebagai berikut :
1. Kepala BPS menyetujui SPT.
2. Kepala BPS dapat melihat laporan.
3. Kepala BPS menyetujui Laporan.
4. Kepala BPS dapat mengubah profil.

Pegawai
Pegawai dapat melakukan sebagai berikut :
Pegawai dapat melihat daftar SPT.
1.
Pegawai dapat mengunggah dan mengubah laporan.
2.
Pegawai dapat melihat daftar SPD.
3.
Pegawai dapat melihat daftar rincian.
4.
Pegawai dapat mengubah profil.
5.

Pejabat Pembuat Komitmen (PPK)
PPK dapat melakukan sebagai berikut :
1.
2.
3.

PPK menyetujui SPD.
PPK menyetujui rincian.
PPK dapat mengubah profil.

Bendahara
Bendahara dapat melakukan sebagai berikut :
Bendahara dapat melihat daftar SPT.
1.

3 / 8

Gambar 3. Flowmap SPPD

2.
3.
4.
5.
6.

Bendahara dapat melihat daftar SPD.
Bendahara dapat melihat daftar laporan.
Bendahara dapat melihat daftar rincian.
Bendahara menyetujui SPB.
Bendahara dapat mengubah profil.

B. Kebutuhan Non-fungsional

Pada penelitian ini digunakan analisis PIECES untuk
mengidentifikasi kebutuhan non-fungsional dari sistem yang
berjalan dan menganalisis sistem yang dibuat.

PERFORMANCE

INFORMATION

ECONOMY

CONTROL

EFFICIENCY

SERVICES

TABEL II
TABEL ANALISIS PIECES

SISTEM LAMA
Pembuatan
surat-surat
perjalanan dinas
masih dengan cara
konvensional dan
rentan terjadi salah
input
File disimpan
dalam folder pc

Terlalu sering
menggunakan
kertas untuk
keperluan
mencetak
surat-surat
perjalanan dinas

Kontrol hanya
dapat dilakukan
oleh admin, karena
sistem yang ada
tidak
menggunakan
autentikasi login

Inputan dalam
pembuatan surat
diambil dari
Microsoft Excel
sehingga rentan
terjadi salah input
Proses pembuatan
dan persetujuan
terbatas hanya
dilakukan di
kantor

SISTEM BARU
Pembuatan
surat-surat
perjalanan dinas
menggunakan
sistem informasi
berbasis web

Surat-surat dan
laporan-laporan
yang sudah selesai
diproses secara
otomatis tersimpan
dalam database
Beberapa
surat-surat dan
juga laporan yang
dibuat tidak perlu
lagi untuk dicetak
karena sudah
dapat langsung
diunggah melalui
web
Sistem yang
dibangun terbagi
menjadi lima
tampilan dengan
tampilan tersebut
digunakan untuk
membedakan hak
akses terhadap
aplikasi yang
digunakan
Data-data yang
dibutuhkan dalam
pembuatan
surat-surat dapat
diambil langsung
dari database
Sistem ini dapat
digunakan kapan
saja dan di mana
saja

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

3. Rancangan Sistem Usulan

Gambar 4. Flowmap SPPD

Gambar 4 merupakan flowmap rancangan sistem usulan,
sebenarnya tidak banyak berbeda dengan sistem sebelumnya,
perbedaannya terletak pada proses pembuatannya yaitu dalam
sistem lama menggunakan Microsoft Word dan Microsoft
Excel sedangkan dalam sistem baru pembuatan sudah berbasis
web dan dalam sistem baru ini pembuatan SPB tidak
dilakukan karena SPB akan secara otomatis dibuat ketika
Rincian Biaya dan SPD disetujui oleh PPK.

4.

Rancangan Basis Data

struktur

Rancangan

dalam sistem ini
digambarkan menggunakan ERD, struktur ERD dapat dilihat
pada Gambar 5.

basis

data

Gambar 5. Rancangan Basis Data

4 / 8

5.

Tampilan Web
a. Halaman Login

Gambar 6. Tampilan Halaman Login

Gambar 6 adalah halaman yang akan ditampilkan pertama
kali ketika user mengetikkan alamat sistem aplikasi SPPD
pada browser, ada beberapa yang membedakan hak akses
antara user yang satu dengan yang lainnya. Perbedaannya
dapat
akan
ditampilkan jika user mempunyai username dan password
yang valid guna mengakses aplikasi.

pada menu-menu

nantinya

dilihat

yang

b. Halaman Dashboard

Gambar 7. Tampilan Dashboard

Tampilan pertama yang ditampilkan ketika login adalah
tampilan dashboard seperti terlihat pada Gambar 7. Terlihat
informasi
dalam tampilan dashboard terdapat berbagai
mengenai
(SPT, SPD, SPB, Rincian Biaya,
Laporan Perjalanan Dinas).

surat-surat

c. Halaman Admin

Gambar 8. Tampilan Data Pegawai

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Gambar 8 merupakan tampilan untuk data pegawai, data
pegawai yang ditampilkan meliputi Nomor Induk Pegawai
(NIP), Nama, Pangkat, Golongan, dan Jabatan.

2

1

3

Gambar 9. Tampilan Daftar SPT

Gambar 9 merupakan tampilan daftar SPT. Untuk
menampilkan halaman daftar SPT admin harus menekan
tombol buat surat lalu pilih opsi buat SPT pada menu bar
seperti ditunjukkan dalam kotak merah nomor 1 pada gambar,
untuk melakukan proses pembuatan SPT admin klik tombol
buat SPT pada kotak merah nomor 2, dan untuk melakukan
berbagai aksi seperti edit, hapus, dan cetak dapat dilakukan
dengan menekan tombol-tombol yang ditunjukkan dalam
kotak merah nomor 3. Selain itu admin dapat melihat status
SPT yang telah dibuat, status SPT dapat dilihat pada kolom
status.

1

2

3

Gambar 10. Tampilan Daftar SPD

Gambar 10 merupakan tampilan daftar SPD untuk
menampilkan halaman daftar SPD admin harus menekan
tombol buat surat lalu pilih opsi buat SPD pada menu bar
seperti ditunjukkan dalam kotak merah nomor 1 pada gambar,
untuk melakukan proses pembuatan SPD admin klik tombol
buat SPD pada kotak merah nomor 2, dan untuk melakukan
berbagai aksi seperti edit, dan cetak dapat dilakukan dengan
menekan tombol-tombol yang ditunjukkan dalam kotak merah
nomor 3. Selain itu admin dapat melihat status SPD yang telah
dibuat, status SPD dapat dilihat pada kolom status.

5 / 8

1

2

3

Gambar 11. Tampilan Daftar Rincian

Gambar 11 merupakan tampilan Daftar Rincian untuk
menampilkan halaman Daftar Rincian admin harus menekan
tombol buat Rincian pada menu bar seperti ditunjukkan dalam
kotak merah nomor 1 pada gambar, untuk melakukan proses
pembuatan Rincian admin klik tombol buat Rincian pada
kotak merah nomor 2, dan untuk melakukan berbagai aksi
seperti edit, dan cetak dapat dilakukan dengan menekan
tombol-tombol yang ditunjukkan dalam kotak merah nomor 3.
Selain itu admin dapat melihat status rincian yang telah dibuat,
status rincian dapat dilihat pada kolom status.

d. Halaman Kepala BPS

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Gambar 13 merupakan tampilan Daftar Laporan untuk
menampilkan halaman Daftar Laporan Kepala BPS harus
menekan tombol Daftar Laporan pada menu bar seperti
ditunjukkan dalam kotak merah nomor 1 pada gambar, untuk
melakukan proses persetujuan Laporan Kepala BPS terlebih
dahalu diharuskan mengunduh laporan yang diunggah oleh
pegawai, download laporan dapat dilakukan dengan klik
tombol download (warna hijau) pada kotak merah nomor 2,
setelah Kepala BPS memeriksa laporan tersebut, Kepala BPS
dapat melakukan persetujuan laporan dengan klik tombol edit
(warna kuning) pada kotak merah nomor 3.

e. Halaman Pegawai

Hak akses yang dimiliki oleh pegawai hanya terbatas untuk
mengunggah laporan yang telah dibuat setelah melaksanakan
perjalanan dinas. Untuk tampilan unggah laporan dapat dilihat
pada Gambar 14. Untuk menampilkan tampilan tersebut
pegawai
terlebih dahulu klik tombol unggah laporan pada
menu bar yang ditunjukkan pada kotak merah nomor 1, untuk
unggah laporan pegawai klik tombol unggah (warna biru)
pada kotak merah nomor 2, selain itu pegawai dapat edit
laporan yang telah diunggah dengan klik tombol edit (warna
kuning) pada kotak merah nomor 3.

1

2

1

2

3

Gambar 12. Tampilan Daftar SPT (Kepala BPS)

Gambar 14. Tampilan Unggah Laporan

Gambar 12 merupakan tampilan Daftar SPT untuk
menampilkan halaman Daftar SPT Kepala BPS harus
menekan tombol Daftar SPT pada menu bar
seperti
ditunjukkan dalam kotak merah nomor 1 pada gambar, untuk
melakukan proses persetujuan SPT Kepala BPS klik tombol
edit (warna kuning) pada kotak merah nomor 2.

f.

Halaman PPK

1

2

1

2

3

Gambar 13. Tampilan Daftar Laporan

Gambar 15. Tampilan Daftar SPD

Gambar 15 merupakan tampilan daftar SPD. Untuk
menampilkan halaman daftar SPD, PPK harus menekan
tombol Daftar SPD pada menu bar seperti ditunjukkan dalam
kotak merah nomor 1 pada gambar. Untuk melakukan proses
persetujuan SPD, PPK klik tombol edit (warna kuning) pada
kotak merah nomor 2. Selain itu PPK dapat melihat status

6 / 8

SPD yang telah disetujui, status SPD dapat dilihat pada kolom
status.

6. Kuesioner SUS

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

1

2

Gambar 16. Tampilan Daftar Rincian

Gambar 16 merupakan tampilan Daftar Rincian untuk
menampilkan halaman Daftar Rincian, PPK harus menekan
tombol Daftar Rincian pada menu bar seperti ditunjukkan
dalam kotak merah nomor 1 pada gambar, untuk melakukan
proses persetujuan rincian, PPK klik tombol edit (warna
kuning) pada kotak merah nomor 2. Selain itu PPK dapat
melihat status rincian yang telah disetujui, status rincian dapat
dilihat pada kolom status.

g. Halaman Bendahara

1

2

Gambar 17. Tampilan Daftar SPB

Gambar 17 merupakan tampilan Daftar SPB untuk
menampilkan halaman Daftar SPB, Bendahara harus menekan
tombol Daftar SPB pada menu bar seperti ditunjukkan dalam
kotak merah nomor 1 pada gambar, untuk melakukan proses
persetujuan SPB, Bendahara klik tombol edit (warna kuning)
pada kotak merah nomor 2. Selain itu Bendahara dapat
melihat status SPB yang telah disetujui, status SPB dapat
dilihat pada kolom status.

Responden

1

2

3

4

5

Responden

1

2

3

4

5

P
1

5

5

5

4

5

P
1

4

4

4

3

4

Responden
1
2
3
4
5
Jumlah

TABEL III
TABEL DATA ASLI RESPONDEN
P
2

P
5

P
3

P
4

P
6

1

2

2

2

1

4

4

4

5

5

1

1

2

4

1

4

4

1

4

4

2

1

3

2

2

TABEL IV
TABEL DATA HASIL HITUNG SUS
P
2

P
5

P
6

P
4

P
3

4

3

3

3

4

3

3

3

4

4

4

4

3

1

4

3

3

0

3

3

3

4

2

3

3

TABEL IV
TABEL SKOR SUS
Skor SUS
32
36
26
28
35

P
7

5

5

4

3

5

P
7

4

4

3

2

4

P
8

1

1

2

2

1

P
8

4

4

3

3

4

P
9

1

5

4

4

5

P
9

0

4

3

3

4

P
10

2

2

3

2

4

P
10

3

3

2

3

1

Skor SUS*2.5
80
90
65
70
87.5
392.5

x

Rumus :

x 
n
dengan keterangan,
x
= skor rata-rata
Σx = jumlah skor SUS*2.5
= jumlah responden
n

x

5.392
5

5.78x

Analisis Skor SUS

penilaian

SUS merupakan

usability
(efektivitas, efisiensi, dan kepuasan) secara subjektif yang
dirasakan oleh pengguna. Skor SUS dapat menunjukkan
tingkat penerimaan pengguna. Skor SUS harus bernilai lebih

global

aspek

7 / 8

dari 70 (Brooke, 2013) agar termasuk ke dalam kategori dapat
diterima. Skor SUS website SPPD BPS Kabupaten Jember
sebesar 78.5. Skor SUS dianggap baik apabila bernilai lebih
dari 70.4 (Bangor et al., 2009). Skor SUS website SPPD BPS
Kabupaten Jember sudah lebih dari 70, hal ini berarti website
SPPD BPS Kabupaten Jember sudah baik dan dapat diterima.

7.

Surat Pernyataan

Surat pernyataan ini berisi persetujuan sistem yang telah
dibangun dan disepakati berdasarkan dokumen SRS, surat
tersebut telah ditandatangani oleh Kepala Sub Bagian Umum
BPS Kabupaten Jember, screenshot surat pernyataan dapat
dilihat pada gambar 18.

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

DAFTAR PUSTAKA
[1] S. Mamase. “Rancang Bangun Sistem Informasi Surat Perintah

Perjalanan Dinas”. vol. 6, no. 2, pp. 7-11, 2016.

[2] D. Saepuloh. “Pembuatan Sistem Informasi Surat Perintah Perjalanan
Dinas Pada Pusat Riset Kelautan,” Jurnal Pari, vol. 3, no. 2, pp. 89-97,
2017.

[3] M. Shamad. Pembangunan Sistem Informasi Manajemen Administrasi,
Monitoring, Dan Evaluasi Anggaran STIS, Jakarta: Politeknik Statistika
STIS, 2017.

[4] R. M. Jr And G. P. Schell, Management Information Systems. New

Jersey: Pearson Education, 2007.

[5] Z. Sharfina and H. B. Santoso, ""An Indonesian adaptation of the System
Usability Scale (SUS),"" 2016 International Conference on Advanced
Computer Science and Information Systems, ICACSIS 2016, pp. 145-148,
2017.

[6] Anhar, Panduan Menguasai PHP & MySQL secara Otodidak, Jakarta:

Mediakita, 2010.

[7] Supono and V. Putratama, Pemrograman Web dengan Menggunakan
PHP dan Framework Codeigniter, Yogyakarta: Deepublish (Grup
Penerbitan CV Budi Utama), 2016.

[8] H. M. Jogianto, Analisa dan Desain Sistem Informasi: Pendekatan
Terstruktur Teori dan Praktik Aplikasi Bisnis, Yogyakarta: ANDI, 2005,
p. 11.

[9] J. L. Whitten and L. D. Bentley, Introduction to Systems Analysis &

Design, New Delhi: Mc Graw Hill, 2008, p. 5.

[10] Kertahadi, Sistem Informasi Manajemen, Jakarta: PT Pustaka Binaman

Pressindo, 2007.

[11] J. Brooke, ""SUS: a retrospective,"" Journal of Usability Studies 8, No 2,

pp. 29-40, 2013.

[12] J. Brooke, ""SUS-A quick and dirty usability scale,"" Usability Evaluation

in Industry 189, no.194, pp. 4-7, 1996.

[13] A. Bangor, P. Kortum and J. Miller, ""Determining What Individual SUS
Scores Mean: Adding an Adjective Rating Scale,"" Journal of Usability
Studies, vol. 4, no. 3, pp. 114-123, 2009.

Gambar 18. Screenshot Surat Pernyataan

VII. PENUTUP

Kesimpulan

1.

2.

3.

Sistem informasi SPPD telah berhasil dibuat berdasarkan
SRS yang disepakati, dalam sistem tersebut dibuat lima
tampilan berdasarkan pegguna sistem, yaitu; Admin,
Kepala BPS, Pegawai, PPK, dan Bendahara.
Sistem sudah siap digunakan dan sistem ini sudah
di-hosting pada web hosting yang dimiliki oleh BPS
Kabupaten Jember dan dapat diakses melalui
tautan
berikut: http://simsuperjadin.bps3509.com/.
Berdasarkan hasil pengujian yang dilakukan dengan
skor untuk sistem
menggunakan kuesioner SUS,
informasi SPPD ini adalah 78.5 dan skor
tersebut
dianggap baik karena bernilai lebih dari 70.

Saran

1. Ditambahkan fitur notifikasi pada pegawai jika pegawai

yang bersangkutan mendapatkan SPT.

2. Membuat tampilan upload laporan hanya untuk laporan

pegawai yang login saja.

8 / 8

"
221710022,"Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik 

Visualisasi dan Diseminasi Kerangka Induk BS2020 
Blok Sensus 2020 

Sukron Al Amin (221710022, 4SD2) 
Dosen Pembimbing: Dr. Drs. Waris Marsisno, M.Stat.

Ringkasan—    Kerangka  induk  Blok  Sensus  2020  (BS2020) 
merupakan  hasil  kegiatan  pemetaan  oleh  Badan  Pusat  Statistik 
(BPS) pada tahun 2019 yang berisi kumpulan blok sensus dalam 
suatu wilayah kerja statistik (wilkerstat) dengan atribut muatan, 
dominan  muatan,  dan  jenis  blok  sensusnya.  Sistem  informasi 
geografis  (SIG)  yang  digunakan  oleh  BPS  saat  ini  belum  dapat 
menampilkan  peta  digital  sampai  ke  level  unit  analisis  terkecil 
yaitu blok sensus dan satuan lingkungan setempat (SLS). Tujuan 
dari penelitian ini adalah untuk membangun SIG BS2020 berbasis 
web  yang  memiliki  kemampuan  untuk  memvisualisasikan  dan 
mendiseminasikan peta digital sampai level Blok Sensus dan SLS. 
Pembangunan  sistem  dilakukan  dengan  pendekatan  metode 
Rapid Application Development (RAD). SIG untuk Visualisasi dan 
Diseminasi  Kerangka  Induk  BS2020  telah  berhasil  dibangun. 
Hasil  pengujian  Black  Box  memperlihatkan  bahwa  semua 
skenario 
telah  berjalan  sesuai  dengan  yang  diharapkan. 
Sementara  dari  penilaian  pengguna  dengan  metode  Scale 
Usability System diperoleh skor 77.95 yang menunjukkan bahwa 
sistem  telah  berfungsi  dan  dapat  diterima  dengan  baik  oleh 
pengguna. 

Kata Kunci— Visualisasi, Blok Sensus, Web, RAD, BPS. 

1.  LATAR BELAKANG 

Tahun  2019,  BPS  melakukan  pemetaan  wilayah  Satuan 
Lingkungan  Setempat  (SLS)  di  seluruh  wilayah  Indonesia. 
Kegiatan  ini  dilakukan  untuk  menyiapkan  wilayah  kerja 
SP2020  dan  untuk  membentuk  Blok  Sensus  di  tahun  2020 
(BS2020).  Kebutuhan  area  pencacahan  yang  operasional, 
efisien, dan mudah dikelola sampai 10 tahun ke depan menjadi 
kebutuhan dalam rancangan survei. Penggunaan klaster dalam 
rancangan sampling bertujuan agar rancangan lebih efisien baik 
secara  biaya,  operasional,  dan  sampling  error  yang  dalam 
toleransi. 

Direktorat  Pengembangan  Metodologi  Sensus  dan  Survei 
jawab  penyusunan  dan 
sebagai  pengampu 
pengembangan kerangka induk, baik area maupun unit analisis, 
memandang  perlu  adanya  kerangka  induk  area  sampling  dan 
unit analisis yang lengkap, akurat, relevan, dan mutakhir. 

tanggung 

Salah  satu  cara  untuk  mengantisipasi  adanya  perubahan 
area  pencacahan  yang  sangat  dinamis  diperlukan  penetapan 
area pencacahan yang statis dengan jumlah muatan yang relatif 
seragam  dan  kemudahan  pengelolaan  kerangka  area 
pencacahan  secara  berkesinambungan.  Oleh  karena 
itu, 
diperlukan  pembentukan  klaster  yang  memiliki  wilayah  yang 
tetap selama 10 tahun ke depan dan jumlah muatan yang relatif 
seragam.  Selanjutnya  klaster ini  disebut  sebagai  Blok  Sensus 
(BS).  

Dalam menciptakan kerangka induk area sampling dan unit 
analisis yang lengkap, akurat, relevan, dan mutakhir, kendala 
yang  dihadapi  dengan  adanya  BS2020  yang  baru  terbentuk 
adalah belum adanya aplikasi/sistem dalam mendiseminasikan 
peta  kerangka  induk  BS2020.  Subdirektorat  Pengembangan 
Pemetaan  Statistik  memandang  visualisasi  peta  BS2020 

diperlukan  guna  memudahkan  BPS  dalam  mendapatkan 
gambaran menyeluruh tentang kerangka induk suatu wilayah. 
Dengan  adanya  visualisasi  peta  BS2020  akan  memudahkan 
BPS  dalam  melihat  karakteristik  wilayah  baik  dilihat  dari 
sebaran  secara  spasial,  kondisi  muatan,  maupun  muatan 
dominan dari kerangka induk suatu wilayah. 

Penelitian  ini  memberikan  sebuah  inovasi  yaitu  sistem 
informasi  geografis  yang  dapat  memperlihatkan  gambaran 
visualisasi  dan  diseminasi  peta  BS2020  berbasis  digital  baik 
secara  spasial  maupun  sebaran  dalam  rangka  menciptakan 
kerangka 
induk  yang  sesuai  dengan  kriterianya  dan 
memanfaatkan  dokumentasi  BS2020  serta  dapat  digunakan 
oleh BPS pusat maupun daerah.   

2.  TUJUAN PENELITIAN 

Tujuan penelitian ini adalah membangun sistem informasi 
geografis  yang  memuat  visualisasi  dan  diseminasi  peta 
kerangka  induk  BS2020  berbasis  web  yang  dapat  membantu 
BPS dalam mendapatkan gambaran umum sebaran BS dengan 
kriteria sistem sebagai berikut : 

●  Visualisasi data pada BS2020 
●  Query blok sensus dengan kriteria tertentu 
●  Output berupa tabel dan visualisasi peta 
●  Batasan akses data yang sesuai dengan wilkerstatnya. 

3.  PENELITIAN TERKAIT 

Arif  Maulana  (STIS,  2016)  mengembangkan  sebuah 
aplikasi  yang  bernama  AVIDITIF  atau  kependekan  dari 
Aplikasi  Visualisasi  Data  Interaktif.  Aplikasi  ini  dibangun 
dengan tujuan agar user dapat membuat visualisasi berdasarkan 
jenis-jenisnya  dengan  latar  belakang  library    D3.js  bahasa  
JavaScript.  User  AVIDITIF  ditetapkan  dan  dibatasi  hanya 
untuk pegawai BPS dengan terlebih dahulu membuat akun dan 
mendaftar sebagai user. Prosedur pendaftaran pada aplikasi ini 
diharuskan  mengisi  direktorat  tempat  user  bekerja  sehingga 
user  dari  aplikasi  ini  dimungkinkan  dari  pegawai  BPS  tetap 
maupun pegawai non-BPS tetap. Penelitian ini juga membahas 
rancangan suatu user management pada sistem sebagai wadah 
interaksi antar user. Jenis visualisasi data yang digunakan pada 
aplikasi ini ada lima jenis yaitu population pyramid, scatterplot 
matrix,  boxplot,  chord  diagram,  dan  bullet  chart.  Aplikasi 
dibangun  dengan  tujuan  visualisasi  data  dilakukan  secara 
interaktif,  namun  jika  data  belum  sesuai  dengan  format  yang 
ditentukan  maka  user  diwajibkan  untuk  merubah  dan 
melakukan  pengolahan  pada  data  agar  sesuai  dengan  format. 
Topik  penelitian  yang  ada  dalam  penelitian  Visualisasi  dan 
Diseminasi  Kerangka  Induk  BS2020  berbasis  web  memiliki 
kesamaan topik dengan penelitian mengenai AVIDITIF, yaitu 
membangun sebuah sistem yang memuat visualisasi data BPS 

 1 / 8 

 
 
dan  digunakan  oleh  BPS,  namun  memiliki  perbedaan  dalam 
cakupan  data  yaitu  BS2020,  fitur-fitur  khusus,  dan  jenis 
visualisasinya. [1] 

Muhammad  Rizki  (STIS,  2017)  dengan  judul  penelitian 
Perancangan Web Visualisasi Data Badan Pusat Statistik, Rizki 
berhasil  membangun  web  visualisasi  yang  digunakan  BPS. 
Data  yang  digunakan  dalam  visualisasi  ini  berasal  dari 
publikasi  BPS  dengan  memberikan  kebebasan  kepada  user 
untuk  memilih  data,  variabel  vertikal,  jenis  visualisasi  sesuai 
dengan yang telah ditentukan. Jenis visualisasi yang digunakan 
terdiri dari tujuh jenis yaitu column chart, bar chart, line chart, 
area  chart,  chord  diagram,  sequences  diagram  dan  peta 
indonesia.  Penelitian  Visualisasi  dan  Diseminasi  Kerangka 
Induk BS2020 berbasis web memiliki kesamaan topik dengan 
penelitian  terkait  yaitu  visualisasi  data  berbasis  web  namun 
memiliki perbedaan sumber data. Sumber data yang digunakan 
dalam penelitian terkait adalah data yang berasal dari berbagai 
jenis  publikasi  BPS.  Unit  area  visualisasi  dalam  penelitian 
terkait juga dibangun hanya sampai visualisasi peta Indonesia. 
[2] 

Sistem  yang  dibangun  oleh  Fawcet  January  Makay  ( 
Polstat-STIS,  2019)  dalam  penelitian  yang  berjudul 
Pembangunan  Sistem  Numbering  Peta  Blok  Sensus  Berbasis 
Web  memiliki  cakupan  yang  sama  dengan  penelitian 
Visualisasi  dan  Diseminasi  BS2020  ini  yaitu  kerangka  induk 
BS2020.  Tujuan  sistem  ini dibangun  untuk  memberikan  user 
kesempatan melakukan numbering pada blok sensus yang ada 
dalam  kerangka 
induk  BS2020  sesuai  dengan  kaidah 
penomoran  yang  BPS  gunakan.  User  dari  sistem  ini  adalah 
pegawai  BPS  baik  tetap  maupun  non-tetap.  Penelitian  yang 
dilakukan  Fawcet  menjadi  referensi  bagi  penulis  dalam 
menyajikan  data  BS2020  ke  dalam  sebuah  web  yang  berasal 
dari sumber data yang sama. Perbedaan penelitian ini dengan 
penelitian  terkait  adalah  penelitian  terkait  hanya  membangun 
sebuah  sistem  tanpa  adanya  informasi  visualisasi  maupun 
diseminasi BS2020 dalam web. [3] 

4.  METODE PENELITIAN  

A.  Metode Pengumpulan Data 

Metode pengumpulan data/informasi penelitian yang 
dilakukan  oleh  peneliti  dalam  mencapai  tujuan  dan 
menyelesaikan permasalahan pada topik ini adalah : 
●  Wawancara 

Wawancara  adalah  suatu  teknik  pengumpulan  data 
dengan  cara  melakukan  diskusi  atau  memberikan 
pertanyaan  kepada  subject  matter,  yaitu  dalam 
penelitian  ini  adalah  Subdirektorat  Pengembangan 
dan  Pemetaan  BPS  RI.  Tujuan  dari  wawancara 
subject  matter  adalah  untuk 
peneliti  dengan 
melakukan 
finding  dalam  permasalahan 
fact 
visualisasi  yang  dihadapi  subject  matter  pada  hasil 
peta  kerangka  induk  BS2020  serta  mendapatkan 
gambaran  umum  mengenai  kebutuhan  sistem  yang 
dibangun. 
●  Studi Pustaka 

Studi  pustaka  adalah  suatu  teknik  pengumpulan 

Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik 

informasi  dengan  cara  membaca  literatur  terkait, 
jurnal, buku, atau penelitian yang memiliki kesamaan 
topik  dengan  penelitian  yang  sedang  dijalani.  Topik 
studi  pustaka  dalam  penelitian  ini  adalah  topik yang 
berkaitan  dengan  blok  sensus,  visualisasi:  jenis  dan 
klasifikasi,  sistem  informasi  geografis  berbasis  web, 
dan  topik  studi  pustaka  lainnya  yang  berhubungan 
dengan pembangunan sistem informasi berbasis web. 

●  Observasi 

Peneliti  melakukan  observasi  guna  mendapatkan 
gambaran 
cara 
melakukan  pengamatan  terhadap  penelitian  terkait 
dan SIG yang serupa serta digunakan BPS.  

permasalahan  umum 

dengan 

B.  Metode Pembangunan Sistem 

Metode  pembangunan  sistem/System  Development 
Life  Cycle  (SDLC)  yang  dipakai  peneliti  adalah  metode 
Rapid  Application  Development  (RAD).  Alasan  peneliti 
memilih RAD sebagai metode SDLC karena permasalahan 
pada  penelitian  ini  dapat  diselesaikan  sesuai  dengan 
keuntungan  dari  metode  RAD.  Professor  Clifford 
Kettemborough  (Whitehead  College)  dari    University  of 
Redlands  (2005)  mendefinisikan  RAD  sebagai  sebuah 
pendekatan  untuk  membangun  sistem  komputer  dengan 
mengkombinasikan  sarana  dan  teknik  dari  Computer-
Assisted Software Engineering  (CASE),  prototyping,  dan 
batas  waktu  yang  singkat  ke  dalam  suatu  rumusan  yang 
teruji untuk mendapatkan produktivitas dan kualitas yang 
terbaik  [4].  metode  RAD  lebih  diperlukan  dibandingkan 
dengan metode tradisional karena waktu pengerjaan yang 
singkat serta dimungkinkan terdapat pengembangan dalam 
setiap prototipe yang dibangun.  
Tahapan SLDC dalam metode RAD ini menurut Kendall 
& Kendall (2002), ada 3 tahapan yang melibatkan peneliti dan 
subject  matter/user  dalam  tahap  perencanaan,  perancangan, 
dan  penerapan.  Ketiga  tahapan  tersebut  adalah  requirements 
planning  (perencanaan  sesuai  kebutuhan),  RAD  Design 
Workshop, dan Implementation (penerapan) [5].  

Gambar 1. Tahapan Metode RAD 

C.  Metode Pengujian Sistem 

Metode  pengujian  sistem  yang  digunakan  peneliti 
untuk mengukur kegunaan sistem berdasarkan kebutuhan 
dan tujuan penelitian adalah Black-Box Testing dan System 
Usability Scale (SUS).  
●  Black-Box Testing 

Black-Box Testing adalah pengujian secara fungsional 
yang dilakukan dengan tidak memperhatikan struktur 

 2 / 8 

 
 
 
 
 
internal  atau  kode  program  sistem  yang  dibangun. 
Pengujian dilakukan dengan cara peneliti hanya perlu 
mengetahui hasil  yang  didapatkan  tanpa  mengetahui 
bagaimana  sistem  mendapatkan  hasil  itu.  Black-Box 
Testing dilakukan dengan cara mengidentifikasi fitur - 
fitur sistem yang seharusnya ada dan tidak ada dalam 
sistem.  

●  System Usability Scale (SUS) 

Pengujian  sistem  dilakukan  oleh  peneliti  agar  bisa 
mengukur  dan  direpresentasikan  oleh 
subject 
matter/user melalui kuesioner yang berisi pernyataan 
mengenai  sistem  yang  dibangun.  Pernyataan  diukur 
menggunakan skala likert 1 - 5 dengan definisi sangat 
tidak setuju hingga sangat setuju. 

5.  KERANGKA PIKIR 

Kebutuhan  sistem  ini  diidentifikasi  dan  dicermati  dengan 
melakukan  wawancara, 
studi  pustaka,  dan  observasi. 
Wawancara dilakukan dengan subject matter, yaitu salah satu 
statistisi  ahli  muda  Pengembangan  Pemetaan  Statistik 
Direktorat Pengembangan Metodologi Sensus dan Survei BPS 
RI,  terkait  diskusi  analisis  kebutuhan  sistem  kemudian 
melakukan  pendalaman  subjek  melalui  studi  pustaka  dalam 
Buku Pedoman Pembentukan BS2020 BPS RI dan penelitian 
terkait serta observasi terhadap sistem informasi geografis yang 
ada. 

Pembangunan  sistem  dilakukan  sesuai  dengan  kebutuhan 
yang  ada  sehingga  dapat  memberikan  solusi 
terhadap 
permasalahan  yang  ada.  Sistem  berupa  sebuah  SIG  berbasis 
web yang dibangun  agar dapat memberikan informasi secara 
spasial  maupun  non  spasial  hingga  informasi  tersebut  bisa 
dijadikan pengetahuan dalam kegiatan BPS yang memerlukan 
daftar maupun gambaran umum BS. Pembangunan sistem ini 
dilakukan  dengan  menggunakan  framework  Laravel  (bahasa 
pemrograman  PHP  7),  database  postgreSQL  dan  PostGIS, 
basemap Google Hybrid dan OpenStreetMap untuk peta dasar 
sistem,    serta  JavaScript  untuk  manipulasi,  visualisasi,  dan 
pengolahan data pada pemrograman.  

Setelah  sistem  berhasil  dibangun,  tahapan  selanjutnya 
adalah melakukan pengujian dengan metode SUS dan  Black-
Box  yang  dapat  diukur  untuk  dijadikan  bahan  evaluasi  atau 
untuk  penelitian  selanjutnya.  Pengujian  SUS  dan  Black-Box  
ditujukan  kepada  user  sistem  ini  yaitu  subject  matter  dan 
pegawai BPS daerah maupun pusat. Apabila sistem sudah layak 
dalam pengujian, maka hasil akhir penelitian ini adalah sebuah 
SIG 
dan 
mendiseminasikan  kerangka  induk  BS2020.  Alur  kerangka 
induk dapat dilihat pada Gambar. 2. 

dalam  memvisualisasikan 

berbasis  web 

6.  HASIL DAN PEMBAHASAN 

A.  Analisis Berjalan 

Berdasarkan  wawancara 

dalam 
menganalisis  kebutuhan  sistem  dengan  subject  matter, 
Subdirektorat 
yaitu 

Statistisi  Ahli 

diskusi 

salah 

satu 

dan 

Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik 

Pengembangan  dan  Pemetaan  BPS  RI,  visualisasi, 
diseminasi, dan output BS2020 berupa sistem atau aplikasi 
yang  dapat  menampilkan  peta  digital  hingga  level  BS 
belum ada atau pernah dilakukan. Adapun eksplorasi data 
yang  dilakukan  pada  BPS  masih  dalam  berupa  level 
nasional  dan  provinsi  serta  data  yang  digunakan  belum 
mencapai level BS dan tidak menggunakan data BS2020. 
Pemanfaatan  data  BS2020  yang 
termutakhir  dapat 
dilakukan  dengan  membangun  sebuah  sistem  untuk 
memperoleh  informasi  dan  pengetahuan  yang  berguna 
untuk  kegiatan  BPS  seperti  sensus  atau  survei.  File 
BS2020  dengan  format  GeoJSON  dimiliki  oleh  setiap 
daerah  sesuai  dengan  wilkerstatnya  masing  -  masing 
namun pemanfaatan data untuk visualisasi dan diseminasi 
belum dapat terpenuhi sehingga prosedur untuk visualisasi 
dan  diseminasi  peta  digital  hingga  level  BS  ke  dalam 
sistem atau aplikasi belum ada atau pernah dilakukan. 

Pengumpulan informasi juga dilakukan dengan studi 
pustaka  utama  dalam  pendefinisian  cakupan,  yaitu 
Pedoman  Pembentukan  BS2020  oleh  BPS  RI[6].  BS 
adalah wilayah kerja statistik yang dibentuk dari SLS/non 
SLS  yang  merupakan  bagian  dari  suatu  wilayah  desa. 
BS2020  dapat  dibentuk  dari  satu  SLS  utuh  atau 
berdasarkan  penggabungan  maupun  pemecahan  SLS. 
juga  memperhatikan  muatan 
Pembentukan  BS2020 
dominan dari suatu segmen. Satu BS harus terletak pada 
satu hamparan, artinya tidak boleh terpisah oleh BS lain. 

Kebutuhan sistem memuat visualisasi atribut - atribut 
yang  terdapat  pada  hasil  peta  kerangka  induk  BS2020, 
atribut - atribut utama dapat dibagi menjadi 3 karakteristik 
yang  penting  dalam  mendapatkan  gambaran  umum  hasil 
peta kerangka induk tersebut, yaitu : 
●  Sebaran Jenis Blok Sensus 

Jenis  Blok  Sensus  yang  akan  didiseminasikan 
melalui  jumlah  dan  sebarannya  dapat  digambarkan 
melalui  ketiga  jenisnya,  yaitu  Blok  Sensus  Biasa, 
Khusus, dan Persiapan. 
●  Sebaran Muatan Blok Sensus 

Muatan  blok  sensus  dapat  digambarkan  melalui 
jumlah  dan  sebaran  muatan  atribut  -  atribut  KK 
(Kepala Keluarga), BSTT (Bangunan Sensus Tempat 
Tinggal),  BSBTT  (Bangunan  Sensus  Bukan  Tempat 
Tinggal), BSTTK (Bangunan Sensus Tempat Tinggal 
Kosong),  dan  muatan  (penjumlahan  maksimum  KK 
atau BSTT, BSBTT dan BSTTK). 
●  Sebaran Muatan Dominan Blok Sensus 

Muatan Dominan Blok Sensus terdiri dari 11 jenis 

muatan dominan. 

B.  Identifikasi Masalah 

Berdasarkan analisis berjalan yang dilakukan, berikut 

identifikasi masalah yang dihasilkan : 
●  Belum  ada  sistem  khusus  yang  dapat  menampung 

kerangka induk BS2020 yang termutakhir. 

●  Belum ada penerapan visualisasi dan diseminasi yang 
dapat  dilakukan  dengan 
tujuan  mendapatkan 
informasi  berupa  spasial  maupun  non  spasial  dari 

 3 / 8 

 
 
 
BS2020. 

●  Output  visualisasi  dan  diseminasi  BS2020  hingga 
level BS dalam media cetak seperti peta cetak atau soft 
file belum ada. 

●  Belum  ada  sistem  khusus  yang  dapat  memberikan 
kesempatan  bagi  BPS  untuk  melakukan  eksplorasi 
data  BS2020  secara  visual  hingga  level  BS  seperti 
melakukan query. 

Gambar 2. Kerangka Pikir Penelitian 

C.  Analisis Kebutuhan 

Berikut kebutuhan sistem yang akan dibangun sesuai 

dengan hasil diskusi bersama subject matter :  
●  Visualisasi sebaran blok sensus 
●  Visualisasi sebaran muatan blok sensus 
●  Query dengan atribut tertentu 
●  Pemilihan warna, metode klasifikasi (natural breaks, 
tematik  peta 

jenis 

quantile,  dan  equal),  dan 
(choropleth dan size) 

●  Tabel  dan  peta  digital  yang  saling  terhubung  dan 

interaktif 

●  Pemilihan  basemaps  yang  menyesuaikan  dengan 

sistem yang sudah ada 

Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik 

●  Diharapkan  dapat  diterapkan  penggunaan  service 
request  API  SSO  namun  untuk  penelitian 
ini 
menggunakan tabel duplikasi yang dibedakan dengan 
server BPS. 

TABEL I 
Kebutuhan Visualisasi Sebaran Blok Sensus dan Muatan Blok Sensus 
Jumlah BS 

Sebaran BS 

Level 

Provinsi 

per kabupaten/kota 

- 

Kabupaten/Kota  per kecamatan 

seluruh kabupaten/kota 

Kecamatan 

per desa 

seluruh kecamatan 

Desa 

- 

seluruh desa 

TABEL II 
Kebutuhan Query Atribut Blok Sensus 
Filter 

Contoh 

Muatan Dominan  “dominan”= ‘10’ 

Muatan BS 

“muatan” > ‘200’ 

Jenis BS 

“Count (right(‘kdbs’’, 1) = ‘p’) 

Metode  visualisasi  peta  yang  digunakan  peneliti 
memperhatikan  3  poin  utama,  yaitu  pemilihan  warna, 
metode  klasifikasi  (natural  breaks,  quantile,  dan  equal), 
dan jenis tematik peta (choropleth dan size). Dilansir dari 
situs  WebGIS  Bappenas[7],    Natural  Breaks  Interval 
adalah metode klasifikasi yang menentukan titik pada data 
dengan melihat pengelompokan dan pola data. Data yang 
digunakan  mempunyai  jangkauan  dari  yang  terkecil 
sampai  yang  besar.  Data  kemudian  dibagi-bagi  dengan 
batas  batas  yang  ditentukan  berdasarkan  nilai  jangkauan 
terbesar.  Quantile  Interval  adalah  metode  klasifikasi 
dengan  menentukan  interval  berdasarkan  jumlah  data 
dibagi  jumlah  kelas.  Equal  Interval  adalah  metode 
klasifikasi 
sebagai 
interval 
jangkauan dibagi jumlah kelas. 

didefinisikan 

dengan 

Dalam  menyesuaikan  kebutuhan  user 

terhadap 
visualisasi  dan  diseminasi  kerangka 
induk  BS2020, 
peneliti  dan  subject  matter  memandang  perlunya  output 
yang tidak hanya dalam bentuk peta, melainkan juga dalam 
bentuk tabel dan rekapitulasi setiap wilayah peta tematik. 
Dalam tabel yang terbentuk, setiap barisnya terhubung ke 
peta  saat  dilakukan  mouse  over  (muncul  highlight)  pada 
tabel  maupun  peta  digital.  Fitur  peta  dan  tabel  juga 
dirancang  sedemikian  rupa  agar  dapat  diunduh  dalam 
bentuk  file  yang  dapat  dicetak  untuk  keperluan  kegiatan 
yang berkaitan dengan pemetaan. 

Visualisasi dan diseminasi pada peta kerangka induk 
BS2020  memerlukan  peta  dasar/basemap  sebagai  acuan 
geografis  BS2020.  Subject  matter  memberi  pilihan 
basemap  OpenStreetMap  (OSM),  Google  Street,  dan 
Google Hybrid. Dalam hal ini peneliti memilih OSM dan 
Google  Hybrid  sebagai  peta  dasar  sistem  dengan  alasan 
bahwa  kedua  peta  dasar  tersebut  merupakan  peta  dasar 
yang sering user dan subject matter gunakan/sukai. 

 4 / 8 

 
 
 
 
 
 
 
 
Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik 

Kebutuhan  sistem  secara  teknis  menurut  subject 
matter  dalam  menyesuaikan  penggunaan  sistem  yang 
menggunakan  fitur  login  di  BPS  disesuaikan  dengan 
sistem  yang  akan  dibangun  dalam  penelitian  ini.  Login 
aplikasi dapat dimodifikasi ke request API Single Sign On 
dengan  data  user  mengakses  service  dari  sistem  yang 
sudah  ada  (existing  system).  Untuk  penelitian  ini  hanya 
menggunakan duplikasi tabel  database user  dari  existing 
system yang akan dibedakan koneksinya. 

User  dalam  sistem  visualisasi  dan  diseminasi  peta 
hasil  kerangka  induk  BS2020  adalah  BPS  Pusat,  BPS 
Provinsi,  dan  BPS  Kabupaten/Kota.  Yang  membedakan 
role masing - masing user adalah hak akses wilayah dan 
disesuaikan  dengan  wilkerstat  masing  -  masing  user. 
Tingkatan  peta  dalam  sistem  ini  dimulai  dari  BS,  desa, 
kecamatan, kabupaten, provinsi hingga peta Indonesia. 

User 

BPS Pusat 

BPS Provinsi 

TABEL III 
User Sistem 

Hak Akses 

Level  Nasional dan setiap Provinsi serta 
turunannya 
Level Provinsi (menyesuaikan wilayah) serta 
turunannya 

BPS Kabupaten/Kota  Level Kabupaten/Kota (menyesuaikan wilayah) 

serta turunannya 

D.  Rancangan Antarmuka 

Rancangan  sistem  terlebih  dahulu  dibangun  dalam 
bentuk  mockup  sebagai  penentuan  antarmuka  pengguna 
/Graphical  User  Interface  (GUI)  yang  di  dalamnya 
terdapat  antarmuka  yang  menjadi  solusi  dari  masalah 
penelitian. 

Gambar 3. Rancangan Antarmuka Pengguna 

Penjelasan setiap fitur - fitur yang dirancang (Gambar. 
2)  sesuai  dengan  analisis  kebutuhan  adalah  sebagai 
berikut: 

1.  Show/Hide SideBar. 
2.  Search Prov/Kab/Kec/Des. 
3.  Memfilter 

lokasi 

tertentu  (Dinamis  dengan 

JavaScript / AJAX). 

4.  Query  untuk  karakteristik 

tertentu  (Dengan 

method POST dan tidak boleh kosong). 

a.  Tipe Query Jenis BS adalah Checkbox 
b.  Tipe  Muatan  BS  adalah  Input  dengan 

tipe data integer dengan rentang 

c.  Tipe  Muatan  Dominan  BS  adalah 

Checkbox  

5.  Menampilkan  Visualisasi 

sesuai  kebutuhan 
(Dinamis  dengan  JavaScript  /  AJAX)  (Dengan 
method POST dan tidak boleh kosong). 

a.  Tipe  Variabel 

isinya  muatan,  KK, 

BSTT, BSBTT, BSTTK 

b.  Tipe  Tematik 

isinya  adalah  opsi 

Chroplet atau Size  

c.  Tipe Kelas isinya adalah angka 3-9 
d.  Tipe  Klasifikasi 

isinya  adalah  opsi 

Natural Break, Quantile, atau Equal  

e.  Tipe  Warna 
visualisasi 

adalah 

color  untuk 

6.  Output  tabel  dan  peta  visualisasi  dan  dapat  di-
export dalam Excel, CSV, PDF dan bisa langsung 
Print. 

7.  Tabel  dengan  fitur  isian  dan  peta  digital  saling 

berinteraksi. 
Info summary dari hasil input wilayah atau query. 
:  Google  Hybrid  dan 

8. 
9.  Pemilihan  Basemaps 
OpenStreetMap 
10.  Legenda visualisasi 

 5 / 8 

 
 
 
 
 
 
 
 
Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik 

E.  Rancangan Arsitektur 

Rancangan  arsitektur  sistem  terdapat  3  user  utama, 
yaitu BPS Pusat, BPS Provinsi, dan BPS Kabupaten/Kota. 
User  terhubung  dengan  sistem  melalui  internet  dengan 
database yang dirancang sesuai dengan kebutuhan. 

Gambar 6. Rancangan Arsitektur Sistem 

Rancangan  proses  bisnis  penelitian  memuat  proses 
yang dapat user lakukan dalam menggunakan sistem yang 
akan dibangun dapat dilihat pada gambar 7. 

F. 

Implementasi 
Implementasi  sistem  dibangun  seefisien  mungkin 
dengan  meminimalisir  kerumitan 
logic  web  dalam 
penggunaannya.  Sistem  hanya  terdiri  dari  dua  halaman 
yaitu,  login  dan  dashboard.  Sistem  dibangun  dengan 
menggunakan  framework  Laravel  (bahasa  pemrograman 
PHP  7),  database  postgreSQL  dan  PostGIS,  basemap 
Google  Hybrid  dan  OpenStreetMap  untuk  peta  dasar 
sistem,  serta JavaScript untuk manipulasi, visualisasi, dan 
pengolahan  data  pada  pemrograman.  Berikut  tampilan 
tangkapan  layar  penggunaan  (Gambar  9-13)  dan  ERD 
database (Gambar. 8) dari sistem. 

Gambar 7. Rancangan Proses Bisnis Sistem 

Gambar 8. Logical Entity Relationship Diagram 

 6 / 8 

 
 
 
 
 
 
 
 
 
 
Gambar 9. Tampilan Login  

Gambar 10. Tampilan Dashboard, tabel, peta digital dan kumpulan fitur 
lainnya 

Gambar 11. Tampilan hasil query dengan atribut tertentu 

Gambar 12. Tampilan hasil visualisasi : metode klasifikasi, tematik, 
pemilihan variabel, jumlah kelas dan pemilihan warna 

Gambar 13. Tampilan output berupa tabel dan visualisasi peta dalam bentuk 
softfile 

Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik 

G.  Evaluasi dan Pengujian 

Setelah  dilakukan  pembangunan  sistem,  peneliti 
melakukan  pengujian  dengan  Black  Box  Testing  yang 
terdiri dari total 6 skenario utama dan SUS yang terdiri dari 
15  pertanyaan  (termasuk  uji  kelayakan)  dengan  jawaban 
berskala  1-5.  Responden  Black  Box  Testing  sejumlah  2 
orang  secara  acak,  respondennya  adalah  teman  kuliah 
peneliti yang diasumsikan mengerti menjalankan skenario 
sederhana. Responden SUS terdiri dari 1 subject matter dan 
10 rekan subject matter dari lingkungan yang sama. Berikut 
hasil pengujian yang telah dilakukan. 

TABEL IV 
Skenario Black Box Testing 

No. 

Skenario 

Hasil yang diharapkan 

1 

2 

3 

4 

5 

6 

Menampilkan dan mengarahkan ke 
dashboard sistem 
Wilkerstat yang tersedia sesuai dengan 
role user yang melakukan login 

Tabel dan peta digital yang dinamis dan 
interaktif serta dapat menunjukkan 
highlight 

Query dengan kriteria atribut BS seperti 
jenis BS, muatan BS, dan dominan 
muatan BS serta menunjukkan hasil 
query dalam bentuk tabel dan peta 
digital 
Visualisasi dilakukan dengan memilih 
variabel, jumlah kelas, metode 
klasifikasi, dan pemilihan warna serta 
dapat menghasilkan visualisasi sesuai 
dengan pilihan user 
Tabel dan peta digital dapat diunduh 
dan disimpan dalam berbagai format 

Melakukan 
Login 
Memilih 
wilkerstat 
sesuai role 
Melihat tabel 
dan peta 
digital yang 
dinamis dan 
interaktif 
Melakukan 
query 

Melakukan 
visualisasi 

Mengunduh 
output dalam 
bentuk tabel 
dan peta 
cetak 

Pengujian SUS dengan 15 pertanyaan, 5 di antaranya 
untuk  uji  kelayakan  (User  Acceptance  Test).  Setiap 
pertanyaan  maksimal  bernilai  5  dan  minimal  1,  sebagian 
pertanyaan  terpilih  dikurangi  dengan  1  dan  sisanya  5 
dikurangi dengan nilai pertanyaan. Total setiap pertanyaan 
akan dikali 2.5 dan akan mendapatkan skor maksimal 100 
dan minimal 0  Skor pengujian SUS adalah 77.95 dan untuk 
pengujian  kelayakan  adalah  78.40.  Berikut  rincian  hasil 
kedua pengujian tersebut. 

TABEL V 
Hasil Pengujian SUS 

Pertanyaan ke- 

Total Poin 

1 

5 

6 

9 

10 

39 

38 

31 

32 

36 

Nilai 

88.64 

86.36 

70.45 

72.73 

81.82 

 7 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
11 

12 

13 

14 

15 

Rata-rata 

36 

34 

32 

31 

34 

34.3 

TABEL VI 
Hasil Pengujian Kelayakan 

Aspek (Pertanyaan ke-) 

Total Poin 

Kemudahan (2) 

Fungsionalitas (3) 

Kepuasan (4) 

Efisiensi (7,8) 

Rata-rata 

34 

36 

32 

36 

34.5 

81.82 

77.27 

72.73 

70.45 

72.27 

77.95 

Nilai 

77.27 

81.81 

72.72 

81.81 

78.40 

7.  PENUTUP 

Hasil  dari  penelitian  ini  adalah  sistem  visualisasi  dan 
diseminasi kerangka induk BS2020 berbasis web telah berhasil 
dibangun  untuk  memenuhi  kebutuhan  dalam  mendapatkan 
gambaran  umum  mengenai  sebaran  muatan  yang  ada  pada 
kerangka induk BS2020 dengan kriteria sebagai berikut : 

1.  Sistem dibangun sudah sesuai dengan kebutuhan yang 
telah dianalisis dan dapat melakukan eksplorasi data 
BS2020 pada semua level wilkerstat. 

2.  Query  pada  data  BS2020  dapat  dilakukan  sesuai 

dengan atribut yang ada. 

3.  Visualisasi  dan  diseminasi  bekerja  sesuai  yang 
diharapkan mulai dari metode klasifikasi hingga jenis 
tematiknya. 

4.  Tabel  dan  peta  digital  sudah  tersedia  dalam  bentuk 

halaman web dan keduanya saling interaktif. 

5.  Sistem  hanya  memberikan  kesempatan  memilih 
wilayah  bagi  user  sesuai  dengan  role  wilkerstatnya 
masing - masing dan struktur tabel request API SSO 
sudah sesuai dengan sistem yang ada. 

6.  Pengujian  dengan  BlackBox  dengan  6  skenario 
memiliki hasil sesuai yang diharapkan dan dilakukan 
oleh 3 responden. 

7.  Pengujian  dengan  SUS  dan  User  Acceptance  Test 
dengan  11  responden  memberikan  skor  akumulatif 
77.95  dan  79.40  dengan  interpretasi  bahwa  sistem 
sudah dapat diterima dengan baik serta sudah berjalan 
sesuai dengan yang diharapkan. 

Pembangunan sistem ini masih memiliki kekurangan dan 
masih  membutuhkan  pengembangan  dari  berbagai  aspek 
seperti  performa,  UI,  UX  dll.  Sistem  dapat  dikembangkan 
menjadi sebuah sistem yang lebih kompleks dan memiliki fitur 
yang  lebih  bagus  dalam  pengoperasiannya.  Berikut  adalah 
saran yang dapat diberikan dalam pembangunan sistem. 

1.  Peningkatan dan pengembangan sistem untuk UI dan 

UX 

Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik 

2.  Peningkatan keamanan data polygon BS2020 
3.  Penambahan  variabel  dan  pengkombinasiannya 
dengan  sumber  data  lainnya  seperti  data  Podes, 
Kependudukan dsb. 

4.  Pemanfaatan  teknologi  Web  Map  Service  dan  Web 

Feature Service 

5.  Penggunaan  ArcGIS  server  yang  dapat  digunakan 

untuk pengembangan sistem 

DAFTAR PUSTAKA 

F.  J.  Makay,  PEMBANGUNAN  SISTEM  NUMBERING  PETA 

[1]  M.  A.  Maulana,  AVIDITIF  :  SISTEM  APLIKASI  VISUALISASI 
DATA INTERAKTIF. Skripsi PS-STIS, 2016. 
[2]  M.  Rizki,  PERANCANGAN  WEB  VISUALISASI  DATA  BADAN 
PUSAT STATISTIK. Skripsi PS-STIS, 2017. 
[3] 
BLOK SENSUS BERBASIS WEB. Skripsi PS-STIS, 2019. 
[4] 
IGI  Global,  Software  Design  and  Development:  Concept, 
Methodologies,  Tools,  adn  Applications.  Harshey,  Information  Science 
Reference IGI Global, 2013.K. E. Kendall & J. E. Kendall, System Analysis 
and Design. Harlow, Pearson, 2002. 
[5]  K. E. Kendall & J. E. Kendall, System Analysis and Design. Harlow, 
Pearson, 2002. 
[6]  BPS, Pedoman Pembentukan BS2020,  No.  Publikasi 03130.2001, 
Katalog BPS 1303132, BPS RI, 2020. 
[7]  Bappenas 
https://webgis-simrenas.bappenas.go.id/simreg/help.html. 

(2020)  Metode  Klasifikasi 

[online].  Available: 

 8 / 8 

 
 
 
"
221710021,"Proposal Skripsi – Program Studi D-IV Komputasi Statistik 

Pengembangan Chatbot Halo WARKOP dengan 
Metode TF-IDF dan Cosine Similarity 

Sukma Nirmala Dewi (221710021, 4SI1) 
Dosen Pembimbing: Yunarso Anang Ph.D 

Ringkasan—  Warung  Kompetensi  Pegawai 

(WARKOP) 
Pusdiklat  BPS  merupakan  platform  pengembangan    Sumber 
Daya  Manusia 
(SDM)  menggunakan  berbagai  metode 
pembelajaran  dengan  mengoptimalkan  teknologi.  Pengunjung 
WARKOP bervariasi baik dari pihak internal maupun eksternal 
BPS.  Pertanyaan  yang  ditanyakan  oleh  pengunjung  web 
WARKOP bervariasi meskipun topik pertanyaannya tidak jauh 
berbeda. Kesamaan topik pertanyaan yang muncul menyebabkan 
admin  WARKOP  menjawab  pertanyaan  yang  sama  secara 
berulang. Selain itu, pertanyaan yang muncul tidak dapat  selalu 
terjawab  dikarenakan  kurangnya  SDM.  Oleh  karena  itu,  fitur 
Chatbot  Halo  WARKOP  dibuat.  Fitur  chatbot  dibuat  dengan 
menggunakan  metode  TF-IDF  dan  Cosine  Similarity.  Pada 
pengujian  sistem  didapatkan  hasil  SUS  bahwa  sistem  dapat 
diterima oleh pengguna, serta precision sebesar 90.45% dan recall 
sebesar 100%. 

Kata  Kunci—  WARKOP,  Pusdiklat,  TF-IDF,  Cosine 

Similarity, Chatbot. 

I.  LATAR BELAKANG 

Pusat  Pendidikan  dan  Pelatihan  (Pusdiklat)  adalah  unsur 
pelaksana  Badan  Pusat  Statistik  (BPS)  di  bidang  pendidikan 
dan  pelatihan,  dengan  tugas  melaksanakan  penyelenggaraan 
pendidikan dan pelatihan  prajabatan  dan kepemimpinan  serta 
pendidikan  dan  pelatihan  teknis  dan  fungsional  [1].  Untuk 
mempermudah  penyelenggaraan  pendidikan  dan  pelatihan, 
Pusdiklat  BPS  membuat  platform  berbasis  web  yang 
dinamakan Warung Kompetensi Pegawai atau WARKOP. 

WARKOP merupakan platform pengembangan SDM dalam 
bentuk pelatihan  non-klasikal  yang menggabungkan  berbagai 
metode  pembelajaran 
inovatif  dengan  mengoptimalkan 
penggunaan  teknologi  informasi  dan  komunikasi.  WARKOP 
merupakan  bagian  pengembangan  kompetensi  Sistem 
Pembelajaran Terpadu untuk melengkapi pembelajaran formal 
atau  klasikal  di  kelas  [2].  Dengan  adanya  WARKOP, 
keterbatasan pegawai dalam mengembangkan kompetensi yang 
disebabkan oleh jarak dan waktu dapat diminimalkan. Di dalam 
WARKOP  terdapat  berbagai  macam  kursus  online  dengan 
narasumber  yang  berasal  dari  beragam 
latar  belakang. 
Narasumber berasal dari internal BPS maupun eksternal BPS. 
Narasumber eksternal BPS seperti Institut Teknologi Bandung, 
Universitas  Padjajaran,  dan  perguruan  tinggi  atau  instansi 
lainnya. 

Pengunjung  WARKOP  sangat  beragam  mulai  dari  pihak 
internal  BPS  hingga  pihak  eksternal  BPS.  Pertanyaan  yang 
ditanyakan  oleh  pengunjung  web  WARKOP  juga  bervariasi 
meskipun topik pertanyaannya tidak jauh berbeda. Pertanyaan 
yang serupa ini dijawab oleh admin web WARKOP secara satu 
per  satu  (manual).  Pertanyaan  yang  sudah  pernah  ditanyakan 
dan  ditanyakan  kembali  oleh  pengunjung  web  perlu 

didiskusikan  kembali  oleh  admin  WARKOP  agar  jawaban 
yang  diberikan 
ini  menyebabkan 
tetap  konsisten.  Hal 
diperlukan  waktu  lebih  lama  untuk  menjawab  pertanyaan 
permasalahan 
pengunjung  web.  Berdasarkan 
terjadi  pekerjaan 
sebelumnya  dapat  disimpulkan  bahwa 
berulang  yang  menghambat  efektivitas  kerja  admin  web 
WARKOP. 

uraian 

Pesan  yang  terkirim  pada  halaman  pelayanan  tanya  jawab 
pengunjung web WARKOP tidak dapat direspons dengan cepat 
setiap  saat.  Hal  ini  disebabkan  oleh  ketiadaan  admin  yang 
khusus  menangani  bagian  pelayanan  tanya  jawab.  Selain  itu, 
admin yang bertugas hanya dapat melayani tanya jawab pada 
waktu  tertentu,  yaitu  jam  09.00  –  15.00  WIB.  Admin  yang 
bertugas  menjawab  pertanyaan  pengunjung  web  merupakan 
pegawai TI Pusdiklat BPS yang memiliki tugas prioritas lain, 
seperti  mengelola  pelaksanaan  diklat,  monitoring 
IT, 
merancang  proyek  TI,  dll.  Oleh  sebab  itu,  admin  web 
WARKOP Pusdiklat jarang menjawab pertanyaan yang masuk. 
Dari  permasalahan  yang  telah  dipaparkan,  dapat  dinyatakan 
bahwa  terdapat  masalah  kekurangan  SDM  untuk  melayani 
pertanyaan pengunjung web WARKOP. 

komputer 

Berdasarkan  masalah  yang  ditemukan,  diperlukan  sistem 
pelayanan  tanya  jawab  yang  dapat  menjawab  pertanyaan 
pengunjung web WARKOP Pusdiklat secara otomatis berupa 
chatbot  Halo  WARKOP.  Keberadaan  fitur  jawab  otomatis 
(chatbot)  pada  sistem  Halo  WARKOP  diharapkan  dapat 
mengurangi  beban  kerja  admin  web  WARKOP  tetapi  tetap 
tampil interaktif  bagi pengunjung WARKOP. Chatbot adalah 
program 
melakukan 
yang 
percakapan  melalui  media  tulisan  [3].  Sistem  chatbot  Halo 
WARKOP  memerlukan  metode  pengolahan  kata  yang  dapat 
menerjemahkan  bahasa  sehari-hari  manusia  menjadi  bahasa 
yang  dapat  dimengerti  oleh  mesin.  Metode  pengolahan  kata 
yang digunakan pada sistem ini adalah metode Term Frequency 
- Inverse Document Frequency (TF-IDF) dan Cosine Similarity.  
berfungsi 

untuk  memberikan 
bobot/weight  pada  setiap  dokumen  jawaban  yang  tersedia 
dalam  database.  Setelah  itu  dilanjutkan  dengan  algoritma 
Cosine Similarity untuk mengukur kesamaan antara input dari 
customer dengan data jawaban yang ada di sistem [4]. 

Algoritma  TF-IDF 

dapat 

II.  TUJUAN PENELITIAN 

Tujuan  dari  penelitian  ini  dibagi  menjadi  dua  yaitu  tujuan 
umum  dan  tujuan  khusus.  Berikut  uraian  tujuan  umum  dan 
khusus dengan uraian sebagai berikut: 
Tujuan Umum 
Mengimplementasikan model pengolahan kata ke dalam sistem 
chatbot  Halo  WARKOP  yang  dapat  menjawab  pertanyaan 
pengunjung  web  secara  otomatis  sehingga  mempermudah 
pekerjaan admin web WARKOP Pusdiklat. 

 1 / 8 

 
 
 
 
 
Tujuan khusus 
1.  Menerapkan metode TF-IDF dan Cosine Similarity untuk 

metode pengolahan kata. 

2.  Membangun sistem yang dapat membantu admin 

mengelola pertanyaan dan jawaban yang 
tersedia.Penelitian Terkait 

III. PENELITIAN TERKAIT 

Penelitian  sebelumnya  mengenai  pembangunan  chatbot  di 
lingkungan Politeknik Statistika STIS oleh W. Syahputra [5]. 
Chatbot tersebut digunakan untuk peningkatan layanan humas 
Penerimaan Mahasiswa Baru Politeknik Statistika. Penggunaan 
chatbot  tersebut    menerapkan  metode  TF-IDF  dan  Cosine 
Similarity. Selanjutnya Pengembangan chatbot dengan metode 
TF-IDF  dan  Cosine  Similarity  juga  dikembangkan  oleh  D. 
Suryani  et  al.[3].  Chatbot  tersebut  diterapkan  dalam  mencari 
kata kunci dari dokumen yang terkait layanan publik kesehatan 
di  Kota  Malang.  Beberapa  ringkasan  mengenai  penelitian 
terkait adalah sebagai berikut. 

1.  Penelitian oleh W. Syahputra (2018) berjudul “Sistem 
Tanya Jawab Humas PMB Politeknik Statistika STIS” 
menjelaskan  tentang  penggunaan  chatbot  FAQ  di 
lingkungan  Penerimaan  Mahasiswa  Baru 
(PMB) 
Politeknik Statistika STIS. 

2.  Penelitian  oleh  D.  S.  Hormansyah  dan  Y.  P.  Utama 
berjudul  “Aplikasi Chatbot berbasis Web pada Sistem 
Informasi Layanan Publik Kesehatan di Malang dengan 
Menggunakan Metode TF-IDF” vol. 4 no. 3 pada Jurnal 
Informatika  Polinema  (2018)  menjelaskan  tentang 
pemanfaatan  TF-IDF  pada  chatbot  untuk  mencari 
informasi mengenai layanan publik Kesehatan di Kota 
Malang. 

3.  Penelitian  oleh  Dimas  et  al.  berjudul  “Penerapan 
Metode TF-IDF Untuk Chatbot Pada Sistem Informasi 
pada  Seminar 
Pelayanan  Percetakan  Online” 
Informatika  Aplikatif  Polinema 
(2020) 
(SIAP) 
menjelaskan tentang pemanfaatan chatbot TF-IDF pada 
pelayanan pelanggan percetakan online. 

4.  Penelitian  oleh  E.  L.  Amalia  dan  D.  W.  Wibowo 
berjudul 
“Rancang  Bangun  Chatbot  Untuk 
Meningkatkan  Performa  Bisnis”  pada  Jurnal  Ilmiah 
Teknologi  Asia  vol.  13  no.  2  pada  Jurnal  Ilmiah 
Teknologi  Informasi  Asia  (2019)  menjelaskan  tentang 
pengembangan chatbot. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

IV. METODE PENELITIAN 

A.  Ruang Lingkup Penelitian 

Ruang lingkup penelitian ini mencakup analisis pada sistem 
pelayanan  pertanyaan  pengunjung  web  WARKOP  Pusdiklat 
BPS  yang  telah  berjalan,  analisis  terhadap  sistem  usulan, 
perancangan sistem usulan (termasuk antarmuka pengguna dan 
basis  data),  serta  implementasi  dan  evaluasi  terhadap  sistem 
usulan. 

Penelitian ini terbatas pada permasalahan metode menjawab 
pertanyaan  yang  masuk  dari  pengunjung  web  WARKOP 
Pusdiklat  BPS,  membuat  sistem  khusus  untuk  admin  web 
dalam  mengelola  pertanyaan  dan  jawaban,  serta  menyimpan 
berbagai  pertanyaan  dan  jawaban  dalam  sistem  knowledge 
management. 

B.  Metode Pengumpulan Data 

1.  Wawancara 

ini  adalah 
Narasumber  utama  dalam  penelitian 
pegawai  bagian  TI  Pusdiklat  BPS 
sekaligus 
merupakan admin web WARKOP Pusdiklat. Metode 
ini  digunakan  untuk  menemukan  masalah  dalam 
sistem yang berjalan. 

2.  Observasi 

Peneliti  memperhatikan  proses  pengunjung  web 
melakukan  pertanyaan  hingga  admin  menjawab 
pertanyaan. 

3.  Kuesioner 

Kuesioner ini bertujuan untuk menguji kelayakan dan 
evaluasi sistem usulan. Pada penelitian ini kuesioner 
yang digunakan yaitu  System Usability Scale (SUS). 
SUS  digunakan  untuk  mengetahui  tingkat  kepuasan 
pengguna layanan. Terdapat 10 item pertanyaan yang 
nantinya diajukan kepada responden.  

4.  Studi Pustaka 

Studi  pustaka  dilakukan  dengan  berbagai  media 
seperti buku, jurnal, dll. Studi Pustaka dilakukan baik 
dengan media cetak maupun dengan media elektronik. 
Tujuan  studi  pustaka  adalah  untuk  mendapatkan 
informasi terkait WARKOP  Pusdiklat dan mengenai 
pemecahan masalah yang ada . 

C.  Metode Analisis Sistem 

1.  Metode  analisis  masalah  pada  penelitian 

ini 
menggunakan  fishbone  diagram  (Ishikawa  diagram) 
yang  dibagi  menjadi  4  bagian  yaitu,  Manpower 
(manusia), Method (metode), Machine, dan Material 
(bahan). 

2.  Metode  analisis  kebutuhan  fungsional  menggunakan 
analisis  deskriptif 
telah 
ditemukan  pada  sistem  sebelumnya.  Sementara 
fungsional  menggunakan  diagram 
analisis  non 
PIECES 
Information,  Economy, 
Control, Efficiency, dan Service). 

terkait  masalah  yang 

(Performance, 

3.  Metode  pengujian  sistem  menggunakan  SUS  dan  

blackbox testing. 

D.  Landasan Teori 

1.  TF-IDF  (Terms  Frequency  –  Inverse  Document 

Gambar 1. Peta literatur 

Frequency) 

 2 / 8 

 
 
 
 
 
dokumen 

[5].  Metode 

Metode  TF-IDF  merupakan  suatu  cara  untuk 
memberikan  bobot  hubungan  suatu  kata  (term) 
TF-IDF 
terhadap 
menggabungkan dua konsep untuk perhitungan bobot, 
yaitu  frekuensi  kemunculan  sebuah  kata  di  dalam 
sebuah  dokumen  tertentu  (Term  Frequency)  dan 
inverse  frekuensi  dokumen  yang  mengandung  kata 
tersebut  (Inverse  Document  Frequence).  Frekuensi 
kemunculan  kata  di  dalam  dokumen  yang 
mengandung  kata  tersebut  menunjukkan  seberapa 
umum penggunaan kata tersebut. Sehingga nilai bobot 
hubungan  antara  sebuah  kata  dan  sebuah  dokumen 
akan  tinggi  apabila  frekuensi  kata  tersebut  tinggi  di 
dalam dokumen dan frekuensi keseluruhan dokumen 
yang  mengandung  kata 
tersebut  sedikit  pada 
kumpulan dokumen. Rumus TF-IDF : 

𝑇𝐹 = 0,5 + 0,5  ×  
𝐷
𝑑𝑓𝑡

𝐼𝐷𝐹𝑡 = log

𝑇𝐹

                (1) 

max (𝑇𝐹)
                              (2) 

𝑊𝑑,𝑡 =   𝑇𝐹𝑑,𝑡   × 𝐼𝐷𝐹𝑑,𝑡 

Keterangan : 
TF 

Max(TF) 

Nilai D    
DFt 

IDF 
D 
T 
W 

:  Banyaknya  kata  yang  dicari  pada 
sebuah dokumen 
:  Jumlah  kemunculan  terbanyak  term 
pada dokumen yang sama  
: Dokumen ke-d 
:  Jumlah  dokumen  yang  mengandung 
term t 
: Inversed Document Frequency 
: Dokumen ke-d 
: Kata ke-t dari kata kunci 
: Bobot dokumen ke-d terhadap kata ke-
t 

2.  Cosine Similarity 

Consine Similarity digunakan untuk melakukan 
perhitungan  kesamaan  dari  dokumen.  Rumus  yang 
digunakan oleh consine similarity adalah [7] : 

Cos a = 

𝐴 • 𝐵

|𝐴||𝐵|

=  

√∑

𝑛
𝑖=1

∑

𝑛
𝑖=1 ×𝐵𝑖
𝐴𝑖
𝑛
𝑖=1

×√∑

(𝐴𝑖)2

(𝐵𝑖)2

Keterangan : 
A  

B  

akan  dibandingkan 

akan  dibandingkan 

:Vektor  A,  yang 
kemiripannya 
:Vektor  B,  yang 
kemiripannya 
: Dot product antara vektor A dan vektor B 
: Panjang vektor A 
: Panjang vektor B 
: Cross product antara |A| dan |B| 

A • B  
|A|    
|B|    
|A||B|  
Dalam  melakukan  perhitungan  cosine  similarity,  hal 
pertama  yang  perlu  dilakukan  adalah  melakukan 
perkalian  skalar  antara  pertanyaan  dengan  dokumen 
kemudian  dijumlahkan, 
itu  melakukan 
perkalian  antara  panjang  dokumen  dengan  panjang 

setelah 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

pertanyaan  yang  telah  dikuadratkan,  setelah  itu  di 
hitung akar pangkat dua. Selanjutnya hasil perkalian 
skalar tersebut di bagi dengan hasil perkalian panjang 
dokumen dan pertanyaan. 
3.  Metode Pengembangan Sistem 

Metode  pengembangan  sistem  yang  digunakan  pada 
penelitian ini adalah Software Development Life Cycle 
(SDLC) dengan model Waterfall. SDLC dengan model 
Waterfall meliputi aktivitas-aktivitas sebagai berikut : 
1.  Project planning 
2.  Analysis 
3.  Design 
4. 
5.  Support 

Implementation 

Gambar 2. Waterfall model 

4.  Python 

Python  adalah  bahasa  pemrograman  interpretatif, 
berorinetasi  objek  dan  semantik  yang  dinamis[8]. 
Python  dilengkapi  dengan  modul-modul  yang  dapat 
digunakan  kembali.  Interpreter  Python  dan  standard 
library-nya tersedia secara gratis untuk semua platform 
dan  merupakan  open  source.  Beberapa  fitur  yang 
dimiliki Python[9]: 

•  memiliki  kepustakaan  yang 

luas;  dalam 
distribusi  Python  telah  disediakan  modul-
modul. 

•  memiliki tata bahasa yang jernih dan mudah 

dipelajari. 

•  memiliki  aturan  layout  kode  sumber  yang 
pembacaan 

memudahkan 
kembali dan penulisan ulang kode sumber. 

pengecekan, 

5.  Flask 

Flask  merupakan  web  framework  yang  dirancang 
untuk bahasa pemrograman python. Flask dibuat agar 
dapat  memudahkan 
dalam 
mengembangkan aplikasi berbasis web. ada  beberapa 
fitur  penting  flask  yaitu  build-in  web  server  dan 
debugger,  mendukung  unit  testing,  RESTful  request, 
secure cookies, WSGI, dan mendukung Unicode [10]. 

pengguna 

python 

6.  System Usability Scale (SUS) 

System  Usability  Scale  (SUS)  merupakan  kuesioner 
yang dapat digunakan untuk mengukur usability sistem 
menurut  sudut  pandang  subyektif  pengguna  [11]. 

 3 / 8 

 
 
 
 
 
   
   
   
   
   
 
 
  
 
 
 
 
 
Kuesioner ini dirancang oleh John Brooke pada tahun 
1986  untuk  mengevaluasi  berbagai  jenis  sistem  atau 
produk  secara  praktis.  SUS  menilai  kepuasan 
pengguna  melalui 10 item pertanyaan dengan pilihan 
jawaban dari sangat tidak setuju hingga sangat setuju 
dengan  nilai  satu  sampai  lima.  Kuesioner  SUS 
memiliki beberapa kelebihan yaitu sebagai berikut : (1) 
SUS 
tidak 
sangat  mudah  digunakan,  karena 
membutuhkan  perhitungan  yang  rumit  [12];  (2)  SUS 
tersedia  secara  gratis,  tidak  membutuhkan  biaya 
tambahan [13]; dan (3) SUS terbukti valid dan andal, 
walau dengan ukuran sampel yang kecil [11]. Berikut 
adalah rumus SUS : 

𝑆𝑘𝑜𝑟 𝑆𝑈𝑆  =   ((𝑅1  −  1)   +   (5  −  𝑅2)   +   (𝑅3

−  1)   + (5  −  𝑅4)   +   (𝑅5  −  1)  
+   (5  −  𝑅6)   + (𝑅7  −  1)   +   (5 
−  𝑅8)   +   (𝑅9  −  1)   + (5 
−  𝑅10))   ∗  2.5 

7.  Precision dan Recall 

Metode  yang  digunakan  untuk  mengukur  ketepatan 
ini  adalah 
jawaban  yang  diberikan  oleh  sistem 
precision  dan  recall.  Recall  adalah  proporsi  jumlah 
dokumen yang dapat ditemukan-kembali oleh sebuah 
proses  pencarian  di  sistem  Information  Retrival. 
Sementara, precision adalah proporsi jumlah dokumen 
yang ditemukan dan dianggap relevan untuk kebutuhan 
si  pencari  informasi[14].  Menurut  B.  Zaman  dan  E. 
Winarko, precision dan recall memiliki rumus sebagai 
berikut sebagai berikut[15] : 

𝑅𝑒𝑐𝑎𝑙𝑙 =  

𝑗𝑢𝑚𝑙𝑎ℎ 𝑘𝑎𝑙𝑖𝑚𝑎𝑡 𝑟𝑒𝑙𝑒𝑣𝑎𝑛 𝑦𝑎𝑛𝑔 𝑑𝑖ℎ𝑎𝑠𝑖𝑙𝑘𝑎𝑛 𝑠𝑖𝑠𝑡𝑒𝑚
𝑗𝑢𝑚𝑙𝑎ℎ  𝑘𝑎𝑙𝑖𝑚𝑎𝑡 𝑟𝑒𝑙𝑒𝑣𝑎𝑛

𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 =  

𝑗𝑢𝑚𝑙𝑎ℎ 𝑘𝑎𝑙𝑖𝑚𝑎𝑡 𝑟𝑒𝑙𝑒𝑣𝑎𝑛 𝑦𝑎𝑛𝑔 𝑑𝑖ℎ𝑎𝑠𝑖𝑙𝑘𝑎𝑛 𝑠𝑖𝑠𝑡𝑒𝑚
𝑗𝑢𝑚𝑙𝑎ℎ 𝑘𝑎𝑙𝑖𝑚𝑎𝑡 𝑦𝑎𝑛𝑔 𝑑𝑖 ℎ𝑎𝑠𝑖𝑙𝑘𝑎𝑛 𝑠𝑖𝑠𝑡𝑒𝑚

V.  KERANGKA PIKIR 

solusi,  perancangan 

Diagram  kerangka  pikir  menggambarkan  bahwa  terdapat 
enam bagian pada pengembangan sistem chatbot. Enam bagian 
tersebut  yaitu  masalah, 
sistem, 
implementasi,  pengujian,  dan  kesimpulan.  Pada  bagian 
masalah terdapat tiga masalah utama yang dapat teridentifikasi. 
Selanjutnya terdapat dua solusi yang dinilai dapat memecahkan 
permasalahan. Pada bagian implementasi sistem dipilih metode 
TF-IDF dan Cosine Similarity. Kemudian, dipilih  framework 
Flask  dan  MySQL  sebagai  media  implementasi.  Setelah  itu, 
sistem diuji dengan metode blackbox dan SUS. Dari pengujian 
ini dapat diketahui apakah sistem sudah dapat menyelesaikan 
permasalahan atau belum. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 3. Kerangka pikir 

VI. HASIL DAN PEMBAHASAN 

A.  Pengolahan Kata 

1.  Preprocessing 
Tahapan 
tokenization, 
  Stopword  Removal  atau  Filtering,  dan  Stemming. 
Tahapan  ini  bertujuan  untuk  mengubah  data  mentah 
menjadi siap olah. 

terdiri 

dari 

ini 

2.  TF-IDF 

Tahapan  ini  terdiri  dari  dua  metode  yaitu  Term 
frequence (TF) dan Inverse Documen Frequency (IDF). 
Metode term frequence (TF) merupakan metode yang 
menghitung  frekuensi  kemunculan  suatu  kata  dalam 
dokumen jawaban dalam basis data. Sedangkan metode 
inverse document frequence (IDF) merupakan metode 
mengenai perhitungan persebaran kata pada dokumen. 

3.  Cosine Similarity 

Pada  tahapan  ini  tujuan  dari  dilakukannya  metode 
cosine similarity adalah untuk melakukan perhitungan 
kesamaan  antar  pertanyaan  pengunjung  web  dan 
jawaban yang ada. 

B.  Analisis Sistem Berjalan 

Pada  tahap  ini,  peneliti  melakukan  wawancara  dengan 
subject matter untuk mendapatkan informasi tentang sistem 
pelayanan  tanya  jawab  pengunjung  web  yang  sedang 
berjalan di web WARKOP Pusdiklat. Dari hasil wawancara 
diketahui  bahwa  sistem  yang  berjalan  hanya  dapat 
menjawab  pertanyaan  pengunjung  web  WARKOP 

 4 / 8 

 
 
 
 
 
 
 
Pusdiklat  saat  admin  sedang  online  saja.  Sehingga,  saat 
pengunjung bertanya melalui sistem ini dan admin sedang 
offline, pengunjung harus menunggu hingga admin  online 
untuk mendapatkan jawaban. Pengunjung web juga sering 
menanyakan pertanyaan seputar topik yang sama dan sudah 
terjawab. Walaupun demikian, admin tetap harus menjawab 
pertanyaan  sejenis  dengan  jawaban  yang  sama  meskipun 
telah dijawab oleh admin sebelumnya. Gambar 4 berikut ini 
menunjukkan  proses  bisnis  pada  pelayanan  tanya  jawab 
pengunjung web WARKOP Pusdiklat yang sedang berjalan 
saat ini. 

 Gambar 4. Proses Bisnis Sistem Berjalan 

C.  Analisis Permasalahan 

Gambar 5. Fishbone Diagram 

Berdasarkan analisis sistem berjalan yang telah dilakukan, 
sumber  permasalahan  yang  terjadi  dapat  digambarkan 
fishbone  diagram.  Fishbone  diagram  dibagi 
dengan 
menjadi  4  bagian  yaitu,  Manpower  (manusia),  Method 
(metode), Machine, dan Material (bahan). 
Penjelasan dapat dilihat pada gambar 5. 

D.  Analisis Kebutuhan 

TABEL II 

Kebutuhan non fungsional 

Faktor 

(1) 

Performance 

Permasalahan 

(2) 
Pertanyaan 
diajukan 

Solusi 

(3) 

yang 
oleh 

Membuat 
yang 

sistem 
dapat 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

pengunjung 
web 
tidak  dapat  selalu 
dijawab oleh admin. 

menjawab 
Untuk 
yang 
pertanyaan 
admin 
berulang 
harus 
memeriksa 
jawaban  sebelumnya 
agar  jawaban  yang 
diberikan konsisten. 
tidak 
Pusdiklat 
memiliki 
Sumber 
Daya  Manusia  yang 
cukup  untuk  selalu 
menjawab 
pertanyaan 
pengunjung 
dengan cepat. 

web 

Admin  web 
tidak 
dapat  memberikan 
secara 
jawaban 
langsung  saat  admin 
sedang tidak online. 

yang 
pernah 
oleh 
web 

Admin  web  perlu 
menjawab 
pertanyaan 
sudah 
ditanyakan 
pengunjung 
secara manual. 
Pelayanan  customer 
service  tidak  dapat 
dilakukan setiap saat 
oleh admin. 

Information 

Economy 

Control 

Efficiency 

Service 

menjawab 
yang 
pertanyaan 
oleh 
diajukan 
web 
pengunjung 
meskipun 
admin 
sedang tidak online. 
sistem 
Membuat 
yang 
dapat 
menjawab 
pertanyaan  berulang 
dengan 
jawaban 
yang konsisten. 

sistem 
dapat 

web 
cepat 

yang 

sistem 
dapat 

web 
admin 

sistem 
dapat 

yang 
pernah 
oleh 

Membuat 
yang 
menjawab 
pertanyaan 
pengunjung 
dengan 
berdasarkan 
pengetahuan 
sudah diberikan. 
Membuat 
yang 
menjawab 
pertanyaan 
pengunjung 
meskipun 
sedang offline. 
Membuat 
yang 
menjawab 
pertanyaan 
sudah 
ditanyakan 
pengunjung web. 
Membuat 
yang 
dapat 
pertanyaan 
pengunjung web. 

setiap 

sistem 
saat 
melayani 

E.  Sistem Usulan 
penulis 
Berdasarkan 
merancang proses bisnis sistem usulan seperti yang terlihat 
pada gambar 6. 

kebutuhan 

tersebut, 

analisis 

Gambar 6. Proses Bisnis Sistem Usulan 

 5 / 8 

 
 
 
        
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pada  sistem  usulan,  sistem  diharapkan  dapat  melayani 
pertanyaan pengunjung web WARKOP Pusdiklat meskipun 
admin sedang offline. Selanjutnya, jika ada pertanyaan yang 
belum ada jawabannya dalam basis data sistem, pengunjung 
dapat mengajukan pertanyaan dalam form dengan mengisi 
nama, alamat email, dan pertanyaan. 
Berdasarkan  pemaparan  dari  proses  bisnis  sebelumnya, 
dilakukan  perancangan  diagram  use 
case  untuk 
mendeskripsikan interaksi antar aktor sistem dengan fitur-
fitur  sistem  tersebut.  Diagram  use  case  tersebut  dapat 
dilihat pada gambar 7 sebagai berikut.  

3.  Tabel  Pengguna  untuk  menyimpan  data  pengguna 
termasuk  aktivitas  penggunaan  halaman  web  Halo 
WARKOP. 

4.  Tabel Konfigurasi untuk menyimpan data konfigurasi 
email seperti server email yang digunakan, port email, 
TLS, SSL, username email, serta password. 

Pada saat pertama kali membuka  sistem Halo WARKOP, 
pengguna  diwajibkan  untuk  mengisi  nama  dan  email 
pengunjung  sesuai  dengan  tampilan  gambar  9.  Setelah 
mengisi nama dan email, pengguna diarahkan ke halaman 
room chat seperti pada gambar 10. 

Gambar 9. Tampilan awal Halo WARKOP 

Gambar 7. Diagram Use Case Sistem Usulan 

Pada  use case terlihat bahwa  pengunjung web WARKOP 
Pusdiklat  tidak  login  terlebih  dahulu  untuk  mengakses 
chatbot. Hal ini dimaksudkan agar penggunaan chatbot oleh 
pengunjung web dapat lebih mudah dan simpel. Admin web 
diharuskan  login  terlebih  dahulu  agar  dapat  melakukan 
pengelolaan data pertanyaan dan jawaban. Pengelolaan data 
pertanyaan dan jawaban meliputi menambah, menghapus, 
dan mengedit. 

Gambar 10. Tampilan Halaman Chat 

Halaman di atas merupakan tampilan antarmuka room chat 
Halo  Warkop.  Terdapat  navbar  berupa  informasi  menu 
yang dapat dipilih oleh pengguna. Selain itu, pengguna juga 
dapat  mengakses  halaman  cari  jawaban.  Halaman    cari 
jawaban  diperuntukkan  kepada  pengguna  web  yang  tidak 
menemukan  jawaban  atas  pertanyaannya  di  halaman  chat 
room. Sementara halaman login diperuntukkan hanya untuk 
admin. Admin harus login untuk dapat mengakses halaman 
pengelolaan data pertanyaan dan jawaban. 

Gambar 8. Entity Relationship Diagram (ERD) Sistem Usulan 

Gambar  8 di atas merupakan gambar rancangan  database 
yang digunakan pada sistem usulan. Terdapat 4 tabel yang 
digunakan  pada  sistem  usulan  dengan  uraian  sebagai 
berikut: 
1.  Tabel  Admin untuk  menyimpan  data  tiap  admin  web 
WARKOP  Pusdiklat  meliputi  username,  email, 
password, dan waktu terakhir kali login. 
Pertanyaan_jawaban 

untuk  menyimpan 
kumpulan  pertanyaan  dan  jawaban  yang  dijadikan 
sebagai pengetahuan awal chatbot. 

2.  Tabel 

 6 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Gambar 11. Tampilan Halaman Admin (Pengelolaan data pertanyaan dan 
jawaban) 

jawaban, 

lima  menu  utama  yaitu 
Halaman  admin  memiliki 
pertanyaan-jawaban, 
pengunjung, 
kirim 
konfigurasi  email,  serta  tempat  sampah.  Gambar  11 
merupakan  halaman  yang  menampilkan  pertanyaan  dan 
jawaban sebagai pengetahuan awal chatbot. Pertanyaan dan 
jawaban  ini  dapat  dikelola  oleh  admin.  Pengelolaan 
pertanyaan  dan  jawaban  yang  dilakukan  oleh  admin 
meliputi menambah, mengedit, dan menghapus. Jika admin 
menghapus pertanyaan pada halaman ini, pertanyaan hanya 
terhapus  sementara.  Dengan  begitu  jika  terjadi  kesalahan 
hapus,  admin  dapat  memulihkan  Kembali  pertanyaan dan 
jawaban. 

Gambar 12. Tampilan Halaman Admin (Halaman Kirim Jawaban) 

Gambar 12 merupakan halaman yang menampilkan pertanyaan 
yang  telah  dikirim  oleh  pengunjung  web.  Pertanyaan  ini 
selanjutnya  dijawab  oleh  admin  melalui  email.  Pengunjung 
web  mengirim  pertanyaan  kepada  admin  jika  jawaban  dari 
pertanyaan  tersebut  belum  tersedia  di  pengetahuan  awal 
chatbot. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 14. Tampilan Halaman Konfigurasi 

Gambar 14 menampilkan halaman konfigurasi email. Admin 
dapat  mengubah  konfigurasi  email  tanpa  harus  membongkar 
kode  sistem.  Admin  dapat  mengubah  username  email, 
password, server SMTP, serta port.  

Gambar 15. Tampilan Halaman Tempat Sampah 

Gambar 15 menampilkan halaman tempat sampah di mana 
pertanyaan  yang  telah  terhapus  sementara  masih  dapat 
dipulihkan  kembali.  Admin  dapat  menghapus  permanen 
pertanyaan  tersebut  jika  pertanyaan  dan  jawaban  dianggap 
sudah tidak relevan. 

Setelah  itu  dilakukan  pengujian  sistem  dengan  kuesioner 
SUS.  Pada  pengujian  ini  kuesioner  disebar  kepada  10  orang 
yang merupakan admin WARKOP, pegawai BPS, serta orang 
yang berpotensi menjadi pengguna WARKOP. Dari pengujian 
didapatkan nilai rata-rata sus sebesar 84,54. Nilai SUS berada 
di atas 70 sehingga dapat dikategorikan Acceptable[11]. 

TABEL III 
Hasil Uji SUS 

Gambar 13. Tampilan Halaman Pengunjung 

Gambar 13 merupakan tampilan halaman pengunjung yang 
menampilkan  aktivitas  pengunjung  web  dalam  jangka  waktu 
satu minggu terakhir. 

Responden 

(1) 
R1 
R2 
R3 
R4 
R5 
R6 
R7 
R8 
R9 
R10 
R11 

Skor 
Q1  Q2  Q3  Q4  Q5  Q6  Q7  Q8  Q9  Q10  Akhir 
(12) 
(2) 
90 
3 
77.5 
4 
85 
4 
100 
5 
95 
5 
62,5 
3 
97,5 
5 
97,5 
5 
65 
4 
70 
3 
90 
5 

(9)  (10)  (11) 
5 
1 
5 
1 
4 
2 
5 
1 
5 
1 
3 
3 
5 
1 
5 
1 
4 
4 
4 
3 
5 
2 

(8) 
5 
5 
5 
5 
5 
3 
5 
5 
4 
3 
4 

(6) 
5 
4 
4 
5 
5 
4 
5 
4 
4 
5 
5 

(4) 
5 
5 
5 
5 
5 
4 
5 
5 
4 
4 
5 

(5) 
1 
2 
1 
1 
2 
1 
1 
1 
1 
1 
1 

(3) 
1 
2 
2 
1 
1 
2 
1 
1 
3 
1 
2 

(7) 
2 
2 
1 
1 
1 
1 
2 
1 
3 
3 
2 

2 
5 
2 
1 
2 
5 
1 
1 
3 
1 
1 
Rata- 
Rata 

84,54 

 7 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Pada Q8 dan Q10 memiliki respons yang beragam dibanding 
pertanyaan  lain.    Q8  merupakan  pertanyaan  mengenai  “Saya 
merasa  sistem  ini  membingungkan”  dengan  enam  respon 
Sangat Tidak Setuju (STS), satu respon Tidak Setuju (TS), dua 
respon  Ragu-ragu  (RG),  satu  respon  Setuju  (ST),  dan  nol 
Sangat  Setuju  (SS).  Dari  seluruh  responden  hanya  1  yang 
menganggap  sistem  ini  membingungkan,  sisanya  cenderung 
mudah  memahami  sistem  ini.  Q10  merupakan  pertanyaan 
mengenai  “Saya  perlu  membiasakan  diri  terlebih  dahulu 
sebelum  menggunakan  sistem  ini”  dengan  tiga  respon  STS, 
empat respon TS, satu respon RG, satu respon ST, dan satu SS. 
Pada respon Q10 responden cenderung mudah menerima dan 
nyaman menggunakan sistem. Beragamnya respon Q8 dan Q10 
dapat  disebabkan  oleh  kekurangan  yang  ada  pada  sistem 
sehingga  user  masih  ada  yang  menganggap  membingungkan 
dan perlu waktu untuk beradaptasi. Hal ini dapat diatasi dengan 
menyediakan  dokumentasi  petunjuk  yang  lengkap  untuk 
pengguna. 

Untuk  menguji  ketepatan  jawaban  yang  diberikan  sistem, 
maka  dilakukan  pengujian  dengan  precision  dan  recall. 
Pengujian dilakukan 95 kali dengan 6 merupakan salam dan 89 
merupakan  pertanyaan.  Dari  pengujian  didapatkan  precision 
sebesar 90.45% dan recall sebesar 100%. 

VII. 

PENUTUP 

Bagian ini berisi uraian kesimpulan dari hasil penelitian dan 

pembahasan, berikut saran penelitian. 

Berdasarkan  pemaparan 

sistem  diatas  maka  dapat 

disimpulkan : 

1.  Metode  TF-IDF  dan  Cosine  similarity  berhasil 
diterapkan  pada  sistem  dengan  precision  90.45%  dan 
recall sebesar 100%. 

2.  Sistem  untuk  mengelola  pertanyaan  dan  jawaban 
berhasil  dibuat  dan  telah  diuji  menggunakan  SUS 
dengan hasil akhir diterima oleh pengguna. 

Adapun kekurangan dari penelitian ini adalah sebagai berikut 

1.  Chatbot  masih  belum  dapat  memahami  pertanyaan 
dengan  makna  serupa  tetapi  kalimat  pertanyaan 
berbeda yang tidak mengandung kata kunci.  

2.  Pada  chatbot  belum  bisa  diterapkan  metode  deep 
learning sebab jumlah data awal belum terpenuhi. 
3.  Halaman  chat  room  masih  belum  dapat  ditampilkan 
dalam mode  widget javascript sehingga belum dapat 
ditampilkan langsung dalam web WARKOP. 

Berikut saran pada penelitian ini: 
1.  Pada  penelitian 

selanjutnya  diharapkan  dapat 
menerapkan  metode  deep  learning  agar  chatbot 
mengerti  pertanyaan dengan  kalimat  berbeda  namun 
konteks yang sama. 

2.  Pada penelitian selanjutnya halaman chat room dapat 

ditampilkan langsung dalam web WARKOP. 

DAFTAR PUSTAKA 
[1]  Pusdiklat  BPS.  (2020,  11)  Profil  Pusdiklat  BPS  [Online].  Available: 

https://www.pusdiklat-bps.id/web/profilpusdiklat 

[2]  WARKOP Pusdiklat BPS. (2020,11) Profil WARKOP [Online]. Available: 

https://pusdiklat-bps.id/warkop/  

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

[3]  D. Suryani & E. L. Amalia, “Aplikasi Chatbot Objek Wisata Jawa Timur 
Berbasis  AIML,”  SMARTICS  Journal,  vol.3,  no.2,  pp.  4754,  Oktober 
2017.  

[4]  D.  W.  Wibowo  et  al.,  “Penerapan  Metode  TF-IDF  untuk  Chatbot  pada 
Sistem  Informasi  Pelayanan  Percetakan  Online,”  Seminar  Informatika 
Aplikatif Polinema (SIAP), pp. 196200, 2020. 

[5]  W.  Syahputra.  “Sistem  Tanya  Jawab  Humas  PMB  Politeknik  Statistika 

STIS,” skripsi, 2018. 

[6]  M.  Nurjannah,  Hamdani,  dan  I.  F.  Astuti,  “Penerapan  Algoritma  Term 
Frequency-Inverse  Document  Frequency  (TF-IDF)  untuk  Text  Mining,” 
Jurnal Informatika Mulawarman, vol.8, no.3, pp. 110113, September 2013. 
[7]  J.  ye,  “Vector  Similarity Measures of  Simplified  Neutrosophic  Sets and 
Their Application in Multicriteria Decision Making,” International Journal 
of Fuzzy Systems, vol. 16, no. 2, pp. 204211, Juli 2014. 

[8]  Python.  (2021,  6).  What  is  Python?  Executive  Summary.  [Online]. 

Available: https://www.python.org/doc/essays/blurb/ 

[9]  T.  R.  Perkasa  et.al.  “Rancang  Bangun  Pendeteksi  Gerak  Menggunakan 
Metode Image Subtraction pada Single Board Computer (Sbc),” Journal of 
Control and Network Systems, vol.3, no.2, pp.9097, 2014.   
6). 

[10] Devopedia. 

Available: 

[Online]. 

(2021, 

Flask. 

https://devopedia.org/flask  

[11] J.  Brooke, “SUS:  A  Retrospective,” JUS,  vol.8  no.2, pp.2940,  February 

2013. 

[12] A.  Bangor  dan  p.  Kortum,  “Determining  What  Individual  SUS  Scores 
Mean: Adding an Adjective Rating Scale,” JUS vol. 4, no. 3, pp.114123, 
Mei 2009. 

[13] I.  Aprilia,  P.  I.  Santoso,  dan  R.  Ferdiana,  “Pengujian  Usability  Website 
Menggunakan  System  Usability  Scale  Website  Usability  Testing  using 
System Usability Scale,” IPTEK-KOM, Vol. 17 No. 1, pp. 3138, Juni 2015. 
[14] D. S. Hormansyah dan Y. P. Utama, “Aplikasi Chatbot berbasis Web pada 
Sistem  Informasi  Layanan  Publik  Kesehatan  di  Malang  dengan 
Menggunakan Metode TF-IDF,” Jurnal Informatika Polinema, vol. 4 no. 3, 
pp. 224228, Mei 2018. 

[15] B. Zaman dan E. Winarko. “Analisis Fitur Kalimat untuk Peringkas Teks 
Otomatis pada Bahasa Indonesia,” IJCCS, Vol.5 No.2, pp. 6068, Juli, 2011. 
[16] D. A. R. Ariantini et al., “Pengukuran Kemiripan Dokumen Teks Bahasa 
Indonesia  Menggunakan  Metode  Cosine  Similarity,”  E-Journal  Teknik 
Informatika, vol.9 no.1, pp. 18, 2016. 

[17] A.  Sudyana.  “Pembangunan  Sistem  Informasi  Helpdesk  Modul  Sistem 

Tanya Jawab,”  skripsi, 2015. 

 8 / 8 

 
 
 
"
221710020,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Analisis Clustering Kerentanan Sosial terhadap 

Bencana Alam Menggunakan Geographically Weighted 
Principal Components Analysis dan Fuzzy 
Geographically Weighted Clustering 
(Studi kasus di seluruh kabupaten/kota di Indonesia tahun 2019) 

Sufyan Aziz Prabaswara (221710020, 4SD2) 
Dosen Pembimbing: Yuliagnis Transver Wijaya 

Ringkasan—  Indonesia  merupakan  negara  dengan  jumlah 
bencana  alam  yang  banyak.  Bencana  alam  yang  ada  dapat 
menyebabkan  berbagai  risiko  pada  masyarakat.  Besarnya  risiko 
yang  harus  diderita  oleh  masyarakat  dapat  dilihat  dari  tingkat 
kerentanan  sosial  terhadap  bencana  alam  di  wilahnya.  Semakin 
tinggi tingkat kerentanan suatu daerah, semakin tinggi juga risiko 
yang  akan  didapatkan.    Indonesia  memiliki  letak  geografis  yang 
bermacam-macam    yang  mana  menyebabkan  kerentanan  sosial 
tiap  daerah  di  Indonesia  berbeda-beda.  Oleh  karena  itu  perlu 
dilakukan  penelitian  untuk  mengetahui  gambaran  mengenai 
kerentanan  sosial  di  seluruh  Indonesia.  Penelitian  ini  meneliti 
tentang  persebaran  kerentanan  sosial  yang  ada  di  Indonesia 
menggunakan  metode  Geographically  Weighted  Principal 
Components  Analysis  (GWPCA)  dan  juga  melakukan  clustering 
yaitu  dengan  menerapkan  Fuzzy  Geographically  Weighted 
Clustering  (FGWC)  menggunakan  optimasi  Particle  Swarm 
Optimization  (PSO).  Hasil  yang  didapatkan  dari  penelitian  ini 
adalah gambaran kerentanan sosial yang ada di seluruh Indonesia 
dalam tinggi dan rendah kerentanan. 

Kata  Kunci—  Bencana  alam,  kerentanan  sosial,  GWPCA, 

FGWC-PSO. 

I.  LATAR BELAKANG 

Bencana alam merupakan peristiwa atau rangkaian peristiwa 
yang mengancam dan mengganggu penghidupan dan kehidupan 
masyarakat yang disebabkan faktor alam sehingga menimbulkan 
korban  jiwa,  kerusakan  lingkungan,  kerugian  harta  benda, dan 
juga  dampak  psikologis  masyarakat.  Indonesia  merupakan 
negara yang sangat rawan dengan bencana alam seperti gempa 
bumi,  tsunami,  gunung  berapi,  tanah  longsor,  banjir,  dan  juga 
angin  puting  beliung.  Secara  geologis,  kepulauan  Indonesia 
berada  pada 
lempeng  bumi,  seperti 
penunjangan lempeng Samudra Indo-Australia dengan Lempeng 
Benua  Eurasia.  Jalur  penunjangan  lempeng  bumi  di  wilayah 
kepulauan Indonesia merupakan jalur penyebab gempa tektonik 
regional  dan  umumnya  kerusakan  yang 
yang  bersifat 
ditimbulkan sangat parah.[1] 

jalur  penunjangan 

Gambar 1. Jumlah bencana alam di Asia Tenggara 2011-2017 (Sumber: [1]) 

tercatat 

Indonesia  dari 

tahun  ke 
dalam 

Faktanya,  Indonesia  merupakan  negara  dengan  jumlah 
bencana alam tertinggi di Asia Tenggara. Jumlah bencana alam 
tahun  semakin  meningkat 
di 
data  Badan  Nasional 
sebagaimana 
Penanggulangan  Bencana  (BNPB)  yang  ditampilkan  dalam 
Gambar  2.  Bencana  alam  yang  terjadi  di  Indonesia  tentunya 
menyebabkan beberapa kerusakan dalam bidang ekonomi, sosial, 
maupun  aktivitas  yang  ada  di  masyarakat.  Kerusakan  yang 
terjadi dapat dilihat dari ketahanannya. Semakin baik ketahanan 
yang ada maka dampak yang ditimbulkan akan semakin sedikit, 
begitu juga sebaliknya. Ketahanan itu sendiri dapat dilihat dari 
kerentanannya, semakin tinggi kerentanan daerah tersebut maka 
ketahanannya semakin rendah dan akan menyebabkan dampak 
yang lebih besar saat terjadi bencana alam. Risiko bencana yang 
ditimbulkan akan kecil jika kerentanan di suatu daerah tersebut 
kecil, begitu juga sebaliknya yang mana risiko akan sangat tinggi 
atau  besar  jika  kerentanannya  tinggi.  Contoh  yang  ada  adalah 
perbandingan  antara  negara  Jepang  dengan  Indonesia.  Jepang 
dan  Indonesia  merupakan  negara  yang  memiliki  keunikan 
wilayah  geografis,  yaitu  sering  dilanda  bencana  alam  seperti 
gempa  bumi.  Dapat  kita  lihat  bahwa  jika  terjadi  gempa  bumi, 
dampak yang diterima negara Jepang relatif lebih kecil daripada 
yang dirasakan di negara Indonesia. Hal ini dikarenakan angka 
kerentanan di Jepang lebih kecil sehingga dampak yang diterima 

 1 / 8 

 
 
 
 
 
 
 
juga lebih sedikit. Sedangkan di Indonesia, kerentanannya lebih 
tinggi sehingga dampak yang ditimbulkan juga lebih besar. 

tematik sehingga dapat digunakan untuk mengetahui gambaran 
umum kerentanan sosial yang ada di Indonesia. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

4500

4000

3500

3000

2500

2000

1500

1000

500

0

2010 2011 2012 2013 2014 2015 2016 2017 2018 2019

Gambar 2. Jumlah bencana alam di Indonesia 2010-2019 (Sumber: BNPB) 

Menurut S. L. Cutter, kerentanan terhadap bencana alam dari 
suatu  wilayah  dapat  berupa  kerentanan  infrastruktur  dan  juga 
kerentanan  sosial  [6].  Kerentanan  sosial  merupakan  gambaran 
kerapuhan sosial pada suatu wilayah. Kerentanan sosial di suatu 
wilayah  dipengaruhi  oleh  beberapa  faktor  sosial  seperti 
karakteristik  demografi  wilayah,  karakteristik  sosial  ekonomi 
wilayah, jaringan sosial atau masyarakat, serta jaringan politik 
atau sistem kelembagaan wilayah. Suatu wilayah yang memiliki 
kondisi sosial yang rentan, akan menimbulkan dampak kerugian 
yang besar ketika menghadapi ancaman atau bencana.[2] 

Beberapa  penelitian  sebelumnya  telah  meneliti  kerentanan 
sosial dalam skala kecil. Misalnya seperti penelitian pada suatu 
bencana alam yang terjadi  di suatu daerah atau beberapa daerah 
saja  yang  ada  di  Indonesia.  Ada  pula  peneliti  yang  meneliti 
mengenai  kerentanan  sosial  di  Indonesia  pada  level  provinsi, 
namun  belum  ada  penelitian  mengenai  hal  ini  yang  dilakukan 
hingga  pada  level  kabupaten  atau  kota  menggunakan  metode 
FGWC-PSO.  Karena  kondisi  geografis  Indonesia  dan  juga 
kondisi ekonomi maupun sosial yang sangat bermacam-macam, 
setiap  kabupaten  atau  kota  pun  dapat  memberikan  tingkat 
kerentanan  sosial  yang  berbeda-beda.  Dengan  banyaknya 
bencana alam yang telah terjadi dan besarnya potensi terjadinya 
bencana alam di Indonesia, diperlukan analisis kerentanan sosial 
yang lebih terperinci agar dapat menjadi acuan bagi pemerintah 
daerah  maupun  pemerintah  pusat  dalam  mencegah  dan 
menanggulangi bencana alam. Penelitian ini akan terfokus pada 
analisis clustering kerentanan sosial terhadap bencana alam pada 
level kabupaten atau kota menggunakan metode Geographically 
Weighted  Principal  Components  Analysis  dan  Fuzzy 
Geographically  Weighted  Clustering  Particle 
Swarm 
Optimization  sehingga  akan  dapat  dilihat  gambaran  mengenai 
kondisi  kerentanan  sosial  pada  level  kabupaten  atau  kota  di 
seluruh Indonesia. 

II.  TUJUAN PENELITIAN 

Tujuan  dari  penelitian 

ini  adalah  melakukan  analisis 
clustering kerentanan sosial terhadap bencana alam pada level 
kabupaten/kota  dan  memvisualisasikannya  ke  dalam  peta 

III. PENELITIAN TERKAIT 

juga  Bucharest,  Romania.  Penelitian 

Sebuah  penelitian  dilakukan  oleh  S.Rufat  dengan  judul 
“Spectroscopy of Urban Vulnerability” [3]. Dalam penelitian ini, 
Rufat menelaah tentang kerentanan sosial yang terdapat di Lyon, 
Francis  dan 
ini 
menggunakan  beberapa  variabel  yang  digunakan  untuk 
mengukur kerentanan sosial yang ada. Variabel-variabel tersebut 
adalah kepadatan penduduk, persentase penduduk dengan usia di 
bawah  10  tahun,  persentase  penduduk  dengan  usia  diatas  75 
tahun  ke  atas,  persentase  penduduk  disabilitas,  persentase 
penduduk  informal  atau  rumah  mobil,  persentase  perumahan 
yang rusak, persentase penduduk dengan pengangguran jangka 
jumlah 
panjang,  persentase  penduduk 
infrastruktur  olahraga,  jumlah  infrastruktur  medis,  jumlah 
infrastruktur  pertanian,  jumlah  infrastruktur  pendidikan  dan 
administrasi, jumlah stasiun transportasi, dan pusat perbelanjaan. 
Analisis  yang  digunakan  adalah  metode  PCA  (Principal 
Component  Analysis)  dan  metode  clustering  yang  digunakan 
yaitu  HAC  (Hierarchical  Ascendeant  Classification).  Dari 
penelitian ini dihasilkan 5 cluster yaitu profil kerentanan tinggi, 
profil kerentanan sosial yang tinggi, profil kerentanan menengah, 
profil kerentanan sosial yang berkurang, dan profil kerentanan 
sosial rendah.  

tanpa  pelatihan, 

Penelitian yang dilakukan oleh Y. N. Maharani, S. Lee, dan S. 
J. Ki dengan judul “Social Vulnerability at A Local Level Around 
The  Merapi  Volcano”  meneliti  tentang  kerentanan  sosial  yang 
ada di sekitar Gunung Merapi [4]. Pada penelitian ini digunakan 
dua belas variabel yang menunjukkan kerentanan sosial yang ada. 
Variabel-variabel  tersebut  adalah  jumlah  populasi  perempuan, 
jumlah  bayi  dan  balita,  jumlah  penduduk  di  bawah  14  tahun, 
jumlah  penduduk  di  atas  60  tahun,  pengangguran,  kepadatan 
penduduk,  jumlah  migrasi  masuk,  jumlah  penduduk  dengan 
pendidikan  menengah  ke  bawah,  penduduk  disabilitas,  jumlah 
penduduk tanpa menggunakan motor, jumlah rumah tangga, dan 
jumlah fasilitas kesehatan. Penelitian ini menggunakan metode 
PCA  (Principal  Component  Analysis)  dan  juga  SOM  (Self 
Organizing Map). Hasil dari penelitian ini adalah  cluster yang 
menunjukkan  kerentanan  sosial  yang  ada  di  sekitar  Gunung 
terdiri  dari  3  cluster  yang 
tersebut 
Merapi.  Cluster 
merepresentasikan variabel-variabel yang ada. 

judul 

dengan 

“Improvement 

Penelitian  yang  dilakukan  oleh  A.  W.  Wijayanto  dan  A. 
Purwarianti 
of  Fuzzy 
Geographically  Weighted  Clustering  using  Particle  Swarm 
Optimization” melakukan pengembangan pada metode PSO [5]. 
Penelitian ini meningkatkan kinerja metode PSO yang dilakukan 
oleh  penelitian  sebelumnya  dan  menambahkan  metode  Fuzzy 
Geographically  Weighted  Clustering.  Penelitian  ini  sukses 
membuat  peningkatan  pada  metode  FGWC  yang  mana 
sebelumnya  pada  metode  FGWC  tersebut  belum  terdapat 
peningkatan PSO. 

IV. METODE PENELITIAN  

Ruang lingkup penelitian 

Ruang lingkup dari penelitian ini adalah  melihat kerentanan 
sosial  yang  berada  pada  level  kabupaten/kota  yang  ada  di 

 2 / 8 

 
 
 
Indonesia,  yaitu  514  kabupaten/kota.  Pengukuran 
ini 
menggunakan sembilan indikator yang terdiri lima belas variabel. 
Adapun  indikator-indikator  yang  digunakan  adalah  sebagai 
berikut : 
Umur  

Umur merupakan salah satu indikator yang dapat menentukan 
kerentanan sosial yang ada. Umur juga memengaruhi dalam segi 
mobilitas  ketika  terjadinya  bencana  alam.  Usia  yang  ekstrem 
yaitu usia dibawah 5 tahun dan juga usia di atas 65 tahun karena 
usia-usia  tersebut  memiliki  mobilitas  yang  rendah.  Kurangnya 
mobilitas  inilah  yang  menyebabkan  beban  perawatan  dapat 
meningkat dan juga berkurangnya ketahanan [6]. 
Jenis Kelamin 

Jenis  Kelamin  dapat  menentukan  besar  kecilnya  kerentanan 
sosial  yang  ada.  Perempuan  dalam  pemulihannya  mengalami 
masa  yang  lebih  sulit  dari  pada  laki  laki.  Hal  itu  disebabkan 
karena  pekerjaan,  upah,  dan 
juga  keharusan  mengurusi 
keluarganya. Oleh karena itu, perempuan akan lebih rentan dari 
pada laki laki [6]. 
Pendidikan 

Pendidikan  merupakan  kecerdasan  yang  dimiliki  oleh 
manusia.  Semakin  tinggi  pendidikan  yang  dilaluinya,  maka 
semakin  tinggi  pula  kecerdasan  seseorang.  Pendidikan  juga 
merupakan salah satu indikator yang  memengaruhi kerentanan 
sosial. Orang yang memiliki tingkat pengetahuan tinggi pastinya 
akan tahu apa saja yang harus di lakukan saat terjadinya bencana 
alam. Mereka akan lebih cepat dalam bertindak dari pada orang 
yang hanya memiliki pengetahuan rendah [7]. 
Infrastruktur rumah 

Infrastruktur rumah merupakan kualitas yang ada pada rumah 
masyarakat. Semakin bagus infrastruktur rumah, maka dampak 
yang  ditimbulkan  akan  berkurang.  Menurut  [8],  kualitas 
perumahan merupakan salah satu indikator yang penting dalam 
mengevaluasi bencana. Sebagai contohnya adalah orang miskin 
sering  kali  tinggal  di  tempat  atau  rumah  yang  konstruksinya 
lebih  buruk.  Rumah  ini  akan  lebih  rentan  terhadap  bencana 
gempa bumi maupun badai. 
Kepadatan Penduduk  

Kepadatan penduduk adalah jumlah penduduk dibagi dengan 
luas  suatu  daerah.  Kepadatan  penduduk  dapat  meningkatkan 
dampak  dari  bencana  alam  yang  terjadi  karena  semakin  padat 
penduduk, maka semakin banyak pula penduduk yang menjadi 
korban. Menurut [9], kepadatan penduduk merupakan salah satu 
indikator kerentanan yang mana jika di suatu daerah populasinya 
besar, akan ada banyak orang yang terkena dampak dari bencana. 
Dalam evakuasi atau penyelamatan juga akan mengalami lebih 
banyak  kesulian.  Hal  itu  dapat  menyebabkan  daerah  tersebut 
lebih rentan terhadap bencana alam.  
Pertumbuhan Penduduk 

Pertumbuhan penduduk adalah perubahan populasi penduduk 
yang  ada  di  suatu  daerah.  Pertumbuhan  penduduk  yang  tinggi 
dapat memengaruhi kerentanan sosial yang ada. Semakin tinggi 
pertumbuhan  penduduk  maka  harus  ada  pertumbuhan  kualitas 
perumahan yang tinggi pula. Jaringan sosial juga harus memiliki 
pertumbuhan  yang  pesat.  Sedangkan  jaringan  layanan  sosial 
mungkin  tidak  memiliki  waktu  untuk  menyesuaikan  diri  dari 
pertumbuhan  penduduk  yang  tinggi  [6].  Oleh  karena  itu, 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

pertumbuhan  penduduk  menjadi  salah  satu  indikator  dalam 
menentukan kerentanan sosial yang ada. 
Struktur Keluarga 

Struktur  keluarga  merupakan  jumlah  dari  anggota  keluarga 
yang  ada  dalam  suatu  rumah  tangga.  Semakin  banyak  jumlah 
anggota  keluarga,  semakin  tinggi  pula  dampak  dari  bencana 
alam  yang  rumah  tangga  tersebut  alami.  Jumlah  anggota 
keluarga  ini  merujuk  pada  tanggungan  keluarga  yang  harus  di 
hadapi ketika adanya bencana alam. Tanggungan ini mencakup 
perawatan seluruh anggota keluarga [6]. 
Pendapatan Penduduk 

Menurut  [6],  kemampuan  untuk  menyerap  kerugian  dan 
meningkatkan  ketahanan  terhadap  bencana  merupakan  salah 
satu  faktor  dari  kerentanan  sosial.  Semakin  tinggi  pendapatan, 
orang  tersebut  akan  semakin  mudah  untuk  mengembalikan 
keadaan  menjadi  seperti  semula.  Berbanding  terbalik  dengan 
orang yang memiliki pendapatan sedikit, yang mana akan sulit 
untuk  mengembalikan  kondisinya  seperti  semula  pasca 
terjadinya bencana. 
Mitigasi 

Kesiapsiagaan bencana atau mitigasi bencana adalah kegiatan 
yang berguna untuk mengantisipasi bencana yang akan datang. 
Mitigasi  bencana 
juga  merupakan  pemberitahuan  atau 
peringatan  darurat.  Peringatan  ini  harus  disebarluaskan  dalam 
waktu yang cepat agar dapat melakukan tindakan yang tepat dan 
cepat. Dengan peringatan darurat ini juga membuat risiko atau 
dampak yang ditimbulkan akan berkurang [10]. 

Metode Pengumpulan Data 

Data  yang  digunakan  adalah  data  sekunder  dari  BPS,  yaitu 
data  SUSENAS  KOR  2019.  Data  SUSENAS  ini  mencakup 
wilayah  seluruh  Indonesia  dalam  level  kabupatan/kota.  Pada 
SUSENAS  ini,  terdapat  320,000  sampel  rumah  tangga  yang 
dicacah. Kemudian digunakan juga data dari Potensi Desa atau 
PODES updating pada tahun 2019. Pada PODES ini, data yang 
diambil  adalah  data  tentang  mitigasi  bencana  pada  tiap 
kabupaten/kota. Selanjutnya digunakan juga data dari publikasi 
BPS yaitu Provinsi dalam Angka yang mana dapat memberikan 
gambaran  pada  beberapa  variabel  yang  dibutuhkan.  Adapun 
variabel  yang  digunakan  dalam  penelitian  berikut  dengan 
sumber datanya ditampilkan dalam tabel di bawah ini. 

TABEL I 
VARIABEL DAN SUMBER DATA YANG DIGUNAKAN 

Nama 

Keterangan variabel 

X1 

X2 

X3 

X4 

X5 

X6 

X7 

X8 

Rata rata jumlah anggota rumah 
tangga 

Tingkat pertumbuhan penduduk 

Tingkat pengangguran terbuka 

Persentase rumah tanpa listrik 

Persentase penduduk usia 0-4 tahun 

Persentase penduduk usia 65 tahun 
ke atas 

Persentase penduduk wanita 

Persentase kepala rumah tangga 
wanita 

Sumber data 

Provinsi dalam 
Angka 2020 
Provinsi dalam 
Angka 2020 
Provinsi dalam 
Angka 2020 
Provinsi dalam 
Angka 2020 
SUSENAS KOR 
2019 
SUSENAS KOR 
2019 
SUSENAS KOR 
2019 
SUSENAS KOR 
2019 

 3 / 8 

 
 
X9 

Persentase penduduk buta huruf 

X10 

Adanya peringatan dini bencana 
alam 

X11 

Adanya perlengkapan keselamatan 

X12 

X13 

X14 

X15 

Adanya rambu-rambu dan jalur 
evakuasi bencana 

Tingkat kepadatan penduduk 

Persentase penduduk miskin 
Persentase penduduk usia 25 tahun 
maksimal SMA 

SUSENAS KOR 
2019 
PODES Updating 
2019 
PODES Updating 
2019 
PODES Updating 
2019 
Provinsi dalam 
Angka 2020 
BPS 
SUSENAS KOR 
2019 

Tahapan Penelitian 
Penentuan Indikator 

Penelitian ini diawali dengan menentukan indikator-indikator 
yang dapat menggambarkan kerentanan sosial wilayah. Dalam 
menentukan  indikator  apa  saja  yang  akan  digunakan  dalam 
penelitian  ini,  peneliti  mempertimbangkan  indikator-indikator 
yang telah digunakan dalam penelitian-penelitian terdahulu yang 
serupa  dan juga menyesuaikannya dengan kondisi yang ada di 
Indonesia.  Kemudian  diambil  sembilan  indikator  yang  dapat 
menjelaskan  kerentanan  sosial  di  Indonesia  sebagaimana  telah 
dijelaskan  dengan  rinci  pada  Metode  Penelitian  bagian  Ruang 
Lingkup  Penelitian.  Adapun  indikator  berikut  variabel  yang 
dapat merepresentasikannya ditampilkan dalam tabel di bawah 
ini. 

TABEL II 
HUBUNGAN INDIKATOR DENGAN VARIABEL  

No 

Indikator 

Variabel 

1 

2 

3 

4 

5 

6 

7 

8 

9 

Umur 

Jenis kelamin 

Pendidikan 

Infrastruktur 
Kepadatan 
penduduk 
Pertumbuhan 
penduduk 

Struktur keluarga 

Pendapatan 
penduduk 

Mitigasi 

Persentase penduduk usia 0-4 tahun 
Persentase penduduk usia 65 tahun ke atas 
Persentase penduduk wanita 
Persentase penduduk buta huruf 
Persentase penduduk dengan pendidikan 
menengah kebawah 
Persentase rumah tanpa listrik 

Tingkat kepadatan penduduk 

Tingkat pertumbuhan penduduk 

Persentase kepala rumah tangga wanita 
Jumlah anggota rumah tangga 
Persentase penduduk miskin 
Tingkat pengangguran terbuka 
Jumlah peringatan dini bencana alam 
Jumlah perlengkapan keselamatan 

Agregasi Data 

Sumber  data  yang  digunakan  dalam  penelitian  ini  adalah 
SUSENAS  KOR  2019,  PODES  Updating  2019,  dan  Provinsi 
dalam  Angka  2020  yang  masih  berupa  raw  data  atau  data 
tersebut  kemudian 
telah  didapatkan 
mentah.  Data  yang 
diagregasi  ke  dalam  level  kabupaten/kota  sesuai  dengan 
variabel-variabel  yang  akan  digunakan  sebagaimana  telah 
ditentukan  sebelumnya  dan  dijelaskan  pada  Metode  Penelitian 
bagian  Metode  Pengumpulan  Data.  Setelah  itu  dilakukan 
normalisasi data  pada setiap  variabel menggunakan metode  Z-
score dengan formula sebagai berikut: 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

𝑥𝑛𝑒𝑤 =

𝑥𝑜𝑙𝑑−𝜇
𝜎

(1)    

Keterangan: 
𝑥𝑛𝑒𝑤 : nilai observasi yang baru 
𝑥𝑜𝑙𝑑  : nilai observasi sebelum dinormalisasi 
𝜇      : rata-rata dari seluruh observasi pada variabel tersebut 
𝜎      : standar deviasi dari variabel tersebut 

Pengujian Autokorelasi Spasial 

Uji  autokorelasi  spasial  dilakukan  dengan  menggunakan  uji 
Moran’s I. Koefisien Moran's I merupakan pengembangan dari 
korelasi Pearson pada data univariate series. Koefisien Moran’s 
I digunakan untuk uji dependensi spasial atau autokorelasi antar 
amatan/lokasi. Hipotesis yang digunakan adalah sebagai berikut: 
H0: I = 0 (tidak ada autokorelasi atau dependensi spasial) 
H1: I ≠ 0 (ada autokorelasi atau dependensi spasial) 
Statistik ujinya dinyatakan dalam persamaan berikut: 

𝑍ℎ𝑖𝑡𝑢𝑛𝑔 =  

𝐼−𝐸(𝐼)

√𝑉𝑎𝑟(𝐼)

(2) 

− 𝑥̅) (𝑥𝑙

− 𝑥̅)

∑ w𝑗𝑙 (𝑥𝑗

𝑛
𝑙=1
𝑛
∑
𝑗=1

(𝑥𝑗− 𝑥̅ )2

1

di mana 
𝑛
∑
𝑗=1

𝐼 =  

𝑛
𝑆0
𝐸(𝐼) = −

𝑣𝑎𝑟(𝐼) =  

−   [𝐸(𝐼)]2  

∑ (𝑛

𝑛−1
2
 𝑛2 𝑆1− 𝑛2 𝑆2+3𝑆0
2
(𝑛2−1 )𝑆0
𝑗 ≠𝑙 w𝑗𝑙 +   𝑤𝑙𝑗 )2  
𝑛
𝑛
𝑛
+   ∑ 𝑤𝑙𝑗
  (∑ 𝑤𝑗𝑙
𝑙=1
𝑙=1
𝑗=1
𝑛
𝑛
∑ 𝑤𝑗𝑙
𝑙=1
𝑗=1

2

)

1

𝑆1 =

2
𝑆2 =   ∑
𝑆0 =   ∑

𝑥𝑗          = data ke-j (𝑗 = 1,2, . . . , 𝑛)  
𝑥𝑙          = data ke-l (𝑙 = 1,2, . . . , 𝑛)  
𝑥̅           = rata-rata data  
𝑤𝑗𝑙        = matriks  pembobot  spasial  tersandarisasi  antara  lokasi 
ke-j  dan  lokasi  ke-l.  peneliti  menggunakan  matrik 
pembobot k-nearest neighbors 

𝑉𝑎𝑟(𝐼) = varian Moran’s  
𝐼            = nilai harapan Moran’s I 

Mereduksi Variabel yang ada dengan GWPCA 

Metode  GWPCA  atau  Geographically  Weighted  Principal 
Components  Analysis  diawali  dengan  melakukan  uji  KMO 
(Kaiser-Meyer-Olkin)  dan  juga  uji  Bartlett.  KMO  merupakan 
perbandingan  antara  koefisien  korelasi  terobservasi  dengan 
koefisien  korelasi  parsialnya  secara  keseluruhan.  KMO  dapat 
digunakan sebagai alat ukur untuk menguji apakah kecukupan 
sampel  setiap  variabel  telah  terpenuhi  dan  dapat  memberikan 
gambaran  seberapa  cocok  data  yang  digunakan  dalam  analisis 
faktor. Adapun formula dari KMO adalah sebagai berikut : 

𝑲𝑴𝑶 =  

𝒑
𝟐
∑ ∑ 𝒓𝒊𝒋
𝒋=𝒊

𝒑
𝒊=𝟏
𝒑
𝟐
∑ ∑ 𝒓𝒊𝒋
𝒋=𝒊

𝒑
𝒊=𝟏

𝟐
+   ∑ ∑ 𝒂𝒊𝒋

𝒑
𝒊=𝟏

𝒑
𝒋=𝒊

(3) 

Keterangan: 
𝑖     = 1,2,3, … , 𝑝 
𝑗     = 1,2,3, … , 𝑝 
𝑟𝑖𝑗   = koefisien korelasi terobservasi antara variabel i dan j 

 4 / 8 

 
 
 
 
 
 
 
  
 
  
  
  
  
 
 
𝑎𝑖𝑗 = koefisien korelasi parsial antara variabel i dan j 

Menurut (Kaiser & Rice, 1974), nilai KMO berkisar antara 0 
hingga  1.  Semakin  tinggi  nilai  KMO  menandakan  bahwa 
variabel penelitian yang digunakan sudah sesuai untuk analisis 
faktor.  Adapun  kriteria  kesesuaian  dalam  pemilihan  analisis 
faktor adalah: 
0.9  ≤  KMO  <  1.0  marvelous  atau  data  sangat  baik  untuk 
analisis faktor, 
0.8 ≤ KMO < 0.9 meritorius atau data baik untuk analisis faktor, 
0.7 ≤ KMO < 0.8 middling atau data cukup baik untuk analisis 
faktor, 
0.6 ≤ KMO < 0.7 mediocre atau data kurang baik untuk analisis 
faktor, 
0.5  ≤  KMO  <  0.6  miscrable  atau  data  buruk  untuk  analisis 
faktor, 
0.0 ≤ KMO < 0.5  unacceptable  atau  data  tidak  cocok  untuk 
analisis faktor. 

Bartlett’s  Test  of  Sphericity  merupakan  alat  ukur  yang 
digunakan  untuk  mengetahui  apakah  terdapat  korelasi  yang 
signifikan antar variabel yang diteliti. Hipotesis nol pada asumsi 
ini adalah tidak terdapat korelasi yang signifikan antar variabel 
yang  diteliti.  Menurut  (Williams  et  al.,  2010),  hasil  yang 
diharapkan dari pengujian ini adalah keputusan tolak hipotesis 
nol.  Keputusan  tersebut  menandakan  bahwa  terdapat  korelasi 
yang  signifikan  antar  variabel  yang  diteliti  sehingga  dapat 
dilakukan analisis faktor. Adapun formula dari Bartlett’s Test of 
Sphericity adalah sebagai berikut : 

2𝑝 + 5
6

] ln|𝑅| 

(4) 

𝐵𝑎𝑟𝑡𝑙𝑒𝑡𝑡’𝑠 𝑇𝑒𝑠𝑡 =   − [(𝑛 − 1) −

Keterangan: 
|𝑅| = determinan matriks korelasi 
𝑛    = jumlah sampel atau observasi 
𝑝    = jumlah variabel 

Clustering FGWC-PSO 

peningkatan 

dari  metode  FCM 

FGWC-PSO  merupakan  metode  clustering  dari  Fuzzy 
Geographically  Weighted  Clustering  (FGWC)  menggunakan 
(PSO).  FGWC 
optimasi  Particle  Swarm  Optimization 
merupakan 
dengan 
menambahkan  pengaruh  geografis  berupa  jarak  antar  wilayah 
dan populasi di dalam wilayah tersebut. Hal ini membuat FGWC 
menjadi  sensitif  terhadap  NE  (Neighborhood  Effect  atau  efek 
ketetanggaan)  dan  berpengaruh  terhadap  nilai  centroid  untuk 
membuat  centroid  yang  “geographically  aware”.  Pengaturan 
nilai  keanggotaan  pada  FGWC  yang  dihitung  di  dalam  setiap 
iterasi ditunjukkan dalam persamaan berikut [11]: 

′ = 𝛼𝜇𝑖 + 𝛽
𝜇𝑖

1

𝐴

∑ 𝑤𝑖𝑗𝜇𝑖𝑗

𝑛
𝑖=1

(5) 

Dengan 𝜇𝑖

′ adalah nilai keanggotaan yang baru dari observasi 
atau wilayah ke-i dan 𝜇𝑖 adalah nilai keanggotaan  cluster yang 
lama  dari  wilayah  ke-i.  Parameter  𝐴  digunakan  untuk 
memastikan  rata-rata  penimbang  nilai  keanggotaan  dalam 
rentang  0  sampai  1.  𝛼  dan  𝛽  merupakan  penimbang  nilai 
keanggotaan lama dan rata-rata nilai keanggotaan pada wilayah 
sekitarnya  yang  berjumlah  1.  𝑤𝑖𝑗  merupakan  penimbang 
interaksi  antara  dua  wilayah  geografis.  Nilai  penimbang 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

dipengaruhi oleh jumlah populasi dan jarak antara kedua wilayah 
dan dirumuskan sebagai berikut: 

𝑤𝑖𝑗 =

(𝑚𝑖𝑚𝑗)𝑏
𝑎
𝑑𝑖𝑗

(6) 

Di  mana 𝑚𝑖 dan 𝑚𝑗 adalah  populasi  di  wilayah  𝑖  dan  𝑗  dan 
𝑑𝑖𝑗  adalah  jarak  antara  wilayah  𝑖  dan  𝑗,  𝑎  merupakan  besar 
pengaruh  interaksi  jarak,  dan  𝑏  merupakan  besar  interaksi 
populasi  antara  dua  wilayah.  Kedua  parameter  ini  dapat 
ditentukan oleh  pengguna.  Selama  proses  iterasi,  FGWC  terus 
melakukan modifikasi geografis dalam proses  clustering untuk 
menyesuaikan nilai lima belas keanggotaannya. 

Algoritma  FGWC-PSO  merupakan  hasil  integrasi  dua 
algoritma  yaitu  Fuzzy  Geographically  Weighted  Clustering 
(FGWC) dan Particle Swarm Optimization (PSO)[5]. Algoritma 
FGWC  memiliki  beberapa  keterbatasan  ketika  melakukan 
pengelompokkan,  karena  FGWC  menentukan  pusat  cluster 
secara  acak  selama  proses 
inisialisasi. 
Keterbatasan  dalam  memilih  titik  pusat  secara  acak  tersebut, 
menyebabkan proses iterasi gagal untuk mencapai solusi global 
optimum.  Masalah  ini  akan  memberikan  dampak  terhadap 
kualitas cluster yang dihasilkan oleh FGWC. Selanjutnya, untuk 
mengatasi keterbatasan ini, maka algoritma PSO akan digunakan 
untuk  menentukan  pusat  cluster  atau  matrik  keanggotaan  di 
tahap inisialisasi pada algoritma FGWC. Fungsi objektif FGWC 
(𝐽𝐹𝐺𝑊𝐶) yang akan diminimumkan adalah sebagai berikut: 

iterasi  ditahap 

𝐽𝐹𝐺𝑊𝐶(𝑈, 𝑉; 𝑋) = ∑ ∑

𝑐
𝑖=1

𝑛
𝑘=1

𝑚
𝜇𝑖𝑘

|𝑣𝑖 − 𝑥𝑘|2

→ min 

(7) 

Dimana  m  merupakan  tingkat  fuzzines,  𝜇𝑖𝑘  adalah  sebuah 
elemen dari matrik partisi, 𝑣𝑖 adalah pusat cluster, dan 𝑥𝑘 adalah 
titik data. 

Visualisasi hasil clustering 

Setelah  analisis  clustering  selesai  dilakukan, 

tahapan 
adalah 
selanjutnya 
memvisualisasikan  hasil  clustering  tersebut  ke  dalam  peta 
tematik dengan menggunakan bantuan software QGIS. 

sekaligus 

tahapan 

terakhir 

V.  KERANGKA PIKIR 

Menurut S. L. Cutter, kerentanan terhadap bencana alam dari 
suatu  wilayah  dapat  berupa  kerentanan  infrastruktur  dan  juga 
kerentanan  sosial  [6].  Penelitian  ini  hanya  berfokus  kepada 
kerentanan sosial di setiap wilayah kabupaten/kota di Indonesia. 
Berdasarkan penelitian terdahulu sebagaimana  telah dijelaskan 
sebelumnya,  indikator  yang  memengaruhi  kerentanan  sosial 
yang  dipakai  pada  penelitian  ini  adalah  umur,  jenis  kelamin, 
pendidikan,  infrastruktur,  kepadatan  penduduk,  pertumbuhan 
penduduk, struktur keluarga, pendapatan penduduk, dan mitigasi. 
Setelah  didapatkan  hasil  clustering  menggunakan  metode 
Geographically  Weighted  Principal  Components  Analysis  dan 
Fuzzy  Geographically  Weighted  Clustering  Particle  Swarm 
tematik  persebaran 
Optimization,  akan  ditampilkan  peta 
kabupaten/kota  berdasarkan  hasil  clustering  kerentanan  sosial 
tersebut.  Kerangka  pikir  tersebut  dapat  digambarkan  alurnya 
secara ringkas seperti dalam diagram di bawah ini. 

 5 / 8 

 
 
 
 
  
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Penelitian  bagian  Tahapan  Penelitian.  Adapun  hasil  uji  KMO 
dan Bartlett yang telah dilakukan adalah sebagai berikut: 

TABEL IV 
KMO AND BARTLETT’S TEST 

Kaiser-Meyer-Olkin 

Bartlett’s Test of 
Sphericity 

Approx. Chi-Square 
df 
Sig. 

0.654 
4101.358 
105 
0.000 

Uji  KMO  merupakan  uji  untuk  mengukur  kecukupan 
sampling. Dari tabel di atas, angka dari uji KMO adalah 0.654. 
Angka tersebut > 0.5 yang berarti bahwa variabel yang ada dapat 
digunakan karena telah mencukupi dalam segi samplingnya.  

Kemudian  uji  selanjutnya  adalah  uji  Bartlett.  Uji  ini 
merupakan  uji  statistik  yang  menguji  sifnifikasi  pada 
keseluruhan korelasi yang ada pada matrik korelasi. Signifikan 
yang  dicari  adalah  dengan  p-value  <  0.05.  Pada  tabel  di  atas, 
angka  signifikasi  yang  didapatkan  adalah  0.00,  artinya 
keseluruhan korelasi telah signifikan. 

Kemudian  untuk  melakukan  metode  GWPCA,  perlu 
penimbang atau pembobot dari fungsi kernel dan bandwithnya. 
Penelitian  ini  menggunakan  fungsi  kernel  bisquare.  Fungsi 
kernel ini menerapkan fungsi kernel adaptive yang menerapkan 
nilai bandwith yang berbeda-beda.  

Komponen 
Utama 

PC1 
PC2 
PC3 
PC4 
PC5 

TABEL V 
IMPORTANCE OF COMPONENT 
Proportion of 
Standard 
deviation 
Variance 
1.985695 
1.591844 
1.34306 
1.293583 
1.066941 

0.262866 
0.168931 
0.120254 
0.111557 
0.075891 

Cumulative 
Proportion 

0.262866 
0.431797 
0.552051 
0.663608 
0.739499 

Dapat dilihat pada tabel V di atas bahwa hasil yang didapatkan 
adalah sebanyak lima komponen utama. Lima komponen utama 
ini diambil karena dengan menggunakan lima komponen utama 
pertama yang ada sudah dapat menjelaskan 73,9 persen varians 
yang  ada  pada  data  secara  keseluruhan.  Ini  sudah  dapat 
digunakan karena sudah memenuhi minimum proporsi varians 
yang dianjurkan yaitu lebih dari 70 persen.  

Setelah  mengetahui  lima  komponen  utama  yang  digunakan, 
selanjutnya  adalah  melihat  variabel-variabel  apa  saja  yang 
paling berpengaruh pada komponen utama. Variabel yang paling 
berpengaruh  dapat  dilihat  dari  winning  variable  pada  tiap-tiap 
variabelnya. Hasil yang diperoleh adalah pada tabel VI dibawah : 

PC1 

X8 

X12 

X13 

TABEL VI 
WINNING VARIABEL TIAP KOMPONEN 
PC4 
PC3 

PC2 

X3 

X6 

X9 

X1 

X2 

X10 

X15 

X5 

X14 

PC5 

X4 

X7 

X11 

 6 / 8 

Gambar 3. Kerangka pikir penelitian 

VI. HASIL DAN PEMBAHASAN 

Pengujian Autokorelasi Spasial 

Setelah 

dilakukan 

spasial 
pengujian 
menggunakan  uji  Moran’s  I,  didapatkan  hasil  sebagaimana 
ditunjukkan dalam tabel berikut: 

autokorelasi 

TABEL III 
HASIL PENGUJIAN MORAN’S I 

Variabel  Koefisien Moran’s I 
X1 
X2 
X3 
X4 
X5 
X6 
X7 
X8 
X9 
X10 
X11 
X12 
X13 
X14 
X15 

0.596 
0.846 
0.328 
0.477 
0.45 
0.382 
0.215 
0.449 
0.692 
0.382 
0.274 
-0.001 
0.569 
0.719 
0.59 

Kesimpulan 

Terdapat dependensi spasial 
Terdapat dependensi spasial 
Terdapat dependensi spasial 
Terdapat dependensi spasial 
Terdapat dependensi spasial 
Terdapat dependensi spasial 
Terdapat dependensi spasial 
Terdapat dependensi spasial 
Terdapat dependensi spasial 
Terdapat dependensi spasial 
Terdapat dependensi spasial 
Tidak terdapat dependensi spasial 
Terdapat dependensi spasial 
Terdapat dependensi spasial 
Terdapat dependensi spasial 

Dari tabel di atas, dapat dilihat bahwa hampir seluruh variabel 
memiliki nilai koefisien Morans’I tidak sama dengan nol yang 
memberikan  keputusan  untuk  menolak  H0  atau  menunjukkan 
adanya  dependensi  spasial.  Hanya  variabel  X12  yang 
memberikan keputusan gagal tolak H0 karena nilai koefisiennya 
sangat mendekati nol. Karena hanya terdapat satu dari lima belas 
variabel yang gagal menolak H0, dapat disimpulkan bahwa data 
ini sangat cocok untuk diolah menggunakan analisis spasial. 

Mereduksi Variabel yang ada dengan GWPCA 

Metode ini diawali dengan melakukan uji KMO dan juga uji 
Bartlett sebagaimana telah dijelaskan sebelumnya pada Metode 

 
 
 
 
 
 
 
 
 
 
 
   
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

TABEL VII 
PENGUJIAN JUMLAH CLUSTER  

Jenis 

Cluster 2 

Cluster 3 

Cluster 4  Cluster 5  Cluster 6 

PC 

CE 

SC 

SI 

0.5 

0.3333 

0.25 

0.2 

0.1667 

0.6931 

1.0986 

1.3863 

1.6094 

1.7918 

2.72E+11 

1.33E+11 

4.96E+12 

9.61E+09 

1.55E+09 

2.71E+11 

2.16E+11 

4.96E+12 

8.58E+11 

2.89E+13 

IFV 

3.69E-12 

6.98E-12 

1.94E-11 

5.58E-11 

3.84E-11 

Kwon 

6.97E+13 

3.70E+14 

6.37E+14 

8.83E+13 

4.13E+14 

TABEL VIII 
CLUSTER CENTROID UNTUK 2 CLUSTER 

Komponen utama 

PC1 
PC2 
PC3 
PC4 
PC5 

Cluster 

Tinggi 
 0.1383485 
 0.2840845 
 0.1849032 
-0.05003886 
-0.01558375 

Rendah 
 0.1383442 
 0.2840867 
 0.1849023 
-0.05003896 
-0.01558357 

Berikut  adalah  hasil  dari  clustering  yang  telah  dilakukan. 
Gambar  4  merupakan  peta  Cluster  1  dengan  menggunakan 
jumlah cluster sebanyak 2. Dapat dilihat dari TABEL VII bahwa 
PC1, PC2, dan PC3 memberikan pengaruh yang lebih besar pada 
cluster Tinggi. Sedangkan PC2 dan PC4 memberikan pengaruh 
yang lebih besar pada cluster Rendah. Untuk hasil yang didapat, 
sebanyak 265 kabupaten/kota termasuk ke dalam cluster Tinggi 
dan  sebanyak  249  kabupaten/kota  termasuk  ke  dalam  cluster 
Rendah.

Gambar 4. Persebaran Komponen Utama (Sumber: data diolah) 

Dari  TABEL  VI,  variabel  yang  memengaruhi  pada 

komponen utama adalah : 

1. 

PC1  paling  dipengaruhi  oleh  variabel  X8,  X12,  dan 
X13. Variabel-variabel tersebut adalah persentase kepala rumah 
tangga wanita, adanya rambu-rambu dan jalur evakuasi bencana, 
serta tingkat kepadatan penduduk 

2. 

PC2 paling dipengaruhi oleh variabel X3, X6, dan X9. 
Variabel-variabel tersebut adalah tingkat pengangguran terbuka, 
persentase  penduduk  usia  65  tahun  ke  atas,  dan  persentase 
penduduk buta huruf 

3. 

PC3 paling dipengaruhi oleh variabel X1, X2, X10, dan 
X15. Variabel-variabel tersebut adalah rata-rata jumlah anggota 
tingkat  pertumbuhan  penduduk,  adanya 
rumah 
peringatan dini bencana alam, dan persentase penduduk usia 25 
tahun maksimal SMA 

tangga, 

4. 

PC4  paling  dipengaruhi  oleh  variabel  X5  dan  X14. 
Variabel-variabel tersebut adalah persentase penduduk usia 0-4 
tahun dan persentase penduduk miskin 

5. 

PC5 paling dipengaruhi oleh variabel X4, X7, dan X11. 
Variabel-variabel tersebut adalah persentase rumah tanpa listrik, 
persentase  penduduk  wanita,  dan  adanya  perlengkapan 
keselamatan 

Clustering FGWC-PSO 

Tahap  berikutnya  adalah  melakukan  metode  clustering. 
Metode ini menggunakan beberapa parameter yang dipilih oleh 
peneliti  berdasarkan  penelitian  terdahulu.  Adapun  parameter 
yang  digunakan  dalam  melakukan  clustering  pada  metode  ini 
adalah:  a  =  1,  b  =  1,  error  tolerance  =  1e-6,    dan  maksimum 
iterasinya adalah 1000. Pada penelitian ini nilai  fuzziness fixed 
yang digunakan adalah 2 berdasarkan penelitian sebelumnya [3]. 
Penentuan  jumlah  cluster  dilakukan  dengan  menggunakan 

beberapa uji pada tabel VII. 

Gambar 5. Peta Cluster  (Sumber: data diolah) 

VII. 

PENUTUP 

Dari  hasil  pembahasan  yang  ada  di  atas,  terdapat  beberapa 

kesimpulan yang di peroleh, yaitu : 
Dari  hasil  peta  yang  ada,  dengan  melihat  hasil  cluster  maka 
daerah  kerentanan  sosial  yang 
tinggi  berada  di  sekitar 
Kalimantan sebelah timur, Jawa, Sulawesi Utara, dan beberapa 
daerah di Papua. Persebaran kerentanan sosial yang dapat dilihat 
di  peta  memiliki  pola  yang  mengelompok  yang  menyebabkan 
adanya kesamaan karakteristik pada daerah yang berdekatan. 

Dari penelitian yang sudah di dilakukan ini, terdapat saran 

untuk penelitian kedepannya, yaitu: 
1.  Peneliti hanya menggunakan beberapa variabel saja dalam 
ini  dapat 

pembentukan  kerentanan  sosial,  variabel 

 7 / 8 

 
 
 
 
 
 
 
 
ditambahkan sesuai dengan variabel pembentuk kerentanan 
sosial yang ada. 

2.  Dalam  melakukan  clustering  FGWC  –  PSO,  peneliti 
menggunakan parameter yang sama untuk tiap clusternya, 
maka dari itu dapat ditambahkan parameter yang berbeda-
beda untuk mendapatkan cluster terbaik. 

3.  Untuk  pemerintah  atau  pemangku  kebijakan,  dengan 
menggunakan  parameter  cluster  yang  sudah  konsisten, 
dapat digunakan untuk membuat kebijakan-kebijakan yang 
sesuai  untuk  meningkatkan  kualitas  dari  sarana  maupun 
prasarana  untuk  mengurangi  kerentanan  sosial  yang  ada 
pada  daerah-daerah  yang  merupakan  wilayah  dengan 
tingkat kerentanan sosial yang tinggi. 

[1] 

[2] 

[3] 

[4] 

[5] 

[6] 

[7] 

[8] 

[9] 

[10] 

[11] 

DAFTAR PUSTAKA 

APEC Climate Center, “Disaster Management Research Roadmap for 
the ASEAN Region,” p. 142, 2017. 
T. H. Siagian, P. Purhadi, S. Suhartono, and H. Ritonga, “Social 
vulnerability to natural hazards in Indonesia: Driving factors and 
policy implications,” Nat. Hazards, vol. 70, no. 2, pp. 1603–1617, 
2014, doi: 10.1007/s11069-013-0888-3. 
S. Rufat, “Spectroscopy of Urban Vulnerability,” Ann. Assoc. Am. 
Geogr., vol. 103, no. 3, pp. 505–525, 2013, doi: 
10.1080/00045608.2012.702485. 
Y. N. Maharani, S. Lee, and S. J. Ki, “Social vulnerability at a local 
level around the Merapi volcano,” Int. J. Disaster Risk Reduct., vol. 
20, no. October, pp. 63–77, 2016, doi: 10.1016/j.ijdrr.2016.10.012. 
A. W. Wijayanto and A. Purwarianti, “Improvement of fuzzy 
geographically weighted clustering using particle swarm 
optimization,” 2014 Int. Conf. Inf. Technol. Syst. Innov. ICITSI 2014 
- Proc., no. November, pp. 7–12, 2014, doi: 
10.1109/ICITSI.2014.7048229. 
S. L. Cutter, B. J. Boruff, and W. L. Shirley, “Social vulnerability to 
environmental hazards,” Soc. Sci. Q., vol. 84, no. 2, pp. 242–261, 
2003, doi: 10.1111/1540-6237.8402002. 
K. F. Dintwa, G. Letamo, and K. Navaneetham, “Quantifying social 
vulnerability to natural hazards in Botswana: An application of cutter 
model,” Int. J. Disaster Risk Reduct., vol. 37, no. December 2018, p. 
101189, 2019, doi: 10.1016/j.ijdrr.2019.101189. 
B. E. Flanagan, E. W. Gregory, E. J. Hallisey, J. L. Heitgerd, and B. 
Lewis, “A Social Vulnerability Index for Disaster Management,” J. 
Homel. Secur. Emerg. Manag., vol. 8, no. 1, 2020, doi: 
10.2202/1547-7355.1792. 
B. M. de Loyola Hummell, S. L. Cutter, and C. T. Emrich, “Social 
Vulnerability to Natural Hazards in Brazil,” Int. J. Disaster Risk Sci., 
vol. 7, no. 2, pp. 111–122, 2016, doi: 10.1007/s13753-016-0090-9. 
D. S. K. Thomas, B. D. Phillips, W. E. Lovekamp, and A. Fothergill, 
Social vulnerability to disasters. 2009. 
G. A. Mason and R. D. Jacobson, “Fuzzy Geographically Weighted 
Clustering,” no. 1998, pp. 1–7, 2007. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

 8 / 8 

 
 
 
 
 
 
 
"
221710013,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Penerapan Big Data pada Small Area Estimation 
Rata-Rata Pengeluaran per Kapita 
(Studi Kasus: Provinsi Daerah Istimewa Yogyakarta) 

Shinta Mahaputri Hakim (221710013, 4SD1) 
Dosen Pembimbing: Dr. Azka Ubaidillah, S.S.T., M.Si. 

Ringkasan— Indonesia sebagai negara yang menganut sistem 
pemerintahan  desentralisasi,  tentunya  penting  bagi  pemerintah 
untuk  mengetahui  gambaran  wilayah  melalui  data  dalam 
mewujudkan  pembangunan  daerah  otonom.  Salah  satu  tolak 
ukur  keberhasilan  pembangunan  tersebut  diukur  melalui  IPM 
yang  salah  satu  komponennya  adalah  pengeluaran  per  kapita. 
Akan tetapi, estimasi pengeluaran per kapita tingkat kecamatan 
sulit dilakukan karena kurangnya sampel. SAE sebagai salah satu 
solusi  BPS  dalam  melakukan  estimasi  pada  wilayah  kecil. 
Biasanya,  BPS  memanfaatkan  data  Podes  sebagai  variabel 
penyerta dalam SAE, akan tetapi pengumpulan data Podes hanya 
dilakukan tiga kali dalam sepuluh tahun sehingga terkadang hasil 
estimasi SAE menjadi out of date. Penelitian ini bertujuan untuk 
mengkaji  penggunaan  big  data  (dalam  hal  ini  hasil  olahan  citra 
satelit)  sebagai  variabel  penyerta  untuk  estimasi  rata-rata 
pengeluaran per kapita dengan membandingkan tiga model SAE 
berdasarkan  variabel  penyertanya  yaitu  Podes,  olahan  citra 
satelit,  dan  kombinasi  citra  satelit-Podes.  Hasil  Estimasi 
menunjukkan variabel penyerta olahan citra satelit menghasilkan 
nilai MSE  dan RSE  lebih  kecil  daripada  variabel  penyerta  data 
Podes.  Sedangkan  MSE  dan  RSE  paling  kecil  dihasilkan  pada 
variabel kombinasi citra satelit-Podes. Dengan demikian, big data 
hasil  olahan  citra  satelit  berpotensi  sebagai  alternatif  variabel 
penyerta  dalam  SAE  karena  tersedia  secara  berkala,  mencakup 
hingga level kecil, dan biaya lebih rendah.  

Kata  Kunci—  big  data,  citra  satelit,  Pengeluaran  per  Kapita, 

Small Area Estimation. 

I.  LATAR BELAKANG 

sebagai  negara  yang  menganut 
sistem 
Indonesia 
tentunya  penting  bagi  pemerintah  dalam 
desentralisasi, 
menyediakan data hingga ke level otonom pemerintahan daerah 
sebagai  dasar  pengetahuan  dalam  proses  perencanaan  dan 
pemantauan  dalam  proses  pembangunan.  Keberhasilan 
pencapaian tujuan pembangunan tersebut secara umum diukur 
dari tingkat kemakmuran dan kesejahteraan masyarakat melalui 
indikator Indeks Pembangunan Manusia (IPM) di suatu negara. 
IPM  terdiri  dari  tiga  dimensi  dasar  sebagai  penyusun  ukuran 
kualitas hidup manusia, salah satu dimensinya adalah standar 
hidup  yang  layak  yang  diukur  melalui  Rata-Rata  Pendapatan 
per Kapita secara akurat [1]. Akan tetapi, sulit untuk melakukan 
pengukuran  pendapatan  per  kapita  masyarakat  sehingga 
didekati  melalui  pengeluaran  per  kapita  karena  keduanya 
memiliki hubungan yang kuat [2].   

Badan  Pusat  Statistik  (BPS)  baik  pusat  maupun  provinsi 
setiap tahun melakukan perhitungan rata-rata pengeluaran per 
kapita. Saat ini, pengeluaran per kapita yang diestimasi hanya 
sampai  level  kabupaten  atau  provinsi,  sehingga  kurang  dapat 
memberikan arti untuk level kecamatan atau bahkan yang lebih 

kecil  lagi.  Di  sisi  lain,  pengumpulan  data  yang  diperlukan 
dalam  menghitung  pengeluaran  per  kapita  dilakukan  melalui 
Survei Sosial Ekonomi Nasional (Susenas). 

Survei dipilih karena memerlukan tenaga, waktu, dan biaya 
yang  lebih  sedikit  daripada  sensus.  Hal  ini  sebabkan  karena 
informasi yang dikumpulkan pada survei hanya mencakup ke 
sampel  yang  terpilih  saja  [3].  Akan  tetapi  kecukupan  sampel 
pada survei sering menjadi masalah dalam mengestimasi area 
kecil,  estimasi  langsung  dengan  jumlah  sampel  yang  tidak 
tercukupi  dapat  menyebabkan  standar  error  yang  besar  [4]. 
Sehingga  dalam  melakukan  pengoptimalan  ketersediaan  data 
yang 
terbatas,  BPS  menggunakan  metode  Small  Area 
Estimation (SAE). 

Metode SAE menggunakan pendugaan tidak langsung yang 
bersifat 
“meminjam  kekuatan”  dengan  memanfaatkan 
informasi variabel penyerta yang memiliki hubungan yang kuat 
dengan  variabel  amatan  [4].  Biasanya  BPS  menggunakan 
variabel penyerta Data Potensi Desa (Podes) karena mencakup 
keseluruhan  observasi  sehingga  tidak  mengandung  sampling 
error 
[5].  Namun,  pengumpulan  data  Podes  hanya 
dikumpulkan  tiga  kali  dalam  sepuluh  tahun.  Sehingga  data 
Podes  terkadang  kurang  relevan  dan  reliabel  jika  digunakan 
sebagai variabel penyerta SAE dalam estimasi setiap tahun.  

Di sisi lain, big data kerap menjadi solusi ketersediaan data 
yang digaungkan beberapa waktu belakangan ini. Karakteristik 
yang  dimiliki  yaitu  volume  yang  besar,  data  yang  bervariasi, 
dan memiliki laju pertumbuhan yang cepat dapat menjadi solusi 
yang inovatif dalam mengatasi ketersediaan data yang terbatas. 
Salah  satu  sumber  big  data  gratis  adalah  citra  penginderaan 
jarak jauh (citra satelit). Citra satelit dirilis secara periodik serta 
mampu  memantau  permukaan  bumi  hingga  ke  wilayah  kecil 
melalui  resolusi  yang  ditawarkan.  Saat  ini,  citra  satelit  mulai 
digunakan untuk mempelajari aktifitas manusia dan peristiwa 
alam yang mempengaruhi pertumbuhan ekonomi, kemiskinan, 
ketimpangan  terutama  untuk  area  yang  sulit  dijangkau  untuk 
mendapatkan datanya [6].  

Hingga saat ini, belum ada yang memanfaatkan citra satelit 
sebagai variabel penyerta dalam SAE rata-rata pengeluaran per 
kapita.  Oleh  karena  itu,  pada  penelitian  ini  akan  mengkaji 
penggunaan  big  data  hasil  pengolahan  citra  satelit  sebagai 
variabel  penyerta  SAE  dalam  menyajikan  estimasi  rata-rata 
pengeluaran  per  kapita  level  Kecamatan  di  Provinsi  Daerah 
Istimewa  Yogyakarta.  Penelitian  ini  dilakukan  pada  Provinsi 
Daerah  Istimewa  Yogyakarta  dikarenakan  sebagai  provinsi 
dengan penduduk terpadat setelah DKI Jakarta dan Jawa Barat. 
Kepadatan  penduduk  mempengaruhi  dinamika  permukaan 
bumi  yang  disebabkan  oleh  aktifitas  manusia  secara 

 1 / 8 

 
 
 
 
 
berkelanjutan  seperti  pembangunan,  mobilitas,  dan  lainnya 
yang tertangkap citra satelit [7]. 

II.  TUJUAN PENELITIAN 

Tujuan dilakukannya penelitian ini adalah: 

1.  Mengestimasi rata-rata pengeluaran per kapita di Provinsi 
Daerah  Istimewa  Yogyakarta  Tahun  2019  pada  level 
kecamatan menggunakan metode SAE EBLUP-FH dengan 
variabel  penyerta  hasil  olahan  citra  satelit,  Podes,  dan 
kombinasi citra satelit-Podes.  

2.  Membandingkan  hasil  estimasi  rata-rata  pengeluaran  per 
kapita di Provinsi Daerah Istimewa Yogyakarta Tahun 2019 
pada level kecamatan menggunakan metode SAE EBLUP-
FH berdasarkan variabel penyerta hasil olahan citra satelit, 
Podes, serta kombinasi antara citra satelit-Podes. 

3.  Mengkaji penerapan model SAE dengan memanfaatkan big 
data  sebagai  variabel  penyerta  pada  estimasi  rata-rata 
pengeluaran per kapita level kecamatan di Provinsi Daerah 
Istimewa Yogyakarta Tahun 2019. 

3. 

4. 

III. PENELITIAN TERKAIT 

Berikut  merupakan  penelitian  terkait  yang  digunakan 

sebagai studi literatur pada penelitian ini. 

No 

Judul 

1. 

2. 

 VIIRS 
Nighttime 
Light  Data 
for 
Income 
Estimation  at 
Local Level 

 Modelling the 
Relationship 
the 
between 
Gross 
Domestic 
Product  and 
Built-Up 
Area  Using 
Remote 
Sensing  and 
GIS Data 
CO 
Emissions 
from 
Pakistan and 
India and 
Their 
Relationship 
with 

TABEL I 
TABEL LITERATUR 
Penulis, 
Publikasi 
Kinga 
Ivan, 
dkk,  MDPI, 
1-19, 2020. 

Tertulis 

Hasil 
penelitian 
menunjukkan bahwa 
terdapat  hubungan 
yang  kuat  antara 
jumlah  cahaya  dan 
tingkat  pendapatan 
untuk 
kota-kota 
yang diamati [8]. 

Kamil  Faisal, 
2016,  MDPI, 
1-17. 

terdapat 
Hasilnya, 
hubungan 
linier 
yang  kuat  antara 
persentase  kawasan 
terbangun 
dengan 
data  sosial  ekonomi 
sebesar 0,8 [9]. 

S, 
Tariq 
U.H.,  2017, 
Ecology  and 
Environment
al  Research, 
1301-1312. 

ekonomi 
Variabel 
memiliki  pengaruh 
secara 
parsial 
simultan 
maupun 
CO. 
terhadap 
Hasilnya, gas karbon 
dioksida  memiliki 
yang 
hubungan 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Alexander 
Buyantuyev, 
J.  W.,  2009, 
Springer,  17-
33. 

dengan 
ekonomi 

signifikan 
variabel 
[10]. 
Variabel 
sosio-
ekonomi  memiliki 
terhadap 
pengaruh 
permukaan 
suhu 
tanah. 
ini 
hal 
ditunjukkan  adanya 
korelasi tinggi antara 
permukaan 
suhu 
pada  siang  hari  dan 
pendapatan  rata-rata 
keluarga 
pada 
wilayah  studi  kasus 
[11].   

Dissanayake, 
Takehiro 
Morimoto, 
Yuji 
Murayama, 
Manjula 
Ranagalage, 
Hepi 
Handayani, 
2018, MDPI. 

H. 

terdapat 
Bahwa 
antara 
hubungan 
peningkatan 
LST 
dengan  peningkatan 
penduduk 
jumlah 
dan  kegiatan  sosial 
termasuk 
ekonomi 
industrialisasi 
dan 
pembangunan 
infrastruktur[12]. 

Economic 
Variables. 

 Urban  Heat 
Islands  and 
Landscape 
Heterogeneit
y: 
Linking 
Spatiotempor
al  Variations 
Surface 
in 
Temperature 
Land-Cover 
and 
Socioeconom
ic Patterns. 

of 

Impact 
Urban 
Surface 
Characterist 
cs and Socio-
Economic 
Variables  on 
the 
Spatial 
Variation  of 
Land  Surface 
Temperature 
in Lagos City, 
Nigeria 

Penelitian terkait pada Tabel I. menjadi dasar dalam pemilihan 
big data citra satelit sebagai variabel penyerta pada SAE. 

IV. METODE PENELITIAN  

A.  Landasan Teori 

Pengeluaran Rata-Rata Per Kapita 
Menurut Badan Pusat Statistik [13], Pengeluaran Rata-Rata 
per  Kapita  sebulan  adalah  biaya  yang  dikeluarkan  untuk 
konsumsi semua anggota rumah tangga selama sebulan dibagi 
dengan  banyaknya  anggota  rumah  tangga.  Pengeluaran  per 
Kapita sebulan dapat dirumuskan sebagai berikut:  

𝑦 =

𝑡

𝑞

(1) 

Keterangan: 
𝑦 = Pengeluaran Rata-Rata per Kapita 
𝑡 = Pengeluaran rumah tangga sebulan 
𝑞 = jumlah anggota rumah tangga 

Small Area Estimation (SAE) 

Small Area Estimation (SAE) adalah metode yang digunakan 
untuk  menduga  parameter  yang  berasal  dari  area  atau  sub 
populasi  dengan  ukuran  sampel  yang  kecil.  SAE  perlu 
""meminjam  kekuatan""  pada  area  kecil  melalui  informasi 
tambahan untuk menghasilkan estimasi ""tidak langsung"" lebih 
presisi  [4].  Suatu  area  disebut  kecil  apabila  sampel  yang 

 2 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

diambil 
tidak  mencukupi  untuk  melakukan  pendugaan 
langsung. Salah satu jenis model dasar yang digunakan adalah 
model  berbasis  area  level  yang  didasarkan  pada  ketersediaan 
data variabel penyerta yang hanya ada untuk level area tertentu. 
Pendugaan 𝑌𝑖 dapat diketahui dengan mengasumsikan penduga 
langsung 𝑌𝑖̂ tersedia dengan model sebagai berikut [4]. 

X17 
X18 
X19 
X20 
X21 
X22 
X23 
X24 

Jumlah Hotel  
Jumlah toko/warung kelomtong 
Jumlah Bank Umum Pemerintah 
Jumlah KUD 
Build-Up Area 
Land Surface Temperature 
Nighttime Light Intensity 
Air Pollution 

Podes 
Podes 
Podes 
Podes 
Landsat 8 
Landsat 8 
NPP-VIIRS 
Sentinel -5P 

2018 
2018 
2018 
2018 
2019 
2019 
2019 
2019 

(2) 

C.  Metode Analisis Data 

𝑌𝑖̂
𝑌𝑖̂

𝐹𝐻
(𝐸𝐵𝐿𝑈𝑃)
𝐹𝐻
(𝐸𝐵𝐿𝑈𝑃)

= 𝑥𝑖
= 𝑥𝑖

𝑇𝛽̂
𝑇𝛽̂

dimana 𝛾̂𝑖

𝐹𝐻 =

𝐺𝐿𝑆 + 𝑣̂𝑖
𝐺𝐿𝑆 + 𝛾̂𝑖
2
2𝑏𝑖
𝜎𝑣
2+𝛹𝑖
2𝑏𝑖
𝜎𝑣

𝐹𝐻  
𝐹𝐻(𝑌𝑖̂ − 𝑥𝑖

𝑇𝛽̂

𝐺𝐿𝑆), 𝑖 = 1, 2, … , m   

B.  Metode Pengumpulan Data 

Data yang digunakan dalam penelitian ini berasal dari tiga 
sumber yaitu Susenas Modul Konsumsi dan Pengeluaran Bulan 
Maret  tahun  2019  dan  data  Podes  2018  yang  diperoleh  dari 
Badan Pusat Statistik, serta data hasil olahan citra satelit yang 
merupakan  image  collection  dengan  periode  amatan  antara  1 
Januari  2019  hingga  31  Desember  2019.  Variabel  rata-rata 
pengeluaran  per  kapita  rumah  tangga  diperoleh  dari  Susenas 
2019. Sedangkan untuk variabel penyerta diperoleh dari hasil 
olahan citra satelit dan data Podes.  

Pengumpulan data spasial pada penelitian ini menggunakan 
bahasa  pemrograman  Python  dengan  Google  Earth  Engine 
Python  API  pada  Google  Colab.  Kumpulan  citra  satelit 
diekstraksi dan diolah untuk menghasilkan variabel citra satelit 
yang digunakan sebagai variabel penyerta dalam estimasi rata-
rata  pengeluaran  per  kapita.  Citra  satelit  yang  digunakan 
sebagai variabel penyerta dalam SAE dipilih berdasarkan hasil 
penelitian  sebelumnya  pada  Tabel  1.  tentang  potensi  remote 
sensing  sebagai  proksi  perekonomian  suatu  wilayah  yang 
dalam  hal  ini  adalah  rata-rata  pengeluaran  per  kapita. 
Sedangkan  sebagai  pembanding  dari  hasil  estimasi  variabel 
hasil  olahan  citra  satelit,  digunakan  variabel  penyerta  yang 
berasal  dari  data  Podes  yang  variabelnya  dipilih  berdasarkan 
pada  penelitian  sebelumnya  [14]  Untuk  lebih  lanjut variabel-
variabel  yang  akan  digunakan  dalam  penelitian  ini  beserta 
sumbernya pada Tabel II. 

TABEL II 
VARIABEL PENELITIAN 

Variabel 
(1) 

Nama Variabel 
(2) 

Y 
X1 
X2 
X3 
X4 
X5 
X6 
X7 
X8 
X10 
X11 
X12 
X13 
X14 

X15 
X16 

Pengeluaran rata-rata per kapita 
Jumlah keluarga pengguna listrik PLN 
Jumlah SD/MI 
Jumlah SMP/MTS 
Jumlah SMU/MA 
Jumlah Akademi/perguruan Tinggi 
Jumlah Rumah Sakit 
Jumlah Puskesmas Rawat Inap 
Jumlah Poliklinik/balai pengobatan 
Jumlah Tempat Praktik Dokter 
Jumlah Tempat Praktik Bidan 
Jumlah Polindes 
Jumlah Apotek 
Jumlah  Posyandu  menurut  Kegiatan 
Setahun Terakhir 
Jumlah Minimarket/swalayan 
Jumlah Restoran Rumah Makan 

Sumber 
(3) 
Susenas 
Podes 
Podes 
Podes 
Podes 
Podes 
Podes 
Podes 
Podes 
Podes 
Podes 
Podes 
Podes 
Podes 

Podes 
Podes 

Periode 
(4) 
2019 
2018 
2018 
2018 
2018 
2018 
2018 
2018 
2018 
2018 
2018 
2018 
2018 
2018 

2018 
2018 

Penelitian ini akan mengkaji pemanfaatan big data (dalam 
hal  ini  citra  satelit)  sebagai  variabel  penyerta  untuk  estimasi 
Rata-Rata Pengeluaran Rata-Rata per Kapita level kecamatan 
Studi  Kasus  Provinsi  Daerah  Istimewa  Yogyakarta.  Pada 
penelitian  ini  akan  dibentuk  tiga  model  SAE  EBLUP  Fay-
Herriot  berdasarkan  variabel  penyerta  yang  dipakai  yaitu 
variabel  hasil  olahan  citra  satelit,  data  Podes,  dan  kombinasi 
antara  variabel  penyerta  citra  satelit-Podes. Estimasi  rata-rata 
pengeluaran  per  kapita  2019  pada  penelitian  ini  dilakukan 
langkah-langkah sebagai berikut. 

Mengolah Data Spasial 
1.  Nighttime Light Intensity 

Data  Nighttime  Light  Intensity  didapatkan  melalui  data 
komposit  bebas  awan  dari  VIIRS  Day  /  Night  Band  (DNB). 
VIIRS DNB komposit bebas awan adalah salah satu data yang 
disediakan dengan cakupan bebas awan berdasarkan rata-rata 
cahaya malam dalam waktu satu bulan [8]. Aktifitas manusia 
yang  membutuhkan  cahaya  pada  malam  hari,  sehingga  pada 
wilayah yang memiliki aktifitas yang tinggi dapat ditunjukkan 
oleh pantulan cahaya yang tinggi. 

2.  Built-Up Area 

Indeks  Terbangun  atau  Built-Up  Area  diperoleh  melalui 
Citra  Landsat  8.  Citra  satelit  melalui  proses  pre-processing 
berupa koreksi awan terlebih dahulu sebelum memasuki tahap 
penghitungan  untuk  mendapatkan  Built-Up  Area.  Metode 
cloud filtering dilakukan untuk meminimalisasi tutupan awan 
dengan toleransi sebesar 10 persen. Setelah melalui tahap pre-
processing, maka selanjutnya dilakukan perhitungan Built-Up 
Area berdasarkan persamaan (3) [15]. 

(3) 
𝐵𝑈 = 𝑁𝐷𝐵𝐼 − 𝑁𝐷𝑉𝐼 
sedangkan untuk NDBI dan NDVI pada Landsat 8 didapatkan 
dengan formulasi (4) dan (5).  

𝑁𝐷𝐵𝐼 =

𝑁𝐷𝑉𝐼 =

𝑆𝑊𝐼𝑅(𝐵𝑎𝑛𝑑 6) − 𝑁𝐼𝑅(𝐵𝑎𝑛𝑑 5)
𝑆𝑊𝐼𝑅 (𝐵𝑎𝑛𝑑 6) + 𝑁𝐼𝑅𝐵𝑎𝑛𝑑 (5)
𝑁𝐼𝑅 (𝐵𝑎𝑛𝑑 5) − 𝑅𝐸𝐷 (𝐵𝑎𝑛𝑑 4)
𝑁𝐼𝑅 (𝐵𝑎𝑛𝑑 5) + 𝑅𝐸𝐷 (𝐵𝑎𝑛𝑑 4)

(4) 

(5) 

dimana: 
𝐵𝑈 
𝑁𝐷𝐵𝐼  = Normalized Difference Build-Index (NDBI) 
𝑁𝐷𝑉𝐼  = Normalized Difference Vegetation Index (NDVI) 

= Build-Up Index (BU) 

3.  Land Surface Temperature (LST) 

Land Surface Temperature diperoleh melalui Citra Landsat 
8 dikarenakan memiliki data thermal yang terdapat pada band 
10.  Sebelum  dilakukan  kalkulasi  untuk  mendapatkan  Land 
Surface Temperature, dilakukan metode cloud filtering untuk 
meminimalisasi  tutupan  awan  dengan  toleransi  sebesar  10 

 3 / 8 

 
 
 
       
 
 
 
 
 
 
 
 
 
persen.  Selanjutnya  dilakukan  kalkulasi  untuk  mendapatkan 
Land  Surface  Temperature  dengan  tahapan  sebagai  berikut 
[16]. 

Pollution  untuk  tiap  kecamatan  yang  selanjutnya  digunakan 
sebagai variabel penyerta dalam SAE EBLUP Fay-Herriot. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

1.  Menghitung nilai radians (𝐿𝜆) 

𝐿𝜆 = 𝑀𝐿 ∗ 𝑄𝑐𝑎𝑙 + 𝐴𝐿 

(6) 

dimana: 
𝑄𝑐𝑎𝑙   = thermal band citra Landsat 8 
𝑀𝐿 
𝐴𝐿 

= band-specific multiplicative rescaling factor  
= band-specific additive rescaling factor  

2.  Konversi TOA ke kecerahan suhu (𝐵𝑇) 

𝐾2
𝐾1
𝐿𝜆

) + 1

ln (

𝐵𝑇 =

dimana: 

− 273,15 

(7) 

= suhu kecerahan 
𝐵𝑇 
= konstanta thermal band 10 atau 11  
𝐾2 
𝐾1 
= konstanta thermal band 10 atau 11  
untuk dapat menghasilkan nilai dalam satuan celcius, 
maka hasil perhitungan di tambah -273,15 
3.  Menghitung NDVI berdasarkan persamaan (5) 
4.  Menghitung proporsi vegetasi (𝑃𝑣) 

𝑃𝑣 = (

𝑁𝐷𝑉𝐼 − 𝑁𝐷𝑉𝐼𝑚𝑖𝑛
𝑁𝐷𝑉𝐼𝑚𝑎𝑥 − 𝑁𝐷𝑉𝐼𝑚𝑖𝑛

2
)

(8) 

5.  Menghitung emisivitas permukaan tanah (ε) 

ε = 0,004 ∗ 𝑃𝑣 + 0,986 

(9) 

6.  Menghitung Land Surface Temperature (LST) 

LST =

𝐵𝑇

ƛ ∗ BT
𝑐2

(1 + (

))∗ ln(ε)

(10) 

 Keterangan: 
𝑐2= 14388 μm K 
Nilai dari ƛ untuk Landsat 8 adalah 10,8 ƛ (μm) 

4.  Air Pollution 

Air  Pollution  atau  polusi  udara  pada  penelitian  ini  diukur 
melalui  kandungan  Karbon  Monoksida  (CO).  Hal 
ini 
dikarenakan CO merupakan polutan utama yang berkontribusi 
lebih  dari  50  persen  pada  total  polusi  udara  yang  ada  [17]. 
ini  menggunakan  Citra  Satelit  Sentinel-5P 
Penelitian 
total  CO  ditangkap  melalui 
(Precursor). 
TROPOspheric  Monitoring 
Instrument  yang  melakukan 
pengamatan pada langit yang cerah dalam menghasilkan kolom 
total CO dengan kepekaan hingga lapisan batas troposfer [18]. 

  Kolom 

Selanjutnya,  tiap-tiap  variabel  hasil  olahan  citra  satelit 
tersebut  dilakukan  agregasi  dengan  batas  wilayah  kecamatan 
menggunakan fitur Zonal Statistics yang tersedia pada Google 
Earth Engine. Pada fitur ini, masing-masing variabel tersebut 
akan dihitung mediannya sehingga didapatkan Nighttime Light 
Intensity,  Built-Up  Area,  Land  Surface  Temperature, dan  Air 

Pemilihan Variabel Penyerta  
Pemilihan 

variabel 

penyerta 

dilakukan 

dengan 
menggunakan  metode  stepwise  yang  merupakan  gabungan 
antara backward dan forward selection. Variabel yang pertama 
kali  masuk  adalah  variabel  yang  korelasinya  baik  dan 
signifikan  dengan  variabel  amatan.  Kemudian  dilakukan  uji 
asumsi  multikolinieritas.  Variabel 
yang 
multikolinieritas akan dikeluarkan dari model. 

penyerta 

Estimasi menggunakan SAE Metode EBLUP Fay Herriot 
Pada  penelitian  ini,  estimasi  Pengeluaran  Rata-Rata  Per 
Kapita  tingkat  Kecamatan  tahun  2019  di  Provinsi  Daerah 
Istimewa  Yogyakarta  dihitung  menggunakan  Metode  SAE 
EBLUP  Fay  Herriot  dengan  membentuk  tiga  model  yang 
dibedakan  berdasarkan  variabel  penyertanya,  yaitu  variabel 
penyerta  data  Podes  2018,  hasil  olahan  citra  satelit,  dan 
kombinasi  citra  satelit-Podes.  Pemodelan  SAE  EBLUP  Fay-
Herriot dilakukan menggunakan persamaan (2). 

Membandingkan MSE dan RSE 
Nilai  Mean      Square      Error  (MSE)  digunakan  untuk 
mengukur seberapa baik penduga EBLUP, semakin kecil nilai 
MSE  maka  menghasilkan  penduga  yang  semakin  presisi. 
Perhitungan MSE dilakukan dengan rumus (11) [17]. 
2) 

𝑀𝑆𝐸(𝑌𝑖̂) = 𝑔1𝑖(𝜎̂𝑣

2) + 2𝑔3𝑖(𝜎̂𝑣

2) + 𝑔2𝑖(𝜎̂𝑣

Square  Error 

(RSE)  merupakan 

(11) 
ukuran 
Relative 
kekonvergenan  dari  estimasi  yang  dihasilkan  rasio  nilai  akar 
kuadrat  MSE  dengan  nilai  estimasi  variabel  respon  [19]. 
Semakin kecil nilai RSE suatu estimator menunjukkan penduga 
tersebut  semakin  akurat.    Nilai  RSE  dapat  diperoleh  dengan 
perhitungan pada persamaan (12). 

𝑅𝑆𝐸 = 

√𝑀𝑆𝐸(𝑌𝑖̂ )
𝑌𝑖̂

× 100 

(12) 

V.  KERANGKA PIKIR 

akan  mengalami 

Rata  Rata  Pengeluaran  per  Kapita  merupakan  salah  satu 
elemen penting dalam mengukur IPM, sehingga diperlukannya 
pengukuran hingga ke level mikro. Pengeluaran Rata-Rata Per 
Kapita dihitung berdasarkan data Susenas yang diperuntukkan 
untuk  estimasi  hingga  tingkat  kabupaten.  Sehingga  untuk 
estimasi  wilayah  yang  lebih  kecil  lagi  khususnya  pada  level 
kecamatan, 
dan 
menyebabkan  error  yang  besar.  Estimasi  tidak  langsung 
melalui  SAE  menjadi  solusi  keterbatasan  data  dengan 
meminjam  informasi  sebagai  variabel  penyerta  dari  wilayah 
yang  bersesuaian.  Akan 
tetapi,  variabel  penyerta  yang 
digunakan sebaiknya tidak mengandung error [4]. Sedangkan 
data Podes yang merupakan data tidak mengandung error biasa 
dipakai sebagai variabel penyerta, akan tetapi data Podes hanya 
dikumpulkan 
tahun,  sehingga 
dibutuhkan data alternatif yang lebih relevan dan reliabel untuk 
digunakan  sebagai  variabel  penyerta  dalam  estimasi  SAE. 

tiga  kali  dalam  sepuluh 

kekurangan 

data 

 4 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
Dalam hal ini, big data citra satelit duga menjadi salah satu data 
yang potensial digunakan sebagai variabel penyerta pada SAE. 
Berdasarkan penelitian sebelumnya pada Tabel .1, hasil olahan 
citra satelit dapat digunakan untuk mengukur proksi ekonomi 
dalam  hal  ini  adalah  rata-rata  pengeluaran  per  kapita.  Untuk 
lebih  lanjut  penelitian  ini  akan  melakukan  kajian  tentang 
penggunaan citra satelit sebagai variabel penyerta SAE dengan 
alur penelitian pada Gambar 1 sebagai berikut. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

tersebut  dilakukan  agregasi  dengan  batas  wilayah  kecamatan 
menggunakan fitur Zonal Statistics yang tersedia pada Google 
Earth Engine. Pada fitur ini, masing-masing variabel tersebut 
akan dihitung mediannya sehingga didapatkan Nighttime Light 
Intensity,  Built-Up  Area,  Land  Surface  Temperature, dan  Air 
Pollution  untuk  tiap  kecamatan  yang  selanjutnya  digunakan 
sebagai  variabel  penyerta  dalam  SAE  EBLUP  Fay-Herriot. 
Ukuran  median  dipilih  karena  tidak  sensitif  terhadap  nilai 
pencilan  serta  cocok  untuk  nilai  yang  homogen  maupun 
heterogen  [20].  Visualisasi  hasil  zonal  statistics  tiap  variabel 
citra  satelit  pada  tiap-tiap  kecamatan  di  Provinsi  Daerah 
Istimewa Yogyakarta digambarkan melalui Gambar 2. 

Gambar 1. Kerangka Pikir Penelitian 

VI. HASIL DAN PEMBAHASAN 

Berdasarkan  hasil  dari  Susenas  Modul  Konsumsi  dan 
Pengeluaran  bulan  Maret  tahun  2019,  data  estimasi  langsung 
rata-rata  pengeluaran  per  kapita  tingkat  kecamatan  Provinsi 
DIY  tahun  2019  yang  tersedia  pada  BPS  adalah  77  dari  78 
kecamatan  di  Provinsi  Daerah  Istimewa  Yogyakarta,  artinya 
bahwa  terdapat  satu  kecamatan  yang  tidak  tersampel  yaitu 
kecamatan  Pajangan.  Sehingga  pada  penelitian  ini  untuk 
estimasi  menggunakan  SAE  EBLUP-FH  dilakukan  untuk 
estimasi  pada  77  kecamatan  di  provinsi  Daerah  Istimewa 
Yogyakarta.  

A.  Estimasi  Rata-Rata  Pengeluaran Per  Kapita  dengan SAE 

EBLUP Fay Herriot 
SAE EBLUP-FH dengan Variabel Penyerta Citra Satelit 
Terdapat tiga jenis satelit yang digunakan, yaitu terdiri dari 
image  collection  dengan  tangkapan  antara  1  Januari  2019 
hingga  31  Desember  2019.  Sebelum  diolah  untuk  digunakan 
sebagai  variabel  penyerta,  citra  satelit  tersebut  melalui  tahap 
pre-processing terlebih dahulu. Diperolah empat citra Landsat 
8  periode  bulan  Oktober  hingga  November,  14  citra  per  hari 
Sentinel 5P dengan periode 1 Januari hingga Desember 2019, 
dan 12 citra bulanan NPP-VIRS dengan periode Januari hingga 
Desember  2019.  Selanjutnya,  tiap-tiap  citra  satelit  tersebut 
melalui proses pengolahan untuk mendapatkan Built-Up Area 
(X21),  Land  Surface  Temperature  (X22),  Nighttime  Light 
Intensity  (X23),  dan  Air  Pollution  (X24).  Setiap  variabel 

(a) 

(b) 

(c) 

(d) 

Gambar  2.  Visualisasi  Hasil  Olahan  Citra  (a)  Nighttime  Light  Intensity  (b) 
Built-Up Area (d) Land Surface Temperature (e) Air Pollution sesudah Zonal 
Statistics  dengan  Batas  Wilayah  Tiap  kecamatan  Provinsi  Daerah  Istimewa 
Yogyakarta Tahun 2019 

 5 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Berdasarkan hasil visualisasi variabel citra satelit tersebut, 
kecenderungan  yang  paling 
terlihat  berada  pada  Kota 
Yogyakarta yang merupakan ibukota Provinsi Daerah Istimewa 
Yogyakarta. Wilayah perkotaan biasanya cenderung memiliki 
rata-rata  pengeluaran  per  kapita  lebih  tinggi  dibandingkan 
dengan pedesaan [6]. Hal ini mengindikasikan adanya korelasi 
dari ke empat  variabel hasil olahan citra satelit dengan Rata-
Rata  Pengeluaran  Perkapita  khususnya  pada  wilayah 
perkotaan. Selain itu, pada Gambar 2(c) terdapat warna yang 
kurang wajar khususnya pada kecamatan di wilayah Kabupaten 
Gunung Kidul, suhu terlihat lebih tinggi daripada rata-rata yang 
ditandai  dengan  kecamatan  yang  berwarna  merah.  Hal  ini 
disebabkan  pada  tahun  2019  Kabupaten  Gunung  Kidul 
mengalami kemarau panjang sehingga berdampak kekeringan 
[22],  hal  ini  diperparah  dengan  struktur  tanah  karst  pada 
sebagian besar Kabupaten Gunung Kidul. Tanah karst tersebut 
sulit  menyerap  air,  sehingga 
terjadi  kekeringan  dan 
peningkatan suhu pada beberapa kecamatan tanah karst yaitu 
Kecamatan  Wonosari,  Ponjong,  Panggang,  Semanu, 
Purwosari,  Paliyan,  Saptosari,  Rongkop,  Tanjungsari,  Tepus, 
dan Girisubo [23]. 

Selanjutnya,  dilakukan  pemilihan  variabel  penyerta  hasil 
olahan  citra  satelit  dengan  metode  stepwise,  didapatkan  tiga 
variabel  pada  proses  stepwise  yaitu  Nighttime  Light  Intensity 
(X27),  Land  Surface  Temperature  (X28),  dan  Air  Pollution 
(X29).  Kemudian  dilakukan  pemodelan  SAE  EBLUP  Fay-
Herriot untuk mendapatkan estimasi rata-rata pengeluaran per 
kapita.  Berikut  pada  Tabel  III  merupakan  ringkasan  hasil 
estimasi  dengan  menggunakan  variabel penyerta hasil olahan 
citra satelit. 

TABEL III 
RINGKASAN HASIL ESTIMASI RATA-RATA PENGELUARAN PER KAPITA PADA 
77 KECAMATAN DENGAN SAE EBLUP-FH VARIABEL PENYERTA CITRA 
SATELIT (Rp 100000) 
Median 

Mean 

Max 

Min 

1 
5,348 

3 
9,986 

4 

6 

11,412 

14,698 

26,715 

1st 
Quantile 
2 
8,055 

3rd 
Quantile 
5 

SAE EBLUP-FH dengan Variabel Penyerta Podes 
Sebelum  melakukan  pemodelan  SAE,  dilakukan  seleksi 
variabel penyerta  Podes  menggunakan metode  stepwise. Dari 
20  variabel  yang  diikutsertakan  dalam  proses  stepwise, 
terpilihlah  enam  variabel  yang  signifikan  yaitu  X3,  X4,  X5, 
X10, X16, X17, dan X18. Kemudian dilakukan estimasi rata-
rata pengeluaran per kapita menggunakan SAE  EBLUP  Fay-
Herriot dengan variabel penyerta data Podes yang telah terpilih 
pada proses stepwise. Berikut ringkasan hasil estimasi dengan 
variabel penyerta data Podes. 

TABEL IV 
RINGKASAN HASIL ESTIMASI RATA-RATA PENGELUARAN PER KAPITA PADA 
77 KECAMATAN DENGAN SAE EBLUP-FH VARIABEL PENYERTA PODES (Rp 
100000) 

Min 

1 
5,352 

1st 
Quantile 
2 
8,086 

Median 

Mean 

3 
9,719 

4 
11,527 

3rd 
Quantile 
5 
15,587 

Max 

6 
26,715 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

SAE  EBLUP-FH  dengan  Variabel  Penyerta  Citra  Satelit 
dan Podes 
Setelah  melakukan  dua  pemodelan  SAE  EBLUP  Fay 
Herriot  menggunakan  variabel  penyerta  hasil  olahan  citra 
satelit  dan  variabel  penyerta  Podes,  selanjutnya  melakukan 
pemodelan  dengan  mengkombinasikan  variabel  penyerta  dari 
kedua  sumber  data  tersebut.  Terdapat  sembilan  variabel 
penyerta yang terdiri dari tiga variabel hasil olahan citra satelit, 
dan enam variabel data Podes yang diikutsertakan dalam proses 
stepwise, terpilihlah lima variabel yang signifikan pada model. 
Variabel tersebut adalah X4, X5, X10, X28, X29. Berdasarkan 
variabel penyerta yang terpilih, terdapat tiga variabel penyerta 
Podes  dan  dua  variabel  penyerta  citra  satelit.  Selanjutnya 
dilakukan 
rata-rata  pengeluaran  per  kapita 
menggunakan  SAE  EBLUP  Fay-Herriot  dengan  variabel 
penyerta tersebut, didapatkan ringkasan hasilnya pada Tabel V. 
TABEL V 
RINGKASAN HASIL ESTIMASI RATA-RATA PENGELUARAN PER KAPITA PADA 
77 KECAMATAN DENGAN SAE EBLUP-FH VARIABEL PENYERTA  
CITRA SATELIT-PODES ( Rp 100000) 

estimasi 

Min 

1 
5,350 

1st 
Quantile 
2 
8,105 

Median 

Mean 

3 
9,749 

4 
11,442 

3rd 
Quantile 
5 
14,689 

Max 

6 
26,715 

Berdasarkan  pada  hasil  estimasi  pada  tiga  model  SAE 
EBLUP  yang  dibedakan  berdasarkan  variabel  penyertanya 
yaitu variabel hasil olahan citra satelit, Podes, dan kombinasi 
keduanya. Kecamatan yang memiliki rata-rata pengeluaran per 
kapita  tertinggi  adalah  Kecamatan  Pakualaman  terletak  di 
Provinsi Daerah  Istimewa  Yogyakarta,  sedangkan  kecamatan 
dengan  rata-rata  pengeluaran  per  kapita  terendah  adalah 
Kecamatan Purwosari terletak di Kabupaten Gunung Kidul. 

B.  Perbandingan Estimasi Rata-Rata Pengeluaran Per Kapita 

dengan SAE EBLUP-FH 
Perbandingan Nilai Estimasi 
Nilai estimasi rata-rata pengeluaran per kapita tahun 2019 
di 77 kecamatan Provinsi Daerah Istimewa Yogyakarta antara 
estimasi  langsung  dan  tidak  langsung  menggunakan  metode 
SAE EBLUP-FH tidak menunjukan perbedaan yang signifikan. 
Hal  ini  dapat  dilihat  pada  grafik  Gambar  3.  hasil  estimasi 
sebagai berikut. 

Gambar 3. Grafik Hasil Estimasi Rata-Rata Pengeluaran Per Kapita 
pada 77 Kecamatan di Provinsi Daerah Istimewa Yogyakarta tahun 
Pada Grafik 3. tersebut terlihat bahwa hasil estimasi langsung 
2019  
dan  tidak  langsung  pada  semua  model  memiliki  pola  yang 
(Per Rp 100000) 

 6 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
mirip. Selain itu, hasil estimasi menggunakan SAE EBLUP Fay 
Herriot pada tiga jenis variabel juga memiliki pola yang mirip. 
Hal  ini  sesuai dengan ringkasan  pada  Tabel 3, 4, dan 5 yang 
tidak  menunjukkan  perbedaan  yang  signifikan  hasil  estimasi 
pada  semua  model  dalam  estimasi  rata-rata  pengeluaran  per 
kapita pada 77 kecamatan Provinsi DIY. 

Perbandingan MSE dan RSE  
Tinjauan baik atau tidaknya hasil estimasi yang diperoleh 
melalui  estimasi  langsung  maupun  SAE  pada  penelitian  ini 
diperhatikan  melalui  nilai  MSE  dan  RSE  nya.  Pada  gambar 
berikut menunjukkan grafik perbandingan MSE dan RSE hasil 
estimasi langsung, SAE EBLUP Fay-Herriot dengan variabel 
penyerta citra satelit, Podes, serta kombinasi citra satelit-Podes. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

dengan teori yang ada [24]. Selain itu, RSE yang dihasilkan dari 
model SAE juga telah menurunkan RSE dari estimasi langsung 
hingga di bawah 25%, yang merupakan ambang batas toleransi 
RSE dari BPS. 

Ditinjau dari penggunaan hasil olahan citra satelit sebagai 
variabel  penyerta,  pada  Gambar  4(a)  dan  4(b)  terlihat  bahwa 
MSE dan RSE yang dihasilkan pada SAE EBLUP Fay-Herriot 
variabel  penyerta  olahan  citra  satelit  terlihat  lebih  kecil 
daripada  MSE  dan  RSE  variabel  penyerta  data  Podes. 
Sedangkan untuk MSE dan RSE paling kecil dihasilkan pada 
SAE EBLUP Fay-Herriot dengan variabel penyerta kombinasi 
citra satelit-Podes, hal ini dikarenakan variabel penyerta yang 
dikombinasikan  merupakan  variabel  yang  signifikan  pada 
pemodelan  dengan  variabel  penyerta  podes  dan  variabel 
penyerta  olahan  citra  satelit.  Untuk  hasil  lebih  lanjut  terkait 
ringkasan  MSE  dan  RSE  estimasi  langsung  dan  SAE  dapat 
dilihat pada Tabel V. berikut. 

(a) 

(b) 

Gambar 4. Grafik Perbandingan (a) MSE dan (b) RSE Estimasi  
Rata-Rata Pengeluaran per Kapita pada 77 Kecamatan 
 Provinsi Daerah Istimewa Yogyakarta 

Berdasarkan grafik pada Gambar 4(a) dan 4(b), dapat dilihat 
bahwa MSE dan RSE yang dihasilkan pada SAE EBLUP Fay-
Herriot  lebih  kecil  dibandingkan  dengan  estimasi  secara 
langsung  rata-rata  pengeluaran  per  kapita  level  kecamatan  di 
Provinsi  Daerah  Istimewa  Yogyakarta.  Pola  penurunan  MSE 
dan RSE jelas terlihat pada kecamatan yang memiliki MSE dan 
RSE  yang  tinggi  pada  estimasi  langsung.  Sehingga  dapat 
dikatakan  hasil  estimasi  SAE 
lebih  presisi  dan  akurat 
dibandingkan  dengan  estimasi  langsung.  Hasil  ini  sejalan 

TABEL V 
RINGKASAN MSE DAN RSE HASIL ESTIMASI RATA-RATA PENGELUARAN PER 
KAPITA PADA 77 KECAMATAN  PROVINSI DAERAH ISTIMEWA YOGYAKARTA 
Max 

Median  Mean 

Model 

Min 

1st 
Quantile 
0,455 

3rd 
Quantile 
2,847 

0,000 

1,156 

3,451 

31,636 

E
S
M

E
S
R

Var 

Var 

Var 

Estimasi 
Langsung 
SAE 
Big Data 
SAE 
Podes 
SAE 
Kombinasi 
Estimasi 
Langsung 
SAE 
Big Data 
SAE 
Podes 
SAE 
Kombinasi 

Var 

Var 

Var 

0,000 

0,437 

1,039 

1,785 

2,229 

7,486 

0,000 

0,434 

1,059 

1,8233 

2,173 

8,119 

0,000 

0,425 

0,992 

1,5141 

1,992 

5,969 

0,000 

6,754 

10,413  11,698  16,611  35,554 

0,000 

6,568 

9,829 

10,137  14,389  20,719 

0,000 

6,626 

9,715 

10,147  14,702  22,218 

0,000 

6,571 

9,469 

9,650 

13,170  18,944 

C.  Kajian  Pemanfaatan  Big  Data  Citra  Satelit  pada  Metode 

SAE 
Hasil  estimasi  pada  subbab  sebelumnya  menunjukkan 
variabel penyerta hasil olahan citra satelit mampu menurunkan 
RSE  dan  MSE  dari  estimasi  langsung,  Selain  itu,  penurunan 
MSE  dan  RSE  yang  paling  besar  adalah  hasil  estimasi  pada 
SAE  variabel  penyerta  kombinasi  citra 
satelit-Podes, 
Penggunaan  variabel  kombinasi  Citra-Satelit  dan  Podes 
bertujuan untuk mengetahui kinerja SAE dalam meningkatkan 
presisi  dan  akurasi  apabila  kedua  variabel  yang  signifikan 
tersebut  dikombinasikan,  Selain  diukur  dari  sisi  presisi  dan 
akurasi  yang  dihasilkan  pada  model  SAE  EBLUP  dengan 
variabel penyerta big data, pertimbangan penggunaan big data 
citra satelit dapat ditinjau pada komponen-komponen sebagai 
berikut, 

Biaya 
Citra  satelit  pada  penelitian  ini  diakses  melalui  Google 
Earth Engine yang menyediakan citra satelit secara gratis untuk 
pendidikan, penelitian, dan penggunaan non-profit, Sedangkan 
pemanfaatan pada bidang operasional dan komersil diperlukan 
perizinan akses terlebih dahulu [25], Di sisi lain, penggunaan 

 7 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
citra satelit pada pemerintahan Indonesia disediakan gratis oleh 
LAPAN melalui Katalog Inderaja yang telah mendapat lisensi 
pemerintah,  artinya  seluruh  instansi  pemerintah  baik  pusat 
maupun daerah berhak mendapatkan data citra satelit,  

Tenaga 
Sebagai  negara  kepulauan 

terbesar,  menjadi  sebuah 
tantangan  bagi  Indonesia  dalam  pengumpulan  data  yang 
mencakup  seluruh  area  seperti  sensus,  BPS  membutuhkan 
koordinasi dengan mitra atau petugas, pemda, serta KSK pada 
setiap wilayah administratif  dalam pengumpulan data sensus, 
salah satunya yaitu Podes [26], Di sisi lain, big data citra satelit 
dapat  dikumpulkan  maupun  diolah  secara  sistematis  dan 
sentralisasi pada wilayah studi kasus, Sehingga pengumpulan 
hingga pengolahan data citra satelit tidak memerlukan tenaga 
yang banyak, Melalui penginderaan jarak jauh, memungkinkan 
pengumpulan data dengan  mudah melalui berbagai  skala  dan 
resolusi  serta  dapat  diproses  dengan  cepat,  serta  menjadi 
sebuah strategi sesuai dengan kondisi geografis Indonesia, 

Waktu 
Saat  ini,  citra  satelit  yang  tersedia  memiliki  temporal 
resolution yang tinggi, [27], Artinya bahwa untuk mendapatkan 
data citra dilokasi yang sama membutuhkan antar waktu yang 
rendah, biasanya dalam hitungan hari [28], Pada penelitian ini, 
citra satelit yang digunakan adalah Landsat 8 dengan resolusi 
temporal 16 hari, NOAA NPP Viirs 30 hari, dan Sentinel 5P 15 
citra  dalam  sati  hari,  Dengan  demikian,  big  data  citra  satelit 
dapat tersedia lebih cepat dibandingkan dengan pengumpulan 
data secara konvensional terlebih yaitu data yang dikumpulkan 
secara sensus khususnya Podes yang membutuhkan waktu yang 
lama dan tidak tersedia setiap tahun, 

VII. 

PENUTUP 

Hasil  estimasi  rata-rata  pengeluaran  per  kapita  pada  77 
kecamatan  didapatkan  pada  tiga  model  SAE  EBLUP-FH 
berdasarkan  variabel  penyertanya,  Pada  SAE  EBLUP-FH 
dengan  variabel  penyerta  olahan  citra  satelit  didapatkan  rata-
rata  hasil  estimasi  sebesar  Rp  1.141.200,-  dengan  variabel 
penyerta  data  Podes  sebesar  Rp  1.152.700,-  dengan  variabel 
penyerta kombinasi citra satelit-Podes sebesar Rp 1.144.200,- 
Selain itu, MSE dan RSE SAE EBLUP-FH lebih kecil daripada 
estimasi langsung, Ditinjau dari penggunaan variabel penyerta, 
MSE  dan  RSE  pada  SAE  dengan  variabel  hasil  olahan  citra 
satelit  terlihat  lebih  kecil  daripada  SAE  variabel  penyerta 
Podes, sedangkan MSE dan RSE paling kecil dihasilkan pada 
model  SAE  EBLUP  FH  dengan  variabel  penyerta  kombinasi 
citra  satelit-Podes,  Hal  ini  membuktikan  adanya  potensi 
penggunaan  big  data  khususnya  citra  satelit  sebagai  variabel 
penyerta SAE dikarenakan mampu memantau perubahan lebih 
cepat  yang  ditangkap  pada  permukaan  bumi,  Selain  itu,  citra 
satelit  dari  segi  biaya,  waktu,  dan  tenaga  yang  dikeluarkan 
relatif  lebih  rendah  dari  pada  pengumpulan  data  secara 
konvensional, 

Selain  itu,  saran  perbaikan  bagi  penelitian  selanjutnya, 
Sebelum  mengolah  citra  satelit  untuk  menghasilkan  variabel 
penyerta,  perlu  dilakukan  preprocessing  yang  lebih  baik  lagi 
supaya  mendapatkan  informasi  yang  lebih  baik  serta  perlu 
melakukan  pengkajian  lebih  lanjut  terkait  variabel  hasil  citra 
satelit  yang  akan  digunakan  sebagai  variabel  penyerta  pada 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

SAE,  Kemudian  mencari  ukuran  yang  lebih  baik  dalam 
merepresentasikan  nilai  variabel  pada  zonal  statistics, 
Dilakukan eksplorasi lebih lanjut terkait sumber big data citra 
satelit  yang  dapat  menggambarkan  proksi  penduga,  dalam 
penelitian ini adalah rata-rata pengeluaran per kapita. 

DAFTAR PUSTAKA 

[1]  Lesmana, “Perkembangan Indikator Ekonomi dan Kemakmuran Indonesia 
dibandingkan dengan 6 Negara Tetangga periode 2005-2012”, 5(1), 101-
111, 2014, 

[2]  BPS,  (2017),  BPS  [online],  https://www,bps,go,id/subject/5/konsumsi-
dan-pengeluaran,html#subjekViewTab1 diakses pada: 15 Maret 2021, 
[3]  W, Robi, “Metode Survey dan Interpretasi Data Vegetasi”, IPB Press, 
[4]  Rao, J, N, K,, & Molina I, Small Area Estimation, New Jersey: John Wiley 

&Sons, Inc, 2017, 2015, 

[5]  BPS, “Analisis Konsumsi Rumah Tangga 2007”, 2007, 
[6]  International  Monetary  Fund,  Illuminating  Economic  Growth  [Online], 
https://www,bps,go,id/subject/26/indeks-pembangunan-manusia,html 
diakses pada: 15 Maret 2021, 

[7]  Tamaludin, “Pemanfaatan Citra Satelit dalam Mengidentifikasi Perubahan 

Penutupan Lahan”, 14(2), 146-156, 2012, 

[8]  Kinga Ivan, I,-H, H, “VIIRS Nighttime Light Data for Income Estimation 

at Local Level”, MDPI, 1-19, 2020, 

[9]  Kamil  Faisal,  A,  S,  “Modelling  the  Relationship  between  the  Gross 
Domestic  Product  and  Built-Up  Area  Using  Remote  Sensing  and  GIS 
Data”, 1-17, 2016 

[10] Tariq  S,  U,  H,  “CO  Emissions  from  Pakistan  and  India  and  Their 
Relationship  With  Economic  Variables”,  Ecology  and  Environmental 
Research, 1301-1312, 2017, 

[11] Alexander  Buyantuyev,  “J,  W,  Urban  Heat  Islands  and  Landscape 

Heterogeneity”, Springer, 17-33, 2019, 

[12] Dissanayke,  “Impact  of  Urban  Surface  Characteristics  and  Socio-
Economic Variables on the Spatial Variation of Land Surface Temperature 
in Lagos City, Nigeria”, MDPI, 2018, 

[13] BPS, 

IPM 

[Online], 

https://www,bps,go,id/subject/26/indeks-

pembangunan-manusia,html diakses pada: 30 Oktober 2020, 

[14] F,  Hasan,  “Small  Area  Estimation  Terhadap  Pengeluaran  per  Kapita  di 
Kabupaten Sumenep dengan Metode Empirical Bayes”, ITS, Surabaya, 
[15] Zha, Y,, Gao, J,, Ni, S, “Use of Normalized Difference Built-Up Index In 
Automatically Mapping Urban Areas From TM Imagery”, Int, J, Remote 
Sens, 583-594, 

[16] USGS, 

(2017),  Using 

[online], 
https://landsat,usgs,gov/using-usgs-landsat-8-product  diakses  pada:  10 
Juni 2021, 

the  USGS  Landsat  8  Product 

[17] Fardiaz, “Polusi Air dan Udara, Penerbit Kanisius”, Yogyakarta, 2008, 
[18] Tropomi,  Carbon  Monoxide 

[Online],  http://www,tropomi,eu/data-

products/carbon-monoxide diakses pada: 15 Maret 2021, 

[19] Yang, “Consistency   of Cross     Validation     for     Comparing Regression   
Procedures,  Institute      of  Mathematical  Statistics”,  University  of 
Minnesota, 2017, 

[20] Pratikno, A, S, “Pemetaan Ukuran Pemusatan Data, OSF Preprints”, 2020 
[21] BPS, “Analisis Konsumsi Rumah Tangga 2007”, 2007, 
[22] BPBD, “Data Kekeringan Gunung Kidul”, Yogyakarta: BPBD, 2019, 
[23] ESDM, Nomor 1659K/40/MEN/2004 Tanggal 1 Desember 2004 tentang 
Penetapan  Kawasan  Karst  Gunung  Sewu  dan  Pacitan  Timur,  untuk 
Kabupaten  Gunung  Kidul  Kawasan  yang  Ditetapkan  Sebagai  Kawasan 
Karst, 2004, 

[24] Rao,  J,  “A  Comparison  of  Small  Area  Estimation  Methods  for  Poverty 

Mapping”, Calle Madrid:  Universidad Carlos III de Madrid, 2014, 

[25] GEE,  (n,d,),  Platform  GEE,  Diakses  pada:  Google  Earth  Engine: 

https://earthengine,google,com/platform/ 
[26] BPS, “Pendataan Potensi Desa 2018”, 2018, 
[27] Liu,  “Special  Issue  ""Remote  Sensing  Big  Data:  Theory,  Methods  and 

Applications""”, MDPI, 2017, 

[28] Shekhar, “ Temporal Resolution”, Springer, 2008, 

 8 / 8 

 
 
 
 
 
"
221710011,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Perancangan Kembali Antarmuka Pengguna
Aplikasi Romantik Online dengan Metode
User-Centered Design

Shiddaq Quthbuddin Pajriannor (221710011, 4SI1)

Dosen Pembimbing: Lutﬁ Rahmatuti Maghﬁroh

Ringkasan—Aplikasi Romantik Online adalah aplikasi berba-
sis web untuk pengelolaan rekomendasi kegiatan statistik.
Rekomendasi kegiatan statistik merupakan salah satu layanan
utama dari Badan Pusat Statistik (BPS) kepada penyelenggara
kegiatan statistik. Dalam proses pembuatan aplikasi ternyata
hanya berfokus pada kegunaan fungsi aplikasi akan tetapi
antarmukanya tidak diperhatikan. Oleh karena itu peneliti akan
melakukan perancangan kembali antarmuka aplikasi romantik
online dengan melibatkan pengguna. Berdasarkan hasil evalu-
asi antarmuka lama dengan menggunakan kuesioner evaluasi
heuristik dan QUIS dapat dikatakan bahwa aplikasi masih perlu
dikembangkan antarmukanya serta antarmukanya termasuk
kategori buruk. Proses perancangan antarmuka menggunakan
metode user-centered design, dimana solusi desain akan di-
lakukan secara bertahap yaitu iterasi pertama wireframe low
ﬁdelity, kemudian iterasi kedua wireframe high ﬁdelity dan iterasi
terakhir prototype. Hasil penelitian ini adalah rancangan desain
antarmuka baru berupa prototype. Pada evaluasi antarmuka baru
menunjukkan bahwa kepuasan pengguna telah meningkat serta
masalah pada indikator-indikator usability pada antarmuka baru
lebih baik dari antarmuka lama.

Kata Kunci—Evaluasi Heuristik, User-Centered Design, Ro-

mantik Online.

I. LATAR BELAKANG

Undang-Undang Republik Indonesia Nomor 16 Tahun 1997
tentang statistik [1] menekankan bahwa Badan Pusat Statistik
(BPS) merupakan lembaga yang diberi tugas dan tanggung
jawab terhadap penyelenggaraan statistik di Indonesia. Instansi
pemerintah wajib memberitahukan, mengikuti rekomendasi
BPS sebelum menyelenggarakan kegiatan statistik, dan meny-
erahkan hasil penyelenggaraan kepada BPS. Kewajiban terse-
but guna menghindari terjadinya duplikasi penyelenggaraan
kegiatan statistik, mendorong diperolehnya hasil yang secara
teknis dapat dipertanggung jawabkan, serta mengurangi ker-
aguan konsumen data atas beberapa sajian jenis data yang
sama tetapi angkanya berbeda [2].

Berdasarkan Peraturan Pemerintah Nomor 51 Tahun 1999
tentang penyelenggaraan statistik [3], BPS memiliki kewenan-
gan untuk memberikan rekomendasi penyelenggaraan kegiatan
survei statistik sektoral. Perkembangan teknologi dan infor-
masi yang semakin pesat mendorong BPS membangun ap-
likasi rekomendasi kegiatan statistik (romantik) online berbasis
web yang dapat di akses melalui https://romantik.bps.go.id
dengan harapan proses pelaksanaan kegiatan rekomendasi
statistik dapat dilaksanakan secara eﬁsien [2]. Aplikasi ro-

mantik online adalah suatu aplikasi berbasis web untuk pen-
gelolaan rekomendasi kegiatan statistik. Aplikasi romantik
online bermanfaat baik dari sisi
internal (BPS) dan juga
eksternal (penyelenggara kegiatan statistik). Manfaat dari sisi
internal yaitu memudahkan dalam memberikan rekomen-
dasi kegiatan statistik, memudahkan monitoring, dan seba-
gai media dokumentasi kegiatan statistik sektoral. Ada pula
manfaat dari sisi eksternal yaitu memudahkan Kemente-
rian/Lembaga/Organisasi Perangkat Daerah (K/L/PD) mem-
beritahukan rencana kegiatan statistiknya tanpa harus datang
ke BPS, memudahkan pengajuan pertanyaan terkait dengan
mekanisme rekomendasi kegiatan statistik, dan sebagai alat
bantu untuk menghindari duplikasi kegiatan statistik [2].
Tetapi menurut wawancara dari Kasi Rekomendasi Kegiatan
ternyata hanya
Statistik, dalam proses pembuatan aplikasi
berfokus pada kegunaan fungsi aplikasi akan tetapi antar-
mukanya tidak diperhatikan.

Pada penelitian ini dilakukan evaluasi awal untuk menda-
patkan masalah pada antarmuka pengguna aplikasi romantik
online. Alat evaluasi menggunakan kuesioner evaluasi heuris-
tik [4] untuk usability dan Questionnarie for User Interface
Satisfaction (QUIS) [5] untuk kepuasan pengguna. Tan, Liu
dan Bishu [6] percaya evaluasi heuristik harus dilakukan pada
tahap awal proses pengembangan desain antarmuka pengguna.
Perbedaan evaluasi heuristik dibandingkan metode lain ter-
letak pada kecepatan dalam menemukan masalah usability
[7]. Sedangkan penggunaan QUIS untuk mengukur kepuasan
pengguna yang mana merupakan ukuran penting dalam ke-
berhasilan sistem [8].Dari hasil evaluasi awal tersebut dapat
disimpulkan bahwa nilai kepuasan pengguna kurang dari tujuh,
menurut Subekti [9] nilai
ini berarti aplikasi masih perlu
dikembangkan antarmukanya dan nilai hasil evaluasi heuristik
di antara dua puluh sembilan hingga empat puluh sembilan
termasuk kategori buruk, menurut Turner [4] kategori buruk
berarti pengguna mengalami kesulitan dalam menggunakan
aplikasi romantik online.

Berdasarkan hasil tersebut, berarti diperlukan pengemban-
gan antarmuka supaya pengguna tidak mengalami kesulitan
dalam menggunakan aplikasi romantik online. Salah satu
caranya adalah dengan perancangan kembali antarmuka peng-
guna aplikasi romantik online. Dalam proses perancangan
antarmuka, salah satu metode yang dapat digunakan adalah
metode User-Centered Design (UCD)
[10]. UCD adalah

1 / 8

proses desain berulang di mana desainer fokus pada pengguna
dan kebutuhannya pada setiap fase proses desain [11]. Abras
Chadia, dkk [10] dalam penelitian mereka menyimpulkan
bahwa keterlibatan pengguna dalam desain dengan satu atau
lain cara telah terbukti tertuju kepada pengembangan desain
yang lebih bermanfaat dan memuaskan. Pada penelitian Danny
Murillo dan Jeremias Herrera [12] juga menyimpulkan bahwa
metode UCD memiliki kegunaan dan peran dalam proyek web
atau digital yang tidak hanya berada dalam sebuah cakupan
analisis sistem dan programmer. Nallan dan Jaiswal [13]
menjelaskan bahwa perbedaan metode UCD dengan metode
lain adalah pada fokus prinsip mereka, fokus UCD pada
meningkatkan usability dan pengalaman pengguna. Memang
umumnya metode UCD dianggap telah meningkatkan kegu-
naan dan usability produk, meskipun tingkat adopsi metode ini
tidak cukup merata di berbagai organisasi [14], tetapi metode
ini dapat menjembatani kesenjangan antara pengembang den-
gan pengguna [15], dan juga metode ini dapat menunjukkan
manfaat terhadap kegunaan sistem, kualitas informasi, dan
kualitas antarmuka. [16]. Oleh karena itu dengan keterli-
batan pengguna pada proses perancangan akan meningkatkan
kepuasan pengguna dan mengurangi masalah pada indikator-
indikator usability dengan menggunakan metode UCD.

II. TUJUAN PENELITIAN

Berdasarkan latar belakang tersebut, maka tujuan dari
penelitian ini adalah melakukan perancangan kembali antar-
muka pengguna aplikasi romantik online untuk meningkatkan
kepuasan pengguna dan mengurangi masalah pada indikator-
indikator usability dengan menggunakan metode UCD.

III. PENELITIAN TERKAIT

Pada penelitian Johnson, dkk.

[16] membahas secara
mendetail bagaimana penerapan metode UCD dan bagaimana
penerapan metode ini pada proses merancang kembali antar-
muka. Penelitian ini menyimpulkan bahwa dengan metode ini
dapat menunjukkan manfaat terhadap kegunaan sistem, kuali-
tas informasi, dan kualitas antarmuka. Sama halnya penelitian
yang dilakukan oleh Abras Chadia, dkk [10] yang menyim-
pulkan bahwa keterlibatan pengguna dalam desain dengan satu
atau lain cara telah terbukti tertuju kepada pengembangan
desain yang lebih bermanfaat dan memuaskan. Kaitannya
dengan penelitian ini adalah penggunaan metode UCD.

Pada penelitian terkait yang dilakukan oleh Danny Murillo
dan Jeremias Herrera [12] yang melakukan penelitian tentang
penggunaan wireframe dan mockup pada merancang kem-
bali situs web universitas menggunakan metode UCD. Pada
proses evaluasi dan desain, mereka menggunakan wireframes
karena memungkinkan pengguna untuk membuat evaluasi, dan
juga memungkinkan untuk menunjukkan kemajuan proyek
kepada institusi. Pada penelitian ini pesan yang mereka coba
sampaikan bahwa teknik dan konsep metode UCD memiliki
kegunaan dan peran dalam proyek web atau digital yang
tidak hanya berada dalam sebuah cakupan analisis sistem dan

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

programmer. Kaitannya dengan penelitian ini adalah penggu-
naan metode yang sama serta penggunaan wireframe untuk
menampilkan hasil rancangan desain kepada pengguna.

IV. METODE PENELITIAN

Proses dan tahapan yang dilakukan dalam penelitian ini
dimulai dari identiﬁkasi masalah, dilanjutkan dengan studi
literatur, berdasarkan studi literatur yang ada peneliti kemudian
melalukan analisis. Ada dua analisis yang digunakan yaitu
analisis kebutuhan pengguna dan analisis sistem yang berjalan.
Analisis kebutuhan pengguna diperlukan dalam mendesain
ulang, tujuannya untuk mengetahui karakter pengguna dalam
menggunakan aplikasi [16].

Pada analisis sistem berjalan peneliti melakukan evaluasi
awal. Alat evaluasi menggunakan kuesioner evaluasi heuristik
[4], QUIS [5], [8] dan observasi sistem [12]. Kuesioner
evaluasi heuristik dan QUIS akan dibagikan kepada empat
orang responden, respondennya adalah pengguna dari aplikasi
romantik online dari sisi pegawai BPS yang menggunakan
aplikasi romantik online. Nielsen dan Molich [17] merekomen-
dasikan jumlah responden berada di antara tiga hingga lima
responden.

Dari analisis-analisis yang telah dilakukan akan digunakan
pada tahap perancangan. Proses perancangan desain antarmuka
menggunakan metode UCD. Proses iterasi UCD terdapat em-
pat fase dapat dilihat pada gambar 1 [11], [18] yaitu :

1) Understand context of use

Pada fase ini peneliti akan mengidentiﬁkasi dan mema-
hami konteks di mana pengguna dapat menggunakan
sistem dan untuk apa mereka menggunakan.

2) Specify user requirements

Setelah memahami konteks di mana pengguna dapat
menggunakan sistem dan untuk apa mereka meng-
gunakan. Kemudian menentukan persyaratan/kebutuhan
bisnis atau tujuan pengguna yang harus dipenuhi.

3) Design solutions

Pada fase ini akan merancang/mengembangkan solusi
desain yang dilakukan secara bertahap.

4) Evaluate against requirements

Pada fase ini akan mengevaluasi apakah solusi desain su-
dah sesuai terhadap konteks dan persyaratan/kebutuhan
pengguna.

Gambar 1. Metode UCD [11]

Dalam proses perancangan akan dilakukan sebanyak tiga
kali
iterasi UCD yaitu iterasi yang pertama solusi desain
berbentuk wireframe low ﬁdelity, kemudian pada iterasi
berikutnya solusi desain berbentuk wireframe high ﬁdelity, dan

2 / 8

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

pada iterasi terakhir solusi desain berbentuk prototype [10],
[12], [16].

Pada evaluasi iterasi pertama dan kedua berbeda dengan
evaluasi awal, yaitu menggunakan tanggapan dan umpan balik
dari pengguna dengan menggunakan kuesioner ”suka/tidak
suka” beserta alasannya yang akan dibagikan kepada empat
orang responden yang berbeda di setiap iterasinya. Sedangkan
pada iterasi terakhir, evaluasi yang digunakan sama dengan
evaluasi awal pada analisis sistem berjalan serta dibagikan
kepada responden yang sama. Setelah semua iterasi dilakukan
dan akan dilakukan perbandingan antara antarmuka lama
dengan antarmuka baru. Proses dan tahapan disajikan dalam
bentuk Gambar 2.

Gambar 2. Alur Penelitian

Gambar 3. Kerangka Pikir Penelitian

V. KERANGKA PIKIR
Penelitian dimulai dari identiﬁkasi masalah yaitu proses
pembuatan aplikasi ternyata hanya berfokus pada kegunaan
fungsi aplikasi akan tetapi antarmukanya tidak diperhatikan.
Setelah dilakukan evaluasi awal ditemukan bahwa pengguna
kesulitan menggunakan aplikasi dan kepuasan pengguna ku-
rang. Oleh karena itu peneliti melakukan perancangan kem-
bali antarmuka pengguna dengan melibatkan pengguna secara
langsung untuk meningkatkan kepuasan pengguna dan men-
gurangi masalah pada indikator-indikator usability.

Untuk melakukan hal

itu, penelitian ini menggunakan
metode UCD yang mana metode ini menjembatani kesen-
jangan antara pengembang dengan pengguna [15]. Menurut
Chadia Abras, dkk. keuntungan utama dari metode ini adalah
pemahaman yang lebih dari pengguna serta pengguna akan
merasa bahwa ide dan saran mereka diperhitungankan selama
proses perancangan desain. Sedangkan kerugiannya adalah
memerlukan banyak waktu yang lebih dan biaya yang lebih
[10]. Kemudian pada proses perancangan desain menerapkan
tiga tujuan yaitu: (1) menyediakan fungsi yang tepat; (2)
menawarkan kegunaan dan keandalan yang lebih; serta (3)
melibatkan pengguna dengan ﬁtur-ﬁtur yang menyenangkan
[19].

Dalam proses perancangan akan dilakukan sebanyak tiga
kali
iterasi UCD yaitu iterasi yang pertama solusi desain
berbentuk wireframe low ﬁdelity, kemudian pada iterasi
berikutnya solusi desain berbentuk wireframe high ﬁdelity,

dan pada iterasi terakhir solusi desain berbentuk prototype
[10], [12], [16]. Evaluasi awal akan menjadi dasar pada
iterasi pertama, dilanjutkan membuat rancangan desain. Pada
iterasi pertama dan kedua rancangan desain menggunakan alat
desain Figma [20], sedangkan pada iterasi terakhir akan berupa
prototype yang sudah menjadi kode-kode program agar dapat
langsung digunakan.

Hasil penelitian berupa rancangan desain antarmuka baru
di mana perbandingan antarmuka lama dan baru menun-
jukkan bahwa kepuasan pengguna terhadap antarmuka baru
meningkat serta masalah pada indikator-indikator usabil-
ity pada antarmuka baru lebih baik dari antarmuka lama.
Kerangka pikir penelitian disajikan dalam bentuk Gambar 3.

VI. HASIL DAN PEMBAHASAN

A. Analisis Kebutuhan Pengguna

Berdasarkan dari

jawaban responden didapatkan bahwa
pengguna menggunakan aplikasi romantik online dengan Per-
sonal Computer (PC)/Komputer. Sistem operasi yang digu-
nakan pada PC/Komputer pengguna adalah windows, dan rata-
rata lama penggunaan PC/Komputer dalam satu hari adalah 9
jam.

Beberapa alasan pengguna menyukai menggunakan ap-
likasi romantik online ini yaitu memangkas birokrasi, memu-
dahkan dalam pengisian kuesioner, serta memudahkan dalam
melakukan peninjauan terhadap survei (setiap prosesnya dapat
terpantau). Ada pula beberapa alasan mengapa pengguna tidak

3 / 8

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

(karakter) pada layar, dan indikator sistem cenderung
tidak dapat diandalkan - dapat diandalkan.

3) Analisis Tugas

Pada analisis ini dilakukan observasi dan studi literatur.
Untuk mempermudah dalam observasi peneliti membuat
Diagram hierarki analisis tugas (HTA). Pada Gambar 4
menunjukkan bahwa ada langkah-langkah yang redun-
dant dalam mencapai tujuannya.

menyukai yaitu menu pada aplikasi kurang dapat dimengerti,
tidak ada pengertian dari tiap-tiap pertanyaan yang ada di
formulir pengisian pengajuan kegiatan statistik, tampilan awal
kurang menarik, serta pengguna menganggap aplikasi
ini
masih agak rumit untuk digunakan oleh pengguna baru.

Hasil analisis lingkungan pengguna menunjukkan bahwa
pengguna menggunakan aplikasi romantik online di tempat
dengan pencahayaan yang terang dengan skala empat dari
lima dan dengan kebisingan tenang dengan skala dua dari
lima, serta tersedia listrik, jaringan internet, komputer dan
sumber daya lain yang diperlukan untuk menggunakan aplikasi
romantik online.

B. Analisis Sistem Berjalan

1) Evaluasi Heuristik

Pada evaluasi ini ada sepuluh prinsip usability yang di-
jabarkan menjadi empat puluh lima indikator. Hasil eval-
uasi menunjukkan bahwa dari empat puluh lima indika-
tor ditemukan dua puluh sembilan indikator berkategori
buruk, empat belas indikator berkategori sangat buruk,
dan dua indikator berkategori biasa. Selain itu nilai hasil
evaluasi juga berada di antara dua puluh sembilan hingga
empat puluh sembilan yang artinya termasuk kategori
buruk, dan dapat diartikan bahwa pengguna mengalami
kesulitan dalam menggunakan aplikasi. Hasil evaluasi
disajikan dalam bentuk Tabel I.

TABEL I
HASIL EVALUASI HEURISTIK AWAL

No.
1

Prinsip Usability
Fitur dan Fungsi

Kategori
Buruk

Beranda atau Halaman Awal

Buruk

2

3

4

5

6

7

8

9

Navigasi

Pencarian

Kontrol dan Umpan Balik

Formulir

Kesalahan

Konten dan Teks

Bantuan

10

Performa

Sangat Buruk

Buruk

Buruk

Buruk

Buruk

Buruk

Buruk

Buruk

Berdasarkan Tabel I, sepuluh prinsip usability diatas
hanya navigasi yang berkategori sangat buruk dan lain-
nya berkategori buruk.

2) Questionnarie for User Interface Satisfaction (QUIS)

Gambar 4. Diagram hierarki analisis tugas (HTA)

C. Iterasi I - Wireframe Low Fidelity

Dari evaluasi
ini dapat disimpulkan bahwa nilai
kepuasan pengguna kurang dari tujuh, menurut Subekti
[9] nilai ini berarti aplikasi masih perlu dikembangkan
antarmukanya dan terdapat
tiga indikator yang men-
dapatkan nilai lebih dari tujuh yaitu indikator Sistem
cenderung tidak memadai - memadai, indikator tulisan

Tahap pertama dari iterasi ini adalah Understand context of
use. Pada tahap ini berdasarkan analisis kebutuhan pengguna
dapat disimpulkan bahwa pengguna menggunakan aplikasi
pada PC/Komputer dan salah satu tujuan pengguna menggu-
nakan aplikasi adalah memudahkan dalam pengisian kuesioner
pada pengajuan rekomendasi survei. Tahap selanjutnya adalah

4 / 8

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Specify user requirements. Setelah memahami konteks dan
tujuan pengguna, peneliti menentukan kebutuhan pengguna
yaitu formulir yang menyenangkan dan mudah, navigasi menu
yang sederhana, dan terdapat bantuan pada pengisian formulir.
Kemudian pada tahap Design solutions akan dibuat wire-
frame low ﬁdelity. Pertama melakukan brainstorming solusi
desain dengan membuat sketsa di kertas. Sketsa yang telah
terbentuk akan diubah menjadi wireframe low ﬁdelity di
Figma. Menurut Gemayel [21] langkah terbaik untuk membuat
wireframe yaitu menggunakan elemen yang sederhana, warna
harus abu-abu, menampilkan graﬁk dan gambar dengan bentuk
kotak, dan memperhatikan ukuran layar. Selanjutnya di Figma
akan dibuat sebuah frame dengan menekan ikon frame atau
menekan tombol ”F” pada keyboard, lalu akan muncul pilihan
ukuran layar. Setelah memilih ukuran layar akan dibuat elemen
yang sesuai dengan sketsa yang ada. Ketika sketsa tidak
mencakup kebutuhan pengguna, maka akan dilakukan brain-
storming dengan Mood Board untuk memberikan gambaran
ide atau pemikiran secara visual [22]. Peneliti mengumpulkan
beberapa visual layout, gambar, tulisan, dan elemen yang dapat
memenuhi kebutuhan pengguna. Salah satu rancangan desain
dapat dilihat pada Gambar 5.

Tahap terakhir adalah Evaluate against requirements. Dari
rancangan desain wireframe low ﬁdelity yang selesai akan
dilakukan evaluasi untuk mendapatkan umpan balik mengenai
layout, serta komponen-komponen lainnya. Dari Tabel II dapat
dikatakan bahwa jumlah pengguna yang suka lebih banyak
dari pada jumlah yang tidak suka. Adapun alasan-alasan dari
umpan balik ini akan dijadikan bahan evaluasi dan digunakan
pada tahap iterasi selanjutnya.

TABEL II
UMPAN BALIK ITERASI I

No.
1

Tampilan
Halaman Depan

Suka
4

Tidak Suka
0

2

3

4

5

6

7

8

9

10

11

12

13

Daftar

Halaman
Rekomendasi
Rincian Konten Kegiatan
Statistik
Halaman
Saya
Halaman
Rekomendasi
Statistik
Halaman FAQs

Pengajuan
Kegiatan

Rekomendasi

Halaman Panduan

Halaman Bantuan
Dukungan
Halaman Pertanyaan Saya

dan

Halaman Tentang Kami

Halaman Hubungi Kami

Halaman Proﬁl Saya

Halaman Pesan Notiﬁkasi

2

3

4

4

3

4

4

3

4

4

4

4

2

1

0

0

1

0

0

1

0

0

0

0

Gambar 5. Contoh Rancangan Wireframe Low Fidelity

D. Iterasi II - Wireframe High Fidelity

Pada tahap ini proses iterasi UCD dimulai dari

tahap
Design solutions. Solusi desain akan mengalami perubahan
yang awalnya berupa wireframe low ﬁdelity menjadi wireframe
high ﬁdelity. Perubahan yang berbeda yaitu adanya visual
yang lebih jelas seperti warna, gambar, tipograﬁ, dan lainnya.
Misalnya pada perancangan desain halaman depan, pertama
melihat dan membaca hasil umpan balik Iterasi I, dapat
dilihat pada tabel II. Dari tabel II dapat diketahui jumlah
pengguna yang suka dan tidak suka beserta alasannya. Sete-
lah mengetahui hal tersebut, dilanjutkan membuat rancangan
desain wireframe high ﬁdelity. Kedua menyalin rancangan
desain tampilan halaman depan dari wireframe low ﬁdelity.
Kemudian mengganti warna utama menjadi warna biru tua,
menurut Cheery [23] warna ini dapat membawa perasaan
tenang, memberikan kesan dapat diandalkan, dan sering ada
pada dekorasi kantor. Kemudian menambahkan gambar atau

5 / 8

ilustrasi untuk memberikan gambaran-gambaran dari kalimat
atau kata di samping gambar. Gambar atau ilustrasi diperoleh
dari pihak ketiga yaitu DrawKit [24], sedangkan untuk ikon
diperoleh dari pihak ketiga yaitu Font Awesome [25]. Pada ti-
pograﬁ menyesuaikan warna pada latar belakang seperti ketika
warna latar belakang biru maka warna tulisan menjadi putih,
dan ketika warna latar belakang putih maka warna tulisan
menjadi abu-abu tua, juga menyesuaikan ketebalan tulisan,
mengatur jarak antar karakter yang ada pada tulisan. Ter-
akhir menambahkan beberapa bentuk lingkaran yang masing-
masing memiliki warna biru, hijau, dan jingga yang ukurannya
berbeda-beda. Salah satu rancangan desain dapat dilihat pada
Gambar 6.

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

TABEL III
UMPAN BALIK ITERASI II

No.
1

Tampilan
Halaman Depan

Suka
3

Tidak Suka
1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

Daftar

Halaman
Rekomendasi
Rincian Konten Kegiatan
Statistik
Halaman
Saya
Halaman
Rekomendasi
Statistik
Halaman FAQs

Pengajuan
Kegiatan

Rekomendasi

Halaman Panduan

Halaman Bantuan
Dukungan
Halaman Pertanyaan Saya

dan

Halaman Tentang Kami

Halaman Hubungi Kami

Halaman Proﬁl Saya

Halaman Pesan Notiﬁkasi

Rincian Catatan Perbaikan

Halaman Lihat Isian Form

4

3

4

4

4

3

4

4

2

3

4

4

4

3

0

1

0

0

0

1

0

0

2

1

0

0

0

1

E. Iterasi III - Prototype

Pada tahap ini proses iterasi UCD dimulai dari tahap Design
solutions. Solusi desain akan mengalami perubahan yang awal-
nya berupa wireframe high ﬁdelity menjadi prototype berupa
halaman web.

Peneliti memanfaatkan salah satu ﬁtur dari Figma untuk
mempermudah proses pengembangan prototype yaitu Tab In-
spect. Tab Inspect memungkinkan untuk dapat melihat dan
menyalin kode dan nilai yang ada dari sebuah desain. Setelah
kode disalin, kemudian dilanjutkan pada berkas html dengan
membuat tag element dengan atribut class yang diberi nilai
unik untuk menandakan gaya atau style desainnya. Sementara
itu pada berkas css akan ditambahkan nilai unik tersebut dan
masukan nilai dan kode yang telah disalin dari Tab Inspect.
Jika ada komponen memerlukan interaksi, maka akan ditam-
bahkan berkas JavaScript untuk memberikan interaksinya.

Setelah semua rancangan desain berubah menjadi halaman-
halaman web. Untuk merapikan atau menyusun semua berkas,
peneliti menggunakan package Webpack. Menurut Dulanga
[26] Webpack memberikan kemudahan untuk menyusun kode
dan mengompilasinya ke dalam output yang dioptimalkan. Se-
lain itu Webpack bukan hanya dapat memudahkan membundel
berkas-berkas modul, gambar, styles, assests, dan scripts tetapi
juga dasar dari pengembangan web modern.

Tahap terakhir adalah Evaluate against requirements. Dari
rancangan desain wireframe low ﬁdelity yang selesai akan di-
lakukan evaluasi akhir. Yang pertama adalah evaluasi heuristik.
Berdasarkan Tabel IV, sepuluh prinsip usability di atas, hanya

6 / 8

Gambar 6. Contoh Rancangan Wireframe High Fidelity

Tahap selanjutnya adalah Evaluate against requirements.
Dari rancangan desain wireframe high ﬁdelity yang selesai
akan dilakukan evaluasi. Dari Tabel III dapat dikatakan bahwa
jumlah pengguna yang suka lebih banyak dari pada jumlah
yang tidak suka. Adapun alasan-alasan dari umpan balik ini
akan dijadikan bahan evaluasi dan digunakan pada tahap iterasi
selanjutnya.

formulir yang berkategori buruk sedangkan lainnya berkategori
biasa dan baik. Pada evaluasi heuristik akhir ini, nilai hasil
evaluasi berada di antara empat puluh sembilan hingga enam
puluh sembilan yang artinya termasuk kategori biasa, dan
dapat diartikan bahwa pengguna dapat menggunakan aplikasi
dan dapat menyelesaikan tugas yang paling penting, namun
pengalaman pengguna masih bisa ditingkatkan. Selain itu
hasil evaluasi menunjukkan bahwa dari empat puluh lima
indikator ditemukan dua indikator berkategori sangat buruk,
sembilan belas indikator berkategori buruk, tiga belas indikator
berkategori biasa, sembilan indikator berkategori baik, dan dua
indikator berkategori sangat baik.

TABEL IV
HASIL EVALUASI HEURISTIK AKHIR

No.
1

Prinsip Usability
Fitur dan Fungsi

Kategori
Baik

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Gambar 7. Graﬁk Perbandingan Hasil Evaluasi QUIS Antarmuka Lama dan
Baru

Beranda atau Halaman Awal

Biasa

2

3

4

5

6

7

8

9

Navigasi

Pencarian

Kontrol dan Umpan Balik

Formulir

Kesalahan

Konten dan Teks

Bantuan

10

Performa

Biasa

Baik

Biasa

Buruk

Biasa

Biasa

Biasa

Biasa

Terakhir dilakukan evaluasi QUIS. Dari evaluasi ini dapat
disimpulkan bahwa nilai kepuasan pengguna lebih dari tujuh
dan tidak ada indikator yang memiliki nilai kurang dari tujuh.

F. Perbandingan Antarmuka Lama dan Baru

Perbandingan dilakukan dengan membandingkan hasil eval-
uasi awal dengan evaluasi akhir. Perbandingan hasil evaluasi
QUIS antarmuka lama dan baru dapat dilihat pada Gambar
7 yang menunjukkan bahwa kepuasan pengguna meningkat
terhadap antarmuka baru dibanding antarmuka lama.

Perbandingan hasil evaluasi heuristik antarmuka lama dan
baru dapat dilihat pada Gambar 8 yang menunjukkan bahwa
indikator-indikator usability pada antarmuka baru keseluruhan-
nya mengalami peningkatan dapat diartikan bahwa masalah
pada indikator-indikator usability pada antarmuka baru lebih
baik dari antarmuka lama.

Perbandingan-perbandingan tersebut menunjukkan bahwa
kepuasan pengguna telah meningkat serta masalah pada
indikator-indikator usability pada antarmuka baru lebih baik
dari antarmuka lama.

VII. PENUTUP

Pada hasil evaluasi awal ditemukan bahwa dapat disim-
pulkan pengguna kesulitan dalam menggunakan aplikasi dan
antarmukanya masih perlu dikembangkan. Sehingga dilakukan

Gambar 8. Graﬁk Perbandingan Hasil Evaluasi Heuristik Antarmuka Lama
dan Baru

perancangan kembali antarmuka dengan metode UCD. Dalam
proses perancangan akan dilakukan sebanyak tiga kali iterasi
UCD yaitu iterasi yang pertama solusi desain berbentuk wire-
frame low ﬁdelity, kemudian pada iterasi berikutnya solusi
desain berbentuk wireframe high ﬁdelity, dan pada iterasi
terakhir solusi desain berbentuk prototype. Pada hasil akhir
dapat dilihat bahwa rancangan antarmuka dapat meningkatkan
kepuasan pengguna dan mengurangi masalah pada indikator-
indikator usability dengan menggunakan metode UCD.

DAFTAR PUSTAKA

[1] Pemerintah Indonesia, Undang-Undang Republik Indonesia Nomor 16

Tahun 1997 Tentang Statistik.

Jakarta: Sekretariat Negara, 1997.

[2] Badan Pusat Statistik, Petunjuk Teknis Aplikasi Rekomendasi Kegiatan

Statistik (Romantik) Online.

Jakarta: Badan Pusat Statistik, 2019.

[3] Pemerintah Indonesia, Peraturan Pemerintah Nomor 51 Tahun 1999

Tentang Penyelenggara Statistik.

Jakarta: Sekretariat Negara, 1999.

[4] N. Turner. (2011, 2) A guide to carrying out usability reviews. [Online].

Available: http://www.uxforthemasses.com/usability-reviews/

[5] Superadmin. (2012, 10) Questionnaire for user interface satisfaction
(quis). [Online]. Available: https://ext.eurocontrol.int/ehp/?q=node/1611
[6] W.-S. Tan, D. Liu, and R. Bishu, “Web evaluation: Heuristic evaluation
vs. user testing,” International Journal of Industrial Ergonomics, vol. 39,
no. 4, pp. 621–627, July 2009.

[7] M. Allen, L. M. Currie, S. Bakken, V. L. Patel, and J. J. Cimino,
“Heuristic evaluation of paper-based web pages: A simpliﬁed inspection

7 / 8

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

usability methodology,” Journal of Biomedical Informatics, vol. 39,
no. 4, pp. 412–423, August 2006.

[8] J. P. Chin, V. A. Diehl, and K. L. Norman, “Development of an instru-
ment measuring user satisfaction of the human-computer interface,” in
Proceedings of the SIGCHI Conference on Human Factors in Computing
Systems, ser. CHI ’88. Washington, D.C., USA: Association for
Computing Machinery, May 1988, pp. 213–218.

[9] G. W. Subekti, Thrust : Sistem Informasi Manajemen Proyek Berbasis
Jakarta:

Android Pada Badan Pusat Statistik Republik Indonesia.
Politeknek Statistika STIS, 2019.

[10] C. Abras, D. Maloney-krichmar, and J. Preece, “User-centered design,”
in In Bainbridge, W. Encyclopedia of Human-Computer Interaction.
Thousand Oaks: Sage Publications. Publications, 2004.

[11] Interaction Design Foundation. User centered design. [Online]. Avail-
able: https://www.interaction-design.org/literature/topics/user-centered-
design

[12] D. Murillo and J. Herrera, “Use of wireframes and mockup on the
redesign of a university website using the methodology user centered
design,” in Memorias del VII Congreso Iberoamericano de Telematica,
ser. CITA2015. Spain: Memorias del VII Congreso Iberoamericano de
Telematica, Jan 2015, pp. 239–242.

[13] H. Nallan and M. Jaiswal. (2019, 6) Ucd vs. design thinking. [Online].

Available: https://think.design/blog/ucd-vs-design-thinking/

[14] J.-Y. Mao, K. Vredenburg, P. W. Smith, and T. Carey, “The state of user-
centered design practice,” Commun. ACM, vol. 48, no. 3, pp. 105–109,
March 2005.

[15] C. Liang, W. Chou, Y. Hsu, and C.-C. Yang, “A user-centered design
approach to develop a web-based instructional resource system for
homeland education,” Knowledge Management and E-Learning: An
International Journal, vol. 1, pp. 67–80, 2009.

[16] C. M. Johnson, T. R. Johnson, and J. Zhang, “A user-centered framework
for redesigning health care interfaces,” Journal of Biomedical Informat-
ics, vol. 38, no. 1, pp. 75–87, February 2005.

[17] J. Nielsen and R. Molich, “Heuristic evaluation of user interfaces,” in
Proceedings of the SIGCHI Conference on Human Factors in Computing
Systems, ser. CHI ’90. New York, NY, USA: Association for Computing
Machinery, March 1990, pp. 249–256.
design

[18] Usability.gov. User-centered

[Online]. Available:

basics.

https://www.usability.gov/what-and-why/user-centered-design.html
[19] B. Shneiderman, “Designing for fun: How can we design user interfaces
to be more fun?” Interactions, vol. 11, no. 5, pp. 45–50, September 2004.
the ﬁght between ﬁgma and
adobe xd? [Online]. Available: https://uxdesign.cc/ﬁgma-vs-adobe-xd-
d3a624cb7885
[21] T. Gemayel.

(2019, 8) How to wireframe.

(2018, 12) Who wins

[Online]. Available:

[20] A. Park.

https://www.ﬁgma.com/blog/how-to-wireframe/

[22] R. S. Berry. (2020, 9) 11 inspiring mood board examples for your next
design project. [Online]. Available: https://maze.co/blog/mood-board-
examples/

[23] K. Cherry. (2020, 2) The color psychology of blue. [Online]. Available:

https://www.verywellmind.com/the-color-psychology-of-blue-2795815

[24] V. Verma. (2019, 8) 5 free design resources — illustrations. [Online].
Available: https://blog.prototypr.io/5-free-design-resources-illustrations-
5bf4959cc6e4

[26] C. Dulanga.

[25] V. Garcia. (2015, 11) The beginners guide to font awesome. [Online].
Available: https://www.ostraining.com/blog/general/font-awesome/
need

(2021,
be webpack experts.
frontend-developers-need-to-be-webpack-experts-32e734b6f04a

to
[Online]. Available: https://blog.bitsrc.io/why-

developers

3) Why

frontend

8 / 8

"
221710008,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Kajian Teknlogi Single Sign On 
Untuk Sistem Autentikasi Satu Pintu 
Studi Kasus: Politeknik Statistika STIS 
Setyo Dwi Saputra (221710008, 4SI1) 
Dosen Pembimbing: Ibnu Santoso, SST, MT 

Ringkasan—  Single  Sign  On  adalah  suatu  sistem  yang 
memungkinkan  pengguna  dapat  masuk  ke  berbagai  aplikasi 
dengan  menggunakan  satu  user  credential  (username  dan 
password). Penerepan SSO di lingkungan Polstat STIS diperlukan 
karena  terdapat  berbagai  macam  aplikasi  dengan  sistem 
autentikasi  yang  berbeda-beda.  SSO  juga  dapat  berguna  untuk 
memudahkan  pengembang  baru  ketika  akan  membangun 
aplikasinya sehingga dapat dipusatkan menjadi sistem autentikasi 
satu  pintu.  Polstat  STIS  memiliki  tiga  layanan  autentikasi  yang 
juga  digunakan  oleh  aplikasi  dan  layanan  lain  utnuk  keperluan 
autentikasi  mereka,  yakni  Layanan  SIPADU,  Google  Suite  for 
Education,  dan  Active  Directory.  Berdasarkan 
analisis 
permasalahan  dan  kebutuhan  sistem  kemudian  dibangun  SSO 
menggunakan CAS dan LDAP. Pembangunan SSO denggan basis 
LDAP dapat digunakan untuk lingkungan jaringan Polstat STIS. 
Sistem  SSO  kemudian  diuji  dengan  JMeter  dan  AB  didapatkan 
kesimpulan  bahwa  sistem  dapat  menerima  akses  bersamaan 
sampai  100  pengguna.  Namun  untuk  mengoptimalkan  kinerja, 
dapat  diberikan  pengaturan  jeda  minimal  100  ms.  Kemudian 
berdasarkan  kerangka  TOE,  disimpulkan  bahwa  lingkungan 
Polstat STIS dapat mengadopsi penerapan teknologi SSO.  

Kata  Kunci—  SSO 

(Central 
Authentication  Service),  LDAP  (Lightweight  Directory  Access 
Protocol), autentikasi, TOE Framework. 

(Single  Sign  On),  CAS 

I.  LATAR BELAKANG 

Dengan  pertumbuhan  teknologi  dan  internet,  bermacam 
layanan  telah  bekembang  sangat  pesat.  Layanan  tersebut 
termasuk World Wide Web, Mobile & Web App, Desktop App, 
File  Transfer  Protocol  (FTP),  sampai  Remote  Login. 
Manajemen  autentikasi  menjadi  konsekuensi  yang  harus 
diperhatikan,  yakni  seperti  pengamanan  username/ 
ID 
pengguna, kata sandi, dan sertifikat keamanan [5]. Kebutuhan 
akan  kemanan  data  dan  jaringan  diperlukan  untuk  memberi 
setiap  pengguna.  Namun  dengan 
kenyamanan  bagi 
beranekanya  aplikasi  dan  layanan,  kemampuan  pengguna 
dalam mengingat username/ ID dan kata sandi sangat terbatas. 
Di samping itu diperlukan manajemen autentikasi yang aman 
dan efektif, sehingga untuk mengatasi hal tersebut SSO dapat 
menjadi solusinya.  

Single  Sign  On  adalah  sebuah  mekanisme  yang  membuat 
pengguna hanya perlu mengingat satu username dan password 
yang  autentik  untuk  membuka  beberapa  layanan  sekaligus. 
SSO  memberi  layanan  autentikasi  satu  pintu,  hal  ini  dapat 
mengatasi  permasalahan  memori  pengguna  dan  masalah 
kemanan  [6].  Politeknik  Statistika  STIS  (Polstat  STIS) 
merupakan  lembga  pendidikan  yang  menggunakan  berbagai 

macam layanan. Layanan-layanan tersebut diakses oleh banyak 
pengguna  dengan  sistem  autentikasi  yang  berbeda  antara 
masing-masing  layanan.  Layanan  di  Polstat  STIS  antara  lain 
SIPADU,  e-mail  (menggunakan  G-Suite),  WiFi,  Aactive 
Directory,  dan  login  laboratorium  komputer.  Referensi  [4] 
menunjukkan  bahwa  SSO  mampu  diterapkan  di  lingkungan 
Polstat  STIS,  akan  tetapi  belum  keseluruhan  layanan  dapat  
terhubung.  

Penerapan Single Sign On berhubungan erat dengan jumlah 
pengguna dalam organisasi, jumlah aplikasi atau sistem  yang 
terhubung dengan sistem Single Sign On, dan spesifikasi server 
(agar  dapat  tersedia  untuk  semua  pengguna  dan  mencakup 
semua  aplikasi/sistem  sesuai  perancangannya).  Maka,  selain 
kebutuhan keamanan pengguna, isu tingkat performa menjadi 
kajian  penting  dalam  penerapan  Single  Sign  On  pada  suatu 
organisasi.  

Pada  penelitian  ini  akan  dilakukan  kajian  menggunakan 
server  asli/non-local  agar  dapat  diketahui  kemampuan  SSO 
yang  dikaji  dalam  menangani  permintaan  pengguna  terutama 
ketika  mengakses  halaman  autentikasi/login.  Penggunaan 
performance  test  untuk  menguji  kemampuan  sistem  dengan 
pengujian processing-time karena dalam keadaan nyata Polstat 
STIS  memiliki  pengguna  yang  cukup  banyak  sehingga 
memungkinkan dalam suatu waktu sistem diakses oleh banyak 
pengguna sekaligus. Maka diperlukan kajian seperti kebutuhan 
minimal sistem dan hasil performance testing. 

II.  TUJUAN PENELITIAN 

Tujuan dalam penelitian ini diuraikan dalam uraian sebagai 

berikut: 
a.  Menganalisis sistem autentikasi tiap-tiap layanan berbasis 

jaringan dan internet di Polstat STIS 

b.  Menemukan  kandidat  infrastuktur  SSO  sehingga  dapat 
diterapkan di semua layanan berbasis jaringan dan internet 
di Polstat STIS 

c.  Melakukan  percobaan  dari  kandidat  sistem  sesuai  dengan 
keadaan lingkungan  jaringan dan aplikasi di Polstat STIS 
d.  Memberikan  evaluasi  performance  test  terhadap  kajian 
penerapan Single Sign On untuk lingkungan Polstat STIS 

III. PENELITIAN TERKAIT 

Terdapat  penelitian  terkait  yang  juga  dilakukan  oleh 
tahun  2016  yaitu  Krisdianto.  Dalam 
mahasiswa  STIS 
penelitiannya, sudah berhasil melakukan percobaan penerapan 
SSO dengan menggunakan dummy app dan  dummy database 
[4].  Penggunaan LDAP service  dengan tambahan framework 

 1 / 8 

 
 
 
 
 
CAS lebih tepat digunakan untuk penerapan teknologi SSO di 
lingkungan Polstat STIS [5]. CAS dipakai karena lebih lengkap 
jika dibanding dengan OAuth. Hal ini sejalan dengan penelitian 
Anshorudin  [2]  yang  membandingkan  kinerja  CAS  dengan 
OAuth. Dalam penelitiannya, mengungkapkan jika CAS dapat 
menerima request lebih banyak daripada OAuth. Namun OAuth 
lebih tepat jika SSO yang dibangun hanya seputar layanan web, 
desktop,  dan  mobile  [2].    Dikarenakan  Polstat  STIS  juga 
memiliki  layanan  seperti  access  active  directory,  maka  CAS 
lebih tepat diterapkan. 

TABEL I 
PENELITIAN TERKAIT 

Penulis 
Publikasi 
Bima 
Sakti 
Krisdianto, STIS 

Judul 

Implementasi 
Teknologi Single Sign 
On (SSO) Di 
Lingkungan Sekolah 
Tinggi Ilmu Statistik 
(STIS) 

Perbandingan 
CAS 
Dan  OAuth  Dalam 
Single Sign On (SSO) 

Imam 
Anshorudin, 
Universitas 
Muhammadiyah 
Surakarta 

Analisis  Performansi 
Autentikasi 
Single 
Sign  On  pada  Web 
Menggunakan LDAP 

Futuh  Huilmi, 
dkk., 
Institut 
Teknologi 
Telkom 

Bangun 
Rancang 
Sistem 
Informasi 
Akademik  Berbasis 
Web  pada  Sekolah 
Dasar 
2 
Landungsari 

Negeri 

Satria 
Aldy 
Gumilar,  dkk., 
Universitas 
Muhammadiyah 
Malang 

Tertulis 

‘’Setelah melalui studi 
literatur, peneliti 
menyarankan agar CAS 
dapat dikembangkan lebih 
lanjut dan diterapkan di 
STIS.’’ 

‘’Peneliti telah menentukan 
database yang mudah, 
sederhana dan aman, yaitu 
LDAP. Selain bersifat open 
source, LDAP bekerja di 
jaringan SSL sehingga lebih 
terjamin keamanannya.’’ 

request 

(Halaman 91) 
‘’Dari  hasil  test  performa 
dapat  dilihat  bahwa  sistem 
mampu 
login 
CAS 
menangani 
jauh 
lebih  banyak  dari  sistem 
login  OAuth.’’ 
(Halaman 
10) 
Pengujian  performa  SSO 
dengan  CAS  dan  LDAP 
menggunakan 
pengujian 
load  test  melalui  Apache 
JMeter. 
TOE 
Penggunaan 
Framework  untuk  analisis 
kebutuhan sistem dan untuk 
tingkat 
menguji 
keterterimaan 
organisasi 
apabila    sistem  yang  dibuat 
akan diimplementasikan 

IV. METODE PENELITIAN   

1.  Studi Literatur 

Mencari  beberapa  referensi  dan  penelitian  terkait  agar 
memberikan  gambaran  bagaimana  sistem  akan  dibangun. 
Secara  garis  besar 
teknologi  yang  digunakan  adalah 
Lightweight  Directory  Access  Protocol  (LDAP)  dengan 
menggunakan  Framework  Central  Authentication    Service 
(CAS).  LDAP  biasa  digunakan  untuk  menyimpan  berbagai 
informasi  terpusat  yang  dapat  diakses  oleh  berbagai  macam 
mesin atau aplikasi dari jaringan. Penggunaan LDAP didalam 
sistem akan membuat pencarian informasi menjadi terintegrasi 
dan  sangat  mudah.  Sebagai  contoh,  LDAP  seringkali 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

digunakan untuk  menyimpan nama  pengguna dan sandi  yang 
terdapat di dalam sistem secara terpusat  [3].  

Gambar 1. Arsitektur CAS (Central Authentication Service) 

CAS memiliki basis Spring Boot dan menggunakan banyak 
aspek dari Spring Framework seperti Spring MVC dan Spring 
Webflow [1]. Arsitektur CAS memungkinkan pengguna untuk 
mengakses berbagai macam aplikasi yang berbeda protokolnya 
hanya  dengan  satu  autentikasi.  CAS    terdiri  dari  CAS  Client 
dan CAS Server. 

2.  Wawancara 
a.  Kepada perancang sebelumnya 

Dikarenakan  pada  beberapa  periode  sebelumnya,  sudah 
pernah dilakukan penelitian serupa, maka baiknya melakukan 
wawancara kepada peneliti sebelumnya. Langkah ini dilakukan 
agar  penelitian  ini  dapat  melanjutkan  tahapan  yang  dibuat 
sebelumnya 
tetapi  menyesuaikan  dengan  perkembangan 
kebutuhan dan teknologi yang ada.  

b.  Kepada Subject Matter dan pihak kampus 

Pada  wawancara  yang  dilakukan  kepada  pihak  subject 
matter, dikemukakan kebutuhan tentang sistem autentikasi satu 
pintu yang memungkinkan penerapan di berbagai jenis sistem 
baik  yang  sudah  ada  atau  yang  akan  dibangun.  Kemudian 
disampaiakan  mengenai  penggunaan  sumber  daya  yang  lebih 
efektif, dalam hal ini adalah server. Penggunaan sumber daya 
yang  lebih    efektif  dilakukan  karena  pada  penelitian  kali  ini 
diharapkan sistem dapat diterapkan pada server asli, sehingga 
ketika akan diimplementasikan pada produksi yang nyata akan 
ada pertimbangan yang dapat diambil.  

3.  Metode Analisis 

Pada  penelitian  kali 

ini  digunakan  Analisis  TOE 
Framework.  TOE  Framework  memiliki  tiga  elemen  yang 
digunakan  untuk  analisis  keterterimaan  adopsi 
inovasi 
teknologi pada suatu organisasi yakni, Teknologi, Organisasi, 
dan  Lingkungan.  Berikut  analisis  TOE  untuk  kasus  Polstat 
STIS. 

a.  Teknologi 
Polstat  STIS  memiliki  server  yang  stabil.  Karakteristik 
jaringan yang ada di Polstat STIS ada dua, yaitu LAN (Local 

 2 / 8 

 
 
 
 
 
 
 
 
 
 
Area Ntwork) dan WAN (Wide Area Network). Contoh layanan 
yang  menggunakan  LAN  adalah  Laboratorium  Komputer, 
WiFi, dan Active Directory. Sedangkan untuk WAN, layanan 
yang  masuk dalam  karakteristik ini adalah  layanan  yang bisa 
diakses pada jarak jauh seperti antar provinsi dan pulau. Contoh 
layanan yang termasuk dalam WAN adalah SIPADU, Layanan 
Google, dan layanan lain yang tersedia di web dengan domain 
stis.ac.id. 

b.  Organisasi 
Polstat  STIS  adalah  organisasi  dengan  anggota  yang  cukup 
banyak  dan  bervariasi.  Polstat  STIS  merupakan  organsisasi 
pendidikan yang memiliki struktur organisasi seperti organisasi 
pendidikan  lain.  Secara  umum,  struktur  pengguna  di  Polstat 
STIS terdiri dari dosen, mahasiswa, dan pegawai. Polstat STIS 
juga menamakan dirinya sebagai “IT Based Campus”. Layanan 
tersedia,  secara  umum  digunakan  untuk  proses 
yang 
perkuliahan baik ketika daring ataupun KBM normal. 

c.  Lingkungan 

4.  Analisis Sistem Berjalan 

Di Polstat STIS saat ini terdapat tiga sistem autentikasi yang 
digunakan  ntuk  keperluan  autentikasi  layanan  yang  berbeda. 
Masing-masing  layanan  autentikasi  memiliki  peraturan  user 
credential  yang  tidak  sama.  Pada  analisis  sistem  berjalan  ini 
akan dijelaskan mengenai proses bisnis layanan dan gambaran 
umum layanan autentikasi tersebut. 

a.  SIPADU  
Layanan autentikasi SIPADU  yang digunakan oleh aplikasi 
lain  menggunakan  teknologi  web  service.  Aplikasi  hanya 
mengambil  informasi  credential  user  dari  suatu  pengguana 
untuk  keperluan  autentikasi.  Apliakasi  tidak  perlu  terhubung 
langsung ke basis data SIPADU namun melewati web service 
terlebih  dahulu.  Contoh  penggunaan  aplikasi  yang  memakai 
autentikasi SIPADU adalah SIKOKO PKL59. Peramban akan 
memuat  windows  baru  untuk  memunculkan  halaman  login 
layanan SIPADU. 

Secara  sederhana,  penggunan  layanan  SIPADU  sebagai 
autentikasi  aplikasi  cara  kerjanya  mirip  dengan  SSO,  yakni 
mengecek  apakah  masih  ada  session  atau  tidak  ketika  awal 
mengakses aplikasi. Penggunaan layanan autentikasi SIPADU 
hanya  untuk  keperluan  login,  sementara  unttuk  pengubahan 
sandi  dan  melakukan  pencarian  informasi  mengenai  kegiatan 
akademik dan administrasi berada di web SIPADU. 

Gambar 2. Autentikasi Layanan SIPADU 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

b.  Layanan Google 
Polstat  STIS  menggunakan  layanan  Google  Suite 

for 
Education  untuk  proses  kegiatan  perkuliahan.  Penggunaan 
layanan  ini  lebih  dirasa  ketika  musim  pandemik  COIVD-19 
dengan dilakukannya Pembelajaran Jarak Jauh (PJJ). Layanan 
seperti Google Classroom digunakan untuk proses pemberian 
materi,  presensi,  dan  tugas.  Selain  itu  layanan  Google  yang 
rutin digunakan adalah e-mail dan Drive.  

Gambar 3. Autentikasi Layanan Google 

session 

Layanan  yang  terhubung  dengan  ini,  menggunakan  gmail 
sebagai autentikator aplikasi. Seperti halnya SIPADU, Google 
akan  mengecek  session  login  untuk  aplikasi,  namun  bedanya 
Google  memiliki  waktu 
lama  karena 
menggunakan pemberian izin autorisasi kepada suatu layanan. 
Biasanya  penggunaan  layanan  Google  tidak  perlu  melakukan 
login  lagi  selama  belum  logout.  HaloSTIS  merupakan  forum 
tanya  jawab  yang  juga  menggunakan  autentikasi  Google. 
Aturan yang digunakan untuk alamat e-mail mahasiswa adalah 
dosen  menggunakan 
untuk 
NIM@stis.ac.id 
nama@stis.ac.id (disesuaikan). 

lebih 

dan 

c.  Active Directory 
Layanan dengan autentikasi  Active Directory  menggunakan 
user  credential  username  dan  password.  User  Credential  ini 
diberikan  dan  di-generate  oleh  Tim  IT  kampus.  Seringkali 
pengguna baik mahasiswa maupun dosen tidak mengingat user 
credential untuk layanan autentikasi Active Directory. Hal ini 
disebabkan  karena 
tidak  ada  fitur  lupa  password  atau 
penggantian  password  sendiri.  Penggantian  password  hanya 
bisa  dilakukan  oleh  administrator.  Hal  ini  mengakibatkan 
layanan yang menggunakan autentikasi Active Directory jarang 
digunakan oleh pengguna. 
5.  Analisis Permasalahan 

Setelah  dilakukan  analisis  sistem  yang  sedang  bejalan  di 
ruang  lingkup  Polstat  STIS  dan  analisis  TOE  Framework, 

 3 / 8 

 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

waktu  akses  minimal,  waktu  akses  maksimal,  dan  troughput 
server  (kemampuan  server  untuk  menangani  permintaan  per 
satuan waktu).  

V.  KERANGKA PIKIR 

ditemukan  beberapa  permasalahan  terkait  kajian  penerapan 
SSO di Pollstat STIS.  
a.  Polstat  STIS  menamakan  dirinya  sebagai  “IT  Based 
Campus” sehingga semua aplikasi yang berbasis teknologi 
membutuhkan dukungan infrastruktur dari organisasi. 
b.  Layanan berbasis teknologi informasi di Polstat STIS masih 
terbagi  menjadi  tiga  layanan  autentikasi  dengan  masing-
masing sistem autentikasi memiliki aturan  credential user 
yang berbeda. 

c.  Karena  memiliki  sistem  autentikasi  yang  terpisah,  maka 
manajemen pengguna juga terpisah sehingga membutuhkan 
SDM yang lebih banyak untuk pengelolaanya. 

d.  Di samping dari sisi manajemen, sistem autentikasi terpisah 

juga membebani pengguna dalam manajemen akun. 

e.  Beban  ingatan  pengguna  bertambah  disebabkan  harus 

menghafal credential user  lebih dari satu. 

f.  Kecenderungan  untuk  menggunakan  isian  username  dan 
password yang sama apabila terdapat banyak sistem yang 
menggunakan  autentikasi,  apalagi  jika  terdapat  aplikasi 
yang  membuat  basis  datanya  sendiri.  Perilaku  ini  dapat 
menimbulkan celah keamanan bagi dirinya. 

g.  Diperlukan  sistem  satu  pintu  untuk  autentikasi  layanan 

yang ada. 

h.  Perbedaan  karakteristik  pengguna  dalam 

frekuensi 
penggunaan sistem menyebabkan aplikasi yang tidak sering 
digunakan lupa credential user-nya. Akibatnya, sistem yang 
dibuat menjadi tidak efektif. 

Gambar 4. Fishbone Diagram Analisis Permasalahan Sistem 

6.  Metode Evaluasi 

Pada  penelitian  ini  akan  dilakukan  dua  pengujian  secara 
umum.  Pertama  melakukan  pengujian  berdasarkan  Kerangka 
TOE yang dibuat. Pengujian ini dilakukan dengan pencocokan 
kerangka  TOE  dengan  hasil  perocbaan  untuk  mengetahui 
tingkat  penerimaan  organisasi  (dalam  hal  ini  adalah  Polstat 
STIS) untuk mengadopsi teknologi Single Sign On. Pengujian 
ini hanya pencocokan kebutuhan tugas seperti pengujian Black 
Box pada perancangan atau pembangunan sistem informasi. 

Pengujian  kedua  adalah  dengan  melakukan  pengujian 
performa  sistem  berdasarkan  spesifikasi  jaringan  yang  akan 
dijabarkan  di  bawah.  Pengujian  ini  dilakukan  dengan  test 
menggunakan  JMeter  dan  Apache  Bench.  Apache  Bench 
digunakan  untuk pengujian  secara umum  kepada server  yang 
dipakai.  Sementara  pengujian  menggunakan  JMeter  akan 
dilakukan  lebih  detail  seperti    dari  perbadingan  beban  server 
ketika  diakses  oleh  pengguna  (dilakukan  diferensiasi  jumlah 
pengguna  yang  mengakses  server  secara  bersamaan). 
Pengujian  JMeter  akan  dilakukan  perbadingan  pada  variabel 

Gambar 5. Kerangka Pikir Penelitian 

Penelitian ini muncul bermula karena permasalahan terkait 
sistem  autentikasi  yang  berbeda-beda  pada  masing-masing 
sistem  yang  ada  di  Polstat  STIS.  Permasalahan  dasar  ini 
mengacu  kepada  beban  ingatan  pengguna  untuk  mengingat 
sistem  autentikasi  yang  berbeda  untuk  setiap  layanan.  Untuk 
permasalahan  dasar  ini,  sebetulnya  sudah  ada  kajian  yang 
dilakukan  oleh  mahaiswa  STIS  beberapa  tahun  sebelumnya, 
namun  dari  kajian  yang  dilakukan  oleh  mahasiswa  terkait, 
kembali muncul permasalahan baru terkait dengan efektivitas 
dan kemudahan pengembangan sistem.  

VI. HASIL DAN PEMBAHASAN 

a.  Kandidat Sistem Usulan 

TABEL II 
KANDIDAT SISTEM USULAN 

No 

Kandidat  

1  AD, 

SIPADU, 
dan Email 
terpusat 

Deskripsi 
Email tidak 
menggunakan 
Google Suite, 
namun diganti 
dengan layanan 
lain yang lebih 

Kelebihan 
Tiga layanan 
utama 
menggunakan 
satu credential 
user 

Kekurangan 
Dilakukan 
pemmbangun
an yang lebih 
lama karena 
menggunaka
n layanan 
email baru. 

 4 / 8 

 
 
 
 
 
 
 
 
 
 
fleksibel seperti 
Zimbra 

2  AD & 

SIPADU 
terpusat dan 
E-mail tetap 
Google Suite 

Layanan e-mail 
Google Suite 
tetap terpakai 
dan pemusatan 
sistem 
autentikasi pada 
AD & SIPADU 

3  SIPADU & 

Google Suite 
terpusat dan 
AD terpisah 

Login SIPADU 
dan Aplikasi lain 
menggunakan 
Google (OAuth 
 login with 
Google) dan AD 
terpisah 

Jika layanan 
Google Suite 
tidak dipakai 
maka akan 
muncul 
masalah baru 
karena sistem 
ujian dan 
perkuliahan 
di masa 
pandemic 
banyak 
memakai 
layanan suite.  

Ketiga 
layanan 
belum dapat 
terhubung 

Terpisahnya 
AD tidak 
terlalu 
berguna dan 
web sevice 
SIPADU 
tidak 
terpakai.  

Penggunaan 
layanan 
Google untuk 
memudahkan 
PJJ masih 
tersedia dan 
pemusatan AD 
& SIPADU 
(untuk 
SIPADU 
tinggal 
membuat akun 
SSO yang 
kemudian 
disambungkan 
dengan DB 
SIPADU 

Layanan 
OAuth 
termasuk 
layanan SSO 
yang mudah 
penerapannya 

Tabel di atas merupakan table kandidat sistem usulan yang 
dibuat  sesuai  dengan  permasalahan  yang  ada.  Kemudian  dari 
kandidat tersebut dilakukan Analisis Kelayakan untuk memilih 
salah satu dari tiga kandidat yang dijabarkan. 

-  Analisis Kelayakan Sistem (Feasibility) 

TABEL III 
ANALISIS KELAYAKAN 

Unsur 

Ting
kat 

Bobo
t 

Ekonomi 

Teknis 

Operasio
nal 

Waktu 

3 

4 

4 

2 

0,230
76 

0,307
69 

0,307
69 

0,153
84 

Rating 

Skor 

1 

2 

3 

1 

2 

3 

80 

80 

80 

18,46 

18,46 

18,46 

60 

80 

70 

18,46 

24,62 

21,54 

70 

75 

70 

21,54 

23,08 

21,54 

45 

60 

70 

6,92 

9,23 

10,77 

13 

1 

65,38 

75,38 

72,31 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Analisis  Kelayakan  digunakan  untuk  mengetahui  tingkat 
kelayakan  kandidat  sistem  usulan  yang  telah  dijelaskan 
sebelumnya  untuk  dipilih  berdasarkan  empat  unsur  yaitu 
Ekonomi, Teknis, Operasional, dan Waktu. Dari keempat unsur 
tersebut  kemudian  ditentukan  tingkat  (kepentingan/urgensi) 
untuk setiap unsur dengan rentang 1-5 (semakin besar tingkat 
maka  semakin  diutamakan).  Kemudian  dari  tingkat  tersebut 
ditemukan  bobot  untuk  penghitungan  skor  nantinya.  Ketiga 
kandidat  sistem  tersebut  dianalisis  berdasarkan  empat  unsur 
dengan  memberi  rating  0-100  (semakin  besar  rating  maka 
semakin  layak).  Dari  perolehan  rating,  kemudian  dikalikan 
dengan bobot sehingga menghasilkan skor total.  

Pada  unsur  ekonomi  digunakan  nilai  rating  yang  sama 
yakni 80 karena penggunaan sumber daya adalah sama, yakni 
dua  VPS.  Total  skor  untuk  masing-masing  kandidat 
mengindikasikan tingkat kelayakan sistem dibuat berdasarkan 
sumber  daya  dan  keadaan  saat  penelitian.  Terpilih  kandidat 
nomor  2  dengan  skor  75,38  yakni  AD  dan  Sipadu  dibuat 
terpusat kemudian layanan google tetap digunakan mengingat 
lingkungan  Polstat  STIS  masih 
beberapa  kegiatan  di 
menggunakan  layanannya  (seperti  e-mail,  drive,  HaloSTIS, 
dsb..) 
b.  Rancangan Arsitektur 

Sistem  SSO  yang  dilakukan  pada  penelitian  ini  dibuat 
dengan  menggunakan  framework  CAS  dengan  dasar  Spring 
Boot  dan  menggunakan  LDAP  sebagai  DBMS.  Pada  proses 
deployment  digunakan  dua  server  terpisah  untuk  LDAP  dan 
CAS.  

Pengguna  mengakses  peramban  yang 

terhubung  ke 
internet.  Kemudian  pada  peramban  dituliskan  alamat  CAS 
untuk  mengakses  portal  SSO.  CAS  memverifikasi  pengguna 
apakah  terdaftar  di  LDAP  atau  belum.  Kemudian  setelah 
terverifikasi, CAS akan mengarahkan ke aplikasi yang dituju, 
misalnya SIPADU. 

Berikut merupakan gambaran rancangan arsitektur sistem 

SSO di mana fokus pada penelitian ini adalah pada box SSO. 

Gambar 6. Rancangan Arsistektur 

 5 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
c.  Proses Bisnis Sistem 

d.  Implementasi 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Dari tiga kandidat tersebut terpilih kandidat kedua, yakni 
AD & SIPADU terpusat  dan Google Suite (e-mail dan layanan 
yang 
termasuk  di  dalamnya)  berdiri  sendiri.  Berikut 
digambarkan Proses Bisnis Sitem Usulan.  

Gambar 7. Proses Bisnis Sistem Usulan 

SSO  memisahkan  kegiatan  autentikasi  dan  autorisasi. 
Autentikasi  adalah  proses  verifikasi  apakah  pengguna  dapat 
masuk ke dalam suatu sistem atau tidak. Sedangkan autorisasi 
adalah  suatu  proses  lebih  detail  mengenai  apakah  pengguna 
memiliki  izin  untuk  mengakses  fitur  dalam  aplikasi  tersebut. 
Autentikasi  berada  sebelum  autorisasi,  jadi  pengguna  akan 
melakukan  autentikasi  terlebih  dahulu  sebelum  memperoleh 
autorisasi.  Dalam  pengembangannya,  CAS  menyediakan  dua 
proses tersebut. Proses autentikasi dimulai pada saat pengguna 
mengkases portal SSO. Ketika pengguna mengisikan username 
dan  password  kemudian  mengirimkannya  ke  sistem  untuk 
dilakukan verifikasi berarti proses autentikasi sedang berjalan. 
Apabila  pengguna  terverifikasi  oleh  sistem,  kemudian  proses 
autorisasi  berlangsung  di  dalam  sistem.  Untuk  kasus  SSO, 
autorisasi  berada  pada  apakah  pengguna  memiliki  izin  untuk 
mengakses  layanan  yang  terhubung  dengan  SSO.  Di  dalam 
CAS,  kegiatan  autorisasi  ini  berada  ketika  pengguna  berhasil 
masuk dan mengakses halaman layanan/services. 

-  Rancangan Database 
Sistem yang dibuat bertujuan untuk mengakomodasi kandidat 
usulan  sistem  yang  terpilih  (usulan  sistem  kedua).  Pada 
penelitian  ini  tidak  digunakan  RDBMS  seperti  pembangunan 
aplikasi  yang  biasa  digunakan,  namun  menggunakan  LDAP 
(Lightweight  Directory  Access  Protocol).  Pengelolaan  file 
menggunakan  format  ekstensi  .ldif.  Rules  yang  digunakan  di 
LDAP  adalah  menggunakan  domain  name  sebagai  base  DN 
(distinguished  name).  Susunan  hirarki  pada  LDAP  disebut 
dengan DIT (Directory Information Tree). Berikut rancangan 
susunan DIT untuk LDAP-nya. Deployment LDAP dilakukan 
di Server A (kuncimasuk.com  IP:156.67.220.243). Berikut 
susunan DIT untuk LDAP server. 

Gambar 8. Directory Information Tree stisldap.com 

Setelah LDAP dibuat, kemudian digunakan LDAP Account 
Manager  (LAM)  untuk  memudahkan  manajemen  pengguna. 
LDAP  ini  digunakan  oleh  AD  (dicontohkan  dengan  login  ke 
computer) dan CAS. 

-  Melakukan  Percobaan  Penggunaan  LDAP  Client  ke 

pGina (login AD) 

Implementasi  untuk  AD  dapat  dicontohkan  dengan 
menggunakan 
login  windows  dengan  pGina.  pGina 
membutuhkan  alamat  IP  atau  nama  host  dan  domain  untuk 
membaca LDAP yang dipakai. Percobaan ini dilakukan dengan 
membuat VirtualMachine Windows 7 di remotelab STIS.  

Gambar 9. Contoh penggunaan LDAP untuk autentikasi di lab komputer 

-  Membangun SSO dengan CAS  
CAS dibuat dengan mengambil data LDAP untuk keperluan 
autentikasi login. CAS dibangun dengan menggunakan gradle. 
Gradle  digunakan  agar  lebih  efisien  dibandingkan  menyusun 
mulai  dari  awal  menggunakan  file  pom.xml.  Pengembang 
hanya  perlu  menambahkan  dependency  yang  dipelukan  dan 

 6 / 8 

 
 
 
 
 
 
 
 
 
 
mengaturnya  di 
file  build.gradle.  Kemudian  dilakukan 
pengaturan server dan integrasi ke LDAP di file cas.properties. 
CAS memerlukan SSL agar dapat diakses oleh pengguna.  

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pembangunan 

CAS 
(septette.cartjonye.com  IP:162.55.42.212). 
dipaparkan bentuk SSO menggunakan CAS dan LDAP. 

dilakukan 

Server 

di 

B 
Berikut 

Gambar 14. Halaman Panel berisi informasi CAS 

Gambar 10. Contoh penggunaan LDAP untuk autentikasi di lab komputer 
Proses  build  dan  konfigurasi  dinilai  berhasil  jika  dapat 
memunculkan keluaran seperti gambar tersebut. 

Gambar 11. Pembuatan Tiket ketika pengguna akses halaman CAS 
Log pada server dapat menginformasikan tentang IP Pengguna 
dan  tiket  apa  yang  sedang  dibuat  untuk  mengetahui  kegiatan 
pengguna ketika akses ke CAS. 

Gambar 15. Halaman Panel berisi layanan yang terhubung dengan CAS 

Gambar 16. Penghapusan Tiket ketika ketika pengguna logout 

e.  Hasil Pengujian Performa 
Penelitian  ini  menggunakan  dua  buah  server  yakni  Server  A 
(kuncimasuk.comIP:  156.67.220.243)  dan  Server  B 
(septette.cartjonye.com  IP:162.55.42.212).  Pembangunan 
LDAP di Server A dan CAS di Server B. 

-  AB Test 
Pengujian  ini  ditujukan  untuk  mengetahui  sejauh  mana 
server dalam menangani request dari pengguna. Pengujian AB 
Test dilakukan pada Server B karena di sini proses permintaan 
terjadi paling besar yakni saat autentikasi CAS. 

TABEL IV 
HASIL PENGUJIAN AB TEST PADA SERVER B 

Connect 
Processing 
Waiting 
Total 

min 
156 
157 
156 
313 

mean 
162 
143 
165 
406 

Std. dev 
44,8 
269,1 
34,0 
272,8 

median 
161 
164 
161 
325 

max 
1162 
1562 
1044 
1726 

Gambar 12. Halaman Login CAS 

Gambar 13. Pembuatan Tiket ketika ketika pengguna login 

 7 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Gambar 11. Grafik Waktu Request Max (ms) Server B tiap jumlah pengguna 

Grafik  tersebut  menggambarkan  jumlah  pengguna 
optimum  yang  memungkinkan  server  bekerja  dengan 
maksimal.  Pada  jumlah  pengguna  90  %  ke  95%  terjadi 
kenaikan 869 ms atau 0,8 detik. Walaupun masih di bawah 1 
detik  atau  1000  ms,  namun  dari  grafik  tersebut  dapat 
disimpulkan bahwa jumla pengguna optimum berada di angka 
90 % dari 1000 yakni 900 pengguna 

JMeter 

- 
Pengujian  JMeter  dilakukan  hanya  kepada  CAS  yakni 
Server  B.  Pengujian  dilakukan  dua  kali  yaitu  menggunakan 
Ramp  Time  nol  dan  sepuluh.  Berikut  hasil  pengujian 
menggunakan JMeter. 

TABEL V 
HASIL PENGUJIAN JMETER RAMP = 0 

Jumlah pengguna 

5 user 
10 user 
25 user 
50 user 
100 user 

Waktu respon 

Min 
2553 
2755 
2160 
2144 
2454 

Max 
3707 
17110 
14697 
32649 
77537 

Ramp  time  =  0  artinya  sejumlah  pengguna  mengakses 
server secar bersamaan tanpa ada jeda satu milisekon pun. Dari 
table di atas terlihat bahwa semakin banyak jumlah pengguna 
yang  mengkases  server  secara  bersamaan  tanpa  waktu  jeda 
maka waktu respon cenderung meningkat. 

TABEL VI 
HASIL PENGUJIAN JMETER RAMP = 10 

Jumlah pengguna 

5 user 
10 user 
25 user 
50 user 
100 user 

Waktu respon 

Max 
3707 
34315 
41085 
69004 
74939 

Min 
2389 
2566 
2264 
2308 
2180 

Untuk  kasus  ramp  =  10  waktu  request  minimal  setiap 
penambahan  jumlah  pengguna  justru  semakin  menurun.  Ini 
disebabkan  karena  yang  awalnya  missal  100  pengguna 
mengakses  bersama  menjadi  10  pengguna  akan  mengakses 
bersama  setiap  1  detik  (Jumlah  pengguna/ramp).  Sementara 
untuk waktu request maksimal cenderung meningkat. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

VII. 

PENUTUP 

a.  Kesimpulan 

Berdasarkan  penelitian  yang 

telah  dilakukan  dapat 
disimpulkan  bahwa  peneitian  ini  dapat  mencapai  tujuan 
penelitian utama yaitu melakukan kajian penerapan Single Sign 
On untuk lingkungan Polstat STIS.  
a.  Setelah dilakukan analisis dan permaslahan ditemukan satu 
kandidat  SSO  dengan  melalui  Uji  Kelayakan  yaitu 
pembuatan  SSO  dengan  SIPADU  dan  AD  digabung  serta 
untuk  Layanan  Google  masih  tetap  dipakai.  Dari  analisis 
tersebut kemudian dihasilkan kesimpulan untuk melakukan 
percobaan  pembangunan  sistem  SSO  menggunakan  CAS 
dan berbasis LDAP. 

b.  Percobaan  pembangunan  CAS  dan  LDAP  sudah  dapat 
dilakukan  dengan  mengkaji  kebutuhan  sistem  dan 
menyesuaikan  dengan  keadaan  Polstat  STIS.  Penerapan 
LDAP untuk  AD dilakukan dengan percobaan autentikasi 
masuk  ke  computer  lab  dan  hasilnya  berhasil.  Kemudian 
LDAP  digunakan  untuk  autentikasi  dan  manajemen 
pengguna yang nantinya akan digunakan oleh aplikasi lain 
seperti  SIPADU.  CAS  dibuat  halaman  portal  untuk 
memudahkan  proses  autentikasi  SSO  dan  penggunaan 
layanan yang terikat dengannya. 

c.  Pengujian 

terhadap  pembangunan  CAS  dan  LDAP 
dilakukan  dengan  dua  cara  yaitu  pengujian  performa  dan 
menggunakan  kerangka  TOE.  Dari  pengujian  performa 
didapatkan  hasil  bahwa  untuk  memaksimalkan  sistem, 
pengguna  harus  diberi  jeda  dalam  mengakses  halaman 
secara  bersamaan.  Namun  jika  tidak  diberi  jeda  tidak 
masalah, hanya saja waktu respon akan sedikit lebih lama. 

b.  Saran 

Berikut beberapa saran yang dapat digunakan untuk dasar 

penelitian atau kajian berikutnya: 
a.  Mengimplementasikan kajian SSO menggunakan CAS dan 
LDAP  di  lingkungan  jaringan  Polstat  STIS  yang  asli 
bersamaan  dengan  dilakukan  pengujian  lebih  lengkap 
seperti koneksi WiFi. 

b.  Membangun 

pengelolaan  CAS 

keperluan 
penambahan  layanan  kedalam  SSO  agar  dapat  dilakukan 
secara dinamis. 

untuk 

DAFTAR PUSTAKA 
[1]  apereo.github.io  (2020,  10)  CAS  –  Architecture  [Online].  Available: 

https://apereo.github.io/cas/6.2.x/planning/Architecture.html 

[2]  Cartealy. Linux Networking. Jakarta: Jasakom, 2013 
[3]  Imam. Perbandingan CAS dan  OAuth dalam Single Sign On. Surakarta: 

Universitas Muhammadiyah Surakarta, Oktober 2016 

[4]  Krisdianto, Bima Sakti. Implementasi Teknologi Single Sign On (SSO) di 
Lingkungan  Sekolah  Tinggi  Ilmu  Statistik  (STIS).  [Skripsi].  Jakarta: 
Sekolah Tinggi Ilmu Statistik, September 2016 

[5]  Nobayashi,  Daiki,  dkk..  Development  of  Single  Sign  On  Sistem  with 
Hardware Token and Key Management Server, Vol.E92-D No.5: IEICE, 
F[12009 

[6]  Nugroho, Priyo Puji dan Purnama, Endang.  Pengembangan Model Single 

Sign On untuk Layanan Internet dan Proxy IPB: IPB, 2012 

 8 / 8 

 
 
 
 
 
 
 
 
 
 
"
221710003,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pembangunan Web National Quality Assurance 
Framework (NQAF) di Badan Pusat Statistik 

Satrio Waskitho (221710003, 4SI1) 
Dosen Pembimbing: Dr. Margaretha Ari Anggorowati, M.T. 

Ringkasan—  National  Quality  Assurance  Framework  (NQAF) 
merupakan  pedoman  atau  instrumen  yang  digunakan  untuk 
menjamin  kepercayaan  dan  kualitas  produk  statistik,  serta 
meyakinkan bahwa  official  statistics  dapat  tetap  relevan.  NQAF 
menjadi  wujud  implementasi  dari  penjaminan  kualitas  (quality 
assurance,  QA)  pada  Badan  Pusat  Statistik  (BPS)  dan  instansi 
terkait  yang  terlibat  di  dalam  Sistem  Statistik  Nasional.  BPS 
diharapkan  dapat  berperan  dalam  pelaksanaan  penjaminan 
kualitas terhadap kegiatan produksi data statistik yang dilakukan 
kementerian dan lembaga lain. Untuk itu diperlukan web NQAF 
terkait 
yang  dapat  menjadi 
penjaminan  kualitas  serta  media  terintegrasinya  instrumen-
instrumen  penjaminan  kualitas  yang  dimiliki  BPS.  Pada 
penelitian 
ini,  pembangunan  web  menerapkan  System 
Development Life Cycle (SDLC) metode Agile dengan pendekatan 
Extreme  Programming  (XP)  dan  pengujian  web  menggunakan 
Black  Box  Testing  dan  System  Usability  Scale    (SUS).  Hasil  dari 
penelitian 
informasi  tentang 
penjaminan kualitas pada Sistem Statistik Nasional melalui web 
NQAF yang dapat diakses oleh internal dan eksternal BPS serta 
instrumen penjaminan kualitas yang terintegrasi dan terstruktur. 
Kata  Kunci—  penjaminan  kualitas,  NQAF,  official  statistics, 

ini  diharapkan  dapat  tersaji 

tempat  penyajian 

informasi 

Sistem Statistik Nasional 

I.  LATAR BELAKANG 

Badan  Pusat  Statistik 

(BPS)  merupakan  Lembaga 
Pemerintah Non-Kementerian (LPNK) yang mempunyai tugas 
dan  tanggung  jawab  untuk  menyediakan  data  statistik  yang 
andal  dan  terpercaya  sebagaimana  amanat  Undang-undang 
Nomor  16  Tahun  1997  tentang  Statistik.  BPS  juga  memiliki 
kewajiban  untuk  memberikan  jaminan  kepada  pengguna  data 
dengan memastikan produk statistik yang dihasilkannya telah 
sesuai  dengan  kaidah  statistik  dan  memenuhi  persyaratan 
dalam  proses  pengolahannya.  Dengan  begitu,  pengguna  data 
dapat  sepenuhnya  meyakini  bahwa  data  maupun  informasi 
yang mereka dapatkan bisa dimanfaatkan dengan tepat dalam 
berbagai kebutuhan mereka [1]. 

Kredibilitas  publik  terhadap  statistik  nasional  memiliki 
peranan  penting  dalam  informasi  statistik,  di  mana  informasi 
yang  tidak  dapat  dipercaya  merupakan  informasi  yang  tidak 
berguna [2]. BPS bertekad dalam visinya, pada periode tahun 
2020 sampai 2024, menjadi penyedia data statistik berkualitas 
untuk Indonesia maju. Guna mewujudkan visi tersebut,  salah 
satu  dari  empat  misi  BPS  adalah  menyediakan  statistik 
berkualitas yang berstandar nasional dan internasional. Upaya 
tersebut  semakin  diperkuat  dengan  penerapan  penjaminan 
kualitas  di  BPS  yang  menjadi  bagian  terpenting  di  dalam 
kegiatan  statistik  yang  mengarah  pada  perkembangan  Sistem 
Statistik Nasional (National Statistical System) di Indonesia. 

Penjaminan kualitas (quality assurance) merupakan bagian 
dari  manajemen  kualitas  yang  difokuskan  pada  pemberian 
kepercayaan  bahwa  persyaratan  kualitas  akan  terpenuhi  [3]. 
Penjaminan  kualitas  pada  Sistem  Statistik  Nasional  dapat 
diwujudkan melalui implementasi National Quality Assurance 
Framework  (NQAF)  yang  melibatkan  National  Statistical 
Office  (NSO),  dalam  penelitian  ini  adalah  BPS,  dan  instansi 
terkait yang terlibat di dalam Sistem Statistik Nasional [4]. Hal 
tersebut  merupakan  salah  satu  rekomendasi  dari  lima  inti 
Rekomendasi Perserikatan Bangsa-Bangsa (PBB) berdasarkan 
Prinsip  Fundamental  Official  Statistics.  Meskipun 
tidak 
disebutkan secara eksplisit di dalam sepuluh prinsip tersebut, 
kualitas  statistik  dan  manajemen  kualitas  memiliki  perhatian 
dan  penekanan  khusus  terhadap  produk  statistik  melalui 
pelaksanaan NQAF. 

NQAF merupakan pedoman atau instrumen untuk menjamin 
kepercayaan  dan  kualitas,  serta  guna  meyakinkan  bahwa 
official  statistics  tetap  relevan  [4].  Sebagai  lembaga  yang 
menghasilkan official statistics di Indonesia, BPS memiliki isu 
penting terkait penjaminan kualitas di dalam proses bisnisnya. 
Banyak  pihak  mempertanyakan  terkait  kualitas  data  maupun 
informasi yang dipublikasikan oleh BPS, sehingga BPS perlu 
untuk  membangun  penjamin  kualitas  data  statistik  yang 
dihasilkannya.  BPS 
telah  mulai  mengimplementasikan 
penjaminan  kualitas  pada  tahun  2015.  Penerapan  kegiatan 
tersebut  terus  dikembangkan  oleh  BPS  sehingga  penjaminan 
kualitas yang dilaksanakan dapat sesuai dan memenuhi standar 
internasional.  Ke  depannya,  BPS  diharapkan  berperan  untuk 
dapat  melaksanakan  proses  penjaminan  kualitas  terhadap 
kegiatan  sensus  dan  survei  yang  dilakukan  oleh  kementerian 
dan lembaga lain. 

Sistem  yang  tersedia  di  BPS  dalam  proses  penjaminan 
kualitas  adalah  Sistem 
Informasi  Quality  Assurance 
Framework  (SIQAF).  BPS  menghadapi  beberapa  kendala 
khususnya  terkait  dengan  sistem  yang  semestinya  dapat 
diimplementasikan  di  dalam  proses  kegiatan  penjaminan 
kualitas dan peran barunya untuk melakukan asesmen kualitas 
kegiatan statistik terhadap kementerian dan lembaga lain. Salah 
satu kendalanya adalah BPS belum memiliki web yang secara 
khusus  dapat  diakses  oleh  seluruh  instansi  yang  terlibat  di 
dalam  Sistem  Statistik  Nasional  terkait  dengan  asesmen 
penjaminan  kualitas  yang  akan  dilakukan  oleh  BPS.  Web 
tersebut harus dapat menunjang semua tools yang dimiliki BPS 
dalam melakukan asesmen terhadap kementerian dan lembaga. 
Integrasi  seluruh  tools  penjaminan  kualitas  merupakan  salah 
satu cara yang dapat dilakukan untuk melakukan peningkatan 
kualitas di dalam lingkup penghasil data official statistics [5]. 
Selain  itu,  informasi-informasi  yang  terkait  dengan  NQAF 

 1 / 8 

 
 
 
 
harus terdapat di dalam web sehingga dapat membantu proses 
pelaksanaan  dan  diseminasi  dari  hasil  asesmen  penjaminan 
kualitas yang dilaksanakan BPS. Informasi yang dibutuhkan di 
antaranya  pedoman,  publikasi/laporan,  metodologi,  waktu 
pelaksanaan, dan kegiatan yang seluruhnya tercakup di dalam 
NQAF. 

telah 

yang 

Berdasarkan 

permasalahan 

disebutkan 
sebelumnya,  BPS  memerlukan  suatu  web,  yang  kemudian 
dapat  disebut  web  National  Quality  Assurance  Framework 
(NQAF),  yang  akan  menjadi  media  terintegrasinya  seluruh 
tools yang dimiliki BPS dalam melakukan penjaminan kualitas. 
Web  NQAF  diharapkan  dapat  digunakan  sebagai  tempat 
penyajian  informasi  yang  mencakup  seluruh  kegiatan  yang 
berkaitan  dengan  penjaminan  kualitas  di  lingkup  Sistem 
Statistik Nasional. Dengan demikian, dari web ini diharapkan 
dapat  tersaji  informasi  tentang  NQAF  yang  dapat  diakses, 
terintegrasi, dan terstruktur.  

II.  TUJUAN PENELITIAN 

Penelitian  ini  bertujuan  untuk  membangun  web  National 
Quality  Assurance  Framework  (NQAF)  BPS  sebagai  media 
yang  dapat  menyajikan  dan  menyediakan  informasi  terkait 
pelaksanaan  kegiatan  penjaminan  kualitas  dalam  lingkup 
Sistem Statistik Nasional. Selain itu, pembangunan web NQAF 
juga  bertujuan  untuk  mengintegrasikan  instrumen-instrumen 
penjaminan kualitas yang telah ada beserta penyajian laporan 
terkait hasil asesmen pada tiap instrumen untuk dapat diakses 
dan  digunakan  oleh  BPS  serta  instansi  lain  yang  terlibat  di 
dalam Sistem Statistik Nasional.  

III. PENELITIAN TERKAIT 

Terdapat  beberapa  penelitian  yang  sebelumnya 

telah 
dilakukan  oleh  para  peneliti  terkait  dengan  sistem  informasi 
yang  akan  dibangun.  Penelitian  Keret  dan  Eminkahyagi 
menghasilkan  aplikasi  quality  assurance  framework  yang 
menekankan  bahwa  penjaminan  kualitas  yang  hanya 
menggunakan kuesioner dalam proses asesmennya tidak akan 
cukup  [6].  Penjaminan  kualitas  memerlukan  proses  ulasan 
secara  komprehensif  yang  dapat  mencakup  ulasan  pada  tiap 
proses produksi statistik. 

Turkish Statistical (TurkStat) merupakan salah satu NSO di 
dunia yang telah menerapkan quality framework pada  official 
statistics yang disebut TurkStat quality assurance framework. 
Dalam  publikasinya disebutkan,  laporan  kualitas  dari  seluruh 
produk  statistik  disiapkan  secara  berkala  oleh  TurkStat. 
Penilaian  internal  secara  self-assessment  dan  laporan  kualitas 
dari produk statistik digunakan untuk memonitor kualitas dari 
seluruh tahap pada proses produksi statistik. Laporan kualitas 
berbasis  prinsip  kualitas 
internasional  disiapkan  dan 
dipublikasikan pada situs web TurkStat untuk dapat digunakan 
oleh masyarakat umum [7]. 

Sementara  itu,  Mukund  dalam  patennya  menyebutkan, 
kegagalan  untuk  menjadwalkan,  melakukan,  dan  melaporkan 
hal-hal  yang  berkaitan  dengan  informasi  penjaminan  standar 
proses bisnis dapat mengakibatkan tertundanya operasi sistem, 
pemeliharaan  tambahan,  penambahan  biaya,  dan  munculnya 
kasus  hukum  perdata  maupun  pidana  [8].  Berdasarkan 
penelitian tersebut, tercapainya penjaminan kualitas yang baik 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

dapat diwujudkan dengan memerhatikan asesmen di tiap proses 
produksi suatu data maupun informasi dan memastikan segala 
informasi  terkait  penjaminan  kualitas  dapat  dikelola  serta 
dilaporkan  kepada  pengguna  guna  meminimalisasi  dampak 
kerugian yang bisa terjadi. 

IV. METODE PENELITIAN  
Pada penelitian ini, proses pembangunan sistem menerapkan 
System  Development  Life  Cycle  (SDLC)  metode  Agile  [9] 
dengan  pendekatan  Extreme  Programming  (XP)  [10].  Agile 
merupakan salah satu model siklus pengembangan sistem yang 
mengadopsi pengguna sebagai bagian dari tim pengembangan 
dan  menerapkan  teknik  incremental  (bertingkat),  yang  mana 
tiap  tingkatannya  dikembangkan  melalui  proses  iteration 
(pengulangan). 

Gambar 1. Alur pengembangan Extreme Programming (XP) 

Extreme  programming  terdiri  dari  enam  fase,  yaitu  fase 
eksplorasi,  perencanaan,  iterasi,  produksi,  pemeliharaan,  dan 
akhir. Alur pengembangan extreme programming dapat dilihat 
pada  gambar  1.  Pada  fase  eksplorasi,  peneliti  menulis  user 
stories yang dibutuhkan oleh subject matters. Tiap user stories 
mendeskripsikan fitur yang perlu ada di dalam web. Di waktu 
yang  sama,  peneliti  mulai  membiasakan  untuk  menggunakan 
tools  dan 
library  yang  akan  digunakan  pada  proses 
pembangunan web. Fase eksplorasi memakan waktu beberapa 
minggu sejak penelitian mulai dilaksanakan.  

Pada  fase  perencanaan,  peneliti  mengelompokkan  user 
stories  dan  diurutkan  berdasarkan  prioritas  yang  harus 
dikerjakan  terlebih  dahulu.  Beberapa  user  stories    yang  telah 
diurutkan akan dipilih untuk dikerjakan dalam satu iterasi pada 
fase  selanjutnya.  User  stories  tersebut  dipilih  berdasarkan 
waktu pengerjaan yang memungkinkan dalam satu iterasi, yaitu 
dua sampai tiga minggu.  

Pada  fase  iterasi,  tiap  iterasinya  memuat  fase  tradisional 
pengembangan perangkat lunak, yaitu analisis, desain, coding 
dan testing. Tiap iterasi dilakukan review secara berkelanjutan 
baik dilakukan oleh dosen pembimbing, pegawai Subdirektorat 
Konsistensi  Statistik  BPS,  maupun  perwakilan  dari  Sistem 
Informasi  Statistik  (SIS)  BPS.  Pengujian  akan  memberikan 
umpan balik sebelum dapat diteruskan ke fase selanjutnya.  

Fase  produksi  merupakan  fase  ketika  tiap  iterasi  dianggap 
telah  selesai  dan  dirilis  dalam  skala  kecil.  Setelah  fase  ini, 

 2 / 8 

 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

peneliti  dapat  kembali  ke  tahap  perencanaan  untuk  memulai 
kembali tahap iterasi bila masih terdapat user stories. 

Pada fase pemeliharaan, pembangunan web telah masuk ke 
tahap production tetapi masih dimungkinkan untuk melakukan 
rilis  baru.  Pengujian  dilakukan 
terhadap  web  dengan 
menerapkan  metode  pengujian  yang  telah  dipilih.  Pengujian 
sistem yang digunakan adalah black box testing [11] dan system 
usability scale (SUS) [12]. Black box testing dipilih untuk dapat 
memastikan bahwa fungsionalitas pada tiap fitur dapat berjalan 
dengan semestinya. Sementara itu, SUS merupakan salah satu 
instrumen  pengujian  sistem  yang  digunakan  untuk  mengukur 
kegunaan  dari  sistem.  SUS  dapat  menyajikan  hasil  penilaian 
yang  jauh  lebih  baik  untuk  website  usability  dibandingkan 
kuesioner  lainnya  yang  sejenis  [13].  Pada  fase  ini  diperlukan 
kerja  sama  banyak  pihak  untuk  dapat  memberikan  penilaian 
dan umpan balik kepada web yang telah dibangun. 

ditemukan  beberapa  kendala  salah  satunya,  yaitu  belum 
tersedianya  sarana  penunjang  yang  dapat  menyediakan  akses 
bagi  pihak-pihak  yang  terlibat  di  dalam  Sistem  Statistik 
Nasional  terhadap  kegiatan  maupun  informasi  terkait  NQAF. 
Selain  itu,  terdapat  instrumen  yang  penggunaannya  belum 
dapat  diaplikasikan  terhadap  pengguna  eksternal  sehingga 
terdapat perbedaan hak akses di antara pengguna internal dan 
eksternal.  Selanjutnya,  pada  penelitian  ini  akan  dilakukan 
proses  pembangunan  web  NQAF  dengan  mengembangkan 
beberapa fitur yang diharapkan dapat menjadi solusi terhadap 
masalah-masalah  yang  berkaitan  dengan  kegiatan  NQAF. 
Gambaran  kerangka  pikir  penelitian  ini  dapat  dilihat  pada 
gambar 2. 

VI. HASIL DAN PEMBAHASAN 

A. Analisis Sistem Berjalan 

dengan 

narasumber 

Berdasarkan  wawancara 

dari 
Subdirektorat  Konsistensi  Statistik  di  Badan  Pusat  Statistik, 
sebelumnya  BPS  telah  memiliki  Sistem  Informasi  Quality 
Assurance  Framework  atau  biasa  disebut  SIQAF  dan  kini 
dikenal  dengan  sebutan  web  Pengukuran  Kualitas  Mandiri. 
Web Pengukuran Kualitas Mandiri merupakan tools penjamin 
kualitas  yang  digunakan  untuk  mengukur  kualitas  dari  suatu 
output  statistik.  Pengisian  pengukuran  kualitas  dilakukan 
dengan  metode  self-assesment  oleh  subject  matter  pelaksana 
survei 
terkait  yang  mencakup  seluruh  tahapan  Generic 
Statistical Business Process Model (GSBPM). Asesmen yang 
mencakup tahapan GSBPM dimulai dari penentuan kebutuhan 
hingga evaluasi yang bertujuan untuk menilai produk kegiatan 
statistik  berdasarkan  enam  sisi  dimensi  kualitas.  Setiap 
tahunnya  dilakukan  pengukuran  dan  dipublikasikan  hasilnya. 
Sementara itu, instrumen penjaminan kualitas yang lain, seperti 
quality  gates  dan  monitoring  kualitas,  juga  memiliki  proses 
bisnis terpisah antara satu dengan yang lain. Alur proses bisnis 
sistem berjalan secara umum dapat dilihat pada gambar 3. 

Gambar 2. Kerangka Pikir 

Ketika  user  stories  telah  seluruhnya  diimplementasi  maka  
hal ini menandakan bahwa pembangunan telah memasuki fase 
ini  merupakan  waktu  untuk  menyelesaikan 
akhir.  Fase 
dokumentasi  terkait  perancangan  dan  pembangunan  web 
karena  sudah  tidak  ada  lagi  perubahan  arsitektur,  desain, 
maupun kode yang seluruhnya telah final. 

V.  KERANGKA PIKIR 

Penelitian ini berfokus pada permasalahan yang terdapat di 
dalam pelaksanaan kegiatan NQAF. Dalam kegiatan tersebut, 

Gambar 3. Alur proses bisnis sistem berjalan secara umum 

 3 / 8 

 
 
 
 
Gambar 4. Diagram fishbone sistem berjalan 

Bagian 

B. Analisis Masalah 

Berdasarkan gambaran pada diagram fishbone (gambar 4), 
teridentifikasi  empat  masalah  utama  pada  sistem  berjalan,  di 
antaranya  method,  man,  machine,  dan  materials.  Instrumen 
penjaminan kualitas yang belum terintegrasi menjadi salah satu 
masalah  pada  sisi  method.  Selain  itu,  untuk  dapat  melayani 
setiap  pertanyaan  yang  diajukan  oleh  pengguna  diperlukan 
suatu  pelayanan  tanya-jawab  dan  frequently  asked  questions 
(FAQ) yang berkaitan dengan pelaksanaan NQAF. 

Pada sisi man, proses asesmen yang melibatkan pengguna 
eksternal  juga  menjadi  permasalahan  tersendiri.  Hal  tersebut 
dikarenakan terdapat perbedaan hak akses pengguna (privilage) 
antara pengguna internal dan eksternal BPS. Selain itu, terdapat 
juga  perbedaan  privilage  dalam  mengakses  sebagian  fitur 
asesmen  pada  proses  bisnis  NQAF.  Masalah  aspek  machine 
disebabkan  oleh  belum  tersedianya  web  yang  dapat  diakses 
oleh pengguna. Tidak adanya web tersebut dapat menghambat 
proses  bisnis  asesmen  dari  NQAF  yang  dilakukan  oleh  BPS 
kepada  kementerian  dan  lembaga  lain.  Sarana  ini  diperlukan 
sebagai  media  berkumpulnya  seluruh  instrumen  penilaian 
NQAF yang dimiliki BPS dalam satu tempat. 

Permasalahan  materials  muncul  akibat  belum  tersedianya 
informasi  terkait  NQAF  yang  akan  diakses  oleh  pengguna 
khususnya dari kementerian dan lembaga lain. Informasi yang 
belum  tersedia,  yaitu  publikasi,  pedoman  asesmen,  kegiatan. 
Informasi-informasi  tersebut  diperlukan  untuk  menunjang 
pelaksanaan  penjaminan  kualitas  yang  dilakukan  BPS  pada 
kementerian  dan  lembaga  serta  pertanggungjawaban  untuk 
menyajikan hasil laporan asesmen yang telah dilakukan. 

C. Analisis Kebutuhan 

Kebutuhan  sistem  dapat  dibagi  menjadi  dua  jenis,  yaitu 

kebutuhan fungsional dan kebutuhan non-fungsional. 
1.  Kebutuhan Fungsional 

Kebutuhan fungsional dari web usulan yang akan dibangun, 

di antaranya: 

i.  Sistem dapat mengintegrasikan instrumen penjaminan 
kualitas yang dimiliki oleh BPS, yaitu  quality gates, 
pengukuran kualitas, dan monitoring kualitas; 

ii.  Sistem dapat menyajikan dan memperbarui informasi 
terkait penjaminan kualitas yang dilakukan oleh BPS 
maupun kementerian/lembaga; 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

iii.  Sistem dapat melakukan pengelolaan informasi seperti 
mengubah, menambahkan, dan menghapus informasi 
terkait kegiatan NQAF. 
2.  Kebutuhan Non-Fungsional 

Kebutuhan non-fungsional dapat diidentifikasikan melalui 
penerapan  analisis  PIECES.  Analisis  PIECES  menerapkan 
enam  aspek  dalam  mengidentifikasi  kebutuhan  dari  suatu 
sistem. Analisis lebih jelasnya dapat dilihat pada tabel 1. 

TABEL I 
ANALISIS PIECES PADA WEB NQAF 

Masalah 
Pengguna harus 
mengakses tiap web 
instrumen yang tersedia 
bila ingin mendaftarkan 
survei/sensus, 
menyerahkan berkas 
proses bisnis, dan 
melihat hasil laporan. 
Informasi berkaitan 
dengan penjaminan 
kualitas yang 
diselenggarakan oleh 
BPS belum dapat 
diakses oleh pengguna. 
Belum sepenuhnya 
survei yang dilakukan 
oleh kementerian dan 
lembaga telah 
melaksanakan proses 
penjaminan kualitas, hal 
tersebut dapat 
berpotensi menimbulkan 
kerugian salah satunya 
pada aspek finansial. 
Tidak adanya kontrol 
yang mampu membagi 
hak akses yang berbeda 
terhadap pengguna 
internal BPS dan 
kementerian/lembaga. 
Tidak efisiennya waktu 
dalam melakukan 
tahapan penjaminan 
kualitas, khususnya 
pendaftaran dan 
pelaporan, yang masih 
terbagi sesuai instrumen. 
Belum terdapat fitur 
pengelolaan informasi 
terkait pelaksanaan 
NQAF. 

Solusi 
Web NQAF dapat menjadi 
media terintegrasinya 
instrumen penjaminan 
kualitas yang dimiliki BPS. 

Subject matter pelaksana 
survei dapat mengakses 
informasi seputar 
penjaminan kualitas 
melalui web NQAF. 

Kementerian dan lembaga 
dapat melakukan asesmen 
secara mandiri terhadap 
survei yang telah mereka 
laksanakan, sehingga 
diharapkan dapat 
mengurangi potensi 
dampak kerugian pada 
survei-survei yang akan 
datang. 
Terdapat kendali yang 
dapat memberikan hak 
akses yang berbeda pada 
tiap pengguna sesuai 
dengan role yang 
dimilikinya. 
Subject matter dapat 
melakukan penjaminan 
kualitas melalui web 
NQAF yang diharapkan 
dapat memangkas waktu 
proses tahapan penjaminan 
kualitas yang akan dilalui. 
Terdapat fitur yang dapat 
digunakan oleh admin 
untuk mengelola informasi 
seputar NQAF. 

Performance 

Information 

Economy 

Control 

Efficiency 

Service 

D. Rancangan Sistem Usulan 

Berdasarkan permasalahan yang telah dianalisis pada bagian 
sebelumnya,  perancangan  sistem  dilakukan  untuk  dapat 
mengimplementasikan solusi dan menyelesaikan masalah dari 
penerapan  NQAF  di 
akan 
diimplementasikan adalah pembangunan web National Quality 

Indonesia.  Solusi  yang 

 4 / 8 

 
 
 
 
Assurance Framework yang akan mengintegrasikan instrumen-
instrumen  penjaminan  kualitas  dan  menjadi  media  bagi 
pengguna kementerian dan lembaga maupun masyarakat dalam 
mengakses informasi terkait pelaksanaan NQAF. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 5. Arsitektur sistem usulan 

Rancangan Arsitektur Sistem 
Rancangan  arsitektur  sistem  dapat  dilihat  pada  gambar  5. 
Untuk  dapat  mengunjungi  web  NQAF,  pengguna  terlebih 
dahulu  mengakses  web  browser  yang  terhubung  dengan 
internet. Pada sisi front-end, web NQAF mengandalkan jQuery 
dan Bootstarp versi lima yang didukung oleh penggunaan CSS 
preprocessor,  yaitu  Sass.  MariaDB  akan  bertindak  sebagai 
DBMS untuk mengelola dan menjalankan operasi ke basis data 
yang  terdapat  di  server.  Selain  itu,  terdapat  REST  API  yang 
digunakan  untuk  dapat  mengintegrasi  dan  melakukan 
komunikasi  data  dengan  instrumen  penjaminan  kualitas  yang 
ada,  yaitu  web  Quality  Gates  dan  web  Pengukuran  Kualitas 
Mandiri. 

Rancangan Proses Bisnis 
Alur  proses  bisnis  yang  digambarkan  secara  umum  dapat 
dilihat melalui gambar 6. Proses bisnis dimulai dari pengguna 
lalu  mendaftarkan 
yang  masuk  ke  dalam  web  NQAF 
survei/sensus  yang  akan  dilakukan  asesmen  sampai  asesmen 
tersebut  dinyatakan  selesai.  Pengguna  dapat  melihat  hasil 
laporan di tiap instrumen saat asesmen telah selesai. 

Rancangan Use Case 
Untuk dapat menjelaskan interaksi antara aktor dan sistem 
yang  terlibat  dalam  proses  bisnis,  maka  dirancang  suatu 
gambaran yang disebut diagram use case. Fungsi-fungsi yang 
masuk  ke  dalam  use  case  merupakan  komponen  utama  dari 
sistem yang sedang dibangun. Interaksi sistem akan melibatkan 
aktor  yang  memiliki  peran  sebagai  pengguna  (internal  dan 
eksternal BPS) dan admin (Subdirektorat Konsistensi Statistik) 
dalam  sistem  usulan.  Interaksi  sistem  dengan  pengguna  dan 
admin tergambar pada gambar 7. 

Rancangan Basis Data 
Basis  data  pada  web  NQAF  menggunakan  beberapa  tabel 
yang  digunakan  untuk  menyimpan  data  dan  informasi  yang 
berkaitan dengan pelaksanaan NQAF. Relasi antartabel dapat 
digambarkan melalui Entity Relationship Diagram (ERD) pada 
gambar 8. 

Gambar 6. Diagram swimline proses bisnis secara umum dari sistem usulan 

Gambar 7. Diagram use case sistem usulan 

 5 / 8 

 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

mengacu  pada  hasil  penelusuran  yang  dilakukan  peneliti 
terhadap gambaran web yang memiliki umpan balik yang baik 
dari pengguna dan dievaluasi secara berkala oleh subject matter. 
Beberapa  gambaran  dari  rancangan  antarmuka  dapat  dilihat 
pada gambar 9 sampai 12. 

Gambar 8. Entity relationship diagram 

Gambar 11. Rancangan antarmuka halaman dashboard 

Gambar 9. Rancangan antarmuka landing page pada halaman beranda 

Gambar 12. Rancangan antarmuka daftar survei/sensus halaman dashboard 

Gambar 10. Rancangan antarmuka halaman tentang 

Rancangan Antarmuka 
Rancangan  antarmuka  didesain  menggunakan  aplikasi 
Microsoft Visio dengan memanfaatkan fitur wireframe. Desain 

Gambar 13. Implementasi antarmuka landing page pada halaman beranda 

 6 / 8 

 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

E. Implementasi Antarmuka 

F. Implementasi Kode 

Implementasi  antarmuka  adalah  tahap  penerapan  dari 
rancangan  antarmuka  yang  telah  didesain  dalam  bentuk 
wireframe dan diimplementasikan menjadi user interface yang 
dapat  dilihat  dan  digunakan 
langsung  oleh  pengguna. 
Implementasi  antarmuka  meliputi  halaman  landing  page 
(termasuk menu beranda, berita, kegiatan, tentang dan FAQ), 
dashboard,  dashboard  admin,  menu  quality  gates,  menu 
pengukuran  kualitas,  menu  monitoring  kualitas,  serta  menu-
menu  pengelolaan  informasi  yang  hanya  dapat  diakses  oleh 
admin. Beberapa gambaran hasil implementasi dari rancangan 
antarmuka web NQAF dapat dilihat pada gambar 13 sampai 16. 

Berikut ini merupakan beberapa potongan kode terpilih dari 
web NQAF yang telah diimplementasikan. Potongan kode yang 
dipilih merupakan fungsi yang dianggap dapat mewakili kode 
lainnya yang menjadi back-end dari web NQAF. 

Pendaftaran Survei/Sensus 
Fungsi daftarSurvei akan melakukan validasi masukan dari 
informasi survei/sensus sebelum data tersebut ditambahkan ke 
dalam  basis  data.  Kemudian,  berkas  permohonan  akan 
diberikan nama unik secara acak dan disimpan ke dalam server. 
Ketika  fungsi  berhasil  diproses,  maka  pengguna  akan 
mendapatkan  umpan  balik  berupa  keterangan  bahwa 
survei/sensus telah berhasil didaftarkan. Potongan kode dapat 
dilihat pada gambar 17. 

Gambar 14. Implementasi antarmuka halaman tentang NQAF 

Gambar 17. Potongan kode pendaftaran survei/sensus 

Gambar 15. Implementasi antarmuka halaman dashboard user eksternal 

Gambar 16. Implementasi antarmuka isian pendaftaran survei/sensus 

Gambar 18. Potongan kode pengajuan pertanyaan 

Pengajuan Pertanyaan 
Pada gambar 18, sebelum menambahkan data ke dalam basis 
data,  fungsi  ajukanPertanyaan  akan  melakukan  validasi  isian 
pertanyaan  yang  diajukan  oleh  pengguna.  Tiap  isian  yang 

 7 / 8 

 
 
 
 
 
 
 
divalidasi  harus  sesuai  dengan  jenis  masukan  yang  ada. 
Pengguna  akan  mendapatkan  umpan  balik  ketika  fungsi 
berhasil dijalankan maupun gagal berjalan. 

Fungsi filter pada tabel survei/sensus admin 
Gambar  19  menunjukkan  potongan  kode  dari  fitur  filter 
sehingga  admin  dapat    menampilkan  survei/sensus  sesuai 
atribut  yang 
tersebut  merupakan 
pengembangan dari library datatable yang mana akan terdapat 
satu baris pada akhir tabel yang menampilkan dropdown pada 
tiap atribut tabel. Dropdown tersebut akan berisikan tiap item 
atribut yang terdapat pada tabel. 

telah  dipilih.  Kode 

Gambar 19. Potongan kode fungsi filter pada tabel survei/sensus admin 

G. Hasil Pengujian 

Pada penelitian ini digunakan dua metode pengujian sistem, 
yaitu Black Box Testing [11] dan System Usability Scale (SUS) 
testing  menunjukkan  bahwa 
[12].  Hasil  black  box 
fungsionalitas  dari  seluruh  fungsi  pada  tiap  fitur  telah  sesuai 
dan  berjalan  sebagaimana  mestinya.  SUS  dilakukan  terhadap 
13 
responden,  di  antaranya  7  pegawai  Subdirektorat 
Konsistensi Statistik BPS, 3 pegawai BPS dari satuan kerja lain, 
dan sisanya adalah mahasiswa Politeknik Statistika STIS. Hasil 
dari SUS didapatkan nilai sebesar 77,5 yang dapat disimpulkan 
bahwa nilai tersebut berada di atas angka 70 sehingga masuk 
ke  dalam  kategori  Acceptable  [14]  dan  lebih  besar  dari  71,4 
sehingga masuk kategori Good [15]. 

VII. 

PENUTUP 

A. Kesimpulan 

Berdasarkan  penelitian  yang 

telah  dilakukan 

terkait 

pembangunan web, dapat diperoleh kesimpulan di antaranya: 
1.  Pada  penelitian  ini  telah  dirancang  dan  dibangun  web 
National Quality Assurance Framework (NQAF) BPS dan 
berdasarkan hasil evaluasi sistem diperoleh hasil pengujian 
SUS sebesar 77,5 yang masuk ke dalam kategori acceptable 
dan good. 

2.  Web National Quality Assurance Framework (NQAF) BPS 
telah  dibangun  sesuai  dengan  kebutuhan,  yaitu  dapat 
mengintegrasikan  instrumen  penjaminan  kualitas  yang 
dimiliki  BPS,  dapat  menyajikan  dan  memperbarui 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

informasi terkait penjaminan kualitas yang dilakukan oleh 
BPS  dan  kementerian/lembaga,  serta  dapat  melakukan 
pengelolaan  informasi  seperti  mengubah,  menambahkan, 
dan menghapus informasi. 

3.  Pada  penelitian  ini  telah  dibangun  subsistem  pengelolaan 
akun,  berita,  galeri,  dan  pertanyaan  terkait  informasi 
kegiatan NQAF di BPS. 

B. Saran 

Berdasarkan hasil yang telah diperoleh dalam penelitian ini, 
berikut  merupakan  saran  yang  dapat  menjadi  pertimbangan 
untuk penelitian yang akan datang dan juga bagi pengguna. 
1.  Sistem  ini  dapat  dikembangkan  lebih  lanjut  sehingga 
mampu terintegrasi dengan sistem lainnya yang telah ada di 
BPS,  seperti  Metadata  Management  System  (MMS) 
maupun Integreted Collection System (ICS). 
integrasi 

terhadap  web 
lebih 
pengukuran kualitas mandiri khususnya  terkait pertukaran 
data dan sinkronisasi tahapan. 

2.  Perlu  adanya 

lanjut 

3.  Hasil  SUS  menunjukkan  bahwa  pertanyaan  nomor  10 
menjadi  skor  rata-rata  terendah  yang  berarti  pengguna 
membutuhkan waktu lebih untuk mempelajari terkait sistem 
yang telah dibangun. Sehingga diperlukan tambahan konten 
petunjuk  operasional  untuk  dapat  membantu  pengguna 
dalam mengoperasikan web. 

4.  Pengguna  disarankan  untuk  mengakses  web  NQAF 
menggunakan  web  browser,  seperti  Google  Chrome, 
Microsoft Edge, Mozilla Firefox, Opera dan aplikasi sejenis 
lainnya dengan versi yang paling mutakhir. 

DAFTAR PUSTAKA 
[1]  Badan  Pusat  Statistik,  Kerangka  Penjaminan  Kualitas  Data  Statistik. 

Jakarta: BPS, 2015. 

[2]  I. P. Fellegi, “Characteristics of an effective statistical system,” Stat. J. UN. 

Econ. Comm. Eur., vol. 13, no. 2, pp. 89–117, 1996. 

[3]  International  Organization  for  Standardization,  “Quality  management 
systems – Fundamentals and vocabulary (ISO Standard No. 9000:2015),” 
2015. 

[4]  United Nations, United Nations National Quality Assurance Frameworks 

Manual for Official Statistics. New York, 2019. 

[5]  W. J. Radermacher, “The european statistics code of practice as a pillar to 
strengthen public trust and enhance quality in official statistics,”  J. Stat. 
Soc. Inq. Soc. Irel., vol. 43, pp. 27–33, 2014. 

[6]  N. Keret and G. Eminkahyagil, “Quality assurance framework applications 

in Turkish Statistical System,” 2014. 

[7]  Turkish Statistical, “TurkStat Quality Assurance Framework,” 2015. 
[8]  R. Mukund, “Web Based Methods and Systems for Managing Compliance 

Assurance Information,” U.S. Patent No. 7,640,165, 2009. 

[9]  J. L. Whitten and L. D. Bentley, System Analysis & Design Methods, 7th 

ed. New York: McGraw-Hill/Irwinn, 2007. 

[10] A. Cockburn, “The Agile Software Development Series,” Int. Conf. Softw. 

Eng. 2000, p. 220, 2001. 

[11] R.  Black,  Managing  Test  Process,  3rd  ed.  Indianapolis,  IN:  Wiley 

Publishing, Inc., 2009. 

[12] K. Finstad, “The system usability scale and non-native English speakers,” 

J. Usability Stud., vol. 1, no. 4, pp. 185–188, 2006. 

[13]J. R. Lewis and J. Sauro, “The factor structure of the system usability scale,” 
Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. 
Notes  Bioinformatics),  vol.  5619  LNCS,  pp.  94–103,  2009,  doi: 
10.1007/978-3-642-02806-9_12. 

[14] J. Brooke, “SUS: a retrospective,” J. usability Stud., vol. 8, no. 2, pp. 29–

40, 2013. 

[15] A. Bangor, P. Kortum, and J. Miller, “Determining what individual SUS 
scores mean: adding an adjective rating scale,” J. usability Stud., vol. 4, no. 
3, pp. 114–123, 2009. 

 8 / 8 

 
 
 
 
 
"
221709994,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pembangunan Aplikasi Peringkasan Dokumen Berita 
Otomatis Menggunakan Metode Latent Semantic 
Analysis dan Cross Latent Semantic Analysis 

Salim Satriajati (221709994, 4SD1)  
Dosen Pembimbing: Ibnu Santoso, S.S.T., M.T. 

Ringkasan—  Kemajuan  teknologi  informasi  dan  komunikasi 
saat ini menyebabkan akses terhadap berita menjadi lebih cepat. 
Hal  tersebut  dimanfaatkan  oleh  Biro  Hubungan  Masyarakat 
Badan Pusat Statistik (Humas BPS) dalam menunjang kinerjanya 
dengan  melakukan  pengumpulan  berita  dan  meringkas  berita 
dari media daring. Namun, dalam meringkas berita, Humas BPS 
perlu  membaca  seluruh  berita  tersebut  satu  per  satu  sehingga 
kurang  efektif  dan  efisien.  Untuk  mengatasi  permasalahan 
tersebut,  maka  dalam  penelitian  ini  akan  dirancang  sebuah 
aplikasi peringkasan teks pada artikel berita berbahasa Indonesia 
secara  otomatis. Di  samping  peringkasan  teks,  dalam  penelitian 
ini  juga  akan  dibangun  sebuah  modul  pengumpul  berita  untuk 
mendapatkan berita dari media daring. Pembuatan peringkasan 
teks  otomatis  dalam  penelitian  ini  menggunakan  metode  Latent 
Semantic  Analysis  (LSA)  dan  Cross  Latent  Semantic  Analysis 
(CLSA).    Berdasarkan  evaluasi  hasil  peringkasan  teks  otomatis 
menggunakan  ROUGE,  metode  peringkasan  CLSA  dengan 
metode  pemilihan  kalimat  avesvd  adalah  yang  terbaik  dengan 
rata-rata nilai f1-score ROUGE-1 0.51892, ROUGE-2 0.42842 dan 
ROUGE-L  0.54177.  Selanjutnya,  metode 
tersebut 
diimplementasikan pada aplikasi. 

terbaik 

Kata Kunci— berita, peringkasan, LSA, CLSA 

I.  LATAR BELAKANG 

Saat  ini,  dunia  sedang  berada  pada  era  dimana  teknologi 
informasi dan komunikasi berkembang sangat pesat. Salah satu 
dampaknya adalah adanya media berita daring atau online [1]. 
Jika  sebelumnya  berita  didapatkan  melalui  televisi,  radio, 
maupun  media  cetak  seperti  koran,  majalah,  tabloid  dan 
sebagainya,  kini  masyarakat  dapat  mengakses  berita  hanya 
dengan  menggunakan  gawai  dan  koneksi  internet.  Beragam 
jenis media menyediakan berita terkini secara cepat, sehingga 
memudahkan  masyarakat  dalam  mengakses  fenomena  dan 
kejadian yang terjadi di tengah-tengah mereka. 

Berita yang beredar luas di dunia maya juga dimanfaatkan 
oleh  Biro  Hubungan  Masyarakat  Badan  Pusat  Statistik  RI 
(Humas  BPS).  Humas  BPS  telah  melakukan  pengumpulan 
berita  dari  media  massa  baik  cetak  dan  digital  yang  didapat 
melalui  kerjasama  dengan  pihak  ketiga,  yakni  Isentia.  
Pengumpulan  berita  tersebut  digunakan  sebagai  penilaian 
kinerja dan evaluasi pandangan masyarakat terhadap BPS.  

Namun,  dalam  memahami  informasi  pada  artikel  berita, 
pihak  Humas  BPS  perlu  membaca  berita  tersebut  secara 
keseluruhan. Maka dari itu, dibutuhkan suatu alat bantu yang 
dapat  mereduksi  artikel  berita  menjadi  bagian  yang  hanya 

mengandung bagian terpenting saja, yakni berupa peringkasan 
teks. 

Peringkasan teks adalah proses membuat versi terkompresi 
pada teks yang berisi informasi paling berguna bagi pengguna 
[2].  Dalam  meringkas  teks,  informasi  tambahan  dalam  teks 
tersebut dapat dikesampingkan sehingga hasil dari peringkasan 
hanya menyisakan bagian teks yang terpenting saja, sehingga 
memudahkan  pembaca  untuk  mencerna  makna  utama  dari 
keseluruhan teks. 

Humas BPS sendiri telah melakukan peringkasan dokumen 
berita.  Hasil  ringkasan  tersebut  kemudian  dilaporkan  dalam 
bentuk  kliping  dan  dikirim  ke  seluruh  pegawai  BPS  lainnya 
melalui e-mail.  

Namun  dalam  prakteknya,  pembuatan  ringkasan  dokumen 
berita  secara  manual  tersebut  dirasa  kurang  efisien  karena 
Humas  BPS  perlu  membaca  isi  teks  secara  intensif  dan 
menyeluruh  agar  dapat  menemukan  inti  dan  hubungan  antar 
bagian  teks.  Karena  melibatkan  manusia,  kegiatan  membaca 
dan  meringkas  teks  akan  menemui  sejumlah  keterbatasan, 
terutama  keterbatasan  secara  fisik  seperti  kelelahan.  Selain 
keterbatasan  fisik  manusia,  pembuatan  ringkasan  teks  juga 
memerlukan  teknik  dan  keterampilan  dalam  memahami  isi 
artikel agar hasilnya bagus dan dapat mewakili isi artikel. 

teks 

Peringkasan 

secara  otomatis  dapat  menjawab 
keterbatasan-keterbatasan  tersebut.  Dengan  peringkasan  teks 
secara  otomatis,  kegiatan  meringkas  teks  yang  sebelumnya 
dilakukan secara manual oleh manusia,  dapat digantikan oleh 
mesin.  Tentunya  dengan  melibatkan  mesin,  kegiatan 
peringkasan teks dapat meminimalisasi bahkan menghilangkan 
keterbatasan  yang  bersumber  dari  manusia, 
terutama 
keterbatasan dari segi fisik. 

Peringkasan  teks  secara  otomatis  dapat  dilakukan  dengan 
memanfatkan keilmuan Natural Language Processing (NLP). 
NLP sendiri merupakan cabang dari ilmu komputer, kecerdasan 
buatan  dan  linguistik  yang  berkaitan  dengan  interaksi  antara 
komputer dan bahasa manusia [3].  

Terdapat  beberapa  metode  yang  dapat  digunakan  dalam 
peringkasan dalam teks. Penelitian ini akan menggunakan dua 
buah metode, yakni Latent Semantic Analysis (LSA) dan Cross 
Latent  Semantic  Analysis  (CLSA).  Selain  itu  juga  digunakan 
dua buah metode pemilihan kalimat, yakni metode Steinberger 
&  Jezek  dan  avesvd.  Metode-metode  tersebut  kemudian 
diperbandingkan  dengan  ringkasan  ideal  yang  dibuat  oleh 
manusia  melalui  evaluasi  hasil  peringkasan  teks  otomatis 
menggunakan nilai ROUGE (Recall-Oriented Understudy for 

 1 / 8 

 
 
 
 
 
Gisting Evaluation). Metode peringkasan teks bersama metode 
pemilihan kalimat yang terbaik selanjutnya diimplementasikan 
ke dalam sebuah aplikasi. 

Berdasarkan permasalahan tersebut, maka dalam penelitian 
ini  akan  dirancang  suatu  aplikasi  peringkasan  teks  secara 
otomatis  pada  artikel  berita  berbahasa  Indonesia  dengan 
memanfaatkan keilmuan Natural Language Processing. Selain 
aplikasi  peringkasan 
juga 
mengembangkan modul pengumpul berita yang memanfaatkan 
teknik  web  crawling  dan  web  scraping  yang  dapat 
mengumpulkan berita untuk selanjutnya diintegrasikan dengan 
aplikasi  peringkasan  teks  otomatis.  Pada  akhirnya,  aplikasi 
yang  dibuat  dalam  penelitian 
ini  diharapkan  dapat 
dimanfaatkan oleh Humas BPS untuk menunjang kegiatannya. 

teks  otomatis,  penelitian 

ini 

II.  TUJUAN PENELITIAN 

Berdasarkan latar belakang di atas, selanjutnya dirumuskan 

tujuan penelitian sebagai berikut: 
1.  Membangun  modul 

pengumpul 

mendapatkan berita dari media daring. 

berita 

untuk 

2.  Menerapkan  Natural  Language  Processing  dalam 

meringkas teks secara otomatis. 

3.  Membangun  aplikasi  antar  muka  peringkasan  teks 

otomatis berbasis web. 

III. PENELITIAN TERKAIT 

Dalam tahap ini, dilakukan studi literatur dengan menelusuri 
penelitian  yang  pernah  dilakukan  sebelumnya  yang  terkait 
dengan  sistem  pengumpul  berita  yang  digunakan  untuk 
mendapatkan  berita  dari  media  daring  dan  peringkasan  teks 
otomatis. Peta literatur dapat dilihat pada Gambar 1. 

Gambar 1. Peta literatur (literature map) 

Penelitian [1] menjelaskan mengenai web scraping. Tujuan 
penelitian tersebut adalah untuk membangun sebuah situs web 
yang  dapat  menyimpan  secara  otomatis  data  berita  hasil  web 
scraping  ke  dalam  database.  Hasil  akhir  penelitian  tersebut 
adalah  sebuah  situs  web  yang  dapat  mengambil  isi  dari 
beberapa situs media berita daring yang selanjutnya digabung 
menjadi satu situs web. 

Penelitian [4] menjelaskan mengenai pembuatan sistem web 
scraping  secara 
tersebut  berhasil 
terjadwal.  Penelitian 
membangun  sebuah  Sistem  Pengumpul  Berita  Terjadwal  dan 
Aplikasi Dashboard Berita berbasis web. Scraper dalam sistem 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

web scraping tersebut dibuat dengan menggunakan Python dan 
package Scrapy. 

Penelitian  [5]  menjelaskan  mengenai  pembuatan  modul 
Automatic  Text  Summarizer  menggunakan  metode  Latent 
Dirichlet  Algorithm  yang  disematkan  ke  dalam  aplikasi  yang 
bernama WMSS (We Mining Something Serious). 

Penelitian  [6]  menjelaskan  mengenai  pembuatan  aplikasi 
pengumpul berita dan peringkas berita otomatis yang berkaitan 
dengan  perubahan  nilai  Produk  Domestik  Bruto  (PDB). 
Peringkas  berita  otomatis  dalam  penelitian 
tersebut 
menggunakan tiga metode yang melibatkan sentence extraction 
dan lexical chain. 

Penelitian  [7]  menjelaskan  mengenai  pengimplementasian 
metode peringkasan teks yakni Latent Semantic Analysis (LSA) 
dan Cross Latent Semantic Analysis (CLSA). Peringkasan teks 
dalam penelitian ini dilakukan pada teks berbahasa Indonesia. 
Dengan compression rate (rasio hasil peringkasan dengan teks 
aslinya)  sebesar  30%,  evaluasi  hasil  ringkasan  dari  240  buah 
artikel menghasilkan nilai F-Measure 72% pada metode CLSA. 
Sedangkan  dengan  metode  LSA  didapatkan  nilai  F-Measure 
70%. Penelitian ini menyimpulkan bahwa metode CLSA lebih 
baik dari metode LSA. 

Penelitian  [8]  menjelaskan  mengenai  beberapa  metode 
pemilihan  kalimat  dalam  peringkasan 
teks  otomatis, 
diantaranya metode Gong & Liu, Steinberger & Jezek, Murray 
&  Renals  &  Carletta,  Cross  Method  dan  Topic  Method. 
Peringkasan teks dalam penelitian ini diterapkan pada Bahasa 
Turki.  Penelitian  ini  menggunakan  nilai  ROUGE  dalam 
evaluasi hasil peringkasan teks otomatis.  

Penelitian [9] menjelaskan mengenai metode Steinberger & 
Jezek  sebagai  metode  pemilihan  kalimat  dalam  peringkasan 
teks  otomatis.  Ide  dari  metode  pemilihan  kalimat  ini  adalah 
menghitung  length  setiap  kalimat  berdasarkan  formula  yang 
dibuat  oleh  Steinberger  dan  Jezek.  Kalimat  yang  menjadi 
ringkasan adalah yang memiliki length terbesar, diikuti kalimat 
berikutnya hingga mencapai batas ringkasan yang ditetapkan. 
Penelitian  [10]  menjelaskan  mengenai  metode  avesvd  dan 
ravesvd sebagai metode pemilihan kalimat dalam peringkasan 
teks  otomatis.  Kedua  metode  tersebut  adalah  pengembangan 
dari  metode  Gong  &  Liu.  Kalimat  yang  menjadi  ringkasan 
dalam metode avesvd adalah yang memiliki nilai rata-rata term 
terbesar. 

Penelitian  [11]  menjelaskan  mengenai  metode  evaluasi 
peringkasan  teks.  Metode  evaluasi  secara  garis  besar  dibagi 
menjadi  dua.  Pertama  adalah  evaluasi  ekstrinsik,  yakni 
memeriksa  ringkasan  berdasarkan  bagaimana  pengaruhnya 
terhadap  penyelesaian  tugas  lain  seperti  klasifikasi  teks, 
pencarian informasi, dan lain-lain. Ringkasan dianggap sebagai 
baik  jika  hasil  ringkasannya  dapat  membantu  tugas  tersebut. 
Kedua  adalah  evaluasi  intrinsik,  yakni  memeriksa  sistem 
peringkasan  itu  sendiri  dimana  kualitas  ringkasan  ditentukan 
berdasarkan perbandingan antara ringkasan yang dibuat secara 
otomatis dan ringkasan manual buatan manusia. Salah satu alat 
evaluasi secara intrinsik ini adalah Recall Oriented Understudy 
of Gisting Evaluation (ROUGE). 

 2 / 8 

 
 
 
 
 
Penelitian  [12]  khusus  menjelaskan  mengenai  ROUGE 
sebagai  alat  evaluasi  otomatis  untuk  mengukur  kualitas 
ringkasan teks. 

IV. METODE PENELITIAN  

A. Ruang Lingkup Penelitian 

Penelitian  ini  akan  menghasilkan  artefak  berupa  sebuah 
aplikasi yang dapat melakukan peringkasan teks berita secara 
otomatis  yang  terintegrasi  dengan  modul  pengumpul  berita, 
dimana berita dikumpulkan dari media daring berdasarkan kata 
kunci  yang  diinput  oleh  pengguna.  Berita  yang  dapat 
dikumpulkan  dibatasi  dari  empat  situs  media  daring  yakni 
Detik,  Kompas,  Antara  dan  Bisnis.com.  Jumlah  berita  yang 
dapat  dikumpulkan  juga  dibatasi,  yakni  seluruh  berita  yang 
muncul  pada  halaman  pertama  pencarian  kata  kunci  pada 
masing-masing situs media daring. 

B. Metode Pengumpulan Data 

Data  yang  dikumpulkan  untuk  menunjang  penelitian  ini 

berasal dari berbagai sumber, sebagai berikut: 

1. Studi Pustaka 

Studi Pustaka dilakukan untuk menemukan informasi 
yang dapat menunjang penelitian. Studi pustaka dilakukan 
melalui  jurnal,  internet,  skripsi  terdahulu  dan  media 
lainnya.  

2. Data sekunder  

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

rancangan arsitektur sistem dan use case diagram. 

3.  Development 

Dalam pengembangan sistem ini, digunakan beberapa 
alat.  Bahasa  pemrograman  yang  digunakan  di  sisi 
backend adalah Python versi 3.7. Python dimanfaatkan 
dalam  penggunaan  web  framework  Flask  dan  package 
Scrapy  untuk  web  scraping.  Sedangkan,  implementasi 
sistem  di  sisi  frontend  menggunakan  HTML,  CSS, 
Javascript  dengan  memanfaatkan  web 
framework 
Codeigniter 3 yang berbasis PHP. 

4.  Evaluation  

Evaluasi dilakukan untuk memastikan sistem berjalan. 

Penelitian ini akan menggunakan uji blackbox. 

5.  Conclusion 

Dalam  tahap  ini,  dilakukan  penarikan  kesimpulan 
apakah  sistem  berjalan  sudah  memenuhi  harapan  atau 
belum. 

otomatis 

D. Metode Peringkasan Teks Otomatis 
teks 

ini 
dalam 
Peringkasan 
menggunakan 
teknik  ekstraksi,  dimana  hasil  ringkasan 
merupakan  gabungan  dari  kalimat-kalimat  terpenting  yang 
terdapat dalam dokumen berita. Selanjutnya, dalam pembuatan 
peringkasan  teks  otomatis  ini  akan  melalui  beberapa  tahapan 
sebagai berikut: 
1.  Preprocessing 

penelitian 

Data sekunder dalam penelitian ini adalah kumpulan 
berita  berdasarkan  kata  kunci  tertentu  yang  berasal  dari 
empat  media  daring,  yakni  Detik,  Kompas,  Antara  dan 
Bisnis.com. Data sekunder ini kemudian diproses menjadi 
ringkasan. 

Preprocessing dilakukan untuk mengubah data teks agar 
lebih  terstruktur  dan  agar  dapat  digunakan  untuk  proses 
selanjutnya.  Dalam  penelitian  ini,  tahapan  preprocessing 
yang dilakukan adalah sebagai berikut: 
•  Case folding, yakni mengubah seluruh kata dalam teks 

3. Ringkasan Manual 

menjadi huruf kecil.  

Ringkasan  manual  digunakan  sebagai  dasar  evaluasi 
terhadap 
ringkasan  yang  dihasilkan  oleh  sistem. 
Ringkasan manual bersumber dari enam partisipan yang 
seluruhnya  memiliki  latar  belakang  pendidikan  S1  pada 
program  studi  Sastra  Indonesia  dan  Pendidikan  Bahasa 
Indonesia. 

C. Metode Analisis 

Metode analisis yang digunakan dalam penelitian ini adalah 
metode  Design  -  Oriented  Research.  Design  -  Oriented 
Research berfokus pada evaluasi yang dilakukan pada artefak 
hasil  perancangan  [13].  Artefak  harus  memenuhi seperangkat 
kriteria  demi  memenuhi  kepuasan  pengguna  dan  pemangku 
kepentingan.  

Tahap analisis ini akan melalui lima tahap sebagai berikut: 
1.  Awareness of Problem  

Identifikasi  masalah  dilakukan  dengan  melakukan 
wawancara  dengan  pemangku  kepentingan  (subject 
matter),  yakni  Biro  Hubungan  Masyarakat  (Humas) 
BPS RI.  
2.  Suggestion 

Berdasarkan 

permasalahan, 
selanjutnya  akan  dirancang  sistem  usulan  berupa 

identifikasi 

hasil 

•  Stopword  Removal,  yakni  membuang  kata-kata  yang 
dianggap  tidak  memiliki  makna.  Stopword  Removal 
dilakukan  dengan  menggunakan  Python  dan  package 
Sastrawi. 
•  Tokenize, 

kalimat  menjadi 
potongan-potongan kata atau kalimat tersendiri. Dalam 
penelitian  ini,  tokenisasi  dilakukan  per  kalimat  dan 
menggunakan Python dan package NLTK. 

yakni  memisahkan 

•  Stemming, yakni mengubah seluruh kata hingga menjadi 
kata  dasarnya  saja  dengan  membuang  imbuhan  yang 
melekat  pada  kata.  Stemming  dilakukan  dengan 
menggunakan  Python  dan  Algoritma  Nazief  dan 
Adriani dalam package Sastrawi. 

2.  Pembuatan TF-IDF 
TF-IDF 

- 

(Term  Frequency 

Inverse  Document 
Frequency)  adalah  metode  yang  digunakan  dalam 
merepresentasikan dokumen menjadi bentuk vektor dengan 
menghitung bobot setiap kata yang paling sering digunakan 
dalam  dokumen  tersebut.  Proses  pembuatan  matriks  TF-
IDF  menggunakan  Python  dan  modul  TfidfVectorizer 
dalam package scikit-learn. 

3.  Proses Peringkasan dan Pemilihan Kalimat 

Untuk  menghasilkan  ringkasan  terbaik,  penelitian  ini 

 3 / 8 

 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

setiap  baris  pada  matriks  V  dihitung  rata-ratanya  dan 
selanjutnya, matriks diurutkan dari kalimat yang memiliki 
nilai rata-rata terbesar hingga terkecil. 

Jumlah kalimat  yang diringkas  ditentukan  berdasarkan 
compression  rate  dan  diambil  berdasarkan  nilai  length 
(pada metode pemilihan kalimat Steinberger & Jezek) dan 
nilai rata-rata (pada metode pemilihan kalimat avesvd) yang 
terbesar.  Compression  rate  yang  digunakan  dalam 
penelitian ini adalah 30%. 

4.  Evaluasi Hasil Peringkasan Teks 

Metode peringkasan teks dan metode pemilihan kalimat 
tersebut kemudian diperbandingkan dengan ringkasan ideal 
yang  dibuat  oleh  manusia  melalui  evaluasi  hasil 
peringkasan  teks  otomatis  menggunakan  nilai  ROUGE 
(Recall-Oriented  Understudy  for  Gisting  Evaluation). 
Penelitian  ini  menggunakan  dua  jenis  ROUGE.  Pertama 
yakni  ROUGE-N  yang  mengukur  frekuensi  kecocokan  n-
gram  antara  teks  yang  dihasilkan  oleh  peringkasan  teks 
otomatis  dengan  ringkasan  ideal.  Adapun  jumlah  n-gram 
yang  digunakan  adalah  unigram  (ROUGE-1)  dan  bigram 
(ROUGE-2).  Kedua  yakni  ROUGE-L  yang  mengukur 
frekuensi  kecocokan  urutan  kata  terpanjang  atau  Longest 
Common  Subsequence  (LCS)  antara  teks  yang  dihasilkan 
oleh peringkasan teks otomatis dengan ringkasan ideal.  

Selanjutnya,  hitung  f1-score  masing-masing  ROUGE. 
Kombinasi metode peringkasan teks dan metode pemilihan 
kalimat  terbaik  adalah  yang  memiliki  nilai  f1-score 
ROUGE  terbesar.  Metode  terbaik  tersebut  kemudian 
diimplementasikan ke dalam aplikasi. 

V.  KERANGKA PIKIR 
Kerangka pikir dalam penelitian ini adalah sebagai berikut. 

akan  menggunakan  beberapa  metode.  Peringkasan  teks 
otomatis  dalam  penelitian  ini  menggunakan  dua  metode, 
yakni Latent Semantic Analysis dan Cross Latent Semantic 
Analysis. Latent Semantic Analysis (LSA) adalah salah satu 
metode yang dapat digunakan untuk membuat peringkasan 
teks  otomatis  dengan  cara  menelusuri  dan  memodelkan 
hubungan  antara  kata  dan  kalimat  [10].  Sedangkan  Cross 
Latent 
(CLSA)  merupakan 
Analysis 
pengembangan metode Latent Semantic Analysis [7][8]. 

Semantic 

Kedua  metode  peringkasan  teks  otomatis  di  atas 
memanfaatkan  matriks  hasil  dekomposisi.  Dekomposisi 
sendiri adalah suatu proses untuk mengubah suatu matriks 
menjadi  faktor-faktornya  berupa  matriks  yang  lebih  dari 
satu  [14].  Dalam  penelitian  ini,  jenis  dekomposisi  yang 
digunakan  adalah  Dekomposisi  Nilai  Singular  (Singular 
Value Decomposition). 

𝐴 = 𝑈  ∑ 𝑉𝑇                                  (1)  

Berdasarkan formula di atas, matriks U, ∑ dan VT adalah 
hasil dekomposisi dari sebuah matriks Amxn. Matriks U yang 
berukuran m x n dapat disebut dengan matriks singular kiri. 
Matriks  ∑  yang  berukuran  n  x  n  merupakan  matriks 
diagonal, dimana elemen diagonalnya adalah nilai singular 
tunggal non-negatif pada matriks A. Sedangkan matriks VT 
yang berukuran n x n dapat disebut dengan matriks singular 
kiri.  Proses  SVD  dilakukan  dengan  menggunakan  Python 
dan modul np.linalg.svd dalam package numpy. 

 Teks  yang  telah  didekomposisi,  selanjutnya  diproses 
dengan  metode  LSA  dan  CLSA.  Proses  LSA  dilakukan 
dengan menggunakan matriks VT hasil dari Singular Value 
Decomposition  yang  selanjutnya  ditranspose  menjadi 
matriks  V.  Proses  CLSA  juga  menggunakan  matriks  VT. 
Namun,  matriks  tersebut  dimodikasi  terlebih  dahulu, 
dimana  apabila  nilai  sel  dalam  matriks  VT  memiliki  nilai 
yang  lebih  kecil  dibanding  nilai  rata-rata  setiap  baris 
matriks,  maka  nilai  sel  tersebut  diubah  menjadi  nol  (0). 
Kemudian  matriks 
sehingga 
tersebut 
membentuk matriks V yang baru. 

ditranspose 

Setelah proses LSA dan CLSA, tahap selanjutnya adalah 
proses  pemilihan  kalimat  ringkasan.  Pemilihan  kalimat 
ringkasan  menggunakan  dua  metode  pemilihan  kalimat, 
yakni metode Steinberger & Jezek dan avesvd [9][10].  

Pada  metode  pemilihan  kalimat  Steinberger  &  Jezek, 
setiap  baris  pada  matriks  V  dihitung  length-nya  dengan 
menggunakan  Persamaan  2  berikut,  dimana ∑  merupakan 
matriks  diagonal  hasil  SVD.  Lalu,  matriks  diurutkan  dari 
kalimat yang memiliki length terbesar hingga terkecil. 

𝑛

𝑙𝑒𝑛𝑔𝑡ℎ =   √∑ 𝑉𝑖𝑗

2 ∗  ∑𝑗𝑗

2 

                     (2) 

𝑗=1

Gambar 2. Kerangka Pikir  

VI. HASIL DAN PEMBAHASAN 

Sedangkan  pada  metode  pemilihan  kalimat  avesvd, 

A. Analisis Permasalahan 

Analisis  permasalahan  digunakan  untuk  mengidentifikasi 
seluruh  permasalahan  yang  melatarbelakangi  penelitian  ini. 

 4 / 8 

 
 
 
 
 
 
 
 
Analisis  permasalahan  dilakukan  bersama  dengan  subject 
matter, dalam hal ini adalah Humas BPS.  

Berdasarkan  hasil  wawancara  dengan  pihak  Humas  BPS, 
ditemukan  beberapa  permasalahan.  Secara  garis  besar, 
permasalahan  yang  ditemukan  ada  dua.  Pertama  yakni 
penggunaan pihak ketiga dalam pengumpulan berita yang lama 
dan  berbayar  dan  yang  kedua  yakni  peringkasan  teks  secara 
manual  tidak  efisien.  Kedua  permasalahan  tersebut  dipetakan 
dalam Diagram Ishikawa di bawah ini. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

dan  keterampilan  dalam  meringkas  teks  berita  agar  hasilnya 
bagus  dan  dapat  merepresentasikan  isi  teks  sesungguhnya. 
Kegiatan  meringkas  teks  berita  secara  manual  juga  menguras 
tenaga  dan  rentan  akan  kelelahan.  Dari  sisi  Methods, 
peringkasan  teks  berita  secara  manual  dirasa  kurang  efisien. 
Peringkasan teks secara manual juga memakan banyak waktu, 
terutama dalam membaca teks berita. Terlebih, teks berita yang 
perlu  diringkas  jumlahnya  tidak  sedikit.  Dari  sisi  Machines, 
peringkasan teks secara manual dilakukan karena Humas BPS 
tidak  memiliki  alat  yang  dapat  meringkas  teks  berita  secara 
otomatis. Adapun dari sisi Materials, jumlah berita yang perlu 
diringkas sangat banyak. 

B. Analisis Kebutuhan 

Berdasarkan  permasalahan 

tersebut,  selanjutnya  dapat 
dirumuskan  kebutuhan  dari  Humas  BPS  yang  harus  tersedia 
pada sistem atau aplikasi yang akan dibuat dalam penelitian ini 
•  Sistem  dapat  mengumpulkan  berita  dari  media  daring 
melalui modul pengumpul berita berdasarkan kata kunci 
tertentu. 

Gambar 3. Diagram Ishikawa pada Permasalahan Pertama 

•  Sistem  dapat  melakukan  peringkasan  berita  secara 

otomatis. 

•  Sistem  dapat  menghasilkan  output  berupa  laporan 
seperti yang selama ini dilakukan oleh Humas BPS. 

C. Perancangan Sistem Usulan 

 Rancangan arsitektur sistem pada penelitian ini dapat dilihat 

pada Gambar 5 di bawah ini. 

Berdasarkan  Diagram  Ishikawa  pada  Gambar  3  di  atas, 
mengenai  permasalahan  penggunaan  pihak  ketiga  dalam 
pengumpulan  berita  yang 
lama  dan  berbayar  dapat 
diidentifikasi  menjadi  dua  faktor  penyebab,  yakni  dari  sisi 
Materials  dan  dari  sisi  Methods.  Dari  sisi  Materials, 
pengumpulan  berita  tidak  bisa  dilakukan  secara  cepat  karena 
Humas  BPS  perlu  menunggu  setoran  berita  yang  dikirimkan 
oleh pihak ketiga. Selain itu, karena menggunakan jasa pihak 
ketiga, tentunya ada biaya. 

Dari  sisi  Methods,  Humas  BPS  menggunakan  pihak  ketiga 
dalam  mengumpulkan  berita  karena  belum  tersedianya  web 
crawler maupun web scraper yang dapat mengumpulkan berita 
secara cepat. 

Gambar 4. Diagram Ishikawa pada Permasalahan Kedua 

Selanjutnya, berdasarkan Diagram Ishikawa pada Gambar 4 
di atas, mengenai peringkasan teks secara manual tidak efisien 
dapat diidentifikasi menjadi empat faktor penyebab, yakni dari 
sisi Man, Methods, Machines dan Materials.  

Dari sisi Man, Humas BPS tentunya perlu untuk membaca 
teks secara menyeluruh. Peringkas juga perlu memiliki teknik 

Gambar 5. Rancangan Arsitektur Sistem 

berjudul 

Peringkasan dokumen berita otomatis dalam penelitian ini, 
dalam pengimplementasiannya akan menjadi sebuah fitur atau 
modul pada aplikasi bernama “Evaluasi Respon Publik” yang 
telah dibangun oleh Novi Kanadia (2019) dalam penelitiannya 
yang 
“Pengembangan  Model  Named-Entity 
Recognition pada Berita Indikator-indikator BPS untuk Social 
Network  Analysis”.  Maka  dari  itu,  web  framework  yang 
digunakan  dalam  penelitian  ini  akan  menyesuaikan  dengan 
aplikasi tersebut, yakni Codeigniter dan Flask [15]. CodeIgniter 
digunakan untuk merancang aplikasi berbasis web, sedangkan 
Flask  merupakan  web  framework  berbasis  Python  yang 
digunakan untuk pengolahan data pada beberapa fungsi. Dalam 

 5 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
  
penelitian ini, Flask dimanfaatkan untuk proses webcrawling - 
web scraping dan peringkasan teks otomatis. 

Gambar 6. Use Case Diagram 

Melalui Rancangan Use Case pada Gambar 6 di atas, terlihat 
bahwa  User  dapat  melakukan  dua  aktivitas  utama,  yakni 
menjalankan  modul  pengumpul  berita  dan  menjalankan 
peringkasan teks otomatis. 

D. Pengembangan Peringkasan Teks Otomatis 

Berikut  ini  adalah  kutipan  dari  proses  peringkasan  teks 
otomatis pada artikel berita, mulai dari  preprocessing sampai 
menjadi ringkasan berita yang utuh. 

Indeks  Harga  Saham  Gabungan  (  )  sore  ini  ditutup  menguat  59  poin 
(0,99%) ke level 6.091. Indeks LQ45 juga menguat 14 poin (1,64%) ke 
level  919.  IHSG  berada  di  level  tertingginya  6.091  dan  terendahnya 
6.005. Sebanyak 235 saham naik, 258 turun dan 157 stagnan… 

Sumber:  https://finance.detik.com/bursa-dan-valas/d-5592135/naik-
dari-siang-ihsg-ditutup-menguat-ke-6091 

1.  Preprocessing 
Dalam 

penelitian 

dilakukan 

tahapan 
ini, 
preprocessing,  mulai  dari  case  folding,  stopword 
removal,  tokenize  dan  stemming.  Teks  berita  juga 
diubah menjadi tipe data list, dimana setiap anggotanya 
merupakan  kalimat  yang  telah  melalui  preprocessing. 
Kutipan hasilnya adalah sebagai berikut. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

TABEL II 
BENTUK MATRIKS TF-IDF 

Kalimat 1 

Kalimat 2 

… 

Kalimat 14 

ada 
0.000000 
0.000000 

… 
0.243932 

amerika 
0.000000 
0.000000 

… 
0.000000 

anggur  … 
0.00000  … 
0.00000  … 

… 

… 
0.00000  … 

zona 
0.00000 
0.00000 

… 
0.00000 

3.  Proses Peringkasan dan Pemilihan Kalimat 

Selanjutnya  dilakukan  dekomposisi  dengan  SVD. 
Matriks hasil SVD selanjutnya digunakan pada metode 
peringkasan  LSA  dan  CLSA  dan  metode  pemilihan 
kalimat Steinberger & Jezek dan avesvd. 

Hasil  metode  peringkasan  LSA  metode  pemilihan 
kalimat Steinberger & Jezek dapat dilihat pada Tabel III. 
Selanjutnya  ambil  beberapa  kalimat  berdasarkan 
compression  rate,  yakni  30%.  Misalnya,  berita  ini 
terdiri  dari  14  kalimat.  Maka  jumlah  kalimat  hasil 
ringkasannya adalah sebanyak 4 kalimat. 

TABEL III 
PEMILIHAN KALIMAT DENGAN METODE STEINBERGER & JEZEK 
length 
1.000120 

… 
-0.165997  … 

0 
-0.519684 

13 
-7.07E-01 

1 

-0.519684 

-0.165997  … 

7.07E-01 

1.000120 

-0.23145 

-0.196857  … 

-1.11E-17 

1.000091 

-0.112645  0.414437  … 

6.10E-17 

1.000040 

Kalimat 2 
Kalimat 8 
Kalimat 7 
Kalimat 10 

… 
Kalimat 4 

… 

… 

… 

… 

… 

-0.091694  0.037918  … 

-9.80E-18 

0.999882 

Sedangkan hasil metode peringkasan LSA dan metode 

pemilihan kalimat avesvd dapat dilihat pada Tabel IV. 

TABEL IV 
PEMILIHAN KALIMAT DENGAN METODE AVESVD 

Kalimat 3 

Kalimat 2 
Kalimat 6 
Kalimat 10 
… 
Kalimat 7 

0 
-0.15626 

1 

… 
-0.200681  … 

13 
1.16E-16 

average 
0.134668 

-0.519684 

-0.165997  … 

-7.07E-01 

0.121859 

-0.317032 

-0.27103  … 

-1.06E-16 

0.11622 

-0.112645  0.414437  … 

6.10E-17 

0.097281 

… 

… 

… 

… 

… 

-0.23145 

-0.196857  … 

-1.11E-17 

0.010727 

Lalu, pada metode CLSA, hasil matriks V yang baru 

dapat dilihat pada Tabel V berikut. 

TABEL V 
MATRIKS PADA METODE CLSA 
0 
0 

… 
0.137342  … 

1 

12 
0.230487 

0 

… 

-0.009904 

0 

… 

0 

… 

… 

… 

0 

… 

0 

13 
5.99E-17 

0.00E+00 

… 

1.40E-16 

-2.03E-01 

3.88E-02  … 

1.32E-02 

-1.06E-17 

['indeks harga saham gabung sore tutup kuat poin level', 
 'indeks lq kuat poin level', 
 'ihsg ada level tinggi rendah', 
… 
 'ini  investor  perhati  pulih  ekonomi  tingkat  inflasi  duga  masih 
ada atas target the fed'] 

Kalimat 1 
Kalimat 2 

… 
Kalimat 14 
average 

2.  Pembuatan TF-IDF 

Tabel II di bawah ini adalah contoh hasil konversi teks 

di atas menjadi bentuk TF-IDF. 

Matriks V yang baru tersebut kemudian dapat diproses 
pemilihan  kalimatnya,  baik  menggunakan  metode 
Steinberger & Jezek, maupun avesvd. 

 6 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
E. Evaluasi Hasil Peringkasan Teks Otomatis 

Evaluasi  hasil  peringkasan  teks  otomatis  menggunakan 
metode  evaluasi  intrinsik,  yakni  dengan  membandingkan 
antara  ringkasan  yang  dibuat  secara  otomatis  dan  ringkasan 
manual buatan manusia.  

ini 
Untuk  menghindari  adanya  subjektivitas,  evaluasi 
melibatkan  enam  orang  partisipan  yang  seluruhnya  memiliki 
latar belakang pendidikan S1 Sastra Indonesia dan Pendidikan 
Bahasa dan Sastra Indonesia. Seluruh partisipan diminta untuk 
melakukan peringkasan teks berita secara manual. Teks berita 
yang  diringkas  berjumlah  10  buah  teks  yang  bersumber  dari 
berbagai situs media daring. 

Setelah  ringkasan  manual  dari  seluruh  partisipan  berhasil 
dikumpulkan, selanjutnya hitung nilai  f1-score ROUGE, baik 
ROUGE-1, ROUGE-2 dan ROUGE-L 

Metode 
Peringkasan 

LSA 

LSA 

CLSA 

CLSA 

Metode 
Pemilihan 
Kalimat 
Steinberger 
& Jezek 
avesvd 
Steinberger 
& Jezek 
avesvd 

TABEL VI 
HASIL ROUGE 

f1-score 

ROUGE-1  ROUGE-2  ROUGE-L 

0.50179 

0.40444 

0.52373 

0.49311 

0.39445 

0.51853 

0.44680 

0.35291 

0.48101 

0.51892 

0.42842 

0.54177 

Berdasarkan hasil evaluasi, didapatkan metode peringkasan 
teks CLSA dengan metode pemilihan kalimat avesvd memiliki 
nilai  ROUGE  tertinggi  dibanding  kombinasi  metode  lainnya, 
baik pada ROUGE-1, ROUGE-2 dan ROUGE-L. Selanjutnya, 
metode terbaik ini akan diimplementasikan pada sistem. 

F. Implementasi Sistem  

Berikut  ini  adalah  tangkapan  layar  hasil  implementasi 
sistem.  Sebelumnya,  dapat  dilihat  bahwa  aplikasi  bernama 
“Evaluasi  Respon  Publik”  ini  memiliki  beragam  fitur  yang 
dibuat oleh peneliti sebelumnya. 

Untuk  mengakses  modul  pengumpul  berita  berikut 
peringkasan teks otomatis, pilih menu “Peringkas Teks” yang 
ada pada sidebar. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

berita  yang  memiliki  kata  kunci  “Inflasi”.  Setelah  itu  tekan 
tombol “Scrap Berita”. 

Gambar 8. Tampilan daftar berita hasil pengumpulan berita 

Setelah itu, muncul daftar berita yang berhasil dikumpulkan 
dari keempat media daring seperti yang ditunjukkan Gambar 8. 
User dapat melakukan pengunduhan berita dalam format .xlsx 
atau  .pdf  dan  menghapus  berita  yang  tidak  diinginkan  untuk 
diringkas nantinya. User juga dapat melihat detail berita seperti 
yang ditunjukkan Gambar 9 di bawah ini. 

Gambar 9. Tampilan detail berita 

Selanjutnya,  untuk  melakukan  peringkasan  berita,  pada 
daftar berita hasil pengumpulan berita pada Gambar 8 di atas, 
klik  tombol  “Ringkas  Berita”.  Hasil  dari  proses  peringkasan 
berita dapat ditunjukkan pada Gambar 10 di bawah ini. 

Gambar 7. Tampilan modul pengumpul berita 

Gambar 10. Tampilan daftar berita hasil peringkasan 

Gambar 7 di atas adalah tampilan awal modul pengumpul 
berita.  User  diminta  untuk  melakukan  input  kata  kunci  yang 
terkait dengan berita yang ingin dikumpulkan. Pada Gambar 7 
di  atas,  dicontohkan  berita  yang  ingin  dikumpulkan  adalah 

Sama  seperti  pada  modul  pengumpulan  berita,  User  juga 
dapat melakukan pengunduhan berita dalam format .xlsx atau 
.pdf.  User  juga  dapat  melihat  detail  berita  yang  ditunjukkan 
Gambar 11 di bawah ini. 

 7 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
Gambar 11. Tampilan detail berita hasil peringkasan 

Dari  Gambar  11  di  atas,  dapat  dilihat  detail  berita  hasil 
peringkasan.  Hasil  peringkasan  menggunakan  metode 
peringkasan  terbaik,  yakni  CLSA  dengan  metode  pemilihan 
kalimat avesvd pada berita tersebut yakni: 

IHSG berada di level tertingginya 6.091 dan terendahnya 6.005. 
Dow  Jones  ditutup  34.600,38  (+0,07%),  NASDAQ  ditutup 
13.756,33  (+0,14%),  S&P  500  ditutup  4.208,12  (+0,10%). 
Investor  tidak  melakukan  banyak  transaksi  yang  kemungkinan 
kelanjutan dari libur sejak awal minggu. Saat ini fokus investor 
adalah  menanti  data  mingguan  yaitu  data  pengangguran  dan 
komentar dari Jerome Powel selaku Gubernur The Fed. 

G. Evaluasi Sistem 

Evaluasi  sistem  dalam  penelitian  ini  menggunakan  black-
box  testing.  Tujuannya  adalah  untuk  meninjau  dan  menguji 
apakah  sistem  yang  telah  diimplementasikan  sudah  sesuai 
dengan harapan peneliti. Hasil black-box testing  dapat dilihat 
pada tabel di bawah ini. 

TABEL VII 
HASIL BLACK-BOX TESTING 

No. 

1. 

3. 

4. 

5. 

Deskripsi 
Pengujian 
Masuk ke menu 
“Peringkas Teks” 
Menjalankan modul 
pengumpul berita. 
Klik “Scrap Berita” 

Mengunduh hasil 
pengumpulan berita 

Menghapus berita 
terpilih 

6.  Melihat detail berita 

Menjalankan 
peringkasan teks 
berita. Klik 
“Ringkas Teks” 
Mengunduh hasil 
peringkasan teks 
berita 
Melihat detail berita 
hasil ringkasan 

7. 

8. 

9. 

10.  

Mengakses URL 
berita 

Hasil yang diharapkan 

Muncul isian kata kunci untuk 
pengumpul berita 

Muncul daftar berita hasil 
pengumpul berita 

Berita dapat diunduh, baik 
dengan format .xlsx maupun 
.pdf 
Berita yang dihapus, hilang 
dari daftar berita 
Dapat melihat detail berita, 
berisi judul, tanggal, isi berita 
dan sumber 

Hasil 
Aktual 

Berhasil 

Berhasil 

Berhasil 

Berhasil 

Berhasil 

Muncul daftar berita beserta 
ringkasannya 

Berhasil 

Berita hasil ringkasan dapat 
diunduh, baik dengan format 
.xlsx maupun .pdf 
Dapat melihat detail berita, 
hasil ringkasan 
Muncul tab baru pada 
peramban yang mengarah pada 
URL berita 

Berhasil 

Berhasil 

Berhasil 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Berdasarkan  hasil  black-box 

testing  di  atas,  dapat 
disimpulkan  bahwa  seluruh  fungsi  pada  aplikasi  yang  telah 
dibuat dapat berjalan sesuai harapan peneliti.  

VII. PENUTUP 

Berdasarkan  hasil  dan  pembahasan  penelitian  ini,  dapat 

ditarik beberapa kesimpulan sebagai berikut: 
1.  Modul  pengumpul  berita  untuk  mendapatkan  berita  dari 
media  daring  berhasil  dibuat  dengan  menggunakan  web 
framework Flask. 

teks 

rata-rata  nilai 

2.  Peringkasan 

secara  otomatis  berhasil  dibuat. 
Berdasarkan  evaluasi  hasil  peringkasan  teks  otomatis 
menggunakan ROUGE, metode peringkasan CLSA dengan 
metode  pemilihan  kalimat  avesvd  adalah  yang  terbaik 
f1-score  ROUGE-1  0.51892, 
dengan 
ROUGE-2  0.42842  dan  ROUGE-L  0.54177.  Selanjutnya, 
metode terbaik tersebut diimplementasikan pada aplikasi.  
3.  Aplikasi  antar  muka  peringkasan  teks  otomatis  berbasis 
web  berhasil  diimplementasikan  serta  diintegrasikan 
dengan modul pengumpul berita untuk mendapatkan berita 
berdasarkan kata kunci dari situs media daring. 

DAFTAR PUSTAKA 
[1]  M. A. Adli and L. Firgia, “Rancang Bangun Web Scraping Pada Media 
Online Berita Nasional,” Jurnal ENTER, vol. 1, pp. 118–128, 2018. 
[2]  D. R. Radev, A. Winkel, and M. Topper, “Multi Document Centroid-based 

Text Summarization”, ACL 2002. 2002. 

[3]  A. Reshamwala, D. Mishra, and P. Pawar, “Review on Natural Language 
IRACST  Engineering  Science  and  Technology:  An 

Processing”. 
International Journal (ESTIJ), vol. 3, no. 1, pp. 113-116. 2013. 

[4]  Ponimin.  “Pengembangan  Sistem  Pengumpul  Terjadwal  dan  Aplikasi 
Dashboard untuk Data Berita”. Bachelor thesis. Program Studi Komputasi 
Statistik, Politeknik Statistika STIS, Jakarta, 2020. 

[5]  Rapif,  D.  F.,  “Pembangunan  Aplikasi WMMS Modul Text  Summarizer 
Bahasa Indonesia dengan LDA dan GA”. Bachelor thesis. Program Studi 
Komputasi Statistik, Politeknik Statistika STIS, Jakarta, 2017. 

[6]  Syafrizal,  M.,  “Aplikasi  Lembar  Kerja  PDB:  Penerapan  Text 
Summarization  Untuk  Analisis  PDB”.  Bachelor  thesis.  Program  Studi 
Komputasi Statistik, Politeknik Statistika STIS, Jakarta, 2019. 

[7]  G. Mandar and Gunawan, ""Peringkasan dokumen berita Bahasa Indonesia 
menggunakan  metode  Cross  Latent  Semantic  Analysis"",  Jurnal  Ilmiah 
Teknologi Sistem Informasi, pp. 94-104, vol. 3, no.2, 2018. 

[8]  Ozsoy,  M.  G.,  Cicekli,  I.,  &  Alpaslan,  F.  N.  “Text  summarization  of 
Turkish  texts  using  Latent  Semantic  Analysis”.  Coling  2010  -  23rd 
International  Conference  on  Computational  Linguistics,  Proceedings  of 
the Conference, 2(May), 869–876. 2010. 

[9]  Steinberger,  J.  and  Ježek,  K.  “Using  Latent  Semantic  Analysis  in  Text 
Summarization”. In Proceedings of ISIM 2004, May 2014, 93--100. 2004. 
[10] Dokun, O. and Celebi, E. “Single-Document Summarization Using Latent 
1–13. 

Semantic 
https://doi.org/10.13140/RG.2.1.4075.6320. 2015. 

Analysis”. 

1(2), 

[11] Saziyabegum, S. and Sajja, P. “Review on Text Summarization Evaluation 
Methods”. Indian Journal of Computer Science and Engineering (IJCSE), 
8(4), 497-500, 2017. 

[12] Lin, C-Y. “ROUGE: A Package for Automatic Evaluation of summaries”. 
Proceedings of the ACL Workshop: Text Summarization Braches Out 2004. 
10. 2004. 

[13] Verschuren, P. and Hartog, R. “Evaluation in Design-Oriented Research. 
Quality and quantity”: international journal of methodology 39 (2005) 6. 
39. 10.1007/s11135-005-3150-6. 2005. 

[14] Ariyanti,  G.  “Dekomposisi  Nilai  Singular  dan  Aplikasinya”,  Prosiding 
Seminar Nasional Matematika dan Pendidikan Matematika, 33-39. 2010. 
[15] Kanadia  N.,  “Pengembangan  Model  Named-Entity  Recognition  pada 
Berita Indikator-indikator BPS untuk Social Network Analysis”. Bachelor 
thesis.  Program  Studi  Komputasi  Statistik,  Politeknik  Statistika  STIS, 
Jakarta, 2019. 

 8 / 8 

 
 
 
"
221709980,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Penerapan Named Entity Recognition (NER) melalui 
Twitter pada SpaCy Bahasa Indonesia 
(Studi Kasus : Gangguan Listrik di Daerah Istimewa Yogyakarta) 

Rizka Maulida Yanti (221709980, 4SI1) 
Dosen Pembimbing: Ibnu Santoso, SST, MT 

terkait  gangguan 

Ringkasan—  SpaCy  merupakan  tools  yang  dapat  menangani 
masalah Natural Language Processing (NLP) dengan efisien, salah 
satunya adalah Named Entity Recognition (NER). NER digunakan 
untuk  mengekstrak  dan  mengidentifikasi  entitas  bernama  pada 
suatu teks. Namun, sejauh ini SpaCy belum merilis pre-train model 
NER  untuk  Bahasa  Indonesia  secara  resmi.  Di  sisi 
lain, 
berdasarkan  laporan  statistik  PLN  tahun  2019,  Provinsi  D.I. 
Yogyakarta  merupakan  provinsi  yang  sering  mengalami 
gangguan listrik serta banyak ditemukannya keluhan masyarakat 
di  Twitter  terkait  gangguan  listrik  yang  terjadi  di  provinsi 
tersebut. Dikarenakan belum ditemukannya penelitian ekstraksi 
informasi 
jarang 
ditemukannya  penelitian  tentang  NER  dengan  menggunakan 
ini  akan 
SpaCy  Bahasa  Indonesia.  Maka  pada  penelitian 
dilakukan ekstraksi informasi terkait gangguan listrik di Provinsi 
D.I.  Yogyakarta  melalui  Twitter  dengan  menggunakan  SpaCy 
Bahasa  Indonesia.  Penelitian  ini  menghasilkan  performa  hasil 
yang  baik  dengan  hasil  perhitungan  precision  95.52%,  recall 
93.27%,  dan  f1-score  94.38%..  Kemudian,  dilakukan  pemetaan 
berdasarkan  entitas  lokasi  yang  terdapat  dalam  tweet  terkait 
gangguan  listrik.  Dari  proses  tersebut  didapat  bahwa  jumlah 
lokasi yang tersebutkan di tweet terkait gangguan listrik tertinggi 
berasal  dari  Kabupaten  Sleman,  sedangkan  jumlah  terendah 
berasal dari Kabupaten Gunung Kidul. Lalu, bulan yang paling 
banyak  mengalami  gangguan  listrik  adalah  Bulan  Maret  2020, 
sedangkan yang paling sedikit adalah Bulan Juli 2020. 

serta  masih 

listrik 

Kata  Kunci—  Information  Extraction,  NER,  SpaCy,  Twitter, 

Gangguan Listrik. 

I.  LATAR BELAKANG 

Bahasa  merupakan  suatu  alat  komunikasi  yang  penting. 
Komunikasi adalah proses pertukaran informasi antar manusia 
dengan  bantuan  bahasa.  Pesatnya  perkembangan  teknologi 
informasi  memberikan  kebebasan  lebih  kepada  masyarakat 
untuk menyampaikan pendapat atau keluhannya di media sosial. 
Terdapat  miliaran  informasi  yang  didistribusikan  melalui 
berbagai  teknologi  dan  beberapa  di  antaranya  menggunakan 
bahasa  alami.  Bahasa  alami  adalah  bahasa  yang  biasa 
digunakan  manusia  untuk  berkomunikasi  satu  sama  lain. 
Bahasa  yang  diterima  oleh  komputer  harus  diproses  dan 
dipahami  sebelum  komputer  dapat  memahami  maksud 
pengguna  dengan  benar.  Artificial 
(AI) 
berkembang pesat dalam  bidang penelitian dan penerapannya 
di dunia nyata. Sebagai contoh, salah satu perkembangan pesat 
di  bidang  komputasi  bahasa  adalah  Natural  Language 
Processing (NLP). NLP adalah cabang AI yang berfokus pada 
pemrosesan  bahasa  alami  dengan  pendekatan  komputerisasi 

Intelligence 

yang  digunakan  untuk  menganalisis  data,  teks,  ucapan,  dll. 
Oleh  karena  itu,  NLP  dapat  melakukan  pemrosesan  bahasa 
alami seperti bahasa manusia [1]. 

Seiring  berkembangnya  internet,  ketersediaan  data  online 
pun ikut berkembang, terutama data tekstual. Data teks tersebut 
mengandung banyak informasi. Namun, informasi dalam teks 
seringkali  tidak  terlihat  dikarenakan  bentuk  teks  yang  tidak 
terstruktur.  Sehingga,  diperlukan  ekstraksi  informasi,  yang 
merupakan  suatu  sistem  untuk  menemukan  data  spesifik  dari 
Natural  Language  Text  [2].  Contohnya  adalah  keluhan 
masyarakat  terkait  gangguan  listrik.  Dalam  berita  tersebut 
terkandung  beberapa  informasi  seperti  lamanya  gangguan 
listrik  yang  terjadi,  tanggal,  dan  lokasi  .  Namun,  semua 
informasi tersebut disembunyikan dalam bentuk kalimat. Oleh 
karena  itu,  untuk  mendapatkan  informasi  penting  ini,  perlu 
melakukan  analisis  terhadap  keseluruhan  teks.  Jika  keluhan 
tersebut  dalam  jumlah  yang  sangat  banyak,  tentu  akan  sulit 
untuk  menganalisisnya.  Dari  melimpahnya  data  tersebut  dan 
informasi  yang  dikandungnya, 
timbul  kebutuhan  untuk 
mengekstraksi informasi secara otomatis dari data yang berupa 
teks [3].  

Data teks yang turut mengalami perkembangan adalah data 
yang  terdapat  pada  Twitter.  Twitter  merupakan  salah  satu 
media  sosial  yang  saat  ini  berkembang  pesat,  dikarenakan 
memudahkan  pengguna  untuk  berkomunikasi.  Referensi  [4] 
menunjukkan  bahwa  pada  bulan  Mei  2016,  Indonesia 
merupakan  salah  satu  negara  dengan  jumlah  pengguna  aktif 
Twitter terbesar ke-3 di dunia. Jumlah pengguna aktif Twitter 
di  Indonesia  pada  bulan  Mei  2016  mencapai  24,34  juta 
pengguna. Kemudian disusul oleh India dengan total pengguna 
aktif  mencapai  41,19  juta  pengguna.  Sementara  itu,  Amerika 
Serikat menjadi negara yang paling banyak memiliki pengguna 
aktif  Twitter  hingga  mencapai  67,54 
juta  pengguna. 
Meningkatnya  pengguna  Twitter  setiap  tahun,  menunjukkan 
bahwa  Twitter  merupakan  media  sosial  yang  sangat  populer. 
Oleh  karena 
itu,  dibutuhkan  suatu  metode  yang  dapat 
mengekstraksi  informasi-informasi  yang  terkandung  dalam 
tweet yang terdapat pada Twitter.  

Sehingga  pada  penelitian  ini,  peneliti  akan  menggunakan 
salah  satu  task  NLP  yaitu  Information  Extraction  (IE).  IE 
memiliki  salah  satu  sub-task  yang  dapat  membantu  proses 
pengidentifikasian  dan  ekstraksi  informasi  informasi  yang 
berupa  entity,  atau  biasa  disebut  dengan  Named  Entity 
Recognition  (NER).  NER  digunakan  untuk  mengekstrak  dan 
mengidentifikasi  entitas  bernama  (orang,  lokasi,  organisasi) 
pada suatu teks. Pada penelitian ini, NER akan dikembangkan 

 1 / 7 

 
 
 
 
 
melalui  pendekatan  Machine  Learning  dengan  menggunakan 
metode  Supervised  Learning.  Dimana  model  akan  belajar 
mengenali pattern-pattern entitas dari data yang sudah dilabeli 
sebelumnya. Sehingga, nantinya NER dapat mengenali entitas 
dari suatu teks  yang telah dimasukkan ke dalam  model NER. 
Pembangunan  model pada penelitian  ini akan dibantu dengan 
library  SpaCy.  SpaCy  merupakan  sebuah  library  open  source 
yang menggunakan bahasa pemrograman Python dan berguna 
untuk menangani masalah NLP dengan cara yang efisien, salah 
satunya  ialah  NER.  Model  SpaCy  sudah  tersedia  dalam 
beberapa  bahasa  diantaranya  Bahasa  Inggris,  Bahasa  Jerman, 
Bahasa  Spanyol,  Bahasa  Portugis,  Bahasa  Prancis,  Bahasa 
Italia,  Bahasa  Belanda,  Bahasa  Yunani,  Bahasa  Cina,  Bahasa 
Jepang,  Bahasa  Rusia,  Bahasa  Romania,  Bahasa  Denmark, 
Bahasa  Polandia,  dan  Bahasa  Norwegia.  Sejauh  ini,  SpaCy 
belum  merilis  pre-train  model  NER  untuk  Bahasa  Indonesia 
secara resmi.  

Selain itu, masih jarang ditemukan penelitian tentang NER 
yang  menggunakan  SpaCy  Bahasa  Indonesia  sebagai  tools 
untuk  membuat  model  NER.  Hal  tersebut  membuat  peneliti 
tertarik  untuk  melakukan  penelitian  ini.  Meskipun  SpaCy 
belum  merilis pre-train  model untuk Bahasa Indonesia secara 
resmi,  masih tetap bisa dilakukan training  model  dengan data 
yang  dimiliki.  Dimana,  data  training  tersebut  disesuaikan 
terlebih  dahulu  dengan  format  dari  SpaCy,  yang  selanjutnya 
akan di-train dengan beberapa iterasi sebelum dapat digunakan. 
Untuk  data  yang  digunakan  adalah  tweet  terkait  gangguan 
listrik  di  Provinsi  D.I.  Yogyakarta.  Peneliti 
tertarik 
menggunakan  data  tersebut  dikarenakan  listrik  merupakan 
salah  satu  kebutuhan  pokok  manusia  dan  hal  itu  terlihat  dari 
pola hidup manusia yang selalu bergantung pada listrik. Tanpa 
adanya  listrik,  semua  kebutuhan  dasar  manusia  tidak  dapat 
terpenuhi. Selain  itu, aktivitas komersial,  layanan publik, dan 
jaringan  telepon  akan  terganggu.  Sehingga,  dengan  adanya 
gangguan  listrik  akan  sangat  merugikan  masyarakat.  Pada 
penelitian  ini,  penulis  akan  menggunakan  SAIDI  (System 
Average  Interruption  Duration  Index)  dan  SAIFI  (System 
Interruption Frequency Index) sebagai indikator untuk melihat 
provinsi  yang  mengalami  gangguan 
tertinggi  di 
Indonesia. Referensi [5] menunjukkan bahwa pada tahun 2019, 
daerah  Yogyakarta  merupakan  daerah  dengan  SAIDI  dan 
SAIFI tertinggi ke-2 di Pulau Jawa setelah Jawa Barat. SAIDI 
merupakan rata-rata lama padam, sedangkan SAIFI merupakan 
rata-rata frekuensi padam. SAIDI dan SAIFI merupakan indeks 
yang  menunjukkan  tingkat  keandalan  dari  suatu  sistem 
distribusi  dalam  melayani  konsumen.  Semakin  kecil  nilai 
SAIDI  dan  SAIFI,  maka  keandalannya  semakin  baik. 
Begitupun sebaliknya. 

listrik 

II.  TUJUAN PENELITIAN 

Berdasarkan rumusan masalah di atas, tujuan dilakukannya 

penelitian ini adalah : 

1.  Untuk  membangun  model  NER  terkait  gangguan 
listrik  di  provinsi  D.  I.  Yogyakarta  spaCy  Bahasa 
Indonesia. 

2.  Untuk mengetahui evaluation metric dari model yang 

telah dibangun. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

3.  Untuk memetakan persebaran lokasi yang tersebutkan 
di  tweet  terkait  gangguan  listrik  di  provinsi  D.  I. 
Yogyakarta pada tahun 2020. 

4.  Untuk  mengetahui  perbandingan  jumlah  lokasi  yang 
tersebutkan  di  tweet  terkait  gangguan  listrik  tiap 
kabupaten/kota  di  provinsi  D.  I.  Yogyakarta  pada 
tahun 2020 tiap bulan nya.  

III. PENELITIAN TERKAIT 

Berikut  ini  merupakan  uraian  hasil  penelitian  sebelumnya 
yang berkaitan dengan topik skripsi dan akan disajikan dalam 
bentuk tabel serta peta literatur. 

TABEL I 
TABEL LITERATUR 

No 

Judul 

1.  Named 
Entity 
Recognition 
Approaches 
and Their 
Comparison 
for Custom 
NER Model 

Penulis, 
Publikasi 

Hemlata 
Shelar, 
Gagandeep 
Kaur, Neha 
Heda, and 
Poorva 
Agrawal. 
Science & 
Technology 
Libraries 
ISSN: 0194-
262X, 1541-
1109 

Tertulis 

Komentar 

Dalam 
perbandingan 
Custom NER 
model, 
SpaCy 
menunjukkan 
akurasi yang 
lebih bagus 
dan waktu 
prediksi yang 
lebih cepat 
dibandingkan 
Tensorflow 
Keras, dan 
Apache 
OpenNLP 

From our 
observation of 
all the models, 
we found that 
the SpaCy is a 
better and 
accurate Custom 
NER system. 
The SpaCy gives 
better and 
accurate results 
as compared to 
Apache 
OpenNLP and 
TensorFlow 
when it comes to 
identifying 
entities in the 
input text. The 
prediction time 
for the spaCy 
model is very 
less and the 
model also gives 
false positive 
and false 
negative 
which is useful 
to improve the 
model 
performance. 
Our work proves 
that Spacy is 
better than 
Apache 
OpenNLP and 
TensorFlow for 
the Custom NER 
model. 

2.  Design And 
Implementati
on Of An 
Open Source 
Greek 

Eleni 
Partalidou, 
Eleftherios 
Spyromitros-
Xioufis, 

This paper 
proposes a 
machine 
learning 
approach to 

Model 
SpaCy ini 
menggunaka
n Bahasa 
Yunani dan 

 2 / 7 

 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pos Tagger 
And Entity 
Recognizer 
Using SpaCy 

Stavros 
Doropoulos, 
Stavros 
Vologiannidis
, and 
Konstantinos 
I.Diamantaras
. 

Chantana 
Chantrapornc
hai & Aphisit 
Tunsakul 

3. 

Information 
Extraction 
based on 
Named 
Entity for 
Tourism 
Corpus 

named entity 
recognition for 
Greek, a highly 
inflected 
language using 
spaCy, a very 
robust and 
popular 
framework.  
The result of F1 
score is 93.15 
percent. 

For LOC/ORG 
in SpaCy, the the 
accuracy is 
higher than 
100% for both 
training and 
testing.  
For LOC/ORG 
in BERT, the 
accuracy is 
around 70 % 
for both training 
and testing. 
Origina BERT 
relies on its 
tokenizer which 
includes the 
token conversion 
to ID. It cannot 
tokenizer our 
special 
multiword name 
which are proper 
noun very well. . 
The unknown 
proper nouns are 
chopped into 
portions which 
makes the labels 
shift out. So, 
BERT needs to 
be solved by 
using own 
wordpiece 
tokenizer. 

4. 

Real-world 
Conversation
al AI for 
Hotel 
Bookings 

Bai Li, Nanyi 
Jiang, Joey 
Sham, Henry 
Shi, and 
Hussein Fazal 

The model 
initialized with 
SpaCy’s English 
NER model. Our 
first model treats 

nilai 
akurasinya 
menunjukkan 
93.15% 

Untuk entitas 
LOC/ORG, 
hasil akurasi 
pada model 
SpaCy lebih 
tinggi 
dibandingkan 
dengan 
BERT. 
Karna BERT 
mengandalka
n tokenizer-
nya yang 
mencakup 
konversi 
token ke ID. 
Sehingga 
tidak dapat 
menandai 
dengan baik 
multiword 
khusus yang 
merupakan 
kata benda. 
Akibatnya, 
proper nouns 
yang tidak 
diketahui 
dipotong 
menjadi 
beberapa 
bagian yang 
membuat 
labelnya 
bergeser. 
Jadi, BERT 
perlu 
diselesaikan 
dengan 
menggunaka
n wordpiece 
tokenizer 
sendiri. 

Pada model 
SpaCy 
dengan 
bahasa 
inggris, 
akurasi pada 

Ridong Jiang, 
Rafael E. 
Banchs, and 
Haizhou Li 

5. 

Evaluating 
and 
Combining 
Named 
Entity 
Recognition 
Systems 

hotels and 
locations 
as separate 
entities, while 
our second 
model merges 
them and 
considers both 
hotels and 
locations as a 
single combined 
entity type.  
The results of 
Accuracy are 
65% for entity 
Hotel, 87% for 
entity Location, 
and 96% for 
entity 
Hotel+Location 

ORGANIZATIO
N appears to be 
the entity type 
which is more 
difficult for 
detecting for all 
the NER tools. 
This is proved 
by its lower 
scores compared 
with the 
PERSON and 
LOCATION 
types. 

Stanford NER 
and spaCy 
generally show 
better 
performance in 
this data set for 
both exact 
matching and 
partial matching 
if compared to 
LingPipe and 
NLTK. But, 
Stanford NER  
scores on DATE 
entity type are 
not as good as 
spaCy 

6. 

Ekstraksi 
Informasi 
Berita Online 
dengan 
Named 
Entity 
Recognition 
(NER) dan 
Rule-Based 
untuk 

Atika Dessy  Metode Library 

Spacy untuk 
Named Entity 
Recognition 
(NER) sebagai 
ekstraksi entitas 
informasi dan 
text 
classification, 
pendekatan 

entitas hotel 
lebih kecil 
dibandingkan 
entitas 
lokasi. 
Namun jika 
kedua entitas 
tersebut 
digabungkan 
akan 
menghasilka
n akurasi 
yang tinggi. 

Entitas ORG 
lebih sulit 
dideteksi 
dibandingkan 
entitas PER 
& LOC. 
Hasil akurasi 
Stanford dan 
SpaCy 
menunjukkan 
nilai yang 
lebih bagus 
dibandingkan 
NLTK dan 
LingPipe. 
Namun, nilai 
akurasi 
Stanford 
untuk entitas 
DATE tidak 
sebagus 
SpaCy.  

Model 
SpaCy ini 
menggunaka
n bahasa 
indonesia 
dan 
mendapatkan 
nilai f1 score 
sebesar 82% 

 3 / 7 

 
 
 
 
Visualisasi 
Penyakit 
Tropis di 
Indonesia 

dengan Rule 
Based Method 
sebagai 
visualisasi 
penyakit tropis 
di Indonesia 
dengan entitas 
berupa lokasi 
kejadian, tanggal 
kejadian, jumlah 
korban dalam 
kejadian tersebut 
sistem mampu 
mengekstraksi 
informasi dari 
keseluruhan data 
uji model 
mendapatkan 
perhitungan 
Evaluation 
Scorer yaitu 
Precision 84%, 
Recall, 73% dan 
F1 score 82% 

IV. METODE PENELITIAN  

A.  Pengumpulan Data 

Pengumpulan data pada penelitian  ini  mengacu pada 
keyword    “gangguan  listrik”,  “listrik  padam”,  “mati 
listrik”,  dan  pada  akun  @pln_123  (akun  resmi  dari 
PT.PLN),  akun  @infolistrikdjty  (akun  resmi  dari  PLN 
Unit Induk Distribusi Jawa Tengah dan D.I. Yogyakarta), 
dan akun @pln_jogja.  

Selain itu, pengumpulan data juga diperoleh dari tagar 
tertentu.  Hashtag/tagar  yang  biasa  digunakan  untuk 
mengkategorikan  tweet  terkait  gangguan  listrik  adalah 
#InfoPLN.  Hashtag/tagar  tersebut  biasanya  digunakan 
untuk  menginformasikan 
jadwal  pemadaman  listrik, 
pemeliharaan listrik, perbaikan listrik, dsb. 

Dalam  pengumpulan  data  menggunakan 

twint, 
peneliti  menuliskan  keyword  berupa  kombinasi  nama 
kecamatan di provinsi D. I. Yogyakarta dengan keyword-
keyword  yang  sebelumnya  sudah  disebutkan  diatas. 
Penggunaan  nama  kecamatan  bertujuan  agar  data  yang 
jika  hanya 
didapat 
menggunakan nama kota/kabupaten. 

lebih  banyak  dibandingkan 

B.  Pre-Processing 

Dalam  tahapan  ini  terdapat  beberapa  proses  seperti 

berikut ini : 

1.  Cleaning 

Tahap  ini  dilakukan  untuk  menghilangkan 
akun  user  dan  membersihkan  kata  dari  karakter 
yang  tidak  berpengaruh  terhadap  hasil  klasifikasi. 
Karakter  yang  dihilangkan  berupa  angka,  tanda 
baca, hashtag (#), simbol-simbol, teks “RT” dan url 
dari situs web. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

2.  Stopword 

Tahap ini dilakukan untuk menghapus kata-
kata  yang  memiliki  informasi  rendah  dari  sebuah 
teks.  Misalnya  “yang”,  “dan”,  “di”,  “dari”,  dan 
sebagainya. 

3.  Tokenization 

Pada  tahap  ini  terdapat  3  parameter  yang 
digunakan,  yaitu  preserve_case  (mengubah  teks 
menjadi lower case), strip_handles (menghilangkan 
mention),  dan  reduce_len  (mengurangi  karakter 
yang  diulang  menjadi  3,  misal  :  minnnnnnn 
menjadi minnn).  
4.  Remove Duplicates 

Tahap  ini  bertujuan  untuk  memfilter  data 
tweet yang sama, hal ini disebabkan karena Twitter 
re-tweet  yang  menyebabkan 
memiliki 
banyaknya  tweet  yang  sama  di  re-tweet  oleh 
pengguna  lain  dan  menimbulkan  teks  berulang 
dengan topik yang sama. 

filter 

C.  Filtering data 

Proses 

filtering  data  dilakukan 

setelah  proses 
pengumpulan  data,  pada  saat  proses  pelabelan  data,  dan 
setelah proses pelabelan data. Proses ini dilakukan secara 
manual  dengan  mengecek  tweet  satu  per  satu  untuk 
menghilangkan 
tidak  berkaitan  dengan 
tweet  yang 
gangguan listrik di provinsi D.I. Yogyakarta.  

Pada  proses  filtering  data  yang  dilakukan  setelah 
pengumpulan data, peneliti mengecek semua tweet di tiap 
file  kecamatan.  Hal  ini  bertujuan  untuk  mempermudah 
pengecekan, dibandingkan mengecek satu per satu di file 
kabupaten/kota.  Kemudian,  pada  proses  filtering  data 
yang  dilakukan  pada  saat  pelabelan  data,  peneliti 
melakukan  pengecekan 
sambil  melakukan 
pelabelan.  Jika  ada  tweet  yang  tidak  sesuai,  maka  tweet 
tersebut  dihilangkan  (tidak  dilabeli).  Terakhir,  pada 
proses filtering data yang dilakukan setelah pelabelan data, 
peneliti  melakukan  pengecekan  duplikasi  data  dengan 
menggabungkan  semua  file  kabupaten/kota,  hasilnya 
terdapat  data  berduplikasi  dan  salah  satunya  memiliki 
informasi  lokasi  yang  salah.  Tweet  dengan  informasi 
lokasi salah itulah yang dihilangkan. 

tweet 

D.  Pelabelan Data 

Tweet  hasil  pre-processing  kemudian  akan  dilabeli 
dengan  menggunakan  notasi  BIO  (Begin,  Inside  dan 
Other) sebagai skema pelabelan yang menandakan urutan 
yang  kemudian  diklasifikasikan  menjadi  dua  kelas  yaitu 
B-LOC  dan  I-LOC.  Pada  penelitian  ini,  peneliti  sengaja 
tidak  menggunakan  notasi  O  untuk  memudahkan  dan 
mempercepat proses pelabelan.  

Proses  pelabelan  tweet  dilakukan  lewat  website 
agateteam.org/spaCynerannotate/.  Sebab,  data  training 
harus  dalam  bentuk  format  spaCy  dan  website  tersebut 
sudah  mendukung  format  data  tersebut.  Berikut  format 
data training nya : 

 4 / 7 

 
 
 
 
 
 
 
 
 
Gambar 1. Format data training spaCy 

Kemudian  data  yang  telah  dilakukan  pelabelan  akan 
di-split  menjadi  data  training  dan  data  testing  dengan 
perbandingan  90:10.  Selanjutnya,  data  tersebut  akan 
digunakan untuk melakukan pemodelan NER. 

E.  Pemodelan NER 

Proses  pembangunan  model  NER  dilakukan 
menggunakan library SpaCy. Arsitektur model NER yang 
digunakan  SpaCy  adalah  Transition-Based  Parser.  Pada 
NER  Transition-Based  terdapat  beberapa  konfigurasi, 
yaitu  buffer,  stack,  output,  dan  fungsi  oracle.  Buffer 
merupakan  tempat  penyimpanan  sementara  seluruh  kata 
dari  suatu  tweet  yang  belum  diproses.  Stack  merupakan 
tempat  yang  akan  menerima  kata  dari  buffer.  Output 
merupakan tempat penyimpanan seluruh kata yang sudah 
diproses. Sedangkan fungsi oracle merupakan fungsi yang 
menentukan  tindakan  transisi  apa  yang  akan  digunakan 
untuk  memproses  suatu  kata.  Terdapat  3  jenis  tindakan 
transisi,  yaitu : SHIFT, REDUCE (y), dan OUT. SHIFT 
merupakan  tindakan  memindahkan  kata  dari  buffer  ke 
stack.  REDUCE  (y)  merupakan  tindakan  memindahkan 
kata dari stack ke output dengan label y. Sedangkan, OUT 
merupakan  tindakan  memindahkan  kata  dari  buffer  ke 
output. Berikut ini akan ditampilkan  contoh proses pada 
NER Transition-Based : 

TABEL II 
PROSES NER TRANSITION-BASED PADA SPACY 

F.  Pengujian Model NER 

Pengujian  model  NER  dilakukan  menggunakan 
library SpaCy terhadap data testing, yang diukur dengan 
perhitungan presisi, recall, dan f1-score . 

G.  Pemetaan Entitas Lokasi Gangguan Listrik 

Pemetaan  entitas  lokasi  gangguan  listrik  dilakukan 
dengan menggunakan library shiny dan leaflet di R Studio. 
Tujuan  dari  pemetaan  ini  adalah  untuk  melihat  jumlah 
lokasi gangguan listrik yang tersebutkan di seluruh tweet 
pada  tiap  kabupaten/kota  di  provinsi  D.I.  Yogyakarta 
yang terjadi pada periode waktu 1 Januari 2020 hingga 31 
Desember 2020 dalam bentuk area.  

Kemudian  dilakukan  perbandingan  entitas  lokasi 
gangguan listrik dengan menghitung secara manual lokasi 
yang  tersebutkan  di  seluruh  tweet,  yang  kemudian  di 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

kelompokkan berdasarkan bulan dan kabupaten/kota nya. 
Tujuan dari perbandingan ini adalah untuk melihat bulan 
mana  yang  memiliki  keluhan  gangguan  listrik  tertinggi 
dan  terendah  di  tiap  kabupaten/kota  di  provinsi  D.I. 
Yogyakarta  yang  terjadi  pada  periode  waktu  1  Januari 
2020 hingga 31 Desember 2020 

V.  KERANGKA PIKIR 

Berikut  ini  akan  ditampilkan  kerangka  pikir  peneliti  yang 

berisi permasalahan, tujuan, serta proses dari penelitian ini : 

Gambar 3. Kerangka pikir 

VI. HASIL DAN PEMBAHASAN 

A.  Pengumpulan Data 

library 

Pengumpulan data tweet pada penelitian ini dilakukan 
menggunakan 
twint.  Data  yang  diambil 
merupakan  tweet  yang  berisi  informasi  gangguan  listrik 
(baik  yang  berupa  keluhan  masyarakat  maupun 
pemberitahuan  dari  petugas  PLN)  selama  setahun,  yaitu 
sejak 01 Januari 2020 hingga 31 Desember 2020. Jumlah 
tweet yang berhasil diambil berjumlah 15.374 tweet. 

Berikut  ini  ditampilkan  cara  pengumpulan  data 

menggunakan library twint : 

Gambar 4. Syntax pengumpulan data 

B.  Pre-Processing Data 

Dari  15.374  tweet,  dilakukan  pre-processing  dan 
menghasilkan 6.105 tweet. Berikut ini ditampilkan output 
pre processing : 

 5 / 7 

TransitionOutputStackBufferSegmentLabel[][][rongkop, mati, min]SHIFT[][rongkop][mati, min]REDUCE(LOC)[(rongkop)-LOC][][mati, min](rongkop)-LOCrongkop ""B-LOC""OUT[(rongkop)-LOC, mati][][min]OUT[(rongkop)-LOC, mati, min][][] 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Gambar 5. Output pre-processing 

C.  Filtering data 

Setelah  dilakukan  proses  filtering  beberapa  kali, 
didapatlah tweet akhir dengan jumlah 5.955 tweet. Berikut 
ini  akan  disajikan  tabel  jumlah  tweet  terkait  gangguan 
listrik tiap kabupaten/kota : 

TABEL III 
TABEL JUMLAH TWEET 

D.  Pelabelan Data 

Berikut  ini  akan  ditampilkan  visualisasi  dari  proses 

pelabelan data : 

Gambar 6. Proses pelabelan 

E.  Pengujian model NER 

Pengujian model NER dilakukan menggunakan library 
SpaCy  terhadap  data  testing  dengan  jumlah  596  tweet, 
yang  diukur  dengan  perhitungan  presisi,  recall,  dan  f1-
score.  Berikut  ini  hasil  evaluation  metric  untuk  seluruh 
entitas: 

TABEL IV 
TABEL HASIL EVALUATION METRIC SELURUH ENTITAS 

Dari  tabel  diatas  dapat  disimpulkan  bahwa  hasil  f-1 
score  dari  model  NER  yang  telah  dibangun  sebesar 
93,04%. 

Kemudian,  selain  mendapatkan  nilai  presisi,  recall, 
dan  f1-score  seluruh  entitas,  peneliti  juga  mendapatkan 
nilai presisi, recall, dan f-1 score tiap entitas yaitu B-LOC 
dan I-LOC. Berikut ini hasil evaluation metric untuk tiap 
entitas: 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

TABEL V 
TABEL HASIL EVALUATION METRIC TIAP ENTITAS 

Dari  tabel  diatas  dapat  disimpulkan  bahwa  hasil  f-1 
score entitas B-LOC adalah 94,53% dan f1-score entitas 
I-LOC adalah 92,31%. Sehingga dapat dikatakan  bahwa 
entitas  I-LOC  merupakan  entitas  yang  lebih  banyak 
errornya. 

F.  Pemetaan Entitas Lokasi Gangguan Listrik 

Berikut merupakan tampilan pemetaan jumlah lokasi 
gangguan  listrik  yang  tersebutkan  di  tweet  pada  tiap 
kota/kabupaten  selama  tahun  2020    pada  dashboard 
Rshiny: 

Gambar 7. Pemetaan lokasi gangguan listrik 

jumlah  sebesar  2.736 

Dari  gambar  diatas,  dapat  disimpulkan  bahwa 
Kabupaten  Sleman  merupakan  daerah  yang  memiliki 
jumlah  lokasi  yang  tersebutkan  di  Twitter  paling  tinggi, 
dengan 
tersebut 
ditunjukkan  dengan  daerah  yang  berwarna  merah. 
Sedangkan,  Kabupaten  Gunungkidul  merupakan  daerah 
yang memiliki jumlah lokasi yang tersebutkan di Twitter 
paling  rendah,  dengan  jumlah  sebesar  225  lokasi,  hal 
tersebut ditunjukkan dengan saerah yang berwarna cream 
pucat. 

lokasi,  hal 

TABEL VI 
TABEL JUMLAH LOKASI YANG TERSEBUTKAN DI TWEET 

 6 / 7 

Presisi95.52Recall93.27F-1 Score94.38Evaluation Metric ScrappingPre-ProcessingTweet AkhirBantul5,543       1,887                  1,857               Gunungkidul821           237                      225                  Kulon Progo921           539                      502                  Sleman6,969       2,783                  2,736               Yogyakarta1,120       659                      635                  15,374     6,105                  5,955               Jumlah Tweet Kab/KotTotalB-LOCI-LOCPresisi95.9390Recall93.1794.74F1-Score94.5392.31EntityEvaluation MetricBantulGunungkidulKulon ProgoSlemanYogyakarta11491744193972167185823212132564081299424167254420667512784520729610612271444876192188338140162921559993103319861102492846372581118621363619612156213822133Total18572255022736744Jumlah lokasi yang tersebutkan di tweetBulan ke - 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

[2]  U.  Nahm, 

""Text  mining  with 

https://repositories.lib.utexas.edu/, 
http://hdl.handle.net/2152/1280. [Accessed: 07- Jul- 2021]. 

2004. 

information 

extraction"", 
[Online].  Available: 

[3]  F.  Peng  and  A.  McCallum,  ""Information  extraction  from  research 
papers  using  conditional  random  fields"",  Information  Processing  & 
Management,  vol.  42,  no.  4,  pp.  963-979,  2006.  Available: 
10.1016/j.ipm.2005.09.002. 

[4]  ""Indonesia  Pengguna  Twitter  Terbesar  Ketiga  di  Dunia"", 
Available: 

Databoks.katadata.co.id, 
https://databoks.katadata.co.id/datapublish/2016/11/22/indonesia-
pengguna-Twitter-terbesar-ketiga-di-dunia. 
2020]. 

[Accessed:  05-  Nov- 

[Online]. 

2016. 

[5]  ""Laporan  Statistik  -  PT  PLN  (Persero)"",  PT  PLN  (Persero),  2021. 
https://web.pln.co.id/stakeholder/laporan-

[Online].  Available: 
statistik. [Accessed: 05- Nov- 2020]. 

[6]  H.  Shelar,  G.  Kaur,  N.  Heda,  and  P.  Agrawal,  “Named  Entity 
Recognition  Approaches  and  Their  Comparison  for  Custom  NER 
Model,”  Sci. Technol. Libr., vol. 39, no. 3, pp. 324–337, 2020, doi: 
10.1080/0194262X.2020.1759479. 

[7]  E.  Partalidou,  E.  Spyromitros-Xioufis,  S.  Doropoulos,  S. 
Vologiannidis, and K. I. Diamantaras, “Design and implementation of 
an open source Greek POS Tagger and Entity Recognizer using spaCy,” 
Proc. - 2019 IEEE/WIC/ACM Int. Conf. Web Intell. WI 2019, pp. 337–
341, 2019, doi: 10.1145/3350546.3352543. 

[8]  C. Chantrapornchai and A. Tunsakul, “Information Extraction based 
on Named Entity for Tourism Corpus,” JCSSE 2019 - 16th Int. Jt. Conf. 
Comput. Sci. Softw. Eng. Knowl. Evol. Towar. Singul. Man-Machine 
Intell., pp. 187–192, 2019, doi: 10.1109/JCSSE.2019.8864166. 
[9]  B.  Li,  N.  Jiang,  J.  Sham,  H.  Shi,  and  H.  Fazal,  “Real-world 
conversational  AI  for  hotel  bookings,”  Proc.  -  2019  2nd  Int.  Conf. 
Ind.  AI4I  2019,  pp.  58–62,  2019,  doi: 
Artif. 
10.1109/AI4I46381.2019.00022. 

Intell. 

[10] R.  Jiang,  R.  Banchs  and  H.  Li,  ""Evaluating  and  Combining  Name 
Entity Recognition Systems"", Proceedings of the Sixth Named Entity 
Workshop, 2016. Available: 10.18653/v1/w16-2703 

[11] Atika.  Dessy,  “Ekstraksi  Informasi  Berita  Online  dengan  Named 
Entity Recognition (NER) dan Rule-Based untuk Visualisasi Penyakit 
Tropis di Indonesia”, Medan, 2021. 

terkena  gangguan 

Kemudian  dilakukan  perbandingan 

lokasi  yang 
tersebutkan  di  Twitter 
tiap  Kab/Kot  di  Provinsi 
listrik  per 
D.I.Yogyakarta  yang 
bulannya.  Tabel  VI  diatas  menunjukkan  bahwa  total 
seluruh  jumlah  yang  tersebutkan  di  tweet  adalah  6.064 
lokasi.  Pada  tabel  tersebut  ditunjukkan  bahwa  3  dari  5 
kabupaten/kota  memiliki  jumlah  lokasi  gangguan  listrik 
yang tersebutkan di tweet paling tinggi pada Bulan Maret. 
Dimana  3  kabupaten  tersebut  adalah  Kabupaten  Bantul, 
Gunungkidul, dan Kulon Progo.  

Sementara  itu,  untuk  Kabupaten  Sleman,  jumlah 
lokasi gangguan  listrik  yang tersebutkan di tweet paling 
tinggi  berada  pada  Bulan  Oktober,  sedangkan  Kota 
Yogyakarta  Bulan  Februari.  Pada  tabel  tersebut  juga 
ditunjukkan  bahwa  3  dari  5  kabupaten/kota  memiliki 
jumlah lokasi gangguan listrik yang tersebutkan di tweet 
paling  rendah  pada  Bulan  Juli.  Dimana  3  kabupaten 
tersebut  adalah  Kabupaten  Bantul,  Kulon  Progo,  dan 
Sleman.  Sementara  itu,  untuk  Kabupaten  Gunungkidul 
dan Kota Yogyakarta, jumlah lokasi gangguan listrik yang 
tersebutkan di tweet paling rendah berada pada Bulan Mei.  
Sehingga dapat disimpulkan bahwa bulan Maret 2020 
merupakan  bulan  yang  paling  banyak  mengalami 
gangguan listrik. Sedangkan, bulan Juli 2020 merupakan 
bulan yang paling sedikit mengalami gangguan listrik 

Dari hasil dan pembahasan diatas, dapat disimpulkan bahwa: 

VII. 

PENUTUP 

Indonesia 

(NER)  Bahasa 

1.  Telah  berhasil  dibangun  model  Name  Entity 
Recognition 
dengan 
menggunakan library SpaCy. Entitas pada tweet terkait 
gangguan  listrik  yang  telah  berhasil  diekstraksi  adalah 
entitas lokasi, dengan label B-LOC dan I-LOC. Proses 
pelabelan  sudah  disesuaikan  dengan  format  SpaCy 
seperti sebagai berikut :  Train_data = [ ( ""Free Text 1"", 
entities : { [(start,end, ""TAG 1""), (start,end, ""TAG 2""), 
(start,end, ""TAG 3"")] } ) 

2.  Model Name Entity Recognition (NER) yang dibangun 
dan digunakan untuk klasifikasi tweet terkait gangguan 
listrik  menghasilkan  performa  hasil  yang  baik  dengan 
hasil perhitungan precision 95.52%, recall 93.27%, dan 
f1-score 94.38%. 

tweet 

3.  Jumlah 

lokasi  yang 

lokasi  yang 

tersebutkan  di 

terkait 
gangguan 
listrik  tertinggi  berasal  dari  Kabupaten 
Sleman dengan jumlah sebesar 2.736 lokasi. Sedangkan 
terkait 
jumlah 
gangguan  listrik  terendah  berasal  dari  Kabupaten 
Gunung Kidul dengan jumlah sebesar 225 lokasi.   
4.  Bulan Maret 2020 merupakan bulan yang paling banyak 
mengalami  gangguan  listrik.  Sedangkan,  Bulan  Juli 
2020  merupakan  bulan  yang  paling  sedikit  mengalami 
gangguan listrik. 

tersebutkan  di 

tweet 

DAFTAR PUSTAKA 

[1]  M.  Chaudhari  and  S.  Govilkar,  ""A  Survey  of  Machine  Learning 
Techniques  for  Sentiment  Classification"",  International  Journal  on 
Computational Science & Applications, vol. 5, no. 3, pp. 13-23, 2015. 
Available: 10.5121/ijcsa.2015.5302. 

 7 / 7 

 
 
 
 
 
"
221709977,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Implementasi Speech to Text Bahasa Indonesia 
dalam Survei BPS 

Risandha Reza El Kariem (221709977, 4SD2) 
Dosen Pembimbing: Budi Yuniarto, S.S.T., M. Si. 

Ringkasan—  Wawancara  dan  kuesioner  merupakan  teknik 
pengumpulan  informasi  dalam  survei  Badan  Pusat  Statistik. 
Pengumpulan data ini memiliki masalah pada waktu pencatatan 
teknologi 
dan  keterbukaan  narasumber.  Perkembangan 
berdampak  dalam  perbaikan  pengumpulan  data  survei.  Solusi 
dari  masalah  pengumpulan  data  adalah  pembangunan  sistem 
pengenalan  suara  membantu  penulisan  otomatis  dengan  suara 
narasumber  dengan  pemanfaatan  sistem  Speech  to  Text.  Hasil 
penulisan  dapat  secara  langsung  dengan  penyesuaian  entitas 
pertanyaanya.  Pendefinisian  dengan  pemanfaatan  Named  Entity 
Recognition  yang  merupakan  penerapan  ilmu  natural  language 
processing.  Hasil  uji  coba  dengan  menggunakan  dua  puluh 
jawaban  narasumber  dari 
rekaman 
pertanyaan kuesioner Survei Sosial Ekonomi Nasional pada blok 
tiga  ringkasan  dan  blok  empat  keterangan  demografi    menjadi 
teks tulisan dengan akurasi kata sebesar 94.5%. Pembagian 50% 
data teks hasil sebagai data train dan testing dalam Named Entity 
Recognition. Uji coba teks hasil data testing untuk pendefinisian 
kata  sesuai  dengan  entitas  pertanyaan  dengan  akurasi  sebesar 
100%. 

suara  hasil 

suara 

Kata  Kunci—  pengumpulan  data,  BPS,  SUSENAS,  Speech to 

Text, Named Entity Recognition, natural language processing. 

I.  LATAR BELAKANG 
teknologi  saat 

Perkembangan 

ini  banyak  membantu 
penyelesaian  masalah dalam  berbagai aspek kehidupan, salah 
satu  aspek  yang  semakin  berkembang  adalah  wawancara. 
Wawancara  merupakan 
teknik  pengumpulan  data  untuk 
menghimpun  keterangan  dengan  melakukan  tanya  jawab, 
bertemunya  narasumber  dengan  pewawancara,  dan  memiliki 
tujuan  tertentu  [1].  Wawancara  merupakan  pertemuan  antara 
dua  orang  untuk  bertukar  informasi  dan  ide  melalui  tanya 
jawab  sehingga  dapat  dikonstruksikan  makna  dalam  suatu 
topik tertentu [2]. 

Teknik  pengumpulan  data  dengan  wawancara  dilakukan 
dalam  survei  Badan  Pusat  Statistik  (BPS)  dibantu  teknik 
pengumpulan data lain yaitu kuesioner. Kuesioner merupakan 
metode  pengumpulan  data  yang  dilakukan  dengan  cara 
memberi  seperangkat  pernyataan  atau  pertanyaan  tertulis 
kepada  responden  untuk  diberikan  respon  sesuai  dengan 
permintaan pengguna [3]. 

Wawancara  dan  kuesioner  berisi  kumpulan  pertanyaan, 
terdapat dua jenis pertanyaan. Pertanyaan terbuka dan tertutup. 
Perbedaan  terdapat  dalam  pembatasan  jawaban  responden, 
pertanyaan 
responden 
terbuka  mengandung  arti  bahwa 
memberikan  kebebasan  pada  setiap  jawaban  yang  hendak 
diberikan  oleh  responden  tanpa  batasan  apapun  yang  masih 
sejalan dengan permasalahan penelitian, terutama disesuaikan 
dengan rumusan masalahnya. Survei BPS terdapat pertanyaan 
jenis  semi  terbuka  dengan  jawaban  sudah  disediakan  berupa 
pilihan  jawaban  akan  tetapi  tetap  diberikan  tempat  kosong 

untuk menjawab pertanyaan sesuai dengan kondisi responden 
yang tidak terdapat pada pilihan jawaban. 

Pelaksanaan  pengumpulan  data  terdapat  berbagai  masalah. 
Memerlukan  waktu  dalam  pencatatan  dan  narasumber  yang 
kurang  terbuka  ketika  wawancara  merupakan  masalah  yang 
terjadi  ketika  wawancara.  Memerlukan  waktu  dalam 
pencatatan 
terjadi  karena  kekurangan  waktu  dalam 
pengumpulan  data,  orang  lebih  cepat  mengucapkan  daripada 
menulis  kata.  Narasumber  yang  kurang  terbuka  karena 
kurangnya  kepercayaan  dan  merasa  pelaksanaan  wawancara 
yang  membosankan 
cenderung 
menjawab sekadarnya.  

narasumber 

sehingga 

Kelemahan 

teknik  wawancara 

secara  konvensional 
diantaranya  memerlukan  waktu,  tenaga,  dan  biaya  yang 
banyak  diperlukan  untuk  mendapatkan  informasi  sampai 
dengan  menjalin  hubungan  baik  dengan  narasumber, 
menganalisis jawaban serta menafsirkan data yang didapatkan 
dari  subjek  sehingga  masalah  bertambah  sulit  apabila  jumlah 
populasi  penelitian  besar  serta  kerjasama  dan  jawaban  yang 
diberikan  oleh 
responden 
sehingga  pewawancara  lazimnya  dapat  menjangkau  interaksi 
yang  mendalam  dengan  subjek  penelitian  dari  permukaan 
sampai informasi yang mendetail [4]. 

responden  dipengaruhi  oleh 

Pengenalan  suara  sebagai  alternatif  penyelesaian  masalah 
pengumpulan  data,  pengenalan  suara  merupakan  istilah  ilmu 
komputer  atau  juga  disebut  pengenalan  suara  otomatis. 
Pengenalan  suara  dalam  teks  atau  disebut  Speech  to  Text 
adalah  metode  yang  dapat  mengubah  suara  ke  dalam  bentuk 
tulisan  [5].  Speech  to  Text  merupakan  fitur  dari  pengenalan 
suara yang mengonversi ke dalam bentuk teks dari input suara, 
memugkinkan  komputer  mengerti  bahasa  manusia  melalui 
suara [6]. 

Implementasi  Speech  to  Text  sudah  mulai  digunakan  pada 
beberapa  bidang  [6].  Salah  satu  aplikasi  yang  memanfaatkan 
Speech  to  Text  adalah  penerjemah.  Speech  to  Text  banyak 
dilakukan pada berbagai bahasa [7], penelitian Speech to Text 
penggunaan Bahasa Indonesia masih belum banyak dilakukan 
[7]. 

Solusi dari permasalahan pengumpulan data adalah  Speech 
to  Text,  maka  dibuatkan  prototype  sistem  yang  mengubah 
suara  dari  narasumber  ke  dalam  suatu  teks  tulisan  Bahasa 
Indonesia.  Uji  coba  sistem  Speech  to  Text  menggunakan 
survey  BPS  yaitu  Survei  Sosial  Ekonomi  Nasional 
(SUSENAS)  bagian  kuesioner  keterangan  pokok  anggota 
rumah tangga, blok tiga ringkasan dan blok empat keterangan 
demografi. 

Tulisan  hasil  Speech 

to  Text  merupakan  data  tidak 
terstruktur,  data  yang  tidak  memiliki  struktur  khusus  yang 
ditetapkan dan tidak sesuai dengan kerangka yang ditentukan. 
Contoh  data  tidak  terstruktur  adalah  audio,  visual,  dan 

 1 / 8 

 
 
 
 
berbagai  macam  teks  berita  maupun  laporan  [8].  Natural 
Language  Processing  (NLP)  membantu  data  tidak  terstruktur 
untuk  diformat  dan  dianalisis  secara  otomatis  dengan  benar, 
memanfaatkan  Machine  Learning  dan  NLP  untuk  membaca 
data 
terstruktur  kemudian  mengkategorikan  dan 
menganalisisnya  seperti  cara  kerja  manusia  dengan  akurasi 
dan waktu singkat. 

tidak 

Tulisan hasil Speech to Text akan dilakukan berbagai tugas 
NLP,  yaitu  mengkategorikan  jenis  kata  yang  sesuai  dengan 
entitas pada jawaban berdasarkan pertanyaan pada SUSENAS 
yang  dilakukan  uji  coba.  Dalam  mendapatkan  informasi 
tersebut  dibutuhkan  pendekatan  menggunakan 
kategori 
Named  Entity  Recognition  (NER),  berguna  sebagai  ekstraksi 
informasi secara otomatis dari data tidak terstruktur hasil teks 
Speech to Text agar dapat dikategorikan atau disimpan dalam 
bentuk terstruktur. 

Ekstraksi  informasi  secara  otomatis  mengacu  pada  data 
terstruktur seperti entitas, hubungan antara entitas, dan atribut 
yang  menggambarkan  dari  sumber  yang  tidak  terstruktur  [9]. 
Ekstraksi  informasi  menggunakan  library  spacy,  membantu 
dalam  pelabelan  kategori  informasi,  train  data  teks  hasil 
sebagai  model NER, dan pengkategorian  informasi pada data 
teks hasil. 

II.  TUJUAN PENELITIAN 

Tujuan  dari  penelitian  ini  adalah  melakukan  uji  coba 
metode  pengumpulan  data  pada  survei  BPS  (SUSENAS) 
dengan  pembangunan  sistem  mengkonversi  suara  jawaban 
narasumber atau percakapan wawancara ke dalam bentuk teks 
(Speech to Text) kemudian dilakukan ekstraksi informasi data 
tidak 
terstruktur  (teks  hasil)  menggunakan  NER  yang 
menyesuaikan kelompok kata sesuai dengan entitas  informasi 
pertanyaan  pada  kuesioner  survei  BPS  yaitu  SUSENAS 
bagian  kuesioner  keterangan  pokok  anggota  rumah  tangga, 
blok  tiga  ringkasan  dan  blok  empat  keterangan  demografi. 
Maka,  didapatkan  tujuan  penelitian  yang  lebih  rinci  sebagai 
berikut : 

Mengetahui  tingkat  keberhasilan  Speech  to  Text  dalam 
mengkonversi  suara  jawaban  narasumber  ke  dalam  bentuk 
teks serta analisisnya. 

Mengetahui  tingkat  keberhasilan  teks  hasil  sebagai  data 

train (model NER) serta analisisnya. 

Mengetahui 

tingkat  keberhasilan  model  NER  dalam 

ekstraksi informasi teks hasil serta analisisnya. 

Manfaat  penelitian  ini  adalah  menguji  sistem  Speech  to 
Text dan pengkategorian isi jawaban sesuai entitas pertanyaan 
dengan  Named  Entity  Recognition  dalam  survei  BPS 
(SUSENAS)  melalui  input  rekaman  suara.  Menguji  sistem 
yang  dibangun  dapat  digunakan  sebagai  alternatif  solusi 
pengumpulan  data  yang  mempunyai  keunggulan  pada 
pemanfaatan waktu dan keterbukaan narasumber. 

III. PENELITIAN TERKAIT 

Areni  &  Bustamin  [10].  dalam  penelitianya  membahas 
mengenai  Speech  to  Text  terhadap  kata  Bahasa  Indonesia. 
Menggunakan  makna  kosa  kata  dengan  pengucapan  yang 
sama,  terdapat  dua  jenis  data  yaitu  data  training  dan  data 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

validasi.  Data  diambil  dari  tiga  narasumber  laki-laki  dan  tiga 
perempuan.  Menggunakan  ekstraksi  ciri  MFCC.  Akurasi 
pengucapan  pada  laki-laki  sebesar  53.44%  dan  perempuan 
sebesar 36.8%. 

Buana  [11].  dalam  penelitianya  membahas  mengenai 
Speech to Text pada proses wawancara. Memanfaatkan modul 
speech  recognition  dan  algoritma  FastICA.  Sumber  data 
diambil dari perekaman suara ketika wawancara dan penulisan 
hasil  wawancara  dalam  bentuk  teks.  Data  diambil  dari  enam 
perekaman suara dari video Youtube. akurasi kesesuaian yaitu 
94.75%. 

Stepanyan  [12].  dalam  penelitianya  membahas  mengenai 
pengenalan  dan  disambiguasi  custom  model  NER  otomatis. 
Menghasilkan  sistem  bernama  fastent  yang  membutuhkan 
vector kata explicit Bahasa inggris yang kemudian diturunkan 
dalam  pembuatan  model  NER.  Menyatakan  bahwa  model 
NER  kustom  dapat  digunakan  dan  ditingkatkan  dengan 
Bidirectional  LSTM  dengan  melakukan  training  data  secara 
rutin. 

sesuai 

Shelar 

dimodifikasi 

tentang 
[13].  dalam  penelitianya  membahas 
perbandingan  pembangunan  model  NER  yang  dapat  diimpor 
dan 
kebutuhan.  Menjelaskan 
perbandingan  penggunaan  modul  library  NLP  yaitu  SpaCy 
Python,  Apache  OpenNLP,  dan  TensorFlow.  SpaCy  Python 
memberikan  akurasi  yang  lebih  baik  daripada  library  lain 
melalui  akurasi  pelatihan,  waktu  prediksi,  ukuran  model,  dan 
kemudahan pelatihan. 

Vijay  et  al  [14].    dalam  penelitianya  memanfaatkan  Spacy 
pada  Named  Entity  Recognition  (NER)  untuk  mengubah 
dokumen  yang  lebih  besar  menjadi teks ringkasan  yang  lebih 
pendek  yang  berisi  data  penting  dalam  menyimpulkan  situs. 
Dengan  perhitungan  yang  dilakukan  berdasarkan  sinopsis 
sebuah  buku  dengan  panjang  1032  kata  dipertimbangkan 
dengan  garis  besarnya  menggunakan  empat  kalkulasi  dan 
setiap  kalkulasi  menghasilkan  teks  ringkasan.  Menemukan 
spacy  adalah  yang  terbaik  memberikan  ringkasan  yang 
panjang 280 kata signifikan. 

IV. METODE PENELITIAN  

1.  Speech to Text 
Speech 

to  Text  adalah 

teknologi  memungkinkan 
komputer  untuk  menerima  input  kata  yang  diucapkan 
kemudian  menjadi  suatu  data  yang  dimengerti  oleh 
komputer  [5].  Input  suara  diubah  menjadi  tulisan,  suara 
yang  diterima  komputer  disesuaikan  pada  pola  tertentu 
untuk suara tersebut dapat dikenali oleh komputer. Tulisan 
yang  dihasilkan  kemudian  dimanfaatkan  untuk  keperluan 
sesuai kebutuhan 

Tahapan  dalam  Speech  to  Text  yaitu:  input  dalam 
bentuk data suara kemudian didigitalisasi komputer untuk 
menghasilkan pola suara yang dicocokan pada bahasa dari 
pola suara tersebut [15]. 

Secara garis besar melakukan pengolahan suara melalui 
rekaman  yang  diolah  menjadi  suatu  tulisan.  Dibagi 
menjadi  dua  kegiatan  yaitu  pengambilan  data  dan 
pengolahan.. 
1.1  Pengambilan Data 

 2 / 8 

 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Data  didapat  dari  hasil  rekaman  suara  ketika 
melakukan  wawancara  dengan  perekaman  suara 
dengan  perangkat  keras  yang  mendukungnya. 
Melakukan konversi format (.wav). 

1.2  Pengolahan Data 

Pengolahan  data  rekaman  suara  sebetulnya  dapat 
dilakukan  secara  realtime,  tetapi  pembangunan  ini 
menggunakan  browser  dengan  google 
collab 
sehingga  tidak  dapat  dilakukan  ketika  wawancara 
membawa  laptop.  Pengolahan  data  memanfaatkan 
python dengan modul SpeechRecognition. 

Tahap pengolahan dibagi menjadi beberapa proses, 

(IE) 

adalah 

Extractions 

Information 

proses 
mengekstraksi informasi terstruktur yang berguna dari data 
tidak  terstruktur  dalam  bentuk  entitas,  relasi,  objek, 
peristiwa,  dan  banyak  jenis  lainnya.  Informasi  yang 
diambil  dari  data  yang  tidak  terstruktur  digunakan  untuk 
menyiapkan data untuk dianalisis [16]. 

Transformasi  data  tidak  terstruktur  yang  efisien  dan 
extraction) 
akurat  dalam  proses 
meningkatkan 
Information 
analisis 
Extractions  (IE)  terdiri  dari  penandaan  entitas  dengan 
proses  Named  Entity  Recognition  (NER)  dengan  Library 
SpaCy. 

IE 
data.  Teknik 

(information 

yaitu: 

4.  Named Entity Recognition 

  Tahap Ekstraksi Fitur 

Ekstraksi  fitur  adalah  memisahkan  satu 
ucapan  satu  dari  yang  lain,  karena  tiap  suara 
memiliki  karakteristik  yang  berbeda  dalam 
sinyal  suara.  Melakukan  perbaikan  noise 
menggunakan  algoritma  FastICa,  memisahkan 
suara asli dengan noise. 

  Tahap Deteksi Suara dan Bahasa 

Tahap  deteksi  suara  menggunakan  modul 
Speech  Recognition  pada  Python.  Tahap  awal 
menggunakan  Recognizer(),  menggunakan 
method  record()  karena  input  suara  berupa 
rekaman,  dan  Try  &  Except  digunakan  untuk 
deteksi error saat proses deteksi suara. 

sehingga 

Indonesia, 

Tahap  deteksi  Bahasa  adalah  suara  pada 
rekaman  dideteksi  dan  hanya  menggunakan 
Bahasa 
apabila 
menggunakan  Bahasa  lain  yang  didapatkan 
kurang sempurna. Di dalam coding sudah diatur 
ke Bahasa indoensia. Melakukan deteksi bahasa 
ini  harus  menggunkan  koneksi  internet  karena 
memanfaat Google Speech Recognition dengan 
sintaks  recognize_Google  (audio, 
language 
=’in’).  ‘in’  merupakan  deteksi  dengan  Bahasa 
indonesia. 

  Output 

Keluaran  berupa  tulisan  hasil  isi  jawaban 
pertanyaan 

ketika  wawancara 

responden 
terbuka 

2.  FastICA 

ekstraksi 

Pengenalan  suara 

terdapat  bagian  penting  untuk 
memisahkan satu ucapan dengan yang lain proses tersebut 
disebut 
suara  mempunyai 
fitur.  Setiap 
karakteristik  yang  berbeda  pada  sinyal  suara  [5].  Pada 
ekstraksi dilakukan perbaikan noise dengan  menggunakan 
algoritma  FastICA,  algoritma  tersebut  untuk  ekstraksi 
dengan  algoritma  ICA.  Memisahkan  suara  asli  dengan 
noise. 

Prinsip  algoritma  tersebut  memisahkan  dua  sumber 
bunyi  yaitu  sumber  bunyi  AB  dan  BA  yang  kemudian 
dapat  diperoleh  nila  A  dan  B.  hasilnya  sumber  bunyi  A 
dan B tersebut independent [11]. 

3.  Information Extractions 

Named  Entity  Recognition  (NER)  merupakan  bagian 
dari  Information  Extraction  (IE)  pada  Natural  Language 
Processing  (NLP).  NER  adalah  langkah  pertama  dalam 
ekstraksi  informasi  yang  mana  NER  digunakan  untuk 
menstrukturkan teks menjadi data. Struktur data digunakan 
untuk mengidentifikasi dan mengklasifikasi kata atau frasa 
kata dalam jenis entitasnya. 

Named Entity Recognition (NER) juga dikenal sebagai 
identifikasi  entitas,  pemotongan  entitas,  dan  ekstraksi 
entitas.  Bergantung  pada  persyaratan  tugas  tertentu,  jenis 
yang  akan  diidentifikasi  dapat  berupa  orang,  lokasi, 
organisasi  dan  tanggal,  yang  sebagian  besar  digunakan 
dalam berita [17]. 

Tahap  ini  terdapat  dua  kegiatan  yaitu  pembangunan 

model NER dan testing hasil tulisan Speech to Text. 

4.1  Pembangunan Model Named Entity Recognition 
Teks  hasil  penulisan  Speech  to  Text  dijadikan 
sebagai  input  dalam  penerapan  model  Named 
Entity Recognition. Data teks hasil dibagi menjadi 
50%  sebagai  data  training  dan  50%  sebagai  data 
testing. 

Model  dibangun  dengan  pelabelan  dengan 
entitas 
sesuai  dengan  pertanyaan.  Melabeli 
kategori kata pada teks hasil didefinisikan dengan 
sistem  BIO  Tagging  secara  manual  melalui 
spreadsheet,  dengan  b-  untuk  beginning,  i-  untuk 
interior  dan  e-  ending.  Jika  terdapat  frasa  kata, 
kumpulan lebih dari satu kata dengan b untuk awal 
kata, i untuk kata yang di  antara frasa kata, dan e 
untuk akhir kata. 
Pendefinisian 

selesai  dilanjutkan  dengan 
mengubah ke format SpaCy yang dijadikan model 
training sebagai dasar pendefinisian isi tulisan. 

Model 

siap  digunakan 

setelah  melalui 
pelatihan dengan membuat grafik dan melihat plot 
dan  loss  yang  kecil  sebagai  tanda  model  kustom 
Named Entity Recognition telah selesai. Nilai loss 
yang  semakin  kecil  menandakan  model  semakin 
baik. 

4.2  Testing Hasil Tulisan 

Model  Named  Entity  Recognition  yang  telah 
dibuat  dijadikan  data  training  untuk  membantu 
pengambilan 
dengan 
training  data  perlu 
pendefinisiannya.  Proses 

frasa 

kata 

isi 

 3 / 8 

 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

mengkompilasi  model  dengan  menentukan  loss 
function  dan  optimizer  yang  akan  digunakan  saat 
training, serta evaluation  metrics Proses train data 
data dapat dilihat pada gambar 1. 

Alur  kerja  dalam  pembangunan  Speech  to  Text  dan 
Named Entity Recognition akan digambarkan pada gambar 
2. 

Gambar 1. Proses Training 

5.  Spacy 

SpaCy  adalah  pustaka  sumber  terbuka  (open  source 
library) untuk Natural Language Processing tingkat lanjut 
yang  ditulis  dengan  Python.  Ini  dapat  digunakan  untuk 
informasi  atau  sistem  Natural 
membangun  ekstraksi 
Language  Processing  atau  untuk  memproses  teks  untuk 
pembelajaran yang mendalam. SpaCy menampilkan sistem 
pengenalan  entitas  statistik  yang  sangat  cepat,  yang 
memberikan  label  ke  rentang  token  yang  berdekatan. 
SpaCy  berfokus  pada  menyelesaikan  sesuatu  daripada 
lebih  pendekatan  akademis,  SpaCy  disertai  dengan  suatu 
algoritma  Parts-of-Speech  dan  Named-entity  Recognition. 
Ini  juga  berarti  bahwa  paket  tersebut  tidak  dipenuhi 
dengan  fitur  yang  tidak  perlu.  Beberapa  fitur  yang 
disediakan  oleh  spaCy  adalah  Tokenization,  Parts-of-
Speech  (PoS)  Tagging,  Text  Classification,  dan  Named 
Entity Recognition [18]. 

6.  BIO Tagging 

token  kata, 

BIO Tagging merupakan skema penandaan pada model 
NER  yang  popular.  Sistem  ini  pertama-tama  menetapkan 
fitur  ke  token  kata-representasi  dari  apa  sufiks  token  itu, 
karakter  n-gram  apa  yang  dikandungnya,  seringkali  fitur 
untuk  mewakili  fitur  token  kata  tetangga,  atau  kombinasi 
dari  fitur  di  banyak  token  kata.  Setelah  menetapkan  fitur 
ke 
sistem  kemudian  mencoba  untuk 
menemukan  urutan  tag  yang  paling  mungkin  diberikan 
fitur token kata, dengan mempertimbangkan kemungkinan 
mengamati  tag  yang  diberikan  sekumpulan  fitur,  dan 
kemungkinan tag yang diberikan tag yang bertetangga [19]. 
Pendefinisian  kata  pada  kustom  model  NER  mengenal 
istilah BIO (beginning, inside, outside), pendefinisian frasa 
kata  atau  kumpulan  kata  yang  memiliki  satu  jenis  frasa. 
Jika terdapat tiga kata untuk frasa kata tersebut, maka kata 
pertama  ditambahkan  b-  ketika  pendefinisian  sebaga  kata 
begin,  kata  kedua  ditambahkan  i-  ketika  pendefinisian 
sebagai  kata  interior,  dan  kata  ketiga  ditambahkan  e- 
ketika pendefinisian sebagai kata ending. 

7.  Ruled Based 

Proses    ini  dilakukan  untuk  menampilkan  beberapa 
karakteristik  khusus  dalam  ekstraksi  informasi.  Ekspresi 
ditingkatkan  dengan  elemen  yang  umumnya  dikenal 
dalam bahasa skrip. Kondisi dan tindakan bisa merujuk ke 
ekspresi,  masing-masing  variabel  untuk  mengubah  nilai 
ini  Rule-Based  untuk 
ekspresi.  Dalam  penelitian 
menghubungkan  hasil  ekstraksi  informasi  yang  telah 
didapatkan  yaitu terdapat entitas sesuai pertanyaan survei 
BPS. 

Gambar 2. Alur kerja 

V.  KERANGKA PIKIR 

konvensional  menggunakan 

Penelitian ini berawal dari perkembangan teknologi saat ini 
utamanya yang terjadi di BPS pada teknik pengumpulan data. 
Wawancara  dan  kuesioner  mengalami  perkembangan  dari 
secara 
hingga 
memanfaatkan 
ini  mengembangkan 
teknik  pengumpulan  data  dengan  menggunakan  rekaman 
suara  jawaban  narasumber  sebagai  data  kemudian  dituliskan 
sebagai  hasil  dan  diidentifikasi  kategori  kata  sesuai  dengan 
entitas  pertanyaan.  Kerangka  pikir  penelitian  ini  ditunjukkan 
pada Gambar 2. 

teknologi.  Penelitian 

kertas, 

Gambar 3. Kerangka Pikir Penelitian 

 4 / 8 

 
 
 
  
 
 
 
VI. HASIL DAN PEMBAHASAN 

Penelitian  ini  dilakukan  melalui  beberapa  tahapan  untuk 
membangun Speech to Text dan pembangunan model Named 
Entity Recognition.  

jawaban 

pertanyaan  SUSENAS 

Pengambilan  data  suara 

responden  dalam 
menjawab 
kuesioner 
bagian 
keterangan pokok anggota rumah tangga, blok tiga ringkasan 
dan blok empat keterangan demografi [20]. Pertanyaan  yang 
diajukan ditunjukkan pada tabel 1. 
TABEL  I 
KUESIONER SUSENAS BLOK 3 DAN 4 
Blok 3 Ringkasan 

Nomor 

Kode 

Pertanyaan 

1 

2 

3 

4 

5 

301 

302 

303 

304 

305 

Banyaknya anggota rumah tangga 

Banyaknya anggota rumah tangga berumur 0-4 

tahun 

Banyaknya anggota rumah tangga berumur 5 

tahun ke atas 

Banyaknya anggota rumah tangga berumur 10 

tahun ke atas 

Banyaknya perempuan berumur 10-54 tahun 

berstatus pernah kawin 

Blok 4 Keterangan Demografi 

Nomor 

Kode 

Pertanyaan 

1 

2 

3 

4 

5 

6 

7 

8 

401 

402 

403 

404 

405 

406 

407 

408 

NAMA ANGGOTA RUMAH TANGGA (ART) 

APAKAH HUBUNGAN (nama) DENGAN 
KEPALA RUMAH TANGGA 
APAKAH STATUS PERKAWINAN (nama)? 

APAKAH (nama) LAKI-LAKI ATAU 
PEREMPUAN 
KAPAN (nama) DILAHIRKAN? 

BERAPAKAH UMUR (nama) 

APAKAH SUAMI/ ISTRI (nama) BIASANYA 
TINGGAL DI RUMAH TANGGA INI? 
PADA UMUR BERAPA (nama) 
MELANGSUNGKAN PERKAWINAN PERTAMA 

Rekaman suara dalam format(.wav). Rekaman suara terdiri 
dari 20 rekaman, 15 rekaman dengan wawancara satu arah dan 
5 rekaman dua arah. Daftar tabel rekaman suara beserta durasi 
lamanya rekaman ditunjukkan pada tabel 2. 

TABEL  II 
DAFTAR REKAMAN SUARA 

No 

Nama File 

Durasi 

1 

2 

3 

4 

5 

6 

7 

8 

9 

Responden 1.wav 

2 menit 7 detik 

Responden 2.wav 

3 menit 26 detik 

Responden 3.wav 

2 menit 10 detik 

Responden 4.wav 

1 menit 2 detik 

Responden 5.wav 

2 menit 

Responden 6.wav 

2 menit 21 detik 

Responden 7.wav 

1 menit 52 detik 

Responden 8.wav 

1 menit 58 detik 

Responden 9.wav 

1 menit 52 detik 

10  Responden 10.wav 

2 menit 1 detik 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

11  Responden 11.wav 

2 menit 3 detik 

12  Responden 12.wav 

1 menit 28 detik 

13  Responden 13.wav 

1 menit 44 detik 

14  Responden 14.wav 

2 menit 9 detik 

15  Responden 15.wav 

2 menit 4 detik 

16  Responden 16.wav 

1 menit 32 detik 

17  Responden 17.wav 

1 menit 27 detik 

18  Responden 18.wav 

1 menit 36 detik 

19  Responden 19.wav 

1 menit 34 detik 

20  Responden 30.wav 

1 menit 27 detik 

Rekaman  suara  dijadikan  sebagai  input  suara  pada  Speech 
to  Text.  Data  tersebut  dilakukan  pengolahan  data  seperti 
ekstraksi fitur, deteksi suara, dan Bahasa. Ekstraksi fitur  

Speech  to  Text  menghasilkan  output  berupa  tulisan,  yang 
kemudian  dihitung  tingkat  akurasinya  dengan  membagi  kata 
yang terdeteksi dengan jumlah kata yang seharusnya. Akurasi 
perkata ditunjukkan pada tabel 3. 

TABEL  III 
AKURASI SPEECH TO TEXT 
Kata yang 
Terdeteksi 

Nama File 

Jumlah Kata 

No 

1 

2 

3 

4 

5 

6 

7 

8 

9 

Responden 1.wav 

Responden 2.wav 

Responden 3.wav 

Responden 4.wav 

Responden 5.wav 

Responden 6.wav 

Responden 7.wav 

Responden 8.wav 

Responden 9.wav 

10  Responden 10.wav 

11  Responden 11.wav 

12  Responden 12.wav 

13  Responden 13.wav 

14  Responden 14.wav 

15  Responden 15.wav 

16  Responden 16.wav 

17  Responden 17.wav 

18  Responden 18.wav 

19  Responden 19.wav 

20  Responden 30.wav 

190 

174 

230 

92 

169 

186 

188 

179 

151 

196 

191 

136 

193 

195 

161 

162 

117 

161 

125 

136 

197 

180 

230 

98 

170 

196 

198 

182 

153 

200 

200 

140 

200 

197 

166 

164 

117 

171 

130 

141 

Persentase 

96% 

93% 

100% 

94% 

99% 

95% 

95% 

98% 

99% 

98% 

95% 

97% 

97% 

99% 

97% 

99% 

100% 

94% 

96% 

96% 

94.5% 

Total Persentase keberhasilan 

Tabel  menunjukkan  bahwa  Speech  to  Text  sudah  berjalan 
dengan  baik, tingkat  keberhasilan  kata  yang  terdeteksi  dibagi 
jumlah kata seharusnya sebesar 94.5%.  

Kesalahan terjadi pada awal kata tidak terekam, yaitu saat 
pengucapan  sistem  tidak  dapat  menerima  sinyal  suara  dari 
rekaman.  Kesalahan  juga  terjadi  pada  pengucapan  kata  yang 
mirip  terdengar  dan  dengan  pola  kata  di  depanya,  seperti 

 5 / 8 

 
 
 
pengucapan  satu  nama 
tertulis  satu  enam.  Kesalahan 
terbanyak  pada  bagian  penulisan  nama,  salah  pengejaan  dan 
menuliskan dengan kata yang mirip. 

Teks  hasil  penulisan  Speech  to  Text  dijadikan  sebagai 
input dalam penerapan model Named Entity Recognition. Data 
teks hasil dibagi  menjadi 50% sebagai data  training dan 50% 
sebagai data testing.  

Data  train  dijadikan  model  dengan  pendefinisian  kata 
sesuai  dengan  entitas.  Melakukan  345  pelabelan  pada 
kelompok  kata  pada  data  train,  dengan  rincian  ditunjukkan 
tabel 4. 

TABEL  IV 
DAFTAR LABEL DAN FREKUENSI 

No 

Label 

Jumlah 

1 

2 

3 

4 

5 

6 

7 

8 

9 

10 

11 

12 

13 

Jumlah art 

Art umur 0-4 

Art umur 5 ke atas 

Art umur 10 ke atas' 

Perempuan umur 10-54 pernah kawin 

Nama 

Hubungan krt 

Umur 

Status perkawinan 

Jenis kelamin 

Tanggal Lahir 

Status tempat tinggal 

Umur perkawinan pertama 

10 

10 

10 

10 

10 

45 

46 

45 

45 

45 

45 

16 

17 

Dilakukan  proses  training  dengan  spacy  dengan  seratus 
kali.  Grafik  dalam  melakukan  training  ditunjukkan  pada 
gambar 4.  

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Model  pada  iterasi  ke  100,  menunjukkan  konvergen 
menuju  satu  arah  dimana  semua  persamaan  terselesaikan  dan 
menuju ke suatu nilai dan tidak  mengalami penurunan secara 
signifikan berada direntang angka loss sebesar 11.5. 

Sisa  data  yaitu  50%  data  testing  dilakukan  percobaan 
dengan menggunakan model NER yang telah terbentuk. Hasil 
testing  dengan  membandingkan  jumlah  kata  yang  dapat 
terdefinisi  sesuai  entitasnya    dengan  kata  yang  seharusnya 
terdefinisi. Ditunjukkan pada tabel 5.  

TABEL  V 
AKURASI SPEECH TO TEXT 

No 

Nama File 

Kata yang 
terdefinisi 

Kata yang 
seharusnya  

Persentase 

1 

2 

3 

4 

5 

6 

7 

8 

9 

Responden 3.wav 

Responden 5.wav 

Responden 8.wav 

Responden 9.wav 

Responden 12.wav 

Responden 14.wav 

Responden 16.wav 

Responden 17.wav 

Responden 18.wav 

10 

Responden 19.wav 

44 

34 

34 

26 

27 

42 

33 

24 

27 

28 

Total Presentase keberhasilan 

44 

34 

34 

26 

27 

42 

33 

24 

27 

28 

100% 

100% 

100% 

100% 

100% 

100% 

100% 

100% 

100% 

100% 

100% 

Tabel  menunjukkan  bahwa  model  NER  berhasil 

mengidentifikasi kata sesuai dengan entitas pertanyaan. 

Hasil  tersebut  kemudian  dikonversikan  ke  dalam  format 
csv  sehingga  memudahkan    untuk  proses  selanjutnya  sperti 
pengolahan.  Hasil  yang  dibentuk  keluaran  sebagai  berikut 
ditunjukkan Tabel 6. 

TABEL  VI 
KONVERSI DALAM DATA CSV 

Gambar 4 Plot dan loss dari data training 
Grafik  menunjukkan  data  train  dapat  digunakan  sebagai 
model NER. Angka loss pada tiga iterasi terakhir sebesar 9.8, 
13.3,  dan  11.5.  Angka  loss  pada  tiga  iterasi  awal  sebesar 
1596.5,  1502,  dan  1599.  Terjadi  penuran  signifikan  dan  nilai 
iterasi  akhir  merupakan  nilai  dengan  loss  rendah  sehingga 
dapat dijadikan model.  

Setelah  model  NER  ditetapkan  maka  dilakukan  testing, 
testing digunakan untuk mengidentifikasi kata sehingga sesuai 
dengan entitas pertanyaan. 

No 

token 

anggota rumah tangga 3 

label 

jumlah art 

1 

2 

3 

4 

5 

6 

7 

8 

9 

10 

11 

12 

umur 0 sampai 4 tahun 0 

art umur 0-4 

umur 5 tahun ke atas 3 

art umur 5 ke atas 

umur 10 tahun ke atas tiga 

art umur 10 ke atas 

pernah kawin 1 

perempuan umur 10-54 pernah kawin 

boni 

nama 

kepala rumah tangga 

hubungan krt 

kawin 

laki-laki 

15 juni 1958 

umur 63 

ya 

status perkawinan 

jenis kelamin 

tanggal lahir 

umur 

status tempat tinggal 

13 

perkawinan pertama umur 28 

umur perkawinan pertama 

14 

15 

16 

17 

siti muhayatun 

istri 

kawin 

nama 

hubungan krt 

status perkawinan 

perempuan 

jenis kelamin 

 6 / 8 

 
 
 
18 

19 

20 

4 oktober 1963 

tanggal lahir 

umur 58 

ya 

umur 

status tempat tinggal 

21 

perkawinan pertama umur 22 

umur perkawinan pertama 

22 

23 

24 

25 

26 

27 

daerah miliknya sanggar ami 

nama 

anak kandung 

belum kawin 

perempuan 

2 januari 2000 

umur 21 

hubungan krt 

status perkawinan 

jenis kelamin 

tanggal lahir 

umur 

Tabel  6  menunjukkan  bahwa  teks  dapat  diidentifikasi 
dengan  benar  dan  sesuai.  Mengubah  dalam  format  data  csv 
akan membantu dalam pengolahan data selanjutnya. 

Data  tersebut  menunjukkan  bahwa  model  NER  ini  dapat 
digunakan  dengan  baik  dalam  melakukan  pendefinisian  kata 
sesuai  entitas  pertanyaan  pada  kuesioner  SUSENAS  bagian 
kuesioner  keterangan  pokok  anggota  rumah  tangga,  blok tiga 
ringkasan dan blok empat keterangan demografi. 

VII. 

PENUTUP 

Berdasarkan  penelitian  yang 

telah  dilakukan,  dapat 

disimpulkan bahwa, 

1.  Pendekatan  Speech  to  Text  dengan  bahasa  indonesia 
dapat  dilakukan  dengan  akurasi  perkata  sebesar  94.5%. 
Kesalahan  terjadi  pada  awal  kata  tidak  terekam,  yaitu 
saat  pengucapan  sistem  tidak  dapat  menerima  sinyal 
suara  dari  rekaman.  Kesalahan 
terjadi  pada 
pengucapan  kata  yang  mirip  terdengar  dan  dengan  pola 
kata di depannya, seperti pengucapan satu nama tertulis 
satu  enam.  Kesalahan  terbanyak  pada  bagian  penulisan 
nama, salah pengejaan dan menuliskan dengan kata yang 
mirip. 

juga 

2.  Model  data  train  dengan  menggunakan  50%  data 
training  menunjukkan  data 
train  dapat  digunakan 
sebagai model NER melalui grafik. Angka loss pada tiga 
iterasi  terakhir  sebesar  9.8,  13.3,  dan  11.5.  Angka  loss 
pada  tiga  iterasi  awal  sebesar  1596.5,  1502,  dan  1599. 
Terjadi  penurunan  signifikan  dan  nilai  iterasi  akhir 
merupakan  nilai  dengan  loss  rendah  sehingga  dapat 
dijadikan model. Pada iterasi ke 100 telah menunjukkan 
model konvergen menuju satu nilai dengan nilai terakhir 
11.5. 

terdefinisi 

3.  Pendekatan  testing  menggunakan  model  NER  pada 
pendefinisian  kata  teks  hasil  dengan  penyesuain  entitas 
pertanyaanya  dapat  dilakukan  dengan  akurasi  100%. 
Data  dapat 
sesuai  dengan  keterangan 
pertanyaanya. 
Uraian  di  atas  menunjukkan  bahwa  teknik  pengumpulan 
data  dengan  melakukan  perekaman 
suara  dengan 
memanfaatkan Speech to Text dapat dilakukan dan  jawaban 
narasumber  dapat  didefinisikan  sesuai  entitas  pertanyaanya 
pembuatan  model  Named  Entity 
dengan 
Recognition. 

bantuan 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Peniliti  menyarankan  untuk  melakukan  pengemabangan 
penilitian  ini  utamanya  dalam  pertanyaan  berulang  dan 
validasi otomatis. 

Peneliti  menyarankan  untuk  melakukan  pengembangan 
metode  pengumpulan  data  utamanya  pada  Badan  Pusat 
Statistik  (BPS)  sebagai  lembaga  yang  berhubungan  secara 
langsung  dengan  pengumpulan  data,  perkembangan 
teknologi  yang  berkembang  akan  memudahkan  dalam 
pengumpulan data serta meringankan beban kerja. Penelitian 
ini  selanjutnya  dapat  dikembangkangkan  dalam  bentuk 
sistem  dengan  tampilan  user  interface  dapat  dalam  bentuk 
tampilan  web  maupun  aplikasi  yang  memudahkan  dalam 
melakukan proses yang ada pada sistem yang telah dibangun. 

DAFTAR PUSTAKA 

[1]   A. Sudijono, Pengantar Evaluasi Pendidikan, Jakarta: PT Rajagrafindo 

Persada, 2011.  

[2]   K. G. Esterberg, Qualitative Methods In Social Research, Boston: Mc 

Graw-Hill, 2012.  

[3]   E. P. Widoyoko, Teknik Teknik, Yogyakarta: Pustaka Pelajar, 2016.  

[4]   M. Rosaliza, ""Wawancara, Sebuah Interaksi Komunikasi dalam 

Penelitian Kulaitatif"". Jurnal Ilmu Budaya. 2015. 

[5]   L. Herlim, ”Pengenalan kata dengan menggunakan fuzzy logic untuk 

menggerakkan robot mobil,” Petra Christian University, Surabaya, 
2002. 

[6]   P. Khilari and P. Bhope, ""A Review On Speech to Text Conversion 

Methods,"" Int. J. Adv. Res. Comput. Eng. Technol., vol. 4, no. 7, 2015.  

[7]   C. F. Annisa and S. Pramana, ""Metode Mel Frequency Cepstral 

Coeffisients (MFCC) Pada klasifikasi Hidden Markov Model (HMM) 
Untuk Kata Arabic pada Penutur Indonesia,"" Matics, vol. 8, no. 1, pp. 
36-39, 2016.  

[8]   B. Geisler, ”Repurposing Existing Medications for Coronavirus Disease 
2019: Protocol for a Rapid and Living Systematic Review.,” MedRxiv : 
The Preprint Server for Health, 2020. 

[9]   S. Sarawagi, ”Information Extraction.,” Indian Institute of Technology, 

CSE, Mumbai 400076, vol. 1, nr 3, pp. 261-377, 2008.  

[10]   I. S. Areni och A. Bustamin, ”Improvement in Speech to Text for 

Bahasa IndonesiaThrough Homophone Impairment Training,” 電腦學
刊, vol. 28, nr 5, p. 1–10, 2017.  

[11]   S. B. I. Komang, ”Implementasi Aplikasi Speech to Textuntuk 

Memudahkan Wartawan Mencatat Wawancara dengan Python,” 
JURNAL SISTEM DAN INFORMATIKA (JSI), vol. 14, nr 2, 1 Juli 
2020.  

[12]   L. Stepanyan, ”Automated Custom Named Entity Recognition and 

Disambiguation,” International Journal of Signal Processing, vol. 5, 
pp. 1-8, 2020.  

[13]   H. Shelar, ”. Named Entity Recognition Approaches and Their 

Comparison for Custom NER Model,” Science & Technology Libraries, 
vol. 39, nr 3, pp. 1-14, 2020.  

[14]   M. e. a. Vijay, ”Text Summarization Using Spacy Algorithm,” 

International Journal of Future Generation Communication and 
Networking, vol. 13, nr 3, p. 1645–1652, 2020.  

[15]   K. Suryadharma, G. Budiman och B. Irawan, ”Perancangan Aplikasi 

Speech to Text,” eProceedings of Engineering, vol. 1, nr 1, 2014.  

[16]   K. Adnan och R. Akbar, ”An analytical study of information extraction 
from unstructured and multidimensional big data,” Journal of Big Data, 
vol. 6, nr 1, p. 91, 2019.  

[17]   E. F. Tjong och F. D. Meulder, ”Introduction to the conll-2003 shared 
task: Languageindependent Named Entity Recognition. In Proceedings 
of the seventh conference on Natural language learning,” HLT-NAACL, 
vol. 4, pp. 142-147, 2003.  

 7 / 8 

 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

[18]   B. D. Srinivasa, Natural Language Processing and Computational 

Linguistics: A practical guide to text analysis with Python, Gensim, 
spaCy, and Keras., Mumbai: Packt Publishing Ltd, 2018.  

[19]   P. Corbett och J. Boyle, ”Chemlistem: chemical Named Entity 

Recognition using recurrent neural networks,” J Cheminform, vol. 10, 
nr 59, 2018.  

[20]   Badan Pusat Stastistik;, ""Survei Sosial Ekonomi Nasional (SUSENAS) 

Kor, 2020,"" BPS, Maret 2020. [Online]. Available: 
https://sirusa.bps.go.id/sirusa/index.php/kuesioner/2631. [Accessed 2 
Juni 2021]. 

 8 / 8 

 
 
 
 
 
 
 
"
221709965,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Estimasi Luas Lahan Perkebunan Karet dengan 
Metode Penginderaan Jauh dan Machine Learning 
Studi Kasus: Kabupaten PALI, Provinsi Sumatera Selatan 

Renata Pradita (221709965, 4SD2) 
Dosen Pembimbing: Dr. Eng. Arie Wahyu Wijayanto, SST. MT. 

salah 

Ringkasan—  Karet  merupakan 

satu  komoditas 
perkebunan  utama  di  Indonesia  dengan  produksi  tahunan  3,4 
juta ton. Tingginya produksi karet didukung oleh luasnya lahan 
ini  metode  pendataan 
perkebunan  karet  Indonesia.  Saat 
perkebunan  karet 
Indonesia  masih  memiliki  beberapa 
keterbatasan  yaitu  diperlukannya  tenaga,  biaya  pelaksanaan, 
dan  waktu  yang  sangat  besar  yang  salah  satunya  menyebabkan 
adanya lag-update selama satu tahun dalam publikasinya. Hal ini 
membuka  peluang  untuk  eksplorasi  alternatif  metode  baru 
dalam pendataan perkebunan karet Indonesia. Teknologi remote 
sensing  merupakan  salah  satu  alternatif  solusi  yang  sangat 
potensial  untuk  dipertimbangkan.  Penelitian 
ini  bertujuan  
untuk mengidentifikasi fitur terbaik untuk mengenali citra lahan 
perkebunan  karet,  mengidentifikasi  model  klasifikasi  terbaik 
dalam  mendeteksi  lahan  perkebunan  karet  dan  mengestimasi 
luas  lahan  perkebunan  karet  dengan  mengambil  studi  kasus  di 
Kabupaten  Penukal  Abab  Lematang  Ilir  (PALI),  salah  satu 
daerah  penghasil  karet  di  Provinsi  Sumatera  Selatan.    Untuk 
ini 
membangun 
mengimplementasikan  dan  membandingkan  metode  machine 
learning  Classification  and  Regression  Trees  (CART)  dan 
Random Forest (RF). 

klasifikasi, 

penelitian 

model 

Kata  Kunci—  Karet,  Remote  Sensing,  Citra  Satelit,  Machine 

Learning. 

I.  LATAR BELAKANG 

(misalnya, 

Hevea  Brasiliensis  (Karet)  merupakan  tanaman  tropis 
penghasil  lateks  yang  merupakan  sumber  utama  karet  alam. 
Sifat  karet  yang  elastis  dan  tahan  banting  menjadikan  karet 
sebagai  sumber  daya  yang  berguna  bagi  kehidupan  manusia. 
transportasi 
Misalnya,  untuk  berbagai  aplikasi  dalam 
(misalnya,  ban), 
lembaran  silikon), 
industri 
konsumen  (misalnya,  pakaian),  dan  sektor  medis  (misalnya, 
tubing  dan  kabel)  [1].  Menurut  Food  and  Agriculture 
Organization  (FAO),  Indonesia  yang  merupakan  salah  satu 
negara  beriklim  tropis  ini  adalah  negara  kedua,  setelah 
Thailand,  sebagai  negara  penghasil  karet  terbesar  di  dunia 
dengan  hasil  produksi  tahunannya  yang  mencapai  angka  3,4 
juta ton [2]. Sebagian besar dari hasil produksi karet Indonesia 
diekspor  ke  negara  China,  Jepang,  Amerika  Serikat,  dan 
beberapa negara lainnya. Produksi karet yang dihasilkan oleh 
negara  agraris 
telah  memenuhi  sekitar  29.8%  dari 
kebutuhan dunia dengan nilai ekspor yang dicapai sebesar 3.9 
miliar  US  Dollar  [3],  sehingga  hal  ini  telah  menghantarkan 
karet  sebagai  salah  satu  komoditas  ekspor  utama  yang 
mempunyai 
kegiatan 
cukup 
perekonomian di Indonesia selain minyak dan gas [4]. 

penting 

dalam 

peran 

ini 

Tingginya  produksi  karet  didukung  oleh  luasnya  lahan 
perkebunan  karet  di  Indonesia  yang  mencapai  luas  kurang 
lebih  sebesar  3.6  juta  hektar  [4].  Perkebunan  karet  di 
Indonesia  tersebar  hampir  di  sebagian  besar  Pulau  Sumatera 
dan  Kalimantan  serta  di  sebagian  besar  Provinsi  Jawa  Barat, 
Jawa  Tengah,  Sulawesi  Tengah,  Sulawesi  Selatan,  Bali  dan 
Maluku.  [4].  Perkebunan  karet  di  Indonesia  menurut  status 
pengusahaannya  dibedakan  menjadi  dua,  yaitu  Perkebunan 
Besar  (dibagi  menjadi  Perkebunan  Besar  Negara  (PBN)  dan 
Perkebunan  Besar  Swasta  (PBS))  dan  Perkebunan  Rakyat. 
Perkebunan Besar di Indonesia tercatat memiliki lahan paling 
luas  di  Provinsi  Sumatera  Utara  dengan  luas  lahan  sebesar 
35,39 ribu hektar (21,39% dari luas nasional) untuk PBN dan 
80  ribu  hektar  (33,13%  dari  luas  nasional)  untuk  PBS. 
Sedangkan Perkebunan Rakyat di Indonesia tercatat memiliki 
lahan  paling  luas  di  Provinsi  Sumatera  Selatan  dengan  luas 
lahan  sebesar  812,42  ribu  hektar  (25,03%  dari  luas  nasional) 
[5]. 

Keterbatasan 

Saat  ini,  metode  pendataan  perkebunan  karet  Indonesia 
dilakukan  melalui  Survei  Perusahaan  Perkebunan  (SKB) 
komoditas  Karet  yang  memiliki  beberapa  keterbatasan. 
Diperlukannya  tenaga,  biaya  pelaksanaan,  dan  waktu  yang 
sangat besar mendorong adanya lag-update selama satu tahun 
dalam  publikasi  Statistik  Karet  Indonesia.  Publikasi  tersebut 
menyajikan  data  luas  lahan  perkebunan  karet  serta  jumlah 
produksi, produktivitas, dan status usia tanamannya. 
peluang 

untuk 
tersebut  membuka 
mengeksplorasi  alternatif  metode  baru  dalam  pendataan 
perkebunan  karet  Indonesia.  Teknologi  remote  sensing  atau 
penginderaan jauh merupakan salah satu alternatif solusi yang 
sangat potensial untuk dipertimbangkan.  Remote sensing atau 
penginderaan  jauh  adalah  ilmu  dalam  penerimaan  atau 
perolehan  informasi  mengenai  permukaan  bumi  tanpa  secara 
langsung  melakukan  kontak  dengannya.  Penginderaan  jarak 
jauh meliputi kumpulan dari informasi sumber daya alam dan 
lingkungan dengan menggunakan citra yang diperoleh melalui 
sensor  dari  pesawat  udara  atau  satelit  [6].  Teknologi  remote 
sensing mengalami perkembangan yang semakin pesat karena 
saat  ini  data  penginderaan  jauh  semakin  mudah  didapat  [7], 
sehingga  pemanfaatan teknologi remote sensing telah banyak 
digunakan  oleh  para  peneliti  di  dunia  untuk  mengidentifikasi 
suatu lahan perkebunan, salah satunya yaitu perkebunan karet. 
Misalnya  B.  Chen,  dkk  mengimplementasikan  metode 
Regression dalam mengidentifikasi lahan perkebunan karet di 
Pulaiu Hainan menggunakan citra satelit Landsat 5 TM [8]. W. 
Anurogo,  dkk  mengimplementasikan  metode  Maximum 

 1 / 8 

 
 
 
 
 
Likelihood dalam mengidentifikasi lahan perkebunan karet di 
Kota Salatiga menggunakan citra satelit ASTER [9]. J. Aziera, 
dkk  mengimplementasikan  metode  Support  Vector  Machine 
(SVM)  dalam  mengidentifikasi  lahan  perkebunan  karet  dan 
menganalisis informasi vegetasi pohon karet di Kuala Lumpur 
menggunakan  citra  satelit  Landsat  5  TM,  Landsat  7  ETM+, 
dan Landsat 8 OLI [10].  

Pada  penelitian 

ini,  dilakukan  pengidentifikasian  dan 
estimasi  luas  lahan  perkebunan  karet  menggunakan  citra 
satelit  Sentinel  2  Multispectral  Instrument  Level  2A  dan 
Landsat 8 Surface Refkectance Tier 1 dengan menggabungkan 
pita  multispektral  pada  masing-masing  citra  satelit  dengan 
beberapa  indeks  komposit  seperti  Normalized  Difference 
Vegetation Index (NDVI), Enhanced Vegetation Index (EVI), 
dan Normalized Difference Vegetation Index (NDBI). Peneliti 
mengimplementasikan algoritma machine learning untuk bisa 
menemukan  model 
terbaik  dalam  mengklasifikasikan 
perkebunan karet.  

II.  TUJUAN DAN BATASAN PENELITIAN 

A. Tujuan Penelitian  

Berdasarkan  latar  belakang,  tujuan  penelitian  ini  adalah 

untuk: 
1.  Mengidentifikasi fitur terbaik untuk mengenali citra lahan 

perkebunan karet 

2.  Mengidentifikasi  model 

klasifikasi 

terbaik 

dalam 

mendeteksi lahan perkebunan karet 

3.  Mengestimasi luas lahan perkebunan karet 

B. Batasan Penelitian 

label  pada  penelitian 

Untuk identifikasi model klasifikasi lahan perkebunan karet, 
penentuan 
ini  dibatasi  dengan 
mekanisme validasi visual secara tidak langsung oleh manusia 
(pakar)  menggunakan  alat  bantu  citra  satelit  milik  Google 
Earth,  informasi  lokasi  Google  Maps,  tangkapan  kamera 
Google  Street  View  dan  data  pendukung  lainnya.  Hal  ini 
disebabkan 
data 
administratif  resmi  perkebunan  karet  rinci  pada  setiap  titik-
titik koordinat di Indonesia. Semua model diuji menggunakan 
data  label  yang  sama.  Karena  adanya  keterbatasan  sumber 
daya,  pemeriksaan  langsung  di  lapangan  (ground  checking) 
belum dapat dilakukan. 

tersedianya 

publikasi 

belum 

oleh 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

III. PENELITIAN TERKAIT 

Selama  beberapa  waktu  terakhir,  remote  sensing  telah 
menjadi  salah  satu  metode  yang  banyak  diterapkan  untuk 
mendeteksi  perkebunan  karet  di  dunia.  Selain  itu,  terdapat 
juga  peneliti  yang  menerapkannya  untuk  mengestimasi  luas 
lahan karet dan mengklasifikasikan tanaman karet berdasarkan 
umur.  Namun,  belum  banyak  peneliti  yang  menerapkan 
metode  remote  sensing 
terhadap  perkebunan  karet  di 
Indonesia. 

Gambar 1. Peta Literatur (Literature Map) 

Peta 

literatur 

(Litterature  Map)  pada  Gambar  1 
menunjukkan  penelitian  terkait  yang  menggunakan  metode 
remote  sensing  dalam  penelitiannya.  Peta  literatur  dengan 
kotak  berwarna  merah  mengindikasikan  penelitian  remote 
sensing  secara  umum  yang  dibagi  berdasarkan  metode 
klasifikasi  citra  yang  digunakan.  Lalu  kotak  berwarna  biru 
terhadap 
mengindikasikan 
perkebunan 
tujuan 
penelitiannya.  Dan  kotak  berwarna  oranye  merupakan  irisan 
dari  kotak  berwarna  merah  dan  biru  yang  mengindikasikan 
penelitian  remote  sensing  terhadap  lahan  karet  berdasarkan 
metode  klasifikasi  citra  dan  tujuan  penelitiannya.  Penelitian 
terkait  pada  kotak  berwarna  oranye  kemudian  dimuat  ke 
dalam perbandingan literatur pada Tabel 1 agar dapat melihat 
perbedaan dari masing-masing penelitian yang  dilakukan. 

sensing 
berdasarkan 

penelitian 
yang 

remote 

dibagi 

karet 

No 

1 

Pengarang, 
Tahun 
B. Chen, dkk., 
2012 [8] 

2  W. Anurogo, 
dkk., 2013 [9] 

Tujuan 

Mengidentifikasi dan 
memetakan lahan 
perkebunan karet 

Mengidentifikasi tanaman 
karet dan menentukan 
transformasi indeks 
vegetasi yang terbaik 
untuk mengestimasi 
produksi tanaman karet 

TABEL I 
PERBANDINGAN LITERATUR 
Satelit 

Metode 

Jenis 

Akses 

Lokasi Studi 

Regression 

Landsat 5 TM 

Optik 

Tidak Berbayar 

Hainan, China 

Maximum 
Likelihood (ML) 

ASTER 

Optik 

Tidak Berbayar 

Kota Salatiga, Prov. 
Jawa Tengah, 
Indonesia 

 2 / 8 

Rai, dkk., 2016 [11]Chen, dkk., 2012 [8]Chen, dkk., 2015 [26]Wicki, dkk., 2017 [12]Saputra, dkk., 2018 [20]Xiao, dkk., 2020 [27]Sudaryatno, dkk., 2020 [13]Anurogo & Murti, 2013 [9]Aziera, dkk., 2017 [10]Ye, dkk., 2018 [21]Dong, dkk., 2019 [14]Gao, dkk., 2019 [22]Kou, dkk., 2015 [28]Singh, dkk., 2020 [15]Koedsin, dkk., 2015 [29] Somching, dkk., 2020 [16]Beckschäfer, dkk., 2017 [23]Dibs, dkk., 2018 [24]Chen, dkk., 2018 [1]Ma, dkk., 2019 [17]Watanabe, dkk., 2020 [18]Tang, dkk., 2020 [19]Hazir & Muda, dkk., 2018 [25]Dai, dkk., 2014 [30]Deteksi Lahan Karet & Mengestimasi Luas Lahan Karet dengan Machine LearningNeed to Study: Implementasi Machine Learning untuk Mendeteksi Perkebunan Karet & Mengestimasi Luas Lahan KaretDeteksi & Klasifikasi Pohon Karet Berdasarkan UmurDeteksi Pohon Karet & Mengestimasi Luas Lahan KaretRemote SensingRemote Sensing KaretDeteksi Lahan Karet & Klasifikasi Pohon Karet Berdasarkan Umur dengan Machine LearningDeep LearningRemote Sensing KaretStatistika KonvensionalDeteksi Lahan KaretDeteksi Lahat Karet dengan Statistik KonvensionalMachine LearningDeteksi Lahat Karet dengan Machine Learning 
 
 
 
 
 
Tujuan 

Metode 

Satelit 

Jenis 

Akses 

Lokasi Studi 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Support Vector 
Machine (SVM) 

Landsat 5 TM, 
Landsat 7 ETM+, 
Landsat 8 OLI 

Optik 

Tidak berbayar 

Landsat 5 TM 

Optik 

Tidak Berbayar 

SPOT 5 

Optik 

Berbayar 

Maximum 
Likelihood (ML) dan 
Mahalanobis 

Mahalanobis, K-
Nearest Neighbor 
(KNN), dan Decision 
Tree 

Regression 

Geo Eye 1, Sentinel 
2A, Landsat 8 OLI 

Optik 

Berbayar dan 
Tidak Berbayar 

Maximum 
Likelihood (ML) 

Landsat 5 TM, 
Landsat 7 ETM+, 
Landsat 8 OLI 

Optik 

Tidak Berbayar 

Landsat 8 

Optik 

Tidak Berbayar 

Landsat 8 OLI 

Optik 

Tidak Berbayar 

Kuala Lumpur, 
Malaysia 

Xishuangbanna, 
Cina 

Hulu Selangor, 
Malaysia 

Desa Lalang 
Sembawa, 
Kabupaten 
Banyuasin, Prov 
Sumatera Selatan, 
Indonesia 
Seima Protection 
Forest, Kamboja 

Wilayah Perbatasan 
Cina, Myanmar dan 
Laos. 

Negeri Sembilan, 
Malaysia 

No 

3 

4 

5 

6 

7 

8 

Pengarang, 
Tahun 
J. Aziera, 
dkk., 2017 
[10] 

P. 
Beckschäfer, 
2017 [23] 

H. Dibs, dkk., 
2018 [24] 

J. Saputra, 
dkk., 2018 
[20] 

S. Ye, dkk., 
2018 [21] 

G. Chen, dkk., 
2018 [1] 

9  M. H. M. 

Hazir & T. M. 
T. Muda, 
2018 [25] 

10 

S. Gao, dkk., 
2019 [22] 

Mengidentifikasi karet 
dan melakukan analisis 
informasi dinamika 
vegetasi pohon karet 
Mengidentifikasi lahan 
karet, memetakan lahan 
perkebunan karet, dan 
membuat peta usia 
tanaman karet 

Mengidentifikasi karet 
dan memperoleh 
informasi usia karet 

Mengetahui pengaruh 
resolusi spasial citra 
terhadap peta hasil 
estimasi kandungan hara 
nitrogen perkebunan karet 

Memantau pembangunan 
dan perluasan perkebunan 
karet di Hutan Lindung 
Seima 
Melakukan pemantauan 
(tepat waktu) terhadap 
usia/kondisi tanaman 
karet 
Mengidentifikasi lahan 
perkebunan karet dan 
mengestimasi luas lahan 
karet 

Mencari pendekatan yang 
menjanjikan untuk 
pemetaan perkebunan 
karet dan deteksi 
perubahan perkebunan 
karet di kawasan tropis 

11 

Penelitian ini  Mengidentifikasi fitur 

terbaik untuk mengenali 
citra lahan perkebunan 
karet, model klasifikasi 
terbaik dalam mendeteksi 
lahan perkebunan karet, 
dan mengestimasi luas 
lahan perkebunan karet 

Mahalanobis dan 
Random Forest (RF) 

False Colour 
Method, 
Unsupervised 
Classification, 
Supervised 
Classification, dan 
Spectral Vegetation 
Indices 
Support Vector 
Machine (SVM) dan 
Random Forest (RF) 

Classification And 
Regression Trees 
(CART) dan 
Random Forest (RF) 

Landsat 8 

Optik 

Tidak Berbayar 

Xishuangbanna, 
Cina 

Sentinel 2A dan 
Landsat 8 

Optik 

Tidak Berbayar 

Kab. Penukal Abab 
Lematang Ilir, 
Provinsi Sumatera 
Selatan 

IV. METODE PENELITIAN  

A. Area Sudi 

Penelitian  dilakukan  terhadap  Kabupaten  Penukal  Abab 
Lematang  Ilir  (PALI),  Provinsi  Sumatera  Utara,  Indonesia. 
Kabupaten seluas 1.840 Km2 yang terletak pada 2°50’ – 3°30’ 
Lintang  Selatan  dan  103°30’  –  104°20’  Bujur  Timur  ini 
merupakan  salah  satu  area  dengan  perkebunan  karet  rakyat 
terluas di Indonesia dengan luas lahan tercatat 71.423 Ha [5]. 
Karena  pada  penelitian  ini  perlu  dilakukan  perbandingkan 
hasil  deteksi  lahan  perkebunan  karet  dari  dua  citra  satelit, 
diperlukan area studi dengan citra terbaik pada masing-masing 

satelit,  di  mana  citra  terbaik  yang  dimaksud  adalah  citra 
dengan  tutupan  awan  paling  sedikit.  Citra  Kabupaten  PALI 
baik  pada  Sentinel  2  maupun  Landsat  8,  keduanya  memiliki 
citra  yang 
tidak  berawan  dibandingkan 
kabupaten  lainnya,  sehingga  lebih  memungkinkan  untuk 
dilakukan analisis remote sensing pada kabupaten tersebut. 

tampak  paling 

B.Metode Pengumpulan Data (Data Collection) 

Metode  pengumpulan  data  merupakan  salah  satu  tahapan 
dalam  data  preprocessing  yang  dilakukan  dalam  penelitian. 
Dua  data  citra  diperoleh  dari  satelit  Sentinel  2  Multispectral 
Instrument Level 2A dan satelit Landsat 8 Surface Reflectance 

 3 / 8 

 
 
 
 
 
 
 
 
 
 
 
Tier  1  yang  direkam  pada  tahun  2019  secara  gratis  melalui 
platform Google Earth Engine. Satelit Sentinel 2 Multispectral 
Instrument  Level  2A  memiliki  resolusi  spasial  atau  akurasi 
hingga  10  meter  dengan  resolusi  temporal  10  hari  [20]. 
Sedangkan  satelit  Landsat  8  Surface  Reflectance  Tier  1 
memiliki resolusi spasial atau akurasi hingga 30 meter dengan 
resolusi  temporal  16  hari  [20].  Setelah  kedua  citra  diperoleh, 
peneliti  melakukan  proses  cloud  masking  yang  merupakan 
metode  untuk  mengurangi  tutupan  awan  dan  bayangan  pada 
citra satelit untuk memudahkan proses pelabelan objek secara 
manual. 

C. Metode Pelabelan (Labelling/Annotation) 

Setelah  memperoleh citra  yang terkoreksi awan, dilakukan 
proses  pelabelan  untuk  melakukan  pengambilan  sampel  area. 
Proses  pelabelan  dilakukan  dengan  memilih  sampel  area 
berupa beberapa poligon yang tersebar secara acak di seluruh 
wilayah  Kabupaten  Penukal  Abab  Lematang  Ilir.  Proses 
pengambilan sampel area dilakukan dengan bantuan informasi 
lokasi dari Google Maps, hasil tangkapan kamera oleh Google 
Street  View,  dan  data  pendukung  lainnya.  Sampel  area  yang 
diperoleh  dibagi  menjadi  beberapa  kelas  tutupan  lahan  yaitu 
kelas  Karet  dan  kelas  Non  Karet,  di  mana  kelas  Non  Karet 
terdiri dari Perkebunan  Non  Karet, Hutan, Bangunan, Tanah, 
dan Badan Air. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Hijau/Green,  dan  Biru/Blue  (RGB),  sedangkan  gelombang 
elektromagnetik  selain  itu  tidak  dapat  dilihat  oleh  mata 
manusia.  Citra  satelit  memancarkan  semua  gelombang 
elektromagnetiknya  pada  siang  hari  dengan  bantuan  sinar 
matahari  kemudian  dipancarkan  ke  permukaan  bumi,  setiap 
objek di permukaan bumi akan menyerap masing-masing pita 
spektral  tersebut.  Berapa  banyak  gelombang  yang  diserap 
tergantung pada karakteristik masing-masing objek. Misalnya, 
objek  dengan  vegetasi  hijau  yant  tinggi  akan  menyerap  lebih 
banyak  gelombang  Infrared  daripada  gelombang  RGB.  Lalu, 
objek  tanah  yang  menyerap  gelombang  Shortwave  Infrared 
lebih banyak daripada gelombang RGB. Sehingga gelombang 
yang 
terserap  akan  dihitung  dan  diproses.  Tabel  2 
menunjukkan  pita  multispektral  yang  dimiliki  oleh  satelit 
Sentinel  2  Multispektral  Instrument  Level  2A  dan  Landsat  8 
Surface  Reflectance  Tier  1  yang  digunakan  dalam  penelitian 
ini: 

TABEL II 
PITA MULTISPEKTRAL CITRA SATELIT 

Pita Multispektral 

Sentinel 2 MSI Level 2A  Landsat 8 SR Tier 1 

Deskripsi 

Blue 

Green 

Red 

Vegetation Red Edge 1 

Vegetation Red Edge 2 

Vegetation Red Edge 3 

Vegetation Red Edge 4 

NIR 

Water Vapour 

SWIR 1 

SWIR 2 

B2 

B3 

B4 

B5 

B6 

B7 

B8A 

B8 

B9 

B11 

B12 

B2 

B3 

B4 

- 

- 

- 

- 

B5 

- 

B6 

B7 

Gambar 2. Citra Tutupan Lahan: (a) Karet; (b) Perkebunan Non Karet; (c) 
Hutan; (d) Bangunan; (e) Tanah; (f) Badan Air. 

Perlu diketahui bahwa proses  pengambilan sampel area ini 
merupakan  tahapan  dengan  tingkat  kesulitan  yang  paling 
tinggi,  mengingat  sulitnya  membedakan  citra  kelas  lahan 
Karet  dari  kelas  Perkebunan  Non  Karet  dan  kelas  Hutan. 
Karena  perkebunan  karet  di  Kabupaten  PALI  seluruhnya 
merupakan  perkebunan  rakyat,  citranya  terlihat  sangat  mirip 
dengan  kawasan  Hutan  yang  tidak  memiliki  pola  tanam  atau 
keteraturan  yang  baik.  Sehingga  diperlukan  ketelitian  yang 
sangat tinggi dalam memperoleh sampel area. 

D. Pita Multispektral dan Indeks Komposit 

Pita  multispektral  adalah  spektrum  atau  gelombang 
elektromagnetik  dengan  panjang  gelombang  tertentu.  Setiap 
satelit  memiliki  beberapa  gelombang  elektromagnetik  yang 
berbeda.  Namun  gelombang  elektromagnetik  yang  hanya 
dapat  dilihat  oleh  mata  manusia  adalah  Merah/Red, 

Kemudian  untuk  membantu  proses  analisis,  beberapa 
indeks  komposit  ditambahkan  ke  dalam  masing-masing  citra 
satelit.  Indeks  komposit  yang  digunakan  adalah  Normalized 
Difference  Vegetation  Index  (NDVI),  Enhanced  Vegetation 
Index  (EVI),  dan  Normalized  Difference  Vegetation  Index 
(NDBI).  NDVI  adalah  indeks  vegetasi  yang  menggambarkan 
tingkat  kehijauan  suatu 
tanaman  dengan  menggunakan 
kombinasi  matematis  antara  pita  Merah  dan  pita  NIR.  EVI 
merupakan  indeks  vegetasi  yang  menggambarkan  tingkat 
kehijauan  suatu  tanaman  menggunakan  kombinasi  matematis 
dari  pita  NIR,  pita  Merah,  dan  pita  Biru  [25].  NDBI 
merupakan  indeks  atau  transformasi  yang  dapat  memetakan 
lahan  terbangun  menggunakan  kombinasi  matematis  pita 
SWIR dan pita NIR [31]. Tabel 3 menunjukkan  formula dari 
masing-masing 
indeks  komposit  yang  digunakan  dalam 
penelitian ini. 

 4 / 8 

 
 
 
 
 
TABEL III 
FORMULA INDEKS KOMPOSIT 

Indeks Komposit 

Formula 

Normalized  Difference  Vegetation 
Index (NDVI) 

(NIR-RED)/(NIR+RED) 

Enhanced Vegetation Index (EVI) 

2.5*((NIR-RED)/(NIR+6*RED-
7.5*BLUE+1) 

Normalized  Difference  Vegetation 
Index (NDBI) 

Normalized  Difference  Vegetation 
Index (NDBI) 

(SWIR-NIR)/(SWIR+NIR) 

(SWIR-NIR)/(SWIR+NIR) 

E. Metode Analisis Data (Data Analysis) 

Dalam  penelitian  ini,  proses  analisis  data  dibagi  menjadi 
tiga  proses  berdasarkan  urutan  tujuan  penelitian  yang  telah 
ditetapkan.  Proses  analisis  data  dibantu  menggunakan 
platform  Google  Colaboratory  dengan  bahasa  pemrograman 
Python. Analisis data diawali dari tahap preprocessing berupa 
cloud  masking  dan  pelabelan  seperti  yang  sudah  dijelaskan 
sebelumnya  pada  bagian  Metode  Pengumpulan  Data  dan 
Metode Pelabelan. 

1.Identifikasi  fitur  terbaik  untuk  mengenali  citra  lahan 
perkebunan karet 

Dilakukan  penghitung  nilai  mean  dari  pita  multispektral 
dan  indeks  komposit  terhadap  masing-masing  kelas  tutupan 
lahan.  Kemudian  dari  hasil  penghitungan  mean  tersebut, 
peneliti  mengelompokkan  nilai  mean  ke  dalam  tiga  kategori 
yaitu  “Rendah”,  “Sedang”,  dan  “Tinggi”  dengan  algoritma 
Natural Break. Natural Break adalah metode menentukan titik 
pada  data  dengan  melihat  pengelompokan  dan  pola  data  di 
mana  data  yang  digunakan  mempunyai  jangkauan  dari  yang 
terkecil sampai yang besar, kemudian data dibagi-bagi dengan 
batas-batas  yang  ditentukan  berdasarkan  nilai  jangkauan 
terbesar. Lalu dilakukan pengelompokkan nilai mean dari pita 
multispektral  dan  indeks  komposit  dari  masing-masing  kelas 
tutupan  lahan  ke  dalam  tiga  kategori  yang  sudah  ditentukan, 
apakah nilai mean tersebut masuk ke dalam kategori Rendah, 
Sedang  atau  Tinggi.  Kemudian  peneliti  membandingkan 
kategori  mean  pita  multispektral  dan  indeks  komposit  pada 
seluruh  kelas  Non  Karet  dan 
kelas  Karet  dengan 
mengindentifikasi 
indeks 
(pita  multispektral  atau 
komposit)  apa  saja  yang  dapat  membedakan  kelas  Karet 
dengan kelas Non Karet. 

fitur 

2.  Identifikasi  model  klasifikasi  terbaik  dalam  mendeteksi 
kahan perkebunan karet 

Data citra terlebih dahulu dibagi menjadi training data dan 
testing  data  dengan  rasio  70%  dan  30%.  Kemudian 
diimplementasikan 
learning  yaitu 
algoritma  machine 
Classification  and  Regression  Trees  (CART)  dan  Random 
Forest 
satelit  Sentinel  2 
Multispectral  Instrument  Level  2A  dan  Landsat  8  Surface 
Reflectance Tier 1. 

(RF)  untuk  masing-masing 

CART  adalah  salah  satu  algoritma  klasifikasi  populer 
dalam  machine  learning  yang  banyak  diterapkan  untuk 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

sebuah 

menganalisis  dan  mengklasifikasi  data  citra  satelit  dan 
sebelumnya  telah  berhasil  digunakan  untuk  klasifikasi  citra 
multispektral  [32].  CART  atau  decision  tree  merupakan 
sebuah  diagram  alir  yang  berbentuk  seperti  pohon  yang 
memiliki 
root  node  yang  digunakan  untuk 
mengumpulkan  data,  Sebuah  inner  node  yang  berada  pada 
root  node  yang  berisi  tentang  pertanyaan  tentang  data  dan 
sebuah leaf node yang digunakan untuk memecahkan masalah 
serta  membuat  keputusan.  CART  mengklasifikasikan  suatu 
sampel  data  yang  belum  diketahui  kelasnya  kedalam  kelas  – 
kelas yang ada [33]. 

Random  Forest  (RF)  juga  merupakan  algoritma  klasifikasi 
populer  dalam  machine  learning  yang  berhasil  diterapkan 
untuk menganalisis dan mengklasifikasi data citra satelit [22]. 
Random  Forest  adalah  kombinasi  dari  masing  –  masing  tree 
yang  baik  kemudian  dikombinasikan  ke  dalam  satu  model. 
Algoritma  Random  Forest  bergantung  pada  sebuah  nilai 
vector  random  dengan  distribusi  yang  sama  pada  semua  tree 
yang  masing-masing  decision  tree  memiliki  kedalaman  yang 
maksimal [33]. 

Setelah itu dilakukan validasi menggunakan metode K-Fold 
Cross Validation. K-Fold Cross Validation merupakan metode 
validasi  dari  teknik  data  mining  yang  bertujuan  untuk 
mendapatkan  hasil  akurasi  yang  maksimal  [34].  Validasi 
dilakukan  dengan  K  =  10  hingga  diperoleh  nilai  Accuracy, 
Precision, Recall dan F1 Score dari masing-masing algoritma 
machine learning. 

3. Estimasi luas lahan perkebunan karet 

Untuk  bisa  mengestimasi  luas  lahan  perkebunan  karet, 
terlebih  dahulu  dilakukan  penghitungan  total  piksel  kelas 
Karet  yang  dihasilkan  dari  model  klasifikasi  terbaik  yang 
diperoleh  pada  tahap  sebelumnya.  Total  piksel  kelas  karet 
yang  diperoleh  kemudian  dikonversi  kedalam  satuan  luas 
yang digunakan oleh BPS yaitu Hektar (Ha). 

V.  KERANGKA PIKIR 

Gambar 3. Kerangka Pikir Penelitian 

Penelitian  ini  didasari  oleh  adanya  permasalahan  yang 
dimiliki  pada  metode  pengumpulan  data  Survei  Perusahaan 
Perkebunan  (SKB)  komoditas  Karet  oleh  Badan  Pusat 
Statistik  karena  dibutuhkannya  banyak  tenaga,  waktu,  dan 
biaya  yang  besar  serta  sulitnya  mencapai  area  perkebunan 
karet  yang  akan  disurvei.  Kemudian  peneliti  menawarkan 

 5 / 8 

 
 
 
solusi  yaitu  metode  remote  sensing  dan  machine  learning 
dalam  mengatasi  permasalahan  tersebut.  Kerangka  pikir 
penelitian  yang  ditunjukkan  pada  gambar  3  mengacu  pada 
penelitian M. Berndtsson, dkk. [35]. 

VI. HASIL DAN PEMBAHASAN 

Hasil  dari  penelitian  ini  dibagi  menjadi  tiga  bagian 

berdasarkan urutan tujuan penelitian yang telah ditetapkan. 

1.Identifikasi  fitur  terbaik  untuk  mengenali  citra  lahan 
perkebunan karet 

Gambar 4. Hasil Pengkategorian Nilai Mean Fitur pada Citra Sentinel 2 
Multispectral Instrument Level 2A 

Berdasarkan  hasil  pengkategorian  pada  Gambar  4, 
diperoleh informasi bahwa fitur (pita multispektral atau indeks 
komposit)  pada  citra  Sentinel  2  Multispectral  Instrument 
Level  2A  yang  dapat  membedakan  citra  lahan  Karet  dengan 
citra Hutan adalah Pita Vegetation Red Ege (B6),  Vegetation 
Red Edge (B7), Near Infrared/NIR (B8), Vegetation Red Edge 
(B8A),  Shortwave 
Indeks 
Infrared/SWIR 
Komposit  EVI.  Kemudian  dari  hasil  tersebut  diperoleh 
informasi  juga  bahwa  seluruh  fitur  dapat  membedakan  citra 
lahan  Karet  dengan  citra  Bangunan,  Tanah,  dan  Badan  Air. 
Akan tetapi, tidak ada fitur yang dapat membedakan citra ahan 
Karet dengan citra Perkebunan Non Karet. 

(B11),  dan 

Gambar 5. Hasil Pengkategorian Nilai Mean Fitur pada Citra Landsat 8 
Surface Reflectance Tier 1 

Berdasarkan  hasil  pengkategorian  pada  Gambar  5, 
diperoleh informasi bahwa fitur (pita multispektral atau indeks 
komposit)  pada  citra  Landsat  8  Surface  Reflectance  Tier  1 
yang dapat membedakan citra lahan Karet dengan citra Hutan 
hanyalah  Pita  Near  Infrared/NIR  (B5).  Kemudian  dari  hasil 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

tersebut  diperoleh  informasi  juga  bahwa  seluruh  fitur  dapat 
membedakan citra lahan Karet dengan citra Bangunan, Tanah, 
dan  Badan  Air.  Sama  seperti  pada  citra  Sentinel  2 
Multispectral Instrument Level 2A, tidak ada fitur yang dapat 
membedakan  citra  lahan  Karet  dengan  citra  Perkebunan  Non 
Karet. 

2.  Identifikasi  model  klasifikasi  terbaik  dalam  mendeteksi 
lahan perkebunan karet 

TABEL IV 
PERBANDINGAN ACCURACY, PRECISION, RECALL DAN F1 SCORE 

Satelit 

Algoritma 

Accuracy  Precision 

Recall 

F1 Score 

Sentinel 
2 MSI 
Level 
2A 

Landsat 
8 SR 
Tier 1 

Classification 
and Regression 
Trees (CART) 

Random 
Forest (RF) 

Classification 
and Regression 
Trees (CART) 

Random Forest 
(RF) 

96,42% 

96,44% 

96,42% 

96,42% 

97,07% 

97,11% 

97,07% 

97,07% 

92,31% 

92,36% 

92,31% 

92,31% 

92,85% 

92,94% 

92,85% 

92,86% 

Berdasarkan  hasil  yang  ditunjukkan  pada  Tabel  4,  terlihat 
bahwa  pengimplementasian  metode  klasifikasi  machine 
learning  terhadap  citra  Sentinel  2  Multispectral  Instrument 
Level  2A  dengan  resolusi  spasial  10  m  menghasilkan 
Accuracy, Precision, Recall, dan  F1 Score yang sedikit lebih 
tinggi dibandingkan dengan hasil pada citra Landsat 8 Surface 
Reflectace  Tier  1  dengan  resolusi  spasial  30  m.  Pada  citra 
satelit  Sentinel  2  Multispectral  Instrument  Level  2A,  metode 
klasifikasi  Random  Forest  (RF)  merupakan  metode  dengan 
hasil  akurasi  dengan  nilai  yang  paling 
tinggi  dalam 
mengklasifikasikan  kelas  tutupan  lahan,  ditunjukkan  dari 
tingginya  nilai  Accuracy  sebesar  97,07%,  Precision  sebesar 
97,11%, Recall sebesar 97,07% dan F1 Score sebesar 97,07%. 
Sama  seperti  citra  satelit  sebelumnya,  pada  citra  satelit 
Landsat  8  Surface  Reflectance  Tier  1,  metode  klasifikasi 
Random  Forest  (RF)  juga  merupakan  metode  klasifikasi 
dengan  hasil  akurasi  dengan  nilai  yang  paling  tinggi  dalam 
mengklasifikasikan  kelas  tutupan  lahan,  ditunjukkan  dari 
tingginya  nilai  Accuracy  sebesar  92,85%,  Precision  sebesar 
92,94%, Recall sebesar 92,85% dan F1 Score sebesar 92,86%. 
Meskipun  metode  klasifikasi  Classification  and  Regression 
Trees (CART) pada citra Sentinel 2 Multispectral Instrument 
Level  2A  dan    Landsat  8  Surface  Reflectance  Tier  1 
merupakan  metode  dengan  hasil  akurasi  dengan  nilai  yang 
paling  rendah  dalam  mengklasifikasikan  kelas  tutupan  lahan, 
metode  tersebut  tetap  menunjukkan  hasil  yang  sama-sama 
tinggi  ditunjukkan  dari  nilai  Accuracy  sebesar  96,42%  dan 
92,31%,  Precision  sebesar  96,44%  dan  92,36%,  Recall 
sebesar 96,42%  dan  92,31%, serta F1 Score sebesar 96,42% 
dan 92,31%. 

 6 / 8 

FiturKaretPerkebunan Non KaretHutanBangunanTanahBadan AirBlue (B2)RendahRendahRendahSedangSedangSedangGreen (B3)RendahRendahRendahSedangSedangSedangRed (B4)RendahRendahRendahSedangSedangSedangVegetation Red Edge (B5)RendahRendahRendahSedangSedangRendahVegetation Red Edge (B6)TinggiTinggiSedangSedangSedangRendahVegetation Red Edge (B7)TinggiTinggiSedangSedangSedangRendahNIR (B8)TinggiTinggiSedangSedangSedangRendahVegetation Red Edge (B8A)TinggiTinggiSedangSedangSedangRendahWater Vapour (B9)TinggiTinggiSedangSedangSedangRendahSWIR (B11)SedangSedangRendahTinggiTinggiRendahSWIR (B12)RendahRendahRendahSedangSedangRendahNDVITinggiTinggiTinggiSedangSedangRendahNDBIRendahRendahRendahTinggiTinggiRendahEVITinggiTinggiSedangRendahRendahRendahFiturKaretPerkebunan Non KaretHutanBangunanTanahBadan AirBlue (B2)RendahRendahRendahSedangTinggiSedangGreen (B3)RendahRendahRendahSedangSedangSedangRed (B4)RendahRendahRendahSedangSedangSedangNIR (B5)TinggiTinggiSedangSedangSedangRendahSWIR (B6)SedangSedangSedangTinggiTinggiRendahSWIR (B7)RendahRendahRendahSedangTinggiRendahNDVITinggiTinggiTinggiSedangSedangRendahNDBIRendahRendahRendahTinggiSedangRendahEVITinggiTinggiTinggiSedangSedangRendah 
 
 
 
Gambar 6. Kurva Akurasi Metode CART dan RF Terhadap Citra Sentinel 2 
Multispectral Instrument Level 2A 

Gambar 7. Kurva Akurasi Metode CART dan RF Terhadap Citra Landsat 8 
Surface Reflectance Tier 1 

Terlihat kurva akurasi yang dihasilkan oleh masing-masing 
model  pada  Gambar  6  dan  7  yang  menunjukkan  bahwa 
seluruh  hasil  akurasi  validasi  tidak  memiliki  nilai  yang 
berbeda  jauh  terhadap  akurasi  trainingnya,  sehingga  hal  ini 
dapat  memastikan  bahwa  nilai  akurasi  yang  dihasilkan  dari 
model adalah benar. 

Dari  model  Random  Forest  pada  citra  Sentinel  2 
Multispectral  Instrument  Level  2A,  diperoleh  Peta  Tutupan 
Lahan  Kabupaten  PALI  yang  ditunjukkan  oleh  Gambar  8  di 
bawah ini. 

Gambar 8. Peta Tutupan Lahan Kabupaten PALI 

3. Estimasi luas lahan perkebunan karet 

luas 

Diperoleh  estimasi 

lahan  perkebunan  karet  di 
Kabupaten  Penukal  Abab  Lematang  Ilir  (PALI)  dari  model 
klasifikasi  Random  Forest  seluas  105.566  Ha.  Hasil  estimasi 
ini  memiliki  selisih  sebesar  34.143  Ha  dari  data  luas  lahan 
perkebunan karet pada publikasi Statistik Karet Indonesia oleh 
Badan Pusat Statistik. Hal ini kemungkinan disebabkan karena 
pada  Kabupaten  PALI,  mayoritas  lahan  perkebunan  karet 
merupakan  perkebunan  rakyat  yang  metode  pengumpulan 
datanya  adalah  pelaporan  mandiri  (self  reporting),  sehingga 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

membuka  peluang  kekurang  tepatan  laporan  luas  lahan 
dibandingkan kondisi sebenarnya. 

A. Kesimpulan 

VII. 

PENUTUP 

Dibandingkan  dengan  hasil  yang  diperoleh  saat  seminar, 
dilakukan  penambahan  sampel  area,  yang  sebelumnya  hanya 
berjumlah sebanyak 155.513 piksel menjadi sebanyak 780.474 
piksel sehingga hasil yang diperoleh pada penelitian ini dapat 
menjadi lebih representatif. 

Menjawab  tujuan  pertama  penelitian,  berhasil  dilakukan 
pengidentifikasian  fitur  terbaik  untuk  mengenali  citra  lahan 
perkebunan  karet  terhadap  kedua  citra  satelit.  Pada  citra 
Sentinel 2 Multispectral Instrument Level 2A, diperoleh hasil 
bahwa  Pita  Vegetation  Red  Ege  (B6),    Vegetation  Red  Edge 
(B7),  Near  Infrared/NIR  (B8),  Vegetation  Red  Edge  (B8A), 
Shortwave  Infrared/SWIR  (B11),  dan  Indeks  Komposit  EVI 
adalah  fitur  terbaik  karena  mampu  mengenali/membedakan 
citra  lahan  Karet  dari  citra  Hutan,  Bangunan,  Tanah,  dan 
Badan  Air.  Sedangkan  fitur  terbaik  dari  citra  Landsat  8 
Surface  Reflectance  Tier  1  hanyalah  Pita  Near  Infrared/NIR 
(B5) karena hanya fitur inilah yang mampu membedakan citra 
lahan Karet dari citra Hutan, Bangunan, Tanah, dan Badan Air. 
Karena  tidak  ada  satupun  fitur  yang  mampu  membedakan 
citra lahan perkebunan Karet dan Perkebunan Non Karet pada 
kedua citra satelit, dapat disimpulkan bahwa citra satelit masih 
membutuhkan fitur baru yang perlu ditambahkan agar mampu 
membedakan  citra  lahan  Karet  dengan  citra  Perkebunan  Non 
Karet,  sehingga  diperlukan  penelitian  lebih  lanjut  untuk 
menambahkan  fitur  baru  yang  sesuai  terhadap  citra  satelit 
tersebut. 

Lalu menjawab tujuan kedua penelitian, berhasil dilakukan 
pengidentifikasian model klasifikasi terbaik dalam mendeteksi 
lahan perkebunan karet yaitu model Random Forest pada citra 
Sentinel  2  Multispectral  Instrument  Level  2A  dengan 
Accuracy,  Precision,  Recall,  dan  F1  Score  senilai  97,07%, 
97,11%,  97,07%,  dan  97,07%.  Kemudian  menjawab  tujuan 
ketiga  penelitian,  berhasil  diperoleh  estimasi  luas  lahan 
perkebunan  karet  di  Kabupaten  Penukal  Abab  Lematang  Ilir 
(PALI)  seluas  105.566  Ha  dari  model  klasifikasi  Random 
Forest. 

B. Saran 

Berdasarkan  hasil  penelitian  ini,  terbuka  peluang  untuk 
melakukan  penelitian  berupa  pembangunan  indeks  komposit 
baru  yang  mampu  membedakan  citra  dari  masing-masing 
kelas  tutupan  lahan.  Untuk  penelitian  selanjutnya,  dapat 
berfokus pada estimasi usia tanam dan produktivitas tanaman 
karet  sesuai  kebutuhan  publikasi  Statistik  Karet  Indonesia, 
misalnya  dengan  mengidentifikasi  perubahan  multitemporal 
pada citra satelit. 

DAFTAR PUSTAKA 
[1]  G. Chen, J. C. Thill, S. Anantsuksomsri, N. Tontisirin dan R. Tao, Stand 
age  estimation  of  rubber  (Hevea  brasiliensis)  plantations  using  an 
integrated pixel and object based tree growth model and annual Landsat 
time  series,  ISPRS  Journal  of  Photogrammetry  and  Remote  Sensing, 
2018, pp. 94-104. 

 7 / 8 

 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

[21] S.  Ye,  J.  Rogan  dan  F.  Sangermano,  Monitoring  rubber  plantation 
expansion using Landsat data time series and a Shapelet-based approach, 
ISPRS  Journal  of  Photogrammetry  and Remote  Sensing, 2018, pp. 134-
143. 

[22] S. Gao, X. Liu, Y. Bo, Z. Shi, dan H. Zhou, Rubber Identification Based 
on  Blended  High  Spatio  Temporal  Resolution  Optical  Remote  Sensing 
Data: A Case Study in Xishuangbanna, Remote Sensing, 2019, pp. 496. 
[23] P.  Beckschäfer,  Obtaining  rubber  plantation  age  information  from  very 
dense  Landsat  TM  &  ETM+  time  series  data  and  pixel-based  image 
compositing, Remote Sensing of Environment (RSE), 2017, pp. 89–100. 

[24] H. Dibs, M. O. Idrees dan G. B. A. Alsalhin,  Hierarchical classification 
approach  for  mapping  rubber  tree  growth  using  per-pixel  and  object-
oriented  classifiers  with  SPOT-5  imagery,  The  Egyptian  Journal  of 
Remote Sensing and Space Sciences, 2017, pp. 21-30. 

[25] M. H. M. Hazir dan T. M. T. Muda,  The viability of remote sensing for 
extracting  rubber  smallholding  information:  A  case  study  in  Malaysia, 
The Egyptian Journal of Remote Sensing and Space Sciences, 2018, pp. 
35-47. 

[26] B. Chen Z, Wu, J. Wang, J. Dong, L. Guan, J. Chen, K. Yang dan G. Xie, 
Spatio temporal prediction of leaf area index of rubber plantation using 
HJ-1A/1B CCD images and recurrent neural network, ISPRS Journal of 
Photogrammetry and Remote Sensing, 2015, pp. 148-160. 

[27] C.  Xiao,  P.  Li,  Z.  Feng,  Y.  Liu  dan  X.  Zhang,  Sentinel  2  red-edge 
spectral  indices  (RESI)  suitability  for  mapping  rubber  boom  in  Luang 
Namtha  Province,  northern  Lao  PDR,  Int  J  Appl  Earth  Obs 
Geoinformationm, 2020. 

[28] W. Kou, X. Xiao, J. Dong, S. Gan, D. Zhai, G. Zhang, Y. Qin, dan L. Li, 
Mapping  Deciduous  Rubber  Plantation  Areas  and  Stand  Ages  with 
PALSAR and Landsat Images, Remote Sensing, 2015, pp. 1048-1073. 
[29] W.  Koedsin  dan  A.  Huete,  Mapping  Rubber  Tree  Stand  Age  Using 
Pleiades  Satellite  Imagery:  A  Case  Study  in  Thalang  District,  Phuket, 
Thailand, Engineering Journal, 2015, pp. 45-56. 

[30] S.  Dai,  H.  Li,  H.  Luo,  M.  Li,  J.  Fang,  L.  Wang,  J.  Cao  dan  W.  Luo, 
Object-Oriented  Classification  of  Rubber  Plantations  from  Landsat 
Satellite  Imagery,  IEEE:  The  Third  International  Conference  on  Agro-
Geoinformatics , 2014. 

[31] I.  N.  Hidayati,  R.  Suharyadi,  dan  P.  Danoedoro,  Developing  an 
Extraction  Method  of  Urban  Built-Up  Area  Based  on  Remote  Sensing 
Imagery  Transformation  Index,  Prosiding  Seminar  Nasional  Geografi 
UMS VIII, 2018. 

[32] A.  Abdel-Hamid,  O.  Dubovyk,  I.  A.  El-Magd  sN  G.  Menz,  Mapping 
Mangroves Extents on the Red Sea Coastline in Egypt using Polarimetric 
SAR  and  High  Resolution  Optical  Remote  Sensing  Data,  Sustainability, 
2018, PP. 646. 

[33] Universitas  Gadjah  Mada  Menara  Ilmu  Machine  Learning.  (2018). 
Random  Forest  [Online].  Available:  https://machinelearning.mipa.ugm. 
ac.id/ 2018/07/28/random-forest/. 

[34] B. Santosa dan A. Umam, Data Mining dan Big Data Analytics Teori dan 
Implementasi  Menggunakan  Python  &  Apache  Spark  Edisi  2,  Penebar 
Media Pustaka, 2018, pp. 260-262. 

[35] M. Berndtsson, J. Hansson, B. Olsson, dan B. Lundell, Thesis Projects—
A  Guide for Students in Computer  Science  and  Information System  2nd 
Edition. London: Springer, 2008. 

[2]  The Agriculture News. (2019) 10 Negara Penghasl Karet Alami Terbesar 
[Online].  Available:  https://theagrinews.com/10-negara-

di  Dunia 
penghasil-karet-alami-terbesar-di-dunia/. 

[3]  Iqbal  Hakim.  (2020).  10  Negara  Penghasil  Karet  Terbesar  di  Dunia. 
[Online].  Available:  https://insanpelajar.com/10-negara-penghasil-karet-
terbesar-di-dunia/. 

[4]  Subdirektorat Statistik Tanaman Perkebunan, Statistik Perkebunan Karet 

2018, Badan Pusat Statistik, 2019, pp. 1-26. 

[5]  Subdirektorat Statistik Tanaman Perkebunan, Statistik Perkebunan Karet 

2019, Badan Pusat Statistik, 2020, pp. 1-26. 

[6]  W.  Setiawan,  “Pengolahan  Citra  Penginderaan  Jarak  Jauh.  Klasifikasi, 
Fusi  Data, dan  Deteksi  Perubahan  Wilayah”, Bandung:  UPIPress,  2012, 
Bab II, pp. 8-10. 

[7]  C.Yao,  Y.  Zhang,  Y.  Zhang  dan  H.  Liu,  Application  of  Convolutional 
Neural  Network  Classification  of  High  Resolution  Agricultural  Remote 
Sensing  Image,  ISPRS  International  Archives  of  the  Photogrammetry, 
Remote Sensing and Spatial Information Sciences,  2017, pp.  989-992. 
[8]  B. Chen, J. Cao, J. Wang, Z. Wu, Z. Tao, J. Chen, C. Yang dan G. Xie, 
Estimation  of  rubber  stand  age  in  typhoon  and  chilling  injury  afflicted 
area with Landsat TM data: A case study in Hainan Island, China, Forest 
Ecology and Management, 2012, pp. 222-230. 

[9]  W.  Anurogo,  R.  D.  Silaban,  C.  B.  Nugroho,  M.  K.  Mufida  dan  D.  S. 
Pamungkas, Pixel-based Remote Sensing Data Processing for Estimating 
Rubber  Plantations  Productivity,  2nd  International  Conference  on 
Applied Engineering (ICAE), 2019, pp. 1-5. 

[10] J.  Aziera,  A.  Rashid,  Noordin  dan  M.  I.  Sameen,  Mapping  rubber  trees 
based on phenological analysis of Landsat time series data-sets, Geocarto 
International, 2017, pp. 627-650. 

[11] P. K. Rai dan R. Prasad, Evaluation of land use/land cover classification 
accuracy using multiresolutin remote sensing images, Forum Geografic, 
2016, pp. 45-53. 

[12] A.  Wicki  dan  E.  Parlow,  Attribution  of  local  climate  zones  using  a 
multitemporal  land  use/land  cover  classification  scheme,  Journal  of 
Applied Remote Sensing, 2017. 

[13] Sudaryatno, P. Widayani, T. W. Wibowo, B. A. S. Pramono, Z. N. Afifah, 
A. D. Meikasari dan  M. R. Firdaus, Multiple linear regression analysis 
of remote sensing data for determining vulnerability factors of  landslide 
in  PURWOREJO,    IOP  Conference  Series:  Earth  and  Environmental 
Science, 2020. 

[14] R.  Dong,  W.  Li,  H.  Fu,  L.  Gan,  L.  Yu,  J.  Zheng dan  M.  Xia,  Oil  palm 
plantation  mapping  from  high  resolution  remote  sensing  images  using 
deep learning, International Journal of Remote Sensing, 2019. 

[15] M. Singh, D. Evans, J. Chevance, B. S. Tan, N. Wiggins, L. Kong dan S. 
Sakhoeun,  Evaluating  remote  sensing  datasets  and  machine  learning 
algorithms  for  mapping  plantations  and  successional  forests  in  Phnom 
Kulen National Park of Cambodia, PeerJ, 2019, pp. 15. 

[16] N. Somching, S. Wongsai, N. Wongsai dan W. Koedsin,  Using machine 
learning algorithm and landsar time series to identify establishment year 
of  para  rubber  olantations:  a  case  study  in  Thalang  district,  Phuket 
Island,  Thailaind,  International  Journal  of  Remote  Sensing,  2020,  pp. 
9075-9100. 

[17] L.  Ma,  Y.  Liu,  X.  Zhang,  Y.  Ye,  G.  Yin  dan  B.  A.  Johnson,  Deep 
learning  in  remote  sensing  applications:  A  meta  analysis  and  review, 
ISPRS  Journal  of  Photogrammetry  and Remote  Sensing, 2019, pp. 166-
177. 

[18] S.  Watanabe,  K.  Sumi  dan  T.  Ise,  Identifying  the  vegetation  type  in 
Google Earth images using a convolutional neural network: a case study 
for Japanese bamboo forests, 2020. 

[19] Z.  Tang,  M.  Li  dan  X.  Wang,  Mapping  Tea  Plantations  from  VHR 
Images Using OBIA and Convolutional Neural Network, Remote Sensing, 
2020. 

[20] J. Saputra, M. Kamal dan P. Wicaksono, The Effect of Spatial Resolution 
Image on The Results of Nitrogen Content Mapping of Rubber Plantation, 
Indonesian J. Nat. Rubb. Res., 2018, pp. 13-24. 

 8 / 8 

 
 
"
221709962,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pemetaan Daerah Rawan Tanah Longsor dengan 
Penginderaan Jauh dan Sistem Informasi Geografis 
Studi Kasus : Kabupaten Lahat, Sumatera Selatan 

Ratu Kintan Karina (221709962, 4SD1) 
Dosen Pembimbing: Robert Kurniawan 

tanah 

Ringkasan—  Berdasarkan  IRBI  2013  dan  2018  Kabupaten 
Lahat merupakan salah satu kabupaten yang mempunyai risiko 
bencana  tanah  longsor  yang  tinggi.  Cukup  tingginya  kejadian 
bencana  tanah  longsor  di  Kabupaten  Lahat  ini  menyebabkan 
perlunya  kesiapsiagaan  mengenai  bencana 
longsor. 
Sehingga  penelitian  ini  bertujuan  untuk  memetakan  daerah 
rawan  tanah  longsor  dengan  memanfaatkan  penginderaan  jauh 
dan  sistem  informasi  geografis,  serta  membandingkan  hasil 
kerawanan  dari  BNPB.  Penelitian  ini  menggunakan  parameter 
curah hujan, kemiringan lereng, arah lereng, penggunaan lahan, 
dan  geologi.  Berdasarkan  hasil  yang  diperoleh  bahwa  dengan 
menggunakan  metode  jumlah  ranking,  metode  ketergantungan 
ranking,  dan  perbandingan  pasangan  (AHP)  menghasilkan 
perbedaan jumlah desa untuk tiap kelas kerawanan. Jika dilihat 
dari  akurasinya  metode  ketergantungan  ranking  memperoleh 
hasil  yang  lebih  baik  dibandingkan  dengan  metode  jumlah 
ranking  dan  AHP  yaitu  sebesar  70.03%  untuk  akurasi 
keseluruhan dan 46.66% untuk akurasi kappa. 

Kata  Kunci—  Tanah  Longsor,  Pemetaan,  Kerawanan, 

Penginderaan Jauh. 

I.  LATAR BELAKANG 

Tanah  longsor  merupakan  salah  satu  jenis  gerakan  massa 
tanan atau batuan, maupun percampuran keduanya, menuruni 
atau  keluar  lereng  akibat  terganggunya  kestabilan  tanah  atau 
batuan  penyusun  lereng  [1].  Berdasarkan  data  metodologi 
untuk  analisis  tingkat  risiko  bencana  Kabupaten/Kota,  tanah 
longsor  merupakan  salah  satu  jenis  bencana  yang  memiliki 
bobot  risiko  bahaya  paling  tinggi  dibandingkan  dengan 
beberapa  jenis  bencana  lainnya.  Hal  ini  disebabkan  karena 
tanah longsor memiliki frekuensi kejadian yang tinggi namun 
tanpa adanya peringatan mengenai bencana tersebut [2]. Pada 
Podes  2018  juga  memberikan  informasi  bahwa  jumlah  desa 
yang  mengalami  kejadian  bencana  tanah  longsor  merupakan 
jumlah terbanyak kedua setelah bencana banjir  yaitu terdapat 
sebanyak 10246 desa dalam tiga tahun terakhir [3] 

Kepala  BPBD  Provinsi  Sumatera  Selatan,  Iriansyah 
mengatakan “Ada 11 daerah di Sumatera Selatan yang rawan 
bencana  pada  musim  hujan. Untuk  wilayah dataran tinggi itu 
rawan berpotensi longsor. Seperti di Kabupaten Empat Lawang, 
Kota  Pagar  Alam,  Kabupaten  Lahat,  Muara  Enim,  dan 
Kabupaten  OKU  Selatan”  [4].  Sehingga  Kabupaten  Lahat 
merupakan  salah  satu  kabupaten  yang  memiliki  potensi 
terjadinya bencana tanah longsor. Hal ini dapat dilihat bahwa 
berdasarkan publikasi Indeks Rawan Bencana Indonesia (2011), 
Kabupaten Lahat memiliki indeks rawan bencana tanah longsor 
yang sedang [1]. Namun dari publikasi tahun 2013 dan 2018, 

menunjukkan bahwa Kabupaten Lahat memiliki indeks risiko 
bencana  tanah  longsor  yang  tinggi  [2]  [5].  Kemudian  dalam 
data  Potensi  Desa  2018  dan  pemberitaan  online 
juga 
menunjukkan  bahwa  terdapat  lebih  dari  3  kejadian  tanah 
longsor di Kabupaten Lahat per tahun. 

tanah 

Cukup 

tingginya  kejadian  bencana 

longsor  di 
Kabupaten  Lahat  ini  menyebabkan  perlunya  kesiapsiagaan 
mengenai bencana tanah longsor. Analisis atau pembuatan peta 
mengenai  kerawanan  tanah  longsor  merupakan  salah  satu 
alternatif  yang  dapat  digunakan  oleh  masyarakat  maupun 
pemerintah  sebagai  acuan  atau  perkiraan  dalam  upaya 
mencegah  dan  mengatasi  bencana  tanah  longsor.  Seperti 
penelitian yang telah dilakukan oleh Kalijati, dkk [6] dan M. A. 
Prasetyo  E  [7].  Namun  dari  kedua  penelitian  tersebut  belum 
dapat menggambarkan sepenuhnya mengenai kerawanan tanah 
longsor di seluruh wilayah Kabupaten Lahat karena dari kedua 
penelitian tersebut hanya melakukan penelitian pada satu desa. 
Selain  itu  juga  pada  penelitian  tersebut  belum  menerapkan 
sistem penginderaan jauh untuk memperoleh data.  

sistem  penginderaan 

Sehingga  dalam  penelitian  ini  dilakukan  pengembangan 
jauh  untuk 
dengan  menerapkan 
memperoleh  data  yang  akan  digunakan.  Kemudian  untuk 
mendapatkan data dari penginderaan jauh dapat memanfaatkan 
citra  satelit,  yang  dapat  diakses  dan  digunakan  secara  gratis. 
Selanjutnya  pada  penelitian  sebelumnya  untuk  mendapatkan 
kelas  kerawanan  diperlukan  perhitungan  pembobotan. 
Terdapat  empat  metode  pembobotan,  yaitu  metode  ranking, 
metode  rating,  metode  perbandingan  pasangan  (AHP),  dan 
metode  analisis  trade  off  [8].  Namun  karena  penelitian 
sebelumnya  hanya  menggunakan  metode  AHP  dalam 
melakukan  pembobotan.  Maka  dalam  penelitian  ini  akan 
menerapkan dua metode, yaitu metode ranking (jumlah ranking 
dan  ketergantungan  ranking)  dan  metode  perbandingan 
pasangan (AHP). Metode ranking bersifat subyektif, sedangkan 
metode  perbandingan  pasangan  cukup  bersifat  obyektif  dan 
mengandalkan penilaian orang yang cukup memahami masalah 
tersebut atau para ahli dalam mendapatkan skala prioritasnya. 

II.  TUJUAN PENELITIAN 

Tujuan dalam penelitian ini yaitu: 
a)  Memetakan daerah rawan tanah longsor di Kabupaten Lahat 
per desa di Kabupaten Lahat berdasarkan data penginderaan 
jauh. 

b)  Membandingkan  hasil  pemetaan  daerah  rawan  tanah 
longsor  dengan  metode  ranking  (jumlah  ranking,  metode 

 1 / 8 

 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

penelitian  ini  menunjukkan  bahwa  Kabupaten  Sumedang 
memiliki  tingkat  kerawanan  longsor  sedang  sampai  dengan 
tinggi. 

IV. METODE PENELITIAN  
Cakupan  pada  penelitian  ini  adalah  Kabupaten  Lahat, 
Sumatera  Selatan  dengan  377  desa  dan  perekaman  data  dari 
periode  1  Juli  2019  sampai  dengan  30  juni  2020.  Parameter 
yang  akan  digunakan  dalam  penelitian 
ini  yaitu  Arah 
Lereng/Aspek  Lereng  (A),  Curah  Hujan  (CH),  Geologi  (G), 
(KL),  dan  Penggunaan  Lahan(PL) 
Kemiringan  Lereng 
(Gambar 1). 

ketergantungan 
pasangan (AHP) dengan data kerawanan BNPB 

ranking),  dan  metode  perbandingan 

III. PENELITIAN TERKAIT 

R.  D.  Abrauw  [9]  melakukan  penelitian  “Wilayah  Rawan 
Longsor di Kota Jayapura”. Pada penelitian ini menggunakan 
dua  pemodelan  yaitu  model  pendugaan  longsor  Direktorat 
Vulkanologi dan Mitigasi Bencana  Geologi (DVMBG) tahun 
2005  dan  model  pendugaan  longsor  kombinasi  dari  Pusat 
Penelitian  Tanah.  Dimana  kedua  model  ini  menggunakan 
parameter  yang  hampir  sama  yaitu  parameter  curah  hujan, 
geologi,  jenis  tanah,  dan  kemiringan  lereng,  Namun  model 
pendugaan  longsor  kombinasi  dari  Pusat  Penelitian  Tanah 
menambahkan parameter lainnya itu parameter jarak sesar/fault 
dan kerapatan vegetasi. Hasil dari penelitian ini menunjukkan 
longsor  kombinasi  dari  Pusat 
bahwa  model  pendugaan 
Penelitian Tanah lebih representatif dan dapat digunakan untuk 
mengindentifikasi wilayah rawan longsor di Kota Jayapura. 

M.  A.  Prasetyo  E  dan  W.  D.  Hastuti  [7]  melakukan 
penelitian “Analisa Rawan Longsor Pada Daerah Kikim Timur 
Kabupaten  Lahat  dengan  Menggunakan  Metode  Mapping 
Permukaan”.  Pada  penelitian 
ini  menggunakan  metode 
pemetaan (mapping). Dimana parameter yang digunakan dalam 
penelitian  ini  adalah  geologi,  kemiringan  lereng,  dan  elevasi. 
Hasil  dari  penelitian  ini  menunjukkan  bahwa  terdapat  10% 
daerah  memilliki  nilai  rawan  terhadap  longsor,  50%  cukup 
rawan longsor, dan 40% tidak rawan longsor. 

M.  A.  Kalijati,  E.  Sutriyono  dan  S.  N.  Jati  [6]  melakukan 
penelitian  “Analisis  Bahaya  Longsor  dengan  Menggunakan 
Metode  Analytical  Hierarchy  Process  (AHP),  Desa  Lubuk 
Atung,  Kabupaten  Lahat”.  Pada  penelitian  ini  menggunakan 
metode  AHP.  Dimana  parameter  yang  digunakan  dalam 
lereng,  curah  hujan, 
penelitian 
penggunaan lahan, geologi, dan permeabilitas tanah. Hasil dari 
penelitian  ini  didapatkan  peta  bahaya  tanah  longsor  yang  
dibagi menjadi 4  kategori  yaitu, sangat rendah, rendah, sedang 
dan tinggi. Dimana daerah yang paling berpengaruh terhadap 
longsor  berada  pada  daerah  sekitar  kemiringan  lereng  (30% 
dari luasan daerah penelitian). 

ini  adalah  kemiringan 

R. K. Karina dan R. Kurniawan [10] melakukan penelitian 
“Identifikasi  Penggunaan  Lahan  Menggunakan  Citra  Satelit 
Landsat  8  Melalui  Google  Earth  Engine  Studi  Kasus  di 
Kabupaten  Lahat”.  Pada  penelitian  ini  menggunakan  metode 
deskriptif  dan  metode  analisis  citra  satelit.  Dimana  pada 
penelitian 
lahan 
menggunakan  supervised  classification).  Klasifikasian  ini 
dilakukan  untuk  kelas  kawasan  vegetasi,  badan  air,  lahan 
terbangun, tambak, rumput/semak,  lahan terbuka, dan  sawah. 
Hasil  dari  penelitian  ini  adalah  peta  penggunaan  lahan  yang 
diperoleh  memiliki  akurasi  keseluruhan  sebesar  89,38%  dan 
akurasi Kappa sebesar 85,21%. 

pengklasifikasian 

penggunaan 

ini 

M.  F.  Yassar,  dkk.  [11]  melakukan  penelitian  “Penerapan 
Weighted  Overlay  Pada  Pemetaan  Tingkat  Probabilitas  Zona 
Rawan  Longsor  di  Kabupaten  Sumedang,  Jawa  Barat”.  Pada 
ini  menggunakan  model  yang  dipakai  pada 
penelitian 
penelitian  Puslittanak  tahun  2004.  Dimana  parameter  yang 
digunakan  dalam  penelitian  ini  adalah  curah  hujan,  geologi, 
kemiringan lereng, penutupan lahan dan jenis tanah. Hasil dari 

Gambar 1. Diagram Alir Metode Analisis 

Selanjutnya  peneliti  melakukan  pengumpulan  data  terkait 
parameter  yang  akan  digunakan  melalui  penginderaan  jauh 
yaitu  citra  satelit  Landsat  8,  citra  satelit  CHIRPS,  dan  DEM 
SRTM,  serta  data  kerawanan  BNPB  2019  untuk  pengecekan 
akurasi.  Citra  Satelit  Landsat  8  akan  digunakan  untuk 
memperoleh  peta  Penggunaan  Lahan.  Dimana  dalam 
menentukan  klasifikasi  dari  penggunaan  lahan  menggunakan 
metode  supervised  classification  dan  kemudian  dilakukan 
pengecekan  keakurasian  peta  dengan  menggunakan  metode 
confussion matrix pada Tabel I (pers 1 dan pers2). Citra satelit 
CHIRPS akan  menghasilkan  peta  Curah Hujan. DEM  SRTM 
akan menghasilkan peta Kemiringan Lereng dan Arah Lereng. 

 2 / 8 

 
 
 
Dimana untuk semua pengolahan Citra satelit akan dilakukan 
pada Google Earth Engine. Sedangkan peta Geologi diperoleh 
dari Pusat Penelitian Pengembangan Geologi tahun 1983 yang 
petanya  bersumber  dari  Pusat  Penelitian  Pengembangan 
Geologi.  

Analisis  dimulai  dengan  melakukan  pembobotan  pada 
semua peta yang akan digunakan dengan menggunakan metode 
Ranking  (Metode  Jumlah  Ranking  (pers3)  dan  Metode 
Ketergantungan  Ranking  (pers4))  dan  Metode  Perbandingan 
Pasangan (AHP). Kemudian dari hasil tersebut akan dilakukan 
pengklasifikasian  menjadi tiga kelas kerawanan  yaitu rendah. 
sedang,  dan  tinggi  berdasarkan  Tabel  II  yang  diperoleh  dari 
pers 5. Selanjutnya untuk membandingkan metode mana yang 
lebih  baik  dari  ketiga  metode  tersebut  maka  akan  dicek 
keakurasiannya  dengan  menggunakan  metode  confussion 
matrix Tabel I (pers 1 dan pers2). 

TABEL I 
TABEL CONFUSSION MATRIX 

Dikelaskan ke kelas 

Kelas 
referensi 

A 

B 

C 

D 

A 

nAA 

nBA 

nCA 

nDA 

B 

nAB 

nBB 

nCB 

nDB 

Total piksel 
n+B 
Sumber: G. M. Foody [12]. 

n+A 

C 

nAC 

nBC 

nCC 

nDC 

n+C 

D 

nAD 

nBD 

nCD 

nDD 

n+D 

Jumlah 
piksel 

nA+ 

nB+ 

nC+ 

nD+ 

N 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

V.  KERANGKA PIKIR 

tanah 

Berdasarkan  Gambar  2,  kerangka  pikir  penelitian  ini 
didasarkan  karena  peneliti  melihat  kurangnya  informasi  dan 
adanya perbedaan informasi, serta kurangnya kewaspadaan dan 
kepedulian  masyarakat  mengenai 
longsor.  Maka 
dilakukan  penelitian  mengenai  kerawanan  tanah  longsor 
informasi  geografis  dan 
dengan  memanfaatkan  sistem 
penginderaan  jauh  agar  dapat  meminimalisasikan  waktu  dan 
biaya  dalam  mendapatkan  informasi  mengenai  daerah  yang 
rawan  tanah  longsor.  Penelitian  ini  akan  mengkombinasikan 
parameter PUSLITTANAK 2004 dan RBI BNPB 2016 dengan 
berdasarkan  ketersediaan  datanya.  Sehingga  parameter  yang 
akan digunakan dalam penelitian ini adalah arah lereng, curah 
hujan,  geologi,  kemiringan  lereng,  dan    penggunaan  lahan. 
Selanjutnya  untuk  mendapatkan  pemetaan  kerawanan  tanah 
longsor akan menggunakan metode pembobotan yaitu metode 
ranking  (metode  jumlah  ranking  dan  metode  ketergantungan 
ranking)  dan  metode  perbandingan  pasangan 
(AHP). 
Kemudian penelitian ini memiliki dua tujuan yaitu memetakan 
daerah  rawan  tanah  longsor  per  desa  dan  membandingkan 
pemetaan  dari  hasil  metode 
ranking,  metode 
ketergantungan  ranking,  dan  metode  AHP.  Dari  dua  tujuan 
tersebut akan menghasilkan output/keluaran berupa peta daerah 
rawan tanah longsor per desa dari metode-metode tersebut dan 
perbandingan hasil dari metode-metode tersebut. 

jumlah 

Persamaan akurasi yang digunakan adalah : 
)
𝑛𝑘𝑘
)   × 100%       (1) 

𝐴𝑘𝑢𝑟𝑎𝑠𝑖 𝐾𝑒𝑠𝑒𝑙𝑢𝑟𝑢ℎ𝑎𝑛  =   (

(∑

𝑞
𝑘=1
𝑛

𝐾𝑎𝑝𝑝𝑎 𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦  = [

𝑞
(𝑛 ∑
𝑛𝑘𝑘−
𝑘=1
𝑞
(𝑛2− ∑
𝑘=1

𝑞
 ∑
𝑘=1
𝑛𝑘+𝑛+𝑘

𝑛𝑘+𝑛+𝑘
)

)
]   × 100% (2) 

dimana: 
n = Banyaknya piksel dalam contoh 
nk+ = Jumlah piksel dalam baris ke-k 
n+k = Jumlah piksel dalam kolom baris ke-k 
nkk  =  Nilai  diagonal  dari  matriks  kontingensi  baris  ke-k  dan 
kolom ke-k 
Metode Pembobotan Ranking 

Gambar 2. Kerangka Pikir 

VI. HASIL DAN PEMBAHASAN 

𝑤𝑗 = (𝑛 − 𝑟𝑗 + 1)/ ∑(𝑛 − 𝑟𝑝 + 1)   

𝑤𝑗 = (1 𝑟𝑗⁄ )/ ∑(1 𝑟𝑘⁄ ) 

A.  Peta Parameter yang Dihasilkan 

1)  Penggunaan Lahan  

(3) 

 (4) 

TABEL II 
TABEL KLASIFIKASI TINGKAT KERAWANAN 

Total Skoring 

𝑆min  – (𝑆min +X) 

(𝑆min +X) – (𝑆min +2X) 

(𝑆min +2X) – 𝑆𝑚𝑎𝑘𝑠 
Sumber: BNPB [2]. 

Klasifikasi Tingkat 
Kerawanan  

Rendah 

Sedang 

Tinggi 

Warna 

Hijau 

Kuning 

Merah 

TABEL III 
SKOR PENGGUNAAN LAHAN  
Skor 
Penggunaan Lahan 
1 
Tambak, Waduk, Perairan 

Kota/Permukiman 
Hutan dan Perkebunan 

Semak Belukar 

Tegalan dan Sawah 

2 
3 

4 

5 

            Sumber: PUSLITTANAK 2004 dalam [11]. 

Persamaan Klasifikasi Tingkat Kerawanan 

𝑋 =

(𝑆𝑚𝑎𝑘𝑠−𝑆min )
3

(5) 

Sebelum peta penggunaan lahan diberi skor seperti pada 
Tabel III, terlebih dahulu dilakukan pengecekan keakurasian 
peta dengan menggunakan metode  confussion matrix. Dari 

 3 / 8 

 
 
 
   
 
 
metode  confussion  matrix  tersebut,  diperoleh  nilai  akurasi 
keseluruhan  untuk  penggunaan  lahan  di  Kabupaten  Lahat 
adalah sebesar 97.26% dan akurasi Kappa untuk penggunaan 
lahan di Kabupaten Lahat ini adalah sebesar 94.71%. Hasil 
pemetaan berdasarkan skor dapat dilihat pada Gambar 3. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Skor  curah  hujan  berdasarkan  PUSLITTANAK  2004 
dalam  M.  F.  Yassar  2020  [11]  memiliki  skor  1-5.  Dimana 
untuk  curah  hujan,  <1500  mm/tahun  diberi  skor  1,  1501-
2000 mm/tahun diberi skor 2,  2001-2500 mm/tahun diberi 
skor 3, 2501-3000 mm/tahun diberi skor 4, >3000 diberi skor 
5.  Hasil  pemetaan  berdasarkan  skor  dapat  dilihat  pada 
Gambar  4.  Berdasarkan  Gambar  4  diperoleh  hasil  bahwa 
curah hujan di Kabupaten Lahat dengan perekaman data dari 
tanggal 1 Juli 2019—30 Juni 2020 jika dilihat dari persentase 
luasannya, curah hujan di Kabupaten Lahat 93.95% masuk 
ke kategori sedang (2001-2500 mm/tahun) dan 6.05% masuk 
ke kategori basah (2501-3000 mm/tahun). 

3)  Kemiringan Lereng 

Skor  kemiringan  lereng  berdasarkan  PUSLITTANAK 
2004  dalam  M.  F.  Yassar  2020  [11]  memiliki  skor  1-5. 
Dimana untuk kemiringan lereng, datar diberi skor 1, landai 
diberi skor 2, agak curam diberi skor 3, curam diberi skor 4, 
sangat curam diberi skor 5. Hasil pemetaan berdasarkan skor 
dapat dilihat pada Gambar 5. 

Gambar 3. Peta Penggunaan Lahan 

Berdasarkan  Gambar  3  diperoleh  hasil  bahwa 
persebaran  penggunaan  lahan  di  Kabupaten  Lahat  dengan 
perekaman  data  dari  tanggal  1  Juli  2019—30  Juni  2020, 
yaitu terdapat 1.38% kawasan tambak,waduk, dan perairan, 
5.54%  kawasan  kota/permukiman,  80.62%  kawasan  hutan 
dan perkebunan, 5.93% kawasan semak belukar, dan 6.52% 
kawasan tegalan dan sawah. 

2)  Curah Hujan 

Gambar 5. Peta Kemiringan Lereng Kabupaten Lahat 

Berdasarkan  Gambar  5  diperoleh  hasil  bahwa 
kemiringan  lereng  di  Kabupaten  Lahat  jika  dilihat  dari 
persentase  luasannya,  mayoritas  kemiringan  lereng  di 
Kabupaten  Lahat  31%  adalah  datar  atau  hanya  memiliki 
kemiringan  sekitar  0%-8%.  Sedangkan  kemiringan  lereng 
yang  paling  sedikit  luasannya  di  Kabupaten  lahat  yaitu 
kemiringan  lereng  yang  sangat  curam,  dimana  kemiringan 
ini >45% dari lereng 

4)  Arah Lereng  

Berdasarkan    RBI  BNPB  2016  [13]  arah  lereng 
memiliki  skor  1-8.  Dimana  untuk  arah  lereng  datar  diberi 
skor  0,  utara  diberi  skor  1,  barat  laut  diberi  skor  2,  barat 
diberi skor 3, timur laut diberi skor 4, barat daya diberi skor 
5,  timur  diberi  skor  6,  tenggara  diberi  skor  7,  dan  selatan 

 4 / 8 

Gambar 4. Peta Curah Hujan Kabupaten Lahat 

 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Berdasarkan    RBI  BNPB  2016  [13]  geologi  memiliki 
skor  1-3.  Dimana  untuk  kategori  aluvial  diberi  skor  1, 
sedimen  diberi  skor  2,  vulkanik  diberi  skor  3.  Hasil 
pemetaan berdasarkan skor dapat dilihat pada Gambar 7. 

Berdasarkan  Gambar  7  diperoleh  hasil  bahwa 
persebaran  geologi  di  Kabupaten  Lahat  jika  dilihat  dari 
persentase luasannya, mayoritas geologi di Kabupaten Lahat 
62.07%  adalah  batuan  sedimen.  Sedangkan  jenis  batuan 
yang paling sedikit di Kabupaten lahat yaitu batuan aluvial 
dengan persentase 03.07%. 

B.  Pemetaan daerah rawan tanah longsor di Kabupaten Lahat 

per desa di Kabupaten Lahat 

1)  Metode Ranking 

a)  Metode Jumlah Ranking 

Menggunakan  pembobotan  dengan  metode  jumlah 

ranking menghasilkan persamaan: 

𝑁𝑖𝑙𝑎𝑖 𝐾𝑒𝑟𝑎𝑤𝑎𝑛𝑎𝑛 = 0.27 × 𝐶𝐻 + 0.33 × 𝐾𝐿 + 0.13 ×

𝑃𝐿 + 0.20 × 𝐺 + 0.07 × 𝐴 (6)   

diberi skor 8. Hasil pemetaan berdasarkan skor dapat dilihat 
pada Gambar 6. 

Gambar 6. Peta Arah Lereng Kabupaten Lahat 

Berdasarkan  Gambar  6  diperoleh  hasil  bahwa 
persebaran arah lereng di Kabupaten Lahat jika dilihat dari 
persentase luasannya, arah lereng di Kabupaten Lahat lebih 
banyak  yang  ke  utara  karena  memiliki  persentase  luasan 
yang  lebih  tinggi  dibandingkan  dengan  arah  lereng  yang 
lainnya, yaitu sebesar 14.94%. Namun semua arah lereng di 
Kabupaten  Lahat  memiliki  persentase  luasan  yang  hampir 
sama yaitu antara 11.20% sampai dengan 14.94%. 

5)  Geologi 

Gambar 8. Peta Kerawanan Tanah Longsor dengan Metode Jumlah 

Ranking 

Dari  hasil  persamaan  metode  jumlah  ranking  tersebut 
(pers  6)  diperoleh  dua  kelas  kerawanan  yaitu  kerawanan 
rendah dan sedang. Dimana dari 377 desa, 148 desa masuk 
ke kerawanan rendah, 228 desa masuk ke kerawanan sedang, 
dan 1 desa masuk ke kerawanan tinggi (Gambar 8). 

b)  Metode Ketergantungan Ranking 

Menggunakan 

pembobotan 

dengan 

metode 

ketergantungan ranking menghasilkan persamaan: 

𝑁𝑖𝑙𝑎𝑖 𝐾𝑒𝑟𝑎𝑤𝑎𝑛𝑎𝑛 = 0.22 × 𝐶𝐻 + 0.44 × 𝐾𝐿 + 0.11 ×

𝑃𝐿 + 0.14 × 𝐺 + 0.09 × 𝐴 (7)   

Dari  hasil  persamaan  metode  ketergantungan  ranking 
tersebut  (pers  7)  diperoleh  tiga  kelas  kerawanan  yaitu 
kerawanan rendah, sedang, dan tinggi. Dimana dari 377 desa, 

 5 / 8 

Gambar 7. Geologi Kabupaten Lahat 

 
 
 
 
 
 
199 desa  masuk ke kerawanan rendah, 174 desa  masuk ke 
kerawanan  sedang,  dan  4  desa  masuk  kerawanan  tinggi 
(Gambar 9). 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 10. Peta Kerawanan Tanah Longsor dengan Metode AHP 

3)  Data Kerawanan Tanah Longsor BNPB 

Gambar 9. Peta Kerawanan Tanah Longsor dengan Metode 

Ketergantungan Ranking 

2)  Metode  Perbandingan  Pasangan  (Metode  Analytical 

Hierarchy Process/AHP) 
Pada metode AHP, untuk mendapatkan pembobotan tiap-
tiap  parameter  dilakukan  terlebih  dahulu  wawancara  dengan 
kuesioner kepada 5 narasumber dari BPBD Provinsi Sumatera 
Selatan.  Dari  hasil  kuesioner  ke-5  narasumber  tersebut  akan 
dilakukan perhitungan untuk mendapatkan pembobotann tiap-
tiap parameter tersebut. 

Tiga  tahapan  untuk  mendapatkan  pembobotan  dengan 

menggunakan metode Analytical Hierarchy(AHP), yaitu. 
Tahap 1: Membuat Matriks Perbandingan Pasangan  
Tahap 2: Menghitung bobot parameter 
Tahap 3: Estimasi rasio konsistensi 
Kemudian setelah mendapatkan estimasi rasio konsistensi dari 
ke-5  hasil  kuesioner  tersebut.  Maka  akan  dipilih  rasio 
konsistensi  (CR)  <  0.10  dan  yang  memiliki  CR  paling  kecil 
diantara ke-5 CR tersebut. 
tahapan 

tersebut,  pembobotan  dengan 

Dari  hasil 

menggunakan metode  AHP menghasilkan persamaan: 

𝑁𝑖𝑙𝑎𝑖 𝐾𝑒𝑟𝑎𝑤𝑎𝑛𝑎𝑛 = 0.19 × 𝐶𝐻 + 0.45 × 𝐾𝐿 + 0.12 ×
𝑃𝐿 + 0.15 × 𝐺 + 0.09 × 𝐴 (8) 

Dari  hasil  persamaan  metode  AHP  tersebut  (pers  8) 
diperoleh tiga kelas kerawanan yaitu kerawanan rendah, sedang, 
tinggi.  Dimana  dari 377 desa, 187 desa  masuk  ke  kerawanan 
rendah,  186  desa  masuk  ke  kerawanan  sedang,  dan  4  desa 
masuk ke kerawanan tinggi (Gambar 10). 

Gambar 11. Peta Kerawanan Tanah Longsor Data BNPB 

Pada  buku katalog desa/kelurahan rawan longsor BNPB 
2019, Kabupaten Lahat  memiliki tiga  kelas  kerawanan  yaitu 
kerawanan rendah, sedang, dan tinggi. Dimana dari 377 desa, 
186  desa  masuk  ke  kerawanan  rendah,  144  desa  masuk 
kerawanan  sedang,  dan  47  desa  masuk  kerawanan  tinggi 
(Gambar 11) [14]. 

C.  Perbandingan Metode 
1)  Metode Ranking 

a)  Metode Jumlah Ranking 

 6 / 8 

 
 
 
   
 
 
TABEL IV 
TABEL CONFUSSION MATRIX METODE JUMLAH RANKING 

Kelas 
referensi 

Dikelaskan ke kelas 
Sedang 

Rendah 

Tinggi 

Rendah 

129 

Sedang 

Tinggi 

18 

1 

Total 

148 

57 

126 

45 

228 

0 

0 

1 

1 

Total  

186 

144 

47 

377 

Berdasarkan  Tabel  IV,  dengan  menggunakan  metode 
jumlah ranking diperoleh akurasi keseluruhan sebesar 67.90% 
dan    akurasi  kappa  sebesar  44.18%.  Dari  hasil  tersebut 
menunjukkan  bahwa  dengan  menggunakan  metode  jumlah 
ranking, untuk kelas kerawanan rendah hanya terdapat  129 
desa  yang  akurat  dari  186  desa,  untuk  kelas  kerawanan 
sedang  terdapat  126  desa  yang  akurat  dari  144  desa,  dan 
untuk  kelas  kerawanan  tinggi  hanya  terdapat  1  desa  yang 
akurat dari 47 desa.  

b)  Metode Ketergantungan Ranking 

TABEL V 
TABEL CONFUSSION MATRIKS METODE KETERGANTUNGAN 
RANKING 

Kelas 
referensi 

Dikelaskan ke kelas 
Sedang 
Rendah 

Tinggi 

Total  

Rendah 

156 

Sedang 

Tinggi 

40 

3 

Total 

199 

30 

104 

40 

174 

0 

0 

4 

4 

186 

144 

47 

377 

Berdasarkan  Tabel  V,  dengan  menggunakan  metode 
ketergantungan  ranking  diperoleh  akurasi  keseluruhan 
sebesar  70.03%  dan  akurasi  kappa  sebesar  46.66%.  Dari 
hasil  tersebut  menunjukkan  bahwa  dengan  menggunakan 
metode  ketergantungan  ranking,  untuk  kelas  kerawanan 
rendah hanya terdapat 156 desa yang akurat dari 186 desa, 
untuk kelas kerawanan sedang hanya terdapat 104 desa yang 
akurat  dari  144  desa,  dan  untuk  kelas  kerawanan  tinggi 
hanya terdapat 4 desa yang akurat dari 47 desa. 

2)  Metode  Perbandingan  Pasangan  (Metode  Analytical 

Hierarchy Process/AHP) 

TABEL VI 
TABEL CONFUSSION MATRIKS METODE AHP 
Total  
Kelas 
referensi 

Dikelaskan ke kelas 
Sedang 
Rendah 

Tinggi 

Rendah 

149 

Sedang 

Tinggi 

36 

2 

Total 

187 

37 

108 

41 

186 

0 

0 

4 

4 

186 

144 

47 

377 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Berdasarkan Tabel VI, dengan menggunakan metode AHP 
diperoleh  akurasi  keseluruhan  sebesar  69.23%  dan  akurasi 
 sebesar  45.59%.  Dari  hasil  tersebut  menunjukkan 
kappa 
bahwa  dengan  menggunakan  metode  AHP,  untuk  kelas 
kerawanan rendah hanya terdapat  149 desa yang akurat dari 
186 desa, untuk kelas kerawanan sedang hanya  terdapat 108 
desa  yang  akurat  dari  144  desa,  dan  untuk  kelas  kerawanan 
tinggi hanya terdapat 4  desa yang akurat dari 47 desa. 

D.  Diskusi dan Pembahasan 

[15],  berisikan 

Penelitian  Mohammed,  dkk 

tentang 
membandingkan  kinerja  empat  teknik  penilaian  pembobotan 
berbasis ranking, Jumlah Ranking, Ketergantungan Ranking, 
Eksponen  Ranking,  dan  Ranking  Urutan  Pusat  (ROC)  pada 
lima  kriteria  e-learning  yang  diidentifikasi  untuk  memilih 
metode  bobot  yang  terbaik.  Dari  hasil  penelitian  tersebut 
menunjukkan  bahwa  bobot  yang  dihasilkan  dengan 
menggunakan  metode  ketergantungan  ranking  adalah  yang 
terbaik  dibandingkan  ketiga  metode  lainnya.  Hal  ini  sejalan 
dengan hasil yang diperoleh dalam penelitian ini, yaitu metode 
ketergantungan ranking menunjukkan hasil keakurasian yang 
lebih baik dibandingkan metode jumlah ranking. Tetapi hasil 
penelitian ini tidak sejalan dengan hasil penelitian Saeid, dkk 
[16] yang  menunjukkan bahwa  metode jumlah ranking  yang 
lebih baik atau lebih disarankan dibandingkan dengan ketiga 
metode 
lainnya  sebagai  pengganti  untuk  bobot  dalam 
mengevaluasi beberapa aplikasi web. 

VII. 

PENUTUP 

Berdasarkan hasil diatas dapat diperoleh kesimpulan bahwa 
dengan menggunakan metode jumlah ranking terdapat 148 desa 
masuk  ke  kerawanan  rendah,  228  desa  masuk  ke  kerawanan 
sedang,  dan  1  desa  masuk  ke  kerawanan  tinggi.  Kemudian 
dengan menggunakan metode ketergantungan ranking terdapat 
199  desa  masuk  ke  kerawanan  rendah,  174  desa  masuk  ke 
kerawanan  sedang,  dan  4  desa  masuk  kerawanan  tinggi. 
Sedangkan  dengan  menggunakan  metode  perbandingan 
pasangan (AHP) terdapat 187 desa masuk ke kerawanan rendah, 
186  desa  masuk  ke  kerawanan  sedang,  dan  4  desa  masuk  ke 
kerawanan tinggi. Kemudian dari hasil akurasi ketiga metode 
tersebut  dapat  disimpulkan  bahwa  metode  ketergantungan 
ranking  lebih  baik  dari  metode  Jumlah  Ranking  dan  AHP 
dalam  kasus  penelitian  ini,  karena  metode  ketergantungan 
ranking  memiliki  akurasi  yg  lebih  tinggi  dari  kedua  metode 
tersebut  yaitu  sebesar  70.03%  untuk  akurasi  keseluruhan  dan 
46.66%  untuk  akurasi  kappa.  Sedangkan  metode  jumlah 
ranking dan metode AHP memiliki keakurasian berturut-turut 
adalah sebesar 67.90% dan 69.23% untuk akurasi keseluruhan 
serta 44.18% dan 45.35% untuk akurasi kappa. 

Kemudian  berdasarkan  kesimpulan  diatas  dapat  diperoleh 

beberapa saran sebagai berikut. 
1) 

BPBD Provinsi Sumatera Selatan dan BNPB disarankan 
telah  menerapkan  Sistem  Penginderaan  Jauh  untuk 
memperoleh parameter-parameter yang akan digunakan 
untuk  lebih  menghemat  biaya  dan  waktu  agar  dapat 
lebih  mudah  memberikan 
terkini 
mengenai pembaharuan kerawanan tanah longsor. 

informasi  yang 

 7 / 8 

 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

2) 

penelitian 

Untuk 
dapat  mencoba 
selanjutnya 
menggunakan  parameter-parameter  yang  sama  yang 
digunakan  oleh  BNPB  sebelumnya  agar  dapat 
diharapkan  memberikan  hasil  yang  lebih  akurat.  Serta 
dapat  menggunakan  metode  pembobotan  yang  telah 
dilakukan  pengembangan 
seperti  pengembangan 
metode  perbandingan  pasangan  yaitu  metode  Fuzzy 
Analytical Hierarchy process (F-AHP), dan sejenisnya. 

DAFTAR PUSTAKA 

[1]  BNPB, Indeks Rawan Bencana Indonesia, 2011. 
[2]  BNPB, Indeks Risiko Bencana Indonesia (IRBI), 2018. 
[3]  BPS,  “Banyaknya  Desa/Kelurahan  Menurut  Jenis  Bencana  Alam  dalam 
Tiga  Tahun  Terakhir  (Desa),  2018,”  2018.  [Online].  Available: 
https://www.bps.go.id/indicator/168/954/1/banyaknya-desa-kelurahan-
menurut-jenis-bencana-alam-dalam-tiga-tahun-terakhir.html.. 

[4]  detiksumsel.com, “Sebagian Sumsel Terancam Longsor dan Banjir,” 2020. 
[5]  BNPB, Indeks Risiko Bencana Indonesia, 2013. 
[6]  M.  A.  Kalijati,  E.  Sutriyono  dan  S.  N.  Jati,  “Analisis  Bahaya  Longsor 
dengan Menggunakan Metode Analytical Hierarchy Process (AHP) , Desa 
Lubuk  Atung,  Kabupaten  Lahat,”  Seminar  Nasional  AVoER  XI  2019, 
Oktober 2019. 

[7]  E.  M.  A.  Prasetyo  dan  W.  D.  Hastuti,  “Analisa  Rawan  Longsor  Pada 
Daerah  Kikim  Timur  Kabupaten  Lahat  dengan  Menggunakan  Metode 
Mapping Permukaan,” 2019. 

[8]  M. B. Selamat , “Modul Praktikum Sistem Iinformasi Geografis Jurusan 

Ilmu Kelautan-FIKP-Universitas Hasanuddin,” Makassar, 2002. 

[9]  R.  D.  Abrauw  ,  “Wilayah  Rawan  Longsor  di  Kota  Jayapura,”  Jurnal 

Geografi Lingkungan Tropik, vol. I, pp. 14-28, 2017. 

[10] R.  K.  Karina  dan  R.  Kurniawan,  “Identifikasi  Penggunaan  Lahan 
Menggunakan Citra Satelit Landsat 8 Melalui Google Earth Engine Studi 
Kasus  di  Kabupaten  Lahat,”  Seminar  Nasional  Official  Statistics  2019: 
Pengembangan Official Statistics dalam mendukung Implementasi SDGs, 
2020. 

[11] M. F. Yassar, M. Nurul, N. Nadhifah, N. F. Sekarsari, R. Dewi, R. Buana, 
S.  N.  Fernandez  dan  K.  A.  Rahmadhita,  “Penerapan  Weighted  Overlay 
Pada Pemetaan Tingkat Probabilitas Zona Rawan Longsor di Kabupaten 
Sumedang,  Jawa  Barat,”  Jurnal  Geosains  dan  Remote  Sensing  (JGRS), 
vol. I, pp. 1-10, 2020. 

[12] G. M. Foody, “Status of Land Cover Classification Accuracy Assessment,” 

Remote Sensing of Environment 80, pp. 185-201, 2001. 

[13] BNPB, Risiko Bencana Indonesia, 2016. 
[14] BNPB, Katalog Desa/Kelurahan Rawan Tanah Longsor, 2019. 
[15] H. J. Mohammed, M. M. Kasim dan  I. N. M. Shaharanee, “Selection of 
suitable  e-learning  approach  using  TOPSIS  technique  with  best  ranked 
criteria weights,” dalam AIP Conference Proceedings, 2017.  

[16] M. Saeid, A. A. A. Ghani dan H. Selamat, “Rank-Order Weighting of Web 
Attributes  for  Website  Evaluation,”  The  International  Arab  Journal  of 
Information Technology, vol. 8, 2011. 

 8 / 8 

 
 
 
 
 
"
221709961,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Mendeteksi Pergerakan Manusia Melalui Passive 
Mobile Positioning Data (MPD) Saat Pandemi 
COVID-19 di Indonesia 

Ratriani Retno Wardani (221709961, 4SD1) 
Dosen Pembimbing: Setia Pramana, Ph.D. 

Ringkasan— Indonesia terdampak pandemi COVID-19 mulai 
tahun 2020. Berbagai kebijakan guna menurunkan angka kasus 
COVID-19 terus diupayakan pemerintah. Namun kenyataannya 
banyak masyarakat yang mengabaikan kebijakan tersebut hingga 
akhirnya pemerintah membuat sebuah aplikasi berbasis GPS dan 
bluetooth untuk memantau pergerakan manusia. GPS merupakan 
jenis  MPD  yang  bersifat  aktif.  Penelitian  ini  dilakukan  untuk 
mendeteksi daerah tempat tinggal dan pergerakan manusia saat 
pandemi  COVID-19  dengan  menggunakan  MPD  yang  bersifat 
pasif  yaitu  data  Call  Detail  Records  (CDR).  Dengan  adanya 
penelitian 
ini  diharapkan  dapat  membantu  mengatasi 
kekurangan yang ada pada GPS dan bluetooth dalam mendeteksi 
pergerakan  manusia.  Hasil  yang  didapatkan,  algoritma  yang 
dibentuk dengan memanfaatkan MPD mampu mendeteksi daerah 
tempat tinggal dan pergerakan manusia dalam wilayah tertentu. 
Pergerakan  pada  level  kecamatan,  dapat  berpotensi  menjadi 
pemicu  penularan  COVID-19.  Pergerakan  yang  terdeteksi 
dalam 
menunjukkan 
mengendalikan pergerakan manusia dinilai cukup berhasil. 

pemerintah 

kebijakan 

bahwa 

Kata Kunci— Human mobility, Mobile Positioning Data, Call 

Detail Records, Reverse Geocoding, Origin Destination. 

I.  LATAR BELAKANG 

Pada  tahun  2020,  dunia  terdampak  pandemi  COVID-19 
salah satunya Indonesia. Berdasarkan data yang diunggah oleh 
KPCPEN, Indonesia memiliki 406.945 kasus COVID-19 yang 
terkonfirmasi  per  31  Oktober  2020.  Dalam  rangka  menekan 
penyebaran  COVID-19,  pemerintah  Indonesia  membuat 
serangkaian  kebijakan  secara  tertulis  dan  tidak  tertulis. 
Misalnya  kegiatan  sekolah  secara  online,  work  from  home, 
social atau physical distancing, hingga kegiatan beribadah pun 
dilakukan di rumah masing-masing.  

Akan tetapi, kebijakan yang dibuat tetap menghasilkan pro 
dan kontra bagi masyarakat. Berita yang diunggah di  website 
detik.com membuktikan bahwa terdapat 12.606 pelanggar saat 
seminggu PSBB Jilid I diterapkan. Pernyataan di atas didukung 
oleh penelitian yang dilakukan oleh Huljanah, dkk. (2020) yaitu 
pelanggar  PSBB  sebanyak  hampir  80  persen,  tidak  memakai 
masker  sebanyak  hampir  80  persen,  keluar  rumah  sebanyak 
hampir  70  persen,  dan  mengabaikan  protokol  kesehatan 
sebanyak lebih dari 80 persen.  

Dalam  menangani  permasalahan 

tersebut,  Kominfo 
berkoordinasi dengan Kemenkes, BUMN, BNPB, dan operator 
aplikasi  pemantau 
telekomunikasi  untuk  menciptakan 
pergerakan  masyarakat  guna  meredam  wabah  COVID-19  di 
Indonesia  yaitu  PeduliLindungi.  Aplikasi  tersebut  bekerja 
dengan Global Positioning System (GPS) dan bluetooth aktif. 

Ketika ada gadget lain dalam suatu radius yang juga terdaftar 
di  PeduliLindungi,  maka  akan  terjadi  pertukaran  ID  anonim 
yang akan direkam oleh gadget masing-masing. 

Dalam 

lokasi  ponsel  dalam 

jurnal  Aryasena,  dkk.  (2016)  GPS  memiliki 
kelemahan, yaitu apabila digunakan di dalam ruangan (gedung) 
dan/atau  apabila  pengguna  lupa  mengaktifkan  GPS  maka 
aplikasi  tersebut  tidak  dapat  bekerja  secara  optimal.  Sama 
seperti  dengan  GPS,  ketika  pengguna  lupa  mengaktifkan 
bluetooth  maka  aplikasi  tersebut  tidak  dapat  bekerja  secara 
optimal sehingga pergerakan masyarakat tidak dapat dipantau.  
Berdasarkan  permasalahan  di  atas  maka  dilakukan  sebuah 
penelitian untuk mendeteksi pergerakan manusia saat pandemi 
COVID-19  menggunakan  Mobile  Positioning  Data  (MPD). 
MPD  adalah  sekumpulan  data  yang  digunakan  untuk 
menemukan  lokasi  telepon  selular  menggunakan  gelombang 
radio.  Terdapat  dua  jenis  MPD,  yaitu  active  dan  passive. 
Penelitian  ini  menggunakan  passive  MPD  yang  terdiri  dari 
koordinat 
jaringan  selular,  yang 
dikumpulkan  secara  otomatis  oleh  Mobile  Network  Operator 
(MNO) saat terjadi transaksi seperti telepon, pesan singkat, dll 
(Ahas,  dkk.,  2010).  Secara  otomatis  setiap  telepon  genggam 
memiliki  data  MPD  ini  sehingga  tidak  ada  alasan  bahwa 
pengguna lupa mengaktifkan data lokasi (GPS) atau bluetooth.  
Penggunaan  MPD  memiliki  keunggulan  dalam  beberapa 
aspek seperti ketepatan waktu (hampir mendekati waktu nyata), 
akses ke informasi statistik (indikator) baru, peluang kalibrasi 
untuk data yang ada, ruang dan resolusi dan akurasi yang tepat 
waktu (Eurostat, 2014). Pendeteksian pergerakan manusia juga 
penting dilakukan pada era ini. Dengan adanya data mobilitas, 
pemerintah  dapat  menganalisis  faktor  risiko  dan  menangani 
penyebaran COVID-19 di suatu daerah (Pramana, dkk., 2021). 
Penelitian  dengan  MPD  ini  juga  didukung  dengan  data  dari 
publikasi Statistik Telekomunikasi Indonesia Tahun 2019 yaitu 
persentase penduduk yang menggunakan telepon selular tahun 
2019 mencapai 63,53 persen dari total penduduk Indonesia.   

II.  TUJUAN PENELITIAN 

Tujuan dari penelitian ini antara lain: 

1. Mendeteksi  daerah  tempat  tinggal  dari  pengguna  telepon 
selular melalui Passive Mobile Positioning Data (MPD). 
2. Mendeteksi  pergerakan  atau  mobilitas  manusia  melalui 
Passive  Mobile  Positioning  Data  (MPD)  saat  pandemi 
COVID-19 di Indonesia. 

3. Membandingkan pergerakan manusia dengan data  COVID-

19 pada suatu daerah dan periode tertentu.   

 1 / 8 

 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

No 

Judul 

1  Feasibility Study on 
the Use of Mobile 
Positioning Data for 
Tourism Statistics 

Penulis, Publikasi 
Eurostat. (2014). 

2  Detecting home 

locations from CDR 
data: introducing 
spatial uncertainty to 
the state-of-the-art. 

Vanhoof, Maarten 
& Reis, Fernando 
& Smoreda, 
Zbigniew & Ploetz, 
Thomas. (2018). 

3  Panduan Penggunaan 
Mobile Positioning 
Data (MPD) Dalam 
Statistik Mobilitas 
Nonpermanen 

Subdirektorat 
Statistik Mobilitas 
Penduduk dan 
Tenaga Kerja 

III. PENELITIAN TERKAIT 
TABEL I 
TABEL LITERATUR 

Tertulis 
The use of mobile positioning data has the potential to improve several aspects of 
tourism (and other) statistics, such as timeliness (in some cases up to near real-time), 
access to statistical information previously not available (new indicators), calibration 
opportunities for existing data, space and timely resolution and accuracy. 
(hal. 8) 
The five algorithms define home:  
1. The majority of both outgoing and incoming calls and texts was made;  
2. The maximal number of distinct days with phone activities —both outgoing and 

incoming calls and texts— was observed;  

3. Most phone activities were recorded during 7pm and 9am;  
4. Most phone  activities  were  recorded,  implementing a  spatial perimeter of  1000 

meter around a cell-tower that aggregates all activities within and  

5. The combination of 3) and 4) thus most phone activities recorded during 7pm and 

9am and implementing a spatial perimeter of 1000 meter. 

(hal. 5) 
Level  data  ini  harus  dibangun  menggunakan  algoritma  tertentu  yang  dapat 
menyederhanakan dan  mentransformasikan  data  transaksi  dari  lokasi  BTS  tertentu 
menjadi  lokasi  titik  menetap  (staypoint)  pelanggan  yang  tetap  merepresentasikan 
pergerakan pelanggan sesuai kondisi pada level aslinya. 
(hal. 19) 

Komentar 
Memperkuat 
latar 
belakang. 

Referensi 
untuk 
mendeteksi 
daerah tempat 
tinggal. 

Referensi 
metode untuk 
membentuk 
staypoint. 

IV. METODE PENELITIAN  
Metode  penelitian  yang  digunakan  digambarkan  pada  alat 

bantu visual berikut ini. 

Gambar 1. Metode penelitian 

Data  yang  digunakan  merupakan  data  sekunder  yaitu  SHP 
Kecamatan  tahun  2019  dari  BPS,  dan  data  CDR  dari  Telco 
Indosat yang diperoleh melalui kerjasama dengan UGM. Data 
CDR memiliki 4 variabel yaitu Timestamp, Msisdn, Longitude, 
dan  Latitude.  Data  ini  berjumlah  16.493.947  record  dengan 
4.446 unique ID (UID) dimana data ini diambil selama 82 hari 
yaitu dari Maret – Juni 2020. Kemudian data COVID-19 yang 
diperoleh dari BNPB (dimulai pada 23 April 2020). 

Pada  tahapan  preprocessing,  data  yang  digunakan  adalah 
MPD.  Pengaturan  format  ditujukan  untuk variabel  timestamp 
yang diubah ke dalam format date. Penghapusan missing value 
dilakukan dengan tujuan tidak ada data yang kosong di setiap 
barisnya. Penghapusan duplikasi data dilakukan dengan tujuan 
agar  tidak  ada  double  data.  Reverse  geocoding  merupakan 
tahapan  mengubah  titik-titik  koordinat  (longitude,  latitude) 
menjadi alamat yang dapat dipahami oleh pembaca. Algoritma 
yang  digunakan  adalah  “join”  dengan  SHP  Kecamatan. 
Penghapusan single transaction adalah menghapus UID yang 
muncul satu kali selama perekaman.  

Di  tahapan  processing  data,  staypoint  digunakan  untuk 
mendeteksi daerah tempat tinggal. Algoritma yang digunakan 
adalah  dengan  menentukan  durasi  dari  setiap  lokasi  yang 
disinggahi  oleh  UID.  Kemudian  menghapus  data  yang 
durasinya kurang dari 15 menit. Data stop detection digunakan 
untuk  menganalisis  variabel  daerah  tempat  tinggal  secara 
mingguan.  Algoritma  yang  digunakan  adalah  menyimpan 
semua lokasi yang disinggahi pengguna.  

Daerah tempat tinggal dibentuk dari data staypoint dengan 
algoritma  yaitu  nomor  selular  dengan  frekuensi  terekam 
terbanyak  pada  suatu  lokasi  di  saat  malam  hari  yaitu  pukul 
19.00 – 09.00 keesokan harinya (Vanhoof, dkk., 2018). Origin 
Destination  (OD)  merupakan  metode  analisis  untuk  melihat 
pergerakan  manusia.  Algoritma  yang  digunakan  adalah 
menyimpan baris ke-i sebagai origin dan baris ke-(i+1) sebagai 
destination  (berulang  hingga  baris  terakhir).  Peneliti  juga 
menggunakan periodisasi dalam menganalisis OD yang dibagi 
dalam 6 (enam) waktu. Periodisasi tersebut dapat dilihat pada 
Gambar 2. 

Gambar 2. Periodisasi 

V.  KERANGKA PIKIR 

Kerangka pikir yang digunakan pada penelitian ini disajikan 
pada  Gambar  3.  Penelitian  ini  dibagi  ke  dalam  dua  kategori 
besar  yaitu  mendeteksi  daerah  tempat  tinggal  (panah  merah) 
dan mendeteksi pergerakan (mobilitas) manusia (panah biru). 

 2 / 8 

 
 
 
 
 
 
 
Gambar 3. Kerangka pikir 

VI. HASIL DAN PEMBAHASAN 

1.  EKSPLORASI DATA 

Data  awal  yang  berjumlah  16.493.947  data  dilakukan 
pengecekan  missing 
sehingga 
menghasilkan  13.060.707  data  dengan  4.446  UID.  Data 
tersebut  tersebar  ke  5  (lima)  pulau  di  Indonesia,  yaitu  pulau 
Sumatera, Jawa, Kalimantan, Sulawesi, dan pulau Bali.  

duplikasi 

value 

dan 

Dengan  dilakukannya  reverse  geocoding,  dapat  diketahui 
bahwa data tersebar ke 21 provinsi dengan 201 kabupaten/kota 
atau 2.350 kecamatan yang ada di Indonesia. Akan tetapi, data 
yang  dihasilkan  setelah  proses  reverse  geocoding  adalah 
sejumlah 13.033.051 data. Data ini berkurang sebanyak 27.656 
dari  data  yang  telah  mengalami  proses  penghapusan.  Data 
tersebut merupakan titik-titik koordinat dari pengguna telepon 
selular  yang  berada  di  pantai/lautan  sehingga  tidak  dapat 
dilakukan proses reverse geocoding dikarenakan data tersebut 
memang tidak memiliki alamat. Hal ini mungkin saja  terjadi, 
misalnya apabila pengguna sedang melakukan pekerjaan yang 
mengharuskan berada di laut, atau bahkan sedang menyebrang 
dari pulau Jawa ke pulau Sumatera atau dari pulau satu ke pulau 
lainnya.  Pada  saat  itu  terjadi  transaksi  ponsel  sehingga  data 
lokasi pengguna tercatat di lautan. Oleh karena itu, data yang 
tidak memiliki alamat ini tidak digunakan. Selain itu, terdeteksi 
2 (dua) UID yang kemunculannya hanya satu kali sehingga data 
ini  juga  perlu  dihapus  karena  tidak  dapat  dianalisis.  Pada 
akhirnya,  data  yang  digunakan  berjumlah  13.033.049  data 
dengan 4.444 UID.  Data tersebut dapat dilihat pada Gambar 4. 

Gambar 4. Persebaran titik pengguna telepon selular 

Titik-titik  warna  pada  Gambar  4  menandakan  bahwa 
pengguna  mengalami  pergerakan  dari  titik  yang  satu  ke  titik 
yang  lainnya.  Terlihat  bahwa  pulau  Jawa  menjadi  lokasi 
dengan titik yang paling banyak. Jika  dieksplorasi pada level 
kecamatan, pergerakan dan UID yang terdeteksi dapat dilihat 
pada  Tabel  II.  Dari  data  tersebut  dapat  diketahui  bahwa 
meskipun  suatu  wilayah  tercatat  lebih  banyak  pengguna, 
namun  belum  dapat  dipastikan  bahwa  wilayah  tersebut  juga 
memiliki banyak titik yang menunjukkan pergerakan dari peng- 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

TABEL II 
EKSPLORASI PADA LEVEL KECAMATAN 

Kecamatan 
dengan Titik 
Terbanyak 

Bantul 

Jagonalan 
Godean 
Kasihan 
Imogiri 

Jumlah 
Titik 

196.186 

192.621 
185.391 
180.935 
180.155 

Kecamatan 
dengan Pengguna 
(UID) Terbanyak 

Jumlah 
Pengguna 
(UID) 

Banguntapan 

Kasihan 
Ngemplak 
Gamping 
Kalasan 

349 

344 
332 
305 
304 

guna.  Atau  dengan  kata  lain,  tidak  semua  pengguna  banyak 
melakukan  perpindahan.  Secara  garis  besar,  pergerakan 
manusia  dari  awal  hingga  akhir  periode  digambarkan  pada 
Gambar 5.  

Gambar 5. Garis besar titik pergerakan manusia periode Maret – Juni 2020 

Terlihat  pada  Gambar  5  bahwa  pergerakan  manusia  setiap 
harinya bersifat fluktuatif. Ada yang meningkat ada juga yang 
menurun, ada yang peningkatan atau penurunannya biasa-biasa 
saja  namun  ada  juga  yang  peningkatan  atau  penurunannya 
tajam.  Berdasarkan  Gambar  5,  tanggal  3  Mei  2020  tercatat 
sebagai  hari  yang  memiliki  paling  banyak  pergerakan,  yakni 
sebanyak 696.818 titik pergerakan dengan 3.405 UID. 

2.  PENDETEKSIAN DAERAH TEMPAT TINGGAL 

Dari data awal yaitu 4.444 UID, tidak semua dapat dideteksi 
daerah  tempat  tinggalnya  yaitu  hanya  sebanyak  96,5  persen 
atau 4.290 UID. Variabel daerah tempat tinggal yang terdeteksi 
dari  pengguna  paling  banyak  tersebar  adalah  di  Pulau  Jawa. 
Jika dilihat lebih dalam lagi, variabel daerah tempat tinggal ini 
tersebar  di  1.223  kecamatan.  Berikut  disajikan  5  (lima) 
kecamatan  yang  memiliki  variabel  daerah  tempat  tinggal 
terbanyak. 

TABEL III 
DAERAH TEMPAT TINGGAL TERBANYAK 
Kabupaten/Kota 

Provinsi 

Jumlah UID 

Kecamatan 

Depok 

Sleman 

Jakarta Barat 
Cengkareng 
Klaten 
Bayat 
Sleman 
Gamping 
Banguntapan  Bantul 

DI Yogyakarta 

DKI Jakarta 
Jawa Tengah 
DI Yogyakarta 
DI Yogyakarta 

41 

37 
34 
34 
31 

Dengan  mempertimbangkan  lokasi  stop  detection,  dapat 
diketahui  jumlah  pengguna  yang  melakukan  perpindahan 
maupun  yang  tidak.  Diasumsikan  bahwa  apabila  pengguna 
tercatat  berada  pada  lebih  dari  satu  kecamatan  dalam  kurun 
waktu  tertentu  maka  pengguna  tersebut  telah  melakukan 
perpindahan  (move). Apabila pengguna tercatat berada dalam 

 3 / 8 

 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
satu  kecamatan  dalam  kurun  waktu  tertentu  maka  pengguna 
tersebut tidak melakukan perpindahan (stay).  

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

daerah tempat tinggal semakin meningkat. Hal ini mungkin saja 
terjadi karena sudah banyak informasi yang mulai meresahkan 
masyarakat  terkait  tingginya  kasus  positif  COVID-19.  Selain 
itu, pada pekan ini mulai dikampanyekan gerakan #dirumahaja 
sehingga  masyarakat  mulai  mengurangi  aktivitas  di  luar 
kecamatan daerah tempat tinggal. 

Gambar 6. Pengguna yang move ke luar kecamatan daerah tempat tinggal 

Berdasarkan Gambar 6, jumlah pengguna yang move di luar 
kecamatan daerah tempat tinggal secara mingguan mengalami 
fluktuasi namun cenderung menurun. Pada minggu pertama (9 
– 15 Maret 2020), sebanyak 3.757 UID melakukan perpindahan. 
Hal  ini  mungkin  saja  terjadi  karena  kasus  positif  COVID-19 
belum  terlalu  tinggi  dan  belum  ada  gerakan  #dirumahaja 
sehingga masyarakat masih banyak yang melakukan aktivitas 
di luar kecamatan daerah tempat tinggal. 

Minggu  ke-5  hingga  minggu  ke-8  (6  April  –  3  Mei  2020) 
jumlah UID yang move semakin meningkat. Pada minggu ke-
5, kebijakan PSBB mulai diberlakukan untuk pertama kalinya. 
Pada  kebijakan 
ini  masyarakat  kembali  diperbolehkan 
melakukan aktivitas di luar rumah namun harus tetap mematuhi 
setiap  peraturan  seperti  menjaga  batasan  sosial  misalnya 
kapasitas dalam suatu kantor tidak boleh lebih dari 50 persen 
dari kapasitas sebenarnya.  

Kemudian  minggu  ke-9  hingga  mingu  ke-12  (4  –  31  Mei 
2020) jumlah UID yang move mengalami penurunan. Periode 
ini  adalah  Bulan  Ramadhan  dimana  masyarakat  melakukan 
ibadah puasa dan merayakan Hari Raya Idul Fitri. Pada periode 
ini juga PSBB mulai diterapkan di banyak wilayah. Hal inilah 
yang  dapat  memicu  pergerakan  manusia  menurun  karena 
pemerintah  tentunya  melakukan  penjagaan  dan  pengendalian 
agar masyarakat tidak banyak melakukan perpindahan. 

Minggu ke-13 hingga minggu ke-17 (1 Juni – 5 Juli 2020) 
jumlah  UID yang move kembali meningkat. Periode ini adalah 
periode dimana diberlakukannya kebijakan New Normal. New 
Normal  adalah  suatu  perubahan  untuk  tetap  menjalankan 
aktivitas  normal  namun  dengan  menerapkan  protokol 
kesehatan  guna  mencegah  terjadinya  penularan  COVID-19. 
Segala  aktivitas  baik  ekonomi,  ibadah,  maupun  sosial  mulai 
mempersiapkan  menuju  keadaan  new  normal agar 
tetap 
produktif. Dengan demikian pengguna yang move meningkat. 
Dilihat  secara  keseluruhan,  pengguna  yang  melakukan 
perpindahan di luar kecamatan daerah tempat tinggal semakin 
menurun.  Tentunya  hal  ini  menjadi  harapan  bagi  pemerintah 
agar  penularan  virus  corona  tidak  semakin  tinggi.  Kemudian 
selain pengguna yang move, di bawah ini disajikan pengguna 
yang stay di kecamatan daerah tempat tinggal. 

Dari Gambar 7, jumlah UID yang stay di kecamatan daerah 
tempat  tinggal  secara  mingguan  mengalami  fluktuasi  namun 
cenderung  meningkat.  Pada  minggu  pertama  hingga  minggu 
keempat (9 Maret – 5 April 2020) UID yang stay di kecamatan 

Gambar 7. Pengguna yang stay di kecamatan daerah tempat tinggal 

Minggu ke-5 hingga minggu ke-7 (6 – 26 April 2020) jumlah 
UID yang stay semakin menurun. Pada minggu ke-5, kebijakan 
PSBB  mulai  diberlakukan  untuk  pertama  kalinya  sehingga 
pengguna  yang  stay  semakin  menurun,  karena  sudah  banyak 
yang beraktivitas di luar kecamatan daerah tempat tinggal. 

Kemudian minggu ke-8 hingga minggu ke-11 (27 April – 24 
Mei  2020)  jumlah  UID  yang  stay  mengalami  peningkatan. 
Periode  ini  adalah  Bulan  Ramadhan.  Pada  periode  ini  juga 
PSBB mulai diterapkan di banyak wilayah dan aturan larangan 
mudik juga mulai diberlakukan. Hal inilah yang dapat memicu 
masyarakat untuk stay. 

Minggu ke-12 hingga minggu ke-17 (25 Mei – 5 Juli 2020) 
jumlah UID yang stay kembali meningkat meskipun ada juga 
ini  adalah  periode  dimana 
yang  menurun.  Periode 
diberlakukannya  kebijakan  New  Normal.  Meskipun  aktivitas 
manusia  diperbolehkan  untuk  kembali  normal,  namun 
pengguna  yang  stay  juga  meningkat.  Banyak  kemungkinan 
yang dapat dilihat pada kasus ini. Seperti misalnya pengguna 
yang kehilangan pekerjaan sehingga mereka stay pada periode 
ini. Bertambahnya proporsi pegawai yang bekerja dari rumah 
(work from home) pada periode ini juga memicu meningkatnya 
pengguna yang stay. Adanya pekan ujian bagi para siswa atau 
mahasiswa  juga  dapat  memicu  meningkatnya  pengguna  yang 
stay. 

Dilihat secara keseluruhan, pengguna yang tetap berada di 
kecamatan daerah tempat tinggal semakin meningkat. Apabila 
dilihat kasus di atas (move dan stay),  tren bergerak ke arah yang 
berlawanan. Tentunya hal ini menjadi harapan bagi pemerintah 
dalam  mengendalikan  pergerakan  manusia  dan  karantina 
wilayah. Selain itu, pada minggu ke-14, jumlah pengguna yang 
move  dan/atau  stay  memiliki  jumlah  yang  paling  sedikit 
dibandingkan  dengan  minggu  yang  lainnya.  Hal  ini  terjadi 
karena  pada  minggu  ini,  data  yang  didapatkan  hanya  pada 
tanggal 14 Juni.  

3.  KORELASI 

Data  yang  digunakan  untuk  mencari  nilai  korelasi  adalah 

data harian seluruh Indonesia. Berikut nilai korelasinya. 

 4 / 8 

 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

TABEL IV 
KORELASI 

COVID-19 

Pearson 

Mobilitas 

Pearson 

COVID-19  Mobilitas 

1 

-0,215 

-0,215 

1 

Pergerakan  manusia  dengan  pertambahan  kasus  positif 
COVID-19  di  Indonesia  memiliki  korelasi  yang  negatif  dan 
lemah, yaitu sebesar -0,215. Artinya, pertambahan kasus positif 
COVID-19  di 
Indonesia  diikuti  dengan  menurunnya 
pergerakan manusia setiap harinya.  

4.  PERGERAKAN MANUSIA SAAT PANDEMI COVID-19 

Secara  garis  besar,  pergerakan  manusia  selama  pandemi 
COVID-19  di  Indonesia  dapat  dilihat  pada  Gambar  8.  Dari 
gambar  tersebut,  pergerakan  yang  paling  banyak  terdeteksi 
berada pada provinsi DI Yogyakarta sebanyak 4.804.684 per- 

Gambar 8. Garis besar mobilitas manusia periode Maret – Juni 2020 

gerakan dengan 934 UID dan Jawa Tengah sebanyak 3.951.503 
dengan 1.496  UID.  Berikut  ini  disajikan  pergerakan manusia 
secara periodisasi. 

Periode 

Pergerakan Manusia 

Kasus COVID-19 

Periode 

Pergerakan Manusia 

Kasus COVID-19 

TABEL V 
PERGERAKAN MANUSIA DAN COVID-19 

1 

2 

3 

4 

5 

6 

 Berdasarkan  tabel  di  atas,  pergerakan  manusia  cenderung 
menurun di 4 (empat) periode. Periode pertama (9 – 15 Maret 
2020), keadaan masih seperti biasa, belum ada peraturan untuk 
karantina wilayah, kerja di rumah dan sebagainya. Mengingat 
akan  hal  tersebut,  pergerakan  manusia  masih  belum  dibatasi 
namun sudah menunjukkan adanya penurunan. Periode kedua 
(16 Maret – 4 April 2020) mulai dikenal gerakan #DiRumahAja, 
di  mana  hampir  seluruh  lapisan  masyarakat  beraktivitas  di 
dalam rumah. Periode keempat (1 – 30 Mei 2020) terdapat hari-
hari  penting  seperti  masyarakat  melaksanakan  ibadah  puasa, 

merayakan Idul Fitri 1441 H, memperingati Hari Kenaikan Isa 
Al  Masih,  Hari  Raya  Waisak,  dan  juga  ada  Hari  Buruh 
bisa 
Internasional.  Menurunnya 
disebabkan  karena  sebagian  masyarakat  Indonesia  sedang 
melaksanakan ibadah puasa dimana orang cenderung memilih 
untuk 
tidak  banyak  melakukan  perpindahan  yang  bisa 
menyebabkan lelah. Periode kelima (1 – 7 Juni 2020) kebijakan 
new normal diberlakukan untuk pertama kalinya.  

pergerakan  manusia 

Pergerakan manusia cenderung meningkat di 2 (dua) periode 
yaitu pada periode ketiga dan keenam. Periode ketiga (8 – 26 

 5 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

wilayah lainnya. Sebanyak 63.191 titik pergerakan dengan 342 
UID terdeteksi melakukan mobilitas di dalam wilayah tersebut. 
Pada  Gambar  10  (bawah)  sebanyak  1.118  titik  pergerakan 
dengan  132  UID  melakukan  pergerakan  dari  daerah  asal 
Kabupaten Sleman ke Kabupaten Klaten.  

Meskipun  Kabupaten  Sleman  dan  Kabupaten  Klaten 
berdekatan, pergerakan lintas batas kabupaten/kota inilah yang 
dapat  memicu  bertambahnya  kasus  COVID-19  di  Indonesia. 
Berikut  ini  akan  ditunjukkan  pergerakan  manusia  di  dalam 
maupun yang menuju Kabupaten Klaten pada level kecamatan.  

April  2020)  kebijakan  PSBB  mulai  diberlakukan.  Meskipun 
ada  pembatasan  sosial  seperti  pengurangan  kapasitas  suatu 
tempat, namun aktivitas di luar rumah yang nampaknya sudah 
diperbolehkan  membuat  pergerakan  manusia  cenderung 
meningkat. Periode keenam (24 – 29 Juni 2020) kebijakan new 
normal  memasuki  pekan  keempat  dimana  masyarakat  sudah 
mulai  terbiasa  dengan  gaya  hidup  baru.  Demi  menjaga 
stabilitas perekonomian, kini kantor-kantor mulai menerapkan 
sistem  shift.  Misalnya  hanya sekitar 25 persen dari karyawan 
yang  secara  fisik  hadir  di  kantor  sisanya  menjalankan  work 
from  home 
setiap  karyawan  minimal 
menghabiskan waktu satu minggu di kantor secara bergiliran. 
Hal inilah yang dapat memicu pergerakan manusia meningkat 
pada periode ini.  

(WHF),  dan 

Mesikpun  pergerakan  manusia  cenderung  menurun  di  4 
(empat)  periode,  kasus  positif  COVID-19  terus  mengalami 
peningkatan. Dalam hal ini, dapat diartikan bahwa pemerintah 
dinilai cukup baik dalam mengendalikan pergerakan manusia. 
Pada kesempatan ini,  dengan segala keterbatasan peneliti akan 
menyajikan pergerakan manusia pada periode pertama, kedua, 
dan keempat. 

Gambar 9. ODM periode pertama 

Pergerakan  manusia  pada  level  provinsi  paling  banyak 
terjadi di dalam Provinsi Jawa Tengah yaitu sebanyak 223.770 
titik  pergerakan  dengan  1.266  UID  dan  dari  Provinsi  DI 
Yogyakarta  menuju  Jawa  Tengah  yaitu  sebanyak  2.335  titik 
pergerakan dengan 309 UID. 

Gambar 11. Pergerakan manusia pada level kecamatan saat periode pertama 

Dari  Gambar  11 

terlihat  bahwa  banyak  garis-garis 
pergerakan  yang  menunjukkan  bahwa  pada  periode  ini, 
pergerakan  manusia  masih  banyak  yang  melewati  batas 
kecamatan.  Dari  kasus 
ini,  perlu  ada  upaya  untuk 
mengendalikan  pergerakan  manusia  dalam  berbagai  wilayah 
kecamatan. 

Gambar 10. Pergerakan manusia pada level kabupaten saat periode pertama 

Dari  Gambar  10  (atas)  pergerakan  yang  terjadi  di  dalam 
Kabupaten Klaten terpantau cukup tinggi dibandingkan dengan 

Gambar 12. ODM periode kedua 

Pada periode kedua, pergerakan manusia pada level provinsi 
dapat  dilihat  pada  Gambar  12.  Pergerakan  yang  terpantau 
cukup tinggi terjadi di dalam provinsi Jawa Tengah sebanyak 
383.737 titik pergerakan dengan 1.004 UID. Rata-rata satu UID 
melakukan perpindahan adalah sebanyak 383 pergerakan dari 
satu titik ke titik yang lainnya. Pergerakan di  dalam Provinsi 
Jawa  Tengah  ini  meningkat  sebesar  42  persen  dari  periode 
sebelumnya. Sedangkan pergerakan antar provinsi yang paling 
tinggi terjadi dari Provinsi DI Yogyakarta menuju Jawa Tengah 
sebanyak 4.258 titik pergerakan dengan 283 UID. Pergerakan 
antar  provinsi  ini  meningkat  sebesar  45  persen  dari  periode 

 6 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
sebelumnya.  Jika  dilihat  pada  level  kabupaten/kota,  daerah 
yang  terdeteksi  memiliki  pergerakan  manusia  paling  banyak, 
terjadi di daerah yang sama seperti periode pertama.  

Pergerakan manusia pada periode keempat dapat dilihat pada 
Gambar  15.  Pergerakan  manusia  paling  banyak  terdeteksi  di 
dalam provinsi DI Yogyakarta yaitu 530.450 titik pergerakan 
dengan 601 UID. Rata-rata satu pengguna melakukan pergera- 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 13. Pergerakan manusia pada level kabupaten saat periode kedua 

Berdasarkan Gambar 13, pergerakan terbanyak terdeteksi di 
dalam Kabupaten Klaten sebanyak 105.996 pergerakan dengan 
334 UID dan dari Kabupaten Sleman menuju Kabupaten Klaten 
yaitu  2.047  titik  pergerakan  dengan  114  UID.  Kabupaten 
Klaten kembali menjadi kabupaten dengan pergerakan tertinggi.  

Gambar 15. ODM periode keempat 

kan  adalah  883  pergerakan.  Selain  itu,  pergerakan  manusia 
antar  provinsi  paling  banyak  terdeteksi  dari  provinsi  DI 
Yogyakarta menuju Jawa Tengah yaitu 1.475 titik pergerakan 
dengan 177 UID.  

Gambar 16. Jumlah kasus positif COVID-19 di dua provinsi pada periode ke-4 

Meskipun  pergerakan  manusia  lebih  rendah  dibandingkan 
dengan periode ketiga, namun Gambar 16 menunjukkan bahwa 
kasus positif  COVID-19 yang ada di provinsi DI Yogyakarta 
dan Jawa Tengah semakin tinggi.  

Gambar 14. Pergerakan manusia pada level kecamatan saat periode kedua 

Gambar 14 menggambarkan kondisi pergerakan manusia di 
dalam Kabupaten Klaten (atas) dan pergerakan mannusia dari 
Kabupaten  Sleman  menuju  Kabupaten  Klaten  (bawah)  pada 
level kecamatan.  Dibandingkan dengan periode sebelumnya, 
pergerakan  manusia  di  dalam  Kabupaten  Klaten  meningkat 
sebesar  40  persen,  sedangkan  pergerakan  manusia  dari 
Kabupaten  Sleman  menuju  Kabupaten  Klaten  meningkat 
sebesar 45 persen. Dari kasus ini, diperlukan upaya yang lebih 
tegas untuk mengendalikan pergerakan manusia. 

Gambar 17. Pergerakan manusia pada level kabupaten/kota saat periode ke-4 

Pergerakan  di  dalam  Kabupaten  Bantul  terpantau  paling 
banyak  di  antara  kabupaten/kota  lainnya  yaitu  sebanyak 
215.798  titik  pergerakan  dengan  274  UID.  Kemudian  jika 
dilihat  pergerakan  antar  kabupaten/kota,  pergerakan  dari 

 7 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Kabupaten Sleman menuju Kabupaten Klaten terpantau cukup 
tinggi  yaitu  sebanyak  869  titik  pergerakan  dengan  81  UID. 
Berikut ini disajikan pergerakan manusia pada level kecamatan. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

VII. 

PENUTUP 

Algoritma yang telah dibangun dengan memanfaatkan MPD 
mampu  mendeteksi  daerah  tempat  tinggal.  Pengguna  telepon 
selular  dominan  bertempat  tinggal  di  Kecamatan  Depok 
(Sleman).  Dilihat  dari  kecamatan  daerah  tempat  tinggal 
pengguna, banyaknya pengguna yang move ke luar kecamatan 
tinggal  cenderung  menurun.  Sebaliknya, 
daerah 
banyaknya  pengguna  yang  stay  di  kecamatan  daerah  tempat 
tinggal cenderung meningkat.  

tempat 

Algoritma yang telah dibangun dengan memanfaatkan MPD 
juga  mampu  mendeteksi  pergerakan  manusia  dalam  wilayah 
tertentu.  Dengan  data  sampel  yang  ada,  didapatkan  bahwa 
pergerakan  di  dalam  Provinsi  DIY,  pergerakan  dari  Provinsi 
Jawa  Tengah  menuju  DIY  dan  sebaliknya  juga  cukup  tinggi. 
Kemudian  pada  level  kabupaten/kota,  pergerakan  di  dalam 
Kabupaten Sleman, Klaten, maupun antar Kabupaten Sleman 
&  Klaten  terpantau  cukup  tinggi.  Hal  ini  dapat  disebabkan 
karena kondisi geografis antara kedua provinsi dan kabupaten 
tersebut  yang  saling  berdekatan.  Pergerakan  pada  level  yang 
paling kecil (kecamatan)  juga  dapat berpotensi menjadi  salah 
satu pemicu penularan COVID-19. 

Gambar 18. Pergerakan manusia pada level kecamatan saat periode ke-4 

DAFTAR PUSTAKA 

Gambar  18 menggambarkan pergerakan  manusia  di  dalam 
Kabupaten  Bantul  (atas)  dan  dari  Kabupaten  Sleman  menuju 
Kabupaten  Klaten  (bawah)  pada  level  kecamatan.  Terlihat 
bahwa  pergerakan-pergerakan  pada  level  kecamatan  masih 
cukup  banyak.  Hal  inilah  yang  mungkin  dapat  memicu 
bertambahnya  kasus  positif  COVID-19  pada  periode  ini. 
Berikut disajikan kasus COVID-19 pada Kabupaten Bantul dan 
Sleman. 

[1]  A. M. Huljanah, N. Rahmawati, N. Hidayah, A. Prio, and A. Santoso, 

“Perilaku Masyarakat Dalam Penerapan Ketahanan Nasional Di Era 
Covid 19 Sebagai Bentuk Bela Negara,” Semin. Nas. Call Pap. 
Hubisintek 2020, pp. 37–43, 2020. 

[2]  A. Aryasena, R. V. H. Ginardi, and F. Baskoro, “Perancangan Indoor 

Localization Menggunakan Bluetooth Untuk Pelacakan Posisi Benda di 
Dalam Ruangan,” J. Tek. ITS, vol. 5, no. 2, pp. 326–330, 2016, doi: 
10.12962/j23373539.v5i2.17043. 

[3]  B. P. Statistik, “Panduan Penggunaan Mobile Positioning Data ( MPD ) 

untuk Statistik Mobilitas.” 

[4]  C. Report and E. C. No, Feasibility Study on the Use of Mobile Positioning 
Data for Tourism Statistics Consolidated Report Eurostat Contract No, no. 
30501. 2014. 

[5]  Call  Detail  Record  (CDR)  ANALYSIS :  REPUBLIC  OF  GUINEA  Final 

Report. 

[6]  D. Tuwu, “Kebijakan Pemerintah Dalam Penanganan Pandemi Covid-19,” 
J. Publicuho, vol. 3, no. 2, p. 267, 2020, doi: 10.35817/jpu.v3i2.12535.  
[7]  Hestiana  Dharmastuti  –  detikNews.  (2020,  April).  Fakta-fakta  di  Balik 
: 

Wajah 
https://news.detik.com/berita/d-4982313/fakta-fakta-di-balik-wajah-
jakarta-selama-sepekan-psbb 

Jakarta  Selama  Sepekan  PSBB. 

[Online].  Available 

Gambar 29. Jumlah positif COVID-19 di provinsi DIY pada periode ke-4 

Berdasarkan data dari BNPB, pada akhir periode ini terdapat 
92  kasus  positif  COVID-19  yang  ada  di  Kabupaten  Sleman 
sedangkan  Kabupaten  Bantul  dengan  angka  56.  Sedangkan 
Kabupaten Klaten memiliki kasus positif COVID-19 sebanyak 
21  kasus  atau  sebesar  1,42  persen  dari  total  kasus  positif 
COVID-19  di  Jawa  Tengah  pada  akhir  periode  ini.  Pada 
periode  ini,  meskipun  pemerintah  telah  melakukan  berbagai 
upaya  untuk  mencegah  penularan  virus  corona,  tampaknya 
tidak  membuat  angka  COVID-19  menurun.  Tetapi  dari  sisi 
pergerakan  manusia,  pemerintah  dinilai  cukup  baik  dalam 
mengendalikan pergerakan manusia pada periode ini. Dengan 
demikian  jika  dilihat  dari  kasus  tersebut,  maka  diperlukan 
tindak  lanjut  dalam  mengendalikan  pergerakan  manusia, 
khususnya di wilayah-wilayah rawan penularan COVID-19. 

[8]  K.  Sultan,  H.  Ali,  and  Z.  Zhang, “Call  Detail Records  Driven  Anomaly 
Detection  and  Traffic  Prediction  in  Mobile  Cellular  Networks,”  IEEE 
Access,vol.6,pp.41728–41737,2018,doi: 10.1109/ACCESS.2018.2859756. 
[9]  M.  Vanhoof,  F.  Reis,  Z.  Smoreda,  and  T.  Ploetz,  “Detecting  home 
locations from CDR data: introducing spatial uncertainty to the state-of-
the-art,” 2018, [Online]. Available: http://arxiv.org/abs/1808.06398. 
[10] P.  L.  Permadi  and  I.  M.  Sudirga,  “Problematika  Penerapan  Sistem 
Karantina  Wilayah  Dan  PSBB  Dalam  Penanggulangan  Covid-19,”  J. 
Kertha Semaya, vol. 8, no. 9, pp. 1355–1365, 2020. 

[11] PeduliLindungi.  (2020).  Apa  itu  PeduliLindungi?.  [Online].  Available  : 

https://pedulilindungi.id/ 

[12] R. Ahas, A. Aasa, A. Roose, Ü. Mark, and S. Silm, “Evaluating passive 
mobile  positioning  data  for  tourism  surveys:  An  Estonian  case  study,” 
Tour.  Manag.,  vol.  29,  no.  3,  pp.  469–486,  2008,  doi: 
10.1016/j.tourman.2007.05.014. 

[13] R.  Ahas,  S.  Silm,  O.  Järv,  E.  Saluveer,  and  M.  Tiru,  “Using  mobile 
positioning data to model locations meaningful to users of mobile phones,” 
J.  Urban  Technol.,  vol.  17,  no.  1,  pp.  3–27,  2010,  doi: 
10.1080/10630731003597306. 

[14] S.  Pramana,  Yuniarti,  D.  Y.  Paranatha,  and  S.  B.  Panuntun,  ""Mobility 
Pattern Changes in Indonesia in Response to COVID-19"", Economics and 
Finance in Indonesia, vol 67, no. 1, June 2021. 

 8 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
"
221709958,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pengkajian Optimasi Algoritma Pencarian  
dari Multi-Version Data Warehouse  
menggunakan Database Non-relasional NoSQL 

Ramadhan Azizulhakim Yusuf (221709958, 4SI1) 
Dosen Pembimbing: Lutfi Rahmatuti Maghfiroh 

tulisan 

sendiri 

Ringkasan—  Badan  Pusat  Statistik  yang  menghasilkan  data 
berskala  besar  membutuhkan  penyimpanan  yang  efektif  dan 
optimal.  Penelitian  terkait  Multi-Version  Data  Warehouse 
(MVDW)  yang  memanfaatkan  penggunaan  NoSQL  berbasis 
sudah  berusaha  dikembangkan  untuk 
dokumen 
kepentingan  penyimpanan  data  BPS,  dan  tulisan  ini  dibuat 
untuk  mengkaji  metode  optimasi  algoritma  dengan  tujuan 
mengurangi  waktu  yang  digunakan  dalam  proses  penyimpanan 
dan  pencarian  data  ketika  dibutuhkan.  Algoritma  yang 
ini  menitikberatkan  pada  proses 
diusulkan  pada 
penyimpanan  data,  yaitu  dengan  menyarankan  suatu  model 
penyimpanan yang menyamaratakan pengkodean variabel pada 
data  warehouse  yang  digunakan  agar  kemudian  pencarian  data 
dapat  dijalankan  dengan  lebih  mudah  dan  optimal.  Adapun 
metode  optimasi  lain  juga  dilakukan  dengan  menerapkan 
metode-metode  optimasi  query  untuk  mendukung  dan 
meningkatkan keoptimalan algoritma yang diusulkan. Hasil dari 
kedua  metode  optimasi  yang  dilakukan  dapat  dikatakan 
berhasil,  dikarenakan  waktu  yang  digunakan  pada  proses 
pencarian  data  dengan  memanfaatkan  algoritma  setelah 
penerapan metode optimasi sudah berkurang jika dibandingkan 
dengan  proses  pencarian  data  dengan  memanfaatkan  algoritma 
yang telah dikembangkan oleh penelitian terdahulu. 

Kata  Kunci—  database  nonrelasional,  NoSQL,  algoritma, 

pencarian data, optimasi query. 

I.  LATAR BELAKANG 

Badan Pusat Statistik (BPS) melakukan kegiatan survei dan 
sensus  secara  rutin  dengan  tujuan  menghasilkan  data  dan 
informasi  yang  nantinya  akan  dipublikasikan,  baik  dalam 
bentuk  raw  data  maupun  buku  cetak.  Sebelum  data  dan 
informasi tersebut diolah, hasil data serta metadata dari setiap 
survei  maupun  sensus  dari  setiap  periode  disimpan  terlebih 
dahulu dalam suatu  data warehouse  (DW). DW adalah suatu 
repositori  data  berorientasi  subjek  yang  terintegrasi,  bersifat 
non-volatil,  serta  dapat  mendukung  pengkoleksian  data  time-
variant yang digunakan dengan tujuan agar penyimpanan data 
lebih terstruktur [1]. 
Eric  Brewer 

teori  yang 
menyatakan  bahwa  indikator  penyimpanan  data  terdistribusi 
agar 
dengan 
memperhatikan  konsistensi  (consistency),  ketersediaan  data 
(availability), dan toleransi terhadap partisi jaringan (partition 
tolerance). Ini kemudian lebih dikenal dengan istilah teorema 
CAP, yang menjelaskan prioritas yang dapat diutamakan dari 
suatu  sistem  penyimpanan  data.  Beberapa  tahun  berikutnya, 
Brewer  melakukan  kajian  lanjutan  dari  teorema  ini,  yang 
kemudian  menjadi  acuan  dari  pembuatan  sistem  database 
nonrelasional  NoSQL  (Not  Only  SQL)  dalam  melakukan 
penyimpanan data [3]. 

[2]  mengembangkan  suatu 

terstruktur 

dikatakan 

adalah 

dapat 

tahun-tahun  belakangan 

NoSQL sendiri telah dipakai dalam berbagai implementasi 
penyimpanan  data  di 
Ini 
dikarenakan  dalam  penggunaannya  terhadap  data  berskala 
besar,  implementasi  sistem  database  nonrelasional  NoSQL 
dinilai  memiliki lebih banyak keuntungan seperti fleksibilitas 
serta  kemampuannya  dalam  melakukan  penyimpanan  dan 
mengambil data dengan volume besar dibandingkan database 
relasional [4].  

ini. 

Menurut  Ade  Koswara,  salah  satu  pengembang  Portal 
Indonesia  Data  Hub 
(INDAH),  pemanfaatan  database 
nonrelasional di BPS adalah penggunaan Hadoop Distributed 
File  Service  (HDFS)  [5]  sebagai  data  lake,  sedangkan 
penggunaan  DW  dalam  penyimpanan  data  BPS  masih 
menggunakan  Microsoft  SQL  Server  [6]  dan  IBM  DB2  with 
BLU  Acceleration  [7]  yang  berbasis  database  relasional. 
Seiring  berjalannya  waktu,  penggunaan  database  relasional 
dalam  kegiatan  yang  tengah  dijalankan  BPS  dengan  tujuan 
membangun  Portal  Satu  Data  Indonesia  akan  menghadapi 
tantangan  terkait  penanganan  volume  data  yang  akan  terus 
ini  sejalan  dengan  sifat  dari  database 
bertambah.  Hal 
relasional  yang  memiliki  performa 
lebih  baik  ketika 
menangani data yang jumlahnya sedikit dan terbatas [8]. 

dan 

pengelolaan 

kemampuan 

Adapun  sebagai  usaha  mitigasi 

tantangan  di  atas, 
penggunaan database NoSQL sebagai alternatif penyimpanan 
data  BPS  sudah  mulai  dikembangkan.  Selain  karena 
datanya, 
fleksibilitas 
pengembangan  ini  juga  dilakukan  dikarenakan  penyimpanan 
data  menggunakan  database NoSQL sendiri tidak didasarkan 
pada  bentuk  skema  penyimpanannya,  namun  dikategorikan 
berdasarkan  struktur  datanya  [9].  Maka  dari  itu,  database 
NoSQL  yang  akan  dijadikan  sorotan  dalam  penulisan  ini 
adalah  database  nonrelasional  dengan  struktur  data  berbasis 
dokumen, mengingat sumber data yang lebih sering dihasilkan 
dari survei dan sensus BPS berupa JSON atau XML [10].  

Melatarbelakangi  penelitian  ini,  Maghfiroh  dan  Baskara 
[11] telah mengkaji mengenai penggunaan Multi Version Data 
Warehouse 
(MVDW)  menggunakan  NoSQL  berbasis 
dokumen  sebagai  alternatif  dalam  penyimpanan  data  hasil 
survei BPS. MVDW yang dikembangkan pada kajian tersebut 
menggunakan  aplikasi  MongoDB,  yang  dinilai  memiliki 
keunggulan  dalam  operasi  read,  write,  dan  delete  jika 
dibandingkan  dengan  database  nonrelasional  berbasis 
dokumen lainnya. [12]  

ini 

Pengkajian 

dilanjutkan 

dengan 
kemudian 
pengembangan  model  MVDW  dan  algoritma  yang  lebih 
mutakhir oleh Maghfiroh dan Santoso [13] agar menjadi lebih 
dinamis,  sehingga  mampu  memberi  kemudahan  dalam 
serta 
mengatasi 
pengambilan  data  ketika  dibutuhkan  dalam  proses  analisis. 

tantangan  pada  proses  penyimpanan 

 1 / 8 

 
 
 
Namun mengingat waktu yang dibutuhkan untuk menjalankan 
query  masih  sedikit  lebih  tinggi,  peneliti  pada  [13]  sendiri 
lebih 
masih  menyarankan  perancangan  algoritma  yang 
sederhana  sehingga  dapat  menjalankan  query  dengan  lebih 
cepat. 

optimasi 

Penelitian 

terdahulu  mengenai 

database 
nonrelasional  NoSQL  lebih  minim  dibandingkan  bentuk 
database  lainnya,  dan  sebagian  besar  melakukan  pembagian 
beban  kerja  atau  berfokus  pada  performa  database  seperti 
implementasi  replikasi  server  oleh  Tinetti  dkk.  [14]  dan  Gu 
dkk.  [15].  Di  sisi  lain,  masih  sedikit  yang  menargetkan  pada 
efisiensi database dari aspek sistemnya, salah satunya adalah 
implementasi  optimasi  query  yang  dilakukan  Mahajan  dkk. 
[16]. 

Referensi [16] sendiri menyebutkan beberapa metode yang 
dapat dilakukan untuk mendapatkan query yang lebih optimal, 
seperti  indeksasi,  pengurutan,  covering,  aggregation,  dan 
sharding.  Metode-metode  optimasi  query  yang  dilakukan 
pada  [16]  juga  mendapatkan  hasil  yang  cukup  besar,  bahkan 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

hingga  ratusan  kali 
lipat 
dibandingkan dengan sebelum dilakukannya optimasi. 

lebih  efektif  dan  cepat 

jika 

Berdasarkan  paparan  singkat  penelitian  terdahulu  di  atas, 
maka  penelitian  ini  akan  dilakukan  mencoba  mengkaji  dan 
menerapkan  metode  optimasi  terhadap  proses  pencarian 
dengan membagi fokus penelitian menjadi dua, yaitu dari segi 
algoritma  pengambilan  datanya  dan  dari  segi  optimasi  pada 
query  yang  akan  digunakan  dalam  proses  pengambilan 
datanya. 

II.  TUJUAN PENELITIAN 

Berdasarkan  masalah  yang  telah  dituturkan  di  bagian  I, 
penelitian  ini  ditujukan  untuk  melakukan  pengkajian  dalam 
bentuk  optimasi  algoritma  agar  mengurangi  waktu  yang 
dibutuhkan,  baik  dalam  proses  penyimpanan  maupun 
pengambilan data. 

III. PENELITIAN TERKAIT 

Penelitian terkait yang telah dimasukkan ke dalam bagian I 

akan disebutkan kembali pada tabel I berikut. 

No 

Judul 

1  Building the Data Warehouse 

Penulis, Publikasi 
W. H. Inmon 

Komentar 
Pengertian data warehouse 

TABEL I 
TABEL LITERATUR PENELITIAN TERKAIT 

2  CAP twelve years later: How the „rules‟ have changed 

E. Brewer 

Penjelasan Teorema CAP 

3  Trend Penggunaan NoSQL Untuk Basis Data Non-Relasional 

M. O. Fitri 

Kajian penggunaan NoSQL dan kelebihannya 

4  A workload-driven logical design approach for NoSQL document 

databases 

5  A Comparative Study of Relational and Non-Relational Database 

Models in a Web- Based Application 

6  Relational and Nonrelational Database Study on BPS-Statistics 

Indonesia Data in Facing 

7  Distributed Search on Large NoSQL Databases 

C. De Lima and  
R. D. S. Mello 
C. Gyorödi, R. Gyorödi, 
and R. Sotoc 
M. A. Rahman and L. R. 
Maghfiroh 
F. G. Tinetti, et al. 

Penjelasan system arsitektur NoSQL 

Penjelasan terkait sifat dan kekurangan 
database relasional 
Penjelasan fokus penulisan 

Penelitian terkait optimasi database 

8  Analysis of data replication mechanism in NoSQL database 

Y. Gu, et al. 

Penelitian terkait optimasi database 

MongoDB 

9  Optimization and application in medical big document-data of 

10 

Apriori algorithm based on MapReduce 
Improving the energy efficiency of relational and NoSQL 
databases via query optimizations 

11  A performance comparison of SQL and NoSQL databases A 
performance comparison of SQL and NoSQL databases 
12  Survey data and metadata modelling using document-oriented 

NoSQL 

13  NoSQL Model Data Warehouse Metadata Survei Dinamis, 

Studi Kasus : Survei Rumah Tangga 

L. Wei, G. Liu, Y. Shao, J. 
Liu, and Y. Zuo, 
D. Mahajan, et al. 

Penelitian terkait MapReduce pada database 
NoSQL 
Penelititan terkait optimasi query 

Y. Li and S. Manoharan 

L. R. Maghfiroh and I. G. 
B. B. Nugraha 
L. R. Maghfiroh and I. 
Santoso 

Penelitian terkait kinerja database 
nonrelasional 
Penelitian terdahulu dan penjelasan objek 
penelitian 
Penelitian terdahulu dan penjelasan objek 
penelitian 

IV. METODE PENELITIAN  
Tahapan  awal  dari  penelitian  ini  adalah  dengan  mengkaji 
kembali  penelitian  terdahulu  yang  telah  dilakukan  dalam 
rangka mengidentifikasi dan mengulas kembali permasalahan 
yang  belum  dapat  diselesaikan.  Tahapan  yang  kemudian 
dilakukan  adalah  studi  literatur  dengan 
tujuan  mencari 
referensi terdahul dan menganalisis model yang sudah pernah 
diterapkan  sebelumnya,  sekaligus  mendefinisikan  tujuan  dari 
penelitian.  

dari 

dimulai 

tahapan, 

observasi 

Setelah  studi  literatur  dilakukan,  inti  penelitian  dilakukan 
dengan  melakukan  experimental  design,  yang  terdiri  dari 
beberapa 
empiris, 
pembangunan teori, percobaan empiris, dan uji coba dari teori 
yang  sudah  dibangun,  yang  mana  akan  dilakukan  secara 
sirkular dan berulang jika dibutuhkan [17]. Hasil yang didapat 
dari experimental design  ini kemudian dijadikan acuan dalam 
mengajukan  solusi,  yang  pada  hal  ini  berupa  pengajuan 
algoritma  yang 
lebih  optimal  disertai  metode  optimasi 
pendukung lainnya dalam rangka pencapaian tujuan.  

 2 / 8 

 
 
 
 
Penelitian  ini  dilakukan  dengan  menggunakan  data  Survei 
Angkatan  Kerja  Nasional  (SAKERNAS)  Tahun  2015  dan 
Tahun  2018 dengan  wilayah  terpilih  provinsi  Lampung,  DKI 
Jakarta, DI Yogyakarta, Bali, dan Sulawesi Selatan. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 1. Alur penelitian 

V.  KERANGKA PIKIR 

Tahapan  awal  yang  merupakan  kajian  terhadap  masalah 
yang didapat pada [11], [13] menghasilkan inti permasalahan 
yang  akan  kemudian  dibahas  dalam  tulisan  ini,  yaitu  proses 
optimasi lebih lanjut terhadap algoritma yang telah ada. Pada 
tulisan  ini,  penulis  akan  menitikberatkan  pada  kinerja  dari 
algoritma pencarian data, mengingat kecenderungan dari data 
hasil  survei  BPS  yang  tidak  akan  diperbaharui  ataupun 
dihapus setelah dilakukan penyimpanan.  

Kinerja suatu algoritma pencarian data, selain dapat diukur 
dengan  melihat    penggunaan  query  sebagai  metode  inti,  juga 
tidak  terlepas  dari  struktur  penyimpanan  datanya,  sekaligus 
metadata  dari  data  itu  sendiri.  Maka  dari  itu,  tahapan 
selanjutnya  yang  dilakukan  penulis  setelah  melakukan  kajian 
metode  adalah  merancang  dan  mengusulkan  suatu  algoritma 
penyimpanan  data  yang  dirasa  lebih  optimal  sebagai  bentuk 
implementasi dari solusi yang sudah dibuat. 

Data  yang  telah  disimpan  sewaktu-waktu  kemudian  akan 
diambil menggunakan suatu tahap pencarian untuk memenuhi 
kebutuhan BPS. Pada hal ini, penulis juga akan mengusulkan 
dengan  query  yang  optimal  dalam  pencarian  data  yang 
dibutuhkan.  Keoptimalan  ini  kemudian  akan  diukur  dengan 
membandingkan  keadaan  sebelum  dan  sesudah  upaya 
optimasi dilakukan. Selanjutnya, metode optimasi yang sudah 
disebutkan  di  atas  akan  diimplementasikan  ke  dalam  proses 
pemenuhan kebutuhan data BPS. Secara garis besar, kerangka 
pikir penelitian dapat dilihat visualisasinya pada Gambar 2. 

Gambar 2. Kerangka pikir penelitian 

VI. HASIL DAN PEMBAHASAN 

Berdasarkan  kajian  awal  yang  telah  penulis  lakukan, 
didukung  hasil  penelitian  [11],  [13],  penyimpanan  data  hasil 
survei  BPS  memiliki  tantangan  tersendiri  mengingat  struktur 
data yang didapat dari tiap-tiap survei akan berubah mengikuti 
kebutuhan  dari  tujuan  survei  tersebut.  Hal  ini  menyebabkan 
penyimpanan  data  yang  dimaksud  akan  memiliki  kebutuhan 
penyimpanan  yang  berbeda  pula,  menyebabkan  perbedaan 
struktur yang tidak dapat dihindari pada setiap survei. 

Salah satu bagian dari struktur data hasil survei yang akan 
mengalami  perbedaan  dari  waktu  ke  waktu  adalah 
pengkodean  variabel  dari  tiap-tiap  data  tersebut.  Perubahan 
pengkodean  variabel  sendiri  dapat  disebabkan  oleh  berbagai 
hal  seperti  penambahan,  penghilangan,  atau  spesialisasi  dan 
generalisasi 
sebelumnya, 
menyebabkan  variabel  dengan  pengkodean  tertentu  akan 
tergeser ke pengkodean yang lain. Sebagai contoh, pertanyaan 
perihal  umur  responden  pada  SAKERNAS  Tahun  2015 
disimpan  ke  dalam  variabel  B4_K5,  sedangkan  pada  Tahun 
2018  disimpan  ke  dalam  variabel  B4_K8  akibat  adanya 
pertanyaan yang baru pada tahun tersebut.  

variabel 

survei 

pada 

dari 

Sebagai 

latar  belakang  dari  penelitian 

ini,  algoritma 
pencarian data sendiri  sudah  dikembangkan  sebelumnya oleh 
[11]  yang  dapat  dilihat  pada  Gambar  3.  Algoritma  ini 
kemudian  akan  disebut  sebagai  algoritma  pencarian  dasar. 
Pada  algoritma  tersebut,  proses  pencarian  data  dapat  penulis 
kelompokkan menjadi dua tahap besar, yaitu tahap penyiapan 
variabel  (a)  dan  tahap  pencarian  data  itu  sendiri  (b).  Pada 
tahap  penyiapan  variabel,  dilakukan  proses  pencocokan 
variabel  yang  akan dipakai pada  query inputan dengan daftar 
kolom  pengkodean  di  metadata  variabel  dari  setiap  versi 
mengingat  perbedaan  pengkodean  yang  dapat  terjadi  pada 
masing-masing  versi.  Daftar  kolom  yang  didapat  kemudian 
disimpan untuk kemudian digunakan dalam penyusunan ulang 

 3 / 8 

 
 
 
 
 
 
query  agar  dapat  diterapkan  pada  semua  versi  yang 
bersesuaian. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Adapun  penelitian 

jadikan  sarana  untuk 
ini  penulis 
mengusulkan  metode  untuk  menghilangkan  tahap  penyiapan 
variabel  (a)  pada  algoritma  yang  sudah  dikembangkan  pada 
[11].  Namun,  metode  yang  diusulkan  penulis 
lebih 
menekankan pada perubahan struktur penyimpanan data hasil 
survei  itu  sendiri.  Algoritma  penyimpanan  data  hasil  survei 
BPS  yang  saat  ini  digunakan  untuk  menyimpan  data  masih 
sederhana.  Pertama,  raw  data  hasil  survei  BPS  akan 
dikumpulkan  dalam  suatu  data  lake,  yang  pada  hal  ini 
memanfaatkan  HDFS.  Pada  penggunaannya,  data  lake  ini 
digunakan  BPS  sebagai  suatu  tempat  penyimpanan  data 
sementara,  mengingat  data  yang  disimpan  masih  berbentuk 
data  mentah,  dan  belum  dapat  langsung  digunakan  untuk 
kebutuhan  analisis  data.  Maka  dari  itu,  data  akan  dieksplor 
dan  ditransformasi  terlebih  dahulu.  Setelah  melewati  tahap 
transformasi  dan  eksplorasi,  data-data  tersebut  disimpan 
dalam  suatu  data  warehouse.  Pada  penyimpanan  ini,  data 
yang  disimpan  sudah  terstruktur,  walaupun  pada  hal  ini 
struktur  penyimpanan  yang  digunakan  masih  berbentuk 
relasional,  mengingat  BPS  masih  menggunakan  Microsoft 
SQL  Server  dan  DB2  BLU  sebagai  data  warehouse  untuk 
penyimpanan datanya. Adapun algoritma yang kemudian akan 
disebut sebagai algoritma penyimpanan dasar ini dapat dilihat 
visualisasinya pada Gambar 4. 

Gambar 3. Algoritma dasar pada [11] 

R.  Munir  [18]  menekankan  bahwa  algoritma  yang  bagus 
adalah  algoritma  yang  mangkus,  atau  dengan  kata  lain 
meminimumkan  kebutuhan  waktu  dan 
ruang.  Beliau 
menambahkan  bahwa  kebutuhan  waktu  dan  ruang  suatu 
algoritma bergantung pada ukuran masukan, yang secara khas 
adalah  jumlah  data  yang  diproses.  Jika  dihubungkan  dengan 
algoritma  pada  Gambar  3,  ukuran  masukan  dalam  bentuk 
langkah-langkah  yang  dilakukan  pada 
tahap  penyiapan 
variabel (a) sendiri akan bertambah, mengingat adanya query 
yang  perlu  disusun  ulang  untuk  setiap  versi  dengan  kolom 
baru yang bersesuaian. 

Sedari penjabaran di atas, penulis menilai bahwa algoritma 
tersebut  dapat  dijadikan  lebih  efektif  jika  tahap  penyiapan 
variabel  (a)  dapat  dihilangkan  agar  langkah-langkah  yang 
harus  dilakukan  lebih  sedikit,  menambah  kemangkusan  dari 
algoritma  tersebut.  Pada  penerapannya  sendiri,  penghilangan 
tahap ini akan menyebabkan query yang diinput di awal dapat 
digunakan untuk setiap query dari versi yang dibutuhkan dan 
nantinya  proses  pengulangan  penyusunan  query  dapat 
dihindari. 

Gambar 4. Algoritma penyimpanan dasar berdasarkan [19] 

Adapun  perubahan  struktur  penyimpanan  yang  diusulkan 
penulis  dilakukan  sebelum  data  disimpan  ke  dalam  data 
warehouse  masing-masing  dan  diterapkan  ke  semua  versi 
struktur penyimpanan hasil survei yang sudah ada. Perubahan 
struktur  dilakukan  dengan  mengklasifikasikan 
tiap-tiap 
variabel berisi jawaban dari pertanyaan yang sama pada setiap 
versi data ke dalam variabel dengan pengkodean yang disama 
ratakan pada tiap data collection. Variabel dengen pengkodean 
yang  sama  ini  kemudian  akan  disebut  sebagai  variabel 
klasifikasi.  Sebagai  contoh,  pertanyaan 
terkait  umur 
responden  akan  dikodekan  sebagai  variabel  UMUR  pada 
setiap  versi.  Dikarenakan  proses  klasifikasi  ini  dilakukan 
sebelum data disimpan, maka proses ini hanya perlu dilakukan 
sekali  pada  seluruh  variabel  survei  yang  berisi  jawaban  dari 
pertanyaan  yang  sudah  pernah  ditanyakan.  Jika  pada  masa 
yang  akan  datang  terdapat  variabel  baru  berisi  jawaban  dari 
pertanyaan  yang  belum  pernah  ditanyakan  pada  versi  survei 

 4 / 8 

 
 
 
 
yang  mendahului,  maka  variabel  klasifikasi  dengan  nama 
yang  disama  ratakan  dapat  langsung  ditambahkan  pada  data 
dengan  struktur  yang  memuat  variabel  tersebut  saja,  tanpa 
mengubah struktur data versi survei sebelumnya. 

Penerapan  klasifikasi  pengkodean  variabel  pada  struktur 
penyimpanan  ini  nantinya  akan  berpengaruh  pada  proses 
pencarian data, dimana variabel yang akan dipakai pada query 
yang  diinput  akan  dapat  langsung  dipakai  semua  versi  data 
yang  ada.  Adapun  algoritma  penyimpanan  data  setelah 
diterapkannya  proses  klasifikasi  variabel  dapat  dilihat  pada 
Gambar  5  dan  menghasilkan  snowballing  effect  pada 
algoritma  pencarian  data  yang  lebih  singkat  dibandingkan 
algoritma dasar pada Gambar 6. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

jembatan  dari 

dalam satu dokumen yang berisikan nama variabel pemetaan, 
keterangan  variabel  pemetaan,  dan  pemetaannya  pada 
dokumen  data  yang  sesungguhnya  berdasarkan  setiap  versi 
yang ada. Collection berisi metadata tersebut nantinya dipakai 
sebagai 
collection  berisi  data  yang 
sesungguhnya  dan  informasi  terkait  perubahan  pengkodean 
variabel  pada  data  collection  dari  tiap-tiap  versi  survei. 
Adapun  metadata  collection  yang  dimaksud  dapat  dilihat 
contoh  penggalannya  pada  Gambar  7  dan  koleksi  collection 
berisi  data  sesungguhnya  dapat  dilihat  contoh  penggalannya 
pada Gambar 8.  

Gambar 7. Penggalan metadata yang digunakan pada simulasi algoritma dasar 

Gambar 5. Algoritma penyimpanan yang diusulkan 

Gambar 6. Algoritma pencarian yang diusulkan 

Pada  hal  ini,  penulis  mencoba  membuat  suatu  database 
berupa  metadata  collection  yang  berisi  pemetaan  tiap-tiap 
variabel  dengan  isi  data  yang  sama  dari  tahun  berbeda  ke 

Gambar 8. Penggalan data asli yang digunakan pada simulasi algoritma dasar 

Selanjutnya,  untuk  membandingkan  algoritma  dasar  dan 
algoritma  yang  dihasilkan,  penulis  melakukan  uji  coba 
penyaringan data  menggunakan kedua algoritma tersebut. Uji 
coba  penyaringan  dilakukan  dengan  melakukan  perulangan 
sebanyak 1.000 kali, dan kemudian dibandingkan rata-rata dan 
standar  deviasi  dari  waktu  yang  digunakan,  dimulai  dari 
proses  penginputan  query  berisi  filter  yang  akan  digunakan 
hingga data ditampilkan. 

 5 / 8 

 
 
 
 
 
 
Sesuai  dengan 

tujuan  awal  yang  bermaksud  untuk 
membandingkan  kedua  algoritma,  yang  pada  hal  ini  adalah 
algoritma  dasar  dan  algoritma  usulan,  penulis  menggunakan 
dua  macam  query,  yang  pertama  adalah  query  dengan 
pengkodean  kolom  yang  masih  belum  sama  untuk  setiap 
versinya,  dan  yang  kedua  adalah  query  dengan  pengkodean 
kolom  yang sudah disama ratakan. Pada  hal ini,  kedua  query 
di-query-kan  pada  database  yang  berbeda  dikarenakan 
perbedaan  struktur  pengkodeannya,  namun  memiliki  data 
yang  sama.  Sebagai  konteks  penjelasan  query  yang  akan 
dipakai,  database  yang  digunakan  oleh  penulis  dinamakan 
SAK2018  untuk  data  Sakernas  2018  dengan  pengkodean 
awal,  SAK2015  untuk  data  Sakernas  2015  dengan 
pengkodean  awal,  serta  SAK2018_v2  dan  SAK2015_v2 
masing-masing  untuk  data  Sakernas  tahun  2018  dan  2015 
berturut-turut dengan pengkodean yang sudah disamaratakan. 
Pada  hal  ini,  digunakan  query  yang  sederhana  untuk 
menampilkan 
jumlah  baris  yang  dihasilkan  dari  hasil 
pencarian  tanpa  menampilkan  setiap  baris.  Penulis  akan 
membandingkannya  dengan  bentuk  SQL-nya  untuk  dapat 
menggambarkan  dengan  mudah  seperti  apa  query  filtering 
yang  digunakan.  Berikut  adalah  bentuk  query  pada  database 
dengan pengkodean yang belum disamaratakan jika di-query-
kan menggunakan SQL: 

  SELECT  COUNT(*)  FROM  SAK2018  WHERE 

(B4_K8>20 AND B4_K6=1 AND B5_R1A>7) 

  SELECT  COUNT(*)  FROM  SAK2015  WHERE 

(B4_K5>20 AND B4_K4=1 AND B5_R1A>7) 

Kedua  query  di  atas,  jika  diterjemahkan  ke  dalam  bahasa 
sehari-hari,  adalah  untuk  melakukan  penyaringan  data  dari 
database  penyimpanan  hasil  survei  dengan  kriteria  memiliki 
umur  lebih  dari  20  tahun,  berjenis  kelamin  laki-laki,  dan 
berpendidikan setidaknya tamat SMA. Ketiga kriteria tersebut 
sama-sama  diterapkan  ke  dalam  dua  database  berbeda,  yaitu 
SAK2018  dan  SAK2015.  Walaupun  kriteria  tersebut  sama, 
namun  penggunaan  nama  kolom  dari  kedua  database 
memiliki perbedaan. Hal ini selaras dengan pengkodean yang 
masih belum disamaratakan, dan mengharuskan penulis untuk 
mencocokkan kembali kriteria yang diinginkan dengan daftar 
kolom yang ada pada setiap versinya. Dapat dikatakan bahwa 
penggunaan  query  ini  merupakan  contoh  yang  layak  untuk 
simulasi algoritma yang sudah ada. 

Adapun  bentuk  query  kedua  berikut  dijalankan  pada 

database yang pengkodeannya sudah disamaratakan, yaitu: 

  SELECT  COUNT(*)  FROM  SAK2018_v2  WHERE 
JENIS_KELAMIN=1  AND 

(UMUR>20  AND 
PENDIDIKAN_AKHIR>7) 

  SELECT  COUNT(*)  FROM  SAK2015_v2  WHERE 
JENIS_KELAMIN=1  AND 

(UMUR>20  AND 
PENDIDIKAN_AKHIR>7) 

Kedua  query  di  atas  memiliki  fungsi  menghasilkan  output 
yang  sama  dengan  query  sebelumnya.  Perbedaannya  hanya 
terletak  pada  nama  kolom  yang  sudah  sama  rata  pada  kedua 
database  yang  menyimpan  data  yang  akan  diambil.  Hal  ini 
menyebabkan  penulis  tidak  perlu  melakukan  pencocokkan 
berulang  kali  untuk  setiap  database,  dan  dapat  dikatakan 
layak untuk menyimulasikan algoritma yang diusulkan. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 9. Perbandingan waktu berdasarkan algoritma pencarian 

Grafik pada Gambar 9 memperlihatkan bahwa  waktu yang 
digunakan  untuk  menampilkan  hasil  oleh  algoritma  usulan 
mengalami penurunan dibanding algoritma dasar. Waktu yang 
digunakan  untuk  menjalankan  algoritma  dasar  rata-rata 
sebesar  816,43  milidetik,  sedangkan  waktu  yang  digunakan 
untuk  menjalankan  algoritma  usulan  rata-rata  sebesar  543,99 
milidetik. 

Proses pencarian dan penyaringan suatu data yang disimpan 
pada pada suatu database dapat dioptimasi lebih lanjut dengan 
metode  optimasi  query.  Seperti  yang  telah  diterapkan  pada 
[16],  terdapat  beberapa  metode  optimasi  query  yang  dapat 
diterapkan  dalam  proses  pencarian  data  menggunakan 
MongoDB.  Maka  dari  itu,  pada  langkah  selanjutnya  penulis 
mencoba  menerapkan  langkah-langkah  optimasi  pada  query 
pencarian  yang  digunakan.  Metode  optimasi  yang  telah 
diterapkan  oleh  penulis  pada  penelitian  ini  adalah  metode 
query indexing dan query covering. 

Pada  dasarnya,  query  yang  digunakan  dalam  pencarian 
data, baik sebelum atau pun sesudah penerapan metode query 
indexing dan  query  covering adalah  sama.  Penggunaan  index 
dalam 
dengan 
mendefinisikan  indeks  yang  akan  digunakan  terlebih  dahulu, 
yang  pada  penelitian 
ini  dapat  dilakukan  dengan 
memanfaatkan fitur native dari MongoDB sendiri. 

digunakan 

pencarian 

query 

suatu 

Seperti  pendefinisian  indeks  pada  umumnya,  proses  ini 
dilakukan  sebelum  query  dijalankan.  MongoDB  sendiri 
memiliki  fungsi  cursor.explain  dan  db.collection.explain() 
yang  dapat  membantu  untuk  mendefinisikan  indeks.  Kedua 
fungsi  tersebut  membantu  dengan  cara  memberikan  rencana 
indeks terbaik untuk query yang akan dijalankan. Untuk query 
covering  sendiri  adalah  salah  satu    metode  lanjutan  dari 
indexing,  dimana  seluruh  key  yang  digunakan  dalam  query  
harus  memiliki  indeksnya  masing-masing.  Query  covering  
sendiri  dimaksudkan  agar  aplikasi  MongoDB  sama  sekali 
tidak perlu mengakses data asli, dan hanya mengakses indeks 
yang sudah didefinisikan saja. 

Untuk  mengukur  keoptimalan  query  yang  dipakai,  penulis 
melakukan  perbandingan  waktu  dan  banyaknya  query/detik 
iterasi  pengambilan  data  yang  dilakukan 
dalam  proses 
sebanyak  1000  kali,  untuk  kemudian  diambil  rata-rata  dan 
standar deviasi dari waktu yang digunakan. Mengenai hal ini, 

 6 / 8 

 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

hasil  dari  setiap  jenis  query  adalah  pencetakan  setiap  baris 
hasil pencarian dan penyaringan. Agar dapat menggambarkan 
query  yang  digunakan,  berikut  bentuknya  jika  diterjemahkan 
dalam bentuk SQL: 
  SELECT 

(NAMA_KAB,  KODE_KAB,  B4_K3, 
B4_K8,  B5_R1A)  FROM  SAK2018  WHERE 
(B4_K8>20, B4_K6=1, B5_R1A>7) 

Dapat  dilihat  bahwa  query  di  atas  memiliki  perbedaan 
dengan  query-query  pada  percobaan  sebelumnya.  Hal  ini 
dikarenakan  output  yang  diinginkan  adalah  seluruh  baris, 
walaupun  tidak  semua  kolom  dicetak  untuk  mengurangi 
waktu dan energy yang digunakan pada uji coba ini. Hasil dari 
perbandingan  waktu  yang  dipakai  dapat  dilihat  pada  Gambar 
10. 

Gambar 11. Perbandingan jumlah query yang dieksekusi per detik 

perbandingan 

Setelah  mengetahui  bahwa  metode  query  covering 
cenderung  mendukung  optimasi  algoritma  pencarian  data, 
penulis  melakukan 
dengan 
menggunakan  query  untuk  menampilkan  jumlah  baris  dari 
hasil pencarian, dan kemudian disandingkan dengan hasil dari 
query  sederhana  menggunakan  algoritma  usulan.  Hasil 
perbandingan  tahap  kedua  ini  dapat  dilihat  pada  Gambar  12, 
sedangkan  perbandingan  rata-rata  dan  standar  deviasi  dari 
waktu  yang  digunakan  ketiga  percobaan  untuk  mendapatkan 
hasil yang sama dapat dilihat pada Tabel II. 

kembali 

Gambar 10. Perbandingan waktu eksekusi query 

Pada  Gambar  10,  dapat  dilihat  bahwa  setiap  metode  yang 
diterapkan  cenderung  mengurangi  waktu  yang  dipakai  untuk 
mencetak  hasil  query.  Query  dasar  yang  digunakan  di  awal 
memiliki 
rata-rata  waktu  pencetakan  sebesar  1.271,24 
milidetik. Pada query yang dilakukan penerapan metode query 
indexing  memiliki 
rata-rata  waktu  pencetakan  sebesar 
1.217,82  milidetik,  sedangkan  penerapan  metode  query 
covering  pada  query  dasar  memiliki 
rata-rata  waktu 
pencetakan  sebesar  1.103,25  milidetik.  Sejauh  ini,  metode 
query  covering  dapat  dikatakan  lebih  optimal  daripada  jenis 
query lainnya. 

Pada  Gambar  10  terlihat  pula  bahwa  waktu  pencetakan 
pada  awal  dan  akhir  iterasi  cenderung  lebih  tinggi,  hal  ini 
terjadi  karena  tenaga  yang  digunakan  pada  sisi  server 
MongoDB  belum  dikerahkan  secara  optimal,  dan  sedang 
dalam  proses  pengerahan  memori  server  untuk  melakukan 
pemrosesan  query  yang  lebih  banyak  perdetiknya.  Dapat 
dilihat  dari  Gambar  11  yang  menunjukkan  banyaknya  query 
yang dilakukan per detik oleh setiap  query, bahwa penerapan 
metode  query  covering  memroses  query  cenderung  lebih 
banyak daripada dua lainnya. Hal ini juga yang menyebabkan 
pencetakan  hasil  dari  query  tersebut  cenderung  lebih  cepat 
dibanding yang lain.  

Gambar 12. Perbandingan waktu eksekusi query pada algoritma usulan 

TABEL II 
PERBANDINGAN RATA-RATA DAN STANDAR DEVIASI WAKTU QUERY 
Standar 
Deviasi 

Jenis Query 

Rata-rata 

No 

1  Query Dasar pada Algoritma Dasar 

2  Query Dasar pada Algoritma Usulan 

816,43 

543.99 

3 

Covered Query pada Algoritma Usulan 

518.91 

4.71 

3.31 

2.96 

Dengan mengacu pada Tabel II di atas, dapat dilihat bahwa 
penerapan  algoritma  yang  diusulkan  memangkas  rata-rata 
waktu  yang  dipakai  untuk  mendapatkan  hasil  yang  sama 
sebesar  34  persen,  atau  dengan  kata  lain  menimbulkan 
percepatan  pada  rata-rata  waktu  hingga  mencapai  50  persen. 

 7 / 8 

 
 
 
 
 
 
Standar deviasi dari waktu yang dipakai juga pada mengalami 
penurunan sebesar 1,75 atau sekitar 60 persen. 

Melihat  pengurangan  dari  kedua  indikator,  yaitu  rata-rata 
dan  standar  deviasi  waktu  yang  digunakan,  maka  dapat 
dikatakan  bahwa  penelitian  terkait  penerapan  algoritma  dan 
optimasi ini dapat dikatakan berhasil. 

Beberapa kesimpulan yang dapat diperoleh pada penelitian 

ini adalah sebagai berikut: 

1.  Algoritma  pencarian  data  yang  sudah  dikembangkan 
pada penelitian terdahulu masih dapat dioptimasi lebih 
lanjut  dari  segi  waktu  yang  dibutuhkan  untuk 
menjalankan query pencarian. 

2.  Optimasi algoritma yang dilakukan pada penelitian ini 
berfokus pada  pengajuan algoritma  penyimpanan baru 
dan optimasi dari sisi pemakaian query. 

3.  Penggunaan  waktu  pencarian  data  menggunakan 
algoritma  yang  diusulkan  penulis  pada  penelitian  ini 
lebih sedikit jika dibandingkan dengan algoritma yang 
telah  dikembangkan  oleh  penelitian  terdahulu,  dimana 
terjadi percepatan rata-rata sebesar 50 persen. 

4.  Metode query covering yang diterapkan sebagai proses 
lanjutan  dapat  dikatakan  efektif  dalam 

optimasi 
mendukung proses optimasi algoritma pencarian data. 

5.  Secara  garis  besar,  penelitian 

terkait  optimasi 
algoritma SKini dapat dikatakan berhasil karena waktu 
yang  dibutuhkan,  baik  dalam  proses  penyimpanan 
maupun pengambilan data sudah berkurang.. 

Adapun  saran  dari  penulisan  yang  dapat  dilakukan  ke 
depannya  untuk  mengembangkan  hasil  penelitian  ini  adalah 
sebagai berikut: 

1.  Merancang  suatu  algoritma  klasifikasi  variabel  efektif 
dan  dapat  dijadikan  acuan  pada  proses  penyimpanan 
data. 

2.  Melakukan  pengukuran  waktu  yang  dipakai  dalam 
proses klasifikasi  variabel  yang dilakukan pada proses 
penyimpanan data secara eksplisit. 

[9] 

[10] 

[11] 

[12] 

[13] 

[14] 

[15] 

[16] 

[17] 

[18] 
[19] 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Application,” Int. J. Adv. Comput. Sci. Appl., vol. 6, no. 11, 2015, 
doi: 10.14569/ijacsa.2015.061111. 
C. De Lima and R. D. S. Mello, “A workload-driven logical design 
approach for NoSQL document databases,” 17th Int. Conf. Inf. 
Integr. Web-Based Appl. Serv. iiWAS 2015 - Proc., no. February 
2019, 2015, doi: 10.1145/2837185.2837218. 
M. A. Rahman and L. R. Maghfiroh, “KAJIAN DATABASE 
RELASIONAL DAN NONRELASIONAL PADA DATA BPS 
DALAM MENGHADAPI ERA BIG DATA (Relational and 
Nonrelational Database Study on BPS-Statistics Indonesia Data in 
Facing Era of Big Data),” pp. 25–30, 2019. 
L. R. Maghfiroh and I. G. B. B. Nugraha, “Survey data and 
metadata modelling using document-oriented NoSQL,” J. Phys. 
Conf. Ser., vol. 971, no. 1, 2018, doi: 10.1088/1742-
6596/971/1/012030. 
Y. Li and S. Manoharan, “A performance comparison of SQL and 
NoSQL databases A performance comparison of SQL and NoSQL 
databases,” 978-1-4799-1501-9/13- 2013 Ieee, no. November, pp. 
15–19, 2015, [Online]. Available: 
http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6625441
. 
L. R. Maghfiroh and I. Santoso, “Nosql Model Data Warehouse 
Metadata Survei Dinamis Studi Kasus : Survei Rumah Tangga,” pp. 
55–74, 2019. 
F. G. Tinetti, F. Paez, L. I. Aita, and D. Barry, “Distributed Search 
on Large NoSQL Databases,” Proc PDPTA, pp. 685–690, 2011. 
Y. Gu, X. Wang, S. Shen, S. Ji, and J. Wang, “Analysis of data 
replication mechanism in NoSQL database MongoDB,” 2015 IEEE 
Int. Conf. Consum. Electron. - Taiwan, ICCE-TW 2015, pp. 66–67, 
2015, doi: 10.1109/ICCE-TW.2015.7217033. 
D. Mahajan, C. Blakeney, and Z. Zong, “Improving the energy 
efficiency of relational and NoSQL databases via query 
optimizations,” Sustain. Comput. Informatics Syst., vol. 22, pp. 
120–133, 2019, doi: 10.1016/j.suscom.2019.01.017. 
P. Cash, T. Stanković, and M. Štorga, Experimental design 
research: Approaches, perspectives, applications, no. July. 2016. 
R. Munir, Matematika Disktrit. 2010. 
A. Koswara, “Dokumentasi pengembang Web Portal INDAH.” 

3.  Melakukan 

penelitian 

lanjut  mengenai 
penyimpanan  perbedaan  struktur  akibat  spesialisasi 
atau generalisasi variabel. 

lebih 

4.  Mengimplementasikan 

database 
nonrelasional sebagai alternatif penyimpanan data hasil 
survei BPS ke depannya. 

penggunaan 

[1] 
[2] 

[3] 

[4] 

[5] 

[6] 

[7] 
[8] 

DAFTAR PUSTAKA 

W. H. Inmon, Building the Data Warehouse, 3rd@book{1. 2002. 
E. A. Brewer, “Towards robust distributed systems,” in Proceedings 
of the Nineteenth Annual ACM Symposium on Principles of 
Distributed Computing, 2000, p. 7, doi: 10.1145/343477.343502. 
E. A. Brewer, “CAP twelve years later: How the „rules‟ have 
changed,” Computer (Long. Beach. Calif)., vol. 45, no. 2, pp. 23–
29, 2012, doi: 10.1109/mc.2012.37. 
M. O. Fitri, “Trend Penggunaan NoSQL Untuk Basis Data Non-
Relasional,” J. Teknosains, vol. 7 Nomor 1, pp. 120–127, 2013. 
“HDFS Architecture.” 
https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html. 
“Microsoft SQL Server,” [Online]. Available: 
https://www.microsoft.com/en-us/sql-server. 
“IBM Db2,” [Online]. Available: www.ibm.com/analytics/db2. 
C. Gyorödi, R. Gyorödi, and R. Sotoc, “A Comparative Study of 
Relational and Non-Relational Database Models in a Web- Based 

 8 / 8 

 
 
 
 
"
221709939,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Proyeksi Tingkat Penghunian Kamar Hotel Periode 
Pandemi Covid-19 di Indonesia  

Pitta Putriwati Hutagalung (221709939, 4SD2) 
Dosen Pembimbing: Nucke Widowati Projo Kusumo 

teroris  11  September 

serangan 
(2001),  Severe  Acute 
Respiratory  Syndrome  (SARS)  (2003),  krisis  ekonomi  global 
pada tahun 2008/2009, dan Middle East Respiratory Syndrome 
(MERS) (2015). Namun, tidak satupun dari peristiwa-peristiwa 
tersebut  menyebabkan  penurunan  jangka  panjang  dalam 
perkembangan pariwisata global, bahkan beberapa diantaranya 
tidak terlihat pada Gambar 1, dimana hanya SARS (-0,4%) dan 
krisis  global  (-4,0%)  yang  terlihat  penurunannya.  Hal  ini 
menunjukkan  bahwa  sistem  pariwisata  global  tahan  terhadap 
guncangan eksternal. Namun, ada terdapat banyak bukti bahwa 
dampak dan pemulihan dari pandemi Covid-19 belum pernah 
terjadi sebelumnya.[2] 

Ringkasan—  Tingkat  Penghunian  Kamar 

(TPK)  Hotel 
menunjukkan  banyaknya 
jumlah  wisatawan  mancanegara 
maupun  nusantara  yang  berkunjung  di  daerah-daerah 
pariwisata.  Pengeluaran  wisatawan  menjadi  salah  satu  sumber 
pendapatan  sektor  pariwisata  termasuk  di  bidang  perhotelan. 
Namun, pandemi Covid-19 yang muncul pada akhir tahun 2019 
menyebabkan  perekonomian  menurun  dan  TPK  juga  menurun 
akibat  aturan  Pembatasan  Sosial  Berskala  Besar  (PSBB)  dan 
protokol  kesehatan  yang  diterapkan  pemerintah.  Melalui 
teknologi  Big  Data,  data  TPK  dihasilkan  melalui  web  scraping 
website  agoda.com--salah  satu  situs  booking  kamar  hotel  yang 
banyak  digunakan.  Peneliti  akan  memproyeksikan  TPK  Hotel 
Bintang dan Non Bintang menggunakan metode Autoregressive 
Integrated  Moving  Average  (ARIMA),  dan  Neural  Network 
(NN),  kemudian    menganalisis  volatilitas  jumlah  omzet  periode 
Indonesia  menggunakan  metode 
pandemi  Covid-19  di 
AutoRegressive Conditional Heteroskedasticity / Generalized 
AutoRegressive 
Heteroskedasticity 
(ARCH/GARCH).  Metode  NN  memiliki  tingkat  akurasi  yang 
lebih  tinggi  daripada  metode  ARIMA,  yaitu  dengan  nilai  Mean 
Absolute  Percentage  Error  (MAPE)  proyeksi  TPK  Hotel  sebesar 
1,21%.  Volatilitas  jumlah  omzet  tampak  tinggi  dan  fluktuatif 
pada periode pandemi tahun 2020 ini. 

Conditional 

Kata  Kunci—  Tingkat  Penghunian  Kamar  (TPK),  Hotel, 

Proyeksi, Covid-19. 

I.  LATAR BELAKANG 

Pneumonia  menular  yang  disebabkan  oleh  virus  corona 
baru  (COVID-19)  pertama  kali  diidentifikasi  di  China  pada 
akhir  2019  [1].  Jumlah  infeksi  telah  meningkat  di  seluruh 
dunia:  Organisasi  Kesehatan  Dunia  menyatakan  wabah  virus 
sebagai ""darurat kesehatan masyarakat global"" pada 31 Januari 
dan  menandainya  sebagai  ""pandemi  global""  pada  11  Maret 
[1]     . Pandemi ini telah merusak pembangunan ekonomi dan 
industri di seluruh dunia. Industri perhotelan, yang padat modal 
dan  tenaga  kerja,  sangat  terpengaruh  karena  investasi  yang 
besar  dan  biaya  operasional  yang  tinggi.  Dalam  menghadapi 
berbagai  bentuk  tekanan,  status  industri  ini  patut  mendapat 
perhatian khusus. 

Pembatasan  perjalanan  internasional,  regional  dan  lokal 
segera  mempengaruhi  ekonomi  nasional  termasuk  sistem 
pariwisata perhotelan. Dengan adanya pemberlakuan larangan 
perjalanan  yaitu  Pembatasan  Sosial  Berskala  Besar  (PSBB), 
menutup  perbatasan,  atau  karantina  di  Indonesia,  tingkat 
hunian  hotel  mulai  menurun  drastis.  Negara  Indonesia 
memulangkan  para  pelancong  dan  membatasi  mobilitas 
penduduk dengan peraturan-peraturan baru.  

Penting  untuk  diketahui  bahwa  pariwisata  secara  global 
telah  dihadapkan  dalam  berbagai  macam  krisis  (Gambar  1). 
Antara tahun 2000 dan 2015, peristiwa besar yang terjadi yaitu 

Gambar 1. Dampak dari peristiwa krisis besar pada pariwisata global 
Sumber data: World Bank (2020a, 2020b) 

Pandemi  Covid-19 

telah  berdampak  negatif  bagi 
perekonomian bangsa, namun perspektif ini bisa berbeda ketika 
dihubungkan dengan Sustainable Development Goals (SDGs) 
yang  telah  diagendakan  UN  mulai  tahun  2015  lalu.  Dalam 
konteks SDGs, perubahan positif sedang berlangsung dan dapat 
dilihat  sebagai  prekursor  untuk  perubahan  pada  tingkat  yang 
lebih  luas  dimana  pariwisata  akan  berorientasi  pada  SDGs, 
daripada pertumbuhan jumlah pariwisata yang selama ini telah 
digagaskan.[2] Dapat dilihat dari semakin berkembangnya tren 
booking  seperti  agoda,  booking.com,  tripadvisor  dan  untuk 
level  hotel  bintang  lima  wisatawan  banyak  menggunakan 
layanan  AirBnB  berupa  layanan  booking  apartemen,  rumah, 
atau vila.[2] 

Di Indonesia, sektor pariwisata merupakan invisible export 
karena  dapat  menghasilkan  devisa  sebagai  salah  satu 
pendapatan  negara.  Devisa  berasal  dari  statistik  kunjungan 
wisatawan mancanegara yang berkunjung ke Indonesia. Selain 
itu,  pengeluaran  wisatawan  nusantara  juga  menjadi  sumber 
pendapatan  sektor  pariwisata  terutama  bidang  perhotelan. 
Statistik kunjungan wisatawan mancanegara menunjukkan tren 
yang  meningkat  dalam  beberapa  tahun  sebelumnya  dan 
mencapai kunjungan tertinggi sebanyak 16,11 juta pada tahun 

 1 / 8 

 
 
 
 
 
 
 
2019.  Sedangkan  untuk  statistik  perjalanan  wisatawan 
nusantara  semakin  meningkat  sebanyak  12,03  persen  pada 
tahun 2018 dibandingkan tahun sebelumnya. [3] 

Oleh  karena  itu,  dengan  melihat  indikator  seperti  Tingkat 
Penghunian Kamar hotel menandakan bahwa semakin banyak 
jumlah  wisatawan  yang  berkunjung  di  daerah  pariwisata. 
Semakin tinggi tingkat penghunian kamar hotel maka semakin 
banyak  jumlah  kamar  yang  terjual,  artinya  semakin  banyak 
juga besaran pajak yang dibayarkan kepada pemerintah daerah 
tersebut. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

III. PENELITIAN TERKAIT 

Proyeksi Total Penghunian Kamar Hotel yang sudah diteliti 
oleh  beberapa  peneliti  lainnya  menjadi  suatu  acuan  bagi 
peneliti  dalam  menulis  dengan  menganalisis  dan  mengkaji 
beberapa  penelitian  yang  terdahulu.  Gambar  2  merupakan 
literature  map  berisi  ringkasan  penelitian  terdahulu  dari 
beberapa jurnal yang terkait dengan penelitian peneliti.  

Data Tingkat Penghunian Kamar Hotel dikumpulkan oleh 
dua  instansi  berbeda  yaitu  Kementerian  Pariwisata  dan 
Ekonomi  Kreatif  (Kemenparekraf)  dan  Badan  Pusat  Statistik 
(BPS).  Dimana  sampai  saat  ini  masih  dikumpulkan  melalui 
survei  dan  data  administratif  dari  tiap  daerah.[4]  Namun, 
peneliti  akan  menggunakan  sumber  data  dari  website  agoda 
yang merupakan salah satu website dan aplikasi paling banyak 
digunakan wisatawan untuk memesan hotel.[12] 

Dalam penelitian ini, peneliti      memproyeksikan Tingkat 
Penghunian Hotel di Indonesia menggunakan beberapa metode      
yaitu  Autoregressive  Integrated  Moving  Average  (ARIMA), 
Neural  Network  (NN),  dan  AutoRegressive  Conditional 
Heteroskedasticity  /  Generalized  AutoRegressive  Conditional 
Heteroskedasticity (ARCH/GARCH) periode Covid-19. Hasil 
penelitian  diharapkan            dapat  membantu            pengambil 
kebijakan  untuk  bertindak  secara  “data  driven-decision 
making”. Sehingga      dapat meningkatkan respons kebijakan 
di sektor industri pariwisata.      Pengusaha di sektor pariwisata 
seperti  perhotelan  juga  akan  dapat  memanfaatkan            data 
proyeksi hunian kamar hotel untuk      mengantisipasi keadaan 
tingkat kunjungan di masa mendatang.      

Penelitian ini menggunakan beberapa metode analisis time 
series untuk proyeksi seperti ARIMA dan NN yang kemudian 
secara  ilmiah,  hasil  proyeksi  akan  dibandingkan  tingkat 
akurasinya agar dapat diketahui metode yang lebih tepat dalam 
memproyeksikan data TPK hotel. Beberapa penelitian seperti 
[13],  [14],  dan  [15]  merekomendasikan  penggunaan  model 
ARIMA dan NN dalam memproyeksikan time series. 

II.  TUJUAN PENELITIAN 

Berdasarkan  uraian  latar  belakang,  maka dapat  dijelaskan 
tujuan dan manfaat dari Proyeksi Tingkat Penghunian Kamar 
Hotel Periode Pandemi Covid-19 di Indonesia adalah sebagai 
berikut: 

1.  Menghitung  Tingkat  Penghunian  Kamar  (TPK)  dan 

Jumlah Omzet Hotel di Indonesia.  

2.  Memproyeksikan  Tingkat  Penghunian  Kamar  (TPK) 
Hotel  menggunakan  metode  ARIMA  dan  Neural 
Network 
(NN)  pada  periode  pandemi  Covid-19 
(Januari-Desember 2020) di Indonesia.  

3.  Menganalisis 

volatilitas 

Jumlah  Omzet  Hotel 
menggunakan  metode  ARCH/GARCH  pada  periode 
pandemi  Covid-19 
(Januari-Desember  2020)  di 
Indonesia. 

4.  Membandingkan  tingkat  akurasi  metode  ARIMA  dan 
Neural Network (NN) dalam memproyeksikan Tingkat 
Penghunian Kamar (TPK) Hotel.  

Gambar 2. Penelitian Terkait 

IV. METODE PENELITIAN  

Sumber data yang digunakan dalam penelitian penelitian ini 
adalah  data  sekunder.  Data  sekunder  yang  digunakan  dalam 
penelitian 
ini  diperoleh  melalui  web  scraping  website 
agoda.com.  Data  yang  diperoleh  tersebut  kemudian  diolah 
untuk  memenuhi  kebutuhan  dalam  proyeksi  TPK  Hotel  di 
Indonesia.  

Pengumpulan  data  dilakukan  oleh 

tim  Subdirektorat 
Pengembangan  Model  Statistik  (PMS)  yang  dibawahi  oleh 
Direktorat Analisis dan Pengembangan Statistik (DAPS) BPS 
Pusat. Proses pengumpulan data  menggunakan metode     web 
scraping  telah  dilakukan  oleh  tim  PMS  dengan  membangun 
program  crawler  berbasis  scrapy  Python.  Proses  scraping 
memerlukan waktu sekitar 2-3 jam, tergantung speed internet.  
Metode  penelitian  yang  digunakan  dalam  teknik  analisis 
data 
adalah  menggunakan  metoda  kualitatif  dengan 
menggunakan data hasil scraping website agoda.com oleh tim 
PMS BPS Pusat. Data tersebut digunakan sebagai sumber data 
utama untuk analisis yang dilakukan peneliti. Informasi yang 
dihasilkan berupa Tingkat Penghunian Kamar dan Omzet Hotel 
yang  disegregasikan  berdasarkan  kategori  hotel  bintang  dan 
non  bintang  serta  provinsi.  Kemudian  setelah  selesai 
preprocessing  data,  peneliti  akan  memproyeksikan  TPK 
dengan  menggunakan  metode  NN  pendekatan  machine 
learning  dan  ARIMA  serta  ARCH/GARCH  untuk  mengukur 
volatilitas jumlah omzet hotel. 

 2 / 8 

 
 
 
 
 
Dalam meneliti Proyeksi Tingkat Penghunian Kamar Hotel, 
tahapan  penelitian  yang 

peneliti  menggunakan  2  (dua) 
mencakup langkah-langkah penelitian sebagai berikut.  
1.  Tahap Perencanaan 

a.  Identifikasi Masalah 

Peneliti mengidentifikasi masalah yang terjadi pada 
proses  menghasilkan  publikasi  Tingkat  Penghunian 
Kamar  di  Indonesia  oleh  BPS  terdapat  beberapa 
masalah sebagai berikut: 

i.  TPK dan omzet hotel menjadi sangat fluktuatif dan 
sulit  untuk  diproyeksikan  karena  kondisi  pandemi 
Covid-19  merupakan  suatu  kejadian  yang  tidak 
normal terjadi. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Sedangkan untuk menghitung omzet menggunakan 
rumus  Revenue  Per  Available  Room  (RevPar)  jumlah 
kamar terpakai dibagi dengan harga sewa kamar[5]. 

𝑂𝑚𝑧𝑒𝑡  =  

c.  Proyeksi Data  

𝑅𝑜𝑜𝑚 𝑂𝑐𝑐𝑢𝑝𝑖𝑒𝑑
𝑃𝑟𝑖𝑐𝑒

Disini  peneliti  menggunakan  tiga  metode  yaitu 
metode Neural Network (NN), ARIMA pada Gambar 3 
dan  Gambar  4  bertujuan  untuk  menghitung  proyeksi 
perkembangan Tingkat Penghunian Kamar (TPK) pada 
masa  pandemi  Covid-19  (Januari-Desember  2020) 

untuk periode Januari-April 2021.  

ii.  Menggunakan  waktu 
pengumpulan  data 
kabupaten/kota. 

yang 

dalam 
tiap  BPS  provinsi  dan 

lama 

iii.  Pendataan  melalui  survei  membutuhkan  biaya  dan 

usaha lebih. 

b.  Mengadakan studi pendahuluan 

Peneliti  melakukan  studi  pendahuluan  dengan 
mengumpulkan 
informasi  yang  berkaitan  dengan 
masalah  publikasi  TPK  dan  omzet  hotel  di  Indonesia.  
Hal tersebut dilakukan untuk mengetahui masalah yang 
ada  beserta  batasannya  sehingga  peneliti  dapat 
menyusun  strategi  pemecahan  masalah.  Menurut 
peneliti, studi tentang proyeksi TPK di Indonesia belum 
menggunakan data agoda.com sebagai sumber data dan 
metode  NN  yang  berbasis  pendekatan  machine 
learning. 

c.  Menentukan sampel penelitian 

Peneliti menentukan sampel yang digunakan dalam 
penelitian  yaitu  Tingkat  Penghunian  Kamar  Hotel  di 
Indonesia periode Januari-Desember 2020.  

d.  Menyusun rencana penelitian 

Peneliti  menentukan  rencana  seperti  data,  sumber 

daya, waktu dan sebagainya dalam penelitian. 

2.  Tahap Pelaksanaan 

a.  Pengumpulan Data (Acquire & Accessed) 

Pengumpulan  data  telah  dilakukan  oleh  tim  PMS 
BPS Pusat. sebelumnya. Teknik pengumpulan data yang 
dilakukan adalah melalui  web scraping  dengan  scrapy 
Python pada website agoda.com. Adapun variabel yang 
diperlukan  adalah  harga  sewa  kamar,  jumlah  kamar 
terpakai  (room  occupied/used),  jumlah  kamar  tersedia 
(room  availability),  harga,  klasifikasi  hotel,  dan 
provinsi.  

b.  Analisis Data (Analytic) 

Peneliti  menganalisis  big  data  agoda  dengan 
menggunakan  tools  RStudio. Untuk  menghitung  TPK, 
digunakan  rumus  jumlah  kamar  yang  dihuni  dibagi 
dengan  banyaknya  jumlah  kamar  yang  tersedia  dikali 
100%[4].  

𝑂𝑐𝑐𝑢𝑝𝑎𝑛𝑐𝑦 =  

𝑅𝑜𝑜𝑚 𝑂𝑐𝑐𝑢𝑝𝑖𝑒𝑑
𝑅𝑜𝑜𝑚 𝐴𝑣𝑎𝑖𝑙𝑎𝑏𝑖𝑙𝑖𝑡𝑦

 𝑥 100% 

Gambar 3. Flowchart Metode ARIMA  

Gambar 4. Flowchart Metode Neural Network (NN) 

Sedangkan metode ARCH/GARCH pada Gambar 5 
bertujuan  untuk  menganalisis  volatilitas  jumlah  omzet 

 3 / 8 

 
 
 
 
 
 
 
 
 
hotel  bintang  dan  non  bintang,  dikarenakan  metode 
tersebut  menganggap  varians  sebagai  time  series. 
Algoritma  dari  kedua  metode  NN  melalui  pendekatan 
machine  learning,  serta  ARIMA  dan  ARCH/GARCH 
diimplementasikan dalam RStudio. 

Gambar 5. Flowchart Metode ARCH/GARCH [8] 

d.  Membandingkan Metode (Compare) 

Hasil  proyeksi  dari  metode  NN  dan  ARIMA  akan 
dibandingkan  tingkat  akurasinya.  Terdapat  beberapa 
jenis  eror  yang  ditampilkan  untuk  membandingkan 
tingkat  akurasi  dari  kedua  metode  yaitu:  Root  Mean 
Square  Error  (RMSE),  Mean  Absolute  Error  (MAE), 
dan Mean Absolute Percentage Error (MAPE). Khusus 
metode  ARCH/GARCH  tidak  dapat  dibandingkan 
karena bertujuan untuk mengukur volatilitas dari jumlah 
omzet hotel bintang dan non bintang. Selanjutnya dalam 
penelitian  ini,  nilai  MAPE  dalam  bentuk  persentase 
akan diinterpretasikan dalam empat kategori, yaitu: [6] 

= “Sangat Baik” 

●  < 10%  
●  10%-20%  = “Baik” 
●  20%-50%  = “Wajar” 
●  > 50% 

= “Tidak Akurat” atau “Gagal” 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Melalui  big  data,  dimana  BPS  tidak  perlu  melakukan 
pengumpulan data lewat survei secara langsung pada masa 
Covid-19, maka peneliti menggunakan data hasil scraping 
dari web agoda.com sebagai bahan analisis. Data TPK pada 
periode  Januari-Desember  2020  akan  diproyeksikan 
menggunakan  metode  NN,  dan  ARIMA  untuk  periode 
Januari-Juni  2021.  Kemudian  khusus  jumlah  omzet  pada 
periode  Januari-Desember  2020  akan  dianalisis  volatilitas 
variansnya dengan menggunakan metode ARCH/GARCH 
pada tools RStudio.  

Berdasarkan  penelitian  sebelumnya  dinyatakan  bahwa 
Covid-19  akan  berdampak  positif  dalam  jangka  panjang 
terhadap  perekonomian  terutama  jika  dilihat  dari  SDGs 
bukan  pertumbuhan  jumlah  pariwisata.[2]  Sehingga  hasil 
proyeksi  TPK  hotel  akan  sangat  berpengaruh  terhadap 
kondisi sebelumnya yang sangat fluktuatif. Setiap daerah di 
Indonesia  mengalami  dampak  yang  berbeda-beda,  namun 
secara umum semakin banyak jumlah kunjungan wisatawan 
di daerah tersebut, maka semakin besar pula pukulan yang 
dirasakan  selama  masa  Covid-19,  karena  kasus  Covid-19 
merupakan kasus pandemi yang mewabah di seluruh dunia 
dan berpotensi menular lewat manusia.  

Gambar 6 merupakan ringkasan kerangka berpikir yang 
dilakukan  peneliti  dalam  penelitian  Proyeksi  Tingkat 
Penghunian  Kamar  Hotel  Periode  Pandemi  Covid-19  di 
Indonesia. 

Gambar 6. Kerangka Berpikir Penelitian 

V.  KERANGKA PIKIR 

Proyeksi  TPK  dan  omzet  hotel  berhubungan  dengan 
manajemen pendapatan dalam bidang pariwisata khususnya 
perhotelan.  Keduanya  merupakan  dalam  manajemen 
pendapatan  perhotelan.  Proyeksi  TPK  adalah  pendorong 
utama  keputusan  penetapan  harga  atau  alokasi  kamar.[7] 
tersebut,  pemerintah  dapat 
Melalui  hasil  proyeksi 
merancang  strategi  untuk  meningkatkan  TPK  agar 
pariwisata di Indonesia kembali stabil. 

VI. HASIL DAN PEMBAHASAN 

jumlah  kamar  yang 

Tingkat  Penghunian  Kamar  hotel  yang  dihitung 
menggunakan rumus jumlah kamar yang dihuni dibagi dengan 
tersedia  dikali  100% 
banyaknya 
divisualisasikan  dalam  bentuk  grafik  pada  Gambar  7.  Grafik 
menunjukkan  penurunan  mulai  bulan  Januari  2020  hingga 
bulan  April  2020  dengan  nilai  TPK  tertinggi  adalah  pada 
tanggal  31  Januari  2020  sebesar  76,14%  dan  mencapai  titik 
terendah pada tanggal 22 April 2020 sebesar 15,23%. Rata-rata 
nilai  TPK  di  Indonesia  pada  periode  Januari-Desember  2020 
adalah 36,92%. 

 4 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

dengan model NNAR(9,1,5) dikategorikan sebagai model yang 
sangat baik. 

Proyeksi TPK Model NNAR(9,1,5)[7] 

l
e
t
o
H
K
P
T

g
n
a
t
n
B

i

l
e
t
o
H
K
P
T

Gambar 7. Grafik TPK Hotel Bintang dan Non Bintang Periode Januari-
Desember 2020 

Data  jumlah  omzet  dihitung  dengan  pendekatan  RevPar 
kemudian divisualisasikan dalam grafik pada Gambar 8 dimana 
sumbu  x  merupakan  jumlah  omzet  dalam  satuan  juta  rupiah. 
Jumlah omzet hotel bintang dan non bintang tampak fluktuatif 
sejak  awal  Januari  2020  namun  cenderung  konstan  sampai 
November  2020  dengan  nilai  tertinggi  adalah  sebesar  194 
miliar  rupiah  pada  tanggal  2  September  2020  dan  mencapai 
titik terendah sebesar 817 juta rupiah pada tanggal 29 Juli 2020. 
Rata-rata  jumlah  omzet  hotel  di  Indonesia  pada  periode 
Januari-Desember 2020 adalah 121 miliar rupiah. 

l
e
t
o
H

t
e
z
m
O
h
a
l
m
u
J

Gambar 8. Grafik Jumlah Omzet Hotel Bintang dan Non Bintang Periode 
Januari-Desember 2020 

Setelah mengklasifikasikan data menjadi data hotel bintang 
dan hotel non bintang, data  disegregasikan menjadi dua  jenis 
klasifikasi  yang  tersusun  berdasarkan  provinsi  dan  tanggal, 
yaitu  TPK  dan  jumlah  omzet  hotel.  Proyeksi  TPK  hotel 
menggunakan metode NN dan ARIMA disajikan dalam bentuk 
visualisasi grafik dan angka. 

Gambar 9 menunjukkan bahwa proyeksi TPK hotel bintang 
menggunakan  metode  NN  untuk  periode  bulan  Januari-Juni 
2021  cenderung  menurun  pada  rentang  40-50%.  Grafik  pada 
sumbu  x  menggambarkan  skala  waktu  dari  tanggal  1  Januari 
2020-31  Desember  2020.  Sedangkan  sumbu  y  pada  grafik 
menggambarkan  nilai  TPK  hotel.  Dimana,  persentase  MAPE 
adalah sebesar 4,54%, yang artinya proyeksi TPK hotel bintang 

Jan 2020 

        Apr 2020                     Jul 2020                 Okt 2020 
Waktu 
Gambar 9. Proyeksi TPK Hotel Bintang dengan Metode NN untuk  
periode Januari-Juni 2021 

    Jan 2021 

Kemudian, pada Gambar 10 dapat dilihat bahwa proyeksi 
TPK hotel non bintang dengan menggunakan metode NN untuk 
periode  bulan  Januari-Juni  2021  mengalami  penurunan  yang 
signifikan  pada  rentang  15-25%.  Dimana  persentase  MAPE 
sebesar  1,21%  Sehingga,  proyeksi  hotel  non  bintang  dengan 
model NNAR(25,1,13) termasuk dalam kategori sangat baik. 

Proyeksi TPK Model NNAR(25,1,13)[7] 

i

g
n
a
t
n
B
n
o
N

l
e
t
o
H
K
P
T

Jan 2020 

        Apr 2020                     Jul 2020                 Okt 2020 

    Jan 2021 

Waktu 
Gambar 10. Proyeksi TPK Hotel Non Bintang dengan Metode NN untuk 

periode Januari-Juni 2021 

Proyeksi  agregat  TPK  hotel  bintang  dan  non  bintang 
menggunakan metode NN menunjukkan grafik yang menurun 
secara  signifikan  pada  rentang  20-40%  untuk  periode  bulan 
Januari-Juli 2021. Jika dilihat dari proyeksi hotel bintang dan 
non  bintang  sebelumnya,  grafik  Gambar  11  menunjukkan 
kesesuaian  skenario  apabila  grafik  hotel  bintang  cenderung 
menurun  dan  hotel  non  bintang  mengalami  penurunan  yang 
signifikan.  Nilai  MAPE  dari  proyeksi  hotel  bintang  dan  non 
bintang dengan model NNAR(22,1,12) adalah sebesar 1,51% 
termasuk dalam kategori model yang sangat baik. 

 5 / 8 

 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Proyeksi TPK Model NNAR(22,1,12)[7] 

Proyeksi TPK Model ARIMA(1,0,1)(1,0,0)[7] 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

l
e
t
o
H
K
P
T

Jan 2020 

        Apr 2020                     Jul 2020                 Okt 2020 

    Jan 2021 

Waktu 
Gambar 11. Proyeksi TPK Hotel Bintang dan Non Bintang dengan Metode 

NN untuk periode Januari-Juni 2021 

Selanjutnya  adalah  proyeksi  menggunakan  metode 
ARIMA.  Grafik  sumbu  x  pada  visualisasi  proyeksi  ARIMA 
menunjukkan  skala  waktu  dari  tanggal  1  Januari  2020-31 
Desember 2020, sedangkan sumbu y menunjukkan nilai TPK 
hotel. Pada Gambar 12 dapat dilihat bahwa proyeksi TPK hotel 
bintang  untuk  periode  bulan  Januari-Juni  2021  cenderung 
konstan  pada  rentang  40-70%.  Dimana,  persentase  MAPE 
model ARIMA(2,1,0) untuk TPK hotel bintang adalah sebesar 
6,29% yang dikategorikan sebagai model yang sangat baik. 

Proyeksi TPK Model ARIMA(2,1,0)[7] 

g
n
a
t
n
i
B

l
e
t
o
H
K
P
T

Jan 2020 

       Apr 2020                    Jul 2020                    Okt 2020 

    Jan 2021 

Waktu 
Gambar 12. Proyeksi TPK Hotel Bintang dengan Metode ARIMA untuk 

periode Januari-Juni 2021 

Grafik  pada  Gambar  13  dibawah  ini  merupakan  hasil 
proyeksi  hotel  non  bintang  untuk  periode  Januari-Juli  2021. 
Grafik menunjukkan proyeksi yang konstan dalam rentang 10-
40%.  Persentase  MAPE  untuk  model  ARIMA(1,0,1)(1,0,0) 
TPK hotel non bintang adalah sebesar 15,85%. Dimana, dapat 
disimpulkan  bahwa  model  tersebut  termasuk  kategori  model 
yang baik. 

g
n
a
t
n
i
B
n
o
N

l
e
t
o
H
K
P
T

Jan 2020 

       Apr 2020                    Jul 2020                    Okt 2020 

    Jan 2021 

Waktu 

Gambar 13. Proyeksi TPK Hotel Non Bintang dengan Metode ARIMA 

untuk periode Januari-Juni 2021 

Kemudian,  Gambar  14  menunjukkan  proyeksi  TPK  hotel 
bintang  dan  non  bintang  periode  Januari-Juni  2021  dengan 
model ARIMA(3,1,3)(1,0,0) menunjukkan grafik yang konstan 
pada  rentang  30-60%.  Dengan  persentase  MAPE  sebesar 
8,35%, dikategorikan sebagai model yang sangat baik. 

Proyeksi TPK Model ARIMA(3,1,3)(1,0,0)[7] 

l
e
t
o
H
K
P
T

Jan 2020 

       Apr 2020                    Jul 2020                    Okt 2020 

    Jan 2021 

Waktu 

Gambar 14. Proyeksi TPK Hotel Bintang dan Non Bintang dengan Metode 

ARIMA untuk periode Januari-Juni 2021 

Data  jumlah  omzet  memiliki  nilai  varians  yang  tinggi 
sehingga  disini  peneliti  akan  menganalisis  volatilitas  varians 
jumlah  omzet  hotel  dengan  metode  ARCH/GARCH.  Grafik 
atas pada Gambar 15 menunjukkan volatilitas varians jumlah 
omzet  hotel  bintang  periode 
Januari-Desember  2020 
menggunakan model sGARCH. Sedangkan grafik di bawahnya 
menunjukkan  volatilitas  varians  menggunakan  model 
tGARCH.  Persamaan  kedua  model  terdapat  pada  distribusi 
yang  digunakan  yaitu  student-t.  Dari  grafik  tersebut  dapat 
disimpulkan  bahwa  volatilitas  varians  omzet  hotel  tinggi, 
karena nilai varians setiap bulannya berubah-ubah. Pada bulan 
Maret-Mei 2020 volatilitas signifikan berfluktuasi naik turun. 
Kemudian  pada  bulan  Juli-September  2020  naik  secara 
signifikan dan di akhir tahun bulan November-Desember 2020 
mengalami kenaikan secara konstan.  

 6 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
2020-01-01 / 2020-12-31 

Jan 01 2020  Apr 01 2020 

   Jul 01 2020 
2020-01-01 / 2020-12-31 

     Okt 01 2020

Jan 01 2020  Apr 01 2020 

   Jul 01 2020 

     Okt 01 2020

Gambar 15. Volatilitas Varians Jumlah Omzet Hotel Bintang dengan Metode 
ARCH/GARCH – sGARCH dan tGARCH untuk periode Januari-Desember 
2020 

Untuk  kasus  hotel  non  bintang  pada  Gambar  16,  grafik 
volatilitas  variansnya  juga  sangat  berfluktuasi.  Dapat  dilihat 
bahwa, baik hotel bintang dan non bintang mengalami kenaikan 
secara  konstan  pada  akhir  tahun  bulan  November-Desember 
2020. Sedangkan untuk bulan Juli-September 2020 volatilitas 
varians tinggi dibandingkan bulan Juni 2020.  

2020-01-01 / 2020-12-31 

Jan 01 2020 

  Apr 01 2020         Jul 01 2020           Okt 01 2020 

2020-01-01 / 2020-12-31 

Jan 01 2020 

  Apr 01 2020         Jul 01 2020           Okt 01 2020 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 16. Volatilitas Varians Jumlah Omzet Hotel Non Bintang dengan 
Metode ARCH/GARCH – sGARCH dan tGARCH untuk periode Januari-
Desember 2020 

Setelah  menghitung  proyeksi  TPK  hotel,  disajikan  tabel 
perbandingan tingkat akurasi metode NN dan ARIMA. Tingkat 
akurasi akan dibandingkan dengan melihat nilai RMSE, MAE 
dan MAPE. Metode yang menghasilkan nilai eror yang lebih 
rendah  merupakan  metode  yang 
tepat  digunakan 
dibandingkan metode dengan nilai eror lebih tinggi. Sehingga 
dari  Tabel  I  dapat  disimpulkan  bahwa  metode  paling  tepat 
digunakan dalam memproyeksikan TPK hotel bintang maupun 
hotel  non  bintang  adalah  metode  NN.  Dimana,  dapat 
diperhatikan  bahwa  persentase  MAPE  terkecil  adalah  1,21% 
untuk proyeksi TPK hotel non bintang. 

lebih 

TABEL I 
TINGKAT AKURASI 

Kasus 
TPK Hotel 
Bintang 

TPK Hotel 
Non Bintang 

TPK Hotel 
Bintang dan 
Non Bintang 

Metode 
NN 

RMSE 
3,195695 

MAE 
1,946425 

MAPE (%) 
4,54 

ARIMA 

4,953599 

2,838415 

6,29 

NN 

0,645629 

0,310032 

1,21 

ARIMA 

6,701603 

3,672026 

15,85 

NN 

1,227458 

0,536429 

1,51 

ARIMA 

5,517004 

3,089988 

8,35 

Pada Tabel II disajikan hasil proyeksi metode terbaik dalam 
memproyeksikan  TPK  hotel  bintang  dan  non  bintang  yaitu 
(NN).  Dalam  nilai  proyeksi 
metode  Neural  Network 
ditampilkan batas bawah dan batas atas dengan tingkat akurasi 
90%  dan  97,5%.  Sedangkan  nilai  aktual  bersumber  dari  fitur 
table dinamis situs bps.go.id oleh Badan Pusat Statistik.[9]  

TABEL II 
PERBANDINGAN HASIL PROYEKSI TERHADAP NILAI AKTUAL TPK 
HOTEL DI INDONESIA 

Bulan 
2021 

Proyeksi (Neural Network) 

Lower 

Upper 

10% 

2,5% 

90% 

97,5% 

Aktual 

Mean 

Januari 

34,25 

33,05 

38,04 

38,96 

36,09 

30,35 

Februari  

31,24 

30,34 

35,92 

37,08 

33,47 

32,40 

Maret 

28,83 

27,30 

34,54 

36,35 

31,54 

36,07 

April 

27,37 

25,04 

34,47 

36,68 

30,95 

34,63 

aHasil proyeksi yang digunakan adalah Proyeksi TPK Hotel Bintang dan 
Non Bintang menggunakan metode Neural Network (NN). 

Nilai aktual TPK hotel pada bulan Januari tidak dalam batas 
rentang  proyeksi  NN,  nilainya  lebih  rendah  daripada  batas 
bawah proyeksi. Hal ini dapat diakibatkan karena Pemerintah 

 7 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
menerapkan  kembali  Kebijakan  Pembatasan  Kegiatan 
Masyarakat  pada  tanggal  11-25  Januari  2021.[10]  dimana 
tercatat kasus Covid-19 pada bulan Januari 2021 menjadi rekor 
terburuk sejak pandemi melanda di Indonesia.[11] Sedangkan 
nilai aktual TPK hotel untuk bulan Februari, Maret, dan April 
2021  masuk  dalam  rentang  batas  proyeksi  dengan  tingkat 
akurasi 97,5%. 

VII. 

PENUTUP 

Berdasarkan  hasil  penelitian  maka  dapat  disimpulkan 

beberapa hal seperti: 

1.  TPK  dan  jumlah  omzet  tidak  menunjukkan  suatu  tren 
naik  atau  turun  namun  tampak  fluktuatif  mulai  bulan 
Januari  2020,  grafik  TPK  hotel  mengalami  penurunan 
yang  signifikan  dengan  nilai terendah  sebesar  15,23% 
sedangkan jumlah omzet menunjukkan fluktuasi mulai 
awal  Januari  2020  namun  cenderung  konstan  sampai 
bulan  November  2020  dengan  nilai  terendah  sebesar 
814 juta rupiah. 

2.  Proyeksi  TPK  hotel  bintang  dan  non  bintang 
menggunakan  metode  NN  untuk  periode  Januari-Juli 
2021  mengalami  penurunan  yang  signifikan  pada 
rentang  20-40%,  sedangkan  proyeksi  TPK    hotel 
bintang dan non bintang menggunakan metode ARIMA 
cenderung konstan pada rentang 30-60%. 

3.  Volatilitas varians jumlah omzet hotel bintang dan non 
bintang  berfluktuasi  secara  signifikan,  juga  di  akhir 
tahun  nampak  volatilitas  varians  jumlah  omzet  baik 
hotel bintang dan non bintang semakin meningkat pada 
bulan November-Desember 2021. 

dengan  metode  ARIMA. 

4.  Metode  Neural  Network  yang  digunakan  untuk 
memproyeksikan TPK hotel menghasilkan nilai akurasi 
yang  lebih  tinggi  atau  nilai  eror  yang  lebih  rendah 
dibandingkan 
Jika 
dibandingkan dengan nilai aktual TPK hotel bintang dan 
non  bintang  pada  bulan  Februari-April  2021,  nilai 
proyeksi TPK menggunakan metode terbaik NN berada 
dalam rentang batas prediksi, sedangkan bulan Januari 
berada  diluar  rentang  batas  proyeksi  karena  terdapat 
kejadian  khusus  yaitu  kebijakan  Pembatasan  Kegiatan 
Masyarakat yang dijalankan pada tanggal 11-25 Januari 
2021. 

Sehingga melalui kesimpulan diatas, peneliti menyarankan 
penelitian lebih lanjut dengan menggunakan  range data  yang 
lebih  luas,  dimana  saat  ini  peneliti  menghadapi  masalah 
keterbatasan  data  scraping  agoda  yang  hanya  tersedia  pada 
periode 1 Januari 2020-31 Desember 2020. Karena data tidak 
dikumpulkan  sendiri  oleh  peneliti,  melainkan  memanfaatkan 
data hasil scraping tim PMS BPS Pusat.  

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

DAFTAR PUSTAKA 
[1]  Haryanto. Tri,  Editorial: Covid-19 Pandemic and International Tourism 
Demand.. Journal of Developing Economics, Vol.5, No.1, pp. 1-5, 2020. 

[2]  Stefan Gössling, Daniel Scott & C. Michael Hall, Pandemics, Tourism and 
Global Change: A Rapid Assessment of Covid-19. Journal of Sustainable 
Tourism, April 2020. 

[3]  BPS,  Tingkat  Penghunian  Kamar  Hotel  2019.  Badan  Pusat  Statistik 

Republik Indonesia, Juni 2020. 

[4]  BPS,  Tingkat  Penghunian  Kamar  Hotel  2012,  Badan  Pusat  Statistik 

Republik Indonesia, 2012. 

Encyclopedia. 

[5]  Wikipedia  contributors.  (2021,  January  14).  RevPar  In  Wikipedia,  The 
Free 
Available: 
https://en.wikipedia.org/w/index.php?title=RevPAR&oldid=1000326207. 
[6]  Montaño, Juan & Palmer, Alfonso & Sesé, Albert & Cajal, Berta,  Using 
the  R-MAPE  Index  as  A  Resistant  Measure  of  Forecast  Accuracy. 
Psicothema. 25. 500-506. 10.7334/psicothema2013.23, 2013. 

[Online], 

[7]  Weatherford, L.R. and Kimes, S.E, A comparison of forecasting methods 
for  hotel  revenue  management,  International  Journal  of  Forecasting 
99(19): 401–415, January 2003. 

[8]  Reinaldo C. Garcia, Javier Contreras, Senior Member, IEEE, Marco van 
Akkeren,  and  João  Batista  C.  Garcia,  A  GARCH  Forecasting  Model  to 
Predict Day-Ahead Electricity Prices. IEEE. Vol. 20, No.2, pp. 867-874, 
May 2005. 

[9]  BPS.  (2021).  Tingkat  Penghunian  Kamar  pada  Hotel  Bintang  2021 
[Online],  Available:  https://www.bps.go.id/indicator/16/122/1/tingkat-
penghunian-kamar-pada-hotel-bintang.html. 

[10] Sekretariat  Kabinet  Republik  Indonesia.  (2021).  Pemerintah  Terapkan 
Kebijakan  Pembatasan  Kegiatan  Masyarakat  pada  11-25  Januari  2021 
[Online],  Available:  https://setkab.go.id/pemerintah-terapkan-kebijakan-
pembatasan-aktivitas-pada-11-25-januari-2021/. 

[11] Jeis  Montesori.  (2021).  [Online],  Kasus  Covid-19  di  Januari  2021 
Available: 

Terburuk 
https://www.beritasatu.com/kesehatan/726469/kasus-covid19-di-januari-
2021-terburuk-sejak-pandemi-landa-indonesia. 

Indonesia. 

Pandemi 

Landa 

Sejak 

[12] Eva  Martin  F.,  Juan  Pedro  Mellinas,  Hotels  That  Most  Rely  on 
Booking.com—Online  Travel  Agencies  (OTAs)  and  Hotel  Distribution 
Channels. Tourism Review, 21 July 2018. 

[13] Atilla As Atilla Aslanargun, Mammadagha Mammadov, Berna Yazici & 
Şenay  Yolaçan,  Comparison  of  ARIMA,  Neural  Networks  and  Hybrid 
Models in Time Series: Tourist Arrival Forecasting, Journal of Statistical 
Computation and Simulation, 77:1, 29-53, 2007. 

[14] Tseng,  F.M.,  Yu,  H.C.  and  Tzeng,  G.H.,  Combining  Neural  Network 
Model  With  Seasonal  Time  Series  ARIMA  Model.  Technological 
Forecasting & Social Change, 69, 71–87, 2002. 

[15] Zhang, G.P., Time-Series Forecasting Using A Hybrid ARIMA and Neural 

Network Model. Neurocomputing, 50, 159–175, 2003. 

 8 / 8 

 
 
 
 
 
 
 
 
 
 
 
"
221709934,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pembangunan Sistem Pendukung Keputusan 
Penerimaan Beasiswa Di Situbondo Dengan Metode 
Simple Additive Weighting Berbasis Web 

Pandu Piwulang (221709934, 4SI1) 
Dosen Pembimbing: Drs. Waris Marsisno M.Stat 

Ringkasan—  Tugas  memilih  urutan  prioritas  penerima 
beasiswa  merupakan  masalah  yang  kompleks.  Ada 
beberapa  pertimbangan  kriteria  sebelum  menentukan 
calon  penerima  yang  berhak  menerima  beasiswa.  Sistem 
Pendukung  Keputusan  dibangun  agar  pemilihan  calon 
penerima  beasiswa  lebih  efektif  dan  efisien.  Penelitian  ini 
dilakukan  untuk  memudahkan  proses  penilaian  dalam 
menentukan  prioritas  penerima  beasiswa.  Metode 
pengambilan keputusan yang digunakan dalam penelitian 
ini  adalah  metode  Simple  Additive  Weighting  (SAW). 
Pengumpulan  data  dilakukan  dengan  cara  melakukan 
wawancara  terhadap  staf  yang  merupakan  tim  penilai, 
mengamati  sistem  sejenis,  serta  menggunakan  data  uji 
untuk  menguji  kesesuaian  proses  perhitungan  yang 
dilakukan  oleh  sistem.  Sistem  dalam  penelitian  ini  sudah 
berhasil  dibangun.  Dari  hasil  pengujian  yang  dilakukan 
terhadap sistem, sistem mampu memberikan rekomendasi 
urutan  prioritas  penerima  beasiswa  kepada  pengguna 
berdasarkan  perhitungan  yang  dilakukan  menggunakan 
metode  SAW.  Fungsi-fungsi  yang  terdapat  dalam  sistem 
sudah berfungsi sebagaimana mestinya. 

Kata  Kunci—Beasiswa,  Sistem  Pendukung  Keputusan,  Fuzzy 

Multi Attribute Decision Making, Simple Additive Weighting. 

minat  belajar,  kurangnya  rasa  keingintahuan,  dan  lingkungan 
yang tidak mendukung untuk memperoleh pendidikan. 

Berdasarkan  publikasi  Situbondo  Dalam  Angka  yang 
Tahun 2020, BPS Situbondo mencatat ada sebanyak 557,9 ribu 
penduduk yang berusia 15 tahun ke atas, namun hanya 40,8 ribu 
penduduk  yang  berhasil  menamatkan  pendidikannya  hingga 
tingkat perguruan tinggi. Di lain sisi, BPS Provinsi Jawa Timur 
juga  mencatat  ada  sebanyak  76,44  ribu  jumlah  penduduk 
miskin  yang 
Ini 
mengindikasikan  bahwa  banyak  lulusan  SMA/SMK/sederajat 
tidak melanjutkan pendidikannya ke perguruan tinggi. 

terdapat  di  Kabupaten  Situbondo. 

II.  TUJUAN PENELITIAN 

Penelitian 

ini  dilakukan  untuk  membangun  sistem 
pendukung  keputusan  berbasis  web  untuk  membantu  proses 
penilaian  dalam  pelaksanaan  penerimaan  beasiswa  di 
Situbondo.  Secara  khusus  penelitian  ini  akan  membangun 
sistem pendukung keputusan yang memiliki fungsi: 
1.  Mengelola  data-data  pendukung  seperti  data  alternatif, 
kriteria,  subkriteria,  maupun  bobot  pada  tiap-tiap  kriteria 
dan subkriteria. 

2.  Melakukan  proses  perhitungan  yang  tepat  dan  sesuai 

dengan metode yang telah ditentukan. 

I.  LATAR BELAKANG 

Memperoleh pendidikan yang layak adalah hak bagi setiap 
Warga Negara Indonesia. Hal tersebut seiring dengan amanah 
tertuang  dalam  UUD  Negara  Republik 
konstitusi  yang 
Indonesia  Tahun  1945  Pasal 31  ayat 1  dan  2.  Pasal  31  UUD 
1945 mengamanatkan bahwa pendidikan merupakan hak bagi 
setiap  warga  negara  tetapi  pendidikan  dasar  merupakan 
kewajiban  yang  harus  diikuti  oleh  setiap  warga  negara  dan 
pemerintah wajib membiayai kegiatan tersebut.  

Dalam UU No. 39 Tahun 1999 Tentang HAM Pasal 12 juga 
menyebutkan tentang hak pendidikan bagi setiap warga negara 
Indonesia.  Pasal  tersebut  mengamanatkan  pula  bahwa  setiap 
orang berhak atas perlindungan bagi pengembangan pribadinya, 
untuk  memperoleh  pendidikan,  mencerdaskan  dirinya,  dan 
meningkatkan  kualitas  hidupnya  agar  menjadi  manusia  yang 
beriman,  bertaqwa,  bertanggung  jawab,  berakhlak  mulia, 
bahagia, dan sejahtera sesuai dengan hak asasi manusia. 

Pendidikan  sangat  berkaitan  erat  dengan  kemiskinan  dan 
saling  mempengaruhi  satu  sama  lain.  Kemiskinan  akan 
mempengaruhi  pendidikan  masyarakat,  di  mana  masyarakat 
miskin sangat rentan tidak mendapatkan pendidikan yang layak. 
Hal ini disebabkan oleh beberapa faktor, misalnya kurangnya 

III. PENELITIAN TERKAIT 

Dalam  penyusunan  skripsi  ini,  penulis  sedikit  banyak 
terinspirasi  dan  mereferensi  dari  penelitian-penelitian 
sebelumnya. Berikut ini penelitian terdahulu yang berhubungan 
dengan skripsi ini antara lain: 

Penelitian  yang  dilakukan  oleh  Rosalia  Hadi  [1],  2018, 
“Penerapan  Metode  Multifactor  Evaluation  Process  Untuk 
Pemilihan  Supplier  Kertas  Pada  Percetakan”.  Ada  5 
faktor/kriteria yang digunakan dalam penelitian tersebut yakni 
(1) Harga dengan bobot 25%, (2) Kualitas bahan dengan bobot 
30%, (3) Keragaman yang ditawarkan dengan bobot 20%, (4) 
Respon supplier dengan bobot 15%, dan (5) Waktu pengiriman 
dengan bobot  10%.  Penelitian  tersebut  menggunakan metode 
Multifactor  Evaluation  Process,  yaitu  total bobot  tiap kriteria 
haruslah  sama  dengan  100.  Penelitian  tersebut  memberikan 
hasil akhir penilaian yang telah di rangkingkan sehingga dapat 
menentukan supplier yang tepat. 

Penelitian lain juga dilakukan oleh Iwan Laengge [2], dkk 
(2016), 
tentang  “Sistem  Pendukung  Keputusan  Dalam 
Menentukan Dosen Pembimbing Skripsi”. Penelitian tersebut 

 1 / 7 

 
 
 
 
 
 
 
menggunakan metode Simple Additive Weighting (SAW). Ada 
4 kriteria yang digunakan yaitu (1) Pendidikan, (2) Fungsional, 
(3)  Kompetensi,  dan  (4)  Kuota  atau  jumlah  bimbingan.  Pada 
setiap  kriteria  juga  terdapat  beberapa  subkriteria  dengan 
pemberian  bobot  masing-masing  yang  berbeda-beda.  Hasil 
yang diperoleh dari penelitian tersebut yaitu hasil perhitungan 
yang tepat antara sistem dan hitung manual dan dapat dijadikan 
tolak  ukur  oleh  perguruan  tinggi  dalam  menentukan  dosen 
pembimbing. 

Fauzan  [3],  dkk  (2017)  melakukan  penelitian  mengenai 
Sistem  Pendukung  Keputusan  Penerimaan  Beasiswa  dengan 
menggunakan  model  optimasi.  Bobot  beberapa  kriteria  yang 
ditentukan seperti Penghasilan Orang Tua, Jumlah Tanggungan, 
Rata-rata nilai raport semester 4-5, Bukti Rekening Listrik, dan 
Bukti Pembayaran PBB, diolah sampai memperoleh nilai akhir 
yang kemudian diurutkan dari nilai terbesar sebagai penentuan 
prioritas  penerima  beasiswa.  Penelitian  ini  membuktikan 
bahwa  dengan  memberikan  bobot  pada  kriteria  yang  telah 
ditentukan,  bisa  memperoleh  daftar  prioritas  mahasiswa 
penerima beasiswa dengan tepat. 

IV.  METODE PENELITIAN 

A.  Fuzzy Multiple Attribute Decision Making 

kemudian 

Menurut  Kusumadewi  dkk  (2006)  “Fuzzy  Multiple 
Attribute  Decision  Making  (FMADM)  adalah  suatu 
metode yang digunakan untuk mencari alternatif optimal 
dari  sejumlah  alternatif  dengan  kriteria  tertentu”[4]. Inti 
dari  FMADM  adalah  menentukan  bobot  untuk  setiap 
atribut, 
proses 
perengkingan  yang  akan  menyelesaikan  alternatif  yang 
sudah  diberikan.  Ada  beberapa  metode  yang  dapat 
digunakan  untuk  menyelesaikan  masalah  FMADM, 
antara lain (Kusumadewi, 2006): 
1. 
2.  Weighting product (WP) 
3.  ELimination  Et  Choix  Traduisant 

Simple Additive Weighting (SAW) 

la  REalité 

dilanjutkan 

dengan 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Berikut adalah langkah-langkah dalam menentukan 
prioritas alternatif menggunakan metode Simple Additive 
Weighting (SAW): 
1.  Menentukan  kriteria-kriteria  yang  menjadi  acuan 
dalam  pengambilan  keputusan,  serta  menentukan 
sifat dari tiap kriteria tersebut (cost atau benefit) 
2.  Menentukan bobot dari tiap-tiap kriteria yang telah 
ditentukan.  Besarnya  bobot  kriteria  terdiri  atas  5 
bilangan fuzzy. 

Gambar 1. Bilangan Fuzzy untuk penentuan bobot 
kriteria 

3.  Menentukan subkriteria dari tiap-tiap kriteria 
4.  Menentukan bobot dari tiap-tiap subkriteria.  
5.  Membuat  normalisasi  matriks  berdasarkan  rumus 

berikut: 

𝑟𝑖𝑗 =

𝑥𝑖𝑗
max 𝑥𝑖𝑗

𝑟𝑖𝑗 =

min 𝑥𝑖𝑗
𝑥𝑖𝑗

 𝑗𝑖𝑘𝑎 𝑗 𝑎𝑑𝑎𝑙𝑎ℎ 𝑎𝑡𝑟𝑖𝑏𝑢𝑡 𝑦𝑎𝑛𝑔 𝑏𝑒𝑟𝑠𝑖𝑓𝑎𝑡 𝑏𝑒𝑛𝑒𝑓𝑖𝑡 

 𝑗𝑖𝑘𝑎 𝑗 𝑎𝑑𝑎𝑙𝑎ℎ 𝑎𝑡𝑟𝑖𝑏𝑢𝑡 𝑦𝑎𝑛𝑔 𝑏𝑒𝑟𝑠𝑖𝑓𝑎𝑡 𝑐𝑜𝑠𝑡 

 : Rating kinerja ternomalisasi 

Dimana: 
𝑟𝑖𝑗 
max 𝑥𝑖𝑗 :  Nilai  maksimum  dari  setiap  baris  dan 
kolom 
max 𝑥𝑖𝑗 : Nilai minimum dari setiap baris dan kolom 
𝑥𝑖𝑗 

  : Baris dan kolom dari matriks 

(ELECTRE) 

4.  Technique  for  Order  Preference  by  Similarity  to 

Ideal Solution (TOPSIS) 

5.  Analytic Hierarchy process (AHP) 

B.  Simple Additive Weighting 

Menurut  Fishburn  dan  MacCrimmon  dalam 
(Munthe,  2013)  mengemukakan  bahwa  Metode  Simple 
Additive  Weighting  (SAW),  atau  sering  juga  dikenal 
dengan  istilah  metode  penjumlahan  terbobot  (Weighted 
Sum  Method)  memiliki  konsep  dasar  yaitu  mencari 
penjumlahan  terbobot  dari  rating  kinerja  pada  setiap 
alternatif pada semua atribut[2]. 

Pada  perhitungan  menggunakan  metode  SAW, 
terdapat dua tipe/sifat kriteria, yakni kriteria yang bersifat 
benefit  (keuntungan),  dan  kriteria  yang  bersifat  cost 
(biaya).  Kriteria  yang  bersifat  benefit,  artinya  semakin 
besar  nilai  pada  kriteria  tersebut,  maka  semakin  baik, 
berlaku  sebaliknya  untuk  kriteria  yang  bersifat  cost. 
Penghitungan  menggunakan  metode  SAW  dapat 
dirumuskan sebagai berikut: 

6.  Mencari  alternatif  terbaik  dengan  penjumlahan 
vector  dari  tiap-tiap  alternatif  dari  normalisasi 
matriks.  Formula  untuk  menentukan  alternatif 
terbaik adalah sebagai berikut: 

𝑛
𝑉𝐼 = ∑ 𝑤𝑗𝑟𝑖𝑗
𝑗=1

Dimana: 
𝑉𝑖  
𝑤𝑖  
𝑟𝑖𝑗  

: Menyatakan preferensi alternatif 
: Menyatakan bobot yang telah ditentukan 
: Normalisasi Matriks 

C.  Metode Pengembangan Sistem 

  Metode penelitian yang digunakan oleh penulis adalah 
System Development Life Cycle (SDLC) Model Waterfall. 
Pada  model  Waterfall  terdapat  5  proses  [5],  yaitu 
Requirements  Analysis,  System  Design,  Implementation, 
Integration  &  Testing,  dan  operation  &  maintenance. 
Susunan dari tahapan tersebut dapat diilustrasikan sesuai 
gambar. 

 2 / 7 

 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

2.  Interview. 

Melakukan interview atau bertanya kepada salah satu 
staff di Dinas Pendidikan Kabupaten Situbondo. Hal 
ini  untuk  mencari  informasi  mengenai  sistem  yang 
akan dikembangkan. 
3.  Mengamati Sistem Sejenis 

 Sistem pendukung keputusan berbasis web bukanlah 
hal  yang  baru  dalam  dunia  IT.  Perkembangannya 
melaju cepat, sehingga banyak model maupun contoh, 
yang  bisa  dijadikan  acuan  dan  dipelajari  dari  sistem 
web  pendukung  keputusan  yang  telah  dibuat  oleh 
programmer  sebelumnya.  Selain  itu  framework  yang 
disediakan  di  internet  memudahkan  penulis  dalam 
membangun sistem. 

4.  Data Dummy 

Untuk  konten  dalam  sistem,  penulis  menggunakan 
data  dummy.  Data  dummy  atau  data  training  sudah 
cukup  mewakili  proses  perhitungan  yang  dilakukan 
dalam sistem. 

Gambar 2. Waterfall Model 

1.  Requirements Analysis 

telah 

diketahui, 

Pada tahapan ini, penulis mencoba mengenali masalah 
yang  ada  di  Subbag  Penyusunan  Program.  Setelah 
masalah 
penulis  melakukan 
pengkajian literatur untuk menemukan masalah yang 
relevan dengan permasalahan yang diperoleh penulis. 
Sebagai  hasil  dari  tahapan  ini,  penulis  melakukan 
pengajuan  untuk  diadakan  penelitian 
terhadap 
masalah tersebut. 

2.  System Design 

E.  Metode Analisis 

Pada tahap ini, System Requirements yang yang telah 
dikumpulkan pada tahap sebelumnya akan dievaluasi 
kembali untuk oleh penulis untuk merumuskan ide-ide 
sementara  yang  dianggap  dapat  menjadi  solusi  dari 
permasalahan yang ada. Ide tersebut akan dituangkan 
dalam  suatu  desain  yang  nantinya  akan  digunakan 
sebagai  pedoman  dalam  merealisasikan  solusi  yang 
telah dirumuskan oleh penulis. 

3.  Implementation 

Tahap  ini  merupakan  tahap  pengembangan  system 
atau penerapan dari hasil desain sementara.  Tahapan 
ini dapat berupa usulan entitas, tipe data, use case serta 
algoritma  maupun 
terhadap 
permasalahan. 

sistem 

solusi 

4.  Integration & Testing 

Tahap  ini  juga  dikenal  sebagai  fase  ""pengujian"",  ini 
telah 
titik  di  mana  produk/sistem  yang 
adalah 
dibangun pada tahap sebelumnya diuji untuk mencari 
kesalahan. 

5.  Operation & Maintenance 

Tahap terakhir adalah maintenance. Ketika ada sistem 
baru  yang  diterapkan,  seringkali  sistem  tersebut 
membutuhkan  pemeliharaan  yang  berkelanjutan.  Ini 
untuk  memastikan  perangkat  lunak  beroperasi  pada 
tingkat kinerja yang optimal. 

D.  Metode Pengumpulan Data 

  Metode  Penelitian  yang  akan  dilakukan  dengan 
beberapa cara, diantaranya : 
1.  Studi Literatur. 

 Mencari  beberapa  referensi  dan  penelitian  terkait 
supaya  dapat  memberikan  gambaran  bagaimana 
nantinya  pembuatan  sistem  akan  dilakukan  dan 
bagaimana sistem yang baik seharusnya dirancang dan 
dikembangkan.  Sumber  utama  study  literatur  yang 
digunakan  pada  penelitian  ini  adalah  jurnal-jurnal 
yang diunduh melalui internet. 

Proses  analisis  dilakukan  melalui  pengamatan  detail 
objek  penelitian,  menguraikan  komponen-komponen 
yang  ada,  serta  memahami  keterkaitan  antarkomponen 
penelitian. Metode analisis yang digunakan oleh penulis 
meliputi  analisis  permasalahan,  dan  analisis  kebutuhan, 
Masing-masing metode analisis tersebut memiliki tujuan 
dan pendekatan yang berbeda.  
1.  Analisis Permasalahan 

Dalam  melakukan  analisis  permasalahan, 
penulis menggunakan diagram fishbone. Diagram ini 
berfungsi  untuk  mengetahui  penyebab 
suatu 
permasalahan. Berikut ilustrasi dari diagram fishbone 
dalam penelitian ini. 

Gambar 3. Diagram Fishbone 

2.  Analisis Kebutuhan 

Berdasarkan  analisis  permasalahan  yang 
telah  dipaparkan  di  atas,  dibutuhkan  analisis  untuk 
solusi 
mengetahui 
yang  mampu  mengatasi 
tersebut  yang  berdasarkan  kepada 
permasalahan 
kebutuhan  sistem  yang 
terdiri  dari  kebutuhan 
fungsional dan kebutuhan non-fungsional. 
a.  Kebutuhan fungsional 

 3 / 7 

 
 
 
 
 
 
 
 
 
 
Kebutuhan  fungsional  dari  sistem  penilaian 
untuk  tim  juri  pada  proses  penerimaan  beasiswa 
yaitu: 
1.  Sistem  dapat  mengelola  data  pendaftar 
beasiswa  dan  dapat  melakukan  penghitungan 
secara  otomatis  dengan  metode  yang  telah 
ditentukan. 

2.  Sistem  dapat  membuat  pelaporan  hasil 

penerima beasiswa. 

b.  Kebutuhan non-fungsional 

Kebutuhan  non-fungsional  diidentifikasi 
dengan  menggunakan  metode  PIECES.  Metode 
PIECES akan mengidentifikasi sistem berjalan dan 
sistem yang diusulkan berdasarkan 6 aspek yaitu, 
performance, 
information,  economy,  control, 
efficiency,  dan  service.  Berikut  hasil  analisis 
PIECES  dari 
sistem  penilaian  di  Subbag 
Penyusunan Program 

Tabel 1. Tabel Analisis PIECES 

Aspek 

Sistem Lama 

Sistem Baru 

(1) 
Perfor
mance 

(2) 
Proses penilaian 
dilakukan secara 
diskusi sehingga 
hasil penilaian 
kurang objektif 

Inform
ation 

Kualitas informasi 
kurang akurat 

Econo
my 

Membutuhkan 
biaya untuk 
pencetakan 
dokumen dan 
biaya lainnya 
yang mendukung 
dalam proses 
penilaian. 

Control  Data masih 

disimpan dalam 
komputer pribadi. 

Efficien
cy 

Proses penilaian 
lambat dilakukan 
secara tradisional 

(3) 
Proses penilaian 
dilakukan oelh 
system sehingga 
lebih objektif 
sesuai dengan 
kriteria-kriteria 
yang telah 
ditentukan. 
Kualitas 
informasi 
(dalam hal ini 
adalah hasil 
perangkingan) 
lebih akurat 
Sistem telah 
mengkonversi 
dokumen cetak 
dalam bentuk 
dokumen digital 
sehingga 
mengurangi 
biaya dalam 
pencetakan 
dokumen 
Kendali 
terhadap data 
pendaftar lebih 
baik karena 
disimpan dalam 
database 
Proses penilaian 
lebih cepat 
karena 
dilakukan 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Service  Pelayanan kepada 

para pendaftar 
kurang 
memuaskan 
karena 
pengumuman 
hasil penerima 
yang memakan 
waktu lebih lama 

otomatis oleh 
system 
Pelayanan 
kepada para 
pendaftar lebih 
memuaskan 
karena 
pengumuman 
hasil penerima 
bisa diumumkan 
lebih cepat. 

Tabel  di  atas  menunjukkan  hasil  analisis 
dengan  melakukan  perbandingan  masalah  pada 
sistem  lama  dan  keadaan  sistem  baru  yang 
dibutuhkan  jika  dipandang  dari  aspek  yang  sama 
dengan permasalahan yang muncul. 

V.  KERANGKA PIKIR 

tersebut  dirumuskan 

Kerangka  piker  pada  penelitian 

ini  dimulai  dari 
penentuan rumusan masalah yang ada. Lalu, dari rumusan 
masalah 
solusi  untuk 
menyelesaikan  permasalahan 
tersebut.  Metode-metode 
penelitian  selanjutnya  ditentukan  untuk  menyelesaikan 
rumusan  masalah  tersebut.  Lebih  lengkapnya,  kerangka 
piker bisa dilihat pada Gambar 4 di bawah ini. 

suatu 

Gambar 4. Kerangka Pikir 

. 

VI. HASIL DAN PEMBAHASAN 

A.  Hasil perhitungan dengan data uji 

Tabel 2. Alternatif Penerima Beasiswa 

Alt. 

Kategori 
Keluarga 

Ani 

Miskin 

Ranking 
Potensi 
Akd. 
4-6 

Tingkat 
Prestasi 

Akred. 
Prodi 

Nasional 

A 

 4 / 7 

 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Budi 

Candra 

Dewi 

Erfan 

Fendi 

Rentan 
Miskin 
Sangat 
Miskin 
Hampir 
Miskin 
Miskin 
Sangat 
Miskin 

1-3 

Nasional 

7-10 

Provinsi 

7-10 

7-10 

7-10 

Nasional 

Kab. 

Kab. 

C 

A 

B 

C 

A 

Langkah  Selanjutnya  yaitu  menentukan  kriteria-
kriteria yang menjadi acuan dalam pengambilan keputusan, 
serta menentukan sifat dari tiap kriteria tersebut (cost atau 
benefit).  Lalu  menentukan  bobot  dari  masing-masing 
kriteria. 

Selanjutnya  memberikan  nilai  kriteria  pada  setiap 
alternatif  yang  ada  ke  dalam  tabel  matriks  keputusan  dan 
membuat matriks normalisasinya. 

Tabel 7. Tabel Matriks Keputusan 

Alt. 

Ani 
Budi 
Candra 
Dewi 
Erfan 
Fendi 

Kategori 
Keluarga 

0.5 
1 
0.25 
0.75 
0.5 
0.25 

Ranking 
Potensi 
Akd. 
0.5 
0.25 
0.75 
0.75 
0.75 
0.75 

Tingkat 
Prestasi 

Akred. 
Prodi 

1 
1 
0.75 
1 
0.25 
0.25 

0.25 
1 
0.25 
0.75 
0.25 
0.25 

Tabel 3. Tabel Nilai Bobot Setiap Kriteria 

Tabel 8. Tabel Normalisasi Matriks Keputusan 

Kriteria 

C1 
C2 
C3 
C4 

Kategori Keluarga (Cost) 
Ranking Potensi Akademik (Cost) 
Tingkat Prestasi (Benefit) 
Akreditasi Prodi (Cost) 

Bobot 
0.75 
0.75 
1.00 
0.50 

Selanjutnya adalah menentukan bobot subkriteria dari 

tiap-tiap kriteria yang telah ditentukan. 

Tabel 4. Tabel Nilai Bobot Subkriteria pada Kriteria 
Pertama 

1 
2 
3 
4 

C1: Kategori Keluarga 

Sangat Miskin 
Miskin 
Hampir Miskin 
Rentan Miskin 

Bobot 
0.25 
0.50 
0.75 
1.00 

Tabel 5. Tabel Nilai Bobot Subkriteria pada Kriteria Kedua 

Alt. 

Ani 
Budi 
Candra 
Dewi 
Erfan 
Fendi 

Kategori 
Keluarga 

0.5 
0.25 
1 
0.333 
0.5 
1 

Ranking 
Potensi 
Akd. 
0.5 
1 
0.333 
0.333 
0.333 
0.333 

Tingkat 
Prestasi 

Akred. 
Prodi 

1 
1 
0.75 
1 
0.25 
0.25 

1 
0.25 
1 
0.333 
0.25 
1 

Selanjutnya  membuat  perangkingan  sesuai  dengan 
matriks yang sudah ternormalisasi dengan cara mengalikan 
nilai-nilai  dalam  matriks  dengan  bobot  tiap  kriteria,  lalu 
menjumlahkannya. 

Ani = (0.75)(0.5) + (0.75)(0.5) + (1)(1) + (0.5)(1) = 2.25 

Candra = (0.75)(1) + (0.75)(0.333) + (1)(0.75) + (0.5)(1) = 
2.24975 

C2: Ranking Potensi Akademik 

1 
2 
3 

1-3 
4-6 
7-10 

Bobot 
0.25 
0.50 
0.75 

Budi = (0.75)(0.25) + (0.75)(1) + (1)(1) + (0.5)(0.25) = 
2.0625 

Fendi = (0.75)(1) + (0.75)(0,333) + (1)(0.25) + (0.5)(1) = 
1.74975 

Tabel 6. Tabel Nilai Bobot Subkriteria pada Kriteria Ketiga 

C3: Tingkat Prestasi 

Kabupaten 
Provinsi 
Nasional 

Bobot 
0.25 
0.75 
1.00 

Tabel 7. Tabel Nilai Bobot Subkriteria pada Kriteria 
Keempat 

C4: Akreditasi Prodi 

A 
B 
C 

Bobot 
0.25 
0.75 
1.00 

1 
2 
3 

1 
2 
3 

Dewi = (0.75)(0.333) + (0.75)(0.333) + (1)(1) + 
(0.5)(0,333) = 1.666 

Erfan = (0.75)(0.5) + (0.75)(0.333) + (1)(0.25) + 
(0.5)(0.25) = 0.99975 

B.  Penerapan pada Sistem 

Pada pembangunan sistem, sistem akan dibagi menjadi 

beberapa page/halaman sebagai berikut : 
1.  Halaman login 

Halaman  login  adalah  halaman  yang  muncul  apabila 
domain di jalankan. Pengguna diharuskan login terlebih 
dahulu  sebelum  masuk  ke  dalam  sistem.  Tampilan 
halaman login bisa dilihat pada Gambar 5 berikut. 

 5 / 7 

 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 5. Halaman Login 

5.  Halaman bobot 

Gambar 8. Halaman Sub Kriteria 

2.  Halaman alternatif 

Setelah  melalui  proses  login,  pengguna  akan  diarahkan 
menuju halaman alternatif. Halaman ini berisi daftar calon 
penerima beasiswa. Halaman ini juga diberikan fitur ubah, 
tambah,  dan  hapus  daftar  alternatif.  Tampilan  antarmuka 
halaman alternatif bisa dilihat pada Gambar 6 berikut. 

Halaman  bobot  berisi  bobot  dari  kriteria  yang  telah 
ditentukan.  Untuk  proses  tambah  bobot,  diawali  dengan 
proses  pilih  tahun  penerimaan  terlebih  dahulu.  Bila  satu 
kriteria tidak digunakan pada satu tahun penerimaan, maka 
bobotnya  bisa  diisi  nol.  Untuk  memudahkan  admin, 
halaman ini juga dilengkapi fitur ubah, tambah, dan hapus 
pada  tahun  penerimaannya.  Tampilan  antarmuka halaman 
bobot bisa dilihat pada Gambar 9 berikut. 

Gambar 6. Halaman Alternatif 

3.  Halaman kriteria 

Gambar 9. Halaman Bobot 

6.  Halaman penilaian 

Halaman  penilaian  berfungsi  untuk  memasukkan  data 
alternatif ke dalam proses penilaian. Proses dimulai dengan 
memilih tahun pendaftaran. Proses ubah, tambah, dan hapus 
juga  tersedia  pada  halaman  ini  untuk  lebih  memudahkan. 
Tampilan  antarmuka  halaman  penilaian  bisa  dilihat  pada 
Gambar 10 berikut. 

Gambar 7. Halaman Kriteria 

Halaman  ini  berisi  kriteria  yang  akan  digunakan  dalam 
proses seleksi. Pada halaman ini terdapat fitur ubah, tambah, 
dan  hapus  kriteria  untuk  memudahkan  pengguna  dalam 
mengelola  data  kriteria.  Tampilan  antarmuka  halaman 
kriteria bisa dilihat pada Gambar 7 di atas. 

4.  Halaman subkriteria 

Halaman subkriteria berisi subkriteria dari tiap-tiap kriteria 
yang  telah  ditentukan  beserta  bobotnya  masing-masing. 
Pada  halaman  ini  juga  tersedia  proses  ubah,  tambah,  dan 
hapus untuk lebih mudah dalam pengelolaan data. Tampilan 
antarmuka halaman subkriteria bisa dilihat pada Gambar 8 
berikut. 

Gambar 10. Halaman Penilaian 

7.  Halaman hasil 

Halaman ini berisi hasil perhitungan menggunakan metode 
SAW. Untuk mengetahui hasil perhitungan, terlebih dahulu 
admin  memilih  tahun  penerimaan.  Kemudian  sistem  akan 
otomatis menghitung poin yang didapatkan calon penerima 

 6 / 7 

 
 
 
 
 
 
 
 
yang  mendaftar pada  tahun  tersebut.  Tampilan  antarmuka 
halaman hasil bisa dilihat pada Gambar 11 berikut. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 11. Halaman Hasil 

VII. 

KESIMPULAN 

Berdasarkan  pembahasan  hasil  penelitian  pada  bab 
sebelumnya,  kesimpulan  dari  penelitian  ini  adalah  sebagai 
berikut. 
1.  Sistem  pendukung  keputusan  berbasis  web  dengan 
menggunakan metode SAW telah berhasil dikembangkan. 
Fungsi-fungsi  untuk  mengelola  data  seperti  menambah, 
menghapus,  maupun  mengubah  data  alternatif,  kriteria, 
subkriteria, dan bobot berfungsi dengan baik. 

2.  Hasil perhitungan metode SAW yang dilakukan oleh sistem 

telah sesuai dengan hasil perhitungan manual. 

DAFTAR PUSTAKA 
[1] Hadi, R. 2018. “Penerapan Metode Multifactor Evaluation 
Process Untuk Pemilihan Supplier Kertas Pada Percetakan”. 
Prosiding SINTAK 2018: 233-238. 

[2] Langgae,  I.,  dkk.  2016.  Sistem  Pendukung  Keputusan 
Dalam Menentukan Dosen Pembimbing Skripsi. E-journal 
Teknik Informatika, Vol. 9. 

[3] Fauzan,  R.,  dkk.  2017.  Sistem  Pendukung  Keputusan 
Penerimaan  Beasiswa  Bidik  Misi  di  POLIBAN  dengan 
Metode SAW Berbasis Web. Vol. 2. 

[4] Kusumadewi, S., Hartati, S., Harjoko, A. Dan Wardoyo, R., 
2006,  Fuzzy  Multi  Attribute  Decision  Making  (Fuzzy 
MADM), Graha Ilmu, Yogyakarta 

[5] Sommerville,  Ian.  2011.  Software  Engineering  (Rekayasa 

Perangkat. Lunak). Jakarta: Erlangga. 

 7 / 7 

 
 
 
 
 
"
221709929,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Rancang Bangun Knowledge Management System
Berbasis Web di Subdirektorat Pengembangan
Kerangka Sampel BPS

Nuzul Izzati Arifin (221709929, 4SD1)

Dosen Pembimbing: Yunarso Anang S, Ph.D

dan

pengembangan

Ringkasan— Subdirektorat Pengembangan Kerangka
Sampel (subdit PKS) memiliki tugas dan fungsi untuk
pemeliharaan
melaksanakan
kerangka sampel bidang statistik sosial, produksi, serta
distribusi dan jasa. Dalam menjalankan tugas tersebut,
subdit PKS mengalami permasalahan baik dari proses
kegiatan survei, dokumen mengenai survei, pelaporan
hasil survei, maupun proses pengajuan pertanyaan. Hal
ini disebabkan karena tidak adanya media untuk
mendokumentasikan setiap kegiatan di subdit PKS.
Maka dari itu, untuk mengatasi permasalahan tersebut,
knowledge
peneliti merancang
management system (KMS) berbasis web di subdit PKS.
Sistem tersebut dibangun menggunakan metode FAST.
Sistem yang telah dibangun akan dievaluasi dengan 3
(tiga) metode yakni black box testing, uji SUS, dan uji
QUIS. Berdasarkan hasil uji evaluasi sistem, dapat
disimpulkan bahwa sistem yang telah dibangun sudah
layak digunakan karena fungsi-fungsi di dalam sistem
sudah berjalan sesuai dengan kebutuhan pengguna.

dan membangun

Kata Kunci— KMS, PKS, Sistem, Web

I. LATAR BELAKANG

Berdasarkan Peraturan Kepala Badan Pusat Statistik
(PERKA BPS) Nomor 1 Tahun 2009 Pasal 100, Subdirektorat
Pengembangan Kerangka Sampel
(Subdit PKS) memiliki
tugas dan fungsi untuk melaksanakan pengembangan dan
pemeliharaan kerangka sampel survei bidang statistik sosial,
produksi, dan distribusi dan jasa. Subdit PKS menyusun dan
melakukan pengelolaan kerangka induk (master
frame),
kerangka
survei, melakukan
(sampling
pemilihan dan pengelolaan sampel.

sampel

frame)

Setiap tahun Subdit PKS menangani beraneka ragam survei.
Survei yang dilakukan menggunakan metode, kerangka
sampel, dan treatment yang berbeda-beda. Hal
tersebut
mengakibatkan pengetahuan tentang pengelolaan frame dan
sampel hanya diketahui oleh tiap-tiap penanggung jawab
survei. Keterbatasan pengetahuan ini sangat berpengaruh
dalam penyelesaian masalah yang ada karena informasi terkait
survei akan sulit dicari jika penanggung jawab tidak berada di
tempat dikarenakan alasan seperti cuti, pindah, atau sekolah.
sistem yang menampilkan
Selain
rangkuman info-info tentang semua survei dan aturan-aturan
yang
sehingga
survei
memperlambat penyelesaian jika terdapat masalah.

ketika mengerjakan

belum adanya

berlaku

itu,

Pengetahuan pegawai

survei baik siapa yang
terkait
bertanggung jawab maupun bagaimana survei tersebut pun
internal
masih kurang. Jika muncul pertanyaan baik dari
maupun eksternal BPS terkait survei, tetapi penanggung jawab
survei sedang tidak di tempat maka proses penyelesaiannya
melalui sosial media ( jarak jauh ) yang mengakibatkan hasil
yang didapat tidak maksimal dan tidak tertampung di satu
wadah. Keterbatasan daya ingat pegawai pun berbeda-beda.
Dengan adanya keterbatasan ini menimbulkan terjadinya
perulangan pertanyaan yang sama karena tidak adanya wadah
yang dapat menampung pertanyaan-pertanyaan yang telah
terjawab sebelumnya. Menurut pegawai di subdit PKS, file-
file terkait survei baik dokumen pendukung maupun laporan
mengenai survei di simpan di dalam suatu kolaborasi yakni
Laci BPS. Penggunaan Laci BPS tersebut sudah baik, tapi
dirasa kurang efektif dalam sharing knowledge dan jika suatu
saat file dibutuhkan kembali, maka proses menemukan file
memerlukan waktu banyak apabila jumlah file tersebut dalam
jumlah yang banyak

Berdasarkan permasalahan tersebut, Subdit PKS menyadari
pentingnya pengelolaan knowledge untuk mengatasi masalah
pendokumentasian kegiatan di Subdit PKS dan juga dapat
meningkatkan kualitas kerja pegawai. Maka dibutuhkanlah
suatu sistem Knowledge Management System (KMS) untuk
fungsi-fungsi
mengkombinasikan
knowledge yang ada dalam suatu dokumentasi subdit PKS.

dan mengintegrasikan

bagi

perusahaan, menstimulasi

Knowledge Management System (KMS) adalah sistem yang
diciptakan untuk memfasilitasi penangkapan, penyimpanan,
pencarian, transfer dan penggunaan kembali suatu informasi
atau dokumentasi. KMS berfungsi untuk menyediakan dasar
nilai
dalam kegiatan
manajemen agar fokus pada hal-hal yang penting. Selain itu,
KMS menjadikan kegiatan yang ada di subdit ini menjadi
sebuah pengetahuan bagi pegawai karena KMS ini dapat
melakukan
dapat
edit
menampilkan dan mencari data serta mampu menyimpan data
yang suatu saat dibutuhkan kembali. Selain itu, tujuan dari
KMS ini agar knowledge yang dimiliki setiap pegawai tidak
hilang begitu saja dan juga dapat berbagi kepada pegawai lain,
serta
dan
mendistribusikan permasalahan, maupun knowledge yang
tidak terjadi
masih tersimpan dalam setiap pegawai agar
pengulangan kesalahan-kesalahan yang pernah dilakukan.

dalam mendiskusikan

terciptanya

sarana

hapus

input

data,

data,

data,

1 / 8

II. TUJUAN PENELITIAN

a. Awareness of Problem

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Tujuan umum dari penelitian ini adalah mengatasi
permasalahan yang ada di subdit PKS dengan merancang dan
membangun Knowledge Management System berbasis web
untuk Subdirektorat Pengembangan Kerangka Sampel.
Sedangkan,
tujuan khusus dalam pembangunan sistem ini
adalah sebagai berikut.

1. Membangun sistem yang dapat menjadi sarana
berbagi informasi dan dapat mendokumentasikan
setiap proses bisnis di Subdit PKS.

2. Membangun sistem yang dapat menyediakan
solusi dari permasalahan yang sering ditanyakan
(FAQ) oleh pihak terkait dengan berbagai topik.
Sistem dapat mendokumentasikan file melalui
fasilitas upload dan download.

3.

III. PENELITIAN TERKAIT

Penelitian terkait

terhadap permasalahan yang dihadapi,
yakni skripsi Conni Setyorini (2019) mahasiswa Politeknik
Statistika STIS angkatan 57. Penelitian tersebut merancang
dan melakukan implementasi knowledge management pada
Subdirektorat Layanan dan Promosi Statistik BPS[1]. Saya
menjadikan skripsi ini sebagai referensi dalam evaluasi sistem
karena pada skripsi ini, Conni menggunakan 3 (tiga) jenis
metode evaluasi, yakni blackbox testing, SUS (System
Usability Scale), dan QUIS (Questionnaire for User Interface
Satisfaction).

Kemudian Yance Sonatha dkk (2018) melakukan
penelitian terkait dengan knowledge management system.
Penelitian Yance dkk mengarah pada pembangunan aplikasi
Knowledge Management berbasis web untuk PT Gamatechno
Indonesia [2]. Saya menjadikan jurnal
ini sebagai referensi
bagaimana manfaat dan fungsi dari Knowledge Management
System (KMS) karena pada jurnal
ini menyatakan bahwa
aplikasi KMS dapat melakukan input data, edit data, hapus
data, dapat menampilkan dan mencari data serta mampu
menyimpan data-data yang suatu saat dibutuhkan kembali.
Aplikasi ini digunakan guna mengatasi masalah masalah yang
sering terjadi di PT. Gamatechno Indonesia.

rancang

penelitian

Selain itu, Ani Oktarini Sari dan Elan Nuari
bangun
terkait

(2017)
melakukan
sistem
persediaan barang berbasis web dengan metode FAST
(Framework for the Application System Thinking) [3]. Saya
menjadikan jurnal ini sebagai referensi dalam pengembangan
sistem karena pada penelitian ini
tertulis bahwa model
pengembangan sistem yang digunakan adalah Metode FAST
(Framework for the Application System Thinking) terdiri dari
fase-fase Scope Definition, Problem Analysis, Requirements
Analysis, Logical Design, Physical Design dan Construction
& Testing

IV. METODE PENELITIAN

1. Metode Penelitian

Metode penelitian yang digunakan oleh peneliti adalah
design science research yang berpedoman pada design
science research model oleh Takeda (1990)[4]. Pada model
terdapat 5 subproses sebagai berikut.

Pada tahapan ini, peneliti mencoba mengenali masalah
yang ada di Subdirektorat Pengembangan Kerangka
Sampel. Setelah masalah telah diketahui, peneliti
melakukan pengkajian literatur untuk menemukan
masalah yang relevan dengan permasalahan yang
diperoleh peneliti.

b. Suggestion

Pada
tahapan ini, peneliti merumuskan ide-ide
sementara yang dianggap dapat menjadi solusi dari
permasalahan yang ada. Ide tersebut akan dituangkan
dalam suatu desain yang nantinya akan digunakan
sebagai pedoman dalam merealisasikan solusi yang
telah dirumuskan oleh peneliti.

development

c. Development
ini
Tahapan
merupakan penerapan dari hasil desain sementara.
Tahapan ini, peneliti memulai untuk membangun
sistem ke dalam algoritma pemrograman sebagai
solusi terhadap permasalahan.

pengembangan

atau

sangat dibutuhkan sebagai

d. Evaluation
Evaluasi
alat untuk
mengukur performa dari hasil pengembangan yang
telah dilakukan. Pada tahap ini, peneliti melakukan
analisis apakah sistem yang telah dibangun sesuai
dengan tujuan diadakan penelitian ini.

e. Conclusion

Conclusion merupakan tahapan akhir, dimana tahapan
ini merupakan kesimpulan dari hasil yang telah
dicapai.

2. Metode Pengumpulan Data

Pada penelitian ini, metode pengumpulan datanya
melalui 3 cara, yakni wawancara dengan subject matter secara
virtual untuk memperoleh gambaran umum permasalahan
yang ada untuk dianalisis bagaimana solusinya, studi literatur
untuk mencari permasalahan yang relevan dengan penelitian
yang akan dilakukan, dan kuesioner untuk evaluasi sistem
sehingga
perbaikan/
pengembangan sistem.

dijadikan

bahan

untuk

dapat

3. Metode Pembangunan Sistem

dan

strategi.

Berdasarkan Whitten,

Metode pembangunan sistem yang digunakan oleh
peneliti pada penelitian ini adalah metode FAST (Framework
for the Application of System Thinking). FAST adalah sebuah
jenis
kerangka kerja yang cukup fleksibel untuk berbagai
proyek
FAST
dikembangkan sebagai gabungan dari praktik-praktik terbaik
yang telah ditemui dalam banyak referensi komersial dari
metodologi [5]. FAST terdiri dari beberapa tahapan, yakni
Scope Definition, Problem Analysis, Requirement Analysis,
Logical Design, Physical Design, dan Construction & Testing .
Berikut penjelasan dari setiap tahapan FAST.
1.

Scope Definition
Tahap ini merupakan langkah awal dalam proses
perancangan sistem informasi yang mendefinisikan
ruang lingkup dari sistem informasi.

2 / 8

2.

3.

4.

5.

6.

dilakukan

perancangan

Problem Analysis
Analisa masalah
untuk mendefinisikan
lingkup dan masalah dalam pengembangan sistem
informasi
Requirement Analysis
Analisa kebutuhan adalah menentukan kebutuhan sistem
apa saja yang dibutuhkan pada sistem informasi, yaitu
kebutuhan pengguna dan kebutuhan sistem.
Logical Design
Metode
yang menggunakan metode
perancangan berorientasi objek dengan menggunakan
UML (Unifed Modelling Language) sebagai alat bantu
perancangan.
Physical Design
Merupakan tahapan menerjemahkan logical design ke
dalam bentuk fisik suatu aplikasi meliputi perancangan
user interface dan detail design.
Contruction & Testing
untuk
Tahapan
ini
rancangan sistem usulan
mengimplementasikan hasil
yang dibuat pada tahapan physical design. Kegiatan ini
mengacu pada pembangunan sistem. Sistem yang
berhasil dibangun akan dilakukan evaluasi yang menjadi
acuan dalam pengembangan dan perbaikan sistem.

merupakan

tahapan

1.

2.

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

VI. HASIL DAN PEMBAHASAN

belum optimalnya

Scope Definition
Pada tahapan ini dilakukan pengumpulan informasi
untuk mendefinisikan masalah dan tujuan pembangunan
sistem serta menetapkan ruang lingkup batasan dari
sistem tersebut. Di mana permasalahan yang terjadi
knowledge
adalah
management terkait pekerjaan yang ada di Subdit PKS
proses
BPS.
ini mencakup
penelitian
perancangan sebuah knowledge management
system
yang mampu memfasilitasi dan mendokumentasikan
seluruh proses knowledge management dari pegawai.
Proses knowledge management merujuk pada kegiatan
membuat, menyimpan, menyebarkan, dan menerapkan
pengetahuan yang berhubungan dengan pekerjaan.

Sehingga

kegiatan

Problem Analysis
Dalam melakukan

peneliti
analisis
menggunakan diagram fishbone. Diagram ini berfungsi untuk
ilustrasi
mengetahui penyebab suatu permasalahan. Berikut
dari diagram fishbone dalam penelitian ini.

permasalahan,

V. KERANGKA PIKIR

Penelitian dilakukan dengan menganalisis permasalahan
yang ada di Subdit PKS. Penjabaran permasalahan tersebut
dijelaskan oleh pegawai BPS di Subdit PKS sebagai subject
matter dalam forum diskusi virtual. Kemudian peneliti
melakukan studi
literatur untuk menemukan kasus serupa
yang dapat dijadikan gagasan solusi permasalahan. Lalu,
peneliti memulai proses perancangan dan pembangunan
sistem sesuai dengan hasil analisis kebutuhan. Kerangka pikir
dalam penelitian ini dapat diilustrasikan sebagai berikut.

Gambar 2. Diagram fishbone

3.

Requirement Analysis
Pada tahapan ini peneliti menentukan dan menganalisis
kebutuhan sistem pada pembangunan sistem informasi yaitu
kebutuhan pengguna dan kebutuhan sistem.

A. Analisis Kebutuhan Pengguna

Pengguna dalam sistem ini terdiri dari 5 aktor, yakni:

1) Admin

Admin memiliki peran untuk mengelola user dan
mengelola master kategori.

2) Koordinator (Kassubag)
untuk memberi
Koordinator memiliki
penugasan
pertanyaan,
rilis
mengunggah/ mengunduh laporan dan seluruh
kategori dokumen.
3) Supervisor (Kasie)

survei,

solusi

peran

peran

Supervisor memiliki
untuk membuat
Assignment Survey, menjawab dan menyetujui
solusi pertanyaan, mengunggah/mengunduh laporan
dan seluruh kategori dokumen.

4) Operator/PIC (Staff)

Gambar 1. Kerangka Pikir Penelitian

3 / 8

Operator memiliki peran untuk menjawab dan
mengajukan pertanyaan, mengunggah/mengunduh
laporan dan seluruh kategori dokumen.

5) Viewer (Eksternal PKS)

Viewer hanya memiliki peran untuk mengajukan
pertanyaan, melihat dan mengunduh sebagian
dokumen yakni SOP, Juknis, dan Survei.

B. Analisis Kebutuhan Sistem

1) Kebutuhan Sistem untuk Admin

Admin dapat mengakses menu pengelola user dan
master
dapat menghapus
termasuk
dokumen yang telah diunggah.

kategori

2) Kebutuhan Sistem untuk Koordinator (Kassubag)

Koordinator dapat mengakses menu Assignment
Survei untuk memberi penugasan survei, menu
solusi pertanyaan, menu
FAQ untuk merilis
Laporan
untuk mengunggah/
mengunduh laporan dan seluruh kategori dokumen.

dan Dokumen

3) Kebutuhan Sistem untuk Supervisor (Kasie)

Supervisor dapat mengakses menu Assignment
Survei untuk memberi assign survei, menu FAQ
untuk menjawab dan menyetujui solusi pertanyaan,
dan mengunggah/ mengunduh laporan dan seluruh
kategori dokumen.

4) Kebutuhan Sistem untuk Operator/PIC (Staff)

Operator dapat mengakses menu FAQ untuk
menjawab
dan
mengunggah/ mengunduh laporan dan seluruh
kategori dokumen.

dan mengajukan

pertanyaan,

5) Kebutuhan Sistem untuk Viewer (Eksternal PKS)

Viewer hanya dapat mengakses menu FAQ untuk
mengajukan pertanyaan, dan melihat/mengunduh
sebagian dokumen yakni SOP, Juknis, dan Survei.

4.
Logical Design
A. Use Case Diagram

Use Case Diagram berfungsi untuk mengetahui aktor
dan peran aktor dalam sistem. Setiap aktor memiliki peran dan
wewenang yang berbeda-beda. Berikut ilustrasi dari use case
diagram.

Gambar 3. Use Case Diagram

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

B.

Proses Bisnis Sistem Usulan
Peneliti merancang proses bisnis sistem usulan untuk
mengatasi permasalahan yang ada. Rancangan proses bisnis
sistem usulan
dengan mempertimbangkan
permasalahan yang terjadi pada proses bisnis yang sedang
berjalan. Pada rancangan proses bisnis usulan, peneliti
membedakan user menjadi 5 (lima), yakni admin, koordinator,
supervisor, operator, dan viewer. Berikut rancangan proses
bisnis sistem usulan.

dibuat

ini

Gambar 4. Rancangan Proses Bisnis Sistem Usulan

Pada rancangan proses bisnis sistem usulan ini, proses
bisnis yang ada di subdit PKS tidak berubah dengan proses
bisnis yang sedang berjalan, hanya saja dengan adanya
penelitian ini, semua proses bisnis tersebut tersimpan di dalam
suatu sistem. Dapat dilihat bahwa untuk mengakses sistem
tersebut, semua user melakukan login terlebih dahulu. Saat
melakukan login, user memasukkan email dan password, lalu
tersebut
sistem akan melakukan verifikasi apakah user
terdaftar di basis data sistem. Jika verifikasi tersebut gagal,
maka user akan dikembalikan ke halaman login.
Jika
verifikasi tersebut berhasil, maka user akan masuk ke halaman
dashboard. Setelah itu, setiap user dapat melakukan kegiatan
sesuai dengan kebutuhannya.

Kegiatan survei di awali dari koordinator yang mendapat
mandat dari direktur untuk melakukan survei, lalu koordinator
memberi penugasan kepada supervisor. Setelah penugasan
diterima supervisor, supervisor membuat assignment survei
dengan memilih operator yang hendak ditugaskan. Setelah itu
operator menjalankan survei.
Jika muncul pertanyaan,
operator dapat mengajukan pertanyaan dan dapat dijawab dan
disetujui oleh supervisor, lalu dirilis oleh koordinator agar
pertanyaan dan solusi pertanyaan tersebut dapat dilihat oleh
seluruh pegawai BPS.
Jika dalam pelakasanaan survei
membutuhkan dokumen, maka dokumen tersebut dapat di
download melalui sistem. Seluruh kegiatan ini disimpan di
dalam basis data sistem sehingga mudah untuk mencari
informasi mengenai kegiatan di subdit PKS jika suatu saat
dibutuhkan kembali.

4 / 8

Physical Design

5.
A. Rancangan Arsitektur Sistem Usulan

arsitektur

Rancangan

sistem merupakan model
konseptual yang mendefinisikan struktur, perilaku, dan
pandangan dari suatu sistem. Berikut
rancangan
arsitektur sistem usulan pada penelitian ini.

ilustrasi

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

1. Menu Pengelolaan User

Menu pengelolaan user (User Management) hanya ada
pada tampilan admin. Pada menu pengelolaan user terdapat
tampilan tabel yang merupakan master data user. Pada menu
ini, admin dapat menambahkan, mengubah, menghapus, dan
melihat user.

Gambar 5. Rancangan Arsitektur Sistem Usulan

Berdasarkan gambar di atas dapat dilihat bahwa kelima
user dapat mengakses KMS Web dengan menggunakan
PC/device lainnya yang terhubung dengan internet. Hal ini
dikarenakan sistem yang dibangun berbasis web sehingga
penggunaan sistem lebih fleksibel.

B. Rancangan ERD

Gambar 7. Menu Pengelolaan User

Pada menu pengelolaan user, terdapat button edit, hapus,

dan tambah user. Berikut tampilan untuk menambahkan user.

Gambar 8. Menu Tambah User

2. Menu Pengelolaan Survei

Menu pengelolaan survei (Assignment Survei) hanya
terdapat pada tampilan admin, koordinator, supervisor, dan
operator saja. Menu ini berisi master data survei yang terdiri
dari beberapa sub menu, yakni

a. Sub menu Semua Survei

Sub menu semua survei berisi tentang daftar semua
survei baik yang telah selesai dilakukan maupun yang
sedang berlangsung. Pada sub menu ini terdiri dari
nama survei, nama supervisor, nama operator, dan
beban tugas yang diberikan untuk operator. Berikut
tampilan dari menu semua survei.

Gambar 6. Rancangan ERD

Rancangan di atas merupakan rancangan basis data yang
ada di dalam sistem ini. Rancangan basis data berguna untuk
mengetahui daftar entitas yang ada dan hubungan antar
tersebut. peneliti merancang basis data dengan
entitas
pendekatan ERD (Entity Relationship Diagram) yang
merupakan desain konseptual.

6.

Construction & Testing
Tahapan

ini

tahapan

merupakan

untuk
mengimplementasikan hasil rancangan sistem usulan yang
dibuat pada tahapan physical design. Implementasi sistem
menggambarkan sistem secara visual sehingga pengguna
dapat berinteraksi dengan fitur-fitur yang ada di dalam sistem.
Proses implementasi menggunakan bahasa pemograman PHP
dengan framework CodeIgniter. KMS ini diberi nama PKS
Web. Berikut tampilan implementasi sistem.

Gambar 9. Sub menu Semua Survei

5 / 8

b. Sub menu Survei Saya

Sub menu ini berisikan survei apa saja yang dipegang
atau diemban oleh user baik itu koordinator,
supervisor, maupun operator. Berikut
ilustrasi sub
menu survei saya pada tampilan operator.

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

e. Sub menu Assign Survei

Sub menu ini hanya terdapat pada tampilan supervisor.
Pada sub menu ini, supervisor dapat memilih operator
yang akan ditugaskan pada suatu survei dengan beban
tugasnya masing-masing. Berikut ilustrasinya.

Gambar 10. Sub Menu Survei Saya

c. Sub Menu Penugasan

pada

hanya

terdapat

ini
karena

tampilan
Sub menu
koordinator,
jawab
yang
memberikan wewenang penugasan adalah koordinator.
Sub menu ini berisikan nama supervisor yang diberi
penugasan oleh koordinator. Berikut ilustrasi pada sub
menu ini.

bertanggung

Gambar 13. Sub menu Assign Survei

3. Menu Pengelolaan Laporan

Pada menu ini

seluruh user kecuali viewer dapat
mengunggah dan mengunduh laporan. Tetapi, laporan hanya
dapat dihapus oleh admin. Berikut tampilan menu pengelolaan
laporan.

Gambar 11. Sub menu Penugasan

d. Sub Menu Kirim Penugasan

Pada menu ini

terdapat button upload laporan yang
berfungsi untuk menambahkan laporan. Berikut ilustrasi dari
upload laporan.

Gambar 14. Sub menu pengelolaan laporan

ini

hanya

Sub menu
koordinator. Sub menu ini berisi
koordinator
pengawas dalam sebuah survei. Berikut ilustrasinya.

tampilan
inputan dari
sebagai

untuk memilih

supervisor

terdapat

pada

Gambar 12. Sub menu Kirim Penugasan

Gambar 15. Upload Laporan

4. Menu Pengelolaan Dokumen

Menu pengelolaan dokumen berisi master data dokumen
yang terdiri dari kategori dokumen, deskripsi dokumen, user
yang mengunggah dokumen, tanggal dokumen diubah, dan
file dokumen tersebut. Dokumen dapat ditambah, diubah,

6 / 8

dihapus, dan dilihat oleh admin. Internal PKS dapat melihat,
mengunggah, dan mengunduh dokumen sedangkan eksternal
PKS (viewer) hanya dapat melihat dan mengunduh sebagian
dokumen yakni dokumen SOP, Juknis, dan survei. Berikut
ilustrasinya.

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

TABEL I
BLACK BOX TESTING

Gambar 16. Menu Pengelolaan Dokumen

5. Menu Pengelolaan Pertanyaan (FAQ)

Menu FAQ berisi pertanyaan-pertanyaan yang sering
diajukan gunanya agar tidak terjadinya perulangan pertanyaan
yang sama. Berikut ilustrasi dari FAQ.

Gambar 17. Menu Pengelola Pertanyaan (FAQ)

C.

Evaluasi Sistem
Setelah sistem dapat diimplementasikan dan telah
berhasil di deploy ke server, selanjutnya peneliti melakukan
kegiatan diskusi bersama subject matter (Pegawai Subdit PKS)
dengan menyertakan link untuk mengakses sistem tersebut
gunanya
terhadap sistem.
Berdasarkan hasil uji coba bersama subject matter tersebut
dapat diambil kesimpulan bahwa sistem tersebut telah berjalan
sesuai dengan kebutuhan pengguna. Selain itu, peneliti
melakukan evaluasi dengan menggunakan 3 (tiga)
jenis
evaluasi yakni, Black Box Testing, System Usability Scale
(SUS), Questionnaire for User Interface Satisfaction (QUIS).

untuk melakukan

coba

uji

1. Black Box Testing

Black Box testing bertujuan untuk mengevaluasi hasil
implementasi dari setiap fungsi yang ada dalam sistem. Pada
penelitian ini, peneliti merancang 56 skenario untuk di ujikan.
Berikut 5 dari 56 skenario yang diujikan dalam black box
testing, dijelaskan pada Tabel I.

Berdasarkan hasil uji pada tabel I dapat disimpulkan bahwa
pembangunan sistem ini telah berjalan sesuai dengan yang
diharapkan oleh peneliti.

2. SUS (System Usability Scale)

SUS merupakan metode evaluasi sistem yang bertujuan
untuk menguji kelayakan dari sistem yang telah dibangun.
Penelitian ini dilakukan terhadap pegawai BPS khususnya
Subdit PKS. Dimana, subdit PKS terdiri dari 14 Pegawai
yakni 1 koordinator, 3 supervisor, dan 10 operator. Namun,
terdapat 10 (sepuluh)
pada pengujian SUS ini hanya
responden dengan rincian 1 orang merupakan admin, 1 orang
merupakan koordinator, 1 orang merupakan supervisor, 5
orang merupakan operator dan 2 orang merupakan viewer.
SUS disajikan dalam bentuk kuesioner yang berisi 10
pernyataan. Setiap pernyataan memiliki rentang 0-4 yakni
sangat tidak setuju hingga sangat setuju. Untuk pernyataan
bernomor ganjil, nilainya akan dikurangi dengan 1. Untuk
penyataan yang bernomor genap, nilainya didapat dari 5
dikurangi nilai jawaban. Setelah dikonversikan, nilai tersebut
dijumlah lalu dikalikan 2,5 agar nilainya tepat 100. Jika suatu
sistem dilakukan uji SUS dan mendapatkan nilai rata-rata di
atas 68, maka dapat dikatakan bahwa sistem tersebut sudah
layak atau valid untuk digunakan. Berikut hasil uji SUS pada
penelitian ini. Hasil dari SUS ini dijelaskan pada Tabel II.

7 / 8

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

TABEL II
SYSTEM USABILITY SCALE

VII.

PENUTUP

A. Kesimpulan

Berdasarkan hasil uji SUS di atas dapat dilihat bahwa rata-
rata nilai SUS nya adalah 84,5. Hal ini menandakan bahwa
fungsi-fungsi yang ada di dalam sistem ini sudah layak
digunakan sesuai dengan peran masing-masing aktor tersebut
meskipun masih terdapat penamaan yang tidak konsisten.
Menurut
dapat
merepresentasikan fungsi yang ada pada sistem dengan
kebutuhan pengguna sistem.

pengujian

peneliti,

telah

hasil

ini

3. QUIS (Questionnaire for User Interface Satisfaction)

tampilan layar,

QUIS bertujuan untuk mengevaluasi kepuasan pengguna
terhadap aspek-aspek interaksi manusia dengan sistem
komputer. Aspek tersebut
terdiri dari aspek keseluruhan
sistem,
terminologi dan sistem informasi,
kemudahan mempelajari sistem, kinerja sistem, dan antar
muka pengguna. Responden pada evaluasi ini sama dengan
responden pada uji SUS. Nilai QUIS didapatkan dari rata-rata
seluruh aspek dimana nilai setiap aspeknya merupakan rata-
rata tiap pernyataan yang memiliki rentang dari 0-9. Berikut
hasil uji QUIS pada penelitian ini.

TABEL III
QUESTIONNAIRE FOR USER INTERFACE SATISFACTION

seluruh aspek sebesar 7,6. Hal

Berdasarkan hasil QUIS Pada Tabel 23 dapat dilihat bahwa
rata-rata total dari
ini
menandakan bahwa pengguna merasa cukup puas terhadap
tampilan dan fungsi pada sistem yang telah dibangun.
Meskipun tata letak informasi pada sistem ini masih kurang
proporsional, Namun istilah yang digunakan pada sistem ini
mudah dipahami dan sistem ini termasuk sistem yang mudah
untuk dipelajari.

dilakukan

Berdasarkan hasil analisis, rancangan dan implementasi yang
telah
Peneliti
bab-bab
menyimpulkan sebagai berikut :
1.

sebelumnya,

pada

Penelitian ini telah berjalan sesuai dengan tujuan yang
diharapkan, baik dari
tujuan umum maupun tujuan
khususnya yakni merancang dan membangun Knowledge
Management System (KMS) berbasis web untuk subdit
PKS. Dimana, sistem ini dapat menjadi sarana berbagi
informasi dan dapat mendokumentasikan setiap proses
bisnis di subdit PKS dan dapat menyediakan solusi dari
permasalahan yang sering ditanyakan oleh pihak terkait
topik serta menyediakan fasilitas
dengan berbagai
upload/download dokumen.
Sistem yang telah dibangun dapat dikatakan layak
digunakan karena fungsi-fungsi yang ada sudah berjalan
sesuai dengan kebutuhan. Hal ini dapat dilihat dari hasil
uji evaluasi sistem.

Saran
Berdasarkan

dari

hasil

evaluasi

sistem dapat
dikatakan bahwa sistem yang telah dibangun peneliti sudah
baik. Fungsi atau fitur di dalam sistem juga sudah berjalan
sesuai dengan yang diharapkan dan sesuai dengan kebutuhan
pengguna. Namun masih diperlukan perbaikan terhadap
tampilan sistem dimana tampilan pada sistem ini masih
kurang responsif dan tata letak informasi yang kurang
proporsional. Selain itu, penggunaan kata-kata didalam sistem
masih kurang konsisten. Maka dari itu, diharapkan adanya
pengembangan/perbaikan sistem agar sistem ini lebih menarik
sehingga lebih sering digunakan.

2.

B.

DAFTAR PUSTAKA
[1] Conni, S., Rancangan dan Implementasi Knowledge Management System
pada Subdirektorat Layanan dan Promosi Statistik BPS. Jakarta, 2019 .
[2] Yance, S., Indri, R., Alde, A., & Iswandi,S., “Rancang Bangun Aplikasi
Knowledge Management Berbasis Web”, Jurnal Inovasi Vokasional dan
Teknologi, Vol. 18, No. 2, 2018.

[3] Ani, O.S., & Elan, N., “Rancang Bangun Sistem Informasi Persediaan
Barang berbasis Web dengan Metode FAST (Framework for
the
Application of System Thinking)”, Jurnal PILAR Nusa Mandiri, Vol. 13,
No. 2, 2017.

[4] Anna, K. C., & Jonte, B., “Design Science Research - An Engineering
Research Approach to Improve Methods for Engineering Education
Research”, Paper Presented at the Research in Engineering Education of
Symposium (REES). Dublin, 2015.

[5] Whitten, J.L., & Bentley, L, System Analysis and Design for Global

Enterprise: Seventh Edition, New York: McGraw Hill, 2007.

8 / 8

"
221709926,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pengembangan Sistem Informasi Pemberian 
Rekomendasi Tugas Belajar Jenjang Pendidikan 
Strata 2 dan Strata 3 di BPS 

Nurul Ni’mah (221709926, 4SI1) 
Dosen Pembimbing: Yunarso Anang Sulistiadi, Ph.D. 

Ringkasan—  Dalam  meningkatkan  kualitas  pegawai  melalui 
pendidikan,  Badan  Pusat  Statistik 
(BPS)  dibantu  Pusat 
Pendidikan  dan  Pelatihan  (Pusdiklat),  salah  satunya  melalui 
program  Tugas  Belajar  (TB)  jenjang  pendidikan  Strata  2  (S-2) 
dan Strata 3 (S-3). Realisasi pegawai BPS yang mendaftarkan diri 
sebagai  calon  peserta  TB  cenderung  menurun.  Untuk 
mengantisipasi hal tersebut, serta memudahkan dalam pemberian 
rekomendasi kepada pegawai yang diusulkan oleh pejabat eselon 
II, diperlukan suatu sistem informasi pemberian rekomendasi TB 
S-2  dan  S-3.  Penelitian  ini  bertujuan  untuk  mengembangkan 
sistem  informasi  pemberian  rekomendasi  kepada  pegawai  yang 
sangat  direkomendasikan,  cukup  direkomendasikan,  atau  tidak 
direkomendasikan  untuk  dapat  melanjutkan  pendidikan  ke 
jenjang S-2 dan S-3 dengan program TB. Pengambilan keputusan 
rekomendasi  menggunakan  metode  Fuzzy  Analytic  Hierarchy 
Process  (Fuzzy  AHP)  dengan  kriteria  dan  subkriteria  penilaian 
beserta  bobotnya  dapat  diubah  oleh  pihak  tertentu.  Sistem 
dibangun  dengan  metode  Framework  for  the  Application  of 
Systems  Thinking  (FAST).  Sistem  dievaluasi  dengan  black  box 
testing  dengan  hasil  semua  fungsi  berjalan  baik  dan  System 
Usability Scale (SUS) dengan hasil 80,71 yang artinya sistem dapat 
diterima  dengan  baik  oleh  pengguna.  Dengan  adanya  sistem  ini 
diharapkan  dapat  membantu  seleksi  pegawai  yang  mengajukan 
diri serta memberikan kesempatan kepada pegawai yang dinilai 
layak untuk melanjutkan pendidikan dengan TB jenjang S-2 dan 
S-3 secara efisien dan mengurangi subjektivitas. 

Kata Kunci— Rekomendasi, tugas belajar, Fuzzy AHP, kriteria, 

subkriteria. 

I.  LATAR BELAKANG 

ditetapkan  pada  setiap  awal 
tahun  [2]-[7].  Salah  satu 
pemicunya  yaitu  berkurangnya  partisipasi  pegawai  dalam 
mengikuti beasiswa S-2 dan S-3 dengan beasiswa APBN BPS 
sehingga  terjadi  pengusulan  calon  peserta  TB  yang  masih 
kurang  dari  kuota.  Misalnya  pada  tahun  2019,  rintisan  gelar 
pada program S-3 ITB tidak ada yang mendaftar [6]. Lalu pada 
tahun 2020, pengusulan Tugas Belajar (TB) S-2 dan S-3 APBN 
BPS oleh pejabat eselon II membutuhkan perpanjangan batas 
waktu pengusulan hingga dua kali untuk S-2 dan sekali untuk 
S-3 [8]-[10]. 

Gambar 1. Perbandingan target dan realisasi peserta tugas belajar BPS 2015-

2019 

*Pendaftar tahun 2015 belum terdokumentasi secara penuh pada Simdiklat 

Dalam  mengemban  tugas  dan  fungsinya,  Badan  Pusat 
Statistik  (BPS)  harus  ditunjang  oleh  sumber  daya  manusia 
(SDM)  yang  berkualitas.  Pada  tahun  2019,  dari  total  16.446 
pegawai, persentase SDM yang berpendidikan D-III ke bawah 
sebesar 29,54 persen, berpendidikan terakhir D-IV/S-1 sebesar 
54,75 persen, berpendidikan S-2 sebesar 15,33 persen, dan S-3 
sebesar 0,38 persen [1]. Dalam peningkatan kualitas SDM, BPS 
dibantu Pusat Pendidikan dan Pelatihan (Pusdiklat). Pusdiklat 
BPS  mempunyai  tugas  salah  satunya  menyelenggarakan 
program Tugas Belajar (TB) yang mencakup pendidikan gelar 
Strata 2 (S-2) dan Strata 3 (S-3). 

Seperti yang ditunjukkan pada Gambar 1, selama tahun 2015 
hingga  2019  capaian  kinerja  yang  diperoleh  pada  indikator 
jumlah  pegawai  yang  mengikuti  tugas  belajar  yang  dibiayai 
BPS  cenderung  mengalami  tren  penurunan  dan  lebih  rendah 
jika  dibandingkan  dengan  target  menurut  Rencana  Strategis 
(Renstra)  Pusdiklat  BPS  tahun  2015-2019  yang  ditetapkan  5 
tahun sekali dan Perjanjian Kinerja (PK) Pusdiklat BPS yang 

Tahapan seleksi administrasi dan pemilihan calon TB dapat 
dilihat pada Gambar 2. Tahapan pertama dalam seleksi pegawai 
tugas  belajar  dimulai  dengan  pemberitahuan/penawaran 
melalui surat dari Pusdiklat BPS kepada pejabat eselon II [2]. 
Setiap  pengiriman  usulan  calon  peserta  TB  untuk  mengikuti 
beasiswa S-2 dan S-3 baik dari beasiswa sponsor BPS maupun 
dari Non BPS harus mendapat usulan tertulis dan rekomendasi 
dari atasan setingkat eselon II [11]. Untuk itu, pada tahapan ini 
pejabat  setingkat  eselon  II  harus  mampu  menyeleksi  setiap 
pegawai  yang  mengajukan  beasiswa  atau  pun  memberikan 
kesempatan  kepada  pegawai  tertentu  yang  telah  memenuhi 
standar kelayakan sesuai dengan satuan kerjanya.  

Penyeleksian  calon  peserta  TB  di  tingkat  eselon  II  dan 
Pusdiklat  pada  tahap  kedua  dan  ketiga  berkaitan  dengan 
pengambilan  keputusan  multikriteria  atau  Multi  Criteria 
Decision Making (MCDM). Pengambilan keputusan ini disebut 
demikian  karena  adanya  kriteria  atau  persyaratan-persyaratan 
tertentu  yang  menentukan  seorang  pegawai  dapat  dinyatakan 

 1 / 8 

 
 
 
 
 
layak  rekomendasi  untuk  dapat  didaftarkan  dan  diseleksi 
lanjutan di Pusdiklat BPS maupun di sponsor pemberi beasiswa 
terkait untuk melanjutkan studi dengan program Tugas Belajar 
baik  S-2  maupun  S-3.  Namun,  pengambilan  keputusan  yang 
selama  ini  dilakukan  masih  secara  manual.  Hal  ini  berisiko 
tinggi  akan  adanya  pengambilan  keputusan  yang  bersifat 
subjektif. Dikarenakan hal tersebut, pendekatan Fuzzy-MCDM 
dapat  dinilai  berguna  untuk  menangani  data  yang  tidak  tepat 
dan  tidak  pasti  [12].  Selain  itu,  metode  Analytical  Hierarchy 
Process (AHP) juga diperhitungkan karena dianggap sederhana, 
mudah,  dan  sangat  fleksibel.  Proses  AHP  juga  digunakan  di 
hampir semua aplikasi terkait MCDM dengan struktur hierarki 
[13].  Sehingga  pendekatan  kombinasi  Fuzzy  AHP  dianggap 
mampu  mengatasi 
ketidaktepatan 
ketidakpastian 
pengambilan  keputusan  secara  hierarkis  dan  membuat  hasil 
evaluasi menjadi lebih ilmiah, akurat, dan objektif [12].  

dan 

Gambar 2. Diagram alir seleksi pegawai tugas belajar 

Pemberian  rekomendasi  didasarkan  pada  kriteria  dan 
subkriteria  dengan  pembobotannya  dalam  suatu  Decision 
Support System (DSS) berupa sistem informasi yang berbasis 
web. Kriteria dan subkriteria ini berupa persyaratan untuk tugas 
belajar, penilaian  kepegawaian, dan  kaitan dengan pekerjaan. 
Pemilihan  kriteria  dan  subkriteria  beserta  pembobotannya 
ditanyakan kepada responden melalui pendekatan wawancara. 
Analisis  konsistensi  akan  menggunakan  metode  AHP  lalu 
dilanjutkan dengan Fuzzy AHP untuk pembobotan [14]. Selain 
diharapkan  sebagai  masukan  bagi atasan eselon II atau pihak 
terkait  untuk  dapat  memberikan  rekomendasi  pegawai  BPS 
dalam menempuh pendidikan lebih lanjut, diharapkan dengan 
sistem  ini  juga  dapat  menjadi  jembatan  proses  antara  Biro 
Kepegawaian dengan Pusdiklat BPS.  

II.  TUJUAN PENELITIAN 

Tujuan  penelitian  yang  akan  dicapai  pada  penelitian  ini 
secara  umum  yaitu  dapat  membantu  dalam  pengambilan 
keputusan  pegawai  yang  sangat  direkomendasikan,  cukup 
direkomendasikan,  atau  tidak  direkomendasikan  untuk  dapat 
melanjutkan  pendidikan  ke  jenjang  S-2  dan  S-3  dengan 
program Tugas Belajar. 

Untuk  dapat  mencapai  tujuan  tersebut,  terdapat  beberapa 

tujuan khusus yang diuraikan sebagai berikut. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

1.  Menyusun  indikator  penilaian  berupa  kriteria  dan 
subkriteria  guna  pembobotan  penilaian  pegawai  untuk 
mengikuti program Tugas Belajar S-2 dan S-3. 

2.  Merumuskan suatu formula dalam pemberian keputusan 
rekomendasi 
sangat 
direkomendasikan, cukup direkomendasikan, atau tidak 
direkomendasikan. 

belajar 

berupa 

tugas 

3.  Membangun sistem informasi berbasis web untuk dapat 
diakses oleh pihak terkait dengan mudah dan cepat. 

Manfaat  penelitian  yang  diharapkan  di  antaranya  sebagai 

berikut. 

1.  Menyeleksi setiap pegawai yang mengajukan beasiswa 
Tugas Belajar secara subjektif yang dapat terukur. 
2.  Memberikan  kesempatan  kepada  pegawai  yang  telah 
memenuhi  standar  kelayakan  untuk  melanjutkan 
pendidikan  ke  jenjang  S-2  dan  S-3  dengan  Tugas 
Belajar. 

3.  Efisiensi  waktu  dan  biaya  dalam  pemberian 

rekomendasi dan seleksi calon peserta TB. 

4.  Dapat  menjadi  penghubung  antara  data  penilaian 
pegawai  dari  Biro  Kepegawaian  dengan  penilaian 
Pusdiklat  BPS  untuk  seleksi  calon  peserta  TB  dengan 
lebih cepat. 

Dalam  penelitian  ini  terdapat  batasan  penelitian  sebagai 

berikut. 

1.  Rekomendasi  diberikan  hanya  untuk  pegawai  BPS 
untuk program TB dan terbatas untuk jenjang  S-2 dan 
S-3. 

2.  Hasil berupa kategori sangat direkomendasikan, cukup 

direkomendasikan, dan tidak direkomendasikan. 
3.  Sistem informasi yang dibangun berbasis web.  
4.  Sistem  penyelenggaraan  TB  yang  terdiri  dari  proses 
pendaftaran  dan  pengelolaannya  tidak  dicakup  dalam 
penelitian ini karena telah terdapat pada sistem yang ada.  

III. PENELITIAN TERKAIT 

Penelitian yang telah ada sebelumnya dan berkaitan dengan 

topik skripsi ini yaitu sebagai berikut. 
1.  Artikel dalam Proceedings of the 17th Asian Network for 
Quality Congress tahun 2019 di Bangkok, Thailand yang 
berjudul  “Pengembangan  Sistem  Informasi  Penilaian 
Kinerja dan Rekomendasi Mitra BPS Berbasis Web” oleh 
Raden  Rara  Nurariza  Ramadhanty,  Yunarso  Anang, 
Yoshimichi  Watanabe,  dan  Masakazu  Takahashi  yang 
berhasil  mengembangkan  sistem  informasi  pemberian 
rekomendasi dengan metode AHP dengan kategori “baik”, 
“cukup”,  dan  “buruk”  yang  didasarkan  pada  penilaian 
kinerja  mitra  pencacahan  dan  pengolahan.  Dalam 
pembangunan  perangkat  lunak  menggunakan  metode 
Software Development Life Cycle (SDLC). Dari penelitian 
ini diambil bagaimana cara melakukan pembobotan yang 
berguna  dalam  mendapatkan  nilai  konsistensi  dan  teknik 
dalam pengategorian [15]. 

2.  Jurnal berjudul “Evaluating Teaching Performance Based 
on Fuzzy AHP and Comprehensive Evaluation Approach” 
dalam  Jurnal  Applied  Soft  Computing  oleh  Jeng-Fung 

 2 / 8 

 
 
 
Chen, Ho-Nien Shieh, Quang Hung Do pada tahun 2015 
yang  melakukan  evaluasi  kinerja  pengajaran  dengan 
kombinasi  dari  metode  Fuzzy  AHP  dan  metode  evaluasi 
komprehensif  Fuzzy  dengan  keluaran  berupa  kategori 
“sangat  baik”,  “baik  sekali”,  “baik”,  “cukup”,  “kurang 
baik”.  Dari  jurnal  ini  dikutip  alasan  penggunaan  metode 
Fuzzy AHP, penghitungannya, dan evaluasi komprehensif 
Fuzzy [12]. 

3.  Penelitian berjudul “Komparasi Fuzzy AHP dengan AHP 
pada  Sistem  Pendukung  Keputusan  Investasi  Properti” 
dalam  Jurnal  Electrics,  Electronics,  Communications, 
Controls,  Informatics,  Systems  (EECCIS)  Universitas 
Brawijaya  tahun  2014  oleh  Ahmad  Faisol,  M.  Aziz 
Muslim,  dan  Hadi  Suyono  yang  menunjukkan  bahwa 
pengambilan  keputusan  dengan  metode  Fuzzy  AHP 
memiliki tingkat akurasi yang lebih tinggi daripada metode 
AHP.  Dari  penelitian  ini  juga  diambil  bagaimana  cara 
melakukan  pengukuran  akurasi  dari  keputusan  yang 
dihasilkan oleh sistem [16]. 

4.  Jurnal berjudul “Theory and Methodology: Applications of 
The  Extent  Analysis  Method  on  Fuzzy  AHP”  dalam 
European Journal of Operational Research oleh Da-Yong 
Chang pada tahun 1996 yang menjadi jurnal rujukan dari 
teori  penggunaan  metode  Fuzzy  AHP.  Dari  jurnal  ini 
dikutip bagian konsep dasar dari penghitungan Fuzzy AHP, 
penggunaan  Triangular  Fuzzy  Number  (TFN),  dan 
pengaplikasiannya [18]. 

IV. METODE PENELITIAN  

A.  Metode Pengumpulan Data 

Dalam penelitian ini terdapat beberapa metode pengumpulan 

data yaitu: 

a  Studi  pustaka  untuk  menemukan  permasalahan,  topik, 

dan metodologi yang sesuai. 

b  Wawancara dengan subject matter yaitu kepala Bagian 
Umum  dan  TI  Pusdiklat  BPS,  serta  satuan  kerja 
setingkat  eselon  II  yang  diwakili  pegawai  dari 
Subbagian  Kepegawaian  dan  Hukum  BPS  Provinsi 
Kalimantan  Selatan  untuk  mendalami  permasalahan, 
memperoleh kriteria dan subkriteria penilaian TB, serta 
memperoleh bobot nilai dari kriteria dan subkriteria. 
c  Kuesioner  yang  berkaitan  dengan  evaluasi  uji  coba 
sistem.  Kuesioner  yang  digunakan  untuk  uji  coba  dan 
evaluasi  sistem  menggunakan  metode  Black  Box  dan 
System Usability Scale (SUS). 

B.  Metode Analisis 

Metode  analisis  menggunakan  pengambilan  keputusan 
multikriteria  dengan  AHP  untuk  pengecekan  konsistensi  dan 
Fuzzy  AHP  digunakan  dalam  pemberian  nilai  bobot. 
Selanjutnya,  menentukan kelayakan dari setiap alternatif atau 
calon peserta TB yang dinilai. Metode analisis  menggunakan 
Fuzzy AHP ditunjukkan pada Gambar 3. 

Metode  dengan  AHP  digunakan  untuk  pengecekan 
konsistensi. Penggunaan AHP meliputi tahapan sebagai berikut. 
a.  Memecah permasalahan ke sejumlah elemen kecil lalu 

menyusun elemen ke dalam bentuk hierarki. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

b.  Membuat  matriks  perbandingan  berpasangan.  Skala 
perbandingan berpasangan dapat dilihat pada Tabel I. 
c.  Mengestimasi bobot relatif setiap elemen dengan eigen 

value. 

d.  Menjumlahkan dan menggabungkan bobot relatif untuk 
pengukuran  akhir  dari  alternatif  keputusan  yang 
diberikan. 

e.  Menghitung nilai konsistensi [17]. 

Gambar 3. Alur penggunaan Fuzzy AHP dalam metode analisis 

Jika  telah  didapatkan  nilai  perbandingan  yang  konsisten, 
maka akan dilanjutkan dengan metode Fuzzy AHP yang akan 
diuraikan secara singkat sebagai berikut. 

a  Membuat  matriks  perbandingan  berpasangan  dengan 
transformasi  Triangular  Fuzzy  Number  (TFN).  Skala 
TFN dapat dilihat pada Tabel I. 

b  Menentukan nilai sintesis fuzzy prioritas. 
c  Menentukan nilai vektor. 
d  Normalisasi nilai bobot vektor fuzzy [18]. 

Intensitas 
kepentingan 
AHP 
1 

2 
3 

4 

5 

6 
7 

8 
9 

TABEL I 
SKALA TRIANGULAR FUZZY NUMBER (TFN) [18] 

Himpunan linguistik 

Kedua elemen sama 
pentingnya. 
Pertengahan  
Elemen yang satu sedikit 
lebih penting daripada 
elemen yang lainnya. 
Pertengahan elemen satu 
lebih cukup penting dari 
yang lainnya. 
Elemen yang satu kuat 
pentingnya daripada 
elemen yang lainnya. 
Pertengahan 
Satu elemen jelas lebih 
mutlak penting daripada 
elemen lainnya. 
Pertengahan 
Satu elemen mutlak 
penting daripada elemen 
lainnya. 

Triangular
Fuzzy 
Number 

Recprocal 
(kebalikan) 

(1,1,1) 

(1,1,1) 

(1/2,1,3/2) 
(1,3/2,2) 

(2/3,1,2) 
(1/2,2/3,1) 

(3/2,2,5/2) 

(2/5,1/2,2/3) 

(2,5/2,3) 

(1/3,2/5,1/2) 

(5/2,3,7/2) 
(3,7/2,4) 

(2/7,1/3,2/5) 
(1/4,2/7,1/3) 

(7/2,4,9/2) 
(4,9/2,9/2) 

(2/9,4,2/7) 
(2/9,2/9,1/4) 

 3 / 8 

 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

C.  Metode Pembangunan Sistem 

Metode  dalam  pembangunan  sistem  yang  digunakan  pada 
penelitian ini yaitu Framework for the Application of Systems 
Thinking (FAST). Tahapan dalam pembangunan menggunakan 
FAST yaitu 

kecuali  pendaftaran  calon  peserta  TB  secara  online  melalui 
Simdiklat. Pada proses yang diberi warna, peneliti menemukan 
adanya  celah  pemberian  rekomendasi  yang  sifatnya  sangat 
subjektif dari penilai atau satuan kerja setingkat eselon II.  

a  Definisi ruang lingkup; 
b  Analisis masalah; 
c  Analisis kebutuhan; 
d  Desain logis; 
e  Analisis keputusan; 
f  Desain fisik dan integrasi; 
g  Konstruksi dan pengujian; dan 
h 

Instalasi dan pengiriman [19] 

V.  KERANGKA PIKIR 

ini  mengangkat 

Kerangka  pikir  penelitian  ini  disajikan  pada  Gambar  4. 
Penelitian 
tema  pengembangan  sistem 
informasi pemberian rekomendasi Tugas Belajar  S-2 dan  S-3 
di  BPS.  Penelitian  ini  berlandaskan  pada  beberapa  masalah 
seperti  yang  dirincikan  pada  Gambar  4.  Berdasarkan 
permasalahan  tersebut,  diambil  pendekatan  penelitian  berupa 
pengembangan sistem berbasis web dengan metode Fuzzy AHP. 
Pendekatan tersebut mempertimbangkan peluang berupa sudah 
tersedianya  web  Tugas  Belajar  dan  Simdiklat  (Sistem 
Informasi  Manajemen  Kediklatan)  yang  dikelola  Pusdiklat 
BPS  dalam  pendaftaran  hingga  pengelolaan  TB.  Hasil  yang 
diharapkan  berupa  sistem  pengambilan  keputusan  berupa 
rekomendasi untuk melanjutkan pendidikan jenjang S-2 dan S-
3  agar  menjadi  lebih  efisien,  efektif,  dan  subjektivitas  yang 
dapat terukur. 

Gambar 4. Diagram kerangka pikir 

VI. HASIL DAN PEMBAHASAN 

A.  Analisis Sistem Berjalan 

Gambaran  umum  proses  penawaran  beasiswa  TB, 
pemberian rekomendasi TB, hingga pemanggilan calon peserta 
TB terpilih dalam sistem yang saat ini berjalan disajikan dalam 
bentuk diagram proses bisnis yang dapat dilihat pada Gambar 
5.  Hampir  seluruh  proses  bisnis  dikerjakan  secara  manual, 

Gambar 5. Diagram alir sistem berjalan 

B.  Analisis Masalah 

Dalam  sistem  berjalan,  pemberian  rekomendasi  dilakukan 
secara  manual  dan  masih  terdapat  beberapa  permasalahan. 
Analisis  permasalahan  pada  sistem  berjalan  ditampilkan 
menggunakan  diagram  fishbone  pada  Gambar  6.  Masalah 
utama  dalam  sistem  berjalan  yaitu  belum  adanya  sistem 
pemberian  rekomendasi  TB  S-2  dan  S-3  di  BPS  yang  efektif 
dan  mengurangi  subjektivitas  yang 
tinggi.  Selanjutnya, 
masalah  tersebut  dibagi  menjadi  beberapa  akar  masalah 
berdasarkan  komponen  pegawai,  informasi,  aplikasi,  dan 
metode. 

Gambar 6. Diagram fishbone 

 4 / 8 

 
 
 
 
  
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

terdapat juga Administrator sebagai pengelola sistem dan calon 
peserta  sebagai  menerima  hasil  seleksi.  Tim  calon  peserta, 
penilaian, dan pengawas dapat masuk ke dalam sistem melalui 
login.  Gambaran  mengenai  pengguna  dari  sistem  usulan  ini 
digambarkan dengan diagram use-case pada Gambar 7. 

C.  Analisis Kebutuhan Sistem 
Setelah  mengetahui  masalah-masalah  yang  terjadi  pada 
sistem,  dilanjutkan  dengan  menganalisis  kebutuhan  sistem 
yang akan dibangun. Analisis kebutuhan sistem menggunakan 
analisis  PIECES  (Performance,  Information,  Economics, 
Control,  Efficiency,  Services).  Analisis  PIECES  ditunjukkan 
pada Tabel II. 

TABEL II 
ANALISIS PIECES SISTEM BERJALAN DAN SISTEM USULAN 

Analisis 

Sistem berjalan 

Performance  Belum  ada  sistem  yang 
dapat memberikan penilaian 
dan 
dari 
rekomendasi 
peserta  pendaftar  TB  S-2 
dan S-3. 

Information 

Economics 

Control 

Efficiency 

Services 

Data  pemberian  keputusan 
rekomendasi  belum  dapat 
terdokumentasi 
dengan 
baik. 

Peserta  pendaftar  TB  S-2 
dan S-3 perlu mengeluarkan 
biaya  dan  waktu  untuk 
surat 
mendapatkan 
atasan 
ke 
rekomendasi 
II  dan 
setingkat  eselon 
mengirimkannya 
ke 
Pusdiklat. 
Dapat  terjadi  kemungkinan 
dalam 
kesalahan 
memberikan 
rekomendasi 
karena penilai hanya menilai 
kelengkapan  berkas,  tidak 
detail  dalam  pemenuhan 
persyaratan. 
Penilaian  yang  dilakukan 
oleh atasan setingkat eselon 
II saja masih kurang cukup, 
subjektivitas  yang 
risiko 
tinggi,  dan 
tidak  efisien 
apabila  tidak  melihat  dari 
pemenuhan persyaratan dan 
kriteria lainnya. 
Tidak  ada  layanan  evaluasi 
pendaftar peserta TB S2 dan 
S-3 di tingkat eselon II. 

S-3 

dan 

sebagai 

Sistem usulan 
Rekomendasi  peserta  TB 
S-2 
dapat 
diketahui  dari  penilaian 
dengan  sistem  yang  dapat 
dijadikan 
alat 
bantu  dalam  pengambilan 
keputusan. 
Data pemberian keputusan 
dapat 
rekomendasi 
didokumentasikan 
dan 
diketahui 
bagaimana 
proses penilaiannya. 
Peserta  pendaftar  TB  S-2 
perlu 
dan  S-3 
membutuhkan 
biaya 
banyak  dan  waktu  lama 
untuk  mendapatkan  surat 
rekomendasi  dari  atasan 
setingkat eselon II. 

tidak 

Tingkat 
kesalahan 
dikurangi dengan penilaian 
yang  melihat  pemenuhan 
dan 
persyaratan 
pertimbangan 
bobot 
kriteria  dalam  pemberian 
rekomendasi. 
dari 
Subjektivitas 
penilaian  yang  dilakukan 
oleh 
setingkat 
eselon II diusahakan dapat 
diminimalkan  dan  dapat 
melihat  penilaian  yang 
terukur serta lebih efisien. 

atasan 

menyediakan 
Sistem 
pemberian 
fasilitas 
pendaftar 
rekomendasi 
peserta TB S-2 dan S-3 di 
tingkat  eselon 
II  dan 
Pusdiklat  sehingga  dapat 
memberikan 
evaluasi 
kepada  pendaftar  yang 
belum dapat diterima. 

D.  Rancangan Sistem Usulan 

Rancangan  sistem  yang  akan  dibangun  dimulai  dengan 
menggambarkan siapa saja yang akan menggunakan sistem dan 
bagaimana  bentuk  interaksi  pengguna  dengan  sistem.  Pihak 
yang berinteraksi dengan sistem secara umum disebut dengan 
tim  pengawas  dan  tim  penilai.  Tim  pengawas  terdiri  dari 
Pusdiklat  dan  Tim  Seleksi  Peserta  Diklat  Instansi  (TSPDI), 
sedangkan  tim  penilai  yang  terdiri  dari  pejabat  eselon  II  dan 
satuan kerja setingkat eselon II yang mendapat limpahan tugas 
dari pejabat eselon II untuk menilai calon peserta TB. Selain itu, 

Gambar 7. Diagram use-case 

Setelah  diagram  use-case  dibuat,  dilanjutkan  dengan 
perancangan basis data. Pada basis data terdapat 14 tabel utama 
yang diperuntukkan login pengguna, penilaian Fuzzy AHP, dan 
data  kepegawaian  calon  peserta  TB.  Rancangan  basis  data 
disajikan pada Gambar 8. 

E.  Implementasi Sistem Penilaian 

Dengan  metode  Fuzzy  AHP  diperoleh  kriteria  dan 
subkriteria  guna  penilaian  pegawai  untuk  mengikuti  TB  S-2 
dan S-3 yang didasarkan pada analisis dari Persyaratan Tugas 
Belajar dalam Peraturan Kepala (Perka) BPS Nomor 48 Tahun 
2012  tentang,  Tugas  Belajar,  Izin  Belajar,  dan  Kenaikan 
Pangkat  Penyesuaian 
Ijazah  Pegawai  Negeri  Sipil  di 
Lingkungan Badan Pusat Statistik; Surat Penawaran Beasiswa 
APBN BPS Tahun 2020; dan penyesuaian kebutuhan melalui 
wawancara dengan subject matter. Kriteria dan subkriteria ini 
disusun ke dalam bentuk hierarki seperti pada Gambar 9. 

F.  Implementasi Sistem Informasi 

Sistem dalam penelitian ini dibangun menggunakan bahasa 
pemrograman  PHP  versi  7.4.15  dengan 
framework 
CodeIgniter4  dan  basis  data  MySQL  phpMyAdmin.  Untuk 
tampilan  menggunakan  templat  dari  Adminty.  Penulisan 
pengodean program dengan text editor Visual Studio Code.  

Terdapat  landing  page  yang  dikunjungi  pertama  kali  oleh 
pengguna  sebelum  mengakses  halaman  berikutnya.  Pada 
landing  page  terdapat  menu  untuk  login,  informasi  kepada 
pengunjung,  dan  petunjuk  penggunaan  sistem.  Implementasi 
antarmuka dari landing page ditunjukkan pada Gambar 10. 

 5 / 8 

 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 8. Rancangan basis data 

Gambar 9. Hierarki dalam menilai calon peserta TB S-2/S-3  

Gambar 10. Implementasi tampilan antarmuka landing page 

Gambar 11. Menu dari role user Pengawas (kiri), Penilai (tengah), dan 

Peserta (kanan) 

Terdapat tiga role user utama yang dapat melakukan  login 
yaitu Pengawas, Penilai, dan Calon Peserta. Menu sidebar dari 
ketiga role user ditampilkan pada Gambar 11.  

Pengawas dapat  mengakses  halaman data  dan bobot untuk 
kriteria,  subkriteria,  dan  grade  alternatif,  serta  memberikan 
penilaian akhir. Implementasi antarmuka dari halaman data dan 
bobot kriteria seperti pada Gambar 12. 

Penilai  dapat  mengakses  halaman  penilaian  dan  melihat 
proses  pembobotan  setiap  calon  peserta  TB.  Implementasi 
antarmuka dari halaman ini seperti pada Gambar 13. 

Peserta  dapat  mengakses  halaman  input  penilaian  untuk 
mengisi  jawaban  dari  beberapa  pertanyaan.  Implementasi 
antarmuka dari halaman ini seperti pada Gambar 14. 

 6 / 8 

 
 
 
 
 
 
 
 
 
  
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

diujicobakan. Hasil uji coba dengan Black box pada Tabel III 
menunjukkan bahwa semua fungsi dalam sistem berjalan baik 
sesuai skenario. 

TABEL III 
HASIL UJI COBA MENGGUNAKAN BLACK-BOX TESTING 

Skenario Pengujian 
Menampilkan landing page 

Melakukan login 

dan 

data 

bobot 

bobot 

kriteria 

kriteria 

Menampilkan data dan bobot kriteria 
dan 
Menambahkan,  mengubah, 
menghapus kriteria 
Mengubah 
dan 
bobot 
menampilkan proses pembobotannya 
setelah 
yang 
dimasukkan bernilai konsisten. 
Menampilkan 
subkriteria 
Menambahkan,  mengubah, 
menghapus subkriteria 
Mengubah  bobot  subkriteria  dan 
menampilkan proses pembobotannya 
setelah  bobot 
subkriteria  yang 
dimasukkan bernilai konsisten. 
Menampilkan  data  dan  bobot  grade 
alternatif 
Menambahkan  dan  menghapus  data 
calon peserta TB 
Menambahkan  penilaian  (menjawab 
pertanyaan) kepada calon peserta TB 

dan 

Role User 
Pengawas, Penilai, dan 
Peserta 
Pengawas, Penilai, dan 
Peserta 

Hasil 
Berhasil 

Berhasil 

Pengawas dan Penilai  Berhasil 
Berhasil 

Pengawas 

Pengawas 

Berhasil 

Pengawas dan Penilai  Berhasil 

Pengawas 

Berhasil 

Pengawas 

Berhasil 

Pengawas dan Penilai  Berhasil 

Penilai 

Berhasil 

Penilai dan Peserta 

Berhasil 

Pengujian  dengan  SUS  bertujuan  untuk  mengevaluasi 
sejauh mana sistem dapat digunakan pengguna untuk mencapai 
tujuan tertentu dengan 10 pertanyaan dan 5 pilihan jawaban. Uji 
coba  ini  dilakukan  pada  8  responden  yang  mewakili  seluruh 
aktor. Hasil uji coba dengan SUS pada Tabel IV menunjukkan 
hasil  rata-rata  80,71  yang  berarti  termasuk  dalam  kategori 
sangat baik (grade B), serta bernilai lebih dari 68 yang artinya 
sistem  dapat  diterima  oleh  pengguna  dan  layak  untuk 
digunakan. 

TABEL IV 
HASIL UJI COBA MENGGUNAKAN SUS 

Pertanyaan ke- 

1  2  3  4  5  6  7  8 

5  2  4  1  5  1  4  2 

4  1  5  1  4  2  4  1 

4  2  4  1  5  2  3  2 

4  3  4  1  4  4  4  2 

4  1  4  1  5  2  4  1 

5  2  5  1  5  2  4  1 

4  1  5  1  3  2  4  2 

5  1  5  2  5  1  5  1 

9 

5 

4 

3 

4 

4 

5 

3 

5 

Total 

(12) × 
2,5 

Hasil 

10 

3 

1 

1 

3 

3 

2 

3 

2 

34 

35 

31 

27 

33 

36 

30 

38 

85 

87.5 

77.5 

67.5 

82.5 

90 

75 

95 

80,71 

Responden 

1 

2 

3 

4 

5 

6 

7 

8 

VII. 

PENUTUP 

Berdasarkan penelitian selama ini dapat ditarik kesimpulan 
sementara dari hasil penelitian dan pembahasan sebagai berikut. 

 7 / 8 

Gambar  12.  Implementasi  tampilan  antarmuka  data  kriteria  (atas)  dan 

pembobotan kriteria (bawah) 

Gambar 13. Implementasi tampilan antarmuka penilaian calon peserta  

Gambar 14. Implementasi tampilan antarmuka input penilaian calon peserta 

G.  Pengujian Sistem Informasi 

Sistem informasi diuji coba dengan menggunakan metode 
Black box testing dan SUS. Black box testing bertujuan untuk 
menguji  fungsionalitas  sistem  tanpa  melihat  struktur  kode 
internal  dan  informasi  detail  dari  perangkat  lunak  yang 

 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

[8]  Pusat  Pendidikan  dan  Pelatihan  Badan  Pusat  Statistik.  (2020,  5). 
Penawaran  Beasiswa  S2  dan  S3  APBN  BPS  Tahun  2020.  [Online]. 
Available: https://www.pusdiklat-bps.id/web/berita/26. 

[9]  Pusat  Pendidikan  dan  Pelatihan  Badan  Pusat  Statistik.  (2020,  5). 
Perpanjangan Batas Waktu Beasiswa S2 dan S3 APBN BPS Tahun 2020. 
[Online]. Available: https://www.pusdiklat-bps.id/web/berita/28. 

[10] Pusat  Pendidikan  dan  Pelatihan  Badan  Pusat  Statistik.  (2020,  6). 
Perpanjangan Batas Waktu Pendaftaran Beasiswa S2 dan S3 APBN BPS 
Tahun  2020.  [Online].  Available:  https://www.pusdiklat/bps.id/web/ 
berita/34. 

[11] Badan  Pusat  Statistik,  “Peraturan  Kepala  BPS  Nomor  48  Tahun  2012 
tentang  Tugas  Belajar,  Izin  Belajar,  dan  Kenaikan  Pangkat  Penyesuaian 
Ijazah  Pegawai  Negeri  Sipil  di  Lingkungan  Badan  Pusat  Statistik,” 
[Online].  Available:  https://jdih.bps.go.id/files/produk_hukum/perka/P0 
82 01248.pdf. 

[12] J. F. Chen, H. N. Shieh, and Q. H. Do, “Evaluating teaching performance 
based  on  fuzzy  AHP  and  comprehensive  evaluation  approach,”  Applied 
Soft Computing, vol. 28, pp. 100-108, March 2015. 

[13] A.  Emrouznejad  and  W.  Ho,  Analytic  Hierarchy Process  and  Fuzzy Set 
Theory in Fuzzy Analytic Hierarchy Process, pp. 1-10, Boca Raton: CRC 
Press, Taylor & Francis Group, 2017. 

[14] I. Afifah, “Sistem Pendukung Keputusan Rekomendasi Kenaikan Jabatan 
Dosen UIN Maulana Malik Ibrahim Malang dengan metode Fuzzy AHP,” 
B.S.  thesis,  Fak.  Sains  dan  Teknologi,  Jurusan  Teknik  Informatika, 
Universitas  Islam  Negeri  Maulana  Malik  Ibrahim,  Malang,  Indonesia. 
2018. [Online]. Available: http://etheses.uin-malang.ac.id/12558/1/14650 
029.pdf. 

[15] R.R.N.  Ramadhanty,  Y.  Anang,  Y.  Watanabe,  and  M.  Takahashi,  An 
Assessment of the Survey’s Officer Performance Using AHP, Proceedings 
of the 17th Asian Network for Quality Congress 2019, Thailand, October 
2019. [Online]. Available: https://stis.ac.id/sipadu/pegawai/upload_jurnal/ 
file_1600073035.pdf. 

[16] A. Faisol, M. A. Muslim, and H. Suyono,“Komparasi Fuzzy AHP dengan 
AHP  pada  Sistem  Pendukung  Keputusan  Investasi  Properti,”  in  Jurnal 
Electrics,  Electronics,  Communications,  Controls,  Informatics,  Systems 
(EECCIS) Universitas Brawijaya, Vol. 8, No. 2, pp. 123-128, Dec. 2014. 
[Online].  Available:  https://jurnaleeccis.ub.ac.id/index.php/eeccis/article/ 
download/249/219. 

[17] A. Ozdagoglu and G. Ozdagoglu, “Comparison of AHP and Fuzzy AHP 
for  The  Multi-criteria  Decision  Making  Process  with  Linguistic 
Evaluations”, Istanbul Ticaret Üniversitesi Fen Bilimleri Dergisi, pp. 65-
85, 2007. 

[18] D.  Y.  Chang,  “Theory  and  Methodology:  Applications  of  the  extent 
analysis  method  on  fuzzy  AHP,”  European  Journal  of  Operational 
Research, vol. 9, pp. 649-655, Dec. 1996. 

[19] J. L. Whitten and L. D. Bentley, Systems Analysis & Design Methods. New 

York: McGraw-Hill/Irwin, 2007. 

1.  Berdasarkan  analisis  yang  dilakukan  pada  proses 
pemberian  rekomendasi  kepada  calon  peserta  Tugas 
Belajar  terdapat  celah  risiko  pengambilan  keputusan 
dengan  subjektivitas  yang 
itu 
diperlukan  pengembangan  Sistem  Informasi  Pemberian 
Rekomendasi  Tugas  Belajar  Jenjang  Pendidikan  S-2  dan 
S-3  di  BPS  Berbasis  Web  menggunakan  Metode  Fuzzy 
AHP. 

tinggi.  Oleh  karena 

2.  Telah dibangun sistem pemberian rekomendasi yang dapat 
menilai  calon  peserta  Tugas  Belajar  menggunakan 
indikator  berupa  kriteria  dan  subkriteria  yang  digunakan 
pada metode Fuzzy AHP yang didasarkan pada Persyaratan 
Tugas  Belajar  dalam  Perka  BPS  Nomor  48  tahun  2012, 
Surat Penawaran Beasiswa APBN BPS Tahun 2020, serta 
penyesuaian dengan kebutuhan melalui wawancara. 
Indikator  kriteria  dan  subkriteria  penilaian  beserta 
bobotnya pada sistem informasi bersifat dinamis atau dapat 
diubah  oleh  pihak  tertentu.  Sebagai  inisiasi,  terdapat  4 
kriteria dan 16 subkriteria. 

3. 

4.  Sistem  informasi  dengan  menggunakan  metode  Fuzzy 
AHP dapat memberikan keputusan terhadap calon peserta 
Tugas  Belajar  berupa  sangat  direkomendasikan,  cukup 
direkomendasikan, dan tidak direkomendasikan. 

5.  Telah dibangun sistem informasi pemberian rekomendasi 
berbasis web dengan hasil evaluasi dengan uji coba black 
box yang menunjukkan semua fungsi pada sistem berjalan 
dengan  baik,  evaluasi  dengan  kuesioner  SUS  (System 
Usability Scale) yang memberikan hasil 80,71 yang artinya 
sistem  dapat  diterima  dengan  baik  oleh  pengguna  dan 
layak untuk digunakan. 

Saran  yang  dapat  diberikan  untuk  penelitian  selanjutnya 
yaitu  diharapkan  pengembangan  sistem  dikolaborasikan 
dengan  data  kepegawaian  maupun  Simdiklat  BPS  sehingga 
beberapa data atau isian dapat terisi secara otomatis, serta dapat 
diujicobakan dengan persyaratan beasiswa APBN BPS periode 
terbaru. 

DAFTAR PUSTAKA 
[1]  Badan  Pusat  Statistik,  “Laporan  Kinerja  Badan  Pusat  Statistik  2019,” 
2020.  [Online].  Available:  https://www.bps.go.id/website/fileMenu/Lap 
oran-Kinerja-BPS-2019-rev.pdf. 

[2]  Pusat Pendidikan dan Pelatihan Badan Pusat Statistik, “Laporan Kinerja 
Pusdiklat  BPS 2015,” 2016. [Online].  Available:  https://www.pusdiklat-
bps.id/storage/app/public/file_monev/1602037922_5f7d28a2144e7.pdf. 
[3]  Pusat Pendidikan dan Pelatihan Badan Pusat Statistik, “Laporan Kinerja 
Pusat  Pendidikan  dan  Pelatihan  Badan  Pusat  Statistik  2016,”  2017. 
[Online]. Available: https://www.pusdiklat-bps.id/storage/app/public/file_ 
monev/1602037778_5f7d28127a2e0.pdf. 

[4]  Pusat Pendidikan dan Pelatihan Badan Pusat Statistik, “Laporan Kinerja 
Pusat  Pendidikan  dan  Pelatihan  Badan  Pusat  Statistik  2017,”  2018. 
[Online]. Available: https://www.pusdiklat-bps.id/storage/app/public/file_ 
monev/1602037613_5f7d276d980c5.pdf. 

[5]  Pusat Pendidikan dan Pelatihan Badan Pusat Statistik, “Laporan Kinerja 
Pusdiklat  BPS 2018,” 2019. [Online].  Available:  https://www.pusdiklat-
bps.id/storage/app/public/file_monev/1602037356_5f7d266caea6c.pdf. 
[6]  Pusat  Pendidikan  dan  Pelatihan  Badan  Pusat  Statistik,  “Laporan 
Akuntabilitas Kinerja Instansi Pemerintah (LAKIP) Pusdiklat BPS Tahun 
Anggaran 2019,” 2020. [Online]. Available: https://www.pusdiklat-bps.id/ 
storage/app/public/file_monev/1602037235_5f7d25f3bdc04.pdf. 

[7]  Pusat Pendidikan dan Pelatihan Badan Pusat Statistik, “Rencana Strategis 
Pusat  Pendidikan  dan  Pelatihan  BPS  2015-2019,”  2015.  [Online]. 
Available:  https://www.pusdiklat-bps.id/storage/app/public/file_monev/ 
1602037949_ 5f7d28bd933ac.pdf. 

 8 / 8 

 
 
 
"
221709923,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Klasifikasi Multimodal pada Produk Marketplace 
Menggunakan Convolutional Neural Network (CNN) 
untuk Mendukung Penyusunan Statistik E-
Commerce 

Nurul Handayani (221709923, 4SD2) 
Dosen Pembimbing: Takdir, SST., M.T. 

Ringkasan—  Perkembangan  teknologi  informasi  membuat 
aktivitas berbelanja secara online berkembang pesat. Terkait hal 
tersebut,  Badan  Pusat  Statistik  (BPS)  melakukan  penyusunan 
Statistik  E-Commerce  untuk  memetakan  perkembangan  e-
commerce  di  Indonesia,  dengan  terdapat  marketplace  salah 
satunya.  Produk-produk  marketplace  dapat  dikumpulkan 
datanya berdasarkan klasifikasi jenis produknya. Dalam hal ini, 
BPS  memiliki  standar  klasifikasi  produk  yang  berbeda  dengan 
standar klasifikasi produk di setiap marketplace. Oleh karena itu, 
perlu  dibuat  sistem  pengklasifikasian  produk  marketplace  yang 
sesuai  dengan  standar  klasifikasi  BPS  untuk  mendukung 
penyusunan  Statistik  E-Commerce.  Pada  penelitian 
ini, 
klasifikasi  produk  dilakukan  menggunakan  data  multimodal 
dengan  pengkodean  teks  pada  nama  produk  yang  disematkan 
pada  gambar  produk  sehingga  menghasilkan  gambar  baru. 
Metode  untuk  klasifikasi produknya yaitu  Convolutional Neural 
Network  (CNN)  menggunakan  fungsi  Tensorflow.  Didapatkan 
hasil  akurasi  klasifikasi  produk  marketplace  baik  pada  data 
validasi  dan  data  test-nya  yaitu  berturut-turut  sebesar  90,8% 
itu,  peneliti  membandingkan  akurasi 
dan  91,16%.  Selain 
klasifikasi  pada  data  gambar  produk  dengan  pengkodean  teks 
dengan  data  gambar  produk  asli.  Peneliti  juga  menguji 
pemodelan klasifikasi dengan data anomali. 

Kata  Kunci—  klasifikasi  produk,  CNN,  multimodal, 

pengkodean teks 

I.  LATAR BELAKANG 

Seiring  dengan  perkembangan  teknologi  informasi  yang 
sangat  pesat  membuat  beberapa  aktivitas  dapat  dilakukan 
secara  digital  melalui  perangkat  media  elektronik,  termasuk 
aktivitas  berbelanja  salah  satunya.  Berbelanja  secara  online 
bagi  beberapa  pihak  dirasa  lebih  praktis,  cepat,  dan  efisien. 
Perdagangan  secara  online  melalui  media  elektronik  seperti 
handphone,  laptop,  dan  perangkat  lainnya  atau  yang  disebut 
dengan  elektronik  commerce  atau  e-commerce  dapat 
memudahkan  baik  dari  sisi  penjual  maupun  pembelinya.  Hal 
ini  menyebabkan  maraknya  perdagangan  via  online,  salah 
satunya  melalui  media  marketplace.  Marketplace  merupakan 
lokasi  jual  beli  produk  dimana  seller  dan  juga  konsumen 
bertemu  di  marketplace/platform  digital 
[3].  Pesatnya 
perkembangan e-commerce tersebut  tentunya berpotensi pada 
pertumbuhan  ekonomi  bagi  Indonesia.  Terkait  hal  tersebut, 
Badan  Pusat  Statistik  (BPS)  selaku  badan  statistik  resmi  di 
Indonesia  melakukan  pengumpulan  data  terkait  Statistik  E-

Commerce  dan  mempublikasi  hasilnya  setiap  tahun  guna 
memetakan perkembangan e-commerce di Indonesia [3]. 

di  marketplace. 

Pada  perhitungan  Statistik  E-Commerce  tersebut  salah 
satunya  dapat  dilihat  berdasarkan  produk  barang/jasa  yang 
Produk-produk  marketplace 
dijual 
diklasifikasikan berdasarkan kategori jenis produknya. Dalam 
hal ini, BPS memiliki standar pengklasifikasian  produk yang 
berbeda  dengan  sistem  pengklasifikasian  produk  di  setiap 
marketplace.  Oleh  karena 
sistem 
pengklasifikasian  produk  marketplace  yang  sesuai  dengan 
kebutuhan  BPS  dalam  klasifikasi  produk  untuk  mendukung 
penyusunan Statistik E-Commerce.  

itu,  perlu  dibuat 

Pada  penelitian 

produk  marketplace 

sehingga  menghasilkan 

ini,  klasifikasi  produk  dilakukan 
menggunakan  metode  Convolutional  Neural  Network  (CNN) 
dengan pendekatan multimodal yang menggabungkan gambar 
dengan  nama  produk,  dimana  nama-nama  produk  dilakukan 
pengkodean  teks  dan  disematkan  pada  gambar  produk  yang 
suatu  gambar  baru. 
berkaitan 
Penggunaan  pengkodean  teks  atau  text  encoding  pada  nama 
produk  yang  disematkan  pada  gambar  produk  berdasarkan 
penelitian  [5]  dapat  meningkatkan  performa  klasifikasi  dan 
menghilangkan 
ambiguitas  pada  produk  marketplace. 
Ambiguitas pada produk  marketplace yang dimaksud adalah, 
pada 
yang 
mencantumkan  nama  produk  dengan  jelas  namun  gambar 
produk  tidak  teridentifikasi  dengan  jelas  kategorinya,  atau 
penjual  yang  mencantumkan  gambar  produk  dengan  jelas 
namun  nama  produk  tidak  dapat  dengan  jelas  teridentifikasi 
kategorinya.  Sehingga  hasil  akurasi  klasifikasi  pada  data 
multimodal  dapat  meningkat  jika  dibandingkan  dengan  data 
unimodal yang hanya menggunakan teks saja atau gambar saja.  
Selain  itu,  metode  Convolutional  Neural  Network  (CNN) 
digunakan untuk proses klasifikasi produk marketplace. CNN 
merupakan  metode  untuk  klasifikasi  gambar  yang  dapat 
melakukan  pelatihan  data  untuk  identifikasi  gambar  secara 
otomatis  [8].  Berdasarkan  [1]  dan  [6],  metode  CNN 
menghasilkan akurasi yang tinggi dan lebih baik pada masalah 
klasifikasi  dengan  kategori  kelas  yang  banyak  dibandingkan 
dengan metode klasifikasi gambar lainnya. 

terdapat 

penjual 

Pemodelan CNN tersebut diimplementasikan menggunakan 
library  Tensorflow.  Tensorflow  merupakan  salah  satu 
perangkat  lunak  open  source  yang  terdapat  pada  bahasa 
ini 
pemrograman  Python 

[9].  Penggunaan  Tensorflow 

 1 / 8 

 
 
 
 
berdasarkan  penelitian  [9]  dapat  membuat  pemodelan  CNN 
menjadi lebih cepat.  

Untuk  meningkatkan  performa  klasifikasi,  model  CNN 
dilatih  menggunakan  parameter  ukuran  batch  (batch  size). 
Ukuran batch merupakan parameter yang paling berpengaruh 
untuk  meningkatkan  akurasi  klasifikasi  [10].  Parameter  ini 
jumlah  sampel  pelatihan  yang  akan 
merepresentasikan 
digunakan  selama  pelatihan  untuk  membuat  satu  pembaruan 
pada  parameter  jaringan  [10].  Penggunaan  ukuran  batch  ini 
dapat mengefisiensikan waktu pelatihan model CNN [11]. 

Peneliti  melakukan  perbandingan  akurasi  klasifikasi  pada 
gambar  produk  yang  dilakukan  text  encoding  pada  nama 
produknya  dengan  gambar  produk  asli  yang  tidak  dilakukan 
text  encoding.  Hal  ini  dilakukan  untuk  melihat  bagaimana 
keterbandingan  hasil  akurasinya.  Selain  itu,  peneliti  juga 
melakukan  pengujian  model  klasifikasi  pada  data  anomali, 
yaitu data gambar produk yang sulit diidentifikasi kategorinya 
oleh  pemodelan  CNN  karena  gambar  produk 
tidak 
menampilkan karakteristik produk dengan jelas. 

II.  TUJUAN PENELITIAN 

Tujuan pada penelitian ini adalah sebagai berikut : 
1.  Merancang  mekanisme  untuk  klasifikasi  produk 
marketplace  agar  sesuai  dengan  standar  klasifikasi 
yang ditetapkan BPS. 

2.  Mengevaluasi  mekanisme  pengklasifikasian  produk 

marketplace yang disusun. 

III. PENELITIAN TERKAIT  
Berikut ini beberapa penelitian terkait implementasi metode 
Multimodal  Fusion  dan  metode  Convolutional  Neural 
Network  (CNN)  yang  dijadikan  bahan  acuan  pada  penelitian 
ini.  

Pada  penelitian  [5]  dilakukan  klasifikasi  multimodal 
menggunakan  gambar  dan  deskripsi  teks  pada  produk  e-
commerce.  Produk  e-commerce  diklasifikasikan  dengan 
penyematan text encoding pada deskripsi teks dengan gambar 
produk  dan  menghasilkan  suatu  gambar  yang  disebut 
information  enriched  image.  Proses  text  encoding  dilakukan 
menggunakan metode CNN untuk klasifikasi teks. Gambaran 
proses  ini  dapat  dilihat  pada  Gambar  1.  Kemudian,  dari 
gambar  multimodal  yang  diperoleh  dilakukan  proses 
klasifikasi  produk  menggunakan  metode  CNN.  Pada 
penelitian 
[5],  dilakukan  perbandingan  performa  hasil 
klasifikasi  produk  dengan  menggunakan  teks  saja,  gambar 
saja,  dan  gabungan  teks  dan  gambar  dari  hasil  penyematan 
text  encoding  pada  gambar  produk.  Hasilnya  menunjukkan 
bahwa  penggabungan  teks  dengan  gambar  untuk  klasifikasi 
menghasilkan akurasi yang lebih tinggi.  Selain itu, dilakukan 
perbandingan  performa  akurasi  klasifikasi  menggunakan 
pendekatan multimodal lainnya, yaitu metode early fusion dan 
late 
akurasi 
menggunakan  pendekatan  multimodal  dengan  penyematan 
text encoding pada gambar produk menghasilkan nilai akurasi 
yang lebih tinggi. 

fusion.  Hasilnya  menunjukkan 

bahwa 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 1. Gambaran Pengkodean Teks Pada Nama Produk dengan Gambar 
Produk. 

Pada  penelitian 

[9],  dilakukan  klasifikasi  gambar 
menggunakan  Advanced  Convolutional  Neural  Network  di 
mana  klasifikasi  produk  dilakukan  dengan  kerangka  kerja 
Tensorflow menggunakan bahasa pemrograman  Python. Pada 
penelitian  tersebut  juga  dilakukan  perbandingan  performa 
akurasi  klasifikasi  dengan  metode  lain,  yaitu  DNN,  CNN 
tanpa  kerangka  kerja  Tensorflow,  dan  Transfer  Learning. 
Hasilnya  menunjukkan bahwa  penggunaan  metode  Advanced 
CNN menghasilkan nilai akurasi lebih tinggi, yaitu sebesar 95% 
dibandingkan dengan metode lainnya. 

Pada  penelitian  [6]  dilakukan  perbandingan  performa 
klasifikasi  gambar  pada  metode  Support  Vector  Machine 
(SVM)  dengan  CNN  dan  didapatkan  bahwa  CNN 
memberikan  akurasi  yang  konsisten  baik  pada  masalah 
klasifikasi biner maupun  multiclass, sedangkan untuk metode 
SVM  hanya  memberikan  akurasi  yang  lebih  baik  pada 
klasifikasi  biner.  Pada  penelitian  [6]  penerapan  metode  CNN 
dibangun dengan package Tensorflow dan Keras. 

Pada  penelitian  [8]  dan  [4]  dilakukan  klasifikasi  gambar 
menggunakan  metode  CNN,  yang  dibangun  menggunakan 
kerangka  Tensorflow  dengan  jaringan  yang  dapat  bekerja 
pada  CPU.  Dari  penelitian  tersebut,  peneliti  mengidentifikasi 
bagaimana  struktur  CNN  diimplementasikan  dan  mengamati 
bagaimana  struktur  jaringan  CNN  dibangun  untuk  proses 
klasifikasi.  

Pada  penelitian [10],  dilakukan  penelitian  terkait performa 
pemodelan CNN yang dilatih menggunakan parameter ukuran 
batch. Dari penelitian tersebut didapatkan bahwa penggunaan 
ukuran  batch  mempengaruhi  proses  pelatihan  CNN  pada 
aspek waktu dan jumlah overfitting. 

IV. METODE PENELITIAN  

A.  SUMBER DATA 

Pada penelitian ini, dataset yang digunakan bersumber dari 
halaman  web  marketplace  Tokopedia  menggunakan  nama 
produk dan gambar produk. Kategori untuk klasifikasi produk 
yang  digunakan  adalah  kategori  berdasarkan  jenis  barang 
yang diimplementasikan oleh BPS dalam penyusunan Statistik 
E-Commerce [3]. Penyusunan data training dilakukan dengan 
terlebih dahulu mengamati produk-produk pada halaman web 
Tokopedia  untuk  melihat  bagaimana  karakteristik  produk-

 2 / 8 

 
 
 
produknya  dan  mengumpulkan  datanya  secara  manual  pada 
setiap produk untuk memastikan kebenaran data training yang 
disusun  sesuai  dengan  kategorinya.  Selain  itu,  dilakukan 
verifikasi data oleh 3 orang selain peneliti untuk memberikan 
jaminan  bahwa  data  training  yang  disusun  sudah  sesuai 
dengan kategorinya. 

klasifikasi 

percobaan 

Peneliti  melakukan 

dengan 
mengambil 6 kategori untuk klasifikasi produk, yaitu 2 produk 
elektronik  berupa  oven  dan  printer,  2  produk  gadget  berupa 
laptop  dan  handphone,  dan  2  produk  furnitur  rumah  tangga 
berupa  kasur  dan  sofa.  Data  training  yang  digunakan  untuk 
pemodelan  klasifikasi  berjumlah  1000  produk  untuk  setiap 
kategorinya  dan  data  test  berjumlah  100 produk  untuk  setiap 
kategori,  sehingga 
training  yang  digunakan 
berjumlah 6000 produk, dengan 16% data dipilih secara  acak 
untuk  data  validasi,  sehingga  data  training  untuk  pelatihan 
model klasifikasinya berjumlah 5040 produk dan data validasi 
untuk perhitungan akurasi model klasifikasinya berjumlah 960 
produk,  serta 
test  untuk  pengujian  model 
klasifikasinya berjumlah 600 produk. 

total  data 

total  data 

B.  METODOLOGI  

Pada  penelitian 

ini  digunakan  2  metode  untuk 
yaitu  metode 
produk  marketplace, 
pengklasifikasian 
Multimodal  Fusion  dengan  pengkodean  teks  pada  nama 
produk  dan  disematkan  pada  gambar  produk  yang  berkaitan, 
dan  metode  Convolutional  Neural  Network  (CNN)  untuk 
proses klasifikasi pada gambar produk hasil penyematan teks 
yang dikodekan.  

Metode  Multimodal  Fusion,  merupakan  implementasi  dari 
penelitian [5]. Metode Multimodal Fusion merupakan metode 
yang menggabungkan teks dengan gambar produk. Metode ini 
bekerja dengan pengkodean pada dokumen teks, dalam hal ini 
adalah  nama  produk,  dan  disematkan  pada  gambar  produk 
sehingga  menghasilkan  suatu  data  multimodal  baru.  Proses 
pengkodean  teks  dilakukan  dengan  menggunakan  metode 
CNN  untuk  klasifikasi  teks.  Pada  jaringan  CNN  tersebut 
terdapat  convolutional  layer,  maxpooling  layer,  dan  fully 
connected 
fully 
connected  layer,  fitur  teks  diekstraksi  dan  ditransformasi 
menjadi  pengkodean  RGB  sehingga  dapat  dilapiskan  dengan 
gambar  produk  yang  berkaitan.  Pada  fully  connected  layer 
tersebut,  dilakukan  perubahan  bentuk  yang  mengubah  array 
menjadi  sebuah  gambar  yang  merepresentasikan  pengkodean 
teks untuk dilapiskan pada gambar aslinya [5]. Gambaran dari 
proses ini dapat dilihat pada Gambar 1.  

layer.  Pada  proses  klasifikasi 

teks  di 

Selanjutnya  yaitu  proses  klasifikasi  produk  marketplace 
menggunakan metode  Convolutional Neural Network  (CNN). 
Data yang digunakan pada tahap ini adalah data gambar hasil 
pengkodean  nama  produk  pada  gambar  produk.  Pemodelan 
CNN  ini  dilakukan  dengan menggunakan  fungsi  Tensorflow. 
Struktur jaringan CNN memiliki input layer, output layer, dan 
hidden  layer.  Pada  hidden  layer  terdiri  dari  convolutional 
layer,  flatten  layer,  dan  fully  connected  layer  [8].  Lapisan 
pertama  yaitu  convolutional  layer,  tugas  utamanya  adalah 
untuk  mengekstraksi 
[8].  CNN  bekerja  dengan 
memindahkan  filter  pada  input  gambar  yang  berfungsi  untuk 
mengenali  pola  di  seluruh  input  gambar.  Input  gambar 

fitur 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

diproses  pada  lapisan  konvolusi  menggunakan  filter-weight 
[11]. Filter tersebut berukuran lebih kecil dari ukuran gambar. 
Pada  lapisan  ini  dilakukan  perkalian  matriks  pada  filter  dan 
potongan  gambar  berukuran  filter.  Filter  digeser  ke  seluruh 
gambar untuk menghitung  output di seluruh gambar.  Lapisan 
selanjutnya  yaitu  maxpooling 
layer,  digunakan  untuk 
mengurangi  dimensi  ukuran  spasial  (tinggi,  lebar)  gambar. 
Maxpooling 
layer  mengurangi  ukuran  gambar  menjadi 
setengah  lebih  kecil  dari  ukuran  sebelumnya.  Selanjutnya 
flatten  layer,  digunakan  untuk  mengubah  gambar  3  dimensi 
menjadi  satu  dimensi.  Pada  lapisan  ini,  output  dari  lapisan 
konvolusi  sebelumnya  diratakan  strukturnya  menjadi  sebuah 
vektor tunggal yang dapat digunakan pada lapisan selanjutnya, 
yaitu fully connected layer. Fully connected layer melakukan 
tugas klasifikasi gambar berdasarkan kelas/kategori yang telah 
ditentukan [8]. 

V.  KERANGKA PIKIR 

Kerangka  pikir  pada  penelitian  ini  ditunjukkan  pada 
Gambar  2  yang  merupakan  gambaran  bagaimana  penelitian 
ini dilakukan. 

Gambar 2. Kerangka Pikir Penelitian. 

Tahap  pertama  yaitu  mengidentifikasi  permasalahan,  di 
mana  BPS  memiliki  standar  klasifikasi  produk  yang  berbeda 
dengan  sistem  klasifikasi  di  setiap  marketplace.  Dari 
permasalahan tersebut kemudian ditentukan tujuan penelitian, 
yaitu  merancang  mekanisme  klasifikasi  produk  marketplace 
yang  sesuai  kebutuhan  BPS  untuk  mendukung  penyusunan 
Statistik E-Commerce dan mengevaluasi hasilnya. Setelah itu 
dilakukan studi literatur untuk menentukan metode klasifikasi 
produk.  Peneliti  menerapkan  2  metode  untuk  klasifikasi 
produk,  yang  pertama  yaitu  metode  multimodal  fusion  untuk 
menggabungkan  nama  dengan  gambar  produk  menggunakan 
text encoding pada nama produk dan disematkan pada gambar 
produk  sehingga  menghasilkan  data  multimodal  baru  berupa 

 3 / 8 

 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

suatu gambar. Kemudian mengimplementasikan metode CNN 
untuk  klasifikasi  gambar  hasil  penggabungan  teks  dengan 
gambar  aslinya.  Kemudian  diterapkan  metode  CNN  untuk 
klasifikasi  gambar  hasil  penggabungan  teks  dengan  gambar 
produk.  Dalam  mengimplementasikan  metode 
tersebut, 
dilakukan pengumpulan data berupa nama dan gambar produk 
marketplace,  kemudian  dilakukan  penggabungan  nama 
dengan  gambar  produk  dengan  text  encoding,  selanjutnya 
dilakukan  perancangan  model  CNN  untuk  pelatihan  data 
training,  setelah 
terbentuk  diuji 
itu  model  CNN  yang 
menggunakan data test, dan didapatkan akurasi klasifikasinya, 
baik pada data validasi dari pelatihan modelnya maupun pada 
data test dari pengujian model klasifikasi yang terbentuk. Pada 
tahap  terakhir  dari  penelitian  ini,  peneliti  mengevaluasi 
rancangan klasifikasi yang disusun. 

VI. HASIL DAN PEMBAHASAN 

Gambar 4. Tahapan Penerapan Metode CNN 

Tahap  pertama  yaitu  mempersiapkan  dataset  berupa  data 
multimodal,  yang  merupakan  penggabungan  nama  produk 
text  encoding. 
dengan  gambar  produk  menggunakan 
Kemudian  menginstall  package  yang  diperlukan  untuk 
pemodelan,  salah  satunya  yaitu  package  Tensorflow  untuk 
membangun  lapisan-lapisan  pada  jaringan  CNN.  Setelah  itu 
menginisialisasi hyperparameter untuk pemodelan CNN. Pada 
penelitian  ini,  hyperparameter  yang  digunakan  adalah  input 
gambar  berukuran  128x128  piksel,  filter  di  setiap  lapisan 
konvolusi  berukuran  3x3  piksel,  jumlah  filter  untuk  lapisan 
konvolusi ke-1, 2, dan 3 berturut-turut adalah 32, 32, dan 64. 
Selain itu, total neuron pada fully connected layer adalah 128 
neuron,  color  channel  adalah  3,  dan  ukuran  batch-nya  yaitu 
32.  Tahap  selanjutnya  yaitu  pemodelan  CNN  dengan 
membangun  lapisan-lapisan  jaringan  CNN  menggunakan 
dari 
fungsi 
convolutional  layer  yang  diikuti  maxpooling  layer,  keduanya 
sebanyak 3 lapisan berturut-turut, kemudian flatten layer, dan 
fully connected layer. Pada tahap ini digunakan fungsi aktivasi 
ReLU.  Hasil  pemodelan  metode  CNN  menggunakan  fungsi 
Tensorflow  ditunjukkan  pada  Gambar  5.  Pemodelan  ini 
dihasilkan berdasarkan inisialisasi hyperparameter yang telah 
ditentukan. 

Lapisan-lapisannya 

Tensorflow. 

terdiri 

Pada  penelitian 

ini,  peneliti  melakukan  percobaan 
klasifikasi  pada  sistem  operasi  Windows  10  dengan  sistem 
CPU,  menggunakan  bahasa  pemrograman  Python  dan 
membuat model CNN berbasis fungsi Tensorflow. 

Implementasi  metode  multimodal  fusion  digunakan  agar 
diperoleh  data  multimodal.  Pada  penelitian  ini,  nama  produk 
yang  ditransformasi  kemudian  disematkan  menjadi  gambar 
RGB dengan ukuran gambar diatur menjadi 227 x 227 piksel. 
Gambar 3 menunjukkan contoh produk sebelum dan sesudah 
proses  text  encoding  pada  gambar  produk.  Gambar-gambar 
produk  hasil  text  encoding  tersebut  kemudian  menjadi  input 
pada struktur jaringan CNN untuk proses klasifikasi gambar. 

Gambar 3. Contoh Produk Sebelum dan Sesudah Proses Text Encoding 

Selanjutnya  yaitu 

implementasi  metode  Convolutional 
Neural Network (CNN) yang dapat melakukan pelatihan data 
untuk  identifikasi  objek  pada  suatu  gambar.  Metode  ini 
digunakan  untuk  proses  klasifikasi  gambar.  Gambar  4  di 
berikut  merupakan  tahapan  penerapan  metode  CNN  untuk 
klasifikasi produk.  

Gambar 5. Hasil Pemodelan CNN menggunakan Fungsi Tensorflow 

 4 / 8 

 
 
 
 
 
 
 
 
Tahap  selanjutnya  yaitu  pelatihan  model  CNN  dengan 
menggunakan  optimizer  Adam,  dengan  epoch  sebanyak  25, 
Categorical  Crossentropy  sebagai  fungsi  loss  dan  learning 
rate  sebesar  0.0001.  Pada  pelatihan  model  CNN 
ini, 
didapatkan nilai akurasi dari perhitungan jumlah produk yang 
benar  diklasifikasikan  per  total  produk  pada  data  validasi. 
Pada  tahap  ini  peneliti  melakukan  perbandingan  performa 
klasifiksi  gambar  produk  yang  telah  dilakukan  text  encoding 
dengan  gambar  produk  asli  tanpa  proses  text  encoding.  Hal 
tersebut  dilakukan  karena  data  multimodal  untuk  klasifikasi 
produknya  berupa  suatu  gambar,  sehingga  peneliti  ingin 
melihat  bagaimana  keterbandingan  akurasi  klasifikasi  pada 
data  multimodal yang berbentuk gambar dengan data  gambar 
produk asli.  Setelah itu, dilakukan pengujian dengan data test. 
Peneliti mengukur akurasi dari data test untuk melihat apakah 
itu, 
hasil  pemodelannya  overfitting  atau 
dilakukan juga pengujian pada data anomali. Data anomali ini 
merupakan data gambar produk yang sulit untuk diidentifikasi 
kategorinya  oleh  model  klasifikasi  karena  gambar  tersebut 
tidak menampilkan karakteristik produk dengan jelas. Gambar 
6 berikut ini merupakan contoh data anomali untuk pengujian 
model klasifikasi.  

tidak.  Selain 

Gambar 6. Contoh Gambar Produk Pada Data Anomali  

Hasil akurasi klasifikasi produk dapat dilihat pada Tabel I. 
Peneliti  melakukan  perbandingan  akurasi  klasifikasi  pada 
gambar  produk  yang  dilakukan  text  encoding  dan  tanpa  text 
encoding.  Selain  itu  peneliti  juga  membandingkan  akurasi 
hasil pemodelan CNN pada data validasi dengan akurasi pada 
data test-nya. 

TABEL I 
PERBANDINGAN AKURASI KLASIFIKASI PRODUK 

Data validasi 

Data test 

Dataset 

Akurasi  

Jumlah 
produk 
yang 
benar 
diprediksi 

Akurasi  

Jumlah 
produk 
yang benar 
diprediksi 

Dengan 
text 
encoding 
Tanpa text 
encoding 

90,8% 

872/960 

91,16% 

547/600 

74,4% 

714/960 

71% 

426/600 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

bahwa 

klasifikasi 

Hasilnya  menunjukkan 

produk 
menggunakan  gambar  yang  dilakukan  text  encoding  pada 
nama produknya menghasilkan nilai akurasi yang lebih tinggi. 
Akurasi  pada  data  validasinya  yaitu  sebesar  90.8%  dengan 
872  produk  diklasifikasikan  dengan  benar  dari  total  960 
produk pada data validasi, dan akurasi pada data test-nya yaitu 
sebesar  91,6%  dengan  547  produk  diklasifikasikan  dengan 
benar  dari  total  600  produk  pada  data  test.  Sedangkan,  pada 
pemodelan  klasifikasi  produk  dengan  gambar  asli  tanpa  text 
encoding menghasilkan nilai akurasi yang lebih rendah, yaitu 
sebesar 74,4% pada data validasinya, dengan 714 produk tepat 
diklasifikasikan sesuai kategorinya dari total 960 produk, dan 
akurasi pada data  test-nya yaitu sebesar 71%, dengan produk 
yang tepat diklasifikasikan sesuai kategorinya yaitu sebanyak 
426 dari total 600 produk pada data test. Hasil ini sudah sesuai 
dengan  penelitian  [5]  yang  menyatakan  bahwa  pengkodean 
teks  pada  gambar  produk  menghasilkan  nilai  akurasi  yang 
lebih tinggi. 

Tabel  II  di  bawah  ini  merupakan  confusion  matrix  hasil 
pemodelan  CNN  pada  gambar  produk  yang  dilakukan  text 
encoding  pada  data  validasi.  Pada  tabel  tersebut,  baris 
menunjukkan  total  produk  yang  diprediksi  oleh  pemodelan 
CNN  pada  setiap  kategorinya.  Misalkan  pada  produk  oven, 
dari  172  produk  pada  data  validasi,  157  produk  diprediksi 
dengan  benar,  sedangkan  5  produk  salah  diklasifikasikan  ke 
dalam  kategori  printer,  2  produk  menjadi  kasur,  2  produk 
lainnya  menjadi  sofa,  5  produk  menjadi  handphone  dan  1 
produk diprediksi menjadi laptop.  

TABEL II 
CONFUSION MATRIX HASIL PEMODELAN CNN PADA DATA 
VALIDASI DENGAN ENCODING TEXT 

Oven   Printer  Kasur  Sofa  HP  Laptop 

Oven 

157 

Printer 

Kasur 

Sofa 

HP 

Laptop 

0 

1 

2 

8 

2 

5 

156 

5 

3 

3 

3 

2 

1 

130 

2 

0 

1 

2 

4 

7 

152 

2 

5 

5 

2 

3 

2 

142 

1 

4 

4 

0 

6 

3 

135 

Sedangkan  Tabel  III  di  bawah  ini  merupakan  confusion 
matrix  hasil  pemodelan  CNN  pada  data  validasi  tanpa  text 
encoding.  Dapat  dilihat  pada  Tabel  III  bahwa  kebanyakan 
produk  pada  data  validasi  tanpa  text  encoding  menghasilkan 
kesalahan  yang  lebih  banyak  dibandingkan  pada  Tabel  II, 
yaitu data validasi dengan proses text encoding. 

 5 / 8 

 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
TABEL III 
CONFUSION MATRIX HASIL PEMODELAN CNN PADA DATA 
VALIDASI TANPA ENCODING TEXT 

TABEL V 
TOTAL KESALAHAN KLASIFIKASI PADA DATA VALIDASI TANPA  
TEXT ENCODING 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

oven   printer 

kasur 

sofa 

laptop 

Data tanpa text encoding 

hp 

10 

1 

9 

oven 

105 

printer 

11 

kasur 

sofa 

hp 

laptop 

4 

5 

1 

8 

18 

136 

12 

14 

2 

18 

3 

6 

110 

9 

1 

8 

17 

11 

16 

6 

12 

105 

13 

160 

9 

98 

4 

5 

7 

3 

3 

Berdasarkan  confusion matrix pada Tabel II dan Tabel III, 
dapat  dirangkum  hasilnya  untuk  melihat  karakteristik  produk 
yang diklasifikasikan pada Tabel IV dan Tabel V di bawah ini. 
Dapat dilihat bahwa secara keseluruhan, klasifikasi pada data 
gambar yang tidak dilakukan text encoding menghasilkan total 
produk yang salah diklasifikasikan lebih banyak dibandingkan 
pada data gambar yang dilakukan text encoding. 

Pada Tabel IV yang menunjukkan jumlah kesalahan produk 
yang  dikategorikan  pada  data  validasi  yang  dilakukan  text 
encoding,  dengan  kategori  produk  kasur,  hp,  dan  laptop 
merupakan  kategori  dengan  persentase  produk  yang  salah 
dikategorikan  paling  banyak,  yaitu  berturut-turut  adalah 
13.3%, 11.8%, dan 9.4%. Total error pada produk yang salah 
diklasifikasi pada data dengan text encoding sebesar 9.2%. 

TABEL IV 
TOTAL KESALAHAN KLASIFIKASI PADA DATA VALIDASI 
DENGAN TEXT ENCODING 

Data dengan text encoding 

Kategori 
produk 

oven 

printer 

kasur 

sofa 

hp 

laptop 

Total 

Total 
produk 
pada 
data 
validasi 

172 

167 

150 

161 

161 

149 

960 

Total produk 
yang salah 
dikategorikan 

Persentase 
produk yang 
salah 
dikategorikan 

15 

11 

20 

9 

19 

14 

88 

8,7% 

6,6% 

13,3% 

5,6% 

11,8% 

9,4% 

9,2% 

Kategori 
produk 

oven 

printer 

kasur 

sofa 

hp 

laptop 

Total 

Total 
produk 
pada 
data 
validasi 

157 

170 

158 

149 

173 

153 

960 

Total produk 
yang salah 
dikategorikan 

Persentase 
produk yang 
salah 
dikategorikan 

52 

34 

48 

44 

13 

55 

246 

33,1% 

20,0% 

30,4% 

29,5% 

7,5% 

35,9% 

25,6% 

Dari  Tabel  IV  dan  Tabel  V,  perbandingan  kesalahan  pada 
data  yang  dilakukan  proses  text  encoding  dengan  data  tanpa 
text  encoding  untuk  setiap  kategorinya  menghasilkan  total 
kesalahan  yang  lebih  besar  pada  data  tanpa  text  encoding 
kecuali  pada  kategori  handphone.  Dan  secara  keseluruhan 
dapat  dilihat  bahwa  klasifikasi  pada  data  gambar  tanpa  text 
encoding  menghasilkan  persentase  produk  yang  salah 
diklasifikasikan lebih besar, yaitu 25,6%, sedangkan pada data 
gambar  dengan  text  encoding  persentase  error-nya  yaitu 
sebesar 9,2%. 

Tabel  VI  berikut  ini  merupakan  confusion  matrix  hasil 
pengujian model klasifikasi pada data test yang dilakukan text 
encoding.  Dapat  dilihat  bagaimana  detil  produk  yang  benar 
dan salah diklasifikasikan pada setiap kategorinya. 

TABEL VI 
CONFUSION MATRIX HASIL PEMODELAN CNN PADA DATA TEST 
DENGAN TEXT ENCODING 

Oven   Printer  Kasur  Sofa  HP  Laptop 

Oven 

92 

Printer 

Kasur 

Sofa 

HP 

Laptop 

1 

0 

0 

4 

3 

1 

93 

2 

1 

1 

0 

1 

2 

94 

0 

2 

1 

0 

1 

2 

97 

5 

5 

4 

2 

0 

1 

84 

4 

2 

1 

2 

1 

4 

87 

Sedangkan Tabel V menunjukkan jumlah kesalahan produk 
yang dikategorikan setiap produknya pada data validasi tanpa 
proses text encoding. Kategori produk laptop, oven, dan kasur 
merupakan  kategori  dengan  persentase  produk  yang  salah 
dikategorikan  paling  banyak,  yaitu  berturut-turut  sebesar 
35.9%,  33.1%,  dan  30.4%.  Total  error  pada  produk  yang 
salah  diklasifikasikan  pada  data  tanpa  proses  text  encoding 
sebesar 25.6%. 

Sedangkan  Tabel  VII  berikut  ini  merupakan  confusion 
matrix  pada  data  test  yang  tidak  dilakukan  proses  text 
encoding.  Sama  halnya  pada  confusion  matrix  pada  data 
validasi,  dapat  dilihat  pada  Tabel  VII  bahwa  kebanyakan 
produk  pada  data  test  tanpa  text  encoding  menghasilkan 
kesalahan  yang  lebih  banyak  dibandingkan  pada  Tabel  VI, 
yaitu data test dengan proses text encoding. 

 6 / 8 

 
 
  
 
 
 
 
 
 
 
  
 
 
 
 
 
 
Kategori 
produk 

Oven 

Printer 

Kasur  

Sofa 

HP 

Jumlah 
produk 
yang salah 
diprediksi 
per total 
produk  

8/100 

7/100 

6/100 

3/100 

16/100 

Laptop 

13/100 

TABEL VII 
CONFUSION MATRIX HASIL PEMODELAN CNN PADA DATA TEST 
TANPA TEXT ENCODING 

Oven   Printer  Kasur  Sofa  HP  Laptop 

Oven 

62 

Printer 

Kasur 

Sofa 

HP 

Laptop 

5 

4 

3 

6 

1 

12 

83 

5 

12 

2 

9 

3 

0 

72 

7 

3 

8 

13 

9 

11 

67 

2 

18 

8 

2 

4 

8 

87 

9 

2 

1 

4 

3 

0 

55 

Rangkuman dari hasil confusion matrix pada Tabel VI dan 
VII  di  atas  ditunjukkan  pada  Tabel  VIII,  terkait  total  produk 
yang salah diklasifikasikan di setiap kategorinya pada data test 
baik dengan text encoding maupun tanpa text encoding. 

TABEL VIII 
PERBANDINGAN TOTAL KESALAHAN KLASIFIKASI PADA DATA 
TEST 

Data test dengan text 
encoding 

Persentase 
produk yang 
salah 
diklasifikasika
n 

Data test tanpa text encoding 

Jumlah 
produk 
yang salah 
diprediksi 
per total 
produk  

Persentase 
produk yang 
salah 
diklasifikasika
n 

8% 

7% 

6% 

3% 

16% 

13% 

38/100 

17/100 

28/100 

33/100 

13/100 

45/100 

38% 

17% 

28% 

33% 

13% 

45% 

29% 

Total 

53/600 

8,83% 

174/600 

Pada  gambar  produk  dengan  text  encoding  yang  salah 
diklasifikasikan  pada  data  test,  kategori  produk  handphone, 
laptop,  dan  oven  memiliki  total  error  paling  banyak,  yaitu 
berturut-turut  persentasenya  sebesar  16%,  13%,  dan  8%. 
Sedangkan pada gambar produk tanpa text encoding pada data 
test,  kategori  produk  laptop,  oven,  dan  sofa  memiliki  total 
error terbesar, yaitu berturut-turut persentasenya sebesar 45%, 
38%, dan 33%. 

Selain  itu,  sama  halnya  dengan  kesalahan  pada  data 
validasinya, data test tanpa proses text encoding menghasilkan 
total  kesalahan  yang  lebih  besar  untuk  setiap  kategorinya 
kecuali  pada  kategori  handphone.  Dan  total  error  secara 
keseluruhan  menunjukkan  bahwa  error  pada  data  test  tanpa 
lebih  besar,  yaitu  29%.  Sedangkan 
text  encoding-nya 
persentase error pada data test dengan text encoding-nya yaitu 
8,83%. 

Total kesalahan pada kategori  handphone yang lebih besar 
pada data dengan text encoding dibandingkan pada data tanpa 
text  encoding 
tersebut  dapat  menunjukkan  bahwa  data 
training  yang  digunakan  untuk  pemodelan  klasifikasi  pada 
semua  datanya  mewakili 
kategori  handphone  belum 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

karakteristik  dari  setiap  jenis  dan  merk  handphone.  Pada 
kategori  handphone,  terdapat  banyak  jenis  handphone  dan 
merk  dari  setiap  jenisnya,  sehingga  diperlukan  data  training 
yang mewakili setiap jenis dan merk handphone tersebut agar 
sistem klasifikasi dapat mengenali produk handphone dengan 
baik sehingga hasil klasifikasinya pun baik pula. 

Tabel  9  berikut  ini  merupakan  hasil  pengujian  model 

klasifikasi pada data anomali. 

TABEL IX 
AKURASI KLASIFIKASI PRODUK PADA DATA ANOMALI  

Data dengan text encoding 

Data tanpa text encoding 

Akurasi 

46,60% 

Jumlah 
produk yang 
benar 
diprediksi 
56/120 

Akurasi 

55,83% 

Jumlah 
produk yang 
benar 
diprediksi 
67/120 

Dari 120 data anomali yang peneliti dapatkan, hasil akurasi 
klasifikasi  pada  data  dengan  text  encoding  yaitu  sebesar 
46,6%,  sedangkan  akurasi  pada  data  tanpa  text  encoding 
menunjukkan nilai yang lebih besar yaitu 55,83%. 

VII. 

KESIMPULAN 

Berdasarkan penelitian yang telah dilakukan, dapat diambil 

kesimpulan sebagai berikut : 
1.  Perancangan  mekanisme  untuk  klasifikasi  produk 
marketplace  yang  sesuai  dengan  standar  klasifikasi  BPS 
telah  berhasil  dilakukan.  Pengklasifikasian  produk 
tersebut  dilakukan  menggunakan  data  multimodal  yang 
didapatkan  dari  hasil  text  encoding  pada  nama  produk 
yang  disematkan  pada  gambar  produk 
sehingga 
dihasilkan  suatu  gambar  baru.  Kemudian  untuk  proses 
klasifikasinya  diterapkan  metode  Convolutional  Neural 
Network (CNN) berbasis fungsi Tensorflow 

2.  Dilakukan  perbandingan  performa  klasifikasi  pada  data 
gambar  produk  dengan  text  encoding  (data  multimodal) 
dengan  data  gambar  produk  asli  tanpa  proses  text 
encoding.  Hasil  perbandingan  akurasi  klasifikasi  pada 
data dengan text encoding yaitu sebesar 90,8% pada data 
validasi dan 91,16% pada data  test, sedangkan pada data 
tanpa  text  encoding  yaitu  sebesar  74,4%  pada  data 
validasi  dan  71%  pada  data  test.  Akurasi  klasifikasi 
menggunakan data multimodal menghasilkan nilai akurasi 
yang  lebih  tinggi.  Hasil  pemodelan  klasifikasi  tersebut 
dapat  digunakan  untuk  mengklasifikasikan  produk 
marketplace menggunakan nama dan gambar produk 
3.  Dilakukan pengujian model klasifikasi pada data anomali, 
yaitu data yang sulit untuk diidentifikasi kategorinya jika 
dilihat  dari  gambar  produk,  baik  pada  data  dengan  text 
encoding maupun pada data tanpa text encoding. Hasilnya 
menunjukkan  bahwa  data  anomali  dengan  text  encoding 
menghasilkan nilai akurasi yang lebih rendah, yaitu 46,6% 
dibandingkan  pada  data  anomali  tanpa  text  encoding, 
yaitu  55,83%.  Hal  tersebut  menunjukkan  bahwa  proses 
text  encoding  pada  gambar  produk  masih  belum  bisa 
meningkatkan performa klasifikasi pada data anomali. 

 7 / 8 

 
 
  
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

VIII. 

SARAN 

Dari penelitian yang telah dilakukan, saran untuk penelitian 
selanjutnya  yaitu  dapat  dilakukan  penelitian  dengan 
pendekatan  multimodal  lain,  dan  dengan  variabel  lain  seperti 
deskripsi  produk  untuk  dapat  meningkatkan  hasil  akurasi 
klasifikasi  produk,  serta  dapat  menangani  data  produk  yang 
ambigu  (data  anomali)  karena  nama  produk  atau  gambar 
produk tidak teridentifikasi dengan jelas kategorinya. 

DAFTAR PUSTAKA 
[1]  A.  Jawale,  G.  Magar,  “Comparison of  Image  Classification  Techniques: 
Binary and Multiclass using Convolutional Neural Network and Support 
Vector Machines,” INFOCOMP, vol. 18, no. 2, pp. 28-35, Dec 2019. 
[2]  Ang  LI,  Yi-Xang  LI,  and  Xue-hui  LI,  “Tensorflow  and  Keras-based 
Convolutional  Neural  Network  in  Cat  Image  Recognition,”  2017  2nd 
International  Conference  on  Computational  Modeling,  Simulation  and 
Applied Mathematics (CMSAM 2017), pp. 529-533, 2017. 

[3]  Badan  Pusat  Statistik,  “Statistik  E-Commerce  2020,”  Publikasi  BPS, 

Jakarta: 2020. 

[4]  K.  Chauhan,  S.  Ram,  “Image  Classification  with  Deep  Learning  and 
Comparison between Different Convolutional Neural Network Structures 
using  Tensorflow  and  Keras,”  International  Journal  of  Advance 
Engineering  and  Research  Development,  vol.  5,  pp.  533-538,  February 
2018. 

[5]  I. Gallo, A. Calefati, S. Nawaz, M.K. Janjua, “Image and Encoded Text 
Fusion  for  Multi-Modal  Classification,”  Digital  Image  Computing: 
Technique and Applications, 2018 (DICTA 2018), pp. 1-7, Des 2018. 
[6]  L.  Arevalo,  E.  Valdez,  R.  Quintela,  N.  Betancourt,  and  J.  Alquisiras, 
“Comparison  of  Image  Classification  Algorithm  using  Meibography 
Images,”  IEEE  Engineering  in  Medicine  and  Biology  Society  (EMBC), 
Feb 2019. 

[7]  M.  E.  H.  Pedersen.  (2020,  April).  TensorFlow-Tutorials  [Online]. 

Available : https://github.com/Hvass-Labs/TensorFlow-Tutorials. 

[8]  M.  Shanmukhi.  K.  Durga,  M.  Mounika,  K.  Keerthana,  “Convolutional 
Neural  Network  for  Supervised  Image  Classification,”  International 
Journal  of  Pure  and  Applied  Mathematics,  vol.  119,  no.  14,  pp.  77-83, 
2018. 

[9]  P.  Kumar,  U.  Dugal,  “Tensorflow  Based  Image  Classification  using 
Advanced  Convolutional  Neural  Network,”  International  Journal  of 
Advance  Engineering  and  Research  Development,  vol.  8,  pp.  994-998, 
March 2020. 

[10] P. M. Radiuk, “Impact of Training Set Batch Size on the Performance of 
Convolutional  Neural  Networks  for  Diverse  Datasets,”  Information 
Technology  and  Management  Science,  vol.  20,  pp.  20-24,  December 
2017. 

[11] Tensorflow.  (2021,  March).  Convolutional  Neural  Network  (CNN) 
https://www.tensorflow.org/tutorials/ 

Available 

[Online]. 
images/cnn#create_the_convolutional_base. 

: 

 8 / 8 

 
 
"
221709883,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pembangunan Sistem Peringatan Dini Dropout 
Politeknik Statistika STIS Menggunakan Analytic 
Hierarchy Process 

Naflah Ariqah (221709883, 4SI1) 
Dosen Pembimbing: Yunarso Anang, Ph.D. 

Ringkasan—  Sebagai  lembaga  pendidikan  tinggi,  Politeknik 
Statistika  STIS  juga  menghadapi  permasalahan  yang  sama 
seperti perguruan tinggi pada umumnya, yaitu mahasiswa tinggal 
kelas dan dropout. Untuk mengatasi masalah tersebut, penelitian 
ini  mengajukan  Dropout  Early  Warning  System  (DEWS)  atau 
sistem  peringatan  dini  dropout  yang  dapat  memberikan 
peringatan  dini  dropout  dan  tinggal  kelas.  Dengan  sistem  ini 
institusi  untuk  memprediksi 
diharapkan  dapat  membantu 
mahasiswa  yang  berpotensi  untuk  dropout  atau  tinggal  kelas. 
Tujuan dari pembuatan sistem ini yaitu untuk membantu dosen 
pembimbing akademik  serta  pemangku  keputusan Polstat  STIS 
dalam mengetahui potensi dropout mahasiswa. Potensi mahasiswa 
dropout dan tinggal kelas diukur melalui sebuah skor potensi yang 
didapatkan  dari  hasil  penilaian  5  kriteria  yaitu  nilai  IP,  jenis 
kelamin, faktor ekonomi, poin pelanggaran, dan catatan tinggal 
kelas. Hasil prediksi disajikan dalam tiga kategori yaitu potensi 
rendah, potensi sedang, dan potensi tinggi yang dihitung dari hasil 
perhitungan  pembobotan  dengan  menggunakan  proses  hierarki 
analitik atau Analytical Hierarchy Process (AHP). Uji coba sistem 
menggunakan  uji  Black  Box  dan  evaluasi  metode  perhitungan 
menggunakan confusion matrix. 

Kata Kunci— Dropout, AHP, Pendidikan, DEWS 

I.  LATAR BELAKANG 

Politeknik Statistika STIS (selanjutnya disebut Polstat STIS) 
merupakan sebuah perguruan tinggi kedinasan yang berada di 
bawah naungan Badan Pusat Statistik (BPS). Mahasiswa yang 
berkuliah  di  Polstat  STIS  berstatus  sebagai  ikatan  dinas  dan 
tugas  belajar.  Mahasiswa  dengan  status  ikatan  dinas,  setelah 
lulus  akan  diangkat  menjadi  calon  pegawai  negeri  sipil  di 
lingkungan  BPS  atau  akan  ditempatkan  di  kementerian  atau 
Lembaga  lain  sesuai  dengan  penempatannya.  Terdapat  tiga 
program  studi  pada  Polstat  STIS  yaitu  Program  Studi  D-III 
Statistika,  Program  Studi  D-IV  Statistika,  dan  Program  Studi 
D-IV Komputasi Statistik.  

Sebagai  sebuah  institusi  pendidikan,  Polstat  STIS  perlu 
mengatasi  masalah  yang  berkaitan  dengan  akademik 
mahasiswa.  Salah  satunya  yaitu  masalah  dropout  dan  tinggal 
kelas,  di  mana  tercatat  setiap  tahunnya  terdapat  5  sampai  10 
mahasiswa  yang  ditetapkan  untuk  dropout  dan  tinggal  kelas. 
Walaupun  mahasiswa  yang  dropout  dan  tinggal  kelas  masih 
dalam jumlah yang kecil, hal ini patut untuk diperhatikan agar 
menghindari  masalah  yang  lebih  besar  di  masa  yang  akan 
datang. 

Dropout  Early  Warning  System  (DEWS)  atau  sistem 
peringatan  dini  dropout  merupakan  sistem  yang  dapat 

membantu  institusi  untuk  memprediksi  mahasiswa  yang 
berpotensi  dropout  sehingga  pemangku  keputusan  dapat 
memberikan tindakan lebih lanjut kepada mahasiswa tersebut. 
Mahasiswa  yang  berpotensi  dropout  dan 
tinggal  kelas 
kemungkinan  besar  tidak  mempertimbangkan  dengan  cermat 
apakah  mereka  telah  melakukan  hal  yang  benar  dalam 
melakukan  kegiatan  akademik  mereka.  Peringatan  dini  yang 
diinformasikan  oleh  DEWS  dapat  mengarahkan  mahasiswa 
menuju kelulusan dan menuntun mereka ke masa depan yang 
lebih baik. 

Karena  permasalahan  ini,  banyak  pemerintah  luar  negeri 
yang  telah  mengembangkan  sistem  peringatan  dini  putus 
sekolah.  Misalnya,  negara  bagian  Wisconsin  di  Amerika 
Serikat  mengembangkan  DEWS  untuk  memprediksi  siswa 
putus  sekolah  [1].  Di  Korea  Selatan,  dikembangkan  juga 
DEWS untuk membantu sekolah mengidentifikasi siswa yang 
berisiko  dropout  dan  tidak  berisiko  dropout  [2].  Mahasiswa 
yang  berpotensi  dropout  dan  tidak  berpotensi  dropout  dapat 
dipetakan  melalui  sistem  peringatan  dini  dropout.  Sistem 
peringatan  dini  dropout  memungkinkan  untuk  memprediksi 
mahasiswa  yang  berpotensi  dropout  dengan  menggunakan 
metode pengambilan keputusan. 

Salah  satu  metode  pengambilan  keputusan  yaitu  metode 
Analytical  Hierarchy  Process  (AHP).  Dalam  menentukan 
kriteria-kriteria yang ada, metode AHP dapat mengelola nilai 
inputan sesuai dengan kriteria-kriteria yang mempunyai bobot 
nilai  tertentu,  sehingga  menjadikan  AHP  dapat  digunakan 
sebagai  pembobotan  untuk  memprediksi  mahasiswa  yang 
berpotensi untuk dropout atau tinggal kelas. 

Berdasarkan  penelitian  yang  dilakukan  oleh  Imran  [3], 
faktor-faktor  yang  berhubungan  dengan  mahasiswa  putus 
sekolah dari 13 peubah penjelas yang diteliti pada mahasiswa 
IPB  dengan  menggunakan  analisis  survival  menghasilkan 
model terbaik dengan tiga peubah penjelas yaitu jenis kelamin, 
IPK,  dan  fakultas.  Faktor  jenis  kelamin  menjelaskan  bahwa 
mahasiswa  laki-laki  memiliki  peluang  dropout  lebih  besar 
dibandingkan mahasiswa perempuan.  

Menurut  Khoirunnisak  dan  Iriawan  [4]  yang  melakukan 
penelitian  pada  Institut  Teknologi  Sepuluh  November  (ITS) 
dengan menggunakan analisis Bayesian mixture survival untuk 
memodelkan  faktor-faktor  yang  mempengaruhi  mahasiswa 
dropout,  ditemukan  faktor-faktor  yang  signifikan  yaitu  usia, 
perbedaan  asal  daerah,  perbedaan  penghasilan  orang  tua, 
perbedaan fakultas dan jalur masuk serta nilai IPK dan TPB. 

 1 / 8 

 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Teknologi  Sepuluh  November  Menggunakan  Analisis 
Bayesian Mixture Survival yang ditulis oleh Khoirunnisak dan 
Iriawan  [4].  Penelitian  ini  menganalisis  faktor-faktor  yang 
mempengaruhi mahasiswa dropout atau berhenti studi di ITS. 
Hasil penelitian menunjukkan faktor-faktor yang berpengaruh 
signifikan terhadap berhenti studi mahasiswa ITS yang terdiri 
dari usia, perbedaan asal daerah, perbedaan penghasilan orang 
tua, perbedaan fakultas dan jalur masuk serta nilai IPK dan TPB. 
Penelitian  ketiga  berjudul  Pembangunan  Sistem  Evaluasi 
Performa Mahasiswa Politeknik Statistika STIS Menggunakan 
Analytic Hierarchy Process yang ditulis oleh Mahendra Angga 
Pratama [5]. Penelitian ini menggunakan AHP yang digunakan 
untuk  mengukur  performa  mahasiswa  Polstat  STIS.  Dalam 
pengukurannya indikator-indikator yang digunakan terdiri dari 
nilai IP, IPKM, poin pelanggaran, dan kehadiran perkuliahan. 
Untuk  penelitian  keempat  berjudul  Pemanfaatan  Sistem 
Informasi  Geografis  Dengan  Metode  Analytical  Hierarchy 
Process  (AHP)  untuk  Prediksi  Daerah  Rawan  Banjir  di  Kota 
Semarang  yang  ditulis  oleh  Abdhika  Resqy  Imanda  dan  DR. 
Pulung  Nurtantio  Andono  S.  T,  M.Kom  [6].  Penelitian  ini 
menggunakan  metode  AHP  yang  digunakan 
sebagai 
pembobotan  untuk  prediksi  rawan  banjir  di  Kota  Semarang. 
Hasil  dari  penelitian  ini  menghasilkan  sistem  Informasi 
Geografis  yang  dapat  dimanfaatkan  sebagai  prediksi  daerah 
rawan banjir. 

Keempat  penelitian  tersebut  digunakan  sebagai  acuan  dan 
bahan penelitian dalam menyusun penelitian ini, baik dari sisi 
pembangunan 
indikator 
penilaian. 

sistem  maupun  pembangunan 

IV. METODE PENELITIAN  

Berdasarkan  kedua  penelitian  yang  telah  diuraikan,  dapat 
diambil faktor-faktor mana saja yang sesuai dengan kondisi dan 
lingkungan Polstat STIS. Ukuran-ukuran yang mempengaruhi 
dropout  seharusnya  bisa  digambarkan  untuk  memprediksi 
mahasiswa-mahasiswa  yang  mungkin  akan  di  dropout  atau 
tinggal  kelas.  Untuk  menyelesaikan  masalah 
tersebut, 
penelitian  ini  mengajukan  DEWS  yang  dapat  memberikan 
peringatan  dini  dropout  atau  tinggal  kelas  mahasiswa  Polstat 
STIS.  Di  mana  pada  sistem  tersebut  dibuat  sebuah  penilaian 
berupa  skor  yang  didapatkan  dari  ukuran-ukuran  yang 
mempengaruhi  dropout.  Skor  ini  dihasilkan  dari  proses  AHP 
yang  akan  dikategorikan  menjadi  3  kategori  yaitu  potensi 
rendah, potensi sedang, dan potensi tinggi untuk dropout atau 
tinggal kelas. Hasil dari skor tersebut akan ditampilkan kepada 
dosen pembimbing akademik mahasiswa yang bersangkutan.  

II.  TUJUAN PENELITIAN 

Tujuan umum dari penelitian ini adalah dibangunnya sebuah 
sistem  informasi  yang  dapat  membantu  pemangku  keputusan 
Polstat  STIS  dan  dosen  pembimbing  akademik  (PA)  dalam 
mengetahui  potensi  dropout  dan  tinggal  kelas  mahasiswa 
Polstat  STIS.  Sistem  ini  diharapkan  dapat  digunakan  oleh 
dosen pembimbing akademik sebagai acuan untuk memberikan 
pengarahan atau bimbingan kepada mahasiswa yang memiliki 
potensi untuk dropout atau tinggal kelas.  

Tujuan  khusus  yang  ingin  dicapai  dalam  penelitian  ini 

sebagai berikut: 
1.  Membuat  model  prediksi  mahasiswa  yang  berpotensi 
dropout  dan  tinggal  kelas  dengan  menggunakan  AHP 
hingga  menghasilkan  skor  potensi  mahasiswa  untuk 
dropout dan tinggal kelas. 

2.  Membangun  sebuah  sistem  informasi  peringatan  dini 
dropout  dan  tinggal  kelas  mahasiswa  menggunakan 
metode  AHP  untuk  menghasilkan  skor  yang  dibagi 
menjadi 3 kategori potensi yaitu potensi rendah, potensi 
sedang, dan potensi tinggi untuk dropout dan tinggal kelas.  

3.  Membangun  sebuah  sistem  informasi  yang  mampu 
memberikan informasi potensi dropout dan tinggal kelas 
mahasiswa yang dapat digunakan sebagai bahan evaluasi 
mahasiswa.  

III. PENELITIAN TERKAIT 

Pada  penelitian  ini,  terdapat  4  penelitian  yang  terkait. 
Penelitian  pertama  yaitu  penelitian  pada  tahun  2019  yang 
diteliti  oleh  Sunbok  Lee  dan Jae  Young  Chung dengan  judul 
The Machine Learning-Based Dropout Early Warning Sistem 
for  Improving  the  Performance  Dropout  Prediction  [2]. 
Penelitian ini mengembangkan sistem peringatan dini dropout 
yang  dapat  membantu  sekolah  untuk  mengidentifikasi  siswa 
SMA yang berisiko untuk dropout. Beberapa fitur atau variabel 
yang  digunakan  yaitu  kehadiran,  keterlambatan,  prestasi,  dan 
partisipasi dalam kegiatan sekolah. Penelitian ini menggunakan 
beberapa metode perhitungan dan menyebutkan bahwa boosted 
dalam 
tree  menghasilkan 
decision 
mengidentifikasi siswa yang berisiko dropout. 

terbaik 

kinerja 

Penelitian  kedua  berjudul  Pemodelan  Faktor-Faktor  yang 
Mempengaruhi Mahasiswa Berhenti Studi (dropout) di Institut 

Gambar 1. Metode Penelitian 

 2 / 8 

 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Diharapkan penelitian ini menghasilkan suatu sistem Informasi 
Peringatan  Dini  Dropout  yang  mampu  menjadi  solusi  atas 
masalah yang telah disebutkan. Kerangka pikir penelitian ini, 
dapat dilihat pada gambar 2. 

1.  Metode Pengumpulan Data 

Dalam penelitian ini metode pengumpulan data yang 
digunakan  yaitu  wawancara  dan  studi  Pustaka.  Metode 
studi Pustaka, dilakukan studi mengenai AHP dan kasus-
kasus  yang  berhubungan  dengan  sistem  peringatan  dini 
dropout.  Selain 
itu,  wawancara  dan  studi  pustaka 
dilakukan  untuk  mendapatkan  kriteria-kriteria  yang 
digunakan  sebagai  acuan  dalam  mengukur  potensi 
dropout dan tinggal kelas mahasiswa Polstat STIS dalam 
penelitian ini. 

2.  Metode Analisis 

Adapun metode analisa yang digunakan yaitu dengan 
menggunakan  metode  AHP.  AHP  akan  menyusun 
hierarki sesuai dengan tujuan penilaian potensi  dropout. 
Kriteria-kriteria  yang digunakan  untuk perhitungan  oleh 
AHP  adalah  faktor-faktor  yang  berpengaruh  terhadap 
potensi mahasiswa untuk dropout dan tinggal kelas. 

3.  Metode Pengembangan Sistem 

Metode  pengembangan  sistem  yang  digunakan  pada 
penelitian  ini  adalah  metode  System  Development  Life 
Cycle (SDLC) dengan model waterfall. Tahapan-tahapan 
dalam pembangunan sistem ini yaitu: 
a.  Perencanaan  Sistem, 

pengumpulan 
informasi  mengenai  sistem  yang  ingin  dibangun, 
berasal dari studi pustaka yang dilakukan. 

dilakukan 

b.  Analisis  sistem, 

identifikasi  sistem  yang 
berjalan, agar dapat diterapkan di Polstat STIS. 
c.  Mendesain  diagram  proses  bisnis,  mendesain  basis 

telah 

d. 

data, dan rancangan antar muka. 
Implementasi  rancangan  yang  telah  dibuat  ke  alat-
alat yang sesuai dengan kebutuhan. 

e.  Evaluasi  sistem,  dilakukan  setelah  sistem  selesai 
dibuat  dengan  menguji  fungsionalitas  keseluruhan 
dari sistem dan masing-masing komponen dari sistem. 

4.  Metode Evaluasi 

Evaluasi  dalam  pembangunan  sistem  bertujuan  untuk 
melakukan pengecekan apakah fungsi-fungsi di dalam sistem 
berfungsi dengan semestinya atau tidak. Kesesuaian masukan 
dan hasil dari fungsi yang digunakan akan diuji menggunakan 
uji Black Box dan perhitungan akurasi menggunakan confusion 
matrix. 

V.  KERANGKA PIKIR 

Pada  penelitian  ini,  topik  utama  yang  diangkat  adalah 
prediksi potensi  dropout dan tinggal kelas mahasiswa  Polstat 
STIS.  Masalah-masalah  yang  ada  adalah  pada  tiap  tahun 
terdapat  beberapa  mahasiswa  Polstat  STIS  yang  mengalami 
dropout dan tinggal kelas dan terdapat faktor-faktor yang dapat 
menyebabkan dropout dan tinggal kelas di luar nilai akademik 
yang belum terekam saat ini. Untuk mengatasi masalah tersebut 
ditawarkan solusi yaitu Pembangunan Sistem Peringatan Dini 
Dropout  menggunakan  Analytic  Hierarchy  Process.  Sistem 
yang  telah  dibangun  akan  dilakukan  uji  coba  dan  evaluasi 
menggunakan  Black  Box  Testing  dan  confusion  matrix. 

Gambar 2. Kerangka Pikir Penelitian 

VI. HASIL DAN PEMBAHASAN 

6.1. Rancangan Sistem Usulan 

1.  Bisnis proses sistem yang diusulkan 

Gambar 3. Bisnis proses sistem yang diusulkan 

 3 / 8 

 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

kriteria yang berisi informasi mengenai prioritas untuk 
alternatif pada masing-masing kriteria. 

Gambar  3  menjelaskan  mengenai  proses  bisnis 
sistem yang diusulkan untuk menyelesaikan masalah 
yang  ada.  Untuk  sistem  sendiri,  proses  dilakukan 
dengan  mengimplementasikan  AHP.  Proses  awal 
dimulai  dengan  memberikan  prioritas  dengan 
akan 
menggunakan  kuesioner  prioritas  yang 
tingkat  kepentingan  dari  setiap 
membandingkan 
kriteria  satu  dengan 
lainnya.  Kemudian  akan 
didapatkan  prioritas  untuk  masing-masing  kriteria. 
Setelah  prioritas  ditentukan,  mahasiswa  melakukan 
pengisian  data  yang  dibutuhkan.  Setelah  data 
terkumpul, dilakukan pembaruan skor potensi dropout 
dan  tinggal  kelas  mahasiswa  untuk  semester  baru, 
nilai  dari  setiap  kriteria  diberikan  skor  berdasarkan 
klasifikasi  masing-masing  kriteria  dan  didapatkan 
skor  potensi  mahasiswa  untuk  dropout  dan  tinggal 
kelas.  Untuk  proses  evaluasi,  program  studi  (prodi) 
mendapatkan  laporan  yang  dapat  digunakan  sebagai 
bahan evaluasi, sedangkan mahasiswa perlu menemui 
dosen  PA  masing-masing  untuk  mendapatkan 
evaluasi hasil potensi dropout atau tinggal kelas. 

2.  Diagram use case sistem usulan 

Diagram  pada  gambar  4  menjelaskan  mengenai 

use case dari sistem yang diusulkan. 

Gambar 4. Diagram use case sistem usulan 

3.  Rancangan Database 

Rancanan  database  yang  akan  digunakan  pada 
tabel  seperti  yang 
sistem  usulan  memiliki  14 
digambarkan  pada  gambar  5.  Tabel  dosen  dan  tabel 
administrator berisi informasi yang digunakan untuk 
login  ke  dalam  sistem.  Tabel  mahasiswa  berisi 
informasi  umum  mengenai  mahasiswa.  Tabel  list 
penghasilan  berisi  aturan  pengkodean  untuk  sumber 
penghasilan mahasiswa. Tabel nilai mahasiswa berisi 
informasi  mengenai  nilai-nilai  mahasiswa 
tiap 
semester  akademik  berisi 
semesternya.  Tabel 
sedang 
informasi  mengenai 
berlangsung  maupun  yang 
tidak.  Tabel  kriteria 
mahasiswa  berisi  data-data  nilai  mahasiswa  yang 
digunakan dalam perhitungan skor potensi mahasiswa 
menggunakan  AHP  dan  menyimpan  skor  akhir 
potensi  mahasiswa.  Tabel  AHP  berisikan  informasi 
umum mengenai metode AHP yang sedang digunakan 
maupun  metode  AHP  yang  digunakan  sebelumnya. 
Tabel prioritas berisi informasi prioritas untuk kriteria. 
Tabel skor jenis kelamin, tabel skor IP, tabel skor poin 
pelanggaran,  tabel  skor  penghasilan  orang  tua,  dan 
tabel  skor  tinggal  kelas  merupakan  tabel  alternatif 

semester 

yang 

Gambar 5. Rancangan ERD logical sistem usulan 

4.  Rancangan Arsitektur Sistem 

Sistem  peringatan  Dini  dibangun  berbasis  web 
dengan  memanfaatkan  basis  data  sebagai  media 
penyimpanan informasi atau data. Sistem ini memiliki 
tiga  pengguna  yaitu  terdiri  dari  administrator,  dosen 
pembimbing akademik, dan mahasiswa. 

 4 / 8 

 
 
 
 
 
 
 
 
 
Gambar 6. Rancangan arsitektur sistem usulan 

6.2. Implementasi Sistem Penilaian 

Implementasi  AHP  dalam  sistem  ini  berupa  sebuah 
hierarki  yang  bertujuan  untuk  mengukur  potensi 
mahasiswa  dropout  dan  tinggal  kelas  dengan  kriteria-
kriteria terpilih adalah IP, jenis kelamin, pendapatan orang 
tua/wali,  poin  pelanggaran  dan  catatan  tinggal  kelas. 
Kelima  kriteria  ini  kemudian  disusun  dalam  bentuk 
hierarki  dengan  tujuan  utama  yaitu  mengukur  potensi 
mahasiswa  dropout  dan 
tinggal  kelas.  Kemudian, 
dimasukkan penggunaan pengukuran relatif yang di mana 
alternatifnya  diberikan  kategorisasi  bertingkat.  Untuk 
kategorisasi  bertingkat  untuk  kriteria  IP  dan  poin 
pelanggaran  ditetapkan  berdasarkan  nilai  terdekat  dari 
batas  ketentuan  untuk  lanjut  semester  Polstat  STIS  serta 
analisis  persebaran  data  nilai  mahasiswa  yang  kemudian 
dikelompokkan  menjadi  4  kategori.  Kategori  untuk 
pendapatan  orang  tua/wali  didasarkan  pada  kategorisasi 
dari penelitian [4]. Kategorisasi jenis kelamin terdiri dari 
perempuan dan laki-laki serta catatan tinggal kelas terdiri 
dari mahasiswa yang pernah tinggal kelas dan mahasiswa 
yang tidak pernah tinggal kelas. 

Gambar 7. Implementasi AHP dalam mengukur potensi mahasiswa 
DO/tinggal kelas 

Penilaian  kriteria  dilakukan  melalui  perbandingan 
berpasangan. Skala yang digunakan adalah skala 1 sampai 
9. Sebagai contoh dalam penghitungan prioritas, misalkan 
didapatkan perbandingan sebagai berikut: 

TABEL II 
CONTOH MATRIKS PERBANDINGAN 
Jenis kelamin 

IP 

kriteria 

IP 

1 

Jenis kelamin 

0,14 

7 

1 

Pendapatan 
orang 
tua/wali 
5 

0,33 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pendapatan 
orang tua/wali 

0,20 

Jumlah 

1,34 

3 

11 

1 

6,33 

Setelah didapatkan matriks perbandingannya, masing-
masing  elemen  matriks  perbandingan  akan  dibagi  sesuai 
dengan  jumlah  kolomnya.  Hasil  pembagian  dilihat  pada 
tabel III 

TABEL III 
HASIL PRIORITAS DARI MATRIKS PERBANDINGAN 
Jenis 
kelamin 

IP 

kriteria 

Bobot 

Pendapatan 
orang 
tua/wali 
0,79 

IP 

0,75 

0,64 

Jenis kelamin 

0,10 

0,09 

Pendapatan 
orang tua/wali 

0,15 

0,27 

0,05 

0,16 

0,73 

0,08 

0,19 

Jumlah 

1 

1 

1 

1 

Hasil dari tabel III sendiri sudah menghasilkan bobot 
kriteria-kriteria yaitu dengan cara mengambil rata-rata tiap 
barisnya. 

Setelah mendapatkan nilai bobot, Langkah selanjutnya 
adalah  memeriksa  konsistensi  perbandingan  yang  ada. 
Pemeriksaan  konsistensi  dimulai  dengan  menjumlahkan 
hasil kali antara jumlah nilai pada setiap kolom dari tabel 
matriks perbandingan dengan nilai bobot pada setiap baris, 
guna mendapatkan nilai 𝜆max. 

𝜆max  = (1,34 * 0,73) + (11 * 0,08) + (6,33 * 0,19) 

= 3,06 

Kemudian hitung Consistency Index (CI) dan Consitency 
Ratio (CR) sebagai berikut: 

𝐶𝐼 =

λ𝑚𝑎𝑥−𝑛
𝑛−1

=

3,06−3

3−1

= 0,03   

𝐶𝑅 =

𝐶𝐼

𝑅𝐼

=

0,03

0,58

= 0,05   

(1) 

(2) 

Dari nilai CR di atas didapat nilai 0,05. Dengan nilai CR 
<= 0,1, maka prioritas di atas sudah konsisten.  

Setelah  didapatkan  nilai  prioritas  untuk  masing-
masing  kriteria  selanjutnya  adalah  penetapan  prioritas 
pada  alternatif  masing-masing  kriteria.  Langkah-langkah 
penentuan  nilai  prioritas  untuk  alternatif  tidak  berbeda 
dengan penentuan prioritas untuk kriteria. 

Setelah semua prioritas sudah didapatkan, selanjutnya 
adalah  dengan  mengategorikan  nilai  masing-masing 
mahasiswa ke dalam batasan nilai alternatif untuk masing-
masing  kriteria.  Berdasarkan  perhitungan  dari  metode 
AHP dengan persamaan sebagai berikut: 

 5 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Antar  muka  beranda  mahasiswa  menunjukkan 
profil utama dari mahasiswa dan informasi mengenai 
nilai-nilai yang terkait dengan mahasiswa tersebut. 

Gambar 10. Implementasi antar muka halaman beranda mahasiswa 

3.  Halaman edit penilaian AHP 

Halaman edit prioritas pada gambar 11 berfungsi 
untuk  menentukan  nilai  prioritas  masing-masing 
kriteria.  Selanjutnya,  ada  satu  form  lagi  untuk 
menentukan  prioritas 
alternatif  masing-masing 
kriteria.  Hasil  dari  form  penilaian  AHP  ini  akan 
terhadap  nilai  akhir  potensi 
berpengaruh  besar 
dropout  dan  tinggal  kelas  mahasiswa.  Halaman  ini 
hanya  dapat  diakses  oleh  kepala  BAAK  atau 
administrator sistem. 

𝑛
∑ 𝑋𝑖
𝑖=1 = 𝑊𝑖 ∗ 𝑉𝑖   

(3) 

 = nilai potensi DO/tinggal kelas 

Keterangan: 
𝑛
∑ 𝑋𝑖
𝑖=1
Wi = Bobot kriteria-i 
Vi = Bobot alternatif kriteria-i 
n = banyak kriteria 

Sehingga  hasil  dari  penjumlahan  gabungan  nilai  tersebut 
dapat diklasifikasikan berdasarkan tingkat potensi dropout 
dan tinggal kelas. Klasifikasi dilakukan dengan membagi 
penilaian  dalam 3 kategori,  yaitu potensi rendah, potensi 
sedang, dan potensi tinggi. 

6.3. Implementasi Sistem Informasi 

Implementasi sistem dilakukan dengan menggunakan 
IDE Visual Studio Code dengan bahasa pemrograman PHP. 
Pengembangan  aplikasi  web  menggunakan  framework 
CodeIgniter.  Sebagai  penyimpanan  data,  sistem  yang 
dikembangkan menggunakan MySQL. 

Berikut merupakan sebagian implementasi antarmuka 

Sistem Peringatan Dini Dropout Polstat STIS. 
1.  Halaman Login 

Antar  muka  halaman  login  dibagi  menjadi  tiga, 
yaitu  login  untuk  dosen,  login  untuk  kepala  BAAK 
(administrator), dan login untuk mahasiswa. 

Gambar 8. Implementasi antar muka halaman login dosen 

2.  Halaman Beranda 

Implementasi antar muka untuk halaman beranda 
dosen  menunjukkan  akumulasi  mahasiswa  yang 
diampu berdasarkan kategori potensi DO/tinggal kelas. 

Gambar 9. Implementasi antar muka halaman beranda dosen 

Sama  seperti  halaman  beranda  dosen,  halaman 
beranda  untuk  kepala  BAAK  merupakan  akumulasi 
dari seluruh mahasiswa Polstat STIS. 

Gambar 11 Implementasi antar muka halaman edit prioritas 

6.4. Uji Coba dan Evaluasi Sistem 

Uji coba dari sistem ini dilakukan dengan cara black 
box testing. Pengujian ini ditujukan untuk menguji apakah 
fungsi-fungsi yang ada pada sistem dapat berfungsi secara 
baik atau tidak. Tabel 4 menunjukkan hasil dari pengujian 
black box. 

TABEL IV 
PENGUJIAN MENGGUNAKAN BLACK BOX TESTING 

 6 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Test Case 

Berhasil  Tidak 

Berhasil 

Pengguna 
BAAK, 
(Koordinator 
mahasiswa,  dan  dosen  pembimbing 
akademik) melakukan login sesuai dengan 
username  dan  password  yang  terdapat 
pada  database  dan  diarahkan  ke  beranda 
masing-masing pengguna 
BAAK, 
(Koordinator 
Pengguna 
mahasiswa,  dan  dosen  pembimbing 
dan 
akademik)  melakukan 
logout 
diarahkan  kembali  ke  halaman 
login 
masing-masing pengguna 
Mahasiswa 
terhadap informasi profilnya 
Dosen  PA  melihat  informasi  detail  dari 
mahasiswa yang diampunya 
Admin  mengubah prioritas untuk  kriteria 
maupun  alternatif  dari  kriteria  dan 
informasinya  tersimpan  serta  langsung 
terupdate  untuk  prioritas  yang  ada  di 
dosen PA dan di administator 
Admin  memperbarui  hasil  penilaian 
potensi  DO/tinggal 
seluruh 
mahasiswa setiap semester 

melakukan 

perubahan 

kelas 

✓ 

✓ 

✓ 

✓ 

✓ 

✓ 

Selanjutnya, 

untuk  melihat  manakah  metode 
penyusunan prioritas kriteria yang memiliki performa yang 
lebih  baik,  dilakukan  evaluasi  menggunakan  confusion 
matrix yang dilihat dari segi tingkat akurasi, presisi, recall, 
dan F-measure. Dalam penelitian ini akan menggunakan 2 
model penyusunan prioritas kriteria.  

Model  1  dengan  kriteria  IP  yang  memiliki  prioritas 
selanjutnya  prioritas  kedua  yaitu  poin 
tertinggi, 
pelanggaran dan jenis kelamin, prioritas ketiga pendapatan 
orang tua/wali, dan prioritas keempat yaitu catatan tinggal 
kelas.  Lalu  untuk  model  2  kriteria  IP  dengan  prioritas 
tertinggi,  kemudian  prioritas  kedua  yaitu  kriteria  jenis 
kelamin, catatan tinggal kelas, pendapatan orang tua/wali, 
dan poin pelanggaran. 

TABEL V 
CONFUSSION MATRIX PENYUSUNAN PRIOTAS KRITERIA MODEL 1 

Nilai Prediksi 

True 
False 

True 
20 
4 

Nilai Sebenarnya 

False 
33 
369 

TABEL VI 
CONFUSSION MATRIX PENYUSUNAN PRIOTAS KRITERIA MODEL 2 

Nilai Prediksi 

True 
False 

True 
8 
16 

Nilai Sebenarnya 

False 
8 
394 

Berdasarkan  evaluasi  menggunakan 

confussion 
matrix dapat diperoleh nilai akurasi, presisi, recall, dan F-
measure, hasilnya dapat dilihat pada tabel 7. 

TABEL VII 
PERBANDINGAN PERFORMANSI METODE PENYUSUNAN 

Model  Akurasi 
0.9131 
0.9438 

1 
2 

Presisi 
0.3774 
0.5000 

Recall 
0.8333 
0.3333 

F-measure 
0.5195 
0.4000 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Dari  tabel  VII,  dapat  dilihat  bahwa  kedua  model 
penyusunan  prioritas  kriteria  memiliki  performa  yang 
cukup  berbeda.  Dibandingkan  dengan  model  2,  nilai 
akurasi, presisi, recall, dan F-measure pada model 1 masih 
lebih  baik.  Walaupun  nilai  akurasi  model  2  lebih  tinggi, 
namun  itu  tidak  jauh  berbeda.  Karena  acuan  pada 
penelitian  ini  yaitu  memilih  model  yang  memiliki  nilai 
recall  dan  akurasi  yang  tinggi.  Recall  yang  tinggi 
mengartikan bahwa model lebih baik dalam memprediksi 
mahasiswa  positif  dropout  atau 
tinggal  kelas  yang 
sebenarnya  tidak  dropout  atau  tinggal  kelas  daripada 
model  yang  salah  memprediksi  mahasiswa  yang  tidak 
dropout atau tinggal kelas padahal sebenarnya mahasiswa 
tersebut dropout atau tinggal kelas. Sehingga dengan kata 
lain lebih tidak diinginkan ada mahasiswa yang sebenarnya 
berpotensi dropout atau tinggal kelas namun salah dalam 
prediksi.  Hal tersebut sesuai dengan tujuan penelitian ini 
yaitu  membangun  sistem  peringatan  dini  dropout  dan 
tinggal kelas, maka dari itu kecepatan informasi mengenai 
mahasiswa yang berpotensi dropout dan tinggal kelas lebih 
diutamakan  dalam  memberikan  peringatan  dini  kepada 
mahasiswa  dan  akurasi  menjadi  prioritas  kedua  dalam 
pemilihan model. 

7.1.  Kesimpulan 

VII. 

PENUTUP 

Berdasarkan  penelitian  yang 

sudah  dilakukan, 

diperoleh beberapa kesimpulan sebagai berikut: 
1.  Telah  berhasil  membuat  model  prediksi  dengan 
menggunakan  metode  AHP  yang  menghasilkan 
hasil  prediksi  mahasiswa  yang  berpotensi  dropout 
dan tinggal kelas hingga menghasilkan skor potensi 
mahasiswa untuk dropout dan tinggal kelas. 

2.  Telah dibangun sebuah sistem informasi peringatan 
dini  dropout  dan 
tinggal  kelas  mahasiswa 
menggunakan  metode  AHP  untuk  menghasilkan 
skor  yang  dibagi  menjadi  3  kategori  potensi  yaitu 
potensi  rendah,  potensi  sedang,  dan  potensi  tinggi 
untuk dropout dan tinggal kelas. 

3.  Telah  dibangun  sistem  yang  mampu  memberikan 
informasi  potensi  dropout  dan 
tinggal  kelas 
mahasiswa  yang  dapat  digunakan  sebagai  bahan 
evaluasi. 

7.2.  Saran 

Integrasi sistem dengan Sipadu Polstat STIS 

Berdasarkan  hasil  uji  coba  dan  evaluasi,  diperoleh 
beberapa  saran  yang  dapat  digunakan  untuk  penelitian 
selanjutnya yaitu: 
1. 
2.  Pembuatan fungsi untuk memberikan kriteria secara 
dinamis agar sistem ini dapat mengikuti perubahan-
perubahan  kriteria  dalam  menentukan  mahasiswa 
yang berpotensi untuk dropout dan tinggal kelas. 
3.  Menggunakan  nilai  ujian  PMB  untuk  digunakan 
sebagai  salah  satu  kriteria  dalam  penilaian  potensi 
dropout mahasiswa yang baru masuk. 

4.  Memperkuat  akurasi  penilaian  baik  dari  sisi 
penentuan  prioritas  ataupun  pemilihan  kriteria  agar 

 7 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

lebih  tepat  dalam  memprediksi  mahasiswa  yang 
berpotensi dropout dan tinggal kelas. 

DAFTAR PUSTAKA 
[1]  J.E. Knowles, “Of Needles and Haystacks: Building an Accurate Statewide 
Dropout Early Warning System in Wisconsin”, Journal of Educational Data 
Mining, vol. 7, no. 3, pp. 18-67, 2015 

[2]  S.  Lee  and  J.  Y.  Chung,  “The  Machine  Learning-Based  Dropout  Early 
Warning Sistem for Improving the Performance of Dropout Prediction”, 
Applied Sciences, vol. 9, no. 15, 2015. 

[3]  F. Imran, B. Susetyo, and, A.H. Wigena, “Identifikasi Faktor-Faktor yang 
berhubungan  dengan  Mahasiswa  Putus  Kuliah  di  IPB  Angkatan  2008 
menggunakan Analisis Survival”, Journal of Statistics, vol. 2, no. 1, pp. 1-
6, 2013. 

[4]  M.  Khoirunnisak  and  N.  Iriawan,  “Pemodelan  Faktor-Faktor  yang 
Mempengaruhi Mahasiswa Berhenti Studi (Dropout) di Institut Teknologi 
Sepuluh  November  Menggunakan  Analisis  Bayesian  Mixture  Survival”, 
2010. 

[5]  M.A.  Pratama,  “Pembangunan  Sistem  Evaluasi  Performa  Mahasiswa 
Politeknik  Statistika  STIS  Menggunakan  Analytic  Hierarchy  Process”, 
Jakarta: Politeknik Statistika STIS, 2019, pp. III.  

[6]  A.R. Imanda and P.N. Andono, “Pemanfaatan Sistem Informasi Geografis 
dengan  Metode  Analytical  Hierarchy  Process  (AHP)  untuk  Prediksi 
Daerah  Rawan  Banjir  di  Kota  Semarang”,  Semarang:  Universitas  Dian 
Nuswantoro Semarang, 2015. 

 8 / 8 

 
 
 
 
"
221709878,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Analisis Respon Publik Terhadap Pelaksanaan 
Program Pemulihan Ekonomi Nasional Melalui Data 
Twitter 

Nadiah „Ainayya Sholihah (221709878, 4SD1) 
Dosen Pembimbing: Nucke Widowati Kusumo Projo, S.Si, M.Sc, Ph.D. 

(WHO) 

Ringkasan—World  Health  Organization 

telah 
menetapkan  bahwa  COVID-19  merupakan  pandemi  yang  telah 
menyebar ke seluruh dunia dan menyebabkan berbagai masalah 
diberbagai  bidang  [1].  Di  Indonesia,  kebijakan  PP  23/2020 
tentang  Pelaksanaan  Program  Pemulihan  Ekonomi  Nasional 
(PEN)  ditetapkan  sebagai  solusi  untuk  mengatasi  masalah 
dibidang ekonomi.  Selanjutnya dilakukan analisis respon publik 
sebagai  bahan  penyempurnaan  kebijakan  tersebut.  Hasilnya 
yaitu  dari  data  hasil  scraping  twitter  diketahui  bahwa  respon 
publik  tinggi  hanya  pada  tanggal  ditetapkannya  kebijakan 
tersebut,  lalu  pada  tanggal-tanggal  selanjutnya  jumlah  tweet 
cenderung  konstan.  Selanjutnya,  dilakukan  analisis  sentimen 
dengan  Multinomial  NBC.  Hasilnya  menunjukan  bahwa  tweet 
mengenai  Program  PEN  didominasi  oleh  opini  netral.  
Kemudian,  diperoleh  empat  topik  yang  diperoleh  dari  topic 
modelling  yang  menjadi  tren  percakapan  di  twitter  mengenai 
Program  PEN  yaitu  tweet  berupa  laporan  penyerahan  bantuan 
di  daerah,  Program  PEN  UMKM  dan  padat  karya  mendukung 
pemulihan 
laporan  pelaksanaan  padat  karya 
mangrove,  dan  tweet  yang  tidak  berhubungan  dengan  Program 
PEN. 

ekonomi, 

Kata  Kunci—  twitter,  program  PEN,  analisis  sentimen,  topic 

modelling. 

I.  LATAR BELAKANG 

Febrio  Kacaribu  dalam  Media  Briefing  mengenai  Program 
Pemulihan  Ekonomi  Nasional,  menjelaskan  bahwa  pandemi 
COVID-19  memberikan  efek  domino  pada  aspek  kesehatan, 
sosial, ekonomi, dan keuangan [2]. Dari aspek-aspek tersebut, 
yang  memiliki  urgensi  paling  besar  yaitu  bidang  ekonomi.  
Hal  ini  ditandai  dengan  keterangan  para  pengamat  ekonomi 
dan  Lembaga  Internasional  (IMF,  Bank  Dunia,  OECD) 
memprediksi bahwa Indonesia mengalami resesi ringan (mild 
recession)  karena  kontraksi  ekonomi  yang  diperkirakan 
“hanya”  sekitar  minus  3%  -  0%  dan  tidak  akan  berlangsung 
lama,  yaitu  sekitar  2  triwulan  [3].  Urgensi  pada  bidang 
ekonomi  juga  terlihat  dari  perubahan  arah  perekonomian 
secara drastis sejak kemunculan pandemi COVID-19. Gambar 
1  adalah  Berita  Resmi  Statistik  (BRS)  No.  39/05/Th.XXIII 
yang  keluar  pada  tanggal  5  Mei    2020.  BRS  tersebut 
menyebutkan  bahwa  Ekonomi  Indonesia  triwulan  I-2020 
terhadap  triwulan  I-2019  mengalami  kontraksi  pertumbuhan 
sebesar 2,97 persen (y-on-y) [2]. 

Gambar 1. Pertumbuhan Ekonomi Indonesia Triwulan I-2020 

  pemerintah  pada 

Maka  untuk  meredam  dampak  negatif  dari  pandemi 
COVID-19, 
tanggal  11  Mei  2020  
memutuskan  untuk  menetapkan  Peraturan  Pemerintah  (PP) 
Nomor  23  Tahun  2020 
tentang  Pelaksanaan  Program 
Pemulihan  Ekonomi  Nasional  dalam  Rangka  Mendukung 
Kebijakan  Keuangan  Negara  Untuk  Penanganan  Pandemi 
Corona  Virus  Disease  2019  (COVID-19)  dan  Menghadapi 
Ancaman  yang  Membahayakan  Perekonomian  Nasional 
dan/atau  Stabilitas  Sistem  Keuangan  Serta  Penyelamatan 
dilakukan 
Ekonomi  Nasional.  Kemudian, 
implementasi  PEN  diharapkan  terjadi  pemulihan  ekonomi 
pada kuartal III, selanjutnya pada kuartal IV ekonomi nasional 
bertumbuh  positif.  Sehingga  pada  tahun  2021  ekonomi 
nasional akan mengalami recovery secara siginifkan [3]. 

setelah 

terhadap  kebijakan 

Untuk  mendukung  supaya  pelaksanaan  Program  PEN 
berjalan  lancar,  diperlukan  analisis  respon  publik  mengenai 
kebijakan  tersebut.  Hal  ini  dilakukan  untuk  melihat  apakah 
pelaksanaanya  sudah  tepat  sasaran  atau  malah  sebaliknya. 
Kemudian  untuk  mendapatkan  gambaran  bagaimana  respon 
publik 
tersebut,  dilakukan  analisis 
deskriptif.  Analisis  ini  berguna  untuk  mengetahui  gambaran 
umum  mengenai  seberapa  sering  para  pengguna  twitter 
tersebut 
membahas  atau  berdiskusi  mengenai  kebijakan 
dengan  melihat    frekuensi  tweet  perhari  pada  topik  Program 
PEN.  Selanjutnya  dilakukan  analisis  sentimen  yang  berguna 
untuk  mengetahui  kecenderungan  opini  masyarakat,  apakah 
cenderung  positif,  negatif,  atau  netral  [4].  Terakhir  adalah 
topic  modelling  atau  pembentukan  topik.  Analisis  tersebut 
bertujuan  untuk  mengetahui  topik-topik  apa  saja  yang  sering 
dibahas  di  twitter  terkait  dengan  kebijakan  PEN  yang  baru 
diluncurkan.  Tentu  saja  topik-topik  tersebut  harus  terkait 
mengenai Program PEN untuk mengetahui bagaimana respon 
masyarakat terhadap peluncuran program ini. Dengan analisis 
tersebut  diharap  dapat  digunakan  untuk  melihat  apakah  ada 

 1 / 7 

 
 
 
 
 
 
keluhan atau hal-hal yang bisa digunakan untuk memperbaiki 
kebijakan  tersebut  agar  dapat  tercapai  tujuan  penetapan  PP 
23/2020 mengenai Pelaksanaan Program PEN. 

II.  TUJUAN PENELITIAN 

Berikut  merupakan 

informasi  mengenai 

tujuan  dari  penelitian 

ini  yaitu 
respon  publik  pada 
mendapatkan 
kebijakan  Pelaksanaan  Program  PEN  yang  selanjutnya  dapat 
digunakan sebagai acuan pemerintah dalam menyempurnakan 
kebijakan tersebut.  

1.  Melakukan  analisis  deskriptif  data  twitter  mengenai 

Program PEN. 

2.  Menganalisis  bagaimana  respon  publik 

terhadap 

Program PEN dengan melakukan analisis sentimen. 
3.  Melakukan  pemodelan  topik  respon  publik  untuk 
mengetahui tren percakapan mengenai Program PEN. 

III. PENELITIAN TERKAIT 

S. Mujilahwati [5] menjelaskan twitter mengandung banyak 
tipe  data  dan  gaya  bahasa,  maka  perlu  penanganan  khusus 
berupa  preprocessing  data.  Proses  ini  penting  agar  dapat 
memperoleh hasil analisis yang optimal. 

Pada penelitian R. Sistem, Y. Sahria, and D. H. Fudholi [6] 
dilakukan  topic  modelling  dengan  metode  topic  modelling 
LDA 
(Latent  Dirichlet  Allocation),  setelah  dilakukan 
pengujian  diperoleh  hasil  bahwa  94,1%  mengatakan  sangat 
baik  dan  5,9%  mengatakan  baik.  Tujuan  dilakukan  topic 
modelling  yaitu  untuk  mengetahui  bagaimana  tren  penelitian 
di penelitian kesehatan. 

A.  Annisa  Raudya  Wibowo,  Nuke  Nidya,  Aisyah  Firdausi 
Rahma  [7]  menjelaskan  bahwa  dimasa  pandemi  COVID-19, 
berbagai  kebijakan  mulai  dimunculkan  dan  sangat  dapat 
memacu cuitan para pengguna sosial media khususnya twitter.  
Pada penelitian A. Yulianto, A. Herdiani, and I. L. Sardi [8] 
metode  Multinomial  Naive  Bayes  Classifier  dipilih  karena 
memiliki rata-rata akurasi yang cukup tinggi antara 85%-90% 
untuk  klasifikasi  sentimen  teks  dengan  data  tweet.  Maka, 
metode  yang  digunakan  pada  penelitian  saya  adalah 
Multinomial Naive Bayes Classifier.  

Menurut  S.  P.  Brilianti  [9]  Dataset  yang  tidak  seimbang, 
mengakibatkan  pengukurannya  semu,  artinya 
tingginya 
akurasi  yang  dicapai  sebenarnya  tidak  mewakili  nilai  data 
yang sebenarnya. Maka, perlu dilakukan resampling pada data 
yang 
tidak  seimbang,  sehingga  kita  dapat  melakukan 
peningkatan  performa  (tuning)  untuk  mendapatkan  model 
optimal [9]. 

IV. METODE PENELITIAN  
Berikut merupakan langkah-langkah untuk mencapai tujuan 

penelitian. 

1.  Pengumpulan Data 

Pengumpulan  data  untuk  penelitian  ini  dilakukan 
tweet  pada  web 
dengan  cara  pemanfaatan  data 
www.twitter.com. 
dengan 
melakukan web scraping melalui library twint . Periode 
pengumpulan  data  yaitu  tanggal  11  Mei  2020  sampai 
dengan 31 Desember 2020. 

dikumpulkan 

Tweet 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

2.  Pemilihan Kata Kunci 

Kata  kunci  untuk  scraping  dipilih  menurut  empat 
program  prioritas  Satgas  Pemulihan  Ekonomi  Nasional 
(PEN)  saat  pandemi  COVID-19  yaitu  perlindungan 
sosial,  UMKM,  padat  karya, 
serta  pembiayaan 
perusahaan[10].  Dari  empat  kategori  yang  ada,  hanya 
tiga kategori yang dipakai, hal tersebut karena kategori 
ke empat yaitu pembiayaan perusahaan memiliki jumlah 
tweet  yang  sangat  sedikit  saat  di  scraping,  sebab 
kebijakan  ini  tidak  langsung  ditujukan  ke  masyarakat. 
Berikut  daftar  kata  kunci  yang  digunakan  untuk 
scraping data twitter. 

Kategori 

1 

2 

3 

TABEL I 
KATA KUNCI UNTUK SCRAPING 

Kata kunci 
Kartu  sembako,  bansos  pkh,  bansos 
jabodetabek,  bansos  non-jabodetabek, 
blt dana desa 
Dana umkm, subsidi bunga 
Program  pkt,  padat  karya  transportasi, 
program  pugar,  padat  karya  ICRG, 
padat  karya  terumbuk  karang,  padat 
karya mangrove, padat karya pertanian. 

tweet-tweet 

  Karena kata kunci „dana umkm‟ pada kategori kedua 
tidak 
masih  men-scraping 
berhubungan  dengan  Program  PEN,  maka  dilakukan 
penghapusan kata kunci.. Berikut merupakan daftar kata 
kunci  yang  digunakan  untuk  menghapus  tweet  hasil 
scraping yang tidak berhubungan dengan Program PEN 
(Tabel II). 

yang 

 TABEL II 
KATA KUNCI YANG DIHAPUS 

Kata kunci 

wallet 

pertamina 

RUU 
Ciptaker 

join 

sm  Dana 

Contoh tweet 
@danawallet  Wihh  banyak  nih  sobi 
gue yg pada punya UMKM, kayanya 
harus 
biar 
pembayarannya  lebih  gampang  terus 
bisnis lancar jaya 
#Pertamina-
Menyambut 
acara 
SMEXPO2020,  ada 
info  soal  27 
tahun  Pertamina  udah  salurkan  dana 
Rp 35 triliun untuk bantu UMKM, ini 
sih  wow  banget.  Pertamina  nyata 
hadir  utk  masyarakat 
https://t.co/nahscS6ieR 
RUU Ciptaker Pasal 102, Pemerintah 
Mengalokasikan  Penggunaan  Dana 
Alokasi  Khusus  Untuk  Mendanai 
Dan 
Kegiatan 
Pengembangan 
UMKM  
https://t.co/ZGR3sbUtqN 

Pemberdayaan 

Indonesia   

3.  Preprocessing Data: 

  Cleaning:  melakukan  case  folding,  menghapus 
angka, menghapus hashtag, menghapus username, 
menghapus  URL,  menghapus  RT,  menghapus 
emoji,  menghapus  karakter  non-ASCII,  serta 
menghapus tanda baca. [5]. 

 2 / 7 

 
 
  
  Tokenizing:  menguraikan  setiap  kalimat  yang 

 

 

adapada data menjadi per kata [11]. 
Stemming:  mentransformasi  kata  berimbuhan 
menjadi kata dasar [11]. 
Stopword  removing:  menghapus  kata-kata  yang 
sering keluar dan di anggap tidak penting [5]. 
  Convert  Word:  untuk  memperbaiki  penggunaan 
slang  word  atau  bahasa  gaul  mengakibatkan 
penggunaan Bahasa Indonesia menjadi tidak baku 
[5]. 
tahap 
Berikut 
preprocessing data pada salah satu tweet hasil scraping 
(Tabel III). 

implementasi 

contoh 

adalah 

Keterangan 

Sebelum 

Setelah 

TABEL III 
PREPROCESSING DATA 

Contoh tweet 
Sumpah  ya  tetangga  w  banyak  yg 
dapet  bansos,  PKH,  even  pegawai 
swasta  pada  dapet  duit 
juga 
perbulan  600rb  apa  brp  gt  itukan 
dikali  jutaan  orang  tuh  jd  berapa 
triliun  malih  itu  bantuan  doang 
lohðŸ¥ºðŸ¥º belum biaya a b c d e 
f  g  :(  miris  bgt  tweet  kaya  gitu 
malah rame banget 
sumpah  iya  tetangga  w  yang  dapat 
bansos  pkh  even  pegawai  swasta 
dapat  uang  bulan  rb  berapa  begitu 
itu  kali  juta  orang  itu  jadi  triliun 
malih bantu saja loh biaya a b c di e 
f  g  miris  banget  tweet  kaya  begitu 
ramai banget 

4.  Analisis Deskriptif 

atau 

  memprediksi  pengguna 

Melakukan  klasifikasi  akun  twitter  dengan  variable 
user_id  menggunakan  library  m3inference.  Libray  ini 
mampu 
twitter  menjadi 
human-status 
organization-status.  Dengan 
tersebut  diharapkan, 
library 
mengimplementasikan 
tweet-tweet  yang  akan  dianalisis  lebih  representatif 
denganmenggunakan  tweet-tweet  dari  akun  berstatus 
manusia  atau  yang  selanjutnya  disebut  dengan  akun 
pribadi. . Selanjutnya, data yang hanya berisikan tweet 
dari 
tahap 
preprocessing  dideskripsikan  melalui  visualisasi  data 
berupa  time  series  frekuensi  tweet,  diagram  batang 
tweet 
mengenai 
terbanyak. 
5.  Analisis Sentimen 

lima  username  dengan 

akun  pribadi  yang 

telah  melalui 

jumlah 

Dengan  menggunakan 

tehnik  klasifikasi  semi-
supervised  learning,  data  tweet      diklasifikasikan 
menggunakan  algoritma  Multinomial  Naïve    Bayes 
Classifier. Model dibangun dengan menerapkan  5-fold 
cross validation.  

Tahapan klasifikasi: 

1.  Melakukan  manual  labelling  pada  tweet  dari 
user_id  dengan  peluang  human-status  sama 
dengan 1. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

2.  Membagi  data  mejadi  dua,  yaitu  data  pseudo 
dengan bobot 25% dan data unlabel dengan bobot 
75%. 

3.  Membuat  Model  1  menggunakan  data  manual 
labelling,  yang  selanjutnya  digunakan  untuk 
memprediksi data pseudo. 

4.  Membuat  Model  2  menggunakan  gabungan  data 
manual 
labelling  dan  data  pseudo,  yang 
selanjutnya  digunakan  untuk  memprediksi  data 
unlabel. 

Evaluasi  performansi    merupakan  parameter  yang 
berguna  untuk  mengukur  performa  dari  model  yang 
telah  dibangun  seberapa  akurat  model  sistem  yang 
telah dibangun tersebut [8]. Parameter yang digunakan 
untuk mengukur performansi diantaranya: 
-  Precision  adalah  perbandingan 

jumlah  data 
relevan  yang  ditemukan  terhadap  jumlah  data 
yang ditemukan [4]. 

-  Recall adalah perbandingan jumlah materi relevan 
yang  ditemukan  terhadap  jumlah  materi  yang 
relevan [4]. 

-  F1-score  adalah  parameter 

tunggal  ukuran 
retrieval  yang  menggabungkan 

keberhasilan 
recall dan precision [4]. 

-  Accuracy  adalah  persentase  dari  total  sentimen 

yang benar dikenali [4]. 

5.  Topic Modelling 
Menggunakan 

topic  modelling  LDA 

(Latent 
dapat 
Dirichlet  Allocation).  Pemodelan 
mengambarkan  makna  dari  dokumen  secara  semantik 
yang  tersembunyi  dalam  teks  yang  jumlahnya  besar 
dan  dapat  menemukan  informasi  dari  data  teks  yang 
tidak  tersetruktur[6].  Menurut  Blei  [12]  Gambar  2 
merupakan bentuk dari representasi model LDA. 

topik 

Gambar 2. Representasi Model LDA 

V.  KERANGKA PIKIR 

2  Munculnya 

Bagian  Kerangka  pikir  dalam  penelitian  ini  dapat  dilihat 
pandemi  COVID-19, 
pada  Gambar 
menyebabkan  kelumpuhan  ekonomi  diseluruh  dunia,  apalagi 
bagi  negara-negara  berkembang  seperti  Indonesia.  Sebagai 
upaya  pemulihan,  pemerintah  akhirnya  menetapkan  PP 
23/2020  tentang  pelaksanaan  Program  Pemulihan  Ekonomi 
Nasional.  Demi  menjamin  kelancaran  pelaksanaan  kebijakan 
tersebut,  maka perlu untuk melakukan analisis respon publik. 
Dengan  mengetahui  bagaimana  respon  publik 
terhadap 
lebih  mudah 
kebijakan  baru,  maka  pemerintah  akan 
tercapainya 
melakukan  penyempurnaan  untuk  menjamin 
tujuan pelaksanaan Program Pemulihan Ekonomi Nasional.  

Dengan  adanya  kebijakan  social  distancing,  tentu  tidak 
memungkinkan  melakukan  pengumpulan  data  dengan  cara 

 3 / 7 

 
 
 
konvensional seperti wawancara. Maka penggunaan Big Data 
berupa  data  media  sosial  twitter  akan  sangat  memudahkan 
proses pengumpulan data serta proses analisis yang  dilakukan 
kapan saja dimana saja. 

selanjutnya  publik  sudah  tidak  terlalu  antusias  lagi  dalam 
merespon kebijakan tersebut. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 5. Time series frekuensi tweet  

Gambar 6. Username dengan jumlah tweet terbanyak 

Diagram  batang  pada  Gambar  6  memperlihatkan  lima 
username  dengan  jumlah  tweet  terbanyak.  Jika  dilihat  dari 
nama  username  tersebut,  menunjukan  bahwa  tweet-tweet 
tersebut sudah terdiri dari akun-akun pribadi. 
2.  Analisis Sentimen 

Dari 1488 tweet yang terlah dilakukan manual labelling, 
terdapat  891  tweet  berlabel  netral  (label:  0),  363  tweet 
berlabel negatif (label: 1), dan 234 tweet berlabel positif (label: 
2).   

Gambar 3. Kerangka Pikir Penelitian 

VI. HASIL DAN PEMBAHASAN 

Berikut  merupakan  penjelasan  hasil  dan  pembahasan  pada 

penelitian 
1.  Analisis Deskriptif 
Scraping  data 

twitter  menggunakan 

twint 
memperoleh  27264  baris.  Selanjutnya  dilakukan  klasifikasi 
user_id menggunakan library m3inference yang menghasilkan 
6677  akun  pribadi  dan  3368  akun  organisasi.  Gambar  4 
merupakan visualisasi jumlah tweet hasil klasifikasi. 

library 

Gambar 4. Jumlah tweet hasil filter akun 

Dari  Gambar  5  dapat  dilihat  bahwa  banyak  tweet 
mengenai Program PEN memilki titik puncak yaitu sebanyak 
520  tweet  mengenai  Program  PEN,  titik  puncak  berada  pada 
tanggal  11  Mei  2020.  Kemudian  tweet  menurun  drastis  pada 
tanggal  12  Mei  2020  menjadi  154  tweet  saja.  serta  pada 
tanggal-tanggal  selanjutnya  jumlahnya  cenderung  konstan. 
Hal  tersebut  menunjukan  bahwa  respon  publik  hanya  tinggi 
saat  ditetapkannya  PP  23/2021  mengenai  Pelaksanaan 
tanggal  11  Mei  2020,  dan 
Program  PEN  yaitu  pada 

Gambar 7. Jumlah tweet per label 

imbalance  data 

Dari Gambar 7 terlihat bahwa tweet yang berlabel netral 
jumlahnya  jauh  lebih  banyak  dari  kedua  label  lainnya. 
Selanjutnya  untuk  mengatasi 
tersebut, 
teknik  oversampling  menggunakan  SMOTE 
dilakukan 
(Synthetic    Minority    Over-sampling    Technique).  SMOTE 
merupakan        metode        pembangkitan        data        minoritas 
sebanyak          data  mayoritas  [13].    Kemudian  membangun 
Model 1 berdasarkan data manual labelling dengan algoritma 
Multinomial  NBC  yang  diterapkan  5-fold  cross  validation 
menghasilkan rata-rata akurasi 70%  dan evaluasi performansi 
di (Tabel V). 

 4 / 7 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
TABEL IV 
PERBANDINGAN RATA-RATA AKURASI 
Model  Akurasi 
Model 1 
Model 2 

0.70 
0.83 

TABEL V 
RATA-RATA EVALUASI PERFORMANSI 

Model 1 
Model 2 

Precision 
0.74 
0.54 

Recall 
0.49 
0.38 

F1-score 
0.49 
0.38 

Setelah  menggabungkan  hasil  prediksi  data  pseudo  dan 
data  manual  labelling,  diketahui  bahwa  data  imbalance  lagi. 
Maka setelah dilakukan oversampling menggunakan SMOTE, 
dibentuk Model 2 dengan rata-rata akurasi naik menjadi 83% 
dan memiliki evaluasi performansi di (Tabel V). 

Gambar  8  merupakan  hasil  analisis  sentimen  yang 
menghasilkan  13318  label  netral,  779  label  negatif,  dan    246 
label positif.  

Gambar 8. Jumlah tweet per label 

Dari  Gambar  8,  terlihat  bahwa  respon  masyarakat 
mengenai  Pelaksanaan  Program  PEN  didominasi  oleh  opini 
netral.  Berdasarkan  Gambar  11,  terlihat  bahwa  tweet  netral 
biasanya  berisi  tweet  berupa  kutipan  berita  atau  laporan 
penyaluran bantuan kepada masyarakat yang terlihat dari kata 
„salur  blt‟,‟langsung  tunai‟,  dst.  Selanjutnya,  tweet  opini 
negatif,  dilihat  dari  Gambar  9  berisi  keluhan  masyarakat 
mengenai pelaksanaan kebijakan ini yang ditunjukkan dengan 
kata  „korupsi‟,  „cair‟,  „mohon‟,  dst.  Terakhir,  opini  positif 
dengan  berisi  tweet  yang  mengungkapkan  rasa  syukur  dan 
konfirmasi  bahwa  pengguna 
twitter  sudah  mendapatkan 
bantuan  yang  ditunjukkan  pada  Gambar  10  dengan  kata 
„alhamdulillah‟, „salur‟, „terima‟, „manfaat‟, dst.  

Gambar 9. Word cloud tweet berlabel negatif 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar10. Word cloud tweet berlabel positif 

Gambar 11. Word cloud tweet berlabel netral 

3.  Topic Modelling 

Dalam melakukan topic modelling menggunakan library 
yang  disediakan  oleh  python  yaitu  LDA.    Langkah  pertama 
yaitu  mencari  nilai    koherensi  untuk  mengetahui  seberapa 
banyak  topik  yang  harus  dibentuk  dengan  membandingkan 
nilai  koherensinya.  Nomor  dengan  nilai  koherensi  terbesar 
akan menjadi banyak topik yang akan dimodelkan. 

Gambar 12. Grafik garis nilai koherensi 

Gambar 13. Nilai koherensi pada setiap topik 

Pada  Gambar 12 terlihat bahwa  empat topik  merupakan 
jumlah  topik  dengan  nilai  koherensi  tertinggi.  Hal  tersebut 
diperjelas  pada  Gambar  13,  yang  menunjukkan  empat 
kluster/topik memiliki nilai koherensi tertinggi yaitu 0,520104. 
topic  modelling  sebanyak  empat 
Kemudian  dilakukan 
kluster/topik dengan hasil output  pada (Tabel VI). 

TABEL VI 
OUTPUT TOPIC MODELLING DAN TOPIK YANG TERBENTUK 

Topik/ Kluster 

10 kata dengan frekuensi 
terbanyak dalam kluster 

Topik 1 (Laporan 

tahap,  kecamatan,  dd, 

serah, 

 5 / 7 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
penyerahan bantuan di 
daerah ) 

Topik 2 (Program PEN 
UMKM dan padat 
karya mendukung 
pemulihan ekonomi ) 

Topik 3 (Laporan 
pelaksanaan padat 
karya mangrove) 

Topik 4 (tweet yang 
tidak berhubungan 
dengan program PEN) 

kabupaten, 
tppikemendesapdtt_tppsumbar, 
langsung, nagari, z 

bagi, 

umkm, 

padat_karya, 
bunga, 
subsidi,  program,  pulih_ekonomi, 
perintah, kredit, yang, usaha 

tanam_mangrove, 
tanam, 
mangrove, 
padat_karya, 
kabupaten, kecamatan, lebih_baik, 
masyarakat, bltdanadesa, pati 
tppijawatimur_diri, 
pkt, 
bltddberkah, 
w, 
tppikemendesapdtt_tppsumateraba
rat,  malu,  bltdanadesa,  program, 
to, i 

Dari  output  pada  (Tabel  VI)  berupa  10  kata  dengan 
frekuensi  terbanyak  dalam  kluster,  maka  terbentuk  empat 
kluster/topik  yang  dimodelkan  berdasarkan  dari  masing-
terklusterisasi  pada 
masing  kumpulan  kata  yang  sudah 
masing-masing topik (Tabel VI). 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pada  Gambar  14  menunjukan  hasil  visualisasi 
menggunakan  library  pyLDAvis  yang  akan  memperlihatkan 
apakah topik-topik yang sudah terbentuk akan saling berkaitan 
atau  tidak.  Dilihat  dari  gambar  di  atas  dari  keempat  model 
topik model topik yang terbentuk sudah tidak saling berkaitan, 
kecuali pada model 2 dan 4 yang saling beririsan. Dari Tabel 
VI  terlihat  bahwa  topik  4  berisi  tweet-tweet  yang  tidak 
berhubungan  dengan  Program  PEN,  namun  ada  kata  kunci 
„pkt‟,  „bltddberkah‟,  dan  „bltdanadesa‟  yang  membuat  model 
topik  ini  beririsan  dengan  topik  2..    Selanjutnya,  terlihat 
bahwa  model  topik  1  dan  3  sudah  saling  berjauhan  yang 
meandakan bahwa topik-topik tersebut tidak saling berkaitan.  
Pada  Gambar  15,  memperlihatkan  30  kata  yang  paling 
menonjol.  Pada  visualisasi  pyLDAvis 
akan 
menampilkan  daftar  kata  yang  menyusun  masing-masing 
model topik saat pengguna melakukan select bubble yang ada 
pada Gambar 14.  

tersebut 

VII. 

PENUTUP 

Setelah  melakukan  analisis  deskriptif,  analisis  sentimen, 
dan topic modelling, maka didapat beberpaa kesimpulan yaitu 
1.  Respon  publik  mengenai  tweet  Program  PEN  hanya 
tinggi  pada  tanggal  ditetapkannya  kebijakan  tersebut. 
Sedangkan  pada  tanggal-tanggal  selanjutnya,  jumlahnya 
tweet 
cenderung  konstan.  Lima  username  dengan 
terbanyak  memiliki  nama  yang  menunjukan  bahwa 
mereka adalah akun pribadi. 

Gambar 14. Visualisasi topic modelling menggunakan pyLDAvis  
Bagian 1 

Gambar 15. Visualisasi topic modelling menggunakan pyLDAvis  
Bagian 2. 

2.  Hasil  analisis  sentimen  menunjukan  bahwa 

tweet 
Program  PEN  didominasi  oleh  opini  netral.  Hal  ini 
berarti,  mayoritas  pengguna  twitter  hanya  menyerap 
informasi  mengenai  PEN  melalui  berita  dan  membuat 
tweet berupa  progress pelaksanaan Program PEN. 
3.  Data  tweet  tentang  Program  PEN  terdiri  dari  tiga  topik 
menurut  hasil  skor  koherensi  topic  modelling.  Berikut 
merupakan  empat  topik  yang  dibentuk  menggunakan 
metode 
(Latent  Dirichlet 
Allocation): 
  Topik 1 (Laporan penyerahan bantuan di daerah) 
  Topik  2  (Program  PEN  UMKM  dan  padat  karya 

topic  modelling  LDA 

mendukung pemulihan ekonomi) 

  Topik  3 
mangrove) 

(Laporan  pelaksanaan  padat  karya 

  Topik  4  (tweet  yang  tidak  berhubungan  dengan 

program PEN)  
Peneliti  menyarankan  pemerintah  untuk  segera 
menyempurnakan  kebijakan  Pelaksanaan  Program  PEN. 
Hal  tersebut  karena  banyak  tweet  dengan  opini  negatif 
yang menyebutkan bahwa bantuan yang diberikan belum 
tepat  sasaran,  yang  ditandai  dengan  kata  pada  word 
cloud  tweet  berlabel  negatif  pada  Gambar  9  seperti 
„korupsi‟, „cair‟, „mohon‟, dst. 

DAFTAR PUSTAKA 

[1] 

[2] 

N.  Mahera,  “Krisis  Ekonomi  di  Indonesia  Disebabkan  Oleh 
Pandemi COVID-19,” Www.Researchgate.Net, 2020. 
F.  Kacaribu,  “Media  Briefing:  Program  Pemulihan  Ekonomi 
Nasional,”  Kementeri.  Keuang.,  p.  23,  2020,  [Online].  Available: 
https://www.kemenkeu.go.id/media/15116/media-briefing-
kabkf.pdf. 

 6 / 7 

 
 
 
 
 
 
[3] 

[4] 

[5] 

[6] 

[7] 

[8] 

[9] 

[10] 

[11] 

[12] 

[13] 

E.  U.  Nainggolan,  “Strategi  Kebijakan  Pemulihan  Ekonomi 
Nasional  (PEN),”  Direktorat  Jenderal  Kekayaan  Negara,  2020. 
https://www.djkn.kemenkeu.go.id/artikel/baca/13287/Strategi-
Kebijakan-Pemulihan-Ekonomi-Nasional.html  (accessed  Nov.  07, 
2020). 
F.  Ratnawati,  “Implementasi  Algoritma  Naive  Bayes  Terhadap 
Analisis Sentimen Opini Film Pada Twitter,”  INOVTEK Polbeng - 
Seri Inform., vol. 3, no. 1, p. 50, 2018, doi: 10.35314/isi.v3i1.335. 
S.  Mujilahwati,  “Pre-Processing  Text  Mining  Pada  Data  Twitter,” 
Semin.  Nas.  Teknol.  Inf.  dan  Komun.,  vol.  2016,  no.  Sentika,  pp. 
2089–9815, 2016. 
R. Sistem, Y. Sahria, and D. H. Fudholi, “JURNAL RESTI Analisis 
Topik Penelitian Kesehatan di Indonesia Menggunakan Metode,” J. 
Rekayasa Sist. dan Teknol. Inf., vol. 1, no. 10, pp. 336–344, 2021. 
A.  Annisa  Raudya  Wibowo,  Nuke  Nidya,  Aisyah  Firdausi Rahma, 
“Analisis  Sentimen  Hashtag  „Dirumahaja‟  Saat  Pandemi  Covid-19 
Di Indonesia Menggunakan Nlp,” J. Inform. dan …, vol. 1, no. 2, pp. 
343–353, 
Available: 
http://jifosi.upnjatim.ac.id/index.php/jifosi/article/view/239. 
A. Yulianto, A. Herdiani, and I. L. Sardi, “Klasifikasi Keberpihakan 
tweet  menggunakan  Multinomial  Naïve  Bayes  (  Studi  Kasus : 
Pemilihan Presiden 2019 ),” vol. 6, no. 2, pp. 9078–9085, 2019. 
S.  P.  Brilianti,  “Peningkatan  Performa  Analisis  Sentimen  Dengan 
Resampling dan Hyperparameter pada Ulasan Aplikasi BNI Mobile,” 
doi: 
pp. 
2018, 
no. 
10.30864/eksplora.v9i2.333. 
H.  A.  Vizki,  “Ini  Empat  Program  Prioritas  Pemulihan  Ekonomi 
Nasional,” 2020. https://republika.co.id/amp/qes69p467. 
A.  Pandhu  and  W.  Diki,  “Analisa  sentimen  dan  Klasifikasi 
Komentar Positif Pada Twitter dengan Naïve Bayes Classification,” 
BRITech (Jurnal Imiah Komputer, Sains dan Teknol. Terap., vol. 1, 
no. 2, pp. 32–40, 2020. 
D. M. Blei, A. Y. Ng, and M. T. Jordan, “Latent dirichlet allocation,” 
Adv. Neural Inf. Process. Syst., no. July, 2002. 
R. Sistem, “Penerapan Teknik SMOTE untuk Mengatasi Imbalance 
Class dalam,” vol. 1, no. 10, 2021. 

September 

140–153, 

[Online]. 

2020, 

2020, 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

 7 / 7 

 
 
 
 
 
"
221709873,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pengembangan Sistem Pendukung Keputusan dalam 
Pemilihan Dosen Pembimbing Skripsi Program Studi 
Komputasi Statistik dengan Metode Fuzzy AHP 

Mustafa Kholiq (221709873, 4SI1) 
Dosen Pembimbing: Yunarso Anang Sulistiadi, Ph.D. 

Ringkasan—  Dalam  pengerjaan  skripsi,  mahasiswa  dibantu 
seorang  dosen  pembimbing.  Untuk 
itu  diperlukan  dosen 
pembimbing yang sesuai dengan minat dan topik penelitian dari 
mahasiswa.  Dalam  penentuan  dosen  pembimbing,  mahasiswa 
diberikan  hak  penuh  untuk  memilih  dosen  pembimbing  yang 
sesuai dengan minatnya. Namun keterbatasan alokasi mahasiswa 
untuk  dibimbing  oleh  dosen  yang  diinginkanya  serta  waktu 
pemilihan  dosen  pembimbing  yang  singkat  menjadi  masalah 
dalam  pemilihan  tersebut.  Ketika  beberapa  mahasiswa  memilih 
dosen  yang  telah  melebihi  formasi,  diperlukan  diskusi  panjang 
agar sesuai dengan formasi dosen pembimbing. Diskusi panjang 
ini  disebabkan  mahasiswa 
saling  memperebutkan  dosen 
pembimbing  tertentu  tanpa  adanya  dasar  pertimbangan  yang 
jelas  dalam  menentukan  siapa  yang  akan  menjadi  mahasiswa 
bimbingannya. Untuk  permasalahan tersebut sistem  pendukung 
keputusan  pemilihan  dosen  pembimbing  yang  sesuai  dengan 
minat  mahasiswa  dapat  menjadi  solusi.  Menggunakan  metode 
Fuzzy Analytical Hierarchy Process (Fuzzy AHP) dengan kriteria 
didasarkan jumlah minat yang ada di program studi Komputasi 
Statistik  dan  penentuan  alternatif  didasarkan  dengan  jumlah 
dosen pembimbing.  

Kata Kunci— Skripsi, Dosen Pembimbing, Fuzzy AHP, DSS. 

I.  LATAR BELAKANG 

tentang 

Tugas  akhir  adalah  skripsi/karya  ilmiah  yang  dibuat  oleh 
mahasiswa  yang  memuat  deskripsi  keilmuan  hasil  penelitian 
implikasi  pengembangan  atau 
atau  pengkajian 
implementasi 
dengan 
teknologi, 
pengetahuan 
memperhatikan  dan  menerapkan  cerminan  kemampuan 
mahasiswa dalam menerapkan ilmu pengetahuan berdasarkan 
kaidah, tata cara dan etika ilmiah, dalam rangka menghasilkan 
solusi, gagasan atau rancangan. 

ilmu 

Skripsi  merupakan  salah  satu  persyaratan  penting  bagi 
mahasiswa untuk mendapatkan gelar sarjana terapan (Diploma 
IV)  di  Politeknik  Statistika  STIS,  tidak  terkecuali  Program 
Studi Diploma IV (Prodi D-IV) Komputasi Statistik.  

Dosen pembimbing memiliki peran penting bagi penentuan 
keberhasilan dalam penyusunan skripsi. Seperti pada penelitian 
Bahrudi, peran dosen pembimbing secara parsial berpengaruh 
positif  dan  signifikan  [1].  Karena  tugas-tugas  dari  dosen 
pembimbing  adalah memberikan ide, topik dan substansi tugas 
proses 
akhir;  memantau 
akhir; 
pelaksanaan 
tugas 
mendiskusikan  hasil  pengerjaan 
tugas  akhir  bersama 
mahasiswa;  mengevaluasi  serta  mengarahkan  mahasiswa 
tugas  akhir;  memutuskan  kesiapan 
dalam  penyelesaian 
mahasiswa  untuk  mengikuti  seminar 
tugas  akhir  serta 
membimbing persiapan seminar tugas akhir; dan memutuskan 
kesiapan  mahasiswa  untuk  mengikuti  ujian  tugas  akhir  serta 

membimbing  persiapan  ujian  akhir,  maka  diperlukan  dosen 
pembimbing  yang  sesuai  dengan  minat  dan  topik  penelitian 
yang ingin diajukan mahasiswa. 

Dalam  alur  penyusunan  skripsi,  khususnya  mahasiswa 
program  studi  D-IV  Komputasi  Statistik,  terdapat  tahapan 
penentuan dosen pembimbing yang dilakukan sepenuhnya oleh 
mahasiswa.  Mahasiswa  memiliki  hak  penuh  untuk  memilih 
dosen yang diinginkannya sebagai dosen pembimbing. Namun 
keterbatasan  alokasi  mahasiswa  untuk  dibimbing  oleh  dosen 
yang  diinginkanya  serta  waktu  pemilihan  dosen  pembimbing 
yang  singkat  menjadi  masalah  dalam  pemilihan  tersebut. 
Ketika beberapa mahasiswa memilih dosen yang telah melebihi 
formasi diperlukan diskusi panjang agar formasi sesuai dengan 
dosen pembimbing. Diskusi panjang ini disebabkan mahasiswa 
saling  memperebutkan  dosen  pembimbing  tertentu  tanpa 
adanya dasar pertimbangan yang jelas dalam menentukan siapa 
yang akan menjadi mahasiswa bimbingannya. 

Untuk  mengatasi  permasalahan  tersebut  diperlukan  sebuah 
sistem  pendukung  keputusan  atau  Decision  Support  Systems 
(DSS)  yang  mampu  memberikan  keputusan  tepat  dan  cepat 
dalam pemilihan dosen pembimbing skripsi yang sesuai dengan 
minat  mahasiswa.  DSS  adalah  suatu  pendekatan  (atau 
metodologi) untuk mendukung pengambilan keputusan dengan 
menggunakan  Computer  Based  Information  System  (CBIS) 
interaktif,  fleksibel,  dan  dapat  disesuaikan  yang 
yang 
dikembangkan khusus untuk mendukung solusi untuk masalah 
manajemen tidak terstruktur tertentu. DSS menggunakan data, 
menyediakan  antarmuka  pengguna  yang  mudah,  dan  dapat 
menggabungkan wawasan pembuat keputusan itu sendiri [2]. 

Dari banyaknya metode DSS yang ada, AHP sangat cocok 
untuk  mengatasi  permasalahan  tersebut  karena  metode  AHP 
menyelesaikan masalah multi atribut [3]. Selain itu, keunggulan 
metode  AHP  dibanding  metode  DSS  lainya  ialah  mudah 
sistematik  dan 
digunakan,  penataan  masalah 
memperhitungkan  bobot  kriteria  dan  alternatif  prioritas  [4]. 
Namun metode AHP memiliki kekurangan yaitu subjektivitas 
kriteria  [5]. Untuk  mengatasi subjektivitas kriteria  AHP dapat 
di  kombinasikan  dengan  konsep  Fuzzy  [6],  menjadi  Fuzzy 
Analytical Hierarchy Process (Fuzzy AHP).  

secara 

Prosedur  membangun  model  fuzzy  AHP  dimulai  dengan 
membuat  struktur  hierarki  masalah  yang  akan  diselesaikan, 
menentukan  nilai  pada  matriks  perbandingan  berpasangan 
antar  kriteria  dengan  skala  Triangular  Fuzzy  Number  (TFN), 
membuat  sintesis  keputusan  dari  beberapa  keputusan, 
fuzzy  dari  masing-masing  kriteria, 
menghitung  bobot 

 1 / 8 

 
 
 
 
normalisasi  nilai  bobot  kriteria,  dan  memeriksa  konsistensi 
rasio [3]. Setelah mendapatkan hasil yang konsisten, diperoleh 
bobot  global  kriteria  untuk  mengurutkan  dosen  yang  terbaik 
berdasarkan ketertarikan penelitian mahasiswa. 

II.  TUJUAN PENELITIAN 

Tujuan umum dari penelitian ini adalah untuk membangun 
sistem  informasi  berbasis  web  yang  dapat  memberikan 
keputusan  dalam  pemilihan  dosen  pembimbing  skripsi  yang 
sesuai dengan minat penelitian mahasiswa. Tujuan khusus dari 
penelitian ini adalah:  

1.  Membangun  suatu  indikator  ketertarikan  mahasiswa 
yang  dapat  digunakan  untuk  menilai  minat  dan  bakat 
mahasiswa  dalam  memilih  salah  satu  kategori  topik 
penelitian.  

2.  Membuat 

suatu 

sistem 

informasi  yang  dapat 
memberikan  pertimbangan  berupa  skor  rekomendasi 
calon  dosen  pembimbing  yang  sesuai  dengan  minat 
penelitian mahasiswa. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

skala  penilaian  yang  digunakan  pada  prioritas  kriteria  tetapi 
tidak pada peringkat kriteria. Metode AHP dapat menguraikan 
masalah  menjadi  hirarki  sub  kriteria  yang  dapat  lebih  mudah 
untuk dipahami. Kesimpulan yang dapat diambil dari penelitian 
ini aplikasi SiPendamping dapat memberikan kemudahan bagi 
ketua program studi Teknik Informatika UM Pontianak dalam 
memberikan keputusan sesuai dengan minat mahasiswa. 

Penelitian Yan Liu, Claudia M. Eckert, dan Christopher Earl 
[4] membahas tentang prosedur membangun model Fuzzy AHP 
dari  pembentukan  matriks  perbandingan,  menggabungkan 
beberapa penilaian, mengukur konsistensi dan menghilangkan 
bobot  fuzzy.  Makalah  ini  mengulas  Berbagai  Teknik  untuk 
keempat  aspek  tersebut.  Dengan  tujuan  untuk  membantu 
akademisi  dan  pakar  industry  untuk  memilih  Teknik  yang 
sesuai.  Penelitian 
ini  dapat  dijadikan  pedoman  dalam 
menggunakan  metode  Fuzzy  AHP  berdasarkan  konteks 
masalah.  

IV. METODE PENELITIAN  
Metode  penelitian  yang  digunakan  dalam  pembangunan 

3.  Membuat  sistem  pemilihan  dosen  pembimbing  skripsi 

sistem pendukung keputusan ini yaitu sebagai berikut: 

agar lebih efektif dan efisien. 

III. PENELITIAN TERKAIT 

Terdapat  beberapa  penelitian  terkait  sistem  pendukung 
keputusan  dalam  pemilihan  dosen  pembimbing  skripsi 
diantaranya adalah: 

Penelitian yang dilakukan Rully Nasrullah [8], Penelitian ini 
membahas  tentang  sitem  yang  dapat  memilih  calon  dosen 
pembimbing  dengan  tepat  dan  kompeten  di  STIS  dengan 
melakukan  pendekatan  by  product.  Sistem  menggunakan 
cosine  similiarity  score  untuk  melihat  kesamaan  anatara 
proposal  mahasiswa  dengan  kriteria  dosen  yang  ada. 
Menggunakan  metode  TOPSIS  dengan  menghitung  jarak 
terpendek dari kesamaan  topik  yang diusulkan pada proposal 
dengan  penelitian  yang  telah  dilakukan  dosen.  Penelitian  ini 
perlu  disesuaikan  dengan  proses  bisnis  pada  program  studi 
Komputasi Statistik yang berbeda. 

Penelitian Asrul Abdullah dan Menur Wahyu Pangestika [9] 
membahas tentang pembangunan, sebuah sistem yang mampu 
memberikan  keputusan  yang  tepat  dan  cepat  dalam  memilih 
dosen  pembimbing  skripsi  sesuai  dengan  minat  mahasiswa. 
Penelitian  ini  menggunakan  metode  Analytical  Hierarchy 
Process  (AHP)  yang  cukup  ampuh  untuk  menentukan 
keputusan  yang  disesuaikan  dengan  minat  mahasiswa,  judul 
skripsi,  konsentrasi  dosen  tetap  yang  ada  di  program  studi 
Teknik  Informatika  UM  Pontianak.  Hasil  dari  penelitian 
tersebut  Secara  umum  sistem  mampu  memberikan  keputusan 
yang tepat dalam pemilihan dosen pembimbing skripsi kepada 
tiap-tiap mahasiswa berdasarkan minat mahasiswa. Kelemahan 
dari  penelitian  tersebut  ialah  metode  AHP  adalah  masalah 
subjektivitas kriteria untuk mengatasinya dapat menggunakan 
skala fuzzy. 

Penelitian  Asrul  Abdullah  dan  Menur  Wahyu  Pangestika 
[10]. Pada penelitian ini mengkaji mengenai efek pemanfaatan 
dari  berbagai  skala  penilaian  pada  estimasi  prioritas  yang 
berbeda  pada  metode  AHP.  Hasil  dari  penelitian 
ini 
membuktikan  bahwa  terdapat  dampak  yang  besar  terhadap 

terhadap 

1.  Menganalis masalah dan proses bisnis yang ada. Analisis 
sistem  yang  berlaku 

yang  dilakukan 
menggunakan diagram fishbone dan analisis PIECES. 
2.  Mengumpulkan  segala  kebutuhan  data  dalam  proses 
penelitian  baik  dari  jurnal,  buku  dan  literatur-literatur. 
Pengumpulan  data  juga  dilakukan  dengan  wawancara 
dengan  dosen  pembimbing  dan  perwakilan  mahasiswa 
guna mendapatkan kebutuhan pengguna. 

3.  Menentukan  kriteria  dan  alternatif 

lalu  merancang 

pemodelan dalam penentuan dosen pembimbing. 

4.  Merancang arsitektur desain perangkat lunak dari sistem 
pendukung  keputusan  berdasarkan  informasi  yang  telah 
didapatkan  pada 
tahapan  sebelumnya.  Perancangan 
dilakukan  dengan  menggunakan  diagarm  Use  Case  dan 
Bisnis  Proses  serta  merancang  database  menggunakan 
A5:SQL Mk-2.   

ini 

lunak 

sistem.  Sistem 

5.  Membangun  perangkat 

sistem  pendukung 
keputusan  pemilihan  dosen  pembimbing  berdasarkan 
minat  mahasiswa  dengan  menerjemahkan  hasil  dari 
desain 
akan  dikembangkan 
menggunakan  framework  CodeIgniter  4  (CI4)  dengan 
Bahasa pemrograman PHP. Dengan desain user interface 
menggunakan template Open Source yaitu AdminLTE 3 
dan  Bootstrap  5.  Untuk  database  sistem  menggunakan 
MySQL yang dikelola menggunakan phpMyAdmin. 
6.  Menguji  keseluruhan  sistem  yang  telah  dibangun  dan 
memperbaiki  kesalahan  dari  sistem  serta  mengevaluasi 
sistem  menggunakan  Blackbox 
testing  dan  System 
Usability  Scale    untuk  mengukur  apakah  sistem  telah 
mengatasi permasalahan yang ada. 

7.  Membuat laporan dan menyimpulkan hasil penelitian. 

Berdasarkan  hasil  pengamatan  dan  wawancara  dengan 
program  studi  Komputasi  Statistik.  Didapatkan  alur  dalam 
pemilihan  dosen  pembimbing  yang  dimulai  dengan  prodi 
mengirimkan  email  berisi  formasi  alokasi  mahasiswa  tiap 
dosen. Setelah mahasiswa mengetahui hal tersebut mahasiswa 

 2 / 8 

 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

dalam  menentukan  dosen  pembimbing  yang  sesuai  dengan 
mahasiswa tersebut.  

memilih  dosen  pembimbing  sesuai  minat  penelitianya.  Agar 
formasi alokasi mahasiswa tiap dosen  sesuai, pengurus tingkat 
4 menyesuaikan hasil pemilihan dengan jumlah formasi yang 
ada.  Apabila ada dosen pembimbing yang dipilih mahasiswa 
melebihi  formasi  yang  ditetapkan,  mahasiswa  yang  memilih 
dosen  tersebut  mendiskusikan  siapa  yang  lebih  pantas  untuk 
semua  mahasiswa 
tersebut.  Setelah 
dibimbing  dosen 
mendapatkan  dosen  pembimbingnya,  maka  dibuat  laporan 
pemilahan  dosen  pembimbing  sesuai  formasi  dan  dikirimkan 
kepada program studi Komputasi Statistik. 

Gambar 2. Diagram fishbone sistem berjalan 

TABEL II 
TABEL ANALISIS PIECES 

Analisis kebutuhan digunakan untuk mengetahui kebutuhan 
dari sistem yang akan dibangun dengan menggunakan analisis 
PIECES. Hasil dari analisis PIECES dapat dilihat pada Tabel 2 
di atas. 

V.  KERANGKA PIKIR 

Pada  penelitian  ini,  topik  utama  yang  diangkat  adalah 
pemilihan dosen pembimbing skripsi program studi Komputasi 
Statistik.  Penelitian  ini  dilatarbelakangi  permasalahan  yang 
dialami mahasiswa ketika pemilihan dosen pembimbing skripsi. 
Sebagian  mahasiswa 
topik 
penelitian yang akan dipilih dan juga tidak mengetahui dosen 
dengan  kemampuan  yang  sesuai  dengan  kategori  topik  yang 
dipilih.  Sehingga  mahasiswa  akan  memiliki  kecenderungan 
untuk  memilih  dosen  yang  pernah  diajarnya.  Namun, 
dikarenakan  formasi  dosen  yang  terbatas  diperlukan  sistem 
yang mampu menyesuaikan pilihan dosen pembimbing dengan 

tidak  mengetahui  kategori 

 3 / 8 

Gambar 1. Bisnis Proses Pemilihan Dosen Pembimbing Saat ini 

Dari  hasil  analisis  sistem  berjalan  pada  Gambar  1, 
didapatkan  fishbone  seperti  pada  Gambar  2.  Permasalahan 
yang  ditemukan  dalam  bisnis  proses  pemilihan  dosen 
pembimbing saat ini ialah terdapat beberapa mahasiswa  yang 
belum  mengetahui  topik  penelitian  yang  ingin  ditelitinya. 
Sehingga  akan  sulit  menentukan  dosen  pembimbing  yang 
sesuai.  Mahasiswa  juga  membutuhkan  rekomendasi  dosen 
sesuai  berdasarkan  keinginan  dan 
pembimbing  yang 
kemampuan  yang  dimilikinya.  Selain  itu  ketika  mahasiswa 
memilih dosen yang sama dengan alokasi mahasiswa terbatas, 
diperlukan diskusi untuk menentukan mahasiswa yang pantas 
untuk  mendapatkan  dosen  tersebut.  Dalam  penentuan  dosen 
subjektivitas  mahasiswa 
pembimbing 
berdasarkan  kecocokan  penelitian  mahasiswa  dengan 
kemampuan  dosen  pembimbing.  Tidak  adanya  standar  baku 

terdapat 

tersebut 

 
 
 
 
 
  
 
 
formasi  formasi  mahasiswa  yang  ada.  Kerangka  pikir  secara 
lengkap pada Gambar 3. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Program  studi  Komputasi  Statistik  dapat  mengelola 
(menambah, mengedit, menghapus) informasi mengenai calon 
dosen  dosen  pembimbing  setelah  program  studi  Komputasi 
Statistik melakukan konfirmasi pada kriteria yang diinputkan.   
Mahasiswa  dapat  mengisi  bobot  antar  kriteria  yang 
digunakan  untuk  mengukur  minat  kategori  topik  penelitian. 
Setelah program studi Komputasi Statistik mengkonfirmasikan 
kriteria yang diinputkan. 

Mahasiswa  dapat  melihat  peringkat  dosen  pembimbing, 
berdasarkan hasil inputan dari bobot antar kriteria. Use case ini 
dapat  berjalan  setelah  program  studi  Komputasi  Statistik 
selesai  mengkonfirmasikan  informasi  dosen  pembimbing  dan 
mahasiswa mengisi bobot antar kriteria.    

Mahasiswa  dapat  mengisi 

formulir  pilihan  dosen 
pembimbing  untuk  menentukan  pilihan  dosen  pembimbing 
yang akan membimbing mahasiswa tersebut. 

Gambar 3. Kerangka pikir penelitian 

VI. HASIL DAN PEMBAHASAN 

Gambar 4. Diagram use case 

Berdasarkan hasil analisis sebelumnya, perancangan sistem 
usulan yang dilakukan dengan menggunakan use case diagram 
seperti di Gambar 4. Mahasiswa dan program studi Komputasi 
Statistik  untuk  masuk  ke  dalam  sistem  perlu  menggunakan 
autentikasi terlebih dahulu dengan sistem login. 

Program studi Komputasi Statistik dapat mengelola kriteria 
seperti menambah, mengubah, dan menghapus kriteria. kriteria 
berisikan tentang informasi mengenai kategori topik skripsi.  

Gambar 5. Bisnis proses usulan 

 4 / 8 

 
 
 
 
 
  
 Program  studi  Komputasi  Statistik  dan  mahasiswa  dapat 
melihat  hasil  keputusan  dosen  pembimbing.  Setelah  seluruh 
akan 
mahasiswa  memilih  dosen  pembimbing 
membimbing mahasiswa tersebut.  

yang 

Diagram proses bisnis yang diusulkan seperti pada Gambar 
5.  Dimulai  dari  program 
studi  Komputasi  Statistik 
menginputkan  kriteria,  yaitu  berisikan  tentang  kategori  topik 
skripsi beserta formula indeks untuk menentukan kategori topik 
penelitian yang sesuai. Tiap mahasiswa akan diberikan indeks 
yang  menunjukkan  kemampuan  yang  diperlukan  untuk 
memilih  kategori  penelitian 
Indeks  dihitung 
menggunakan nilai beberapa mata kuliah dengan formula yang 
ditentukan  oleh  program  studi  Komputasi  Statistik  hasil  dari 
penginputan  kriteria  ini  seperti  pada  Gambar  6.  Setelah 
beberapa seluruh  kategori topik penelitian selesai diinputkan, 
diperlukan  konfirmasi  terlebih  dahulu  hingga  seperti  pada 
Gambar 7. 

tersebut. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

dari inputan informasi calon  dosen pembimbing dapat dilihat 
pada gambar 10. 

Gambar 8. Halaman tambah dosen pembimbing 

Gambar 6. Halaman tambah kriteria 

Gambar 9. Halaman tambah dosen pembimbing 

Gambar 7. Halaman kriteria 

 Gambar 10. Halaman calon dosen pembimbing 

Setelah  kriteria  diinputkan,  program  studi  Komputasi 
Statistik  menginputkan  informasi  mengenai  calon  dosen 
pembimbing.  Informasi  yang  dibutuhkan  sistem  dari  dosen 
pembimbing adalah nomor identitas, nama, formasi bimbingan, 
pendidikan  terakhir,  serta  jumlah  buku,  jurnal,  seminar  yang 
diikuti, desain atau prototipe yang diciptakan, menjadi aggota 
organisasi  aktif,  dan  melakukan  pengabdian  masyarakat 
perkriteria  yang  telah  diinputkan  program  studi  Komputasi 
Statistik seperti pada gambar 8 dan gambar 9. Setelah informasi 
tersebut  diinputkan  didapatkan  nilai  alternatif  untuk  dosen, 
sesuai  bobot  yang  telah  ditentukan  yang  digunakan  sebagai 
indikator kesesuaian minat penelitian dengan mahasiswa. Hasil 

Setelah  program 

studi  Komputasi  Statistik 

selesai 
menginputkan  informasi  dosen  ke  sistem.  Mahasiswa  dapat 
mengisi pairwise comparison antar kriteria seperti pada gambar 
11.  Untuk  dihitung  konsistensi 
rasionya  menggunakan 
persamaan 1 berikut: 

𝐶𝑅 =  

𝐶𝐼

𝑅𝐼

(1) 

Dimana  RI  merupakan  Random  Consistency  Index  yang 
didapatkan dari tabel. CI  merupakan Consistency Index  yang 
didapatkan  dengan  persamaan  2.  Eigen  Value  maksimum 
(λ_max)  didapatkan  dengan  menjumlahkan  hasil  perkalian 
matriks  perbandingan  dengan  eigen  vector  utama  dan 
membaginya dengan jumlah elemen. 

 5 / 8 

 
 
 
 
 
 
 
 
 
    
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Nilai  ordinat  de-fuzzy-fikasi  didapatkan  dengan  mencari 
nilai  minimum  dari  setiap  hasil  sintesis  fuzzy  perkategori 
seperti pada persamaan 7. 

 𝑑′(𝐴𝑖) = min 𝑉(𝑆𝑖 ≥ 𝑆𝑘) 

(7) 

Setelah mendapatkan nilai ordinat defuzzifikasi dari setiap 

kategori dilakukan normalisasi menggunakan formula 8. 

𝑊 = (𝑑(𝐴1), 𝑑(𝐴2), … , 𝑑(𝐴𝑛))

𝑇

(8) 

Dimana W adalah bilangan non-Fuzzy dan nilai ∑ 𝑊 = 1. 
Setelah didapatkan nilai bobot vector atau nilai prioritas kriteria 
diperoleh.  Nilai  ini  dikalkulasikan  dengan  nilai  indeks  yang 
telah didapat mahasiswa sebelumnya untuk dijadikan indikator 
ketertarikan  mahasiswa  yang  dapat  digunakan  untuk  menilai 
minat dan bakat mahasiswa dalam memilih salah satu kategori 
topik  penelitian  serta  sebagai  acuan  untuk  memberikan 
rekomendasi  kesesuaian  ketertarikan  dengan  calon  dosen 
pembimbing skripsi. 

Gambar 12. Halaman nilai akhir mahasiswa 

Setelah  mendapatkan  rekomendasi  hasil  prioritas  dosen 
pembimbing  yang  dapat  dilihat  pada  gambar  12.  Mahasiswa 
baru  dapat  memilih  dosen  pembimbing  yang  sesuai 
berdasarkan  saran  yang  telah  diberikan  sistem  seperti  pada 
gambar 13.  

Gambar 11. Halaman banding kriteria (pairwise comparison) 

N 

RI 

1 

0 

2 

0 

TABEL III 
TABEL CONSISTENCY INDEX 
4 

5 

7 

6 

3 

8 

9 

10 

0,58 

0,9 

1,12 

1,24 

1,32 

1,41 

1,45 

1,49 

𝐶𝐼 =  

λ𝑚𝑎𝑥−𝑛
𝑛−1

(2) 

 Apabila  konsistensi  rasio  lebih  dari  0,1  dapat  dikatakan 
bahwa pairwise comparison yang di inputkan mahasiswa tidak 
konsisten dan diperlukan mengulang hingga konsisten. Apabila 
hasil kurang dari 0,1 mengubah nilai matriks berpasangan antar 
kriteria dengan skala Triangular Fuzzy Number (TFN). Setelah 
itu  menghitung  nilai  sintesis  fuzzy  prioritas  (Si)  dengan 
menggunakan  persamaan  3.  Mendapatkan  Si  dengan  cara 
𝑗
mengalikan  ∑ 𝑀𝑖
yang  merupakan  hasil  kumulatif  dari 
setiap baris lower, middle dan upper seperti pada persamaan 4, 
𝑗
dengan satu per ∑ ∑ 𝑀𝑖
 yang merupakan hasil kumulatif 
dari setiap baris dan kollom upper, middle, dan lower.  

𝑚
𝑗=1

𝑚
𝑗=1

𝑛
𝑖=1

𝑆𝑖 =   ∑ 𝑀𝑖

𝑚
𝑗=1

𝑗 ×

1
𝑗
𝑚
∑ 𝑀𝑖
𝑗=1

 , ∑

𝑚
𝑗=1

𝑢𝑗

𝑛
𝑖=1

∑
dimana 
𝑚
 , ∑ 𝑚𝑗 
𝑗=1
sedangkan 
1
𝑚
,∑ 𝑚𝑖
𝑗=1

𝑢𝑖𝑚
𝑗=1

∑

,∑

𝑙𝑖𝑚
𝑗=1

𝑗
𝑚
∑ 𝑀𝑖
𝑗=1

= ∑

𝑙𝑗𝑚
𝑗=1

1
𝑗
𝑚
∑ 𝑀𝑖
𝑗=1

∑

𝑛
𝑖=1

=

(3) 

(4) 

(5) 

Setelah  mendapakatkan  nilai  sintesis  fuzzy  dari  setiap 
kriteria.  Menghitung  derajat  keanggotaan  dari  perbandingan 
nilai  sintesis  fuzzy  untuk  memperoleh  vector  prioritas.  Jika 
hasil  yang  diperoleh  dari  setiap  matriks  fuzzy,  𝑀2 =
(𝑙2, 𝑚2, 𝑢2) ≥   𝑀1 = (𝑙1, 𝑚1, 𝑢1) ,  maka  nilai  vector  dapat 
diformulakan 
𝑉(𝑀2 ≥ 𝑀1) =
sup [min(𝜇𝑀1(𝑥), 𝜇𝑀2(𝑦))] atau sama seperti peramaan 6. 

sebagai 

berikut 

𝑉(𝑀2 ≥ 𝑀1) = {

1,    𝑖𝑓 𝑚2 ≥ 𝑚1
0,   𝑖𝑓 𝑙1 ≥ 𝑢2
𝑙1−𝑢2
(𝑚2−𝑢2)−(𝑚1−𝑙1)

, 𝑙𝑎𝑖𝑛𝑦𝑎

(6) 

Gambar 13. Halaman pilih dosen pembimbing 

Seluruh  pilihan  dosen  pembimbing  dari  mahasiswa 
dikumpulkan dan diurutkan berdasarkan nilai kesesuaian antara 
dosen  pembimbing  dengan  mahasiswa  dan  dengan  jumlah 
alokasi  mahasiswa  perdosen  seperti  pada  gambar  14.  Setelah 

 6 / 8 

 
 
 
 
    
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
semua  mahasiswa  mendapatkan  dosen  pembimbingnya. 
Program  studi  Komputasi  Statistik  menerima  laporan  dari 
sistem tersebut. 

Gambar 14. Halaman keputusan mahasiswa 

Pada  gambar  15  merupakan  desain  database  yang  telah 
tentang 
diimplementasikan.  Entitas  mahasiswa  berisikan 
informasi  mahasiswa.  Entitas  nilai  mahasiswa  berisikan  nilai 
nilai  yang  dimiliki  mahasiswa.  Entitas  dosen  berisikan 
informasi  mengenai  dosen  dan  alokasi  mahasiswa.  Entitas 
penilaian  dosen  berisikan  informasi  terperinci  antara  dosen 
dengan setiap kriteria. Entitas banding kriteria berfungsi untuk 
menyimpan hasil pairwise comparison antar kriteria dari setiap 
mahasiswa.  Hasil  dari  pairwise  comparison  disimpan  pada 
entitas banding kriteria.  

Gambar 15. Rancangan ERD 

Setelah seluruh pengimplementasian telah selesai dilakukan. 
Dilakukan  pengujian  sistem  yaitu  Blackbox  Testing,  System 
Usability Scale dan uji validasi kesesuaian dosen pembimbing. 
Hasil  dari  blackbox  testing  menunjukkan  bahwa  seluruh 
telah  berfungsi 

telah  diimplementasikan 

sistem  yang 
sebagaimana yang telah diharapkan. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Hasil  dari  SUS  didapatkan  dari  26  responden  mahasiswa 
didapatkan skor SUS sebesar 75,59. Menurut Brook [11] Sebuah 
website  dapat  termasuk  kategori  marginal  high  atau  diterima 
harus  memiliki  skor  SUS  lebih  dari  70.  Sedangkan  menurut 
Bangor [12] skor SUS dianggap Good apabila bernilai lebih dari 
70,4.  Sehingga  dapat  disimpulkan  bahwa  website  sistem 
pendukung  keputusan  dalam  pemilihan  dosen  pembimbing 
sudah  dapat  di  terima.  Namun,  berdasarkan  hasil  evaluasi 
pengguna  beberapa  tampilan  perlu  dikembangkan  lagi  pada 
tambilan agar lebih baik dan dapat digunakan lebih efektif dan 
efisien. 

Uji Validasi hasil kesesuaian dosen pembimbing dilakukan 
dengan  menyesuaikan  kepuasan  mahasiswa  terhadap  dosen 
pembimbing  skripsinya  saat  ini  dengan  hasil  rekomendasi 
dosen  pembimbing  skripsi.  Dari  19  mahasiswa  yang  mengisi 
kuesioner evaluasi dan mencoba sistem, 12 mahasiswa merasa 
sangat  setuju  bahwa  dosen  pembimbing  saat  ini  merupakan 
dosen  pembimbing  skripsi  yang  tepat,  4  mahasiswa  merasa 
setuju  bahwa  dosen  pembimbing  saat  ini  merupakan  dosen 
yang  tepat,  sisanya  3  mahasiswa  merasa  tidak  setuju  bahwa 
dosen  pembimbing  saat  ini  merupakan  dosen  yang  tepat. 
Apabila dilihat dari hasil rekomendasi dari sistem pendukung 
keputusan  dalam  pemilihan  dosen  pembimbing  skripsi  12 
mahasiswa  dapat  diprediksi  dengan  benar.  Dibawah  ini 
merupakan perhitungan akurasi hasil rekomendasi dari sistem 
pendukung  keputusan  dalam  pemilihan  dosen  pembimbing. 
Didapatkan nilai akurasi sebesar 73,68 persen. 

TABEL IV 
TABEL Hasil Prediksi 

Nilai Sebenarnya 

T 

13 

3 

F 

2 

1 

T 

F 

Nilai Prediksi 

𝐴𝐾𝑈𝑅𝐴𝑆𝐼 =  

𝑇𝑃 + 𝑇𝑁
𝑇𝑃 + 𝑇𝑁 + 𝐹𝑃 + 𝐹𝑁

× 100% 

𝐴𝐾𝑈𝑅𝐴𝑆𝐼 =  

13 + 1
13 + 1 + 2 + 3

× 100% = 73.68% 

Berdasarkan  hasil  evaluasi  dari  pengguna.  Dari  26 
responden  mahasiswa.  19  mahasiswa  merasa  sangat  setuju 
bahwa  dengan  adanya  sistem  ini  dapat  membuat  pemilihan 
dosen  pembimbing  menjadi  lebih  efisien  dan  efektif.  4 
mahasiswa  merasa  setuju  dan  2  orang  merasa  tidak  setuju 
bahwa  dengan  adanya  sistem  ini  dapat  membuat  pemilihan 
dosen pembimbing menjadi lebih efisien dan efektif. Sehingga, 
dapat  disimpulkan  dari  26  mahasiswa  88.46  persen  setuju 
bahwa sistem usulan saat ini terbukti efisien dan efektif. 

VII. 

PENUTUP 

Berdasarkan  penelitian  yang 

terkait 
pengembangan  sistem,  dapat  diperoleh  kesimpulan  sebagai 
berikut: 

telah  dilakukan 

 7 / 8 

prodiusername  varchar(50)nama  varchar(50)password  varchar(100)created_at  dateupdate_at  datepenilaian_dosenid  varchar(10)id_dosen  varchar(50) (FK)id_kriteria  varchar(3) (FK)buku  int(11)jurnal  int(11)seminar  int(11)rancangan  int(11)pengabdian_msy  varchar(6)anggota_org  varchar(6)nilai  doublenilai_mahasiswanim  varchar(10) (FK)alin  doublebing1  doublekal1  double<continued>nilai_kriteria_mahasiswanim  varchar(10) (FK)id_kriteria  varchar(3) (FK)nilai  int(11)mahasiswanim  varchar(10)nama  varchar(100)kelas  varchar(5)password  varchar(100)created_at  dateupdated_at  datekriteriaid  varchar(3)nama  varchar(100)alias  varchar(20)indek  textkeputusannim  varchar(10) (FK)pilihan_1  varchar(50) (FK)nilai_pil1  doublepilihan_2  varchar(50) (FK)nilai_pil2  doublepilihan_3  varchar(50) (FK)nilai_pil3  doublepilihan_4  varchar(50) (FK)nilai_pil4  doublepilihan_5  varchar(50) (FK)nilai_pil5  doubleindeknimsisKSKbdSIMKDSktiplspkdosenid  varchar(50)nama  varchar(100)pend_terakhir  enum('S1','S2','S3','')kuota_ks  tinyint(4)kuota_lain  tinyint(4)data_dosenidid_kriterianamakuota_kskuota_lainnilaibanding_kriterianim  varchar(10) (FK)id_kriteria1  varchar(3) (FK)id_kriteria2  varchar(3) (FK)nilai  double 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

[12] Bangor,  Aaron,  Philip  Kortum,  and  James  Miller.  ""Determining  what 
individual SUS scores mean: Adding an adjective rating scale."" Journal of 
usability studies 4, no. 3 (2009): 114-123. 

1.  Telah dibangun suatu indikator ketertarikan mahasiswa 
yang  dapat  digunakan  untuk  menilai  minat  dan  bakat 
mahasiswa  dalam  memilih  salah  satu  kategori  topik 
penelitian.  

2.  Telah  dibangun  suatu  sistem  informasi  yang  dapat 
memberikan  pertimbangan  berupa  skor  rekomendasi 
calon  dosen  pembimbing  yang  sesuai  dengan  minat 
penelitian mahasiswa yang dapat digunakan mahasiswa 
dalam  memilih  dosen  pembimbing  sesuai  ketertarikan 
topik penelitian. 

3.  Sistem  pendukung  keputusan  pemilihan  dosen 
pembimbing skripsi program studi Komputasi Statistik 
usulan  terbukti  lebih  efisien  dan  efektif  dibanding 
sistem yang sedang berjalan. 

untuk  penelitian  selanjutnya  kami  menyarankan  untuk 
melakukan penelitian lebih lanjut: 
1.  Beberapa  pengguna  merasa  kesulitan  dengan  banding 
kriteria  dikarenakan 
antarmuka  yang 
sederhana  dan  tidak  mendukung  tampilan  di  layar 
ponsel pintar. Sehingga diperlukan perbaikan tampilan 
yang  dapat  memperbaiki  user  interface  (UI)  dan 
meningkatkan user experience (UX) pengguna. 

tampilan 

2.  Melakukan  perancangan/pembangunan  Application 
Programming  Interface  (API)  agar  data-data  yang 
dibutuhkan 
langsung  dengan  database 
terintegrasi 
Politeknik statistik STIS. 

DAFTAR PUSTAKA 
[1]  B.  Damanik,  ""Pengaruh  Minat  Baca  dan  Peran  Dosen  Pembimbing 
Idaarah:  Jurnal 

Terhadap  Keberhasilan  Penulisan  Tugas  Akhir"", 
Manajemen Pendidikan, vol. 2, no. 2, p. 198, 2018.  

[2]  E.  Turban,  J.  Aronson,  T.  Liang  and  R.  McCarthy,  Decision  support 
systems and intelligent syjstems, 7th ed. New Delhi: Prentice-Hall of India, 
2006, p. 105. 

[3]  Saaty, T. L. (1980) The Analytic Hierarchy Process. New York: McGraw 

Hill. 

[4]  Y.  Liu,  C.  Eckert  and  C.  Earl,  ""A  review  of  fuzzy  AHP  methods  for 
decision-making  with  subjective  judgements"",  Expert  Systems  with 
Applications, vol. 161, p. 113738, 2020. 

[5]  A.  Abdullah  and  M.  Pangestika,  ""Perancangan  Sistem  Pendukung 
Keputusan  dalam  Pemilihan  Dosen  Pembimbing  Skripsi  Berdasarkan 
Minat Mahasiswa dengan Metode AHP (Analytical Hierarchy Process) di 
Universitas  Muhammadiyah  Pontianak"",  Jurnal  Edukasi  dan  Penelitian 
Informatika (JEPIN), vol. 4, no. 2, p. 184, 2018.  

[6]  L. Zadeh, ""Fuzzy sets"", Information and Control, vol. 8, no. 3, pp. 338-353, 

1965. Available: 10.1016/s0019-9958(65)90241-x  

[7]  F.  Saputra,  N.  Hidayat  and  M.  Furqon,  ""Penerapan  Metode  Fuzzy 
Analytical  Hierarchy  Process  (F-AHP)  Untuk  Menentukan  Besar 
Pinjaman Pada Koperasi"", Jurnal Pengembangan Teknologi Informasi dan 
Ilmu Komputer, vol. 2, no. 4, pp. 1761-1767, 2017. 

[8]  R.Nasrullah.  “Sistem  Komputerisasi  Pemilihan  Dosen  Pembimbing 

dengan Metode TOPSIS”, 2013. 

[9]   A.  Abdullah  and  M.  W.  Pangestika,  “Perancangan  Sistem  Pendukung 
Keputusan  dalam  Pemilihan  Dosen  Pembimbing  Skripsi  Berdasarkan 
Minat Mahasiswa dengan Metode AHP (Analytical Hierarchy Process) di 
Universitas  Muhammadiyah  Pontianak,”  Jurnal  Edukasi  dan  Penelitian 
Informatika (JEPIN), vol. 4, no. 2, p. 184, 2018. 

[10] A. Abdullah and M. W. Pangestika, “Rancang Bangun Sistem Pendukung 
Keputusan Dalam Pemilihan Dosen Pembimbing Skripsi Dengan Metode 
AHP di UM Pontianak,” CYBERNETICS, vol. 2, no. 02, p. 234, 2018. 
[11] Brooke, John. ""SUS: a retrospective."" Journal of Usability Studies 8, no. 2 

(2013): 29-40. 

 8 / 8 

 
 
 
 
 
"
221709870,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pengembangan Metode Neural Machine Translation 
Berdasarkan Parameter Neural Network 
(Studi Kasus: Bahasa Jerman – Inggris) 

Muhammad Yusuf Aristyanto (221709870, 4SD1) 
Dosen Pembimbing: Robert Kurniawan, S.ST., M.Si.  

Ringkasan—Manusia sebagai makhluk sosial yang selalu ingin 
berhubungan dengan manusia lainnya memaksa manusia untuk 
saling  berkomunikasi.  Di  sinilah  peran  bahasa  menjadi  amat 
penting, karena dengan dengan adanya bahasa, kita bisa dengan 
mudah  mengerti  apa  yang  ingin  disampaikan  oleh  orang  lain. 
Untuk itu, perlu adanya media yang dapat membantu memahami 
berbagai  bahasa  di  dunia,  salah  satunya  adalah  mesin 
penerjemah.  Salah  satu  metode  yang  dapat  digunakan  untuk 
membuat mesin penerjemah adalah  Neural Machine Translation 
(NMT). NMT yang sekarang sudah ada masih memiliki berbagai 
kekurangan  dan  perlu  dilakukan  pengembangan  lebih  jauh. 
Diantaranya  pada  masalah  overfitting  yang  membuat  modelnya 
kurang bisa melakukan generalisasi pada data lain yang diujikan. 
Banyak  hal  yang  mempengaruhi  performa  dari  NMT  tersebut, 
salah  satunya  adalah  ukuran  parameter  yang  digunakan  dan 
arsitektur model yang digunakan. Namun belum ada ukuran pasti 
yang  dapat  digunakan  untuk  menghasilkan  model  dengan 
performa yang terbaik. Sehingga penelitian ini bertujuan untuk 
mengembangkan arsitektur model NMT dan melakukan simulasi 
pada masing-masing parameter Neural Network dan ukuran pada 
arsitektur  modelnya,  antara  lain  batch  size,  epoch,  optimizer, 
activation  function,  dan  dropout  rate.  Hasil  yang  didapatkan 
adalah model pengembangan dapat mengatasi masalah overfitting 
dari model sebelumnya dengan akurasi sebesar 72,24% dan skor 
BLEU sebesar 45,83% yang dilakukan pada data uji lainnya. 
Kata Kunci—Arsitektur, Neural Network, NMT, Parameter 

I.  LATAR BELAKANG 
Manusia merupakan sebuah entitas dalam kehidupan sosial, 
oleh  karenanya  manusia  disebut  juga  sebagai  mahluk  sosial. 
Salah satu media yang digunakan dalam bersosialisasi adalah 
bahasa. Bahasa mempunyai sifat yang unik, yang berarti bahwa 
setiap  bahasa  mengandung  ciri  khas  tersendiri  yang  tidak 
dimiliki  oleh  bahasa 
lain  [1].  Bahasa  merupakan  alat 
komunikasi  yang  digunakan  seseorang  untuk  menyampaikan 
ide, gagasan, konsep atau perasaan kepada orang lain [2]. Hal 
ini  berarti,  tanpa  adanya  bahasa,  manusia  akan  sulit  untuk 
menjalankan kehidupannya sebagai makhluk sosial. 

Keberagaman bahasa yang ada di dunia, membuat kita sulit 
untuk memahami satu sama lain. Sehingga perlu adanya media 
untuk menjembatani kita untuk dapat mengatasi keberagaman 
tersebut.  Media  ini  salah  satunya  ialah  mesin  penerjemah. 
Keberadaan  mesin  penerjemah  dapat  membantu  kita  untuk 
memahami  dan  mempelajari  bahasa  lain  yang  tidak  kita 
mengerti  sebelumnya.  Mesin  penerjemah  hadir  sebagai  salah 
satu solusi untuk menjembatani interaksi antar-suku. Selain itu, 
mesin penerjemah juga digunakan dalam pemeliharaan bahasa 
sebagai  wujud  dari  kesadaran  dalam  melestarikan  bahasa 
daerah [3]. 

Mesin  penerjemah  adalah  sebuah  media  yang  dapat 
melakukan pemindahan bahasa satu ke bahasa lain  [4]. Salah 
satu  metode  yang  digunakan  untuk  membangun  suatu  mesin 
penerjemah adalah Neural Machine Translation atau bisa kita 
sebut  sebagai  NMT.  NMT  adalah  sebuah  pendekatan  baru 
dalam  penerjemahan  mesin  yang  menggunakan  arsitektur 
Recurrent  Neural  Network  (RNN)  pada  bagian  encoder    dan 
decoder-nya [5]. Neural Machine Translation yang digunakan 
saat  ini  oleh  para  peneliti  mesin  penerjemah  adalah  NMT 
berbasis attention, sebagai hasil penelitian yang telah dilakukan 
oleh Bahdanau [6]. Namun model yang dihasilkan masih belum 
mampu  menangani  masalah  overfitting  yang  membuat 
modelnya  tidak  mampu  melakukan  generalisasikepada  data 
lainnya selain data latih. Kemudian, Liu  [7] dan Sennrich [8] 
melakukan pengembangan model dari model dasar yang dibuat 
oleh  Bahdanau.  Namun  keduanya  masih  belum  ada  yang 
mencoba mengatasi permasalahan pada masalah overfitting.  

NMT  merupakan  metode  penerjemahan  bahasa  yang 
berbaasis Neural Network. Neural Network adalah suatu proses 
pengolahan  informasi  yang  terinspirasi  dari  sistem  kerja 
jaringan  otak  manusia  [9].  Neural  Network  terdiri  dari 
kumpulan node (neuron) dan relasi. Ada tiga tipe node (neuron) 
yaitu, input, hidden, dan output. SedaVngkan relasi, berfungsi 
untuk  menghubungkan  dua  buah  node  dengan  bobot  tertentu 
dan  juga  terdapat  arah  yang  menujukkan  aliran  data  dalam 
proses  [10].  Neural  Network  diharapkan  dapat  meniru 
kemampuan  otak  manusia,  seperti  mengingat,  menghitung, 
mengeneralisasi,  dan  adaptasi.  Neural  Network  berusaha 
meniru  struktur  /  arsitektur  dan  cara  kerja  otak  manusia 
sehingga diharapkan mampu menggantikan beberapa pekerjaan 
manusia.  Neural  Network  berguna  untuk  memecahkan 
persoalan yang berkaitan dengan pengenalan pola, klasifikasi, 
prediksi  dan  data  mining  [11].  Salah  satu  keuntungan 
menggunakan Neural Network adalah bahwa Neural Network 
sangat andal untuk mengolah data [12].  

learning  rate,  activation 

Pada Neural Network, terdapat sebuah istilah yang bernama 
hyperparameter  yang  harus  ditentukan  oleh  peneliti  untuk 
selanjutnya  digunakan  dalam  proses  pelatihan  model  pada 
Neural Network. Beberapa contoh hyperparameter antara lain 
function, 
epoch,  batch,  split, 
optimizer,  dan  jumlah  layers.  Selain  itu,  kinerja  Neural 
Network 
juga  dipengaruhi  oleh  hyperparameter  yang 
menyusunnya, diantaranya epoch, layer, split data, dan jumlah 
data  [13].  Namun  belum  ada  riset  yang  dilakukan  untuk 
menentukan ukuran yang ideal atau pasti dari masing-masing 
hyperparameter agar dapat menghasilkan performa atau kinerja 
yang terbaik. 

 1 / 8 

 
 
 
 
Selain itu, ada hal yang perlu diperhatikan, yakni performa 
dari  Neural  Network.  Hal  yang  mempengaruhi  performa 
training  Neural  Network  adalah  penentuan  arsitektur  Neural 
Network (jumlah layer dan unit) dan algoritma pembelajaran. 
Tidak  ada  ketentuan  yang  pasti  dalam  menentukan  arsitektur 
Neural  Network,  tetapi  ukuran  arsitektur  yang  terlalu  kecil 
tidak  mampu 
dapat  mengakibatkan  Neural  Network 
mempelajari  data  yang  dimasukkan.  Sebaliknya,  ukuran 
arsitektur yang terlalu besar juga akan mempunyai sifat lemah 
dalam menggeneralisasi data yang baru, serta memakan banyak 
waktu untuk melakukan pelatihan model [14]. 

Berdasarkan  permasalahan  di  atas,  maka  peneliti  mencoba 
untuk  memodifikasi 
arsitektur  model  NMT  dengan 
menambahkan  layer,  yang  diharapkan  dapat  meningkatkan 
performa akurasi pada model yang didapatkan. Selain itu juga 
akan dilakukan simulasi pada masing-masing hyperparameter 
pada  Neural  Network  dan  ukuran  pada  arsitektur  modelnya, 
antara lain batch size, epoch, optimizer, activation function, dan 
dropout rate, untuk mendapatkan model yang paling optimal. 
Kemudian  peneliti  juga  akan  menggunakan  hyperparameter 
tersebut  untuk  membuat  model  baru  dengan  dataset  yang 
berbeda. 

II.  TUJUAN PENELITIAN 

Adapun 

tujuan  penelitian  penulis  berdasarkan  dari 

identifikasi masalah di atas adalah sebagai berikut: 
1.  Mengembangkan  arsitektur  model  NMT  pada  mesin 

penerjemah bahasa 

2.  Membandingkan 

performa 

penggunaan 
hyperparameter  Neural  Network  (batch  size,  epoch, 
optimizer,  activation  function,  dan  dropout  rate)  pada 
model arsitektur NMT yang dikembangkan 

dari 

III. PENELITIAN TERKAIT  
Pada  Gambar  1  di  bawah,  ditunjukan  beberapa  penelitian 

yang mendasari penelitian kali ini. 

Gambar 1. Penelitian Terkait 

Penelitian terkait dengan Analisis Metode  Neural Machine 
Translation telah banyak dikaji. Yang pertama dilakukan oleh 
Abidin  dengan  penelitian  yang  berjudul  “Penerapan  Neural 
Machine  Translation untuk Eksperimen Penerjemahan secara 
Otomatis pada Bahasa Lampung – Indonesia”. Hasil penelitian 
tersebut  menunjukan  bahwa  penerjemahan  bahasa  Lampung-
Indonesia pada 25 kalimat tunggal tanpa OOV diperoleh nilai 
Bilingual Evaluation Under Study (BLEU) sebesar 41.79 dan 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

tanpa  Out-of-Vocabulary 

25  kalimat  majemuk 
(OOV) 
diperoleh  nilai  BLEU  sebesar  37.5.  Adapun  persamaan 
penelitian yang dilakukan oleh Abidin dengan penelitian kali 
ini  terletak  pada  metode  penerjemah  yang  digunakan,  yakni 
Neural Machine Translation (NMT) dan metode evaluasi yang 
digunakan, yakni  Bilingual Evaluation Under Study (BLEU). 
Sedangkan perbedaanya terletak pengujiannya yang dilakukan 
menggunakan  25  kalimat  tunggal  dan  25  kalimat  majemuk 
tanpa Out-of-Vocabulary (OOV) [15]. 

Kedua, penelitian yang dilakukan oleh Hadi yang berjudul 
“Uji  Akurasi  Mesin  Penerjemah  Statistik  (MPS)  Bahasa 
Indonesia  ke  Bahasa  Melayu  Sambas  dan  Mesin  Penerjemah 
Statistik (MPS) Bahasa Melayu Sambas ke Bahasa Indonesia”. 
Hasil  penelitian 
tersebut  menunjukan  bahwa  Mesin 
Penerjemah  Statistik  berhasil  digunakan  menerjemahkan 
Bahasa  Indonesia  ke  bahasa  Melayu  Sambas  dan  sebaliknya 
dengan  tingkat  akurasi  yang  dihasilkan  sebanyak  55%  pada 
pengujian pertama dan 49% pada pengujian kedua. Sedangkan 
penilaian yang dilakukan oleh ahli bahasa menghasilkan skor 
Bilingual  Evaluation  Under  Study  (BLEU)  yang  berbeda 
tergantung  pada  pemahaman  ahli  bahasa.  Adapun  persamaan 
penelitian yang dilakukan oleh Hadi dengan penelitian kali ini 
terletak pada metode evaluasi yang digunakan, yakni Bilingual 
Evaluation  Under  Study  (BLEU).  Sedangkan  perbedaanya 
terletak  pada  metode  penerjemahan  yang  digunakan,  yakni 
metode Mesin Penerjemah Statistik (MPS) [16]. 

Ketiga,  penelitian  yang  dilakukan  oleh  Nurhikmat  yang 
berjudul  “Implementasi  Deep  Learning  untuk 
Image 
Classification Menggunakan Algoritma Convolutional Neural 
Network  (CNN)  pada  Citra  Wayang  Golek”.  Hasil  penelitian 
tersebut  menunjukan  bahwa  Algoritma  CNN  berhasil 
digunakan  untuk  mengklasifikasikan  citra  wayang  golek 
berdasarkan  tokoh-tokohnya.  Model  tersebut  menghasilkan 
tingkat akurasi sebanyak 95% pada pelatihan model dan 90% 
pada  validasi  model.  Adapun  persamaan  penelitian  yang 
dilakukan  oleh  Nurhikmat  dengan  penelitian  kali  ini  terletak 
pada metode untuk meningkatkan performa pada model yang 
digunakan,  yakni  melalui  hyperparameter.  Sedangkan 
perbedaanya terletak pada metode pemodelan yang digunakan, 
yakni metode Convolutional Neural Network (CNN) [13]. 

Keempat, penelitian yang dilakukan oleh Liu yang berjudul 
“Very  Deep  Transformers  for  Neural  Machine  Translation”. 
Liu mengeksplorasi penerapan model Very Deep Transformers 
untuk  Neural  Machine  Translation  (NMT).  Model  tersebut 
meghasilkan skor BLEU pada pasangan bahasa Inggris-Prancis 
43,8 dan pasangan bahasa Inggris-Jerman sebesar 30.1. Adapun 
persamaan  penelitian  yang  dilakukan  oleh  Liu  dengan 
penelitian  kali  ini  terletak  pada  metode  penerjemah  yang 
digunakan,  yakni  Neural  Machine  Translation  (NMT)  dan 
metode  evaluasi  yang  digunakan,  yakni  Bilingual  Evaluation 
Under  Study  (BLEU).  Sedangkan  perbedaanya  terletak  pada 
metode pendukung model yang digunakan, yakni metode Very 
Deep Transformers [7]. 

Kelima,  penelitian  yang  dilakukan  oleh  Sennrich  yang 
berjudul “Improving Neural Machine Translation Models with 
Monolingual  Data”.  Sennrich  menerapkan  metode  untuk 
melatih  dengan  data  monolingual  tanpa  mengubah  arsitektur 
neural network. Dari metode tersebut, Sennrich mendapatkan 

 2 / 8 

 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

B.  Recurrent Neural Network (RNN) 

RNN  adalah  jenis  arsitektur  jaringan  saraf  tiruan  yang 
pemrosesannya berulang kali dipanggil untuk memproses input 
data  sekuensial  [17].  Pemodelan  RNN  dapat  menyelesaikan 
berbagai  tugas  kategorisasi  kalimat  dan  dapat  melakukan 
klasifikasi  [18],  karena  kemampuan  dalam  memprosesnya 
dipanggil berulang-ulang dengan hasil dapat menangani input 
dan outputvariable yang panjangnya bervariasi.  

Pada  dasarnya,  RNN  adalah  jaringan  syaraf  tiruan  yang 
menggunakan rekurensi dengan memanfaatkan data masa lalu. 
Karena itu, beberapa studi terbaru mengenai RNN cukup kuat 
untuk permasalahan klasifikasi [19]. RNN memiliki arsitektur 
yang dapat berbentuk sekuensial seperti Gambar 2 berikut. 

Gambar 2. Arsitektur RNN 

C.  Neural Machine Translation (NMT) 

Neural  Machine  Translation  atau  bisa  kita  sebut  sebagai 
NMT  adalah  sebuah  pendekatan  baru  dalam  penerjemahan 
mesin yang menggunakan arsitektur Recurrent Neural Network 
(RNN)  pada  bagian  encoder    dan  decoder-nya  [5].  Neural 
Machine Translation yang digunakan saat ini oleh para peneliti 
mesin  penerjemah  adalah  NMT  berbasis  attention,  sebagai 
hasil penelitian yang telah dilakukan oleh Bahdanau [6]. 

Untuk  melakukan  penerjemahan  menggunakan  jaringan 
saraf,  yang  pertama  dilakukan  adalah  menghubungkan  RNN 
encoder dengan RNN decoder dari model bahasa.  
•  RNN encoder akan memproses kalimat dalam kata bahasa 
sumber dari kata demi kata dan menghasilkan representasi 
kalimat masukan.  

•  Sedangkan  RNN  decoder  atau  model  bahasa  akan 
mengambil  output  dari  encoder  sebagai  masukan  dan 
menghasilkan terjemahan kata demi kata [15]. 

peningkatan  skor  BLEU  pada  bahasa  Inggris-Jerman  sebesar 
2,8  sampai  3,7  dan  pada  bahasa  Turki-Inggris  sebesar  2,1 
sampai 3,4. Adapun persamaan penelitian yang dilakukan oleh 
Sennrich dengan penelitian kali ini terletak pada pada metode 
penerjemah  yang  digunakan,  yakni  Neural  Machine 
Translation (NMT) dan metode evaluasi yang digunakan, yakni 
Bilingual  Evaluation  Under  Study  (BLEU).  Sedangkan 
perbedaanya  terletak  pada  metode  pendukung  model  yang 
digunakan, yakni Data Monolingual [8]. 

Pada  penelitian  ini,  peneliti  bermaksud  ingin  melakukan 
pengembangan dari metode NMT yang dibuat oleh Bahdanau 
[6], dengan cara menambahkan beberapa layer pada arsitektur 
model  NMT,  kemudian 
juga  melakukan  simulasi  pada 
beberapa hyperparameter Neural Network. Sehingga nantinya 
dapat  mengatasi  masalah  overfitting  pada  model  dasar  yang 
dibuat oleh Bahdanau [6]. 

IV. METODE PENELITIAN  

A.  Dataset 

dari 

situs 

didapatkan 

web 
dengan 

Pada  penelitian  ini,  peneliti  menggunakan  dataset  dummy 
manythings 
yang 
(https://www.manythings.org/anki/) 
judul  Tab-
delimited Bilingual Sentence Pairs. Merupakan situs web yang 
berisi  berbagai  macam  terjemahan  dari  berbagai  bahasa  di 
seluruh  dunia.  Di  sini  penliti  menyebutnya  sebagai  pasangan 
kalimat (Corpus Parallel) dari bahasa satu ke bahasa lainnya. 
Pada  Tabel  I  di  bawah,  ditunjukkan  contoh  sampel  corpus 
parallel yang digunakan pada penelitian ini. 

TABEL I 
CONTOH CORPUS PARALLEL 

Inggris 

Hello! 

Go away. 

Jerman 

Sers! 

Geh weg! 

That was ours. 

Das war unsrer. 

The river is wide. 

Der Fluss ist breit. 

He should thank you. 

Er sollte dir danken. 

Inggris 

Jump. 

Go away. 

Spanyol 

Salte. 

Lárguese. 

She is happy. 

Ella es feliz. 

It doesn't matter. 

No es importante. 

Tonight is the night. 

Esta noche es la noche. 

Peneliti  akan  mengambil  dua  buah  dataset,  yang  pertama 
digunakan untuk melakukan proses pelatihan model dan untuk 
mendapatkan  kombinasi  parameter  Neural  Network  yang 
terbaik,  yaitu  Bahasa  Inggris-Jerman.  Sedangkan  yang  kedua 
akan  digunakan  untuk  menguji  model  dan  parameter  terbaik 
tersebut,  yaitu  Bahasa  Inggris-Spanyol.  Selanjutnya,  Corpus 
Parallel akan digunakan sebagai masukan dari metode Neural 
Machine Translation. 

Gambar 3. Arsitektur Model Encoder-Decoder 

Gambar 3 di atas mengilustrasikan tentang bagaimana cara 
secara 
menghubungkan  model  encoder  dan  decoder 
bersamaan. Arsitektur model untuk terjemahan mesin dikenal 
sebagai model encoder-decoder [20]. 

 3 / 8 

 
 
 
 
 
 
D.  Long-Short Term Memory (LSTM) 

Menurut  Burtsev  (2018),  LSTM  merupakan  varian  khusus 
dari  Recurrent  Neural  Network  (RNN)  yang  dikembangkan 
untuk mengatasi masalah vanishing gradient pada RNN ketika 
digunakan  dalam  environment 
[21]. 
Arsitektur LSTM ditunjukan pada Gambar 4 di bawah ini. 

long-term 

secara 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

•  Activation  Function  mendefinisikan  bagaimana  jumlah 

tertimbang dari input diubah menjadi output 

•  Dropout  Rate  adalah  regularisasi  jaringan  syaraf  saat 
neuron dipilih secara acak dan tidak dipakai saat pelatihan 

F.  Bilingual Evaluation Under Study (BLEU) 

Model yang sudah terbentuk akan dievaluasi menggunakan 
metode BLEU. BLEU adalah sebuah algoritma yang berfungsi 
untuk mengevaluasi kualitas dari sebuah hasil terjemahan yang 
telah  diterjemahkan  oleh  mesin  dari  satu  bahasa  alami  ke 
bahasa  lain.  BLEU  mengukur  modified  n-gram  precission 
score  antara  hasil  terjemahan  otomatis  dengan  terjemahan 
rujukan dan menggunakan konstanta yang dinamakan  brevity 
penalty.  Rumus  BLEU  ditunjukkan  pada  persamaan  (1),  (2), 
dan (3) berikut [20]: 

𝐵𝑃𝐵𝐿𝐸𝑈 = {

1          𝑖𝑓 𝑐 > 𝑟 

𝑟
𝑒1−
𝑐       𝑖𝑓 𝑐 ≤ 𝑟    

                                                  (1) 

Gambar 4. Arsitektur LSTM 

LSTM memiliki prinsip kerja yang sama dengan RNN, tetapi 
LSTM mengganti hidden unit pada RNN dengan memory block 
yang  terdiri  dari  4  komponen  yaitu  input  gate,  output  gate, 
forget gate, dan memory cells [22]. 
• 
•  Sedangkan  output  gate  akan  menentukan  hidden  state 

Input gate melakukan update pada memory cell.  

berikutnya. 

•  Forget  gate  menentukan  informasi  mana  yang  harus 

disimpan atau dibuang.  

•  Memory cells akan mengingat bobot selama interval waktu 
yang  berubah-ubah  dan  tiga  gate  lainnya  mengatur  aliran 
input dan output dari memory cell.  

E.  Parameter Neural Network 

Di sini peneliti akan menggunakan beberapa kombinasi dari 
hyperparameter dan ukuran arsitektur Neural Network, antara 
lain  batch  size,  epoch,  optimizer,  activation  function,  dan 
dropout rate. Selengkapnya ditunjukan pada tabel II berikut ini. 
TABEL II 
PARAMETER NEURAL NETWORK 

Parameter 

Batch Size 

Epoch 

Optimizer 

Nilai 

16, 32, 64 

10, 30, 50 

Adam, RMSprop, SGD 

Activation Function  SoftMax, SoftPlus, SoftSign 

Dropout Rate 

0.2, 0.5, 0.8 

Berikut adalah pengertian dari masing-masing parameter. 
•  Batch Size adalah pembagian data menjadi beberapa bagian 
sehingga akan memudahkan beban pemrosesan memori 
•  Epoch  adalah  jumlah  iterasi  bagi  seluruh  dataset  dalam 
proses pelatihan pada Neural Netwok sampai ke awal lagi 
•  Optimizer  merupakan  algoritma  yang  digunakan  untuk 
mengubah  atribut  jaringan  saraf  Anda  seperti  bobot  dan 
learning rate untuk mengurangi loss pada model 

𝑃𝑛 =

∑ 𝐶 ∈ 𝑐𝑜𝑟𝑝𝑢𝑠 𝑛 − 𝑔𝑟𝑎𝑚  ∈ 𝐶  ∑ 𝑐𝑜𝑢𝑛𝑡𝑐𝑙𝑖𝑝(𝑛 − 𝑔𝑟𝑎𝑚)
∑ 𝐶 ∈ 𝑐𝑜𝑟𝑝𝑢𝑠 𝑛 − 𝑔𝑟𝑎𝑚  ∈ 𝐶  ∑ 𝑐𝑜𝑢𝑛𝑡(𝑛 − 𝑔𝑟𝑎𝑚)

      (2) 

𝐵𝐿𝐸𝑈 = 𝐵𝑃𝐵𝐿𝐸𝑈 ∙ 𝑒∑

𝑁
𝑛−1

𝑤𝑛 log 𝑝𝑛

                                               (3) 

Keterangan: 

𝐵𝑃  = brevity penalty 
𝑐  
= jumlah kata dari hasil terjemahan otomatis  
= jumlah kata rujukan  
𝑟  
𝑃𝑛  = modified precission score  
𝑤𝑛   = 1/N (standar nilai N untuk BLEU adalah 4)  
𝑝𝑛   =  jumlah  n-gram  hasil  terjemahan  yang  sesuai  dengan 
rujukan dibagi jumlah n-gram hasil terjemahan 

G.  Consufion Matriks  

Penentuan  baik  atau  tidaknya  performa  suatu  model 
klasifikasi  dapat  dilihat  dari  parameter  pengukuran 
performanya,  yaitu  tingkat  akurasi,  recall,  dan  presisi.  Untuk 
menghitung  faktor-faktor  tersebut  diperlukan  sebuah  matrik 
yang  biasa  disebut  confusion  matriks.  Salah  satu  Confusion-
matrix  (Tabel  III)  yang  kerap  digunakan  dalam  pengukuran 
dapat dilihat pada Gambar 3.17 (Fawcett, 2006). 

TABEL III 
CONFUSSION MATRIX 

Kejadian Sebenarnya 

Positive  
(P) 

Negative  
(N) 

Hipotesis 
Kejadian 

Positive  
(P) 

True Positive 
(TP) 

False Positive 
(FP) 

Negative  
(N) 

False Negative 
(FN) 

True Negaative 
(TN) 

 4 / 8 

 
 
 
 
 
 
 
 
 
 
 
Berdasarkan gambar di atas, terdapat beberapa nilai di dalam 
matriks  yaitu  True  Positive  (TP),  true  negative  (TN),  False 
Positive (FP), dan False negative (FN), seluruh kemungkinan 
kejadian  sebenarnya  Positive  (P)  dan  seluruh  kemungkinan 
kejadian  sebenarnya  Negative  (N).  Nilai  tersebut  dapat 
digunakan  untuk  menghitung  akurasi  dengan  persamaan  (4). 
Akurasi  digunakan  sebagai  parameter  sebagaimana  akurat 
suatu model melakukan klasifikasi [13]. 

𝐴𝑘𝑢𝑟𝑎𝑠𝑖 =

𝑇𝑃 + 𝑇𝑁
𝑃 + 𝑁

                                                                   (4) 

H.  Diagram Alur Metode Penelitian 

Dalam  melakukan  penelitian  ini,  peneliti  berpedoman  dari 

diagram alur penelitian seperti pada Gambar 5 di bawah. 

Peneliti  memulai  dengan  mengumpulkan  dataset  berupa 
corpus parallel. Peneliti mengambil dua buah dataset, pertama 
Bahasa  Spanyol-Inggris  untuk  pelatihan  model  dan  dataset 
kedua  yakni  Bahasa  Jerman-Inggris,  untuk  menguji  model 
terbaik yang dihasilkan.  

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Dataset  pertama  kemudian  akan  dilakukan  preprocessing 
dan  dilakukan  pemodelan  menggunakan  metode  Neural 
Machine Translation, sehingga akan dihasilkan model NMT. 

Kemudian  untuk  mendapatkan  parameter  terbaik,  model 
NMT  tadi  akan  dilakukan  simulasi  pada  berbagai  parameter 
seperti  batch,  epoch,  optimizer,  activation  function,  dan 
dropout. Masing-masing model yang didapatkan nantinya akan 
dilakukan evaluasi menggunakan metode BLEU.  

Model  dengan  BLEU  terbaik  selanjutnya  akan  digunakan 
untuk  melakukan  pemodelan  menggunakan  dataset  yang 
kedua.  Dari  dataset  kedua,  akan  diamati  skor  BLEU-nya 
kembali. Sehingga didapatkanlah performa dari kedua model di 
kedua dataset. 

V.  KERANGKA PIKIR 
Berdasarkan Gambar 6 di bawah, kerangka pikir penelitian 
ini  didasarkan  karena  peneliti  melihat  belum  adanya  ukuran 
pasti dari nilai parameter Neural Network dalam menghasilkan 
model  yang  optimal.  Maka  dari  itu,  peneliti  akan  melakukan 
simulasi  hyperparameter  serta  ukuran  arsitektur  model  pada 
metode Neural Machine Translation. 

Gambar 5. Diagram Alur Metode Penelitian 

Gambar 6. Kerangka Pikir 

VI.  HASIL DAN PEMBAHASAN 

A.  Pengembangan Metode NMT 

Pada  proses  pengembangan  model  NMT  ini,  peneliti 

menggunakan beberapa hyperparameter antara lain:  

•  Sebanyak 20.000 sampel data, hal ini dikarenakan dengan 

mempertimbangkan performa mesin yang digunakan 

 5 / 8 

 
 
 
 
 
•  Split  dataset  yang  digunakan  yakni  6:4,  dengan 
mempertimbangkan  jumlah  data,  peneliti  berharap  tidak 
terjadi ketimpangan antara data train dan test. 

•  Kemudian  Epoch  yang  digunakan  adalah  sebanyak  50, 
dengan mempertimbangkan lamanya waktu pemodelan. 
•  Batch  size  yang  dipilih  peneliti  ada  16,  dengan 

mempertimbangkan kapasitas RAM yang tersedia. 

•  Penggunaan ‘SoftMax’ sebagai Activation Function sendiri 

karena keandalannya dalam melakukan klasifikasi. 

•  Kemudian pada layer Dropout, peneliti memutuskan untuk 

menggunakan 0.5 sebagai masukannya. 

•  Terakhir,  peneliti  penggunakan  Optimizer  berupa  ‘Adam’ 

karena keandalannya dalam mengoptimalkan bobot. 
Berikut adalah pembahasan dari hasil pengembangan model 

Neural Machine Translation yang akan terbagi menjadi: 

1.  Model Dasar 

Gambar 7. Model Dasar 

Pada model arsitektur dasar (Gambar 7), terdapat beberapa 

lapis layers, yaitu: 

•  Embedding,  merupakan  metode  yang  digunakan  untuk 
merepresentasikan variabel diskrit sebagai vektor kontinu 
•  LSTM,  untuk  mengatasi  ketergantungan  jangka  panjang 

(long term dependencies) pada masukannya 

•  Repeat Vector, jembatan antara encoder dan decoder 
•  Time  Distributed,  membuat  vektor  dengan  panjang  yang 
sama  dengan  jumlah  fitur  yang  dihasilkan  dari  lapisan 
sebelumnya 
Hasil  dari  model  dasar  tersebut  adalah  sebagai  berikut. 
Terlihat  pada  Gambar  8,  loss  pada  model  mencapai  titik 
terendah  pada  epoch  ke-11.  Namun  kemudian  setelahnya 
secara berangsur-angsur, loss model makin bertambah. Hal ini 
mengindikasikan adanya overfit pada model dasar ini. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 8. Tren Akurasi dan Loss pada Model Dasar 

Selain itu, skor BLEU yang dihasilkan pada data train adalah 
sebesar 0,658279 dan pada data test sebesar 0,198291. Jauhnya 
perbedaan  skor  pada  data  train  menunjukkan  bahwa  model 
dasar ini mengalami overfit. Overfit sendiri adalah keadaan saat 
model tidak mampu melakukan generalisasi kepada data test. 

2.  Penambahan Callbacks 

Untuk itu, kita perlu mengimplementasikan callbacks dalam 
model  dasar  yang  sebelumnya.  Callback  adalah  seperangkat 
fungsi  yang  akan  diterapkan  pada  tahapan  tertentu  dari 
prosedur  pelatihan.  Peneliti  menggunakan  salah  satu  fungsi 
callbacks yaitu EarlyStopping. 

Overfitting  adalah  mimpi  buruk  bagi  praktisi  Machine 
Learning. Salah satu cara untuk menghindari overfitting adalah 
dengan menghentikan proses lebih awal. Fungsi EarlyStopping 
memiliki  berbagai  metrik/argumen  yang  dapat  Anda 
modifikasi  untuk  disiapkan  saat  proses  pelatihan  harus 
dihentikan. Berikut adalah beberapa metrik yang relevan: 

•  monitor: nilai yang dipantau, yaitu: val_loss 
•  min_delta: perubahan minimum dalam nilai yang dipantau. 
Di  sini  peneliti  menggunakan  min_delta=0  yang  berarti 
bahwa  proses  pelatihan  akan  dihentikan  jika  perubahan 
absolut dari nilai yang dipantau kurang dari 0 

•  patience:  jumlah  epoch  tanpa  peningkatan  setelah  itu 
pelatihan  akan  dihentikan.  Di  sini  peneliti  menggunakan 
petience 3, sehingga jika dalam 3 epoch masih lebih besar, 
maka proses pelatihan akan dihentikan  

•  restore_best_weights:  setel  metrik  ke  True  jika  ingin 

mempertahankan bobot terbaik setelah dihentikan 

Gambar 9. Tren Akurasi dan Loss pada Model dengan Callbacks 

Pada  Gambar  9,  loss  pada  model  mencapai  titik  terendah 
pada epoch ke-14, dengan train accuracy sebesar 0,888117 dan 
test  accuracy  sebesar  0,677500.  Selain  itu,  skor  BLEU  yang 
dihasilkan  pada data  train  adalah  sebesar  0,519729  dan pada 
data test sebesar 0,162981. Jauhnya perbedaan skor pada data 
train  menunjukkan  bahwa  model  dengan  penambahan 
callbacks ini masih mengalami overfit. 

 6 / 8 

 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

3.  Penambahan Layer Dropout 

4.  Pengimplementasian Cross Validation 

Untuk menangani adanya  overfit, selanjutnya peneliti akan 
menambahkan layer Dropout pada layer sequential. Sehingga 
dihasilkan arsitektur model baru seperti Gambar 8. 

Cross Validation yaitu sebuah metode tambahan dari teknik 
yang  bertujuan  untuk  memperoleh  hasil  akurasi  yang 
maksimal.  Pada  Gambar  12  di  bawah,  loss  pada  model 
mencapai  titik  terendah  pada  epoch  ke-22.  Dibandingkan 
dengan  model  sebelumnya,  jarak  akurasi  dari  data  train  dan 
data test cenderung lebih kecil lagi, yakni sebesar 0,8292 dan 
0,7195. Ini adalah indikasi dari turunnya overfit. 

Gambar 12. Tren Akurasi dan Loss pada Model dengan Cross Validation 
Selain itu, skor BLEU yang dihasilkan pada data train adalah 
sebesar  0,429079  dan  pada  data  test  sebesar  0,431115. 
Perbedaan  skor  pada  data  train  dan  data  test  yang  kecil 
menunjukkan bahwa model dengan cross validation ini sudah 
tidak masih mengalami overfit. 
5.  Perbandingan Model Dasar degan Model Pengembagan 

jauh 

Dibandingkan  dengan  model  dasar  (Gambar  8),  model 
pengembangan 
lebih  mampu  mengatasi  masalah 
overfitting ditunjukkan dengan grafik (Gambar 12) yang tidak 
melebar  antara  train  dan  test,  kemudian  juga  ditunjukkan 
dengan perbedaan jarak akurasi antara data train dan test-nya. 
B.  Simulasi Hyperparameter Neural Network 

Berikut  adalah  pembahasan  dari  hasil  simulasi  berbagai 
hyperparameter Neural Network diantaranya batch size, epoch, 
optimizer, activation function, dan dropout rate. 

Kali ini, peneliti akan melakukan simulasi kepada berbagai 
hyperparameter  tersebut  menggunakan  bantuan  fungsi  Grid 
Search  CV  yang  secara  otomatis  akan  melakukan  pelatihan 
model dari berbagai kombinasi nilai hyperparameter yang ada. 
Grid Search adalah metode yang efektif untuk menyesuaikan 
parameter  dalam  supervised  learning  dan  meningkatkan 
performa generalisasi model. Hasilnya bisa dilihat pada Tabel 
IV di bawah dengan parameter terbaik diberi blok warna hijau. 
TABEL IV 
PARAMETER NEURAL NETWORK 

Value 

Mean Accuracy 

Std Accuracy 

Gambar 10. Model Dasar dengan Dropout 

Layer  Dropout  sendiri  merupakan 

teknik  regularisasi 
jaringan  syaraf  dimana  beberapa  neuron  akan  dipilih  secara 
acak  dan  tidak  dipakai  selama  pelatihan.  Neuron-neuron  ini 
dapat dibilang dibuang secara acak. Dropout  dapat mencegah 
terjadinya overfitting dan juga mempercepat proses learning. 

Gambar 11. Tren Akurasi dan Loss pada Model dengan Dropout 

Pada  Gambar 11, loss pada model mencapai titik terendah 
pada  epoch  ke-25.  Dibandingkan  dengan  model  sebelumnya, 
jarak akurasi dari data train dan data test cenderung lebih kecil, 
yakni sebesar 0,821433 dan 0,694925. Tentu ini adalah indikasi 
yang bagus dari berkurangnya  overfit.  Selain itu, skor BLEU 
yang dihasilkan pada data train adalah 0,517304 dan pada data 
test  0,187630.  Jauhnya  perbedaan  skor  menunjukkan  bahwa 
model masih mengalami overfit. 

h
c
t
a
B

h
c
o
p
E

16 

32 

64 

10 

30 

50 

0,612167 

0,034584 

0,631333 

0,020233 

0,633083 

0,009790 

0,641833 

0,000717 

0,549167 

0,052383 

0,459250 

0,034202 

 7 / 8 

 
 
 
 
 
 
 
 
 
r
e
z
i

m

i
t
p
O

n
o
i
t
a
v
i
t
c
A

t

u
o
p
o
r
D

Adam 

0,464500 

0,137227 

RMSProp 

0,478667 

0,112564 

SGD 

0,701500 

0,005672 

SoftMax 

0.594250 

0.072630 

SoftPlus 

0.533250 

0.085778 

SoftSign 

0.119083 

0.134133 

0,2 

0,5 

0,8 

0.643917 

0.004789 

0.629000 

0.027116 

0.642583 

0.005606 

C.  Pemodelan Menggunakan Data Lain 

Untuk menguji model dan parameter yang sudah didapatkan 
sebelumnya,  peneliti  akan  mencoba  menggunakan  data  lain, 
yakni  dataset  corpus  parallel  Bahasa  Spanyol-Inggris  untuk 
mengujinya. Hasilnya ditunjukkan pada Tabel V. 

TABEL V 
PEMODELAN DENGAN DATA LAINNYA 

Data 

Train BLEU 

Test BLEU 

Jerman-Inggris 

Spanyol-Inggris 

0,454968 

0,453146 

0,458266 

0,457280 

Jika  dibandingkan,  maka  skor  BLEU  pada  masing-masing 
bahasa cenderung mirip, yakni di sekitar 45%. Hal ini berarti, 
dengan  perbedaan  bahasa,  model  dan  parameter  yang 
digunakan  terbukti  mampu  untuk  diimplementasikan  pada 
dataset yang berbeda sekalipun. 

A.  Kesimpulan 

VII.  PENUTUP 

Model  yang  telah  dikembangkan  dengan  penambahan 
callbacks,  layer  dropout,  dan  metode  cross  validation  dapat 
mengatasi  overfit  pada  model  dasar.  Sehingga  mendapatkan 
akurasi sebesar 72,24% dan skor BLEU sebesar 45,83% pada 
data test. Kemudian hasil dari simulasi hyperparameter antara 
lain mendapatkan ukuran dari masing-masing hyperparameter 
sebagai berikut, batch size (64), epoch (10), optimizer (SGD), 
activation  function  (SoftMax),  dan  dropout  rate  (0,2).  Yang 
terakhir, implementasi model dan hyperparameter terbaik pada 
dataset Bahasa Spanyol-Inggris menghasilkan skor BLEU yang 
sangat  mirip  dengan  dataset  Bahasa  Jerman-Inggris,  yakni 
0,453146 pada data train dan 0,457280 data test. 

B.  Saran 

Peneliti sadar kalau penelitian ini jauh dari kata sempurna, 
oleh  karena  itu,  di  sini  peneliti  akan  memberikan  beberapa 
saran antara lain menambahkan jumlah sampel, menggunakan 
lebih  banyak  kombinasi  nilai  pada  hyperparameter,  dan 
menambahkan  layer  lain, misalnya  Flatten  dan  Dense,  untuk 
meningkatkan performa model. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

DAFTAR PUSTAKA 
[1]  R. Nugroho Aditya, T. Adji Bharata, and B. Hantono S, “Penerjemahan 
Bahasa Indonesia dan Bahasa Jawa Menggunakan Metode Statistik 
Berbasis Frasa,” Semin. Nas. Teknol. Inf. dan Komun., vol. 2015, no. 
Sentika, 2015. 

[2]  M. B. Yahya, “Penerapan Metode Interpolation Search Untuk 

Melakukan Pencarian Pada Aplikasi Translator Huruf Pegon Jawa – 
Bahasa Indonesia,” Universitas Islam Negeri Maulana Malik Ibrahim, 
2015. 
J. Pranata and Muljono, “Mesin Penerjemah Bahasa Indonesia-Jawa 
Menggunakan Metode Statistik Berbasis Frasa,” no. 5, pp. 1–5, 2016, 
[Online]. Available: http://eprints.dinus.ac.id/18206/. 
T. B. Adji, Y. Astuti, and S. S. Kusumawardani, “Statistical-based 
machine translation for prepositional phrase using Link Grammar,” 
Proc. 2011 Int. Conf. Electr. Eng. Informatics, ICEEI 2011, no. July, 
2011, doi: 10.1109/ICEEI.2011.6021644. 
S. Hunston, Corpora in Applied Linguistics. Cambridge University 
Press, 2002. 

[3] 

[4] 

[5] 

[6]  D. Bahdanau, K. H. Cho, and Y. Bengio, “Neural machine translation 
by jointly learning to align and translate,” 3rd Int. Conf. Learn. 
Represent. ICLR 2015 - Conf. Track Proc., pp. 1–15, 2015. 
[7]  X. Liu, K. Duh, L. Liu, and J. Gao, “Very Deep Transformers for 
Neural Machine Translation,” 2020, [Online]. Available: 
http://arxiv.org/abs/2008.07772. 

[8]  R. Sennrich, B. Haddow, and A. Birch, “Improving neural machine 
translation models with monolingual data,” 54th Annu. Meet. Assoc. 
Comput. Linguist. ACL 2016 - Long Pap., vol. 1, pp. 86–96, 2016, doi: 
10.18653/v1/p16-1009. 
S. R. Asriningtias, H. S. Dachlan, and E. Yudaningtyas, “Optimasi 
Training Neural Network Menggunakan Hybrid Adaptive Mutation,” 
Eeecis, vol. 9, no. 1, pp. 79–84, 2015. 

[9] 

[10]  Kusrini, E. T, and A. Luthfi, Algritma Data Mining. Penerbit Andi, 

2009. 

[11]  A. Shukla, R. Tiwari, and R. Kala, Real Life Apllications of Soft 

Computing. CRC Press, 2010. 

[12]  D. Larose, Discovering Knowledge in Data. Canada: Wiley 

Interscience, 2005. 

[13]  T. Nurhikmat, “Implementasi Deep Learning untuk Image 

Classification Menggunakan Algoritma Convolutional Neural Network 
(CNN) pada Citra Wayang Golek,” Univ. Islam Indones., 2018. 
[14]  Z. Che, T. Chiang, and Z. Che, “Feed-Forward Neural Networks 
Training : A Comparison Between Genetic Algorithm And Back-
Propagation Learning Algorithm,” vol. 7, no. 10, p. 5839, 2011. 
[15]  Z. Abidin, “Penerapan Neural Machine Translation untuk Eksperimen 

[16] 

Penerjemahan secara Otomatis pada Bahasa Lampung – Indonesia,” 
Pros. Semin. Nas. Metod. Kuantitatif 2017, no. 978, pp. 53–68, 2017. 
I. Hadi, “Uji Akurasi Mesin Penerjemah Statistik (MPS) Bahasa 
Indonesia Ke Bahasa Melayu Sambas Dan Mesin Penerjemah Statistik 
(MPS) Bahasa Melayu Sambas Ke Bahasa Indonesia,” J. Sist. dan 
Teknol. Inf., vol. 2, pp. 1–6, 2014. 

[17]  P. Liu, X. Qiu, and H. Xuanjing, “Recurrent neural network for text 

classification with multi-task learning,” IJCAI Int. Jt. Conf. Artif. 
Intell., vol. 2016-Janua, pp. 2873–2879, 2016. 

[18]  W. Xia, W. Zhu, B. Liao, M. Chen, L. Cai, and L. Huang, “Novel 
architecture for long short-term memory used in question 
classification,” Neurocomputing, vol. 299, pp. 20–31, 2018, doi: 
10.1016/j.neucom.2018.03.020. 

[19]  M. R. Firmansyah, R. Ilyas, and F. Kasyidi, “Klasifikasi Kalimat 

Ilmiah Menggunakan Recurrent Neural Network,” Pros. 11th Ind. Res. 
Work. Natl. Semin., vol. 11, no. 1, pp. 488–495, 2020. 

[20]  K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “BLEU: a Method for 

Automatic Evaluation of Machine Translation,” 2002, doi: 
10.1002/andp.19223712302. 

[21]  T. A. Le, M. Y. Arkhipov, and M. S. Burtsev, “Application of a hybrid 

Bi-LSTM-CRF Model to the task of Russian named entity 
recognition,” Commun. Comput. Inf. Sci., vol. 789, pp. 91–103, 2018, 
doi: 10.1007/978-3-319-71746-3_8. 

[22]  G. Lample, M. Ballesteros, S. Subramanian, K. Kawakami, and C. 

Dyer, “Neural architectures for named entity recognition,” 2016 Conf. 
North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol. 
NAACL HLT 2016 - Proc. Conf., pp. 260–270, 2016, doi: 
10.18653/v1/n16-1030. 

 8 / 8 

 
 
 
 
 
 
 
"
221709869,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Implementasi Topic Modelling dalam 
Pengelompokan Topik Dokumen 
Knowledge Management Badan Pusat Statistik RI 

Muhammad Yunus Hendrawan (221709869, 4SD1) 
Dosen Pembimbing: Nucke Widowati Kusumo Projo, S.Si., M.Sc., Ph.D. 

Pengelolaan 

Ringkasan— 

pengetahuan 

(knowledge 
management) merupakan aktivitas penting dalam meningkatkan 
kualitas suatu organisasi. Sebagai salah satu upaya meningkatkan 
kualitas  dan  efisiensi  proses  bisnis  kerja,  Badan  Pusat  Statistik 
(BPS)  telah  turut  mengimplementasikannya  melalui  knowledge 
management  system.  Untuk  mempermudah  pengguna  sistem 
tersebut mengakses dokumen dan pengetahuan, perlu dilakukan 
pengelompokan  terhadap  dokumen  berdasarkan  topik  yang 
implementasi  metode 
dimuat.  Penelitian 
pemodelan  topik  LSA  dan  LDA  untuk  menentukan  teknik 
pengelompokan  yang  baik.  Dari  hasil  implementasi,  diketahui 
bahwa  model  LDA  dengan  menggunakan  Mallet  menghasilkan 
model  terbaik  yaitu  25  kelompok  topik  dengan  coherence  score 
sebesar  0.4803.  Model  LDA  terbaik  tersebut  kemudian  berhasil 
diimplementasikan  dalam  bentuk  RESTful  web  service  untuk 
membantu  memberikan  pelayanan  dalam  fungsi  preprocessing 
dan  rekomendasi  topik  pada  dokumen  yang  masuk  ke  dalam 
Knowledge Management System BPS.  

ini  menerapkan 

Kata Kunci— KMS, Topic Modelling, LSA, LDA, Mallet 

I.  LATAR BELAKANG 

Pengetahuan (knowledge) merupakan suatu hal yang sangat 
penting  dalam  upaya  mempercepat  perkembangan  dan 
peningkatan  kualitas  dari  suatu  organisasi.  Pengetahuan  dan 
informasi dari organisasi perlu didokumentasikan dan dikelola 
melalui  penerapan  pengelolaan  pengetahuan  (knowledge 
management).  Melalui  penerapan  knowledge  management 
yang  baik,  suatu  organisasi  dapat  berbagi  pengetahuan 
(knowledge sharing) yang baik antar individu dalam organisasi, 
sehingga  individu  terkait  dapat  mengambil  kebijakan  dan 
mengatasi permasalahan dengan tepat. 

BPS  sebagai  organisasi  pemerintah  yang  bertugas  untuk 
menyediakan data statistik, dalam perkembangan proses bisnis 
yang  dilakukannya  telah  turut  serta  mengimplementasikan 
knowledge  management.  Pengembangan  dan  implementasi 
knowledge  management  merupakan  salah  satu  kegiatan 
pembangunan pilar bidang pemberdayaan kelembagaan dalam 
program Statistical Capacity Building- Change and Reform for 
the Development of Statistic (STATCAP-CERDAS) BPS yang 
bertujuan untuk meningkatkan efektivitas dan efisiensi proses 
bisnis  kerja  BPS.  Upaya  BPS  dalam  menerapkan  knowledge 
management  dalam  proses  bisnisnya  dapat  dilihat  dengan 
dikembangkannya 
pengetahuan 
(Knowledge  Management  System/KMS)  sebagai  program 
perubahan  reformasi  birokrasi  BPS  di  area  tata  laksana 
kelembagaan [1] sehingga pengetahuan dan informasi penting 

pengelolaan 

sistem 

berkaitan  dengan  pelaksanaan  kegiatan  di  BPS  dapat  diakses 
oleh setiap pegawai ataupun pihak yang membutuhkan. 

Dengan semakin bertambahnya pengetahuan dan informasi 
yang  dikelola  oleh  Knowledge  Management  System  BPS, 
proses penelusuran pengetahuan dibutuhkan untuk membantu 
pegawai  dalam  mengakses  pengetahuan  dan  informasi  yang 
dibutuhkan.  Salah  satu  cara  untuk  melakukannya  adalah 
melakukan  penelusuran  berdasarkan  kategori  atau  kelompok 
dokumen yang tersedia. Selama ini dokumentasi pengetahuan 
dan 
telah  dikelompokkan 
berdasarkan  jenisnya,  namun  pengelompokan  tersebut  masih 
bersifat  hard  clustering,  yaitu  dokumen  hanya  dapat 
dikelompokkan ke dalam satu jenis kelompok tertentu. Selain 
itu,  pengelompokan  pengetahuan  dan  informasi  pada  KMS 
BPS masih dilakukan secara manual oleh pengguna dan masih 
memiliki  keterbatasan  kategori  dalam  pengelompokan 
dokumen. 

informasi  dalam  KMS  BPS 

Berdasarkan  permasalahan  dan  peluang  untuk  melakukan 
efisiensi  tersebut,  penelitian  ini  bersama  Fungsi  Integrasi 
Sistem  Informasi  Statistik  BPS  sebagai  subject  matter 
menawarkan  solusi  untuk  melakukan  pengelompokan  topik 
dokumen  pengetahuan  dan  informasi  BPS  melalui  salah  satu 
implementasi teknik text mining, yaitu pemodelan topik (topic 
modelling).  Topic  modelling  dapat  digunakan  untuk 
mengidentifikasi  kata-kata  dari  berbagai  dokumen  dan 
menghubungkan dokumen dengan pola yang sama berdasarkan 
distribusi  setiap  kata  dari  dokumen  [2].  Pemanfaatan  topic 
modelling  yang  termasuk  fuzzy  clustering  dapat  digunakan 
untuk  mengetahui  kelompok  atau  topik-topik  terkait  pada 
akan 
Knowledge  Management 
System  BPS 
mempermudah  pihak-pihak 
terkait  dalam  menggunakan 
pengetahuan tersebut. 

yang 

Terdapat  beberapa  algoritma metode  topic  modelling  yang 
populer  digunakan  dalam  bidang  analisis  teks,  diantaranya 
adalah Latent Semantic Analysis (LSA), Non-Negative Matrix 
Factorization, Probabilistic Latent Semantic Analysis (PLSA) 
dan  Latent Dirichlet Allocation (LDA) [3]. Namun, di dalam 
penelitian  ini  akan  difokuskan  untuk  mengimplementasikan 
dan membandingkan metode Latent Semantic Analysis (LSA) 
dan Latent Dirichlet Allocation (LDA). Kedua metode tersebut 
dipilih  karena  paling  sering  digunakan  dalam  beberapa 
penelitian 
telah  merepresentasikan  dua 
kelompok  metode  pemodelan  topik  yaitu  kelompok  non-
probabilistik  untuk  LSA  dan  kelompok  probabilistik  untuk 
LDA.  Hasil 
tersebut  akan 
implementasi  kedua  metode 
dievaluasi,  sehingga  dapat  diketahui  metode  yang  lebih  baik 

terdahulu  serta 

 1 / 8 

 
 
 
 
 
untuk  pengelompokan 
topik  dokumen  pada  Knowledge 
Management  System  BPS.  Metode  terbaik  yang  terpilih 
kemudian  akan  diimplementasikan  dalam  bentuk  fungsi  web 
service untuk memberikan kemudahan akses dan implementasi 
bagi BPS. 

II.  TUJUAN PENELITIAN 

Berdasarkan latar belakang yang telah diuraikan sebelumnya, 

maka penelitian ini bertujuan untuk : 

1.  Mengimplementasikan  topic  modelling  pada  dokumen 
yang  terekam  dalam  Knowledge  Management  System 
BPS  untuk  mengetahui  kelompok  topik  dokumen 
melalui metode LSA dan LDA. 

2.  Mengetahui  metode  pemodelan  yang  lebih  baik  antara 
LSA dan LDA dalam mengelompokkan topik dokumen 
yang  terekam  dalam  Knowledge  Management  System 
BPS. 

3.  Memberikan  rekomendasi  kepada  BPS  terkait  metode 
pemodelan  yang  baik  untuk  diimplementasikan  pada 
pengelompokan  topik  dokumen  di  dalam  knowledge 
management system. 

4.  Membangun 

fungsi  dalam  web 

terkait 
pemodelan  topik  untuk  penggunaan  di  BPS  yang 
mencakup proses preprocessing dan rekomendasi topik.  

service 

III. PENELITIAN TERKAIT 

Dalam  mengerjakan  penelitian  sebagai  tugas  akhir  ini, 
terdapat  beberapa  penelitian  sebelumnya  yang  dijadikan 
sebagai referensi dan acuan oleh peneliti. Pulukadang et al. [4] 
dalam  penelitiannya  menyimpulkan  bahwa  pendekatan 
clustering dapat digunakan untuk ekstraksi pengetahuan pada 
pembangunan  knowledge management system sehingga  dapat 
membantu  proses  penelusuran  pengetahuan.  Penelitian 
terdahulu yang berkaitan dengan implementasi topic modelling 
dapat  dilihat  pada  penelitian  Arianto  dan  Anuraga  [2],  yang 
melakukan  pemodelan  topik  pengguna  twitter  mengenai 
aplikasi  “Ruangguru”  dan  diperoleh  hasil  bahwa  metode 
pengklasteran  dengan  LDA  mampu  mengelompokkan  data 
twitter  yang  digunakan  menjadi  28  buah  topik  dengan  topik 
diskon 
sering 
diperbincangkan. 

topik  yang  paling 

ruangguru  menjadi 

laporan 

Terdapat pula penelitian oleh Setijohatmo et al. [5] yang di 
dalam  penelitiannya  melakukan  analisis  metode  LDA  untuk 
klasifikasi  dokumen 
tugas  akhir  berdasarkan 
pemodelan  topik.  Dari  hasil  penelitian  tersebut,  diketahui 
bahwa metode LDA mampu untuk mengelompokkan dokumen 
dengan topik tertentu namun tidak berlabel, meskipun metode 
tersebut  sensitif  terhadap  komposisi  kata.  Melalui  penelitian 
yang  berkaitan  dengan  perbandingan  teknik  pemodelan  LDA 
dan  LSA  dalam  implementasinya  pada  sistem  rekomendasi 
film yang dilakukan oleh Bergamaschi et al. [6], diperoleh hasil 
bahwa  teknik  pemodelan  LSA  lebih  unggul  dan  sesuai 
dibandingkan  LDA  dalam  memberikan  pilihan  rekomendasi 
film yang akan ditonton pengguna.  

Penelitian  terkait  juga  dilakukan  oleh  Mohammed  dan  Al-
Augby  [7]  yang  melakukan  studi  perbandingan  antara  teknik 
pemodelan  LDA  dan  LSA  pada  buku  elektronik  (e-book). 
Berbeda dengan penelitian Bergamaschi et al., pada penelitian 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Mohammed  dan  Al-Augby  diketahui  bahwa  jika  dilihat 
berdasarkan  coherence  score,  model  LDA 
lebih  baik 
dibandingkan model LSA dalam mengelompokkan topik-topik 
yang ada  pada  buku elektronik. Williams dan Betak  [8] turut 
melakukan studi perbandingan metode LSA dan LDA terhadap 
teks  terkait  kecelakaan  di  jalan  raya,  dan  didapatkan  hasil 
bahwa  penggunaan  kedua metode  tersebut  saling  melengkapi 
karena  masing-masing  metode  menghasilkan  beberapa  topik 
yang tidak diidentifikasi oleh metode lain. 

Penelitian  terhadap  pengembangan  model  LDA,  yaitu 
implementasi  LDA  dengan  menggunakan  modul  MAchine 
Learning for LanguagE Toolkit (Mallet) juga menjadi referensi 
dalam  penelitian  ini.  Zhou  et  al.  [9]  melalui  penelitiannya 
mendapatkan hasil bahwa pemodelan topik menggunakan LDA 
dengan Mallet memberikan nilai evaluasi coherence score yang 
lebih  tinggi  dibandingkan  metode  LDA  dengan  package 
Gensim standar dalam melakukan pengelompokan topik pada 
dokumen  berupa  artikel  terkait  “multi-tier  supply  chain  in 
Industry  4.0”.  Implementasi  pemodelan  topik  menggunakan 
LDA dengan Mallet juga dapat dilihat pada penelitian Athira et 
al.  [10]  yang  berhasil  mengelompokkan  10  topik  terkait  tren 
pekerjaan  pada  sektor 
informasi  berdasarkan 
teknologi 
informasi yang tersedia pada “LinkedIn”. 

Untuk  mempermudah  penelusuran  terhadap  rujukan  dan 
acuan pada penelitian ini, maka peneliti memetakan penelitian-
penelitian  terkait  ke  dalam  sebuah  peta  literatur  seperti  yang 
ditampilkan pada Gambar 1 berikut. 

Gambar 1. Peta literatur (literature map) 

IV. METODE PENELITIAN  

A.  Ruang Lingkup Penelitian 

Ruang 

lingkup  penelitian 

ini  berfokus  pada  proses 
implementasi  pemodelan  topik  dengan  menggunakan  metode 
Latent  Semantic  Analysis  (LSA)  dan  Latent  Dirichlet 
Allocation  (LDA).  Implementasi  pemodelan  topik  dengan 
metode  LSA  pada  penelitian  ini  akan  menggunakan  package 
Gensim,  sedangkan  implementasi  dengan  metode  LDA  akan 
menggunakan  dua  package  model,  yaitu  Gensim  dan  Mallet. 
Implementasi  pemodelan-pemodelan  tersebut  akan  dilakukan 
menggunakan  bahasa  pemrograman  python.  Hasil  dari 
pemodelan-pemodelan  topik  yang  dihasilkan  akan  dievaluasi 

 2 / 8 

 
 
 
dalam  rangka  menentukan  metode  yang  paling  baik  untuk 
diterapkan  dalam  pengelompokan  dokumen  knowledge 
management  BPS  berdasarkan  topik.  Metode  yang  terpilih 
kemudian  akan  dilakukan  pengembangan produk  berupa  web 
service  untuk 
fungsi 
rekomendasi  topik  dalam  rangka  memberikan  kemudahan 
akses dan implementasi bagi BPS.  

fungsi  preprocessing  data  dan 

B.  Sumber dan Metode Pengumpulan Data 

Data  yang  digunakan  pada  penelitian  ini  merupakan  data 
sekunder  terbatas  berupa  seluruh  dokumen  berbasis  teks 
Bahasa  Indonesia  sebagai  knowledge  dan  informasi  yang 
tersedia  pada  Knowledge  Management  System  BPS  hingga 
periode  1  Februari  2021.  Data-data  tersebut  didapat  melalui 
kombinasi  permintaan  data  kepada  pihak  Fungsi  Integrasi 
Sistem  Informasi  Statistik  BPS  sebagai  pengelola  knowledge 
management  system  di  BPS  RI  dan  penggunaan  metode  web 
scraping untuk mengekstrak data  teks dari laman  Knowledge 
turut 
Management  System  BPS.  Metode  web  scraping 
digunakan  dalam  pengumpulan  data  ini  dikarenakan  adanya 
keterbatasan  dari  data  yang  diberikan  secara  langsung  oleh 
pihak Fungsi Integrasi Sistem Informasi Statistik BPS. 

Gambar 2. Tampilan struktur data teks knowledge management 

Keterbatasan yang dimaksud dapat dilihat pada Gambar 2, 
yaitu  adanya  struktur  HTML  yang  berbeda  pada  kolom 
description  yang  berisikan  konten  utama  dari  Knowledge 
Management  System  BPS.  Perbedaan  struktur  HTML  ini 
menyebabkan konten sulit diekstrak langsung jika diambil dari 
kolom  description 
itu,  penulis 
menggunakan metode  web scraping  dengan bantuan  package 
Beautiful Soup pada bahasa pemrograman Python. 

tersebut.  Oleh  karena 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Alur pengumpulan data menggunakan metode web scraping 
yang dilakukan dalam penelitian ini dapat dilihat pada Gambar 
3.  Setelah  semua  daftar  URL  dokumen  dikumpulkan,  daftar 
tersebut dibuka untuk melakukan ekstraksi terhadap judul dan 
konten dokumen knowledge management. Seperti terlihat pada 
Gambar  4,  judul  dan  konten  telah  berhasil  diekstrak  tanpa 
mengikutsertakan kode HTML seperti Gambar 2. Banyaknya 
dokumen  yang  dikumpulkan  dan  digunakan  dalam penelitian 
ini adalah sebanyak 207 dokumen. 

Gambar 4. Struktur data teks knowledge management hasil web scraping 

C.  Preprocessing Data 

Tahapan  praproses  atau  preprocessing  data  merupakan 
tahapan  awal  dalam  text  mining  yang  bertujuan  untuk 
mengubah  teks  dari  bahasa  manusia  ke  dalam  format  yang 
dapat  dikelola  oleh  mesin  serta  menyusun  teks  yang  tidak 
terstruktur  dan  mempertahankan  kata  kunci  yang  berguna 
untuk  merepresentasikan  topik  [11].  Tahapan  preprocessing 
data penelitian ini seperti yang terlihat pada Gambar 5 meliputi 
teks  menjadi  huruf  non 
text  normalization  (mengubah 
kapital/case  folding, menghilangkan  karakter-karakter  khusus 
dan tanda baca, menghilangkan white space), menghapus kata-
kata  yang  sering  muncul  (stop  words),  mengubah  kata-kata 
menjadi  kata  dasar  (stemming),  dan  melakukan  tokenisasi 
transformation 
(tokenization).  Selanjutnya 
diperlukan  untuk  mengubah  data  teks  ke  format  yang  sesuai 
dengan  membentuk  model  bigram  dan 
trigrams  untuk 
mengelompokkan  kata  yang  sering  muncul  bersamaan 
kemudian membentuk korpus dan kamus data melalui metode 
Bag of Words (BOW). 

tahapan 

text 

Gambar 3. Flowchart pengumpulan data dengan web scraping 

Gambar 5. Diagram alir tahapan preprocessing data 

 3 / 8 

 
 
 
 
 
 
D.  Pemodelan Topik 

Pemodelan  topik  (topic  modelling)  merupakan  salah  satu 
teknik dalam text mining yang dilengkapi dengan sekelompok 
algoritma  yang  mengungkap,  menemukan,  dan  menjelaskan 
struktur  tematik  dalam  kumpulan  dokumen  [11].  Dalam 
konsepnya,  pemodelan  topik  memiliki  beberapa  entitas  yaitu 
“kata”, “dokumen (w)” yang disusun oleh berbagai kata, dan 
“korpus  (D)”  yang  merupakan  kumpulan  dari  dokumen. 
Pemodelan  topik  menerapkan  konsep  bahwa  setiap  dokumen 
merupakan kumpulan dari suatu set berbagai macam topik, dan 
setiap  topik  tersebut  adalah  distribusi  peluang  dari  berbagai 
macam kata.  

Setelah rangkaian proses pemodelan topik dilakukan, model 
topik  yang  dihasilkan  perlu  dievaluasi  untuk  melihat  tingkat 
efektivitas  dalam  pengelompokan  topik  melalui  perhitungan 
coherence score. Coherence score adalah ukuran yang umum 
digunakan untuk mengevaluasi model topik dengan mengukur 
derajat  skor  kemiripan  semantik  kata-kata  dalam  suatu  topik 
dan nilai coherence score yang tinggi akan merepresentasikan 
model topik yang baik [7]. Dalam menghitung coherence score, 
terdapat  beberapa  algoritma  umum  dan  populer  digunakan. 
Namun, penelitian ini akan menggunakan algoritma CV karena 
berdasarkan  penelitian  Röder,  Both,  dan  Hinneburg  [12], 
diketahui bahwa  CV  memberikan  korelasi  terkuat  antara nilai 
evaluasi koherensi dan hasil interpretasi manusia terkait dengan 
hasil yang dihasilkan. 

Latent Semantic Analysis (LSA) 

Latent  Semantic  Analysis  (LSA)  merupakan  suatu  metode 
statistik  aljabar  berdasarkan  penerapan  Single  Value 
Decomposition  (SVD)  yang  menyajikan  ruang  semantik 
dokumen  [3].  Dengan  menerapkan  metode  tersebut,  LSA 
mampu  melakukan  ekstraksi  terhadap  struktur  dan  hubungan 
kata  yang  tersembunyi  pada  dokumen  melalui  representasi 
vektor  teks  untuk  menghitung  kesamaan  antara  teks  dan 
menemukan kata-kata yang mirip dalam teks. LSA merupakan 
salah  satu  metode  pemodelan  topik  non-probabilistik,  yang 
dapat dilakukan melalui algoritma berikut [3] : 

1.  Untuk korpus teks dibuat term dengan matriks dokumen 
A di mana M adalah term atau kata-kata dalam dokumen 
dan N adalah banyaknya dokumen dalam korpus. 

2.  Setiap entri diberi bobot dengan fungsi yang mengaitkan 
pentingnya  setiap  kata  dalam  dokumen  dan  di  seluruh 
korpus. 

3.  Menerapkan  SVD  ke  matriks  A,  yang  menunjukkan 
korelasi antar istilah dalam dokumen. SVD menguraikan 
matriks  A  menjadi  tiga  matriks  sehingga  𝐴 = 𝑈∑𝑉𝑇 
dimana U adalah matriks ortogonal (𝑈𝑇𝑈) dan V adalah 
matriks ortogonal (𝑉𝑇𝑉) dan ∑ adalah matriks diagonal 
(∑= diagonal (𝜎1, 𝜎2, … , 𝜎𝑘). 

4.  Matriks A dipotong sebagai Ak, sumbu terpotong adalah 
aproksimasi  k  terbaik  atau  terdekat  dengan  matriks  A 
yang asli. 

Implementasi  pemodelan  topik  LSA  dalam  penelitian  ini 
akan menggunakan modul atau package Gensim yang tersedia 
dalam bahasa pemrograman python. Gensim merupakan salah 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

satu  package  pada  bahasa  python  yang  berfokus  pada 
penggunaan  konsep  korpus,  vektor,  model,  dan  matriks 
sehingga dapat melakukan implementasi sederhana untuk tf-idf, 
LSA dan LDA [13]. 

Latent Dirichlet Allocation (LDA) 

Latent  Dirichlet  Allocation  (LDA)  merupakan  metode 
pemodelan 
topik  model  probabilistic  generative  yang 
dirancang untuk mengekstrak topik dari teks [11]. Konsep dari 
metode  LDA  didasari  bahwa  dokumen  direpresentasikan 
sebagai campuran acak dari topik yang tersembunyi, di mana 
setiap topik dicirikan oleh satu set kumpulan probabilitas kata 
yang merepresentasikan kata termasuk dalam suatu topik. LDA 
mengasumsikan proses generatif berikut untuk setiap dokumen 
w dalam korpus D [11] : 

1.  Pilih 𝑁  ∼ Poisson ( ξ ). 
2.  Pilih 𝜃  ∼ Dir(α). 
3.  Untuk setiap 𝑁 kata 𝑤𝑛 : 

a.  Pilih topik 𝑧𝑛∼ Multinomial(θ). 
b.  Pilih  kata  𝑤𝑛  dari  𝑝(𝑤𝑛|𝑧𝑛, 𝛽) ,  probabilitas 
multinomial yang dikondisikan pada topik 𝑧𝑛. 
Implementasi pemodelan topik dengan metode LDA dapat 
dilakukan  melalui  pendekatan  variational  bayes  atau  pun 
pendekatan  gibbs  sampling  [14].  Di  dalam  penelitian  ini, 
metode  LDA  akan  dilakukan  menggunakan  dua  pendekatan 
tersebut melalui penggunaan modul Gensim untuk pendekatan 
variational bayes, sedangkan modul Mallet untuk pendekatan 
gibbs sampling. Meskipun Mallet dan Gensim merupakan dua 
modul yang berbeda, implementasi Mallet pada penelitian ini 
akan 
yaitu 
fungsi 
gensim.models.wrappers.LdaMallet  yang  disediakan  oleh 
package Gensim. 

menggunakan 

penghubung 

MAchine  Learning 

for  LanguagE  Toolkit 

(Mallet) 
merupakan  modul  atau  paket  berbasis  Java  yang  dapat 
digunakan  untuk  melakukan  pemrosesan  bahasa,  klasifikasi 
dokumen,  pengelompokan,  pemodelan 
topik,  ekstraksi 
informasi, dan aplikasi  machine  learning lain  pada teks  [15]. 
Modul  Mallet  yang  menerapkan  teknik  gibbs  sampling  dapat 
digunakan  untuk  mengestimasi  model  LDA  dari  korpus  data 
latih  dan  distribusi  topik  dari  dokumen  baru.  Penggunaan 
modul  Mallet  dikenal  dapat  memberikan  implementasi  LDA 
yang  efisien  serta  dapat  memberikan  pemisahan  topik  yang 
lebih baik [16]. 

E.  Web Service 

sebuah 

jaringan 
service, 

Web service merupakan suatu sistem perangkat lunak yang 
dirancang  untuk  mendukung  kemampuan  interaksi  mesin  ke 
[17].  Di 
mesin  melalui 
dalam 
terdapat  beberapa 
mengimplementasikan  web 
pendekatan  arsitektur  teknologi yang dapat digunakan seperti 
Simple Object Access Protocol (SOAP) dan REpresentational 
State Transfer (REST). Pada penelitian ini, implementasi web 
service  akan  dibangun  menggunakan  arsitektur  REST  karena 
berdasarkan penelitian Putra dan Putera [18] diketahui bahwa 
implementasi  arsitektur  REST  pada  web  service  berbasis 
microframework  Flask  dapat  menghasilkan  performa  yang 

 4 / 8 

 
 
lebih bagus dibandingkan dengan SOAP dari sisi request dan 
respons untuk web services. 

digabungkan  menjadi  satu  bagian.  Tahapan  pertama  dalam 
proses preprocessing data meliputi text cleaning yang contoh 
hasilnya dapat dilihat pada Tabel I berikut : 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

RESTful Web Service 

RESTful  Web  Service  atau  dapat  dikenal  dengan  RESTful 
API  merupakan  sebuah  web  service  yang  diimplementasikan 
menggunakan  Hypertext  Transfer  Protocol  (HTTP)  melalui 
penerapan  prinsip-prinsip  REST  [18].  Melalui  penerapan 
arsitektur REST, setiap service atau layanan yang dibuat dapat 
diidentifikasi  melalui  Uniform  Resource  Locator  (URL). 
Arsitektur  REST  melibatkan  proses  membaca  laman  web 
tertentu  dan  dapat  memberikan  respons  dalam  bentuk  fail 
berformat XML ataupun JSON. 

V.  KERANGKA PIKIR 

Kerangka  pikir  dalam  penelitian  ini  seperti  dapat  terlihat 
pada  Gambar  6  terdiri  dari  enam  bagian.  Penelitian  diawali 
dengan identifikasi masalah yang dilakukan dengan observasi 
serta  wawancara  dengan  subject  matter  penelitian.  Setelah 
masalah  teridentifikasi,  kemudian  dilakukan  studi  literatur 
untuk  memahami  konsep,  metode,  dan  teknologi  terkait 
solusi  untuk 
sebelum  akhirnya  didapatkan 
memecahkan  permasalahan  penelitian.  Solusi  yang  telah 
dirancang  kemudian  diimplementasikan  dalam  rangkaian 
penelitian  sebelum  akhirnya  didapatkan  kesimpulan  terkait 
penelitian yang dilakukan serta saran terhadap pengembangan 
penelitian selanjutnya. 

rancangan 

TABEL I 
CONTOH DATA SEBELUM DAN SESUDAH TEXT CLEANING 

Sebelum Text Cleaning 

Sesudah Text Cleaning 

Judul: AKSES COOLSIS.BPS.GO.ID 
[CAPI] 
Konten: 
mengakses 
coolsis.bps.go.id jaringan internet harus 
terhubung melalui VPN BPS. 

Untuk 

Judul: akses coolsis bps go id cap 
Konten:  akses  coolsis  bps  go  id 
jaring internet hubung vpn bps 

Setelah  data  melalui  tahapan  text  cleaning,  kemudian 
dilakukan konfigurasi data dengan membuat model bigram dan 
trigrams serta membuat korpus data menggunakan metode Bag 
of  Words.  Data  yang  telah  selesai  dilakukan  preprocessing 
kemudian dapat digunakan untuk melakukan pemodelan topik 
baik  menggunakan  metode  LSA  maupun  LDA.  Struktur  data 
korpus  yang  siap  olah  dalam  pemodelan  dapat  dilihat  pada 
karakter cetak tebal di dalam Tabel II. 

TABEL II 
STRUKTUR KORPUS YANG SIAP DIGUNAKAN 
[(0, 2), (1, 3), (2, 1), (3, 2), (4, 2), (5, 1), (6, 1), (7, 1), (8, 1)] 
[('akses', 2), ('bps', 3), ('cap', 1), ('coolsis', 2), ('go_id', 2), ('hubung', 1), 
('internet', 1), ('jaring', 1), ('vpn', 1)] 

B.  Pemodelan Topik Menggunakan Metode LSA 

Implementasi pemodelan topik menggunakan metode LSA 
dapat  dilakukan  setelah  data  selesai  melewati 
tahapan 
preprocessing.  Dalam  melakukan  implementasi  pemodelan 
topik LSA dengan package Gensim, salah satu parameter yang 
penting  untuk  ditentukan  adalah  banyaknya  topik  (k).  Untuk 
menentukan  parameter  tersebut,  perlu  dilakukan  parameter 
tuning melalui perhitungan coherence score. 

Gambar 6. Kerangka Pikir Penelitian 

VI. HASIL DAN PEMBAHASAN 

Gambar 7. Hasil parameter tuning k-topik LSA 

A.  Hasil Preprocessing 

Setelah  proses  pengumpulan  data  selesai  dilakukan, 
dokumen  yang  terdiri  atas  judul  dan  isi  dokumen  perlu 
dilakukan  preprocessing.  Input  berupa  teks  judul  dan  konten 
akan 
telah  dilakukan  preprocessing  kemudian 
yang 

Setelah  melakukan  parameter  tuning,  seperti  terlihat  pada 
Gambar  7  didapatkan  bahwa  model  dengan  6  topik  yang 
diterapkan dalam LSA menghasilkan coherence score tertinggi 
yaitu 0,4412. Distribusi lima kata teratas yang menyusun enam 

 5 / 8 

 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

tidak  jauh  berbeda  dibandingkan  model  LDA  sebelumnya. 
Namun,  distribusi  dan  pemisahan  topik  yang  dihasilkan  jauh 
lebih baik dengan persebaran lebih beragam meskipun masih 
ada beberapa topik yang beririsan seperti terlihat pada Gambar 
9. Distribusi lima kata teratas dengan frekuensi paling banyak 
dalam menyusun 25 topik dan interpretasi topik tersebut dapat 
dilihat pada Tabel IV. 

topik dan interpretasi mandiri terkait topik tersebut dapat dilihat 
pada  Tabel III.  Dapat diketahui  bahwa  hasil pemodelan  LSA 
ini  masih  memiliki  distribusi  kata  dalam  topik  yang  saling 
berkaitan,  seperti  pada  topik  1  dengan  topik  3  serta  topik  2 
dengan topik 5. 

Topik 
T1 

T2 

T3 

T4 

T5 

TABEL III 
HASIL PEMODELAN TOPIK DENGAN METODE LSA 

5 Kata Teratas 
Terkait dengan pendataan 
keluarga,  isi,  anggota,  duduk. 
tinggal 
keluarga dalam sensus 
task, keluarga, kerja, klik, tombol,  Terkait instruksi untuk 

Bahasan 

sp,  keluarga, 

sensus, 

duduk, 
anggota 
properti,  analisis,  harga, 
commerce 
klik, task, action, isi, duduk 

jual, 

T6 

air, rumah, lantai, tempat, listrik 

mengoperasikan perangkat 
Terkait kegiatan sensus 
penduduk secara umum 
Kegiatan yang berhubungan 
dengan penjualan 
Terkait instruksi untuk 
mengoperasikan perangkat 
Terkait rumah dan 
komponennya 

C.  Pemodelan Topik Menggunakan Metode LDA 

Gambar 9. Sebaran 25 topik hasil implementasi LDA-Mallet 

tahap  awal 

Implementasi pemodelan topik menggunakan metode LDA 
juga  dilakukan  menggunakan  package  Gensim  pada  bahasa 
pemrograman  python.  Pada 
implementasi, 
didapatkan  hasil  parameter  tuning  untuk  parameter  yang 
digunakan  dalam  LDA  Gensim  dengan  alpha=0.9,  beta=0.3, 
dan jumlah topik sebanyak 16 menghasilkan  coherence score 
tertinggi  yaitu  sebesar  0.4825.  Parameter  tersebut  kemudian 
diimplementasikan  dalam  model  LDA,  namun  setelah 
dilakukan visualisasi persebaran topik menggunakan pyLDAvis, 
topik  yang  dihasilkan  cenderung  mengelompok  seperti  yang 
terlihat pada Gambar 8. Untuk mendapatkan persebaran yang 
lebih baik, model LDA ini dikembangkan dengan menerapkan 
model LDA menggunakan Mallet. 

Topik 
T1 

T2 

T3 

T4 

T5 

T6 

T7 

T8 

T9 

T10 

Gambar 8. Sebaran 16 topik hasil implementasi LDA dengan Gensim 

Implementasi  metode  LDA  dengan  model  Mallet  juga 
dilakukan dengan melakukan parameter tuning untuk mencari 
parameter  banyaknya 
topik  yang  optimum  berdasarkan 
coherence  score.  Model  terbaik  yang  didapatkan  adalah 
sebanyak  25  topik  dengan  coherence  score  sebesar  0.4803, 

TABEL IV 
Hasil Pemodelan Topik dengan Metode LDA-Mallet 

tekan,  nama, 

Top 5 Words 
task, 

baik, 

tombol, 
tambah 
isi, 
kirim, 
konfirmasi,  
jumlah, 
bandara, statistik 
keluarga, 
nama, kepala 
online, kk, isi, nik, lengkap 

indonesia, 

anggota, 

perlu, 

besar, 

sebut, 

duduk, sensus, online, sp, 
informasi 
tinggal, alamat, isi, tempat, 
sama 
tanggal, mulai, situs, covid, 
jumlah 
bps, go_id, akses, dashboard, 
online 
klik, grup, kaizala, action, 
admin 

T11  wilayah, satu, tingkat, baik, 

sebut 
analisis, properti, kondisi, 
commerce, bulan 
sp, tugas, pk, koseka, laksana 

sls, kode, periksa, dokumen, 
bila 
cap, ics, update, status, 
aplikasi 
dp, entri, entry, urut, nomor 

T12 

T13 

T14 

T15 

T16 

Bahasan 

Terkait instruksi pengerjaan 
tugas petugas 
Terkait instruksi dan mekanisme 
pengiriman tugas 
Terkait pemanfaatan statistik di 
Indonesia 
Terkait entri anggota keluarga 
dalam sensus penduduk 
Terkait pengisian identitas dalam 
Sensus Penduduk Online 
Terkait dengan Sensus Penduduk 
Online 
Terkait pengisian alamat tinggal 
penduduk 
Terkait kegiatan dalam situasi 
covid 
Terkait aplikasi dan situs online  

Instruksi terkait penggunaan 
aplikasi Kaizala 
Tekait kegiatan yang 
menggunakan tingkat wilayah 
Terkait kegiatan yang 
berhubungan dengan penjualan 
Terkait pelaksanaan tugas 
Koseka dalam sensus penduduk 
Terkait permasalahan 
pemeriksaan dokumen sls 
Terkait penggunaan capi dan ics 

Terkait entri daftar penduduk 

T17  milik, rumah, air, sewa, 

sumber 

Terkait karakteristik tempat 
tinggal penduduk 

 6 / 8 

 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

T18 

T19 

T20 

T21 

T22 

T23 

T24 

T25 

assign, sampel, bahasa, 
mungkin, proses 
klik, browser, tombol, baru, 
muncul 
giat, bantu, tetap, apa, terima 

perlu, beda, rupa, salah, 
aplikasi 
kerja, pilih, beri, sesuai, hasil 

Terkait permasalahan sampel 
dalam kegiatan lapangan 
Terkait  instruksi  menggunakan 
aplikasi berbasis web 
Terkait dengan kegiatan secara 
umum 
Terkait dengan masalah aplikasi 

Terkait hasil pekerjaan 

buka, aplikasi, buat, meeting, 
minta 
indonesia, surat, orang, nikah, 
nomor 
tugas, sebut, login, mitra, 
password 

Terkait instruksi mekanisme 
meeting online 
Terkait isian pernikahan dalam 
kuesioner 
Instruksi terkait login aplikasi 
petugas 

microframework  Flask  dalam  bahasa  pemrograman  python. 
Service  yang  dikembangkan  dalam  penelitian 
ini  yaitu 
mencakup  proses  preprocessing  data  teks  dan  rekomendasi 
topik. 

Service  dalam  proses  preprocessing  dapat  menerima  input 
parameter  dokumen  berupa  judul  dan  konten  serta  dapat 
diakses  melalui  GET  request  dengan  URL.  Output  yang 
dihasilkan dari fungsi service ini adalah korpus data teks yang 
siap  digunakan  untuk  proses  lanjutan  dalam  pengolahan  data 
teks serta status keberhasilan request terhadap fungsi tersebut 
dalam  format  fail  JSON.  Contoh  request  terhadap  fungsi 
preprocessing  parameter  yang  dimasukkan  melalui  URL  dan 
output  yang  dihasilkan  dari  service  preprocessing  ini  dapat 
dilihat pada Gambar 10. 

D.  Evaluasi Metode LSA dan LDA 

Implementasi  pemodelan 

topik  dokumen  KMS  BPS 
menggunakan  metode  LSA  menghasilkan  model  terbaik 
dengan 6 topik dan coherence score sebesar 0.4412. Sedangkan 
implementasi  pemodelan  topik  menggunakan  model  LDA, 
menghasilkan model terbaik dengan model Mallet sebanyak 25 
topik dengan coherence score 0.4803 dan kelompok topik yang 
tersebar dengan lebih baik. Ringkasan terkait hasil pemodelan 
topik  yang  telah  dilakukan  penelitian  ini  dapat  dilihat  pada 
Tabel V. 

TABEL V 
RINGKASAN EVALUASI HASIL PEMODELAN TOPIK 

Variabel 

Banyaknya Topik 

LSA 

6 

LDA-Gensim 

LDA-Mallet 

16 

25 

Coherence Score 

0.4412 

0.4825 

0.4803 

Persebaran Topik 

Tidak Tersedia 
Untuk 
Divisualisasikan 

Cenderung 
Mengelompok 
di 1 Kuadran 

Topik Lebih 
Menyebar 

Jika  dilihat  berdasarkan  nilai  coherence  score  dari  ketiga 
implementasi  metode  pemodelan  topik  tersebut,  pemodelan 
dengan menggunakan metode LDA baik LDA dengan Gensim 
maupun  LDA  dengan  Mallet  menghasilkan  nilai  yang  lebih 
baik  dibandingkan  metode  LSA  dalam  mengelompokkan 
dokumen sejenis berdasarkan topik. Pemodelan menggunakan 
model LDA dengan Mallet juga dapat diinterpretasikan lebih 
baik  melalui  visualisasi  persebaran  topik,  sedangkan  model 
LSA tidak dapat dilakukan visualisasi persebaran topik. Model 
LDA Mallet dengan 25 kelompok topik sebagai model terbaik 
kemudian  digunakan  untuk  menentukan  distribusi  topik  serta 
menentukan  topik  dominan  pada  setiap  dokumen  KMS  BPS 
yang dapat digunakan dasar untuk pengelompokan. 

E.  Pengembangan RESTful Web Service Pemodelan Topik 

dengan  Mallet  menjadi  model 

Setelah evaluasi model dilakukan, diketahui bahwa metode 
dan 
LDA 
direkomendasikan untuk penerapan pada KMS BPS, kemudian 
diimplementasikan dalam bentuk fungsi RESTful Web Service. 
Implementasi  dalam  fungsi  ini  dikembangkan  menggunakan 

terbaik 

Gambar 10. Contoh request dan output dari service text preprocessing 

serta  daftar 

Pada  service  fungsi  rekomendasi  topik  juga  membutuhkan 
input parameter dokumen berupa judul dan konten agar dapat 
dijalankan.  Service  dari  fungsi  ini  akan  memberikan  output 
dalam format JSON berupa status keberhasilan fungsi tersebut 
dijalankan 
topik  yang  direkomendasikan 
berdasarkan tiga probabilitas kelompok topik tertinggi, dengan 
rekomendasi  topik  1  merupakan  topik  dominan  yang  dimuat 
dalam dokumen knowledge management sebagai input. Output 
rekomendasi  topik  ini  dapat  menjadi  dasar  pengelompokan 
dokumen  KMS  BPS.  Contoh  terkait  request  serta  tampilan 
output  yang  dihasilkan  oleh  fungsi  ini  dapat  dilihat  pada 
Gambar 11 berikut. 

Gambar 11. Contoh request dan output dari service rekomendasi topik 

 7 / 8 

 
 
 
 
Implementasi dari fungsi  web service  terkait  preprocessing 
data teks dan rekomendasi topik yang dibangun pada penelitian 
ini  telah  dapat  diakses  secara  daring.  Selain  itu,  untuk 
memberikan kemudahan bagi pengguna dalam mengakses web 
service, penelitian ini telah menyediakan  dokumentasi  terkait 
fungsi preprocessing dan fungsi rekomendasi topik yang dibuat 
tersebut 
menggunakan 
menyediakan  informasi  terkait  deskripsi  fungsi  web  service, 
parameter yang dibutuhkan, hingga tipe data yang digunakan, 
seperti yang ditampilkan pada Gambar 12.  

Swagger  UI.  Dokumentasi 

Gambar 12. Tampilan dokumentasi fungsi web service dengan Swagger 

VII. 

PENUTUP 

Melalui penelitian ini kesimpulan yang dapat diambil adalah 
bahwa  dokumen  pada  Knowledge  Management  System  BPS 
topik. 
dapat  dikelompokkan  melalui  metode  pemodelan 
Berdasarkan evaluasi terhadap penggunaan metode pemodelan, 
model LDA dengan menggunakan Mallet menghasilkan model 
terbaik  dengan  coherence  score  sebesar  0.4803  untuk  25 
kelompok  topik  yang  tersebar  dengan  lebih  baik.  Model  ini 
kemudian  dapat  direkomendasikan  untuk  diimplementasikan 
pada dokumen KMS BPS.  

Model  yang  didapat  dalam  penelitian  ini  juga  telah 
diimplementasikan  dalam  bentuk  fungsi  pada  RESTful  Web 
Service  menggunakan  microframework  Flask  pada  bahasa 
pemrograman  python.  Service  untuk  tahapan  preprocessing 
data  teks  akan  menghasilkan  korpus  data  yang  telah  siap 
digunakan untuk pengolahan data teks selanjutnya, sedangkan 
service  untuk  fungsi  rekomendasi  topik  akan  memberikan 
rekomendasi  kelompok  topik  yang  dimuat  oleh  dokumen 
knowledge  management 
sebagai  dasar  pengelompokan 
dokumen KMS BPS. Dokumentasi  terkait pembangunan  web 
service  tersebut  juga  telah  dimuat  menggunakan  Swagger UI 
untuk memberikan kemudahan implementasi bagi pengguna. 

Adapun saran untuk penelitian selanjutnya yaitu :  

1.  Menggunakan  data  dokumen  pengetahuan  dan 
lebih 
informasi  knowledge  management  yang 
banyak  dan  terkini  untuk  menangkap  informasi 
kelompok topik yang lebih beragam. 

2.  Mengembangkan  metode  atau  fungsi  yang  dapat 
melakukan pemodelan kembali setiap ada dokumen 
baru  yang  dimasukkan  ke  dalam  basis  data 
Knowledge Management System BPS. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

DAFTAR PUSTAKA 
[1]  BPS.  (2018)  8  Area  Perubahan  Reformasi  Birokrasi  BPS.  [Online]. 
Available : https://rb.bps.go.id/CIrbbps/index.php/area/areaperubahan 
[2]  B.  W.  Arianto  and  G.  Anuraga,  “Topic  Modeling  for  Twitter  Users 
Regarding the ‘Ruanggguru’ Application,” Jurnal ILMU DASAR, vol. 21, 
no. 2, p. 149, 2020, doi: 10.19184/jid.v21i2.17112. 

[3]  P. Kherwa and P. Bansal, “Topic Modeling: A Comprehensive Review,” 
EAI Endorsed Transactions on Scalable Information Systems, vol. 7, no. 
24, pp. 1–16, 2019, doi: 10.4108/eai.13-7-2018.159623. 

[4]  D. R. Pulukadang, Mustafid, and Farikhin, “Pendekatan Clustering untuk 
Ekstraksi  Pengetahuan  pada  Pembangunan  Sistem  Manajemen 
Pengetahuan,”  Jurnal  Sistem  Informasi  Bisnis,  vol.  5,  no.  2,  pp.  79–83, 
2015, doi: 10.21456/vol5iss2pp79-83. 

[5]  U. T. Setijohatmo, S. Rachmat, T. Susilawati, Y. Rahman, and K. Kunci, 
“Analisis Metoda Latent Dirichlet Allocation untuk Klasifikasi Dokumen 
Laporan  Tugas  Akhir Berdasarkan  Pemodelan  Topik,”  in  Prosiding  The 
11th Industrial Research Workshop and National Seminar, 2020, pp. 402–
408. 

[6]  S. Bergamaschi, L. Po, and S. Sorrentino, “Comparing topic models for a 
movie recommendation system,” WEBIST 2014 - Proceedings of the 10th 
International Conference on Web Information Systems and Technologies, 
vol. 2, pp. 172–183, 2014, doi: 10.5220/0004835601720183. 

[7]  S.  H.  Mohammed  and  S.  Al-Augby,  “LSA  &  LDA  topic  modeling 
classification:  Comparison  study  on  E-books,”  Indonesian  Journal  of 
Electrical Engineering and Computer Science, vol. 19, no. 1, pp. 353–362, 
2020, doi: 10.11591/ijeecs.v19.i1.pp353-362. 

[8]  T.  Williams  and  J.  Betak,  “A  Comparison  of  LSA  and  LDA  for  the 
Analysis of Railroad Accident Text,” Procedia Computer Science, vol. 130, 
no. January, pp. 98–102, 2018, doi: 10.1016/j.procs.2018.04.017. 

[9]  R. Zhou, A. Awasthi, and J. Stal-Le Cardinal, “The main trends for multi-
tier supply chain in Industry 4.0 based on Natural Language Processing,” 
Computers 
Industry,  vol.  125,  p.  103369,  2021,  doi: 
10.1016/j.compind.2020.103369. 

in 

[10] M. Athira, K. Vhavya, K. Soorya, R. Ajeesh, and V. S. Anoop, “A New 
Way  of  Topic  Modeling  Using  MALLET  for  Current  Job  Trends,” 
in  Computer  and 
International  Journal  of  Advanced  Research 
Communication  Engineering,  vol.  5,  no.  1,  pp.  59–63,  2016,  doi: 
10.17148/IJARCCE. 

[11] D.  M.  Blei,  A.  Y.  Ng,  and  M.  I.  Jordan,  “Latent  Dirichlet  Allocation,” 
Journal of Machine Learning Research, vol. 3, pp. 993–1022, 2003. 
[12] M.  Röder,  A.  Both,  and  A.  Hinneburg,  “Exploring  the  space  of  topic 
coherence  measures,”  in  WSDM  2015  -  Proceedings  of  the  8th  ACM 
International Conference on Web Search and Data Mining, 2015, pp. 399–
408, doi: 10.1145/2684822.2685324. 

[13] I. Akef, J. S. M. Arango, and X. Xu, “Mallet vs GenSim : Topic modeling 
for 20 news groups report .,” 2019. doi: 10.13140/RG.2.2.19179.39205/1. 
[14] R. Rehurek and P. Sojka, “Software Framework for Topic Modelling with 
Large  Corpora,”  Proceedings  of  the  LREC  2010  Workshop  on  New 
Challenges for NLP Frameworks, pp. 45–50, 2010. 

[15] A. K. McCallum. (2002) Mallet: MAchine Learning for Language Toolkit 

[Online]. Available: http://mallet.cs.umass.edu/  

[16] S. Prabhakaran. (2018, 3) Gensim Topic Modeling - A Guide to Building 
Available: 

Best 
https://www.machinelearningplus.com/nlp/topic-modeling-gensim-
python/  

[Online]. 

models 

LDA 

[17] The  World  Wide  Web  Consortium  (W3C).  (2004,  2)  Web  Services 
Architecture [Online]. Available: https://www.w3.org/TR/ws-arch/  
[18] M. G. L. Putra and M. I. A. Putera, “Analisis Perbandingan Metode Soap 
Dan  Rest  Yang  Digunakan  Pada  Framework  Flask  Untuk  Membangun 
Web Service,” SCAN - Jurnal Teknologi Informasi dan Komunikasi, vol. 
14, no. 2, pp. 1–7, 2019, doi: 10.33005/scan.v14i2.1480. 

 8 / 8 

 
 
 
 
"
221709866,"Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik

Pengembangan Sistem Pencatatan Pelanggaran
Online Menggunakan QR Code Berbasis Android
Studi Kasus Politeknik Statistika STIS

Muhammad Sayyid Shabir (221709866, 4SI1)

Dosen Pembimbing: Lutﬁ Rahmatuti Maghﬁroh

Ringkasan—Panjangnya alur proses bisnis membuat alur pen-
catatan pelanggaran di STIS menjadi cukup rumit. Butuh waktu
minimal dua hari hingga data pelanggaran masuk ke dalam
sistem SIPADU. Selain itu, tahapan rekapitulasi dan input data
pelanggaran rentan terjadi human error. Untuk itu dibutuhkan
sistem yang dapat meningkatkan eﬁsiensi dalam pelaksanaan
pencatatan pelanggaran mahasiswa di STIS. Pembangunan sis-
tem pencatatan pelanggaran berbasis Android dan Web dibu-
tuhkan untuk menciptakan eﬁsiensi dari sistem pencatatan terse-
but. Untuk itu dalam penelitian kali ini penulis ingin mengem-
bangkan sebuah sistem pencatatan pelanggaran yang sudah ada
agar lebih eﬁsien dalam melakukan pencatatan pelanggaran
mahasiswa di STIS yang berbasis android dengan memanfaatkan
teknologi scanner dan QR Code yang nantinya terdapat pada
badge mahasiswa STIS dan juga aplikasi berbasis web yang
dapat digunakan oleh pegawai BAAK untuk menambahkan nilai
sanksi pelanggaran mahasiswa. Adapun sistem akan dibangun
dengan menggunakan framework Flutter untuk pembangunan
aplikasi android dan memanfaatkan MySql sebagai manajemen
basis data serta menggunakan smartphone berbasis Android
sebagai alat untuk pencatatan pelanggaran.

Kata Kunci—Aplikasi Android, QR Code, Flutter, Pencatatan

Pelanggaran.

I. LATAR BELAKANG

Politeknik Statistika STIS (yang selanjutnya disebut
STIS) merupakan perguruan tinggi kedinasan yang bernaung
dibawah Badan Pusat Statistik (yang selanjutnya disebut BPS).
STIS dalam menyelenggarakan pendidikan memiliki salah
satu misi, yaitu membentuk insan akademik yang profes-
sional, memiliki integritas dan amanah [1]. Ketertiban dan
Keteraturan merupakan suatu kondisi fundamental yang harus
tercipta di dalam lingkungan STIS [2]. Dalam hal ini, STIS
menerapkan berbagai peraturan yang ditujukan untuk dipatuhi
dan dijalankan oleh setiap mahasiswa STIS. Namun pada
penerapannya masih sering dijumpai pelanggaran-pelanggaran
yang dilakukan oleh mahasiswa terhadap peraturan tersebut,
dan akan diberikan sanksi sesuai dengan aturan yang berlaku.
Pemberian sanksi dilakukan dengan cara memberikan poin
pelanggaran terhadap mahasiswa yang melakukan pelang-
garan.

Saat ini, sistem pencatatan pelanggaran diolah oleh Bagian
Administrasi Akademik dan Kemahasiswaan (yang selanjut-
nya disebut BAAK) yang dibantu hal pencatatan dan pela-
poran oleh dosen dan salah satu organisasi mahasiswa yaitu
Satuan Penegak Disiplin (yang selanjutnya disebut SPD).
ini SPD
Dalam melakukan pencatatan pelanggaran, saat

masih melakukannya dengan cara mencatat pelanggaran ma-
hasiswa melalui paper base dan mengambil badge mahasiswa
yang selanjutnya akan diserahkan ke BAAK untuk diinput
poin pelanggarannya ke dalam database SIPADU mahasiswa
STIS. Berdasarkan wawancara bersama Komandan SPD tahun
kepengurusan 2020-2021 selaku narasumber yang terlibat
dalam pengembangan aplikasi, proses pengembangan sistem
berfokus pada peningkatan eﬁsiensi sistem pencatatan pelang-
garan, karena pada saat ini proses pencatatan dan pelaporan
pelanggaran ini dinilai masih memiliki alur yang panjang. Alur
dari pencatatan dan pelaporan pelanggaran dapat dilihat pada
gambar 1.

Pada tahun 2020 sudah ada penelitian tentang pengemban-
gan sistem pencatatan online berbasis Android [3]. Namun
penulis menilai bahwa aplikasi tersebut masih harus dikem-
bangkan agar bisa digunakan dengan baik oleh SPD dalam
melakukan tugas pencatatan pelanggaran secara online. Sis-
tem pencatatan pelanggaran berbasis Android yang dibangun
sebelumnya belum bisa menjalankan tugas pencatatan pelang-
garan dengan efektif karena hanya bisa mencatat pelanggaran
dalam satu kegiatan saja. Sedangkan SPD memiliki 3 kegiatan
dalam pencatatan pelanggaran, yaitu minggu disiplin, inspeksi
dadakan, dan inspeksi rutin dimana setiap kegiatan mempun-
yai aturan pencatatan pelanggaran yang berbeda-beda. Un-
tuk kegiatan minggu disiplin, mahasiswa diberikan tenggang
waktu selama 3 hari untuk memperbaiki pelanggaran yang
dilakukannya. Jika selama 3 hari tersebut mahasiswa yang
bersangkutan tidak memperbaiki pelanggaran yang dilakukan
dan tidak mengambil badgenya, maka poin pelanggaran yang
dilakukan oleh mahasiswa tersebut akan dilapurkan ke pihak
BAAK untuk dimasukkan ke dalam SIPADU. Untuk inspeksi
rutin dan inspeksi dadakan, setiap pelanggaran yang dida-
pat akan langsung dilaporkan ke BAAK. Hanya saja jika
pelaksanaan inspeksi dadakan tidak diinfokan kapan akan
dilaksanakannya, sedangkan untuk inspeksi rutin diadakan
setiap pelaksanaan apel rutin atau upacara peringatan hari
besar nasional.

Selain itu, berdasarkan hasil wawancara dengan kepala sub-
bagian administrasi kemahasiswaan, bapak Sofyan Ayatullah,
S.ST, saat ini BAAK membutuhkan sebuah sistem yang dapat
menambahkan nilai sanksi pelanggaran yang dilakukan oleh
mahasiswa yang sesuai dengan Peraturan Direktur POLSTAT
STIS No. 3 tahun 2020. Dimana nilai sanksi pelanggaran yang
diberikan bersifat dinamis sesuai dengan jumlah pelanggaran

1 / 8

Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik

didapatkan native java sebesar 0,31 dan ﬂutter sebesar 0,69
dengan nilai Consistency Ratio (CR) sebesar 0,033. Oleh
karena itu dipilihkan ﬂutter sebagai framework untuk pem-
bangunan aplikasi android pada penelitian ini.

II. TUJUAN PENELITIAN

Tujuan dilakukannya penelitian ini adalah:
1) Mengembangkan sistem pencatatan pelanggaran berba-

sis Android dengan framework ﬂutter.

2) Memanfaatkan ﬁtur

pemindai QR Code

dalam

melakukan pencatatan pelanggaran.

3) Membangun sistem input poin pelanggaran berbasis

web.

III. PENELITIAN TERKAIT

Terdapat penelitian terkait yang dilakukan oleh Rian Surya
Pratama (Pratama STIS 2019/2020) [3] yang membangun
sistem pencatatan pelanggaran online yang berbasis Android
dengan memanfaatkan ﬁtur Scanner dan QR Code. Penelitian
ini menggunakan metode penelitian System Development Life
Cycle (SDLC) dengan pendekatan model waterfall. Penelitian
ini menggunakan 3 jenis pengujian, yaitu Black Box Test-
ing, System Usability Scale (SUS), dan kuesioner perceiveid
usefulness Technology Acceptance Test (TAM). Penelitian ini
bertujuan untuk membangun sebuah sistem informasi yang
efektif dan eﬁsien dalam melakukan pencatatan pelanggaran
online dengan memanfaatkan QR Code berbasis Android yang
terintegrasi dengan SIPADU.

Penelitian berikutnya oleh Sanhu Li, Mao Zheng, dan Hao
Fan [6] yang melakukan penelitian tentang aplikasi absensi
untuk siswa dengan memanfaatkan teknologi scanner dan QR
Code. Para siswa nantinya akan memindai QR Code meng-
gunakan perangkat mobile mereka untuk melakukan absensi.
Terdapat dua jenis platform mobile yang dibangun, yaitu IOS
dan Android. Untuk aplikasi berbasis web digunakan untuk
registrasi para siswa dengan memanfaatkan framework Boot-
strap. Pada bagian front-end menggunakan HTML sebagai
halaman statik dan menggunakan jQuery dan Ajax untuk
halaman yang dinamis. Untuk back-end, sistem menggunakan
JSP, framework Spring-mvc, dan MySql sebagai DBMS.

IV. METODE PENELITIAN

Metode yang digunakan untuk pengumpulan kebutuhan
adalah dengan melakukan studi literatur, wawancara dan ob-
servasi. Studi literatur dilakukan guna mencari teori maupun
penelitian terkait yang dapat membantu dalam pembangunan
sistem. Studi literatur dilakukan dengan memanfaatkan be-
brbagai informasi seperti jurnal, internet, peraturan kemaha-
siswaan di STIS dan referensi literatur lainnya. Wawancara
dilakukan kepada subject matter yaitu BAAK selaku admin
dari SIPADU dan SPD. Melalui wawancara tersebut, peneliti
bisa mendapatkan informasi berupa bagaimana sistem yang
digunakan saat ini untuk melakukan pencatatan pelanggaran.
Observasi dilakukan untuk mengamati sistem yang sedang
berjalan saat
ini, sehingga peneliti dapat melihat kendala-
kendala yang bisa terjadi dengan diberlakukannya sistem

2 / 8

Gambar 1. Alur Pencatatan Pelanggaran Berjalan di STIS

yang dilakukan oleh mahasiswa yang bersangkutan. Semakin
sering melakukan pelanggaran, maka sanksi yang diberikan
akan semakin besar. Dari permasalahan tersebut, dibutuhkan
sebuah sistem yang dapat memperbaiki proses pencatatan
pelanggaran menjadi lebih eﬁsien. Aplikasi android dipilih
sebagai aplikasi berbasis mobile yang dapat digunakan untuk
melakukan pencatatan pelanggaran secara online.

Dalam menentukan framework untuk pembangunan aplikasi
android, terdapat dua alternatif yaitu, menggunakan native
java dan melanjutkan pengkodingan dari penelitian selanjutnya
atau menggunakan ﬂutter [4] dan membangun aplikasi dari
awal. Dalam menentukan pilihan dari dua alternatif tersebut,
pada penelitian ini menggunakan metode Analytic Hierarchy
Process (AHP) [5]. Metode AHP merupakan salah satu metode
dalam proses pengambilan keputusan.Metode ini digunakan
untuk mendukung pengambilan keputusan terhadap beber-
apa alternatif pilihan. Proses pengambilan keputusan diawali
dengan menetapkan faktor-faktor/kriteria yang mempengaruhi
pengguna dalam mengambil keputusan. Kriteria yang menjadi
penentu yaitu, dokumentasi, ﬁtur yang disediakan, dan kemu-
dahan mempelajari bahasa pemrograman. Nilai akhir prioritas

saat ini. Penelitian ini menggunakan metode analisis System
Development Life Cycle (SDLC) dengan pendekatan metode
Agile dengan pendekatan Extreme Programing [7]. Metode
tersebut terdiri dari perencanaan dan analisis kebutuhan, desain
aplikasi, implementasi, pengujian dan pemeliharaan.

Pada tahap perencanaan dan analisis kebutuhan, penulis
berfokus kepada pengumpulan informasi tentang sistem yang
telah berjalan baik dengan cara observasi maupun wawancara
subject matter. Kemudian informasi yang didapat dianalisa
guna mengetahui kebutuhan sistem yang akan dibangun. Ke-
butuhan sistem yang didapat dari tahap sebelumnya akan di-
gunakan untuk membuat usulan rancangan sistem. Usulan ran-
cangan tersebut meliputi ﬁtur yang dibutuhkan sistem, tampi-
lan antarmuka sistem, dan arsitektur basis data pada sistem.
Setelanjutnya, usulan rancangan sistem dari tahap sebelum-
nya akan diimplementasikan. Implementasi tersebut meliputi
pemilihan Framework yang akan digunakan, perangkat yang
dibutuhkan, pembuatan ﬁtur-ﬁtur pada sistem, dan pembangu-
nan antarmuka sistem. Selanjutnya sistem yang telah dibangun
pada tahap sebelumnya akan dilakukan pengujian dan pemeli-
haraan guna menjaga sistem yang telah dibangun tetap dapat
digunakan dan mampu menjadi solusi dari permasalahan yang
didapat diawal penelitian. Sistem akan diuji menggunakan
black box testing [8] dan system usability scale (SUS) [9].
Black box testing digunakan untuk menguji apakah ﬁtur-ﬁtur
pada aplikasi sudah dapat berjalan dengan baik. Sedangkan
SUS digunakan untuk mengukur persepsi user tentang kegu-
naan dari sistem tersebut.Sehingga user bisa menilai apakah
aplikasi yang telah dibangun sudah layak untuk digunakan dan
dapat menjadi solusi bagi permasalahan yang ada.

Gambar 2. Tahapan Metode Penelitian Agile

V. KERANGKA PIKIR

Alur pencatatan pencatatan pelanggatan di STIS dapat
dikatakan belum eﬁsien karena alurnya yang panjang dan
waktu pengerjaannya yang membutuhkan waktu minimal 2
hari. Proses pencatatannya pun masih menggunakan paper-
base. Tentu hal ini membuat pelaksanaan pencatatan pelang-
garan membutuhkan anggaran disetiap pelaksanaannya. Dari
latar belakang permasalahan tersebut, peneliti mendapatkan
beberapa analisis kebutuhan sistem yang dapat memper-
baiki sistem pencatatan pelanggaran yang digunakan saat ini.
Kerangka pikir pada penelitian ini dapat dilihat pada gambar
3

Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik

Gambar 3. Kerangka Pikir

3 / 8

VI. HASIL DAN PEMBAHASAN

1) Analisis Permasalahan

Untuk mendapatkan pokok-pokok permasalahan yang
lebih spesiﬁk, dilakukan analisis permasalahan meng-
gunakan metode PIECES [10]. Berikut analisis PIECES
pada tabel I

TABEL I
HASIL ANALISIS SISTEM BERJALAN DENGAN MENGGUNAKAN METODE
PIECES

Aspek Penilaian
Performance

Hasil

a) Rekapitulasi hasil pencatatan pelanggaran
yang dilakukan oleh SPD membutuhkan waktu
secepat-cepatnya 1 hari sebelum diserahkan ke
pihak BAAK.

b) Pada sistem yang berjalan saat ini, penam-
bahan data pelanggaran di BAAK dilakukan
secara manual satu persatu data tersebut di-
masukkan.

Information

Economics

Control

Efﬁciency

Service

b)

a) Output

: hasil dari pencatatan pelanggaran
dapat dilihat di SIPADU masing-masing maha-
siswa, sehingga tidak memerlukan perbaikan.
Input : cara penginputan catatan pelanggaran
yang masih dilakukan dengan cara mema-
sukkan satu per satu data, membuat hal ini
rentan akan kesalahan penginputan oleh ad-
min. Terlebih jika data yang dimasukkan
dalam jumlah yang banyak.

a) Dalam satu kali pelaksanaan operasi rutin,
minimal dibutuhkan buku pencatatan pelang-
garan seharga Rp. 18.000.

b) Proses penginputan data pencatatan pelang-
garan ke dalam database SIPADU cukup
memakan waktu dan tenaga.

a) Rentan terjadi kesalahan dalam penginputan
data pelanggaran yang disebabkan oleh human
error

b) Akses kedalam database SIPADU sangat ter-

batas dan sangat baik

a) Proses mulai pencatatan hingga proses peng-
inputan data pelanggaran ke SIPADU membu-
tuhkan waktu paling singkat 2 hari, sehingga
membuat sistem yang sedang berjalan saat
kurang eﬁsien.

b) Diperlukan perbaikan dalam proses pen-
catatan, agar pencatatan pelanggaran dapat
lebih eﬁsien.

a) Penginputan data pelanggaran membutuhkan
waktu minimal 2 hari dan bisa lebih lama
jika jumlah pelanggaran yang terjadi cukup
banyak.

b) Rentan terjadi kesalahan input data

2) Analisis Kebutuhan

a) Sistem yang dibangun dapat membuat alur pen-

Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik

b) Sistem dapat terintegrasi dengan SIPADU
c) User dapat memasukkan data pelanggaran dengan
cara men-scan QR Code atau dengan cara mema-
sukkan manual

d) Sistem dapat menampilkan data pelanggaran dalam

satu kali periode kegiatan

e) Sistem dapat digunakan oleh pegawai BAAK guna
memasukkan data nilai sanksi pelanggaran yang
dilakukan oleh mahasiswa

3) Rancangan Sistem Usulan

Sistem informasi yang diusulkan merupakan sebuah sis-
tem informasi berbasis android dan web. Dimana proses
penginputan pelanggaran dapat dilakukan di tempat oleh
SPD maupun Dosen menggunakan aplikasis yang berba-
sis android. Setiap pelanggaran kedisiplinan yang di-
lakukan mahasiswa akan diinputkan pada aplikasi yang
berintegrasi dengan web yang bertugas memasukkan
jumlah poin pelanggaran. Proses yang terjadi pada
saat pencatatan pelanggaran, SPD atau Dosen selaku
user memeriksa mahasiswa yang terindikasi melakukan
pelanggaran, apabila terjadi pelanggaran maka user
memasukan nim, nama, kelas, nama pelanggaran, kate-
gori pelanggaran dan tempat dicatatkannya pelanggaran
pada aplikasi. Ketika selesai mengisi mahasiswa da-
pat langsung pergi dan jika besok masih melakukan
pelanggaran maka akan terkena poin pelanggaran yang
sama. Selanjutnya data pelanggaran dikirim ke database
pada server. Data yang telah masuk didatabase server
selanjutnya akan dirubah jumlah poin pelanggarannya
oleh admin BAAK melalui aplikasi web. Alur rancangan
sistem usulan dapat dilihat pada gambar 4. Untuk bisnis
proses sistem usulan dapat dilihat pada gambar 5, dan
6.
Use case diagram untuk sistem ini dapat dilihat pada
gambar 7, 8, 9, dan 10. Terdapat empat kategori peng-
guna. Untuk aplikasi android ditujukan kepada pengguna
yang masuk dalam kategori pegawai STIS dan anggota
SPD. Sedangkan untuk pengguna aplikasi web ditujukan
untuk admin atau pegawai BAAK dan admin SPD.
User harus melakukan login terlebih dahulu agar dapat
menggunakan aplikasi, baik itu aplikasi mobile android
atau aplikasi web. Penjelasan lebih lanjut dijelaskan pada
skenario use case.

4) Implementasi Sistem Usulan

Arsitektur sistem yang diterapkan terdiri dari user dan
server. Untuk user sendiri terdiri dari user aplikasi an-
droid dan user aplikasi web. Aplikasi android ditujukan
untuk mencatat pelanggaran mahasiswa STIS. Sedan-
gkan, aplikasi web digunakan untuk menambahkan data
poin pelanggaran kedalam data pencatatan pelanggaran.
Rancangan implementasi sistem terdiri dari halaman
login, halaman utama, halaman input pelanggaran yang
terdiri dari dua kegiatan, yaitu kegiatan inspeksi rutin
dan kegiatan minggu disiplin, dan halaman monitoring.

catatan pelanggaran di STIS lebih eﬁsien

a) Hasil rancangan aplikasi android.

4 / 8

Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik

Gambar 4. Alur Rancangan Sistem Usulan

Untuk ﬁtur login, penelitian ini memanfaatkan au-
tentiﬁkasi yang disediakan oleh google ﬁrebase.
Untuk akun usernya nanti harus didaftarkan ter-
lebih dahulu melalui google ﬁrebase agar dapat
mengakses aplikasi tersebut. Untuk halaman uta-
manya terdiri dari ﬁtur untuk menuju ke halaman
pencatatan pelanggaran (inspeksi rutin dan minggu
disiplin) dan halaman monitoring. Tampilan dari
halaman tersebut dapat dilihat pada gambar 11 dan
12.
Pada halaman input pelanggaran, baik untuk
kegiatan inspeksi rutin atau minggu disiplin, ter-
dapat ﬁtur scan QR Code. Fitur scan QR Code ini
nantinya akan mengambil data mahasiswa berupa
Nomor Induk Mahasiswa (NIM) yang ada pada
badge masing-masing mahasiswa. Untuk input
pencatatan pelanggaran, user diharuskan mengisi
form yang terdiri dari NIM, nama, kelas, nama
pelanggaran, kategori pelanggaran dan tempat di-
lakukannya pencatatan pelanggaran tersebut.Data
baru dapat terkirim jika kolom pada form tersebut
sudah terisi semua, sehingga tidak ada data kosong

Gambar 5. Bisnis Proses Sistem Usulan

yang terkirim. Tampilan halaman input pelang-
garan dapat dilihat pada gambar 13.
Halaman monitoring pada aplikasi android bertu-
juan untuk melihat data pelanggaran yang sudah
dicatat pada kegiatan minggu disiplin. Dengan
dapat dilihatnya data yang telah tercatat, maka
kemungkinan resiko terjadinya duplikat untuk pen-
catatan pelanggaran yang sama dapat dihindari.
Tampilan pada halaman monitoring dapat dilihat
pada gambar 14.

b) Hasil rancangan aplikasi web

Untuk ﬁtur login aplikasi web, user dibedakan
menjadi dua kategori yaitu, user pegawai BAAK
dan user admin SPD. Hal ini dibedakan karena
masing masing kategori memiliki ﬁtur pada ap-
likasi web yang berbeda.
Untuk user pegawai BAAK,
terdapat 2 ﬁtur
utama, yaitu ﬁtur input nilai sanksi pelanggaran
yang diberika kepada mahasiswa yang tercatat
melakukan pelanggaran dan ﬁtur untuk melihat
data pelanggaran mahasiswa yang telah tercatat.
Sedangkan untuk user admin SPD terdapat satu
ﬁtur utama, yaitu ﬁtur data pelanggaran minggu
disiplin. Fitur ini berfungsi untuk menghapus atau
mengirim data pelanggaran yang tercatat dalam

5 / 8

Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik

Gambar 10. Diagram Use Case Pengguna Admin SPD

Gambar 6. Bisnis Proses Sistem Usulan

Gambar 11. Halaman Login Untuk Pengguna Pegawai dan Anggota SPD

Gambar 7. Diagram Use Case Pengguna Pegawai STIS

Gambar 8. Diagram Use Case Pengguna SPD

Gambar 9. Diagram Use Case Pengguna Admin BAAK

kegiatan minggu disiplin.
a) Pengujian SUS dan BlackBox

Evaluasi terhadap sistem usulan dibagi menjadi 2,
yaitu BlackBox-Testing dan System Usability Scale
(SUS) .
Black box testing digunakan untuk menguji apakah
ﬁtur-ﬁtur pada aplikasi sudah dapat berjalan den-
gan baik. Berikut adalah ﬁtur-ﬁtur yang terdapat
pada aplikasi android untuk pengguna pegawai
STIS dan anggota SPD, dan sudah dapat berfungsi
dengan baik:
i) Fitur login
ii) Fitur Scan qr code
iii) Fitur cari data mahasiswa
iv) Fitur suggestion untuk pengisian kolom nama

pelanggaran dan kategori pelanggaran

v) Fitur Kirim data
vi) Validasi form input pelanggaran
vii) Halaman monitoring pelanggaran
viii) Fitur logout

Dari pengujian yang telah dilakukan, maka didapat
kesimpulan bahwa aplikasi dapat berjalan sesuai

6 / 8

Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik

Gambar 12. Halaman Beranda Untuk Pengguna Anggota SPD

Gambar 14. Halaman Monitoring

Gambar 15. Halaman Beranda

Gambar 13. Halaman Input Pelanggaran

Gambar 16. Halaman Web Pegawai BAAK

7 / 8

Gambar 17. Halaman Web Admin SPD

harapan, dimana ﬁtur maupun fungsi dari setiap
menu maupun objek yang ada berfungsi dengan
baik dan sesuai dengan tujuan perancangan.
Untuk pengujian pada aplikasi web, terdapat be-
berapa ﬁtur yaitu:
i) Fitur login
ii) Fitur mesin pencari data pelanggaran
iii) Fitur edit data pelanggaran (input poin pelang-

garan)

iv) Fitur kirim data pelanggaran
v) Fitur riwayat pelanggaran mahasiswa
Dari pengujian yang telah dilakukan, dapat dis-
impulkan untuk aplikasi web dengan pengguna
admin BAAK, bahwa aplikasi dapat berjalan sesuai
harapan, dimana ﬁtur maupun fungsi dari setiap
menu maupun objek yang ada berfungsi dengan
baik dan sesuai dengan tujuan perancangan
Pengujian selanjutnya adalah pengujian menggu-
nakan metode SUS. Metode penilaian SUS meng-
haruskan pengguna untuk memberikan tanggapan
terhadap 10 pertanyaan [11] dan penilaiannya
menggunakan skala 1 sampai 5 dimana semakin
besar nilai akhirnya maka semakin baik. Setelah
mendapatkan tanggapan dari pengguna terhadap
pertanyaan tersebut, didapatkan nilai SUS sebesar
83,33 atau masuk kedalam kategori B. Artinya
berdasarkan pengujian tersebut, aplikasi yang telah
dikembangkan dapat dikatakan layak.

VII. PENUTUP

Terjadi beberapa perubahan analsis kebutuhan pada proposal
penelitian dan makalah penelitian. Pada proposal penelitian
analisis kebutuhan berfokus pada pembagunan aplikasi android
yang dapat melakukan pencatatan pelanggaran dan terintegrasi
dengan SIPADU. Namun, pada saat penelitian sedang berjalan,
kampus STIS membuat Peraturan Direktur No. 3 Tahun 2020.
Dimana draf peraturan tersebut mengatur tentang kode etik dan
tata tertib mahasiswa STIS. Sehingga draf peraturan tersebut
juga mempengaruhi dari analisis kebutuhan pada penelitian ini.
Adanya permintaan pembangunan sistem berbasis web ditu-
jukan untuk membuat sebuah sistem yang mempunyai tujuan
utama untuk menambahkan poin pelanggaran ke dalam data
pencatatan pelanggaran yang ada, dimana hak untuk penamba-
han poin pelanggaran ini hanya dimiliki oleh pegawai BAAK
yang ditugaskan.

Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik

Saat ini, pembangunan aplikasi android sudah dilakukan
evaluasi sistem dengan menggunakan BlackBox-Testing dan
SUS.Untuk pengujian BlackBox-Testing aplikasi android un-
tuk pengguna pegawai dan anggota SPD didapat kesimpulan
bahwa aplikasi dapat berjalan sesuai harapan, dimana ﬁtur
maupun fungsi dari setiap menu maupun objek yang ada
berfungsi dengan baik dan sesuai dengan tujuan perancan-
gan. Dari evaluasi yang dilakukan, didapat nilai SUS sebesar
83,125.

Hasil penelitian hingga saat ini berupa pengembangan ap-
likasi android dan aplikasi web. Aplikasi android yang diban-
gun sudah dapat menjalankan fungsinya sebagai sistem untuk
pencatatan pelanggaran dengan memanfaatkan ﬁtur pemindai
QR Code dan mengirimkan data pelanggaran tersebut kedalam
server. Untuk aplikasi web yang dibangun sudah dapat men-
jalankan fungsi utamanya yaitu menambahkan nilai sanksi
pelanggaran.

DAFTAR PUSTAKA

[1] P. STIS. Visi dan misi. [Online]. Available: https://stis.ac.id/Hal/17/Visi-

Dan-Misi

[2] P. S. STIS, Buku Kumpulan Peraturan Kemahasiswaan, Jakarta, 2016.
[3] R. S. Pratama, Pengembangan Sistem Pencatatan Pelanggaran Maha-
siswa Politekhnik Statistika STIS dengan QR Code Berbasis Android,
Jakarta, 2020.

[4] W. Wu, React Native vs Flutter, cross-platform mobile application

frameworks, 2018.

[5] T. L. Saaty, “Decision making with the analytic hierarchy process,”
International journal of services sciences, vol. 1, no. 1, pp. 83–98, 2008.
[6] S. Li, M. Zheng, and H. Fan, “Recording class attendance in a student

registration system.”

[7] M. Paasivaara and C. Lassenius, “Could global software development

beneﬁt from agile methods?”

[8] S. Nidhra and J. Dondeti, “Black box and white box testing techniques-
a literature review,” International Journal of Embedded Systems and
Applications (IJESA), vol. 2, no. 2, pp. 29–50, 2012.

[9] J. Brooke, “Sus: a retrospective,” Journal of usability studies, vol. 8,

no. 2, pp. 29–40, 2013.

[10] N. Puspitasari, R. Lestari, M. Taruk, and E. Maria, “Website testing
analysis using pieces and eucs method,” in 2019 International Confer-
ence on Electrical, Electronics and Information Engineering (ICEEIE),
vol. 6.

IEEE, 2019, pp. 298–302.

[11] J. Sauro and J. R. Lewis, “When designing usability questionnaires,
does it hurt to be positive?” in Proceedings of the SIGCHI conference
on human factors in computing systems, 2011, pp. 2215–2224.

8 / 8

"
221709865,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pengembangan Sistem Informasi Manajemen, 
Administrasi, Monitoring, dan Evaluasi Anggaran 
Politeknik Statistika STIS 
Modul E-Loket Permintaan Dana 

Muhammad Rizqi Aulia Rahman (221709865, 4SD2) 
Dosen Pembimbing: Farid Ridho 

Ringkasan— Kegiatan administrasi keuangan yang dilakukan 
oleh  Politeknik  Statistika  STIS  masih  dilakukan  secara  manual 
yaitu pada pencatatan perubahan Petunjuk Operasional Kegiatan 
(POK)  dan  permintaan  dana  oleh  unit.  Perubahan  POK  yang 
dilakukan  menggunakan  aplikasi  SAKTI  (Kemenkeu)  tidak 
dapat  menampilkan  riwayat  perubahan  sampai  level  terkecil. 
Sistem  yang  telah  dikembangkan  sebelumnya  hanya  dapat 
menampilkan  riwayat  perubahan  terakhir.  Sistem  juga  hanya 
dapat  diakses  oleh  petugas  BAU,  sedangkan  beberapa  fiturnya 
dapat  dimanfaatkan  oleh  unit 
tambahan 
diperlukan.  Permintaan  anggaran  secara  manual  menyebabkan 
prosesnya  tidak  dapat  dipantau  secara  langsung  baik  oleh  unit, 
petugas  maupun  pimpinan.  Untuk  mengatasinya  E-loket  akan 
ditambahkan pada sistem. Selain untuk pemantauan, E-loket juga 
dapat  mengirimkan  notifikasi  surel  kepada  unit  sehingga  unit 
segera  tahu  ketika  permintaannya  ditolak  atau  telah  selesai. 
Pengembangan  sistem  menggunakan  metode  SDLC  waterfall, 
NodeJs, Express, serta basis data MongoDB. Hasil uji 15 skenario 
black-box  menunjukkan hasil yang  baik,  namun  pengujian  SUS 
hanya  mendapatkan  skor  64,8 yang berarti  sistem  masih belum 
cukup baik untuk digunakan dan perlu dilakukan perbaikan. 

sehingga  user 

Kata  Kunci—  sistem  informasi  keuangan,  POK,  permintaan 

dana, E-loket 

I.  LATAR BELAKANG 

Politeknik  Statistika  STIS  merupakan  sebuah  perguruan 
tinggi  kedinasan  di  lingkungan  Badan  Pusat  Statistik  (BPS) 
sebagai  sebuah  Satuan  Kerja (Satker)  untuk  mendidik  tenaga 
pelaksana kegiatan statistik pada tingkat semi ahli yang mampu 
melaksanakan  dan  mengembangkan  per  statistikan  nasional. 
Sebagai  satker,  Politeknik  Statistika  STIS  memiliki  beberapa 
unit  kerja  untuk  melaksanakan  setiap  kegiatannya.  Terdapat 
dua jenis unit kerja yaitu unit struktural yang terdiri dari Bagian 
Administrasi  Akademik  dan  Kemahasiswaan  (BAAK)  dan 
Bagian  Administrasi  Keuangan  (BAU)  serta  unit  fungsional 
yang  terdiri  dari  Program  Studi  (PRODI),  Satuan  Penjamin 
Mutu  (SPM),  Unit  Pusat  Penelitian  dan  Pengabdian  kepada 
Masyarakat (UPPPM), Perpustakaan, dan Teknologi Informasi 
(IT)  ditambah  dengan  Satuan  Pengawas  Internal  (SPI)  yang 
berasal dari luar Politeknik Statistika STIS. 

 Untuk  mendukung  pelaksanaan  kegiatan,  Politeknik 
Statistika  STIS  melakukan  kegiatan  administrasi  keuangan 
yang  diserahkan  kepada  BAU  Sub  Bagian  Keuangan.  Salah 
satu  tanggung  jawabnya  adalah  mengawasi  dan  mencatat 
pelaksanaan Daftar Isian Pelaksanaan Anggaran (DIPA) yang 
dijabarkan  lebih  lanjut  pada  Petunjuk  Operasional  Kegiatan 

(POK).  Menurut  [1],  [2],  DIPA  merupakan  dokumen 
pelaksanaan  anggaran  yang  disusun  oleh  pengguna  anggaran 
(PA)/kuasa pengguna anggaran (KPA) dan berlaku untuk satu 
tahun anggaran sedangkan POK adalah dokumen yang memuat 
uraian  rencana  kerja  dan  biaya  yang  diperlukan  untuk 
pelaksanaan  kegiatan,  disusun  oleh  KPA  sebagai  penjabaran 
lebih lanjut dari DIPA. 

jawab 

Dalam  melaksanakan 

tersebut,  BAU 
tanggung 
menggunakan  sebuah  aplikasi  dari  Kementerian  Keuangan 
(Kemenkeu) yang bernama Sistem Aplikasi Keuangan Tingkat 
Instansi (SAKTI) untuk mencatat pelaksanaan anggaran mulai 
dari  perencanaan  hingga  pertanggungjawaban  seperti  yang 
dijelaskan  pada  [3].  Aplikasi  tersebut  dapat  digunakan  oleh 
instansi  pemerintah  sehingga  tidak  terjadi  duplikasi  dan 
keamanan  data  keuangan  lebih  terjaga.  Aplikasi  SAKTI 
terbatas  pada  kebutuhan  Kemenkeu  di  tingkat  pemerintahan 
sehingga  terdapat  beberapa  kebutuhan  BAU  yang  tidak 
tercakup dalam SAKTI. Pada tahun 2017 dikembangkan sistem 
informasi manajemen keuangan berbasis web bernama Sistem 
Informasi Manajemen, Administrasi, Monitoring, dan Evaluasi 
Anggaran  (SIMAMOV)  yang  dapat  mendukung  pelaksanaan 
anggaran  di 
lingkup  Politeknik  Statistika  STIS  seperti 
pembuatan Surat Pertanggung Jawaban  (SPJ), Surat Perintah 
Perjalanan  Dinas  (SPPD),  dan  khususnya  pengelolaan  POK 
hingga level detil. Namun sekarang pembuatan SPJ dan SPPD 
bukan menjadi tanggung jawab dari BAU melainkan menjadi 
tanggung  jawab  unit  masing-masing.  SIMAMOV  mulai 
digunakan  oleh  BAU  pada  tahun  2019  dan  terbatas  pada 
petugas BAU sedangkan pada SIMAMOV terdapat fitur yang 
dapat dimanfaatkan oleh unit, sehingga perlu ditambahkan user 
baru untuk mengakses sistem.  

Pelaksanaan  kegiatan  oleh  unit  dapat  menggunakan 
Anggaran  Pendapatan  dan  Belanja  Negara  (APBN)  yang 
nilainya  telah  ditentukan  pada  DIPA.  Dalam  [1]  disebutkan 
bahwa  pagu  dalam  DIPA  merupakan  batas  pengeluaran 
tertinggi yang tidak boleh dilampaui dan pelaksanaannya harus 
dapat dipertanggungjawabkan. Nilai pagu tidak dapat berubah 
setelah  ditetapkan,  namun  pengalokasiannya  dapat  berubah 
sesuai  dengan  kegiatan  yang  dijalankan.  Dalam 
fitur 
pengelolaan  POK  pada  SIMAMOV,  petugas  hanya  dapat 
melihat  riwayat  perubahan  terakhir,  sehingga  tidak  dapat 
melihat  keseluruhan  riwayat  perubahan  yang  terjadi.  Perlu 
ditambahkan  fitur  pada  sistem  yang  dapat  melihat  seluruh 

 1 / 7 

 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

pengajuan  dana,  rekapitulasi  pencatatan  dan  pembuatan 
laporan keuangan setiap bulannya. Karena berbasis web maka 
kepala sekolah dapat melakukan verifikasi pengajuan dana di 
mana  saja.  Pembuatan  laporan  keuangan  setiap  bulan  secara 
otomatis  dapat  meringankan  beban  bendahara.  9  pengujian 
black-box  menghasilkan  hasil  valid  dan  dapat  diterima 
pengguna sebesar 94.6% pada seluruh kriteria. 

Referensi  [3]  menjelaskan  tentang  pengembangkan  sebuah 
Sistem Informasi Akuntansi dan Pengelolaan Keuangan Modul 
Penganggaran Berbasis Web menggunakan java dan posgresql. 
Dalam  penelitian  tersebut  dijelaskan  bahwa  berbagai  macam 
aplikasi  pendukung  kegiatan  keuangan  yang  digunakan  oleh 
sebuah  satuan  kerja  (satker)  kurang  efisien  karena  basis  data 
yang  digunakan  tidaklah  sama.  Karena  basis  data  yang 
digunakan  berbeda,  maka  output  yang  dihasilkan  masing-
masing  aplikasi  tersebut  dapat  memiliki  interpretasi  yang 
berbeda,  sehingga  diperlukan  sebuah  sistem  informasi  yang 
dapat  menggabungkan  keseluruhan  fungsi  aplikasi  agar 
kegiatan  pengelolaan  keuangan  pada  sebuah  satker  dapat 
berjalan  secara  lebih  efisien.  Perbedaan  basis  data  yang 
digunakan  juga  merupakan  sebuah  celah  untuk  melakukan 
tindak  kriminal  berupa  penggelapan  dana  yang  sangat 
merugikan  negara  dan  masyarakat.  Hasil  penelitian  tersebut 
berupa aplikasi web yang dapat melakukan penganggaran dana, 
pembayaran atau penarikan dana, pembukuan bendahara, aset 
dan persediaan, kegiatan administrasi, dan pelaporan keuangan. 

IV. METODE PENELITIAN  

riwayat  perubahan  sehingga  BAU  dapat  lebih  mudah  dalam 
mengawasi pelaksanaan anggaran. 

Unit  dapat  meminta  anggaran  yang  telah  dialokasikan  di 
POK  kepada  BAU.  Proses  permintaan  anggaran  dan 
pencatatannya masih dilakukan secara  manual sehingga tidak 
dapat  dipantau  oleh  unit,  petugas,  maupun  pimpinan  serta 
rentan  terhadap  human  error.  Petugas  BAU  juga  terkadang 
lupa  untuk  memberitahu  unit  bahwa  proses  permintaan  dana 
telah  selesai.  Untuk  mengatasi  permasalahan  tersebut,  akan 
ditambahkan fitur E-loket yang berfungsi untuk menampilkan 
sejauh  mana  proses  permintaan  dana  berjalan.  Fitur  serupa  
terdapat  pada  [4]  dan  [5].    E-loket  juga  dapat  mengirimkan 
pemberitahuan secara otomatis menggunakan surel kepada unit 
ketika permintaannya ditolak atau telah selesai. E-loket dapat 
digabungkan  dengan  pencatatan  realisasi  yang  sebelumnya 
terdapat dalam fitur pengelolaan POK sehingga petugas dapat 
lebih mudah melakukan pencatatan realisasi. Hasil pencatatan 
realisasi  tersebut  nantinya  akan  ditampilkan  dalam  bentuk 
grafik sehingga BAU, unit, dan pimpinan dapat segera melihat 
jumlah realisasi anggaran secara keseluruhan. 

II.  TUJUAN PENELITIAN 

Penelitian  bertujuan  untuk  memperbaiki  sistem  informasi 
keuangan  yang  telah  dibuat  sebelumnya  sehingga  dapat 
mendukung  kegiatan  administrasi  keuangan  Politeknik 
Statistika  STIS,  khususnya  pencatatan  permintaan  dana  oleh 
unit.  Selain  itu  agar  penggunaan  sistem  tidak  terbatas  pada 
petugas  BAU  dan  dapat  dimanfaatkan  oleh  unit  lainnya. 
terdapat  perubahan-
Perbaikan  sistem  dilakukan  karena 
perubahan  proses  bisnis  yang  menyesuaikan  dengan  keadaan 
terkini  serta  agar  sistem  dapat  terus  dimanfaatkan.  Penelitian 
yang  penulis  lakukan  difokuskan  pada  penambahan  fitur  E-
loket permintaan dana dan penambahan user untuk mengakses 
SIMAMOV. Pengembangan fitur riwayat perubahan POK akan 
dilakukan oleh peneliti lain dan dilakukan bersamaan dengan 
penelitian penulis.  

III. PENELITIAN TERKAIT 

(SPPD) 

Referensi 

[6]  menjelaskan 

tentang  pengembangkan 
SIMAMOV  STIS  menggunakan  NodeJs  dan  MongoDB. 
Penelitian  bertujuan  untuk  memudahkan  kegiatan  keuangan 
dalam hal pengelolaan Petunjuk Operasional Kegiatan (POK), 
pembuatan  Surat  Pertanggung  Jawaban  (SPJ),  dan  Surat 
Perintah  Perjalanan  Dinas 
serta  melakukan 
penghitungan realisasi anggaran pada POK. Penelitian tersebut 
informasi  manajemen  dan 
menghasilkan  sebuah  sistem 
monitoring keuangan yang dapat digunakan untuk melakukan 
pengelolaan  POK  pada  tahun  anggaran  berjalan.  Sistem 
tersebut  juga  dapat  melakukan  pencatatan  realisasi  dan 
menampilkan  ringkasan  dari  realisasi  tersebut  dalam  bentuk 
grafik. Selain itu, pengguna dapat melakukan generate SPJ dan 
SPPD  dengan  template  dan  pengaturan  yang  tersedia.  Uji 
System  Usability  Scale  (SUS)  menghasilkan  nilai 76.07 yang 
dapat dikategorikan baik dan sistem layak digunakan. 

Referensi  [5]  menjelaskan  tentang  pengembangkan  sistem 
informasi manajemen berbasis web yang berfungsi membantu 
pengelolaan keuangan di  KB  dan  TK  Permata  Iman  Malang. 
Sistem  tersebut  digunakan  untuk  pencatatan  dana,  pencairan 

Gambar 1. Alur penelitian 

Penelitian  akan  dilakukan  dengan  metode  kualitatif  yaitu 
wawancara  kepada  pengguna  sistem  dan  studi  literatur 
penelitian  terkait,  kemudian  dilakukan  pengembangan  sistem 

 2 / 7 

 
 
 
dengan metode System Development Life Cycle (SDLC) model 
waterfall seperti yang terlihat pada gambar 1.  

petugas  BAU  juga  akan  dibagi  menjadi  beberapa  role  yang 
memiliki peran masing-masing. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Model  waterfall  dipilih  karena  seluruh  kebutuhan  dapat 
diidentifikasi di awal serta sistem dapat dikembangkan secara 
urut  mulai  dari  merancang 
sistem,  penulisan  kode, 
implementasi,  dan  evaluasi  sistem.  Sistem  akan  dibangun 
menggunakan framework NodeJs dan Express serta basis data 
MongoDB. Framework dan basis data tersebut dipilih karena 
dapat memisahkan tampilan depan dengan proses di belakang 
layar menggunakan Model View Controller (MVC) [7]. Selain 
itu  sistem  yang  telah  dibuat  sebelumnya  juga  menggunakan 
framework  dan  basis  data  tersebut,  sehingga  pengembangan 
lebih mudah dilakukan. Evaluasi sistem akan dilakukan dengan 
beberapa  skenario  pengujian  black-box  dan  feedback  dari 
pengguna menggunakan uji SUS. 

V.  KERANGKA PIKIR 

Pengembangan sistem dalam penelitian ini didasarkan pada 
beberapa  permasalahan  seperti  pada  gambar  2,  yaitu  proses 
pemantauan  permintaan  dana,  terbatasnya  user,  dan  realisasi 
anggaran per unit. Permasalahan tersebut dapat diatasi dengan 
menambahkan fitur berupa E-loket dalam bentuk formulir yang 
akan  diisi  oleh  unit  dan  petugas  secara  bergantian  sehingga 
jelas  sejauh  mana  proses  permintaan  dana  berjalan.  Pada  E-
loket  juga  terdapat  dashboard  berisi  daftar  usulan  kegiatan 
untuk mengetahui rencana alokasi dana, daftar permintaan yang 
sedang  berjalan  dan  telah  selesai  untuk  memantau  proses 
permintaan  dana,  serta  grafik  realisasi  anggaran  secara 
keseluruhan dan per unit untuk melihat penyerapan anggaran. 

VI. HASIL DAN PEMBAHASAN 

Bagian  ini  akan  menjelaskan  tentang  hasil  pengembangan 

sistem yang telah dilakukan. 

A.  Identifikasi Permasalahan 

Masalah  utama  yang  diketahui  adalah  mengenai  proses 
permintaan  dana  yang  pencatatannya  masih  dilakukan  secara 
manual. Berdasarkan gambar 3, masalah utama adalah proses 
permintaan  dana  tidak  dapat  dipantau  baik  oleh  petugas, 
pimpinan, dan unit yang mengajukan permintaan. Hal tersebut 
dapat  menyebabkan  terjadinya  miskomunikasi  antar  petugas 
dan  unit  bersangkutan  sehingga  unit  harus  bertanya  kepada 
petugas  sejauh  mana  proses  berjalan.  POK  yang  digunakan 
sebagai dasar dalam permintaan dana juga berasal dari aplikasi 
SAKTI  sehingga  perlu  dilakukan  crosscheck  manual  terlebih 
dahulu  yang  cukup  menyulitkan  petugas.  Pencatatan  yang 
dilakukan  secara  manual  juga  rentan  terhadap  human  error, 
misalnya  kesalahan  penulisan  dan  petugas  harus  kembali 
melakukan crosscheck terhadap POK. 

Gambar 2. Kerangka pikir 

User yang ada sebelumnya terbatas pada internal BAU dan 
dibagi  menjadi  2  yaitu  admin  dan  editor,  sehingga  perlu 
dilakukan perubahan model user agar unit dan pimpinan dapat 
mengakses  sistem.  Perubahan  model  user  yang  diperlukan 
meliputi jenis petugas BAU, jenis pengguna unit,  serta  jenis 
unit pengguna yang akan dimasukkan pada kolom jabatan, role, 
dan  unit  serta  kolom  email.  Pimpinan  nantinya  terbatas  pada 
pemantauan  proses  permintaan  dana  dan  realisasi  anggaran, 
sedangkan  unit  dapat  melakukan  permintaan  dana  dan 
memantau  prosesnya  serta  membuat  SPJ  dan  SPPD 
menggunakan template dan pengaturan yang tersedia. Selain itu 

Gambar 3. Diagram fishbone masalah permintaan dana 

Dengan  digitalisasi  proses  permintaan  dana,  maka  unit 
memerlukan  akses  terhadap  sistem.  Akses  yang  diperlukan 
tidak hanya untuk permintaan dana namun juga fitur lain dalam 
sistem  yang  dapat  dimanfaatkan  oleh  unit.  Pada  SIMAMOV 
terdapat  grafik  yang  menampilkan  jumlah  realisasi  secara 
keseluruhan namun tidak dapat menampilkan jumlah realisasi 
unit tertentu. Grafik tambahan diperlukan agar unit juga dapat 
mengukur  seberapa  besar  penyerapan  anggaran  yang  telah 
dilakukan unitnya. 

B.  Analisis Kebutuhan 

Berdasarkan  permasalahan  yang  ada  maka  penulis 
menentukan (1) Sistem perlu menampilkan sejauh mana proses 
permintaan  dana  unit  sedang  berjalan,  (2)  Sistem  dapat 
memberitahu  unit  ketika  permintaan  dana  ditolak  atau  telah 
selesai,  (3)  Penambahan  user  untuk  setiap  unit  dan  petugas 
BAU serta pimpinan, (4) Sistem dapat menampilkan realisasi 
anggaran masing-masing unit. 

 3 / 7 

 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

menjadi  3  jabatan  yaitu  petugas  BAU,  pimpinan,  dan  unit. 
Petugas BAU memiliki 7 role, yaitu verifikator, PPK, PPSPM, 
reviewer,  bendahara,  operator  bank,  dan  binagram.  Pimpinan 
terdiri  dari  direktur  dan  wakil  direktur.  Unit  terdiri  dari 
pimpinan dan operator unit dengan jenis unitnya adalah BAU, 
BAAK, PRODI-D3, PRODI-D4, UPPPM, SPM, Perpustakaan, 
IT, dan Lainnya. 

Gambar 5. Rancangan user 

Jabatan dan role user dapat ditambahkan secara langsung 
oleh admin di kemudian hari dengan pengaturan tampilan dan 
fungsinya sama dengan  jabatan  unit.  Kolom unit pada model 
juga  akan  digunakan  untuk  membagi  POK  yang 
user 
ditampilkan sehingga unit hanya dapat melihat POK masing-
masing. POK dan unit memiliki hubungan many to many yang 
artinya  setiap  akun/detil  POK  dapat  dibagi  kepada  beberapa 
unit, sebaliknya setiap unit dapat menggunakan beberapa akun. 

4.  Rancangan Tampilan E-loket 

C.  Desain Sistem 

1.  Rancangan Use Case 

No 

Aktor 

1  Unit 

2  Pimpinan 

3  Petugas BAU 

4  Administrator 

TABEL I 
IDENTIFIKASI AKTOR 
Deskripsi 
Orang yang dapat melakukan pengelolaan 
SPPD (membuat surat tugas, surat perhitungan, 
dan pengaturan SPPD), pengelolaan SPJ 
(membuat SPJ honor dan transport), melihat 
daftar POK, melakukan permintaan dana dan 
melihat daftar permintaan dana serta realisasi 
unit masing-masing . 

Orang yang dapat melihat daftar POK, daftar 
permintaan dana, dan realisasi anggaran secara 
keseluruhan 
Orang yang dapat mengelola permintaan dana 
(menolak dan memproses), melihat daftar 
permintaan dana, dan realisasi anggaran secara 
keseluruhan 
Orang yang dapat melakukan semua yang 
dilakukan aktor lain, melakukan pengelolaan 
pegawai (menambah dan menyunting daftar 
pegawai, melihat penerimaan pegawai) serta 
melakukan pengelolaan pengguna (menambah, 
menghapus, dan menyunting pengguna) 

Pada penelitian sebelumnya, hanya terdapat 2 aktor yaitu 
admin  dan  editor,  sedangkan  pada  penelitian  yang  penulis 
lakukan,  terdapat  4  aktor  seperti  dalam  tabel  I.  Perbedaan 
lainnya adalah pada pengelolaan POK, sebelumnya admin dan 
editor dapat menyunting detil POK, namun pada sistem yang 
baru  POK 
tidak  dapat  disunting  setiap  saat  karena 
penyuntingan  hanya  dapat  dilakukan  oleh  binagram  atas 
permintaan Petugas Pembuat Komitmen (PPK). 

2.  Rancangan Basis Data 
Basis  data  yang  digunakan  sama  seperti  sebelumnya 
dengan dilakukan penambahan model usulan, model loket dan 
perubahan pada model user. Perubahan pada model user adalah 
penambahan  kolom  jabatan,  role,  unit,  dan  email.  Perincian 
kolom model secara keseluruhan terdapat pada gambar 4. 

Gambar 4. Rancangan basis data yang ditambahkan dan disunting 

3.  Rancangan User 
Secara umum terdapat 4 level user seperti pada gambar 5. 
Jenis  user  terdiri  dari  admin dan  pengguna.  Pengguna  dibagi 

Gambar 6. Rancangan dashboard E-Loket 

 4 / 7 

 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 8. Alur permintaan dana secara umum pada sistem bagian 1 

Gambar 7. Rancangan formulir petugas operator bank 

Gambar  6  merupakan  rancangan  dashboard  yang  dibagi 
menjadi 4 tab, masing-masing tab berisi daftar usulan kegiatan 
yang akan dilaksanakan, daftar permintaan dana yang sedang 
berjalan, daftar permintaan dana yang telah selesai dan grafik 
realisasi anggaran unit serta grafik realisasi secara keseluruhan. 
Grafik  realisasi  unit  yang  ditampilkan  akan  sesuai  dengan 
realisasi detil POK yang telah dibagi sebelumnya. 

Formulir yang digunakan dalam E-loket memiliki tampilan 
yang  relatif  sama  dengan  penambahan  isian  formulir  seiring 
proses  permintaan  berjalan  untuk  setiap  petugas.  gambar  7 
merupakan  rancangan  formulir  petugas  terakhir  yang  telah 
memuat formulir secara lengkap. 

D.  Pembangunan Sistem 

Pada bagian ini berisi tentang alur permintaan dana secara 
keseluruhan  yang  diterapkan  penulis  pada  sistem  dan 
ditampilkan  pada  gambar  8  dan  gambar  9,  mulai  dari 
pengusulan kegiatan oleh unit sampai penerimaan pembayaran 
oleh  unit.  Setiap  proses  permintaan  akan  dicatat  oleh  sistem 
secara  otomatis  ke  dalam  basis  data  dan  dilakukan  secara 
bersamaan  ketika proses  petugas  berjalan.  Ketika  permintaan 
dana  selesai  diproses  atau  ditolak  oleh  petugas,  sistem  juga 
akan  mencatatnya  ke  dalam  basis  data  serta  mengirimkan 
pemberitahuan secara otomatis melalui surel. 

Gambar 9. Alur permintaan dana secara umum pada sistem bagian 2 

 5 / 7 

 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

E.  Implementasi 

Implementasi yang dilakukan terbatas pada browser  yang 
dijalankan  dengan  mode  desktop  dan  tidak  di  optimalisasi 
untuk  tampilan  mobile.  Hal  ini  dikarenakan  terdapat  tabel 
dengan kolom yang cukup banyak sehingga secara keseluruhan 
lebih baik diakses menggunakan komputer/laptop. Gambar 10 
merupakan  tampilan  salah  satu  tab  dashboard  yang  berisi 
daftar  permintaan  dana  yang  sedang  diproses.  Tabel  dapat 
membatasi  daftar  yang  ditampilkan  sejumlah  10,  25,  50,  dan 
semua.  Pada  tabel  juga  terdapat  kotak  pencarian  untuk 
memudahkan  pengguna.  Gambar  11  merupakan  contoh 
formulir salah satu petugas untuk memproses permintaan dana. 
Terdapat  beberapa  isian  yang  masih  dapat  diubah  dan  tidak 
dapat diubah yang disesuaikan dengan user-nya. 

Ketika membutuhkan revisi POK, PPK dapat mengirimkan 
permintaan revisi kepada binagram menggunakan tombol yang 
tersedia. Permintaan PPK akan dikirimkan otomatis oleh sistem 
menggunakan 
surel  dengan  mencantumkan  beberapa 
keterangan seperti perincian POK yang digunakan, jumlah nilai 
pengajuan, jenis unit, dan catatan PPK. Kemudian revisi akan 
dilakukan  oleh  binagram  di  luar  sistem.  Setelah  revisi, 
binagram  akan  mengunggah  hasil  revisi  ke  dalam  sistem. 
Proses revisi masih melibatkan manusia ketika  surel diterima 
dan  hasil  revisi  diunggah  ke  dalam  sistem,  sehingga 
dimungkinkan timbulnya error baru yaitu human error.  

F.  Pengujian Sistem 

Pengujian sistem dilakukan terhadap 15 responden dengan 
3 orang berasal dari BAAK, 8 orang dari BAU, dan 4 orang dari 
mahasiswa. Hasil uji black-box dengan 15 skenario pengujian 
menghasilkan  kesimpulan  sesuai  untuk  keseluruhan  skenario 
karena fungsi berjalan sesuai dengan apa yang diharapkan.  

Selain uji black-box, uji yang dilakukan menggunakan SUS 
menghasilkan  skor  rata-rata  sebesar  64,83.  Hal  ini  berarti 
sistem dapat diterima pengguna namun belum cukup baik untuk 
digunakan.  Dari  10  pertanyaan  SUS,  skor  terendah  terdapat 
pada pertanyaan ke-10 yaitu ‘Saya merasa perlu membiasakan 
diri  dahulu  sebelum  menggunakan  sistem  ini’.  Hal  ini 
disebabkan  karena  sebagian  besar  responden  memang  belum 
pernah  menggunakan  sistem  dan  proses  sosialisasi  belum 
dilakukan secara menyeluruh.  

Responden baru mendapatkan penjelasan dalam bentuk teks 
dan  gambar  ketika  pengujian  dilakukan.  Penulis  masih  perlu 
memberikan  penjelasan  dalam  bentuk  visual  dan  audio 
sehingga penggunaan sistem dapat dipahami dengan lebih baik. 

A.  Kesimpulan 

VII. 

PENUTUP 

Perubahan model user dapat membuat unit, pimpinan, dan 
petugas mengakses sistem menggunakan user baru. Unit telah 
dapat  melakukan  permintaan  dana  menggunakan  sistem  dan 
melihat  sejauh  mana  prosesnya  serta  dapat  melihat  grafik 
realisasi unitnya masing-masing. Pimpinan dan petugas BAU 
dapat melihat daftar permintaan dana dan grafik realisasi semua 
unit.  Petugas  BAU  juga  telah  dapat  memproses  permintaan 
dana sesuai dengan role yang dimiliki. Ketika permintaan dana 

 6 / 7 

Gambar 10. Tampilan dashboard tab pengajuan 

Gambar 11. Contoh tampilan formulir petugas 

 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

[3]  H.  Rakhmanto,  “Pengembangan  Sistem  Informasi  Akuntansi  dan 
Pengelolaan Keuangan Modul Penganggaran Berbasis Web Menggunakan 
Teknologi Java dan PosgreSQL,” Jurnal Teknik ITS, vol. 1, no. September, 
pp. 417-420, 2012. 

[4]  A. Rohman and I. y. B, “Aplikasi Asisten Praktikum Menggunakan NodeJs 
dan Database MongoDB (Studi Kasus Lab Stmik Akakom),” in Seminar 
Riset Teknologi Informasi (SRITI), Yogyakarta, 2016. 

[5]  M. Rizsky, S. A. Wicaksono and M. C. Saputra, “Pengembangan Sistem 
Informasi  Keuangan  (Studi  Kasus:  KB  &  TK  Permata  Iman  Malang),” 
Jurnal Pengembangan Teknologi Informasi dan Ilmu Komputer, vol. 2, no. 
12, pp. 7039-7049, 2018.  

[6]  M. Shamad and A. Ghofar, “Pembangunan Sistem Informasi Manajemen 
Administrasi, Monitoring, dan Evaluasi Anggaran STIS,” Sekolah Tinggi 
Ilmu Statistik, Jakarta Timur, 2017. 

[7]  D.  P.  Pop  and  A.  Altar,  “Designing  an  MVC  Model  for  Rapid  Web 
Application Development,” Procedia Eng., vol. 69, pp. 1172–1179, 2014, 
doi: 10.1016/j.proeng.2014.03.106. 

[8]  A. Firdaus, S. Widodo, A. Sutrisman, S. G. F. Nasution and R. Mardiana, 
“Rancang  Bangun  Sistem  Informasi  Perpustakaan  Menggunakan  Web 
Service pada Jurusan Teknik Komputer POLSRI,” jurnal Informanika, vol. 
5, no. 2, pp. 81-87, 2019. 

[9]  H.  F.  Bachri,  B.  Priyambadha  and  D.  S.  Rusdianto,  “Pengembangan 
Aplikasi  Manajemen  Event  Berbasis  Web  (Studi  Kasus:  Fakultas  Ilmu 
Administrasi  Universitas  Brawijaya  Malang),”  Jurnal  Pengembangan 
Teknologi Informasi dan Ilmu Komputer, vol. 2, no. 12, pp. 6752-6760, 
2018. 

[10] S. Rahayu and P. A. Rahayu, “Perancangan Sistem Informasi Pengelolaan 
Keuangan  Berbasis  Web  Di  Sekolah  Menengah  Kejuruan  Islam 
Atturmudziyyah  Garut,”  Jurnal  Algoritma  Sekolah  Tinggi  Teknologi 
Garut, vol. 14, no. 2, pp. 538-545, 2017.  

[11] K. H. Pambudi dan H. Adam, “Analisis Dimensi Kesuksesan Implementasi 
Sistem  Aplikasi Keuangan  Tingkat  Instansi  (SAKTI)  pada  Satuan Kerja 
Wilayah  Provinsi  Jawa  Timur  dengan  Pendekatan  Delone  and  Mclean 
Information  System  Success  Model,”  Jurnal  Ilmiah  Mahasiswa  FEB 
Universitas Brawijawya, vol. 6, no. 2, 2018. 

[12] Z. Sharfina and H. B. Santoso, “An Indonesian Adaptation of the System 

Usability Scale,” in ICACSIS, Jakarta, 2016. 

[13] J. R. Lewis and J. Sauro, “The Factor Structure of the System Usability 

Scale,” in International Conference (HCII), San Diego, 2009.  

[14] J.  Brooke,  “SUS:  A  quick  and  dirty  usability  scale,”  in  Usability 
Evaluation in Industry, London, Taylor and Francis, 1995, pp. 189-194. 
[15] I.  Aprilia.  H.  N.,  P.  I.  Santoso  dan  R.  Ferdiana,  “Pengujian  Usability 
Website Menggunakan System Usability Scale,” IPTEK-KOM, vol. 17, no. 
1, pp. 31-38, 2015. 

[16] M. E. Khan dan F. Khan, “A Comparative Study of White Box, Black Box 
and  Grey  Box  Testing  Techniques,”  (IJACSA)  International  Journal  of 
Advanced Computer  Science  and  Applications,  vol. 3,  no.  6, pp. 12-15, 
2012.  

[17] S. Nidhra dan J. Dondeti, “Black Box and White Box Testing Techniques 
–A Literature Review,” International Journal of Embedded Systems and 
Applications (IJESA), vol. 2, no. 2, pp. 29-50, 2012. 

telah  selesai  maupun ditolak oleh petugas,  sistem  telah  dapat 
mengirimkan pemberitahuan kepada unit melalui surel. 

E-Loket yang dibuat sudah dapat mengatasi permasalahan 
pemantauan  proses  permintaan  dana,  mulai  dari  pengusulan 
kegiatan yang akan dilaksanakan sampai pembayaran diterima 
oleh  unit.  Pengujian  yang  dilakukan  menggunakan  uji  black-
box  mendapatkan  hasil  yang  baik,  namun  pengujian 
menggunakan  SUS  mendapatkan  hasil  yang  kurang  baik, 
sehingga sistem masih perlu diperbaiki. Maka dari itu, penulis 
akan mencoba memperbaiki sistem berdasarkan masukan dan 
tanggapan dari pengguna pada pengujian yang telah dilakukan 
agar sistem lebih dapat diterima. Penjelasan sistem juga harus 
dilakukan  secara  rinci  dan  lengkap  agar  pengguna  dapat 
memahami cara menggunakan sistem dengan baik. 

B.  Saran 
Saran  yang  dapat  penulis  sampaikan  untuk  pengembang  dan 
peneliti selanjutnya adalah 

1.  Proses  revisi  yang  dilakukan  di  luar  sistem  dapat 
menimbulkan  human  error  karena  masih  melibatkan 
manusia.  Error  yang  muncul  dapat  dikurangi  dengan 
melakukan  integrasi  sistem  dengan  sistem  lain  yang 
digunakan  untuk  melakukan  revisi.  Hal  ini  tentunya 
tidak  mudah  karena  kemungkinan  besar,  sistem  lain 
tersebut memiliki arsitektur yang berbeda dengan sistem 
ini.  Namun  integrasi  sistem  patut  dipertimbangkan 
karena  revisi  POK  sering  dilakukan  dan  merupakan 
salah satu hal penting dalam kegiatan pengelolaan POK. 
2.  Bagian  front-end  masih  menggunakan  view  engine 
handlebars  yang  sederhana  dan  kurang  fleksibel.  Jika 
memungkinkan  untuk  penelitian  selanjutnya  bagian 
tersebut diganti menggunakan react atau vue yang lebih 
fleksibel  dan  memiliki  library  pendukung  sehingga 
dapat  mengatasi  pemrograman 
lanjut  dan 
tampilan  yang  lebih  dinamis.  Selain  itu  source  code 
menjadi  lebih  rapi  serta  pengembang  lebih  mudah 
melakukan maintenance pada fungsi-fungsi yang ada. 

tingkat 

3.  E-loket yang penulis kembangkan masih  terbatas pada 
pemantauan  proses  permintaan  dana  dan  pengiriman 
notifikasi kepada unit. Untuk penelitian selanjutnya bisa 
dikembangkan  fitur  lain  seperti  pembacaan  dokumen 
keuangan yang dibutuhkan, formulir yang lebih dinamis, 
atau integrasi dengan sistem informasi yang lain, 

DAFTAR PUSTAKA 
[1]  Peraturan  Menteri  Keuangan  Nomor  164/PMK.05/2011.  (2010,  11). 
Petunjuk Penyusunan dan Pengesahan Daftar Isian Pelaksanaan Anggaran. 
[Online]. 
Available: 
https://jdih.kemenkeu.go.id/fulltext/2011/164~PMK.05~2011PerLamp1.h
tm 

[2]  Peraturan  Menteri  Keuangan  Nomor  91/PMK.02/2020.  (2020,  7). 
Perubahan  atas  Peraturan  Menteri  Keuangan  Nomor  193/PMK.02/2017 
tentang  Tata  Cara  Perencanaan,  Penelaahan,  dan  Penetapan  Alokasi 
Anggaran  Bagian  Anggaran  Bendahara  Umum  Negara,  dan  Pengesahan 
Daftar Isian Pelaksanaan Anggaran Bendahara Umum Negara. [Online]. 
Avalaible: 
https://jdih.kemenkeu.go.id/api/AppMediaCatalogs/Download/d78717d0-
fca2-42b3-b3ad-2825a993f3f5 
or 
https://jdih.kemenkeu.go.id/in/page/dokumen-peraturan/66d5fdd9-cc87-
47a7-a155-08d82fbdfef1 

 7 / 7 

 
 
 
 
"
221709863,"Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik 

Kajian Perbandingan pada Klasifikasi Sentimen 
Twitter Multi-kelas Berbahasa Indonesia yang Tidak 
Seimbang dengan Support Vector Machine (SVM) 

M. Rizki (221709863, 4SD1) 

Dosen Pembimbing:  Budi Yuniarto, SST, M.Si

Ringkasan—  Analisis  sentimen  Twitter 

telah  menjadi 
penelitian yang penting dan menarik banyak pihak, baik dibidang 
keilmuan  maupun  non  keilmuan.  Hal  ini  dikarenakan  Twitter 
merupakan salah satu penyedia data teks mentah terbesar saat ini 
di  mana  orang  bebas  beropini  mengenai  suatu  hal.  Namun, 
permasalahan  ketidakseimbangan  data  pada  analisis 
ini 
cenderung  jarang  diatasi.  Membangun  model  pada  data  yang 
tidak  seimbang  akan  menghasilkan  performa  yang  bias, karena 
hanya  akan  cenderung  mengklasifikasi  data  pada  kelas  dengan 
data yang lebih banyak (mayor). Oleh karenanya, pada penelitian 
ini,  kami  bertujuan  untuk  menghubungkan  pembelajaran 
mengenai  ketidakseimbangan  data  dengan  analisis  sentimen. 
Penelitian ini meliputi tiga metode  resampling: SMOTE, Tomek 
Link,  SMOTTomek;  algoritma  klasifikasi  Multi-kelas  SVM 
dengan  pendekatan  One  vs  Rest  (OVR)  dan  One  vs  One  (OVO); 
Kernel  RBF  dan  Linier;  dan  penggunaan  beberapa  dataset 
Twitter  multi-kelas  berbahasa  Indonesia,  termasuk  dataset 
sentimen masyarakat di Twitter mengenai pelayanan PT KAI. 

Kata  Kunci—  Imbalance  data,  Multi-kelas  SVM,  SMOTE, 

Tomek Link, SMOTTomek. 

I.  LATAR BELAKANG 

Twitter  sebagai  salah  satu  media  sosial  populer  saat  ini 
memberikan  fasilitas  bagi  pengguna  untuk  mencurahkan 
perasaan, emosi, dan sikap terhadap suatu topik melalui cuitan 
biasa disebut dengan twit. Setiap hari, terdapat lebih dari 500 
juta twit diproduksi secara online [1]. Fakta ini memungkinkan 
seseorang untuk melakukan analisis terhadap opini masyarakat 
Twitter. Saat ini telah berkembang metode untuk mempelajari 
opini  dengan  jumlah  yang  besar  baik  pada  level  holistik 
maupun  level  individu  yang  disebut  analisis  sentimen  [2]. 
Tujuan utama dari analisis sentimen adalah untuk mendapatkan 
informasi  tentang  bagaimana kecenderungan  opini  (polaritas) 
pengguna  terhadap  entitas  yang  diperbincangkan  (contoh: 
positif, netral, negatif). Metode ini menciptakan peluang besar 
bagi  sektor  bisnis  untuk  menilai  kualitas  layanan  produk 
mereka secara subjektif dan hemat [1]. 

Pada data Twitter di mana sifatnya yang bersifat acak dan 
tidak  terstruktur  serta  memiliki  volume  yang  sangat  besar, 
sangat  memungkinkan  terjadinya  ketidakseimbangan  data. 
Namun, dibanyak studi mengenai analisis sentimen, seringkali 
ketidakseimbangan data ini tidak diatasi [3]. Banyak penelitian 
mengenai  klasifikasi  sentimen  berasumsi  bahwa  data  pada 
kelas sudah seimbang, padahal pada prakteknya tidak demikian 
[4].  Membangun  model  klasifikasi  pada  data  yang  tidak 
seimbang  akan  menghasilkan  performa  yang  bias  dan  model 
akan salah memprediksi kelas pada data uji karena hanya akan 
cenderung mengklasifikasikan data pada kelas dengan proporsi 
lebih banyak. 

Untuk  mengatasi  permasalahan  ketidakseimbangan  data, 
salah satu yang dapat dilakukan adalah dengan menggunakan 
pendekatan  resampling  [5].  Pendekatan  resampling  berfokus 
pada  mengubah  jumlah  data  menjadi  seimbang  sebelum 
diproses  ke  dalam  algoritma.  Pada  pendekatan  resampling, 
dataset 
algoritma 
klasifikasinya, sehingga lebih baik dan lebih serba guna untuk 
digunakan [5]–[7].  

independen  dengan 

lebih  bersifat 

Terdapat tiga jenis teknik resampling, yaitu oversampling, 
undersampling,  dan  hybridsampling  [6].  Salah  satu  teknik 
oversampling  yang  sering  digunakan  dan  terbukti  dapat 
mengatasi permasalahan ketidakseimbangan data dengan baik 
adalah  synthetic  minority  oversampling  technique  (SMOTE). 
Pada  penerapan  undersampling,  salah  satu  teknik  yang  dapat 
mengatasi  noise  pada  data  adalah  Tomek  Link.  Pada 
hybridsampling,  kita  dapat  menggabungkan  kedua  algoritma 
ini  (SMOTE  dan  Tomek  Link)  menjadi  SMOTTomek 
berdasarkan  penelitian  terdahulu  (Batista,  2003).  Ketiga 
algoritma resampling ini terbukti merupakan algoritma terbaik 
ketika  menangani  kasus  dataset  sentimen  biner  berbahasa 
Indonesia yang tidak seimbang [7]. 

mengenai 

Penelitian 

perbandingan 

penanganan 
ketidakseimbangan  data  pada  area  klasifikasi  sentimen 
berbahasa Indonesia masih sangat jarang dilakukan. Penelitian 
yang ada saat ini pernah dilakukan oleh [7], [8], di mana pada 
penelitian tersebut mereka membandingkan beberapa algoritma 
resampling yang ada, namun masih terbatas pada penanganan 
satu  dataset  saja  dan  terbatas  pada  permasalahan  klasifikasi 
biner (positif/negatif), padahal pengguna informasi sering kali 
menginginkan  informasi  yang  lebih  detil  dalam  bentuk 
multikelas yang lebih halus seperti penambahan kelas netral [1], 
[6].  

Ketika  melakukan  studi  pada  area  klasifikasi  data, 
dibutuhkan  algoritma  klasifikasi  untuk  melakukan  proses 
pembelajaran.  Algoritma  klasifikasi  yang  sudah  terbukti 
bekerja  dengan  baik  pada  data  yang  tidak  seimbang  adalah 
support vector machine (SVM) [9], [10]. SVM bekerja dengan 
cara mencari hyperplane terbaik untuk memisahkan dua kelas. 
Di beberapa kasus, terdapat kelas yang tidak dapat dipisahkan 
secara linier, sehingga  dibutuhkan transformasi ruang data ke 
dimensi yang lebih tinggi agar kelas dapat dipisahkan. Sebuah 
fungsi  kernel  biasanya  digunakan  untuk  melakukan 
transformasi  tersebut.  Beberapa  kernel  yang  biasa  digunakan 
diantaranya  adalah  kernel  linear  dan  kernel  radial  basic 
function  (RBF).  Beberapa  penelitian  terkini  seperti  yang 
dilakukan oleh [9], di mana mereka menggunakan SVM linear 
pada  klasifikasi  data 
tidak  seimbang  dan 
teks  yang 
menyimpulkan  hasil  yang  efektif.  Penelitian  penggunaan 

 1 / 9 

 
 
kernel  RBF  pada  data  tidak  seimbang  pernah  dilakukan  oleh 
[10],  di  mana  mereka  menggunakan  SVM  RBF  untuk 
melakukan klasifikasi data teks yang tidak seimbang dan juga 
terbukti  efektif.  Kedua  kernel  tersebut  terbukti  efektif  dalam 
mengklasifikasikan data teks tidak seimbang, namun belum ada 
yang  berfokus  membandingkan  kedua  kernel  ini  pada  area 
klasifikasi data teks tidak seimbang.  

Sejak  penelitian  mengenai  analisis 

sentimen  dan 
ketidakseimbangan data pada dataset multikelas sangat jarang 
dilakukan, terdapat gap penelitian mengenai perbandingan dari 
teknik  penanganan  ketidakseimbangan  data  pada  area 
klasifikasi sentimen multikelas data [1], [11]. Oleh karenanya 
pada  penelitian  ini,  kami  akan  melakukan  studi  eksperimen 
mengenai  analisis  perbandingan  teknik  resampling  bersama 
dengan  algoritma  SVM  pada  dataset  Twitter  multikelas 
berbahasa  Indonesia  yang  tidak  seimbang.  Untuk  melakukan 
eksperimen ini, kami menggunakan teknik resampling SMOTE, 
Tomek  Link,  dan  SMOTTomek  yang  telah  mewakili  ketiga 
teknik  resampling.  Algoritma  klasifikasi  yang  digunakan 
adalah  SVM  dengan  teknik binerisasi  One  vs  All  (OVA)  dan 
One vs One (OVO) serta kernel SVM yang digunakan adalah 
Linier  dan  RBF.  Dataset  yang  digunakan  pada  penelitian  ini 
terdiri  dari  satu  data  primer  dan  enam  data  sekunder.  Data 
primer dikumpulkan secara mandiri melalui proses scrapping 
akun  @KAI121  yang  merupakan  akun  khusus  layanan 
pelanggan  KAI.  Alasan  penggunaan  data  ini  dikarenakan 
melalui  data  ini,  kita  dapat  mengimplementasikan  model 
machine learning terbaik untuk melakukan penilaian terhadap 
kualitas  pelayanan  dari  PT  KAI.  Sedangkan,  data  sekunder 
dikumpulkan melalui penelitian terdahulu dan publikasi. 

II.  TUJUAN PENELITIAN 

Berdasarkan  latar  belakang  dan  rumusan  masalah,  tujuan 

penelitian ini adalah sebagai berikut: 

Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik 

digunakan dan perbandingan antara teknik binerisasi OVO dan 
OVA. Pada penelitian ini, disimpulkan bahwa teknik binerisasi 
OVO lebih baik dibandingkan dengan teknik binerisasi OVA. 
Indrawati et al. (2020) yang melakukan analisis pengaruh 
metode resampling pada kategorisasi data Indonesian Scientific 
Journal  Database  (ISJD).  Pada  penelitian  ini,  Indrawati 
menggunakan  TF-IDF  sebagai  feature  extraction  dan  Chi-
square sebagai feature selection. Penelitian ini menyimpulkan 
bahwa  metode  resampling  dapat  meningkatkan  performa 
model klasifikasi pada kasus klasifikasi data ISJD melalui nilai 
presisi, recall, F1-Score; dengan SMOTE merupakan metode 
terbaik,  Tomek  Link  merupakan  metode 
oversampling 
undersampling  terbaik,  dan  SMOTTomek  adalah  metode 
gabungan terbaik dalam mengkategorikan dataset ISJD [7]. 

Penelitian  mengenai  penggunaan  algoritma  Multikelas 
SVM  pada  data  sentimen  Twitter  berbahasa  Indonesia  telah 
dilakukan  oleh  Alita  et  al.  (2020)  di  mana  pendekatan  yang 
digunakan  yaitu  One  vs  One  (OVO)  dan  One  vs  All  (OVA). 
Pada penelitian ini, Alita et al. (2020) yang menggunakan TF-
IDF  dengan  unigram  kata.  Kesimpulan  menunjukkan  bahwa 
pendekatan OVO lebih baik untuk nilai presisi, recall, dan F1-
Score,  sedangkan  pendekatan  OVA  lebih  baik  untuk  nilai 
akurasi. 

 Berdasarkan  penelitian  terkait  diatas,  terdapat  beberapa 
perbedaan  dengan  penelitian  ini.  Kami  belum  menemukan 
literatur  terpublikasi  yang  melakukan  penelitian  mengenai 
penanganan  ketidakseimbangan  data  pada  area  klasifikasi 
sentimen multikelas berbahasa Indonesia. Kebanyakan literatur 
melakukan penerapan metode resampling yang berfokus pada 
satu dataset seperti yang dilakukan oleh penelitian terdahulu [8] 
[13] dan terbatas pada dataset biner saja. 

analisis 

perbandingan 

1.  Melakukan 

metode 
resampling (SMOTE, Tomek Link, dan SMOTTomek) 
yang  diterapkan  pada  kasus  klasifikasi  data  Twitter 
multikelas berbahasa Indonesia yang tidak seimbang. 
2.  Melakukan  analisis  perbandingan  antara  kernel  linear 
dan RBF pada kasus klasifikasi data Twitter multikelas 
berbahasa Indonesia yang tidak seimbang. 
3.  Melakukan  analisis  perbandingan  antara 

teknik 
binerisasi  One  vs  All  (OVA)  dan  One  vs  One  (OVO) 
pada kasus klasifikasi data Twitter multikelas berbahasa 
Indonesia yang tidak seimbang. 

4.  Menerapkan model terbaik pada kasus analisis sentimen 
data Twitter multikelas Kereta Api Indonesia (KAI). 

III. PENELITIAN TERKAIT 

Fernandez  et  al.,  (2013)  melakukan  studi  mengenai 
klasifikasi data multikelas yang tidak seimbang menggunakan 
pendekatan  binerisasi.  Dataset  yang  digunakan  terdiri  dari 
beberapa  dataset 
tidak  seimbang  berbasis  multikelas. 
Algoritma  klasifikasi  yang  digunakan  adalah  SVM,  Decision 
Tree, KNN dengan pendekatan OVO dan OVA. Pada studi ini, 
Fernandez dkk melakukan beberapa studi, salah satunya yaitu 
melihat  perbandingan  antara  antar  teknik  resampling  yang 

Gambar 1. Diagram penelitian terkait. 

IV. METODE PENELITIAN  

A.  Ruang Lingkup Penelitian 

Pada  penelitian  ini,  kami  melakukan  penerapan  metode 
resampling  pada  klasifikasi  sentimen  Twitter  multikelas 
berbahasa  Indonesia  yang  tidak  seimbang.  Untuk  mencapai 
tujuan  penelitian,  kami  melakukan  eksperimen  dengan  dua 

 2 / 9 

 
 
 
jenis algoritma klasifikasi, yaitu One vs All SVM dan One vs 
One  SVM  yang  dikombinasikan  dengan  tiga  algoritma 
resampling,  yaitu  SMOTE,  Tomek  Link,  dan  SMOTTomek. 
Dataset yang digunakan terdiri dari dataset primer dan dataset 
sekunder.  Dataset  primer  didapat  melalui  proses  scrapping 
akun  Twitter  @KAI121,  sedangkan  dataset  sekunder  didapat 
melalui  studi  literatur.  Hasil  performa  model  dievaluasi 
menggunakan  nilai  F1-Score  dan  geometric  mean  dengan 
pendekatan  rata-rata  makro.  Seluruh  pembuatan  model  ini 
menggunakan  bahasa  pemrograman  Python.  Hasil  dari  setiap 
performa  model  akan  dilakukan  uji  statistik  nonparametrik 
untuk menggali informasi lebih dalam mengenai pengaruh dari 
penerapan berbagai model. 

B.  Metode Pengumpulan Data 

Pada tahap ini dilakukan proses pengambilan data Twitter 
berbahasa Indonesia. Data yang digunakan pada penelitian ini 
terdiri dari dua jenis, pertama data  primer yang dikumpulkan 
melalui  proses  scrapping,  Kedua  yaitu  data  sekunder  yang 
dikumpulkan  melalui  studi  literatur.  Seluruh  data  pada 
penelitian memiliki indeks rasio ketidakseimbangan >  1.5,  di 
mana batasan ini digunakan pada penelitian terdahulu [1], [6]. 
Indeks rasio ketidakseimbangan dirumuskan sebagai berikut.  

𝐼𝑅 =

𝑁−
𝑁+

(1) 

Di mana 𝑁− dan 𝑁+ anggota pada kelas mayor dan kelas minor. 
Berikut penjelasan mengenai data Primer dan data Sekunder 

yang digunakan. 

Data Primer 

Gambar 2. Diagram alur proses scrapping. 

Pada data primer, data dikumpulkan dengan cara scrapping 
secara  langsung  dengan  menggunakan  library  Twint  pada 
bahasa  pemrograman  Python.  Gambar  2  menjelaskan  alur 
proses  scrapping  data  KAI.  Library  twint  digunakan  karena 
kelebihannya yang tidak memerlukan akun developer dan tidak 
perlu  memahami  API  Twitter  lebih  dalam.  Konfigurasi  yang 
digunakan  sebagai  input  pencarian  yaitu  “@KAI121”  yang 
merupakan akun aduan konsumen terhadap pelayanan Kereta 
Api  Indonesia.  Bahasa  twit  yang  disaring  hanya  bahasa 
Indonesia,  rentang  waktu  yang  digunakan yaitu  satu  minggu, 
mulai 24 Desember 2019 sampai 31 Desember 2019, di mana 
data  yang terkumpul berjumlah 7.129 twit mentah yang akan 
dikategori  menjadi 
Proses 
pengkategorian pada data primer ini menggunakan bantuan dari 
library  polygot  yang  memungkinkan  kita  untuk  melakukan 
labelling  data  secara  otomatis.  Data  yang  telah  di-scrapping 
dan di-labelling akan disimpan di excel dengan format .csv. 

negatif, 

positif. 

netral, 

Data Sekunder 

Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik 

Data  sekunder  merupakan  dataset  yang  dikumpulkan 
melalui  studi 
literatur.  Pada  data  sekunder,  data  yang 
dikumpulkan  adalah  data  sentimen  Twitter  multikelas 
berbahasa Indonesia yang memiliki index ratio > 1,5. Batasan 
ini berdasarkan penelitian yang dilakukan oleh [1], [6]. Dataset 
sekunder  yang  berhasil  dikumpulkan  adalah  sebanyak  enam 
dataset.  Tabel  II  memperlihatkan  deskripsi  singkat mengenai 
keenam dataset. 

C.  Tahapan Penelitian 

Gambar 3. Diagram alur tahapan penelitian. 

Tahap Preprocessing 
Secara  umum  dan  sekaligus  pada  penelitian  ini,  proses 
preprocessing  pada  twit  Twitter  meliputi  cleaning  data,  case 
folding,  tokenisasi,  normalisasi,  penghapusan  stopwords,  dan 
stemming.   

1. Data cleaning  

Pada  tahap  ini  dilakukan  penghapusan  atribut  dan 
elemen  yang  tidak  mendukung  pada  proses  klasifikasi 
sentimen.  Contoh  dari  atribut  dan  elemen  tersebut 
seperti mention (@), hashtag (#), url (“http”, “bit.ly”), 
nomor, kode ASCII, tanda baca (.,”!?...), double space, 
huruf  muncul 
tiga,  emoticon,  dan 
menggantikannya dengan spasi. 

lebih  dari 

2. Case folding 
  Pada tahap ini, semua huruf kapital pada dataset diubah 

menjadi huruf kecil (case folding). 

3. Tokenisasi 
  Pada tahap ini, dilakukan pemotongan kata berdasarkan 

tiap kata yang menyusun setiap twit pada dataset.  

4. Normalisasi 
  Pada tahap ini dilakukan konversi kata yang tidak sesuai 

dengan EYD atau biasa disebut slang word.  

5. Stopwords Removal 
  Pada tahap ini dilakukan penghapusan kata yang sering 
muncul,  namun  tidak  berpengaruh  besar  terhadap 
performa  model  klasifikasi  atau  biasa  disebut  kata 
stopwords.  

6. Stemming 
  Pada  tahap  ini  dilakukan konversi  kata  yang  ada  pada 

dataset kedalam bentuk baku berbasis kamus.  

Ekstraksi Fitur 
Ekstraksi fitur pada penelitian ini menggunakan algoritma 
TF-IDF  dengan  penyetelan  parameter  ngram  yaitu  unigram, 
bigram, dan trigram.  

Resampling Data 

 3 / 9 

 
 
 
 
 
 
 
 
Setelah  dilakukan  ekstraksi  fitur,  maka  berfokus  pada 
penelitian  ini,  akan  dilakukan  proses  resampling  untuk 
mengatasi  ketidakseimbangan  data.  Berikut  konfigurasi 
algoritma resampling yang digunakan. 

1. SMOTE 

Algoritma  SMOTE  diterapkan  dengan  menggunakan 
library  imblearn  yang  ada  di  Bahasa  pemrograman 
Python.  Nilai  K  yang  digunakan  adalah  5  yang 
menunjukkan  bahwa  data  sintesis  dibentuk  melalui  5 
tetangga terdekat dari data minor.  

2. Tomek Link 
  Algoritma  Tomek  Link  diterapkan dengan  konfigurasi 
parameter  yaitu  strategy_sampling  =  ‘majority’,  agar 
algoritma melakukan undersampling pada kelas mayor. 

3. SMOTE-Tomek Link 
  Algoritma ini diterapkan dengan menggunakan  library 
imblearn  yaitu  SMOTETomek.  Parameter  yang 
digunakan adalah parameter bawaan atau default. 

Klasifikasi Data 
Algoritma klasifikasi yang digunakan yaitu Multikelas SVM 
dengan  penyetelan  parameter  kernel  RBF  dan  kernel  linier, 
serta parameter gamma: 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 
1.0 melalui dua pendekatan, yaitu One vs All (OVA) dan One vs 
One  (OVO).  Algoritma  ini  diterapkan  dengan  bantuan  fungsi 
SVC  pada  package  SVM  library  sckit-learn  serta  package 
multiclass pada library sckit-learn. 

Pembangunan Model Klasifikasi 

Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik 

2. Split data menjadi data latih dan data uji dengan proporsi 

70% data latih dan 30% data uji. 

3. Bangun  model  dengan  menggunakan  pipeline  dan 
GridsearchCV  agar  memungkinkan  untuk  melakukan 
tuning  hyperparameter 
terhadap  model.  Proses 
pembangunan model dilakukan menggunakan data latih. 
4. Setelah  model  dengan  parameter  terbaik  didapat, 
lakukan proses pengujian model dengan menggunakan 
data uji. 

Evaluasi Model 
Pada tahap ini, dilakukan evaluasi dari  setiap model yang 
telah diuji dengan menggunakan nilai F1-Score dan geometric 
mean dengan pendekatan rata-rata makro [1], [4], [11].  

D.  Uji Statistik 
Pada  penelitian  ini,  kami  melakukan  uji  statistik  untuk 
mencapai  tujuan  penelitian.  Uji  yang  digunakan  adalah  uji 
nonparametrik,  mengingat  bahwa  jenis  data  yang  digunakan 
merupakan data nonstasioner serta tidak memungkinkan untuk 
memenuhi  persyaratan  uji  parametrik,  serta  uji  ini  lebih 
direkomendasikan  ketika  diterapkan  pada  kasus  machine 
learning  dibandingkan  dengan  uji  parametrik  berdasarkan 
penelitian  terdahulu  [13].  Uji  nonparametric  yang  digunakan 
pertama adalah uji Wilcoxon yang digunakan untuk melakukan 
perbandingan berpasangan antara dua perlakuan. Kedua adalah 
uji  friedman  untuk  melakukan  perbandingan  berpasangan 
antara  tiga  atau  lebih  perlakuan.  Ketiga  adalah  uji  Lanjutan 
yang digunakan apabila terdapat hasil yang Tolak H0 pada uji 
Friedman,  uji  ini  digunakan  untuk  melihat  perlakuan  mana 
yang  signifikan  mempengaruhi  variabel  kontrol.  Uji-uji  ini 
pernah digunakan pada penelitian sebelumnya [1], [6].  

V.  KERANGKA PIKIR 

Berikut  Gambar  5  memperlihatkan  kerangka  pikir  pada 

penelitian ini. 

Gambar 4. Alur pembangunan model. 

Pada  Gambar  4,  dijelaskan  mengenai  alur  pembangunan 
model.  Pembangunan  model  secara  keseluruhan  dilakukan 
menggunakan pipeline dan gridsearchCV. Pipeline digunakan 
agar proses coding yang dilakukan dapat menampung beberapa 
proses  sekaligus  dan  gridsearchCV  digunakan  agar  dapat 
melakukan tuning hyperparameter terhadap model. 

Berikut tahapan pembuatan model klasifikasi: 
1. Lakukan proses preprocessing pada dataset. 

Gambar 5. Kerangka pikir penelitian. 

Pada  Gambar  5,  dijelaskan  mengenai  kerangka  pikir 
penelitian  ini.  Pada  penelitian  ini,  dataset  yang  digunakan 
merupakan  dataset  sentimen  Twitter  multikelas  berbahasa 
Indonesia yang tidak seimbang. Metode yang diusulkan adalah 
menggunakan  algoritma  klasifikasi  Support  Vector  Machine 
(SVM). Indikator yang diobservasi adalah penggunaan kernel 

 4 / 9 

 
 
 
 
 
 
linier dan kernel RBF pada SVM, penggunaan teknik binerisasi 
One  vs  All  (OVA)  dan  One  vs  One  (OVO)  pada  SVM,  dan 
penggunaan  teknik  resampling  SMOTE,  Tomek  Link,  dan 
SMOTTomek  pada  kasus  data  tidak  seimbang.  Tujuan  dari 
penelitian 
indikator. 
ini  adalah  membandingkan  setiap 
Pengukuran  perbandingan 
indikator  dilakukan  dengan 
menggunakan  uji  nonparametrik  Wilcoxon  dan  Friedman 
terhadap nilai  evaluasi  performa  F1-Score  dan  G-Mean yang 
dihasilkan setiap indikator. 

VI. HASIL DAN PEMBAHASAN 

A.  Deskripsi Data 

Data  yang  digunakan  pada  penelitian  ini  sebagaimana 
disebutkan  pada  bab  metode  penelitian  yang  terdiri  dari  dua 
jenis data, data primer dan data sekunder, di mana seluruh data 
memiliki indeks rasio ketidakseimbangan > 1.5. 

Data Primer 

TABEL I 
DESKRIPSI DATA KAI YANG SUDAH BERSIH 

Data 

Jumlah Data Kelas 

Negatif  Netral  Positif 

Fitur 

Sampel 

IR 

KAI 

2224 

2906 

885 

102970 

6015 

3.28 

Data Sekunder 

TABEL II 
DESKRIPSI DATA SEKUNDER YANG SUDAH BERSIH 

Jumlah Data Kelas 

Data 

Fitur 

Sampel 

IR 

Negatif  Netral  Positif 

Random  

2887 

5327 

2592 

127912 

10806 

2.05 

Prabowo 

Covid 

Jokowi 

77 

245 

241 

163 

347 

443 

90 

584 

143 

4152 

330 

2.11 

20369 

1176 

2.38 

8478 

827 

3.09 

Tripadvisor 

3791 

1256 

7087 

279163 

12134 

5.64 

Polisi 

116 

18 

87 

3838 

221 

6.44 

B.  Preprocessing Data 

Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik 

@KAI121 Min aplikasi nya kenapa 
sih? Lemot banget? Mau masuk ke 
pembayaran langsung notif 
â€œserver sibuk bla bla blaâ€(cid:157) 
â€œKeputusan hari ini menentukan 
tiket ke depan minâ€(cid:157) 
#SahabatKAI   Minta tolong di 
follow up min aplikasi &amp; web 
nya ðŸ˜ 

min aplikasi nya sih lambat banget 
masuk pembayaran notif server 
sibuk bla bla bla keputusan 
menentukan tiket min sahabatkai 
follow up min aplikasi amp web 
nya 

Hasil  preprocessing  pada  Tabel  III  merupakan  hasil  setelah 
melewati beberapa tahapan seperti yang dijelaskan pada bab IV 
metode penelitian. 

C.  Evaluasi Model 

One vs All (OVA) 

TABEL IV 
NILAI F1-SCORE DAN G-MEAN TERBAIK DARI SVM OVA UNTUK SETIAP DATASETT DAN KERNEL 

Dataset 

Kernel  F1 Score OVA  G-Mean OVA 

0.563*** 

0.656*** 

Random 

Prabowo 

Covid 

Jokowi 

Tripadvisor 

Polisi 

KAI 

RBF 

Linear 

RBF 

Linear 

RBF 

Linear 

RBF 

Linear 

RBF 

0.577* 

0.629* 

0.624 

0.594* 

0.6* 

0.557** 

0.664* 

0.819*** 

Linear 

0.814*** 

RBF 

Linear 

RBF 

Linear 

0.527* 

0.542 

0.598* 

0.612* 

Ket: Hasil Original. 

 *Hasil SMOTE. 
 **Hasil Tomek Link. 
 ***Hasil SMOTTomek. 

0.674* 

0.697* 

0.696 

0.682* 

0.698* 

0.64** 

0.731* 

0.866*** 

0.854*** 

0.644*** 

0.635 

0.682* 

0.702* 

Sebelum proses analisis dilakukan, data teks harus melalui 
tahap  preprocessing  terlebih  dahulu  untuk  menghilangkan 
noise dan memperbaiki kualitas data. 

Berikut Tabel III yang menunjukkan beberapa contoh hasil 

teks twit sebelum dan sesudah preprocessing. 

TABEL III 
Contoh hasil akhir preprocessing 

Sebelum Preprocessing 

Setelah Preprocessing 

Masih pagi sarapannya udah geprek 
aja, @KAI121 
https://t.co/zp1imiI0e8 
2 hari kemudian belum cair cair, 
gini ya pelayanan kalo kita 
ngecancel tiket?? @KAI121 

pagi sarapannya geprek 

cair cair pelayanan ngecancel tiket 

Pada Tabel IV diatas diperlihatkan hasil dari nilai F1-Score 
dan  geometric  mean  dari  SVM  OVA  terbaik  untuk  setiap 
dataset  dan  kernel.  Pada  Tabel  terlihat  bahwa  nilai  F1-Score 
dan geometric mean yang dihasilkan melalui teknik resampling 
lebih  mendominasi  dibandingkan  dengan  tanpa  resampling, 
baik untuk  F1-Score maupun  geometric mean. Hal ini sesuai 
dengan asumsi awal dan hasil dari beberapa penelitian yang ada 
teknik  resampling  yang  digunakan  pada  model 
bahwa 
klasifikasi  data  yang  tidak  seimbang  dapat  meningkatkan 
performa dari model klasifikasi. 

One vs One (OVO) 

TABEL V 
NILAI F1-SCORE DAN G-MEAN TERBAIK DARI SVM OVO UNTUK SETIAP DATASETT DAN KERNEL 

 5 / 9 

 
 
 
 
 
 
 
Dataset 

Kernel  F1 Score OVO  G-Mean OVO 

Random 

Prabowo 

Covid 

Jokowi 

RBF 

Linear 

RBF 

Linear 

RBF 

Linear 

RBF 

0.537** 

0.633** 

0.576* 

0.605* 

0.624 

0.594* 

0.594* 

0.531*** 

0.673* 

0.683* 

0.696 

0.687* 

0.694* 

0.623* 

Linear 

0.644** 

0.715*** 

Tripadvisor 

Polisi 

KAI 

RBF 

Linear 

RBF 

0.813*** 

0.817* 

0.592* 

Linear 

0.542*** 

RBF 

Linear 

0.585* 

0.612* 

0.86*** 

0.857* 

0.653*** 

0.635*** 

0.67*** 

0.702*** 

Ket: Hasil Original. 
*Hasil SMOTE. 
** Hasil Tomek Link. 
*** Hasil SMOTTomek. 

Pada Tabel V diatas diperlihatkan hasil dari nilai F1-Score 
dan  geometric  mean  terbaik  dari  SVM  OVO  untuk  setiap 
dataset  dan  kernel.  Pada  Tabel  terlihat  bahwa  nilai  F1-Score 
dan  geometric  mean  yang  dihasilkan  sangat  beragam  untuk 
setiap  perlakuan.  Pada  Tabel  dapat  disimpulkan  bahwa  nilai 
F1-Score  dan  G-Mean  dengan  teknik  resampling  cenderung 
lebih  baik  dibandingkan  dengan  tanpa  resampling.  Terlihat 
bahwa  performa  yang  dihasilkan  dengan  teknik  resampling 
lebih  mendominasi  dibandingkan  dengan  tanpa  resampling, 
baik  pada  nilai  F1-Score  maupun  geometric  mean.  Hal  ini 
sesuai dengan asumsi awal dan hasil dari beberapa penelitian 
yang ada bahwa teknik resampling yang digunakan pada model 
klasifikasi  data  yang  tidak  seimbang  dapat  meningkatkan 
performa dari model klasifikasi. 

D. Uji Statistik 

Perbandingan teknik resampling 
teknik 
perbandingan 
Pada 

resampling, 

kami 
membandingkan empat  perlakuan resampling yang dilakukan 
pada penelitian ini, yaitu tanpa resampling (Original), SMOTE, 
Tomek Link, dan SMOTTomek. Uji statistik yang digunakan 
adalah  uji  statistik  nonparametrik  Friedman,  di  mana  uji  ini 
memungkinkan kita untuk melihat apakah ada perbedaan yang 
signifikan  antar  perlakuan  yang  diterapkan  serta  melihat 
ranking  perlakuan  yang  menunjukkan  perlakuan  mana  yang 
memberikan  hasil  terbaik.  Tingkat  signifikansi  alpha  yang 
digunakan adalah 5%. Berikut hipotesis yang terbentuk ketika 
melakukan uji Friedman. 
H0: M1 = M2 = … = Mk atau setiap teknik resampling memiliki 
median  yang  sama  atau  tidak  terdapat  perbedaan  yang 
signifikanantar teknik resampling. 

Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik 

H1:  Minimal  satu  Mi…k  memiliki  median  yang  berbeda  atau 
terdapat perbedaan yang signifikan antar perlakuan. 
Hasil keputusan akan Tolak H0 apabila p-value yang dihasilkan 
≤ 0.05, dan sebaliknya akan Gagal Tolak H0 apabila > 0.05. 

TABEL VI 
RATA-RATA RANKING (UJI FRIEDMAN) TERHADAP TEKNIK RESMPLING  

Algorit
ma 

SVM 
OVA 

SVM 
OVO 

Jenis 
Evalua
si 

F1-
Score 
G-
Mean 
F1-
Score 
G-
Mean 

Teknik Resampling 

Origin
al 
1.8571
4 
1.7857
1 
1.6071
4 
1.5714
3 

SMOT
E 
3.0714
3 
3.2142
9 
3.1785
7 
3.2857
1 

Tomek 
Link 

SMOTTom
ek 

2.35714 

2.71429 

2.14286 

2.85714 

2.39286 

2.82143 

2.17857 

2.96429 

Pada Tabel VI, dipaparkan nilai rata-rata ranking hasil uji 
friedman  dari  teknik  resampling  berdasarkan  setiap  evaluasi 
dari  tiap  algoritma.  Pada  Tabel  terlihat  bahwa  nilai  F1-Score 
dan G-Mean dari kedua algoritma didominasi oleh model data 
SMOTE, diikuti oleh SMOTTomek, Tomek Link, dan terakhir 
Original. 

Berdasarkan  hasil  uji  Friedman  pada  Tabel  VI  dihasilkan 
hasil yang tolak H0 pada nilai  G-Mean pada SVM OVA dan 
nilai  F1-Score  dan  G-Mean  pada  SVM  OVO.  Hal  ini 
menunjukkan  bahwa  terdapat  perbedaan  nilai  G-Mean  yang 
signifikan  pada  SVM  OVA  ketika  diterapkan 
teknik 
resampling, dan nilai F1-Score dan G-Mean pada SVM OVO. 
Untuk  melihat  teknik  resampling  mana  yang  memberikan 
dampak yang signifikan, maka dilakukan uji lanjutan (uji dunn) 
pada  hasil yang tolak H0.  Berikut  Tabel  VII memperlihatkan 
hasil dari uji lanjutan. 

TABEL VII 
HASIL UJI LANJUTAN (UJI DUNN) DARI UJI FRIEDMAN 

P-Value 

Algoritma 

Jenis 
Evaluasi 

Original 
vs 
SMOTE 

Original vs 
SMOTTomek 

Original 
vs 
Tomek 
Link 

SVM OVA  G-Mean 

0.01024 

0.08432 

1 

SVM OVO 

F1-Score 

0.00384 

0.03848 

0.32204 

G-Mean 

0.00133 

0.01293 

0.6402 

Berdasakan hasil pada Tabel VII, dapat disimpulkan bahwa 
teknik  resampling  yang  memberikan  hasil  yang  signifikan 
terhadap  nilai  G-Mean  pada  SVM  OVA  adalah  SMOTE, 
sedangkan  pada  SVM  OVO, 
teknik  resampling  yang 
memberikan dampak yang signifikan terhadap nilai  F1-Score 
dan G-Mean adalah SMOTE dan SMOTTomek. 

Dari hasil ini dapat disimpulkan bahwa teknik  resampling 
terbaik yang diterapkan pada klasifikasi data teks Twitter tidak 
seimbang adalah SMOTE, diikuti oleh SMOTTomek, Tomek 
Link.  Teknik  resampling  yang  memberikan  dampak  yang 
signifikan ketika diterapkan pada SVM OVA adalah SMOTE, 
sedangkan pada SVM OVO adalah SMOTE dan SMOTTomek. 

 6 / 9 

 
 
 
 
 
 
 
 
 
Perbandingan kernel  
Pada bagian ini, kami melakukan perbandingan antar kernel 
yang digunakan pada algoritma SVM yaitu antara kernel RBF 
dan kernel linier. Perbandingan dilakukan berdasarkan hasil uji 
statistik  Wilcoxon  di  mana  uji  ini  digunakan  ketika  ingin 
menguji  apakah  ada  perbedaan  yang  signifikan  antar  dua 
perlakuan.  Perlakuan  disini  yaitu  kernel  RBF  dan  linier. 
Tingkat  signifikansi  yang  digunakan  adalah  5%.  Berikut 
hipotesis dari uji Wilcoxon yang terbentuk: 
H0:  M1  =  M2  atau  tidak  terdapat  perbedaan  median  yang 
signifikan antara kernel RBF dan linier.  
H1:  M1 ≠  M2 atau  terdapat perbedaan  median  yang  signifikan 
antara kernel RBF dan linier. 
Hasil akan tolak H0 apabila p-value yang dihasilkan ≤ 0.05, dan 
sebaliknya akan gagal tolak apabila > 0.05. 

TABEL VIII 
JUMLAH RANKING (UJI WILCOXON) DARI KERNEL RBF DAN LINIER 

Algoritma 

Resampling 

F1-Score 

G-Mean 

RBF  Linier  RBF  Linier 

G-
Mean 

SVM OVA 

SVM OVO 

Original 

SMOTE 

Tomek Link 

3 

9 

3 

25 

19 

25 

1 

6 

1 

27 

22 

27 

SMOTTomek 

14.5 

13.5 

10.5 

17.5 

Original 

SMOTE 

Tomek Link 

SMOTTomek 

0 

9 

0 

8 

28 

12 

28 

20 

0 

9 

0 

8 

28 

19 

28 

29 

Pada Tabel VIII diatas dipaparkan jumlah ranking hasil uji 
Wilcoxon  dari  kernel  linier  dan  kernel  RBF  untuk  setiap 
algoritma  klasifikasi  bersama  teknik  resampling.  Pada  Tabel 
VIII terlihat bahwa jumlah ranking F1-Score dan G-Mean pada 
kernel linier lebih mendominasi dibandingkan jumlah ranking 
pada  kernel  RBF,  kecuali  pada  kombinasi  SVM  OVA  + 
SMOTTomek.  Dari  hal  ini  dapat  disimpulkan  bahwa  kernel 
linier  memberikan  performa  yang  lebih  baik  dibandingkan 
kernel RBF ketika digunakan pada algoritma multikelas SVM. 
Dari  hasil  uji  ini,  kami  menemukan  bahwa  terdapat 
perbedaan  performa  yang  signifikan  antara  kernel  linier  dan 
kernel  RBF  ketika  diterapkan  pada  model  data  Original  dan 
model data Tomek Link. Sedangkan, pada model data SMOTE 
dan  SMOTTomek,  tidak  terdapat  perbedaan  nilai  yang 
signifikan antara kernel RBF dan kernel linier. 

Perbandingan teknik binerisasi 
Pada  perbandingan  teknik  binerisasi,  akan  dilakukan 
perbandingan  antara  teknik  binerisasi  One  vs  All  (OVA)  dan 
One vs One (OVO). Perbandingan dilakukan berdasarkan nilai 
uji statistik Wilcoxon, di mana uji statistik Wilcoxon digunakan 
ketika  ingin  membandingkan  antar  dua  perlakuan.  Tingkat 
signifikansi yang digunakan adalah 5%. Berikut hipotesis yang 
terbentuk dari uji Wilcoxon: 
H0:  M1  =  M2  atau  tidak  terdapat  perbedaan  median  yang 
signifikan antara OVA dan OVO.  

Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik 

H1:  M1 ≠  M2 atau  terdapat perbedaan  median  yang  signifikan 
antara OVA dan OVO. 
Hasil akan tolak H0 apabila p-value yang dihasilkan ≤ 0.05, dan 
sebaliknya akan gagal tolak apabila > 0.05. 

TABEL IX 
JUMLAH RANKING (UJI WILCOXON) DARI OVA DAN OVO 

Jenis 
Evalua
si 

Metode 
Resam
pling 

Original 

Jumlah 
Rank 
OVA 

21 

Jumlah 
Rank 
OVO 

p-value (< 
0.05) 

Hasil 

0 

0.028 

H0 ditolak 

F1-
Score 

SMOTE 

69.5 

21.5 

0.093 

Gagal 
Tolak H0 

Tomek 
Link 
SMOTT
omek 

Original 

SMOTE 

Tomek 
Link 
SMOTT
omek 

20 

1 

0.046 

H0 ditolak 

69.5 

35.5 

0.285 

Gagal 
Tolak H0 

15 

70 

19.5 

66.5 

0 

21 

1.5 

38.5 

0.043 

H0 ditolak 

0.086 

0.058 

0.379 

Gagal 
Tolak H0 
Gagal 
Tolak H0 
Gagal 
Tolak H0 

Pada Tabel IX dipaparkan mengenai hasil uji Wilcoxon dari 
hasil evaluasi OVA dan OVO. Tabel IX memperlihatkan hasil 
uji  Wilcoxon  berdasarkan  nilai  F1-Score  dan  nilai  G-Mean 
untuk  masing-masing  algoritma  SVM  OVA  dan  SVM  OVO. 
Berdasarkan jumlah rank OVA dan OVO pada Tabel IX, dapat 
disimpulkan  bahwa  jumlah  rank  OVA  lebih  mendominasi 
dibandingkan jumlah rank OVO, hal ini menunjukkan bahwa 
teknik binerisasi OVA memiliki hasil yang lebih baik di banyak 
kasus  dataset  dan  berbagai  teknik  resampling  dibandingkan 
teknik binerisasi OVO. 

Berdasarkan hasil p-value yang dihasilkan pada Tabel  IX, 
dapat  disimpulkan  bahwa  antara  teknik  binerisasi  OVA  dan 
OVO,  tidak  terdapat  perbedaan  nilai  yang  signifikan, kecuali 
pada nilai F1-Score pada model data original dan model data 
Tomek link, dan pada nilai G-Mean pada model data original. 

E. Studi Kasus KAI 

TABEL X 
EVALUASI MODEL MENGGUNAKAN DATA KAI 

Evalu
asi 

Algorit
ma 

Ker
nel 

F1- score 

Origi
nal 

SMO
TE 

Tomek 
Link 

SMOTTo
mek 

F1-
Score 

G-
Mean 

SVM 
OVA 

SVM 
OVO 

SVM 
OVA 

SVM 
OVO 

RBF 

0.548 

0.598 

0.563 

Line
ar 

0.568 

0.612 

0.592 

RBF 

0.545 

0.585 

0.552 

Line
ar 

0.568 

0.612 

0.592 

RBF 

0.649 

0.682 

0.658 

Line
ar 

0.66 

0.702 

0.678 

RBF 

0.649 

0.67 

0.654 

Line
ar 

0.66 

0.702 

0.678 

0.59 

0.606 

0.566 

0.605 

0.674 

0.697 

0.659 

0.695 

 7 / 9 

 
 
 
Berdasarkan hasil pada Tabel X, dapat disimpulkan bahwa 
model  terbaik  untuk  mengklasifikasikan  data  sentimen  KAI 
yang  tidak  seimbang  adalah  dengan  menerapkan  teknik 
resampling SMOTE pada data, serta penggunaan kernel linier 
pada  algoritma,  baik  pada  SVM  OVA  maupun  SVM  OVO. 
Nilai performa tertinggi yang dihasilkan melalui model adalah 
61.2% untuk F1-Score dan 70.2% untuk G-Mean.  

Untuk  melihat  apakah  algoritma  SMOTE  sudah  cukup 
mengatasi permasalahan ketidakseimbangan pada dataset KAI, 
berikut Gambar 13 dan Gambar 14 ditampilkan nilai sensitivity 
dan specificity yang dihasilkan oleh algoritma SMOTE. 

Sensitivity Data KAI

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

Original Negatif

SMOTE Negatif

Original Netral

SMOTE Netral

Original Positif

SMOTE Positif

RBF

Linear

RBF

Linear

SVM OVA

SVM OVO

Gambar 6. Sensitivity dari hasil data SMOTE dan Original 

Pada  Gambar  6,  terlihat  bahwa  nilai  sensitivity  yang 
dihasilkan  oleh  teknik  resampling  SMOTE  (garis  titik-titik) 
berhasil  memberikan  keseimbangan  performa  disetiap  kelas. 
Hal ini terlihat dari kelas yang mengalami dampak overfitting, 
yaitu kelas Positif (minor) yang mengalami peningkatan nilai 
sensitivity ketika dilakukan proses SMOTE, artinya pada kelas 
jumlah  data  yang 
Positif 
diklasifikasikan  dengan  benar.  Sedangkan,  pada  kelas  netral 
(mayor), mengalami penurunan sensitivity. Hal ini menandakan 
bahwa  indikasi  overfitting  pada  model  berkurang  ketika 
menggunakan 
berhasil 
mengklasifikasikan  data  latih  secara  merata,  tidak  hanya 
mengklasifikasikan pada kelas mayor saja. 

terjadi  peningkatan 

SMOTE, 

(minor) 

karena 

model 

Specificity Data KAI

1.2

1

0.8

0.6

0.4

0.2

0

RBF

Linear

RBF

Linear

SVM OVA

SVM OVO

Original Negatif

SMOTE Negatif

Original Netral

SMOTE Netral

Original Positif

SMOTE Positif

Gambar 7. Specificity dari hasil data SMOTE dan Original 

Pada  Gambar  14  ditampilkan  nilai  Specificity  yang 
dihasilkan  model  terhadap  data  KAI.  Dari  Gambar  terlihat 
bahwa nilai specificity untuk kelas Netral (mayor) mengalami 
peningkatan,  artinya  terdapat  kenaikan  jumlah  data  yang 

Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik 

diprediksi dengan benar pada kelas lain (Positif dan Negatif). 
Dari hal ini dapat disimpulkan pula bahwa model mengalami 
peningkatan  keseimbangan  dalam  memprediksi  kelas  pada 
dataset.  

VII. 

PENUTUP 

Kesimpulan: 
1. Berdasarkan hasil dari perbandingan teknik resampling, 
teknik  resampling  terbaik  ketika  diterapkan  pada 
klasifikasi teks Twitter multikelas berbahasa Indonesia 
yang  tidak  seimbang  adalah  SMOTE,  diikuti  oleh 
SMOTTomek, dan Tomek Link. Namun, dari beberapa 
teknik  resampling  tersebut,  teknik  resampling  yang 
memberikan nilai yang signifikan terhadap SVM OVA 
adalah SMOTE, sedangkan terhadap SVM OVO adalah 
SMOTE  dan  SMOTTomek,  Ketiga  teknik  resampling 
terbukti  dapat  menaikkan  performa  model  di  banyak 
kasus data teks tidak seimbang. 

2. Berdasarkan  hasil  dari  perbandingan  kernel  antara 
kernel  RBF  dan  linier,  kernel  terbaik  yang  digunakan 
pada  model  klasifikasi  SVM  ketika  melakukan 
klasifikasi teks Twitter multikelas berbahasa Indonesia 
yang  tidak  seimbang  adalah  kernel  linier.  Di  banyak 
kasus  data,  performa  model  yang  dihasilkan 
menggunakan  kernel  linier  memiliki  nilai  yang  lebih 
baik  dibandingkan  ketika  menggunakan  kernel  RBF. 
Hal ini terlihat dari perbandingan jumlah ranking yang 
didominasi oleh kernel linier. Namun, kami menemukan 
bahwa  antara  kernel  linier  dan  RBF,  tidak  terdapat 
perbedaan  nilai  yang  signifikan,  kecuali  pada  model 
data  Original  dan  model  data  Tomek  Link.  Pada 
SMOTE  dan  SMOTTomek,  tidak  terdapat  perbedaan 
nilai yang signifikan antara kedua kernel ini. 

3. Berdasarkan  hasil  dari  perbandingan  teknik  binerisasi, 
teknik binerisasi OVA memberikan performa yang lebih 
baik  dibandingkan  teknik  binerisasi  OVO  di  berbagai 
kasus data teks Twitter multikelas tidak seimbang ketika 
diterapkan  pada  algoritma  klasifikasi  SVM.  Hal  ini 
terlihat dari perbandingan jumlah rank yang dihasilkan 
oleh 
teknik  binerisasi  OVA  yang  mendominasi 
dibandingkan  dengan  teknik  binerisasi  OVO.  Namun, 
antara kedua teknik binerisasi, tidak terdapat perbedaan 
nilai yang signifikan, kecuali pada nilai F1-Score pada 
model  data  Original  dan  Tomek  Link,  serta  nilai  G-
Mean pada model data Original. 

4. Model terbaik pada studi kasus klasifikasi data sentimen 
multikelas  KAI  yang  tidak  seimbang  adalah  dengan 
menerapkan SMOTE pada data, baik pada model SVM 
OVA maupun SVM OVO. Nilai performa terbaik yang 
dihasilkan  adalah  61.2%  untuk  F1-Score  dan  70.2% 
untuk geometric mean. Model ini meningkatkan sekitar 
4%-5% dari model data Original (tanpa resampling). 

Saran: 
1.  Sejak  pada  penelitian 

teknik 
resampling untuk mengatasi ketidakseimbangan data, 
selanjutnya  untuk 
disarankan  untuk  penelitian 

ini  menggunakan 

 8 / 9 

 
 
 
 
 
Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik 

classification  using  SVM:  A  comparative  study,”  Decis.  Support 
Syst., 
doi: 
no. 
10.1016/j.dss.2009.07.011. 

191–201, 

2009, 

vol. 

pp. 

48, 

1, 

[10] 

[11] 

[12] 

[13] 

R.  Moraes,  J.  F.  Valiati,  and  W.  P.  Gavião  Neto,  “Unbalanced 
sentiment  classification:  an  assessment  of  ANN  in  the  context  of 
sampling  the  majority  class,”  PeerJ,  vol.  6,  pp.  1–15,  2018,  doi: 
10.7287/peerj.preprints.26618. 

B. Krawczyk, B. T. McInnes, and A. Cano, “Sentiment classification 
from  multi-class  imbalanced  twitter  data  using  binarization,”  Lect. 
Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. 
Notes  Bioinformatics),  vol.  10334  LNCS,  pp.  26–37,  2017,  doi: 
10.1007/978-3-319-59650-1_3. 

H.  Sain  and  S.  W.  Purnami,  “Combine  Sampling  Support  Vector 
Machine for Imbalanced Data Classification,” Procedia Comput. Sci., 
vol. 72, pp. 59–66, 2015, doi: 10.1016/j.procs.2015.12.105. 

J. Demšar, “Statistical comparisons of classifiers over multiple data 
sets,” J. Mach. Learn. Res., vol. 7, pp. 1–30, 2006. 

penggunaan 

melakukan 
ketidakseimbangan  data 
ensambel  (algorithmic 
sensitive learning. 

teknik 

penanganan 
lainnya,  seperti  metode 
level)  dan  metode  cost-

2.  Pada penelitian ini, penggunaan teknik binerisasi yang 
digunakan  adalah  OVA  dan  OVO,  di  mana  kedua 
teknik  ini  merupakan  dua  dari  tiga  teknik  binerisasi 
yang  ada  saat 
ini.  Oleh  karenanya,  penelitian 
selanjutnya  disarankan  untuk  menggunakan  teknik 
binerisasi  directed  acyclic  graph  SVM  (DAGSVM) 
ketika  memetakan  kelas  multikelas  ke  dalam bentuk 
biner. 

3.  Pada  penelitian  ini,  kami  berfokus  pada  kajian 
membandingkan metode yang diterapkan pada SVM 
ketika  menghadapi  kasus  dataset  Twitter  multikelas 
berbahasa  Indonesia  yang  tidak  seimbang.  Pada 
penelitian  di  masa  depan,  disarankan  untuk 
melakukan studi untuk melihat apakah model terbaik 
yang 
mengatasi 
ketidakseimbangan dan mangatasi indikasi overfitting 
yang diakibatkan data tidak seimbang. 

didapat 

sudah 

dapat 

terdiri  dari 

4.  Pada  penelitian  ini,  jumlah  kelas  pada  dataset  yang 
digunakan 
tiga  kelas.  penelitian 
selanjutnya  disarankan  untuk  menggunakan  dataset 
dengan  jumlah  kelas  lebih  dari  tiga,  untuk  melihat 
bagaimana perbandingan hasil yang dihasilkan untuk 
setiap model. 

DAFTAR PUSTAKA 

M. Lango, “Tackling the Problem of Class Imbalance in Multi-class 
Sentiment Classification: An Experimental Study,” Found. Comput. 
Decis.  Sci.,  vol.  44,  no.  2,  pp.  151–178,  2019,  doi:  10.2478/fcds-
2019-0009. 

A. Sharma and U. Ghose, “Sentimental Analysis of Twitter Data with 
respect to General Elections in India,”  Procedia Comput. Sci., vol. 
173, no. 2019, pp. 325–334, 2020, doi: 10.1016/j.procs.2020.06.038. 

A. Mountassir, H. Benbrahim, and I. Berrada, “An empirical study to 
address 
in  sentiment 
classification,” Conf. Proc. - IEEE Int. Conf. Syst. Man Cybern., pp. 
3298–3303, 2012, doi: 10.1109/ICSMC.2012.6378300. 

the  problem  of  unbalanced  data  sets 

S. Li, G. Zhou, Z. Wang, S. Y. M. Lee, and R. Wang, “Imbalanced 
sentiment  classification,”  Int.  Conf.  Inf.  Knowl.  Manag.  Proc.,  no. 
October, pp. 2469–2472, 2011, doi: 10.1145/2063576.2063994. 

A.  Estabrooks,  T.  Jo,  and  N.  Japkowicz,  “A  multiple  resampling 
method for learning from imbalanced data sets,” Comput. Intell., vol. 
20,  no.  1,  pp.  18–36,  2004,  doi:  10.1111/j.0824-7935.2004.t01-1-
00228.x. 

A. Fernández, V. López, M. Galar, M. J. Del Jesus, and F. Herrera, 
“Analysing the classification of imbalanced data-sets with multiple 
classes:  Binarization 
approaches,” 
Knowledge-Based  Syst.,  vol.  42,  pp.  97–110,  2013,  doi: 
10.1016/j.knosys.2013.01.018. 

techniques 

ad-hoc 

and 

A. Indrawati, H. Subagyo, A. Sihombing, W. Wagiyah, and S. Afandi, 
“Analyzing the Impact of Resampling Method for Imbalanced Data 
Text  in  Indonesian  Scientific  Articles  Categorization,”  Baca  J. 
Dokumentasi  Dan  Inf.,  vol.  41,  no.  2,  p.  133,  2020,  doi: 
10.14203/j.baca.v41i2.702. 

W.  Wijanarto  and  S.  P.  Brilianti,  “Peningkatan  Performa  Analisis 
Sentimen  Dengan  Resampling  dan  Hyperparameter  pada  Ulasan 
Aplikasi BNI Mobile,” J. Eksplora Inform., vol. 9, no. 2, pp. 140–
153, 2020, doi: 10.30864/eksplora.v9i2.333. 

[1] 

[2] 

[3] 

[4] 

[5] 

[6] 

[7] 

[8] 

[9] 

A.  Sun, E.  P.  Lim, and  Y.  Liu, “On strategies  for imbalanced text 

 9 / 9 

 
 
 
"
221709860,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pengembangan Sistem Penarikan Sampel Berbasis 
Web pada Survei Ubinan Komoditas Padi 

Muhammad Qadri (221709860, 4SI1) 
Dosen Pembimbing: Yunarso Anang, Ph.D. 

Ringkasan—  Penyediaan  data  produksi  beras  merupakan 
tanggung  jawab  Badan  Pusat  Statistik  (BPS)  sebagai  lembaga 
statistik  negara.  Untuk  menghasilkan  data  tersebut,  BPS 
memerlukan  data  produktivitas  padi  yang  diperoleh  melalui 
survei ubinan. Desain sampling survei ubinan padi menggunakan 
hasil  amatan  Kerangka  Sampel  Area  (KSA)  berupa  subsegmen 
wilayah.  Penarikan  sampel  subsegmen  pada  sistem  berjalan 
masih  bersifat  manual  sehingga  dinilai  tidak  efisien.  Selain  itu, 
proses  pengajuan  sampel  tambahan  dari  daerah  juga  masih 
dilakukan  melalui  email  sehingga  kurang  dapat  dipantau 
tersebut, 
perkembangannya.  Berdasarkan 
diperlukan 
sistem  berbasis  web  yang  mampu 
sebuah 
meningkatkan  efisiensi  pekerjaan  yang  terkait  dengan  sampel 
pada  survei  ubinan  padi.  Metode  pengembangan  sistem  yang 
digunakan adalah System Development Life Cycle model Iterative 
Waterfall  dengan  alasan  karena  kebutuhan  sistem  sudah  cukup 
spesifik  di 
sistem 
menggunakan  bahasa  pemrograman  PHP  dan  JavaScript  serta 
PostgreSQL sebagai sistem manajemen basis data. Pengujian dan 
evaluasi  sistem  menggunakan  teknik  Black-Box  dan  kuesioner 
System Usability Scale dengan hasil bahwa sistem berfungsi sesuai 
harapan dan dapat diterima pengguna. 

awal  pengembangan. 

permasalahan 

Implementasi 

Kata Kunci— Penarikan Sampel, Survei Ubinan, Efisiensi 

I.  LATAR BELAKANG 

Badan  Pusat  Statistik  (BPS)  memiliki  peran  yang  sangat 
penting  dalam  menghasilkan  data  pangan.  Data  pangan 
terutama  data  produksi  beras  yang  akurat  menjadi  hal  yang 
penting  untuk  menentukan  arah  kebijakan  pemerintah  [1]. 
Salah  satu  data  yang  dibutuhkan  dalam  menghasilkan  data 
produksi beras adalah hasil padi per hektare atau produktivitas 
padi. 

Survei ubinan merupakan kegiatan yang dilakukan oleh BPS 
secara  rutin  setiap  subround  guna  mengumpulkan  data 
produktivitas  tanaman  pangan  terutama  komoditas  padi  [2]. 
Survei  ubinan  komoditas  padi  dilakukan  dengan  memanen 
sebagian area sawah bersamaan dengan masa panen oleh petani. 
Sejak tahun 2018, survei ubinan padi tidak lagi menggunakan 
pendekatan rumah tangga melainkan menggunakan pendekatan 
wilayah  yang  diperoleh  dari  hasil  kegiatan  survei  Kerangka 
Sampel  Area  (KSA)  [3].  Dengan  menggunakan  pendekatan 
KSA,  survei  ubinan  padi  tidak  lagi  memerlukan  kerangka 
sampel rumah tangga usaha tanaman padi. Survei ubinan padi 
berbasis KSA diharapkan dapat menjadikan data produksi padi 
lebih akurat dan tepat waktu. 

Sebelum  pelaksanaan  suatu  survei,  pada  umumnya  BPS 
perlu  melakukan  penarikan  sampel.  Pada  survei  ubinan  padi 
berbasis  KSA 
(selanjutnya  disebut  SUB-KSA),  unit 
pengambilan  sampel  yang  utama  adalah  subsegmen  yang 
merupakan bagian dari segmen wilayah. Ilustrasi segmen dan 

subsegmen ditunjukkan oleh Gambar 1. Metode pengambilan 
sampel pada SUB-KSA adalah stratified multi-stage sampling 
dengan  tiga 
tahap,  yaitu  pemilihan  sampel  subsegmen, 
pemilihan  sampel  petak  sawah,  dan  pemilihan  plot  ubinan. 
Penelitian  ini  difokuskan  pada  pemilihan  sampel  subsegmen 
wilayah yang dilakukan dengan metode systematic sampling. 

Gambar 1. Ilustrasi Segmen dan Subsegmen 
Sumber: Diambil dari [3] 

sampel 

dilakukan 

subsegmen 

Pengambilan 

oleh 
Subdirektorat Pengembangan Kerangka Sampel (Subdit PKS) 
BPS RI dengan bantuan aplikasi R Studio dan Microsoft Visual 
FoxPro. Aplikasi R Studio digunakan oleh subdit PKS untuk 
melakukan  penyusunan  kerangka  sampel,  sedangkan  aplikasi 
Microsoft  Visual  FoxPro  digunakan  untuk  melakukan 
penarikan sampel dan membuat dokumen daftar sampel terpilih. 
Kegiatan  penarikan  sampel  dan  pemilihan  sampel  SUB-
KSA  dilakukan  setiap  bulan  dengan  keharusan  mengubah 
script program yang telah dibuat pada penarikan sampel bulan 
sebelumnya.  Keharusan  mengubah  script  program  setiap 
bulannya dinilai oleh subdit PKS sendiri sebagai proses bisnis 
yang tidak efisien. 

Setelah  penarikan  sampel  dilakukan,  subdit  PKS  akan 
membuat  dokumen  daftar  subsegmen  terpilih  untuk  seluruh 
kabupaten/kota  di  Indonesia.  Dokumen  tersebut  kemudian 
dikirimkan  ke  seluruh  BPS  kabupaten/kota  melalui  email. 
Pengiriman  dokumen  daftar  sampel  yang  dilakukan  secara 
manual ini dinilai rentan terhadap kesalahan manusia (human 
error). 

Sampel subsegmen tidak selalu dapat dilakukan pencacahan. 
Tidak  jarang  petugas  pencacahan  mendapatkan  sampel  yang 
terlewat  panen  sehingga  dapat  membuat  target  sampel  tidak 
terpenuhi.  Agar 
terpenuhi,  BPS 
target  sampel  dapat 
kabupaten/kota dapat melakukan pengajuan sampel tambahan. 
Pengajuan  sampel  tambahan  dilakukan  dengan  mengirim 
informasi  sampel  tambahan  yang  diusulkan  beserta  alasan 
penambahan  sampel.  Informasi  sampel  tambahan  tersebut 
kemudian  dikirim  ke  subdit  PKS  melalui  email  bersamaan 

 1 / 8 

 
 
 
 
 
 
dengan  surat  pengajuan  sampel  yang  telah  ditandatangani 
kepala BPS provinsi. Proses pengajuan sampel tambahan yang 
dilakukan secara manual melalui email juga dinilai tidak efisien. 
Selain  itu,  BPS  kabupaten/kota  yang  melakukan  pengajuan 
sampel 
tidak  dapat  memantau  status 
perkembangannya apakah telah diproses atau belum. 

tambahan 

juga 

Dari  berbagai  kelemahan  dan  kekurangan  proses  bisnis 
maupun aplikasi dalam kegiatan penarikan sampel SUB-KSA, 
subdit PKS merasa perlu dikembangkannya suatu sistem yang 
mampu membuat proses bisnis tersebut menjadi lebih efisien. 
Sistem  tersebut  tidak  terbatas  pada  proses  penarikan  sampel, 
tetapi juga mencakup penyusunan kerangka sampel, pengajuan 
sampel tambahan, dan hal-hal lainnya yang berkaitan dengan 
sampel. Sistem yang dibangun diharapkan dapat meningkatkan 
efisiensi  dan  meminimalkan  kesalahan  dari  setiap  pekerjaan 
yang berhubungan dengan sampel pada SUB-KSA. 

II.  TUJUAN PENELITIAN 

Secara umum, tujuan penelitian ini dilakukan adalah untuk 
mengembangkan  sistem  berbasis  web  yang  dapat  meningkat-
kan  efisiensi  dari  setiap pekerjaan  terkait  sampel  pada  survei 
ubinan  komoditas  padi.  Sistem  tersebut  diharapkan  dapat 
digunakan  oleh  BPS  terutama  subdit  PKS  dalam  kegiatan 
penarikan sampel SUB-KSA. 

Adapun tujuan khusus yang ingin dicapai dalam penelitian 

ini adalah sebagai berikut: 
1)  Mengembangkan sistem informasi berbasis web yang dapat 
membantu subdit PKS dalam melakukan penarikan sampel 
subsegmen pada SUB-KSA. 

2)  Mengembangkan sistem yang dapat membantu subdit PKS 
dalam pengiriman dokumen daftar sampel secara otomatis 
sehingga dapat meminimalkan human error. 

3)  Mengembangkan  sistem  yang  dapat  memfasilitasi  BPS 
provinsi  dan  kabupaten/kota  dalam  mengajukan  sampel 
tambahan sehingga dapat dipantau perkembangannya. 

III. PENELITIAN TERKAIT 
Terdapat beberapa penelitian terdahulu yang membahas ten-
tang sistem yang dapat mengautomasi kegiatan penarikan sam-
pel. Beberapa penelitian yang terkait dengan penelitian ini ada-
lah sebagai berikut. 
1)  Surveykit:  Modul  Penarikan  Sampel  Tahap  Akhir  pada 
CAPI-STIS  oleh  Gede  Ananda  Nartapradnyana  dari 
Sekolah Tinggi Ilmu Statistik tahun 2017. 

2)  Pembangunan  Aplikasi  Penarikan  Sampel 

dalam 
Mengimplementasikan  Desain  Sampling  Survei  Rumah 
Tangga  Tanaman  Padi  2014  (SPD  2014)  oleh  Shintia 
Hafifah dari Sekolah Tinggi Ilmu Statistik tahun 2016. 
Penelitian pertama membahas tentang pengembangan modul 
penarikan sampel pada sebuah aplikasi survei berbasis android 
bernama  Computer  Assisted  Personal  Interviewing  (CAPI). 
Analisis permasalahan yang ada pada penelitian tersebut mirip 
dengan permasalahan yang akan diangkat pada penelitian ini. 
Permasalahan  pada  penelitian  tersebut  terletak  pada  proses 
penarikan  sampel  pada  aplikasi  CAPI  yang  selama  ini  masih 
bersifat manual sehingga tidak efisien dan rawan terhadap hu-
man error [4]. Hasil dari penelitian tersebut berupa subsistem 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

dari aplikasi CAPI yang dapat memudahkan petugas pencaca-
han dalam melakukan penarikan sampel pada berbagai survei 
yang dilakukan oleh BPS. 

Penelitian selanjutnya juga membahas tentang sistem yang 
mengautomasi  penarikan  sampel.  Berbeda  dengan  penelitian 
yang  pertama,  sistem  penarikan  sampel  pada  penelitian  ini 
lebih spesifik pada salah satu survei BPS, yaitu Survei Rumah 
Tangga  Tanaman  Padi  (SPD)  2014.  Permasalahan  yang  ada 
pada penelitian tersebut juga berkaitan dengan kegiatan penari-
kan  sampel  yang  selama  ini  menggunakan  aplikasi  berbasis 
command  line  yang  hanya  dapat  digunakan  oleh  orang  yang 
memahami  cara  penggunannya  [5].  Hasil  penelitian  tersebut 
berupa aplikasi penarikan sampel SPD 2014 berbasis desktop. 

IV. METODE PENELITIAN  

A.  Metode Pengumpulan Data 

Pengumpulan  data  pada  penelitian  ini  menggunakan  tiga 
metode,  yaitu  studi  pustaka,  wawancara,  dan  survei.  Metode 
studi pustaka dilakukan dengan membaca dokumen-dokumen 
milik BPS tentang SUB-KSA dan penelitian-penelitian ilmiah 
terkait.  Sementara  itu,  metode  wawancara  dilakukan  kepada 
subject matter, yaitu Kepala Seksi Produksi Subdit PKS untuk 
mendapatkan permasalahan sistem berjalan, kebutuhan sistem 
usulan,  dan  informasi  lain  terkait  SUB-KSA.  Pengumpulan 
data juga menggunakan survei untuk mengevaluasi sistem yang 
telah dibangun menggunakan System Usability Scale (SUS). 

B.  Metode Pengembangan Sistem 

Metode  pengembangan  sistem  yang  digunakan  dalam 
penelitian ini adalah System Development Life Cycle (SDLC) 
dengan  model  Iterative  Waterfall.  Metode  SDLC  terdiri  atas 
enam tahapan, yaitu tahap perencanaan, analisis, perancangan, 
implementasi, uji coba, dan pemeliharaan. 

Pada tahap perencanaan, wawancara dilakukan dengan sub-
ject matter untuk mengetahui ruang lingkup pengembangan sis-
tem dan strategi yang akan dilakukan dalam pengembangan sis-
tem.  Selain  itu,  pada  tahap  ini  juga  dilakukan  identifikasi 
apakah  permasalahan  yang  ada  dapat  diselesaikan  dengan 
pengembangan sistem. 

Tahapan pengembangan sistem selanjutnya adalah analisis. 
Analisis yang dimaksud mencakup analisis sistem berjalan, an-
alisis  permasalahan,  dan  analisis  kebutuhan  sistem.  Analisis 
sistem bertujuan untuk mendapatkan gambaran yang utuh ter-
hadap  proses  bisnis  dan  permasalahan  yang  ada  pada  sistem 
berjalan, serta kebutuhan sistem usulan. 

Setelah mendapatkan kebutuhan terhadap sistem yang akan 
dikembangkan, langkah selanjutnya adalah menuangkan kebu-
tuhan-kebutuhan 
rancangan  sistem. 
tersebut  ke  dalam 
Perancangan sistem usulan meliputi perancangan  proses bisnis, 
basis data, dan antar muka. 

Pada tahap implementasi, rancangan sistem yang dihasilkan 
dari tahapan sebelumnya diimplementasikan ke dalam bahasa 
pemrograman komputer. Selain itu, rancangan basis data juga 
diterapkan pada tahap ini.  Setelah implementasi sistem, taha-
pan selanjutnya adalah pengujian sistem. Pengujian bertujuan 
untuk memastikan sistem yang dihasilkan berjalan dengan baik 
dan sesuai dengan tujuan pengembangan sistem. 

 2 / 8 

 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

menghasilkan  rekapitulasi  kerangka  sampel.  Rekapitulasi  ter-
sebut  kemudian  diserahkan  ke  subdit  Pengembangan  Desain 
Sensus dan Survei (PDSS) untuk dibuat alokasi sampel. 

Tahapan  terakhir  dalam  pengembangan  sistem  ini  adalah 
pemeliharaan.  Pemeliharaan  berguna  agar  sistem  tetap  dapat 
berjalan sesuai dengan tujuan apabila terdapat kesalahan dalam 
implementasi atau terdapat perubahan kebutuhan terhadap sis-
tem penarikan sampel ini. 

V.  KERANGKA PIKIR 

Pada  Gambar  2,  dijelaskan  kerangka  pikir  penelitian  ini. 
Penelitian ini dilakukan atas dasar adanya masalah pada proses 
bisnis  yang  yang  berjalan.  Terdapat  tiga  permasalahan  pada 
kegiatan penarikan sampel survei ubinan padi, yaitu penarikan 
sampel yang tidak efisien, pengiriman dokumen daftar sampel 
yang rawan terjadi human error, dan pengajuan sampel tamba-
han yang tidak dapat dipantau perkembangannya. 

Gambar 3. Proses Bisnis Penyusunan Kerangka Sampel Sistem Ber-
jalan 

Alokasi  yang  telah  dibentuk  kemudian  digunakan  dalam 
penarikan  sampel.  Gambar  4  menunjukkan  proses  penarikan 
sampel yang dilakukan menggunakan aplikasi Microsoft Vis-
ual FoxPro hingga menghasilkan daftar sampel terpilih. Setelah 
sampel didapatkan, maka dengan bantuan aplikasi tersebut pula 
dihasilkan dokumen daftar sampel dalam format pdf yang se-
lanjutnya dikirim ke seluruh BPS kabupaten/kota melalui surel. 

Gambar 4. Proses Bisnis Penarikan Sampel Sistem Berjalan 

Jika  target  sampel  tidak  terpenuhi,  maka  BPS  kabu-
paten/kota  dapat  mengajukan  sampel  tambahan  dengan  men-
gisi  file  templat  yang  telah  disediakan.  File  tersebut  dikirim 
bersamaan dengan  surat pengajuan  yang  telah  ditandatangani 
kepala  BPS  provinsi.  Usulan  sampel  tambahan  selanjutnya 
akan diproses oleh subdit PKS untuk ditolak atau disetujui. Ter-
akhir,  subdit  PKS  akan  mengirimkan  daftar  sampel  baru  ke 
BPS  kabupaten/kota  yang  mengajukan  penambahan  sampel 
tersebut. Proses pengajuan sampel tambahan ditunjukkan oleh 
Gambar 5. 

 3 / 8 

Gambar 2. Kerangka Pikir Penelitian 

Setelah permasalahan berhasil diidentifikasi, tahap selanjut-
nya adalah memberikan alternatif solusi. Solusi yang diberikan 
terhadap  permasalahan  tersebut  adalah  dengan  mengem-
bangkan sistem penarikan sampel berbasis web. 

ini, 

evaluasi 

Setelah solusi berupa pengembangan sistem tersebut diterap-
kan,  sistem  tersebut  kemudian  dilakukan  evaluasi.  Pada 
penelitian 
dilakukan 
menggunakan  blackbox  testing  dan  system  usability  scale 
(SUS). Apabila hasil evaluasi yang didapat berada di atas am-
bang  batas,  maka  sistem  tersebut  dapat  dikatakan  layak 
digunakan. Hasil akhir penelitian ini berupa sistem penarikan 
sampel survei ubinan berbasis web.  

terhadap 

sistem 

VI. HASIL DAN PEMBAHASAN 

A.  Analisis Sistem Berjalan 

Kegiatan penarikan sampel SUB-KSA setiap bulan diawali 
dengan  penyusunan  kerangka  sampel.  Kerangka  sampel 
disusun menggunakan hasil amatan KSA yang diperoleh me-
lalui sistem KSA. Gambar 3 menunjukkan proses penyusunan 
kerangka sampel yang menggunakan aplikasi R Studio hingga 

 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

a.  Performance 

Sistem yang dibangun menyediakan proses yang lebih 
ringkas  dan  cepat  pada  penyusunan  kerangka  sampel, 
penarikan sampel, dan pengajuan sampel tambahan. 

b.  Information 

Sistem  yang  dibangun  mampu  memberikan  informasi 
yang jelas terkait perkembangan pengajuan sampel tam-
bahan dari BPS provinsi dan BPS kabupaten/kota. 

c.  Control 

Semua data disimpan di dalam database server sehingga 
lebih aman dan terkontrol. 

d.  Efficiency 

Tersedianya antar muka yang baik untuk pengguna se-
hingga tidak memerlukan proses yang panjang termasuk 
melakukan perubahan kode program. 

C.  Rancangan Sistem Usulan 

Proses Bisnis Sistem Usulan 
Gambar 6 menunjukkan proses penyusunan kerangka sam-
pel  pada  sistem  usulan  yang  diawali  dengan  memilih  bulan 
yang ada pada tahun berjalan. Setelah memilih bulan, langkah 
selanjutnya  adalah  mengunggah  file  amatan  KSA  berekstensi 
xlsx yang telah dipisah menjadi dua file csv yang berasal dari 
sheet  segmen  dan  subsegmen.  Selanjutnya  sistem  mengolah 
kedua  file  tersebut  lalu  menyimpan  kerangka  sampel.  Hasil 
kerangka sampel dapat diunduh rekapitulasinya lalu diserahkan 
ke subdit PDSS untuk pembuatan alokasi sampel. 

Gambar 6. Proses Bisnis Penyusunan Kerangka Sampel Sistem Usulan 

Proses bisnis dalam penarikan sampel pada sistem usulan di-
tunjukkan  oleh  Gambar  7.  Penarikan  sampel  dilakukan  oleh 
subdit PKS dengan menekan tombol “tarik sampel”. Setelah itu, 
sistem akan menampilkan halaman untuk melakukan penarikan 
sampel. Subdit PKS kemudian memilih tahun dan bulan penari-
kan sampel serta kerangka sampel yang akan digunakan. Selan-
jutnya, subdit PKS akan mengunggah file alokasi sampel utama 
dan cadangan yang diperoleh dari subdit PDSS. Setelah alokasi 
diunggah, sistem akan melakukan pengecekan apakah alokasi 
sampel melebihi kuota sampel yang tersedia atau tidak. Proses 
penarikan  sampel  hanya  dapat  berlanjut  apabila  alokasi  tidak 

 4 / 8 

Gambar 5. Proses Bisnis Pengajuan Sampel Tambahan Sistem Berjalan 

Berdasarkan analisis terhadap sistem berjalan pada kegiatan 
penarikan sampel SUB-KSA, dapat diidentifikasi berbagai per-
masalahan  yang  mengakibatkan  penarikan  sampel  belum 
efisien  dan  optimal.  Permasalahan-permasalahan  tersebut 
secara  umum  berasal  dari  proses  yang  memakan  waktu  lama 
dan rawan terhadap human error. Selain itu, permasalahan juga 
berasal dari informasi status perkembangan usulan sampel tam-
bahan yang tidak tersedia bagi BPS kabupaten/kota. 

B.  Analisis Kebutuhan 

Berdasarkan  analisis  terhadap  permasalahan  pada  sistem 
berjalan, dibutuhkan solusi yang dapat mengatasi permasalahan 
tersebut. Solusi tersebut dituangkan ke dalam kebutuhan sistem 
yang  kemudian  dapat  dikelompokkan  menjadi  kebutuhan 
fungsional dan kebutuhan non-fungsional. 

1)  Kebutuhan Fungsional 

a.  Penyediaan  layanan  bagi  subdit  PKS  untuk  menyusun 
dan mengelola kerangka sampel dengan menggunakan 
hasil amatan KSA setiap bulan. 

b.  Penyediaan layanan bagi subdit PKS untuk melakukan 
penarikan dan pengelolaan sampel cadangan dan utama 
setiap bulan. 

c.  Penyediaan layanan bagi subdit PKS untuk merilis sam-
pel dan mengirim notifikasi rilis melalui surat elektronik 
kepada BPS kabupaten/kota dan provinsi. 

d.  Penyediaan  layanan  bagi  BPS  kabupaten/kota  dan 
provinsi  untuk  melihat  dan  mengunduh  daftar  sampel 
terpilih pada setiap bulan. 

e.  Penyediaan  layanan  bagi  BPS  kabupaten/kota  dan 
provinsi untuk melakukan pengajuan sampel tambahan. 
f.  Penyediaan layanan bagi BPS provinsi dan subdit PKS 
untuk memproses usulan sampel tambahan dari BPS ka-
bupaten/kota dan BPS provinsi. 

g.  Penyediaan layanan bagi BPS provinsi dan subdit PKS 
untuk  mengelola  daftar  pengguna  yang  diizinkan 
dengan menerapkan Single Sign On (SSO) milik BPS. 

2)  Kebutuhan Non-Fungsional 

 
 
 
 
 
 
melebihi  kuota  sampel  yang  tersedia.  Setelah  melakukan 
penarikan  sampel,  daftar  sampel  terpilih  akan  disimpan  se-
hingga dapat diunduh oleh BPS provinsi dan kabupaten/kota. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 7. Proses Bisnis Penarikan Sampel Sistem Usulan 

Proses  bisnis  pengajuan  sampel  tambahan  pada  sistem 
usulan ditunjukkan oleh Gambar 8. Pengajuan sampel tamba-
han diawali dengan pengisian formulir pengajuan sampel tam-
bahan oleh BPS kabupaten/kota yang kemudian dikirim ke BPS 
provinsi untuk selanjutnya direspons oleh BPS provinsi berupa 
persetujuan atau penolakan. Kumpulan sampel yang disetujui 
akan dikompilasi lalu dilakukan pengajuan kembali ke subdit 
PKS bersamaan dengan surat pengajuan sampel yang ditanda-
tangani kepala BPS provinsi. Sampel-sampel yang diajukan ter-
sebut akan direspons oleh subdit PKS berupa penolakan atau 
persetujuan  juga.  Kumpulan  sampel  yang  disetujui  akan 
digabung lalu dirilis yang selanjutnya akan diterima oleh BPS 
kabupaten/kota yang mengajukan sampel. 

Use Case Sistem Usulan 
Pembuatan  diagram  use  case  dilakukan  untuk  menggam-
barkan siapa yang akan menggunakan sistem dan cara yang di-
harapkan pengguna dalam berinteraksi dengan sistem [6]. Se-
bagaimana  yang  ditunjukkan  oleh  Gambar  9,  terdapat  empat 
pelaku bisnis dalam sistem usulan ini, yaitu super administra-
tor, subdit PKS, BPS provinsi, dan BPS kabupaten/kota. Super 
administrator  merupakan  pegawai  di  subdit  PKS  yang  mem-
iliki  hak  penuh  terhadap  semua  fungsionalitas  sistem.  BPS 
provinsi  yang  dimaksud  adalah  pegawai  di  bidang  statistik 
produksi, sedangkan BPS kabupaten/kota merupakan pegawai 
di seksi statistik produksi. 

Gambar 8. Proses Bisnis Pengajuan Sampel Tambahan Sistem Usulan 

Gambar 9. Diagram Use Case Sistem Usulan 

Rancangan Basis Data 
Perancangan basis perlu dilakukan untuk memudahkan me-
mahami struktur informasi pada sistem usulan. Rancangan ba-
sis data sistem usulan yang menggunakan Logical Entity Rela-
tionship Diagram ditunjukkan oleh Gambar 10. 

Rancangan Antarmuka 
Perancangan antarmuka bertujuan untuk mengantisipasi apa 
yang  mungkin  pengguna  lakukan  dan  memastikan  bahwa 
antarmuka  memiliki  elemen  yang  mudah  diakses,  dipahami, 
dan  digunakan  untuk  memfasilitasi  tindakan  tersebut  [7]. 
Rancangan  antarmuka  pada  sistem  usulan  ditunjukkan  oleh 
Gambar 11 dan 12. 

 5 / 8 

 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 10. Logical Entity Relationship Diagram  

program pada fungsi penarikan sampel ditunjukkan oleh Gam-
bar 14. 

Gambar 11. Rancangan Antarmuka Fitur Penarikan Sampel  

Gambar 13. Implementasi Basis Data 

Gambar 12. Rancangan Antarmuka Fitur Pengajuan Sampel 
Tambahan 

D.  Implementasi Sistem Usulan 

Implementasi Basis Data 

Sistem manajemen basis data yang peneliti gunakan adalah 
PostgreSQL  versi  11.  Implementasi  basis  data  pada  sistem 
usulan ditunjukkan oleh Gambar 13. 

Implementasi Kode Program 
Dalam mengimplementasikan kode program sistem usulan, 
peneliti  menggunakan  bahasa  pemrograman  PHP  versi  7.4.6 
dengan Laravel versi 8.12  sebagai  kerangka kerja di sisi back 
end. Sementara itu, pada sisi front end peneliti menggunakan 
kerangka kerja Vue JS versi 2.5.17. Contoh implementasi kode 

Gambar 14. Implementasi Kode Program Fungsi Penarikan 
Sampel 

Implementasi Antarmuka 
Implementasi  antarmuka  sistem  usulan  fitur  sampel  dan 
pengajuan sampel tambahan masing-masing ditunjukkan oleh 
Gambar 15 dan 16. Keluaran dari sistem usulan berupa doku-
men daftar sampel ditunjukkan oleh Gambar 17. 

 6 / 8 

 
 
 
 
 
 
 
 
Gambar 15. Implementasi Antarmuka Fitur Penarikan Sampel  

Gambar 16. Implementasi Antarmuka Fitur Pengajuan Sampel 
Tambahan 

Gambar 17. Contoh Dokumen Daftar Sampel 

E.  Pengujian dan Evaluasi Sistem 

Pengujian Sistem 
Teknik  pengujian  yang  peneliti  gunakan  terhadap  sistem 
usulan  adalah  pengujian  black-box.  Pengujian  black-box 
dirancang  untuk  memvalidasi  kebutuhan  fungsional  tanpa 
memperhatikan cara kerja suatu program [8]. 

Pengujian  black-box  pada  sistem  usulan  dilakukan  oleh 
seorang penguji yang merupakan mahasiswa Politeknik Statis-
tika STIS. Penguji diminta untuk melakukan berbagai kasus uji 
dengan memasukkan input tertentu lalu membandingkan hasil 
yang  sebenarnya  dengan  hasil  yang  diharapkan.  Hasil  pen-
gujian black-box dijelaskan pada Tabel 1. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

TABEL I 
HASIL PENGUJIAN BLACK-BOX 

No 

Kasus uji 

Hasil yang diharapkan 

1. 

 Melakukan login 

Sistem mengarahkan ke hala-
man dasbor 

 Melakukan 
penyusunan 
kerangka sampel 

Sistem mengarahkan ke hala-
man detail kerangka sampel 
yang telah disusun 

Melihat detail 
kerangka sampel 

1
. 

Detail kerangka sampel dit-
ampilkan menurut rekapitulasi 
dan subsegmen dan file ber-
hasil diunduh 

 Melihat ringkasan 
nilai kerangka 
sampel 

 Melakukan 
penarikan sampel 

 Melihat 
penggunaan 
kerangka sampel 

 Melihat detail 
sampel 

Tabel ringkasan nilai ditampil-
kan dan file berhasil diunduh 

Berhasil 

Sistem mengarahkan ke hala-
man daftar sampel dan sampel 
berhasil diambil 

Tabel penggunaan ditampilkan 
dan file berhasil diunduh 

Berhasil 

Detail sampel terpilih ditampil-
kan menurut rekapitulasi dan 
subsegmen 

 Mengirim notifi-
kasi sampel 

Muncul pesan ""Berhasil 
Mengirim Notifikasi"" dan 
email berhasil dikirim 

 Mengunduh 
dokumen daftar 
sampel 

File dokumen daftar sampel 
dalam format .zip berhasil 
diunduh 

 Mengajukan Sam-
pel Tambahan ke 
Provinsi 

Sampel tambahan yang di-
ajukan muncul di daftar sam-
pel tambahan 

 Memproses 
Usulan Sampel 
Tambahan dari 
Kabupaten 

 Mengajukan Sam-
pel Tambahan ke 
Pusat 

 Memproses 
Usulan Sampel 
Tambahan dari 
Provinsi 

Sampel subsegmen yang telah 
diproses hilang dari daftar 
sampel subsegmen usulan 

Sistem akan mengarahkan ke 
daftar pengajuan ke pusat 

Berhasil 

Status pengajuan sampel beru-
bah menjadi ""Disetujui Pusat"" 
atau ""Ditolak Pusat"" 

Berhasil 

Hasil 
sebenar-
nya 
Berhasil 

Berhasil 

Berhasil 

Berhasil 

Berhasil 

Berhasil 

Berhasil 

Berhasil 

Berhasil 

2. 

3. 

4. 

5. 

6. 

7. 

8. 

9. 

10. 

11. 

12. 

13. 

14. 

 Melakukan rilis 
sampel tambahan 

Sistem mengarahkan ke daftar 
rilis sampel pada bulan terse-
but 

Berhasil 

15. 

 Melakukan logout 

Sistem mengarahkan ke hala-
man login 

Berhasil 

 7 / 8 

 
 
 
 
 
 
 
 
Evaluasi Sistem 
Dalam  mengevaluasi  sistem  usulan,  peneliti  menggunakan 
kuesioner  System  Usability  Scale  (SUS).  SUS  merupakan 
kuesioner  yang  berisi  sepuluh  pertanyaan  sederhana  yang 
memberikan  pandangan  umum  tentang  penilaian  subjektif 
kegunaan [9]. SUS telah menjadi standar industri, dengan ref-
erensi  di  lebih  dari  1300  artikel  dan  publikasi  karena  mudah 
digunakan serta dapat diandalkan untuk sampel kecil [10]. 

Evaluasi sistem usulan dilakukan oleh pengguna yang terdiri 
dari beberapa pegawai di subdit PKS BPS RI serta di seksi/bi-
dang statistik produksi BPS kabupaten/kota dan provinsi yang 
ada  di  provinsi  Bengkulu  dan  Maluku  Utara.  Pengisian 
kuesioner SUS dilakukan pada tanggal 8 – 13 Juni 2021 dengan 
hasil sebagaimana ditunjukkan pada Tabel 2. 

TABEL II 
HASIL EVALUASI SUS 
Pertanyaan ke- 

1 

5 

4 

5 

4 

5 

5 

5 

4 

2 

2 

3 

2 

2 

2 

1 

2 

1 

3 

3 

4 

5 

4 

4 

5 

4 

5 

4 

3 

4 

2 

2 

2 

1 

3 

1 

5 

5 

4 

4 

4 

5 

5 

4 

5 

6 

2 

2 

2 

3 

2 

2 

2 

1 

7 

4 

3 

4 

4 

4 

4 

5 

5 

8 

3 

2 

2 

2 

1 

2 

2 

1 

Respon-
den 

1 

2 

3 

4 

5 

6 

7 

8 

Skor 

67,5 

60 

82,5 

72,5 

77,5 

82,5 

72.5 

95 

10 

4 

4 

2 

2 

4 

4 

4 

2 

9 

4 

4 

5 

4 

4 

4 

4 

5 

Berdasarkan  Tabel  2,  didapatkan  rata-rata  skor  SUS  ter-
hadap delapan orang responden sebesar 76,25. Menurut refer-
ensi [10], sebuah skor SUS di atas 68 dapat  dipertimbangkan 
sebagai di atas rata-rata. Dengan demikian sistem usulan yang 
dikembangkan  layak  diterima  dan  digunakan  karena  rata-rata 
skor SUS di atas 68. 

Namun demikian, terdapat dua responden dengan skor SUS 
di bawah rata-rata. Kedua responden tersebut merupakan peg-
awai BPS kabupaten yang pada saat melakukan uji coba sistem 
memiliki  kendala  dalam  mengakses  sistem  karena  masalah 
jaringan. Hal tersebut mungkin menjadi penyebab respon ter-
hadap pertanyaan keempat yakni “Saya membutuhkan bantuan 
teknis  untuk  menggunakan/menjelajahi  situs  ini”  dan  pertan-
yaan kedelapan yakni “Saya menilai situs ini sangat sulit untuk 
dijelajahi”  memiliki  skor  yang  lebih  rendah  dibandingkan 
dengan responden lain. 

VII. 

PENUTUP 

Berdasarkan  penelitian  yang  telah  dilakukan,  didapat  kes-

impulan sebagai berikut. 
1)  Permasalahan  yang  terjadi  pada  penarikan  sampel  survei 
ubinan  padi  selama  ini  terletak  pada  proses  yang  tidak 
efisien dan rawan terhadap human error. 

2)  Solusi  yang  ditawarkan  untuk  permasalah  tersebut  adalah 
dengan  mengembangkan  sistem  penarikan  sampel  survei 
ubinan padi berbasis web. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

3)  Pengembangan  sistem  telah  dilakukan  dengan  metode 
SDLC  model 
terdiri  dari 
perencanaan,  analisis,  perancangan,  implementasi,  pen-
gujian, dan pemeliharaan. 

iterative  waterfall  yang 

4)  Hasil  dari  pengujian  sistem  dengan  menggunakan  pen-
gujian  black-box  menunjukkan  bahwa  setiap  fungsi  yang 
dibuat dapat berjalan sesuai harapan. Adapun hasil evaluasi 
sistem menggunakan kuesioner SUS kepada delapan orang 
pegawai  BPS  menunjukkan  bahwa  sistem  dapat  diterima 
dengan rata-rata skor SUS sebesar 76,25. 

Adapun saran yang dapat diberikan untuk penelitian selan-

jutnya adalah sebagai berikut. 
1)  Berdasarkan  evaluasi  sistem  oleh  pengguna 

terdapat 
masukan  agar  dilakukan  sinkronisasi  antara  sistem 
penarikan  sampel  survei  ubinan  padi  dengan  sistem 
penarikan sampel survei ubinan palawija karena pengguna 
kedua sistem tersebut sebagian besar sama. 

2)  Selain itu, terdapat masukan pula agar sampel subsegmen 
yang  terpilih  divisualisasikan  dalam  bentuk  geospasial. 
Dengan  demikian,  BPS  provinsi  maupun  BPS  kabu-
paten/kota  dapat  melihat  dengan  mudah  posisi  sampel 
secara geografis. 

DAFTAR PUSTAKA 
[1]  D. D. Susanti, “The Evaluation of the Implementation of the Area Sam-
pling Frame as a New Method to Predict Rice Productivity in Indonesia”, 
Jurnal Ekonomi Pertanian dan Agribisnis (JEPA), vol. 3, no. 3, pp. 487-
494, Apr 2019. 

[2]  O. R. Prasetyo, R. R. Amalia, and Kadir, “The Accuracy Evaluation of the 
Potential  of  Paddy  Harversted  Area  in  Indonesia”,  in  Seminar  Nasional 
Official  Statistics  2019:  Pengembangan  Official  Statistics  dalam  men-
dukung  Implementasi  SDG’s,  Jakarta,  Indonesia:  Politeknik  Statistika 
STIS, Sep 2019. 

[3]  Badan  Pusat  Statistik,  2020.  Pedoman  Pencacahan  Survei  Ubinan 
Komoditas Padi Berbasis Kerangka Sampel Area (KSA). Jakarta: Badan 
Pusat Statistik, pp.15-22. 

[4]  S. Hafifah, “Pembangunan Aplikasi Penarikan Sampel Dalam Mengimple-
mentasikan  Desain  Sampling Rumah  Tangga  Tanaman  Padi 2014  (SPD 
2014)”, thesis, 2016. 

[5]  G. A. Nartapradnyana, “Surveykit: Modul Penarikan Sampel Tahap Akhir 

pada CAPI-STIS”, thesis, 2017. 

[6]  J.  L.  Whitten  and  L.  D.  Bentley,  Systems  analysis  and  design  methods. 

Boston: McGraw-Hill/Irwin, 2007. 

[7]  U.S.  General  Services  Administration.  (2014,  5).  User  Interface  Design 
https://www.usability.gov/what-and-

[Online].  Available: 

Basics. 
why/user-interface-design.html. 

[8]  R.  S.  Pressman,  Software  engineering  a  practitioner's  approach.  New 

York: McGraw-Hill, 2010. 

[9]  J. Brooke, “SUS-A quick and dirty usability scale”, Usability evaluation 

in industry, Boca Raton: Chapman and Hall/CRC, 2014, pp. 191. 

[10] U.S. General Services Administration. (2013, 9). System Usability Scale 
[Online].  Available:  https://www.usability.gov/how-to-and-

(SUS). 
tools/methods/system-usability-scale.html. 

 8 / 8 

 
 
 
 
 
"
221709853,"Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik 
Identifikasi Tweet spam pada Twitter Menggunakan 
Algoritma Support vector machine dan Naïve bayes 

Muhammad Firli (221709853, 4SD1) 

Dosen Pembimbing: Farid Ridho, S.S.T., M.T. 

Ringkasan— Spam merupakan tindakan penyalahgunaan 
yang  selain  merugikan  juga  membahayakan.  Keberadaan 
Spam  pada  Twitter  juga  dapat  mempengaruhi  data  Twitter 
yang  dapat  dimanfaatkan  dalam  penelitian.  Sehingga 
dibutuhkan alat ataupun informasi yang memungkinkan kita 
untuk  mendeteksi  keberadaan  Tweet  spam  sehingga  dapat 
menghindari Spam. Oleh karena itu disusunlah penelitian ini 
dengan  tujuan  untuk  membangun  model  pengklasifikasian 
Tweet menggunakan metode Support vector machine dan Naïve 
bayes dalam mendeteksi Tweet ke dalam kelas Spam dan bukan 
Spam  dan  membandingkan  hasil  kedua  metode  klasifikasi 
tersebut, guna melihat metode mana yang lebih efektif. Selain 
itu  dibutuhkan  juga  informasi  tentang  metode  Preprocessing 
apa yang paling mampu membersihkan data Twitter sehingga 
dapat meningkatkan analisis klasifikasi yang dilakukan. Dari 
penelitian  yang  dilakukan  kombinasi  metode  preprocessing 
menggunakan  algoritma  python  dan  metode  analisis  support 
vector  machine  menjadi  kombinasi  yang  memiliki  tingkat 
akurasi tertinggi pada kedua data yaitu sebesar 88,847% dan 
84.894%. 

Kata Kunci— Spam, Twitter, Support vector machine, Naive 

Bayes, Preprocessing. 

I.  LATAR BELAKANG 

teknologi 

Revolusi  industri  4.0  sudah  menjadi  topik  yang  kerap 
sekali  kita  dengan  pada  beberapa  tahun  belakangan  ini. 
Istilah  yang  muncul  akibat  revolusi  industri  yang  terjadi 
secara  global  ini  menjadi  sebuah  lompatan  besar  di  sektor 
perindustrian  yang  mana 
informasi  dan 
komunikasi  dimanfaatkan  sepenuhnya  guna  mencapai 
efisiensi yang tertinggi sehingga menghasilkan model bisnis 
baru berbasis digital [1]. Revolusi industri 4.0 memberikan 
efek  yang  sangat  besar  pada  ekosistem  dan  tata  cara 
kehidupan  di  dunia  sekarang  termasuk  Indonesia  [2]. 
jumlah 
Indonesia  sebagai  salah  satu  negara  dengan 
penduduk  terbesar  di  dunia  juga  mulai  mengadaptasi 
revolusi pada sektor perindustriannya. Sektor perindustrian 
yang bertransformasi dari cara konvensional ke hal-hal yang 
berbasis  digital  memberikan  dampak  langsung  kepada 
maraknya  penggunaan  sosial  media  di  masyarakat. 
Keberadaan  media  sosial  beserta  kelebihannya 
telah 
menjadi bagian dari kehidupan manusia modern ini. Media 
sosial  sendiri  adalah  media  yang  berbasis  internet  yang 
memungkinkan pengguna untuk berinteraksi, bekerjasama, 
berkomunikasi dan saling terhubung dengan pengguna lain 
sehingga  membentuk  ikatan  sosial  secara  virtual.  Media 
sosial merupakan media digital tempat realitas sosial terjadi 
dan ruang-waktu para penggunanya berinteraksi. Nilai-nilai 
yang  diterapkan  masyarakat  dalam  komunitas  juga  dapat 
hadir dalam bentuk yang sama atau berbeda di dengan yang 
ada di internet [3].  

Diadopsinya  prinsip  dari 

revolusi 
mengakibatkan  beberapa  perkembangan 

industri  4.0 
sektor  baik 

teknologi  maupun  sektor  informasi  di  Indonesia.  Dikutip 
dari  survey  yang  diadakan  oleh  World  Economic  Forum 
(Future of Jobs Survey 2018) terdapat 4 teknologi yang akan  
mendominasi di tahun 2018-2022 yaitu: high-speed mobile 
internet, artificial intelligence, big data analytics, dan cloud 
technology.  Selain  itu  hingga  tahun  2022  nanti  juga 
diperkirakan 92% perusahaan di Indonesia akan mengadopsi 
penerapan dan pemanfaatan big data analytics sebagai salah 
satu  teknologi  utama  [1].  Dari  survei  berbeda  yang 
dilakukan oleh Hootsuite dan WeAreSocial tentang jumlah 
penetrasi pengguna internet dan media sosial di Indonesia, 
angka  pengguna  media  sosial  di  Indonesia  mengalami 
peningkatan di tahun 2020. Media sosial seperti Instagram, 
Facebook,  dan  Twitter  masih  menjadi  pilihan  utama 
masyarakat Indonesia. Tingginya minat Indonesia terhadap 
teknologi seperti big data ditambah dengan tingginya angka 
pengguna sosial media di Indonesia melahirkan banyaknya 
studi maupun penelitian yang menggabungkan kedua topik 
ini  dengan  tujuan  untuk  lebih  meningkatkan  pengetahuan 
Indonesia atas kedua topik tersebut. 

Dengan 

berbasis 

microblogging, 

Twitter  sebagai  media  sosial  cukup  digemari  oleh 
masyarakat  Indonesia,  selain  karena  lebih  hemat  dalam 
konsumsi data juga karena memungkinkan konten tersebar 
luaskan  dengan  sangat  cepat  dan  luas.  Secara  struktur 
Twitter sendiri adalah media sosial berbasis microblogging 
[4]. 
Twitter 
memungkinkan penggunanya mengirim dan membaca post 
teks maksimal mengandung 140 karakter [5]. Salah satu fitur 
media sosial  twitter  yang cukup banyak  menyita perhatian 
baik  masyarakat  maupun peneliti di Indonesia adalah  fitur 
Trending Topic, fitur ini memungkinkan pengguna melihat 
topik yang sedang ramai dibicarakan di media sosial twitter. 
Suatu kata kunci atau hashtag dapat menjadi Trending Topic 
apabila  hal  tersebut  terus  di  tweet  oleh  banyak  pengguna 
dalam rentang waktu berdekatan. Trending Topic umumnya 
menampilkan  himpunan  hashtag  atau  kata  kunci  yang 
apabila diklik dapat menampilkan seluruh Tweet pengguna 
yang  mengandung  hashtag  atau  kata  kunci  tersebut  yang 
telah diurutkan dari yang paling ramai di  tweet. Algoritma 
twitter  memungkinkan  urutan  Trending  Topic  untuk  terus 
terbaharui secara dinamis.  

Mekanisme  Trending  Topic  yang  sedemikian  rupa 
tersebut  tentu  saja  menghasilkan  banyak  manfaat  bagi 
pengguna  twitter,  mulai  dari  penyebaran  tweet  yang  bisa 
semakin luas sampai penyebaran informasi atau berita yang 
semakin  cepat.  Mekanisme  tersebut  mengakibatkan  fitur 
Trending  Topic  kerap  disalahgunakan  oleh  pengguna 
twitter.  Salah  satu  penyalahgunaan  fitur  Trending  Topic 
yang sering ditemui adalah Tweet spam. Spam pada Twitter 
sendiri  merupakan  suatu  konten  yang  dikirimkan  oleh 
Spammer menggunakan akan palsu yang mereka buat atau 

 1 / 8 

 
 
 
akan yang milik pengguna lain yang mereka curi [5], dimana 
tweet  yang  dikirimkan  oleh  Spammer  ini  tidak  memiliki 
hubungan  dengan  topik  yang  ada,  umumnya  Spammer 
hanya menambahkan hashtag atau kata kunci yang sedang 
trending  pada  Tweet  asli  yang  ingin  Spammer  kirimkan, 
selain itu Tweet spam juga umumnya dikirimkan oleh satu 
atau lebih akun yang mana memiliki bunyi tweet yang sama 
persis. Tweet spam pada umumnya disisipkan pada hal-hal 
atau kejadian yang sedang ramai dibicarakan pada saat itu. 
Misalnya pada beberapa waktu belakangan ini sosial media 
twitter  di  Indonesia  sedang  ramai  oleh  kejadian  yang 
menimpa  partai  demokrat.  Dikutip  dari  Warta  Ekonomi 
co.id  topik  “Kami  Bersama  AHY”,  merujuk  pada  Agus 
Harimurti  Yudhoyono  selaku  ketua  umum  DPP  Partai 
Demokrat  yang  usai  kegiatan  konfrensi  luar  biasa  yang 
dilaksanaka  parta  demokrat  tanpa  kehadirannya.  Topik  ini 
menjadi trending di Twitter Indonesia dengan lebih dari 20 
ribu tweet. Sementara “Partai Demokrat” juga beberapa kali 
keluar  masuk  ke  dalam  daftar  trending  topic  Indonesia 
Selain  itu  topik  maupun  hashtag  lain  mengenai  kejadian 
yang  menimpa  partai  demokrat 
terus  bergantian 
menduduki  jajaran  trending  Indonesia  selama  seminggu 
lebih [6]. Beberapa tweet yang dapat dikategorikan sebagai 
spam  pada  kata  kunci  “Demokrat”  diilustrasikan  pada 
Gambar 1. 

ini 

Gambar 1. Ilustrasi dari Tweet spam pada penelusuran kata kunci 
demokrat 

Tingginya perhatian masyarakat pada suatu kata kunci 
atau  hashtag  di  twitter  menarik  perhatian  spammer  untuk 
menyebarkan Tweet spam dapat didorong oleh beberapa hal 
seperti  untuk  menyebarkan  iklan  untuk  menghasilkan 
penjualan,  menyebarkan  pornografi,  virus,  phishing, 
menciptakan framing pada masyarakat, atau yang sederhana 
hanya untuk merusak reputasi sistem. Spammer tidak hanya 
mencemari pencarian waktu nyata, tetapi mereka bisa juga 
mengganggu statistik yang disajikan oleh alat penambangan 
tweet  dan  mengkonsumsi  sumber  daya  tambahan  dari 
pengguna  dan  sistem.  Bagi  pengguna,  keberadaan  Tweet 
spam  dapat  mengakibatkan  kerusakan  pada  ekosistem 
Twitter  yang  dapat  merugikan  hingga  membahayakan 
pengguna.  Selain  itu  keberadaan  Tweet  spam  juga  dapat 
menghambat  proses  suatu  penelitian  yang  memanfaatkan 
big data dari Twitter. Keberadaan tweet dengan pesan yang 
sama  oleh  banyak  akun  ini  dapat  mempengaruhi  suatu 
analisis yang dilakukan pada suatu penelitian.  

Twitter  sendiri  telah  berupaya  semaksimal  mungkin 
untuk  terus  melawan  perilaku  spam.  Tercatat  twitter  telah 
memiliki 
terus 
mengembangkan  usahanya  dalam  memberantas  bentuk-
bentuk  baru  dari  spam  sehingga  tercipta  ekosistem  twitter 
yang  bebas  spam.  Akan  tetapi  sistem  anti-spam  yang 

anti-spam  yang 

sejak 

lama 

tim 

Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik 

ditawarkan  oleh  twitter  dinilai  masih  memiliki  banyak 
kendala  dalam  pelaksanaannya, 
seperti  hanya  bisa 
twitter  dan  keterbatasan 
dioperasikan  oleh  pengguna 
pengguna  yang  hanya  dilibatkan  hingga  melaporkan  spam 
ke  pihak  twitter  saja,  tidak  adanya  upaya  lain  yang  dapat 
digunakan  baik  oleh  pengguna  maupun  bukan  pengguna 
untuk  mengklasifikasikan  tweet    spam  dan  bukan  spam  di 
twitter [7].  

juga 

lainnya 

upaya 
spam 

telah  dilakukan 

dalam  melakukan 
Beberapa 
pemberantasan 
seperti 
pengembangan  metode-metode  pengklasifikasian  dalam 
statistik yang melahirkan berbagai jenis metode hingga saat 
ini  dapat  langsung  digunakan  umumnya  pada  upaya 
menangani  Tweet  spam  dalam  bidang  penelitian.  Akan 
tetapi  dari  banyaknya  metode  pengklasifikasian  dalam 
statistik,  hingga  saat  ini  belum  adanya  penelitian  yang 
bertujuan untuk membandingkan keefektifan metode Naïve 
bayes 
dalam 
mengklasifikasikan Tweet spam. 

machine 

support 

vector 

dan 

II.  TUJUAN PENELITIAN 

Tujuan yang ingin dicapai dalam penelitian ini adalah 
pertama  membangun  model  pengklasifikasian  Tweet 
kedalam kelas Spam dan bukan Spam menggunakan metode 
Support vector machine dan Naïve bayes. Membandingkan 
hasil  pre-processing  menggunakan  algoritma  python  dan 
operator  rapidminer  versi  7.5.  Membandingkan  hasil 
pengklasifikasian 
tweet  menggunakan  metode  support 
vector machine dan naïve bayes. 

III. PENELITIAN TERKAIT 
Pembangunan model pendeteksi  spam secara umum sudah 
kerap dilakukan pada berbagai penelitian sebelumnya. Dari 
berbagai penelitian ini, beberapa media telah dipilih sebagai 
objek penelitian. Seperti pendeteksi spam pada email dalam 
penelitian  P.  Calais  dkk  [8],  pendeteksi  spam  pada  blog 
dalam  penelitian  A.  Thomason  [9],  pendeteksi  spam  pada 
mesin pencarian web dalam penelitian D. Fetterly dkk [10], 
pendeteksi spam dengan objek penelitian video juga sempat 
dilakukan  oleh  F.  Benevenuto  [11],  hingga  yang  paling 
banyak dilakukan yaitu pendeteksi spam pada sosial media. 
Penelitian terkait pendeteksi  spam pada sosial media tidak 
terpusat  pada  satu  jenis  sosial  media  saja,  berbagai  jenis 
sosial  media  juga  kerap  dijadikan  objek  penelitian  terkait, 
misalnya  pada  penelitian  A.  R.  Chrismanto  yang 
mengidentifikasi spam pada komentar postingan instagram 
[12]  dan  penelitian  oleh  L  Satrionugroho  yang  dilakukan 
pada  twitter  [13].  Penelitian  L  Satrionugroho  tentang 
mendeteksi  spam  pada  twitter  hanya  menggunakan  satu 
metode saja dalam pengklasifikasiannya yaitu metode naïve 
bayes (NB), yang mana menghasilkan tingkat akurasi yang 
cukup  tinggi  yaitu  sebesar  93%.  pada  penelitian  lain  oleh 
Fabr´ıcio Benevenuto, dkk [5] juga melakukan pendeteksian 
spam pada twitter dan melakukan klasifikasi menggunakan 
metode, yang mana pada penelitian ini menghasilkan tingkat 
akurasi sekitar 70% pada Tweet spam dan 96% pada tweet 
non-spam [13]. 

Penelitian A. R. Chrismanto tentang pengidentifikasian 
Spam pada Instagram membandingkan dua metode berbeda 
yaitu Support vector machine (SVM) dan Naïve bayes (NB). 

 2 / 8 

 
 
Dari  penelitian 
ini  didapatkan  hasil  bahwa  model 
identifikasi  spam  dengan  metode  SVM  memiliki  nilai 
akurasi  yang  lebih  tinggi  dari  metode  NB  yaitu  78.49% 
berbanding  77.25%.  Sehingga  dapat  diambil  kesimpulan 
bahwa  pengklasifikasian  spam  dengan  metode  Support 
vector  machine  memiliki  kinerja  yang 
lebih  baik 
dibandingkan  metode  Naïve  bayes  namun  dengan  nilai 
signifikansi yang tidak begitu tinggi [9]. 

IV. METODE PENELITIAN  

Pada penelitian ini dilakukan perbandingan antara dua 
metode analisis dan dua media preprocessing, dalam kasus 
mengidentifikasi tweet spam pada twitter. Scraping twitter 
dilakukan  pada  dua  kasus  berbeda  yaitu  pada  kasus 
konferensi  luar  biasa  partai  demokrat  dan  pemberhentian 
sejumlah anggota komisi pemberantasan korupsi. Kemudian 
dilakukan  preprocessing  menggunakan  dua  media  untuk 
masing-masing kasus yaitu dengan menggunakan algoritma 
python dan operasi pada rapidminer guna membandingkan 
lebih  efisien  dalam  melakukan 
media  mana  yang 
pembersihan terhadap noise yang masih terdapat pada hasil 
scraping.  Kedua  hasil  preprocessing  akan  masing-masing 
dilakukan dua analisis pengklasifikasian  yaitu naive  bayes 
dan  support  vector  machine  guna  mencari  kombinasi 
terefisien  dalam  hal  melakukan  pengklasifikasian  Tweet 
spam.  Hasil  klasifikasi  kemudian  akan  dianalisis  tingkat 
ketepatannya berdasarkan data aktual hasil labelling secara 
ini  diharapkan  dapat  memberikan 
manual.  Penelitian 
kombinasi antara media preprocessing dan metode analisis 
terefisien  dalam  melakukan  pengklasifikasian  tweet  spam. 
Penelitian  ini  akan  dilakukan  dengan  mengacu  pada 
beberapa tahapan metodologi yang telah disusun, yang dapat 
dilihat pada gambar 2. Berikut: 

Gambar 2. Bagan Metodologi 

3.1 Pengumpulan Data 

Pengumpulan  data  dilakukan  dengan  cara  crawling 
pada twitter dengan bantuan API (application programming 

Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik 

interface)  yang  disediakan  oleh  developer  Twitter. 
Pengumpulan data dilakukan dengan menggunakan bahasa 
pemrograman  python.  Pada  pengumpulan  data  digunakan 
package  tweppy  untuk  memungkinkan  pengumpulan  data 
dilakukan.  Data  yang  dikumpulkan  berupa  username 
pengguna beserta tweet yang mengandung kata kunci yaitu 
Demokrat atau KPK.  

Kata  kunci  Demokrat  dan  KPK  dipilih  karena  kedua 
kata kunci ini menjadi kata kunci yang kerap kali menyita 
perhatian  publik  twitter  hingga  sering  menduduki  posisi 
trending 
Indonesia  dan  masih  sering 
diperbincangkan dalam rentang waktu yang cukup lama.  

twitter 

topik 

3.2 Tahapan Penelitian 
3.2.1 Preprocessing 

Tweet  yang  disebarkan  masyarakat  Indonesia  lebih 
beragam  dibandingkan  Negara  lain.  Banyaknya  emoji, 
Bahasa  slang  yang  cenderung  jauh  dari  bahasa  baku,  dan 
penggunaan  singkatan  [15].  Sehingga  dibutuhkan  tahapan 
preprocessing untuk menghilangkan noise tersebut sehingga 
dapat meningkatkan kualitas dari data yang didapatkan.  

Pada  penelitian  ini  dilakukan  tahapan  preprocessing 
pada  dua  media  yang  berbeda  untuk  dibandingkan  hasil 
keduanya.  Kedua  media  tersebut  adalah  operator  pada 
rapidminer  7.5  dan  algoritma  python.  Kedua  media 
mengandung  tahapan  preprocessing  yang  sama  yaitu  data 
cleaning  dan  symbol  handling,  case  folding,  tokenizing, 
stopword removal, normalisasi, dan steaming.  

Pada media algoritma python, syntax untuk melakukan 
preprocessing  ditulis  di  google  collab  sedangkan  pada 
media  operator  RapidMiner  versi  7.5  digunakan  operator 
yang telah disediakan pada software RapidMiner. Tahapan 
preprocessing  pada  kedua  media  dilakukan  dengan  urutan 
yang sama dimana diawali dengan tahap data cleaning dan 
symbol  handling,  pada  proses  ini  dilakukan  penghapusan 
karakter lain selain huruf dan dianggap delimiter. Setelah itu 
tahapan preproessing dilanjutkan dengan menghapus semua 
spasi berlebih dan trimming pada spasi berlebih yang ada di 
awalan dan akhiran kalimat. Tahapan berikutnya yaitu case 
folding,  yang  merupakan  tahap  pengubahan  semua  huruf 
dari ‘a’ sampai ‘z’ pada data menjadi huruf kecil. Setelah itu 
tahapan preprocessing dilanjutkan ke tahap tokenizing yaitu 
proses  pembagian  teks  dari  yang  berupa  kalimat  atau 
paragraf menjadi token-token individu. Setelah data diubah 
menjadi  token  maka  data  sudah  bisa  memasuki  tahapan 
yaitu 
preprocessing 
selanjutnya 
penormalan  kata  tidak  baku  yang  ada  pada  data  diganti 
menjadi kata yang lebih baku, di proses ini pada algoritma 
python  digunakan  kamus  normalisasi  yang  disusun  oleh 
nasalsabila [16]. Pre-processing selanjutnya yaitu stopword 
removal  yang  merupakan  proses  penghapusan  kata  umum 
(common  word)  yang  dinilai  tidak  memberikan  informasi 
penting dalam  kalimat, diproses ini pada algoritma  python 
digunakan daftar stopword yang disusun oleh P. P. Adikara 
[17].  Serta  diakhiri  dengan  tahapan  steaming  yaitu  proses 
untuk  mengembalikan 
bentuk 
sederhananya  tanpa  imbuhan.  Proses  steaming  ditujukan 
untuk mengurangi jumlah token yang ada.  

normalisasi 

kedalam 

kalimat 

yaitu 

3.2.2 Text Transformation dan Feature Selection 

 3 / 8 

 
 
 
 
 
Data  yang  telah  diberikan  label  selanjutnya  melalui 
proses  text  transformation,  pada  tahapan  ini  dilakukan 
proses transformasi data dari yang sebelumnya berupa token 
teks  diubah  menjadi  vector  data  berbobot  sehingga  dapat 
digunakan  dalam  perhitungan  data.  Pada  proses  text 
transformation  dilakukan  dengan  menggunakan  metode 
pembobotan  TF-IDF.  TF-IDF  atau  Term  Frequency  – 
Inverse Document Frequency memiliki prinsip yaitu token 
dengan  frekuensi  kemunculan  yang  lebih  tinggi  pada  data 
mengindikasikan token tersebut memiliki bobot yang lebih 
rendah dibandingkan token yang frekuensi kemunculannya 
pada data lebih rendah, yang mana bobot token mengartikan 
penting tidaknya  suatu  token  tersebut di data. Tahapan ini 
dilakukan menggunakan software RapidMiner. 

Tahapan  berikutnya  yaitu  features  selection,  yaitu 
tahapan yang dilakukan untuk mengurangi jumlah fitur pada 
data sehingga didapatkan toke-token dengan bobot setinggi 
mungkin, pada tahapan ini fitur yang ada pada token yang 
telah  memiliki  bobot  dilakukan  pemilahan  sesuai  besar 
ini  dilakukan  dengan 
kecilnya  bobot 
menggunakan  software  RapidMiner  dan  menerapkan 
metode pruning dengan nilai di bawah 0.1 dan di atas 0.98. 

token.  Proses 

3.2.3 Klasifikasi 

Pada penelitian ini dilakukan dua analisis untuk masing-
masing  data  dan  masing-masing  media  preprocessing. 
Analisis yang digunakan adalah Support vector machine dan 
Naïve  bayes.  Analisis  dilakukan  menggunakan  software 
rapidMiner. Kedua analisis ini dipilih karena cukup sering 
digunakan  pada  penelitian-penelitian  dengan  analisis 
klasifikasi,  selain 
juga  karena  adanya  penelitian 
terdahulu  yang  sudah  membandingkan  kedua  analisis  ini 
namun dengan media data yang berbeda. Berikut dijelaskan 
apa  itu  metode  analisis  support  vector  machine  dan  naïve 
bayes. 

itu 

3.2.3.1 Support vector machine 

Metode Support vector machine atau SVM digolongkan 
sebagai algoritma classifier yang berbasis model supervised 
learning.  Metode  SVM  mempunyai  kekurangan  yaitu 
dibutuhkannya waktu yang cukup lama dalam perhitungan 
dan sulitnya diaplikasikan pada data dengan jumlah sampel 
dan  dimensi  yang  besar,  namun  metode  SVM  memiliki 
kelebihan  yaitu  dalam  mengklasifikasikan  data  kedalam 
kelas-kelas yang berjumlah sedikit, sehingga dinilai sangat 
cocok untuk melakukan klasifikasi spam yang mana hanya 
terdiri dari dua kelas yaitu kelas tweet spam dan tweet bukan 
spam [14]. 

3.2.3.2 Naïve bayes 

Metode  Naïve  bayes  atau  NB  merupakan  algoritma 
classifier  yang  menggunakan  teori  kemungkinan  dalam 
statistik untuk memprediksi peluang di masa depan dengan 
memperhitungkan peluang di masa lalu. Metode naïve bayes 
kemudian digabungkan dengan kondisi antar atribut dalam 
universe  yang  saling  bebas  dan  tidak  saling  berhubungan. 
Pada  penelitian  ini  analisis  SVM  dan  NB  dilakukan 
menggunakan  bantuan  software  rapidMiner  7.5.    Adapun 
parameter-parameter  yang  diterapkan  pada  penelitian  ini 
disajikan seperti pada Tabel 1 berikut: 

Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik 

Metode 

Support vector machine 

Naïve bayes 

TABEL I. 
PARAMETER KLASIFIKASI 
Parameter 
Max iteration 
Epsilon 
Karnel Function 
Gamma 
C 
No of kernels 
Minimum Width 
Laplace Correction 
Estimation Mode 

Nilai 
100000 
0.001 
RBF 
1 
1 
10 
0.1 
Yes 
Greedy 

3.2.4 Manual Labelling 

Labelling  merupakan  proses  tahapan  pemberian  label 
atau keterangan pada data sehingga mengubah bentuk data 
dari  unsupervised  menjadi  supervised  sehingga  dapat 
dilakukan  analisis  dengan  teknik  supervised  [16].  Dalam 
pemberian label pada tweet terdapat cara-cara manual yang 
dapat diterapkan dalam pendeteksian  komentar  spam pada 
twitter seperti pendeteksian komentar duplikat, penggunaan 
plugin  dalam  blog,  penonaktifan  komentar  tanpa  disertai 
login,  penggunaan  CAPTCHA,  moderasi  komentar  secara 
manual,  tidak  memperbolehkannya  keberadaan  hyperlink, 
pendeteksian  kata  bermakna  aneh,  kesalahan  pada 
gramatikal  kalimat,  kalimat  yang  tidak  rasional,  tidak 
topik  pembicaraan,  dan 
relevannya  komentar  dengan 
umumnya bersifat sangat general [19]. 

Labelling 

dilakukan 

secara  manual 

dengan 
menggunakan  software  Microsoft  excel.  Proses  labelling 
pada  penelitian  ini  memperhatikan  dua  sifat  yang  dimiliki 
oleh Tweet spam yaitu komentar yang kerap terduplikasi dan 
mengandung  kata  bermakna  aneh,  kesalahan  pada 
gramatikal  kalimat,  kalimat  yang  tidak  rasional,  tidak 
topik  pembicaraan,  dan 
relevannya  komentar  dengan 
umumnya bersifat sangat general [12]. 

3.2.5 Evaluasi 

Tahap  evaluasi  pada  penelitian 

ini  dilakukan 
menggunakan  software  RapidMiner  7.5  dan  metode 
pengujian k-fold validation sesuai pengaturan pada Tabel 2 
berikut: 

TABEL II. 

Algoritma 
Support vector 
machine 

Nilai 

PARAMETER EVALUASI PENELITIAN 
Parameter 
Metode validasi 
Matriks pengujian 
Jumlah data 
Tool 
Kriteria output yang 
dihasilkan 
Sampling type 

k-Fold 
Confusion Matrix 
15000 data 
RapidMiner 7.5 
Accuracy dan 
Classification 
Shuffled sampling 

V.  KERANGKA PIKIR 

Penelitian  ini  disusun  atas  latar  belakang  yang  telah 
disampaikan  untuk  mencapai  tujuan  yang  diharapkan 
dengan menggunakan metode serta analisis terukur sehingga 
dapat  ditarik  kesimpulan  yang 
terukur  serta  dapat 
dimanfaatkan  kedepannya,  kerangka  pikir  penelitian 
disajikan pada bagan dibawah ini: 

 4 / 8 

 
 
 
 
 
 
 
 
 
Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik 

TABEL III. 
TOP FREQUENT TERMS TWEET DEMOKRAT 

Kata 

Demokrat 
Partai 
moeldoko 
Klb 
Dan 
Ahy 
di 
pd 
penyelamat 
ini 

Frekuensi 

13891 
7370 
5977 
4521 
4255 
3861 
3601 
3439 
2279 
2145 

TABEL IV. 
TOP FREQUENT TERMS TWEET KPK 

Kata 

Frekuensi 

kpk 
pegawai 
kpkri 
twk 
lantik 
negara 
bangsa 
rakyat 
pancasila 
asn 

12625 
3064 
2901 
2193 
1711 
1596 
1072 
1067 
1040 
1033 

Kata  Demokrat  dan  KPK  menjadi  kata  yang  paling 
sering  muncul  dengan  frekuensi  kemunculan  yang  cukup 
jauh  dari  kata  yang  berada  di  posisi  kedua  pada  masing-
masing kata kunci. Hal ini dapat disebabkan karena kedua 
kata  tersebut  merupakan kata kunci  yang digunakan untuk 
mengumpulkan  data  pada  penelitian  ini.  Dalam  masing-
masing  top  frequent  term  didominasi  oleh  kata-kata  yang 
berhubungan dengan kejadian yang melatar belakangan kata 
kunci  bersangkutan  menjadi  trending  topik.  Diluar  hal  itu 
terdapat  beberapa  kata  seperti  “dan”,  “di”,  dan  “ini”  yang 
tidak berhubungan langsung dengan kejadian yang melatar 
belakangi kata kunci,  namun  merupakan  kata  hubung atau 
konjungsi yang kerap digunakan oleh masyarakat Indonesia. 
Dari list kata pada Tabel 5 dan Tabel 6 kemudian dapat 
di visualisasikan dalam bentuk diagram batang pada gambar 
8 berikut: 

1
9
8
3
1

5
2
6
2
1

0
7
3
7

7
7
9
5

I
S
N
E
U
Q
E
R
F

K
P
K

I

A
T
R
A
P

T
A
R
K
O
M
E
D

O
K
O
D
L
E
O
M

1
2
5
4

5
5
2
4

1
6
8
3

1
0
6
3

9
3
4
3

4
6
0
3

1
0
9
2

9
7
2
2

3
9
1
2

5
4
1
2

1
1
7
1

6
9
5
1

2
7
0
1

7
6
0
1

0
4
0
1

3
3
0
1

B
L
K

N
A
D

Y
H
A

I

D

D
P

I

I

R
K
P
K

A
W
A
G
E
P

…
M
A
L
E
Y
N
E
P

I

N

I

K
W
T

K

I
T
N
A
L

A
R
A
G
E
N

A
S
G
N
A
B

T
A
Y
K
A
R

N
S
A

A
L
I
S
A
C
N
A
P

GAMBAR 4. GRAFIK TOP FREQUENT TERMS KEDUA DATA 

Diagram batang tersebut memvisualisasikan antara top 
frequent  terms  dengan  nilai  frekuensinya.  Semakin  tinggi 
batang  pada  diagram  mengartikan  semakin  sering  kata 

 5 / 8 

Gambar 3. Bagan Kerangka Pikir 

VI. HASIL DAN PEMBAHASAN 

6.1 Pengumpulan Data 

Proses  scrapping  menghasilkan  data  sebanyak  17253 
data untuk kata kunci Demokrat dan 17681 data untuk kata 
kunci  KPK.  Masing-masing  kata  kunci  dikumpulkan  pada 
rentang waktu yang berbeda yaitu pada tanggal 5-11 maret 
2021 untuk untuk kata kunci demokrat dan rentang waktu 26 
mei  hingga  2  juni  2021.  Berikut  analisis  deskriptif  yang 
dilakukan  pada  penelitian  inii  guna  melihat  karakteristik 
data  secara  keseluruhan,  analisis  deskriptif  dilakukan 
dengan  menggunakan  Microsoft  Excel  guna  melihat 
karakteristik  data  secara  keseluruhan,  analisis  deskriptif 
dilakukan dengan menggunakan Microsoft Excel.  

Dari data yang telah dikumpulkan dapat diketahui 
kata-kata yang sering muncul dalam tweet yang membahas 
topik Demokrat atau KPK. Berikut ini  merupakan 10 kata 
yang  paling  sering  muncul  dalam  data 
tweet  yang 
dikumpulkan beserta frequensinya pada masing-masing kata 
kunci: 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
tersebut  muncul.  Perbedaan  warna  pada  batang  diagram 
menandakan  perbedaan  asal  kata  kunci  kata  tersebut 
diambil.  Warna  merah  menandakan  data  berasal  dari  kata 
kunci  KPK  dan  warna  biru  menandakan  data  berasal  dari 
kata kunci demokrat: 

6.2 Lang Detect 

Melihat data hasil scraping twitter yang belum 

sempurna dalam memfilter tweet berdasarkan bahasa maka 
dilakukan pendeteksi bahasa asing. Pendeteksian bahasa 
dilakukan pada google colab dengan menggunakan bahasa 
pemrograman python, dengan menggunakan package 
langdetect.  

Setelah tahapan pendeteksian Bahasa selesai dilakukan 
didapatkan  data  beserta  pengklasifikasian  bahasanya  yang 
dapat  unduh  dalam  format  csv.  Selanjutnya  pada  fail  csv 
dapat  dihapus  data  Tweet  yang  tidak  tergolong  Bahasa 
Indonesia.  Pada  proses  penghapusan  data  ini  juga  peneliti 
melakukan penghapusan data tweet yang kosong. Hasil dari 
proses penghapusan data menghasilkan tersisa 15785 tweet 
pada kata kunci demokrat dan tersisa 15658 tweet pada kata 
kunci KPK. 

6.3 Preprocessing 

Setelah mendapatkan data yang dibutuhkan, selanjutnya 
data  harus  melalui  tahapan  preprocessing  sebelum  dapat 
dilakukan analisis. Tujuannya adalah untuk menghilangkan 
noise  tersebut  sehingga  dapat  meningkatkan  kualitas  dari 
data  yang  didapatkan.  Tahapan  preprocessing  yang 
dilakukan seperti berikut: 

GAMBAR 5. BAGAN PREPROCESSING 

Contoh  tweet  (data  mentah):  [AHY  akan  dilaporkan 
kepolisi  karena  diduga  telah  memalsukan  akta  AD/ART 
2020  khususnya  mengubah  mukadimah  dari  pendirian 
partai,  kata  Sekjen  Demokrat  Moeldoko,  Jhoni  Allen 
Marbun.  @agusyudhoyono  Moeldoko  Penyelamat  PD 
#TitaniDemokratSBY] 

Langkah  1:  penghapusan  karakter  selain  huruf  yang 
dapat berupa link URL, username, hashtag, tanda baca, dan 
angka. Selain itu juga dilakukan trim untuk menghilangkan 
spasi ganda atau spasi pada awal dan akhir  Tweet. Contoh 
hasil  data  setelah  melaui  tahapan  cleaning  dan  symbol 
handling:  [AHY  akan  dilaporkan  kepolisi  karena  diduga 
telah  memalsukan  akta  ADART  khususnya  mengubah 
mukadimah  dari  pendirian  partai  kata  Sekjen  Demokrat 

Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik 

Moeldoko Jhoni Allen Marbun. agusyudhoyono Moeldoko 
Penyelamat PD TiraniDemokratSBY] 

Langkah 2: mengkonversi Tweet menjadi lower case. 
Mengecilkan huruf kapital yang termuat dalam data Tweet. 
Contoh hasil data setelah melalui tahapan case folding: [ahy 
akan  dilaporkan  kepolisi  karena  diduga  telah  memalsukan 
akta adart khususnya mengubah mukadimah dari pendirian 
partai  kata  sekjen  demokrat  moeldoko  jhoni  allen  marbun 
agusyudhoyono 
pd 
tiranidemokratsby] 

penyelamat 

moeldoko 

Langkah 3: pembagian teks dari yang berupa kalimat, 
paragraf atau dokumen, menjadi token-token/bagian-bagian. 
Contoh  data  setelah  melalui  tahapan  tokenisasi:  [‘ahy’, 
‘akan’,  ‘dilaporkan’,  ‘kepolisi’,  ‘karena’,  ‘diduga’,  ‘telah’, 
‘memalsukan’, ‘akta’, ‘ad’, ‘art’, ‘khususnya’, ‘mengubah’, 
‘mukadimah’,  ‘dari’,  ‘pendirian’,  ‘partai’,  ‘kata’,  ‘sekjen’, 
‘marbun’, 
‘demokrat’, 
‘agusyudhoyono’, 
‘pd’, 
‘tiranidemokratsby’] 

‘allen’, 
‘penyelamat’, 

‘moeldoko’, 

‘moeldoko’, 

‘jhoni’, 

Langkah 4: penormalan kata tidak baku yang ada pada 
data  diganti  menjadi  kata  yang  lebih  baku,  mengbah  kata 
slang menjadi kata yang lebih umum, mengubah singkatan, 
dan  menghilangkan  huruf  ganda  pada  kata.  Contoh  hasil 
data  setelah  melalui  tahapan  normalisasi:  [‘ahy’,  ‘akan’, 
‘telah’, 
‘dilaporkan’, 
‘memalsukan’, 
‘khususnya’, 
‘mengubah’,  ‘mukadimah’,  ‘dari’,  ‘pendirian’,  ‘partai’, 
‘kata’,  ‘sekjen’,  ‘demokrat’,  ‘moeldoko’,  ‘jhoni’,  ‘allen’, 
‘marbun’,  ‘agusyudhoyono’,  ‘moeldoko’,  ‘penyelamat’, 
‘pada’, ‘tiranidemokratsby’] 

‘kepolisi’, 
‘akta’, 

‘karena’, 
‘ada’, 

‘diduga’, 

‘art’, 

[‘ahy’, 

Langkah  5:  menghapus  stopword  atau  kata-kata 
umum.  Contoh  hasil  data  setelah  melalui  tahapan  stopwor 
‘diduga’, 
removal: 
‘memalsukan’,  ‘art’,  ‘mengubah’,  ‘pendirian’,  ‘partai’, 
‘sekjen’, ‘demokrat’, ‘moeldoko’, ‘jhoni’, ‘allen’, ‘marbun’, 
‘agusyudhoyono’, 
‘penyelamat’, 
‘titranidemokratsby’] 

‘dilaporkan’, 

‘moeldoko’, 

‘kepolisi’, 

Langkah  6:  mengembalikan  kalimat  kedalam  bentuk 
sederhananya  tanpa  imbuhan.  Contoh  hasil  data  setelah 
melalui  tahapan  stemming:  [‘ahy’,  ‘lapor’,  ‘polis’,  ‘duga’, 
‘palsu’,  ‘art’,  ‘ubah’,  ‘diri’,  ‘partai’,  ‘sekjen’,  ‘demokrat’, 
‘moeldoko’,  ‘jhoni’,  ‘allen’,  ‘marbun’,  ‘agusyudhoyono’, 
‘moeldoko’, ‘selamat’, ‘tiranidemokratsby’] 

Pada penelitian ini dilakukan dua metode preprocessing 
yaitu  menggunakan  algoritma  python  dan  operasi  pada 
rapidminer 7.5 

6.4 Klasifikasi 

Proses  klasifikasi  merupakan  tahapan  analisis 
menggunakan  metode  Support  vector  machine  dan  Naïve 
bayes  dalam  mengelompokkan  tweet  kedalam  kelas  tweet 
spam atau kelas tweet bukan spam. Masing-masing metode 
analisis  akan  melakukan  pengelompokkan  data  yang  telah 
melalui preprocessing menggunakan algoritma  python dan 
operator rapidminer. Analisis akan dilakukan menggunakan 
software  rapidminer,  yang  hasilnya  akan  dibandingkan 
dengan data yang telah dilakukan pelabelan secara manual. 
untuk  melakukan 
rapidminer 
Hasil 

konfigurasi 

 6 / 8 

 
 
 
 
 
 
 
 
pengklasifikasian  kelas  ditampilkan  pada  gambar  6  dan 
gambar 7 berikut:  

Gambar 6. Konfigurasi klasifikasi metode SVM 

Gambar 7. Konfigurasi klasifikasi metode NB 

Konfigurasi  metode  klasifikasi  yang  dibangun  pada 
software  rapidminer  akan  mengklasifikasikan  data  tweet 
kedalam  kelas  tweet  spam  dan  kelas  tweet  bukan  spam. 
Setiap  metode  klasifikasi  dijalankan  sebanyak  empat  kali 
masing-masing  untuk  melihat  pengaruhnya  terhadap  kata 
kunci data tweet yang digunakan dan metode preprocessing 
yang diberikan terhadap data. Hasil pengklasifikasian tweet 
menggunakan  analisis  SVM  dan  NB  terhadap  masing-
masing hasil preprocessing pada kedua kata kunci disajikan 
pada tabel 5 berikut: 

TABEL V. 
TOP FREQUENT TERMS TWEET KPK 

Kata 
Kunci 

Metode Preprocessing 

Demokrat 

KPK 

Algoritma Python 
Operator RapidMiner 
Algoritma Python 
Operator RapidMiner 

Support vector 
machine 

Naïve bayes 

Spam 

12251 
13157 
11215 
13499 

Bukan 
Spam 
3530 
2724 
4441 
2157 

Spam 

11612 
12884 
10793 
13080 

Bukan 
Spam 
4169 
2897 
4863 
2576 

kedua  metode 

Dari hasil pengklasifikasian pada tabel 5 dapat dilihat 
bahwa 
pengklasifikasian  memiliki 
kecedenderungan  yang  sama  dalam  mengklasifikasikan 
kedua  data  pada  masing-masing  metode  preprocessing, 
lebih  banyaknya  data  yang 
dimana  menunjukan 
digolongkan  dalam  kelas  spam  dibandingkan  kelas  bukan 
Spam  pada  setiap  kondisi.  Dari  data  juga  dapat  dilihat 
kemiripan  kedua  metode  klasifikasi  terhadap  banyaknya 
data  yang  digolongkan  pada  suatu  kelas.  Misalnya  pada 
kelas spam dapat dilihat baik metode Naïve bayes maupun 
support  vector  machine  sama-sama  menunjukan  data  dari 
kata  kunci  KPK  yang  di  preprocessing  menggunakan 
operator  rapidminer  menunjukan  angka  spam  tertinggi 
disusul  oleh  data  dari  kata  kunci  demokrat  yang  di 
preprocessing  juga  menggunakan  operator  rapidminer. 
Sedangkan  data  yang  melalui  preprocessing  dengan 
algoritma python masing-masing menjadi terbanyak ketiga 
dan keempat untuk kata kunci demokrat dan KPK. 

6.5 Manual Labelling 

Cara 

yang 

dilakukan 
pengelompokan  data  yang  duplikasi  yang  kemudian 
keseluruhan  data  yang  memiliki  duplikat  diberikan  label 

dilakukan 

pertama 

Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik 

tweet  spam.  Pada  data  yang  tidak  memiliki  duplikat 
kemudian  di  satu  per  satu  apakah  memenuhi  sifat  seperti 
mengandung  kata  bermakna  aneh,  kesalahan  pada 
gramatikal  kalimat,  kalimat  yang  tidak  rasional,  tidak 
relevannya  komentar  dengan 
topik  pembicaraan,  dan 
umumnya  bersifat  sangat  general,  apabila  memenuhi  sifat 
tersebut  maka  data  diberi  label  tweet  spam  dan  data  yang 
tidak  memenuhi  sifat  tersebut  diberikan  label  tweet  bukan 
spam.  Contoh  tweet  yang  dilabeli  spam  dapat dilihat  pada 
ilustrasi pada Gambar 1. Berikut ditampilkan hasil pelabelan 
pada kedua data pada tabel 6: 

TABEL VI. 
HASIL MANUAL LABELLING 

Kata Kunci 

Demokrat 

KPK 

Label 

Tweet spam 
Tweet Bukan Spam 
Tweet spam 
Tweet Bukan Spam 

Frekuensi 
13257 
2425 
11898 
3758 

Total 

15781 

15658 

6.5 Evaluasi 

labelling  manual.  Dalam  proses 

Tahapan evaluasi dilakukan pada software rapid miner 
setelah  seluruh  data  yang  dimiliki  dilakukan  klasifikasi 
dengan  metode  naïve  bayas,  metode  Support  vector 
machine,  dan 
ini 
pengelompokan data dengan labelling manual akan menjadi 
pembanding  dari  data  yang  diklasifikasikan  menggunakan 
metode Naïve bayes dan Support vector machine. Data yang 
terdiri dari dua kata kunci, dilanjutkan dengan dua metode 
preprocessing  untuk  masing-masing  kata  kunci  dan 
dilakukan  masing-masing  dua  analisis  memungkinkan 
terjadinya  delapan  kombinasi  berbeda  pada  penelitian  ini, 
sehingga terdapat delapan confusion matrix yang terbentuk.  
Didapatkannya nilai dari cunfusion matrix dari seluruh 
kondisi  memungkinkan  untuk  dilakukan  perhitungan 
akurasi,  recall,  presisi,  dan  f-measure.  Hasil  perhitungan 
akurasi  dan  f-measure  pada  setiap  kondisi  disajikan  pada 
tabel 7 berikut: 

TABEL VII. 
HASIL PERHITUNGAN CONFUSION MATRIX 

Kata Kunci 

Metode 
Preprocessing 

Demokrat 

KPK 

Python 

RapidMiner 

Python 

RapidMiner 

Metode 
Klasifikasi 
SVM 
NB 
SVM 
NB 
SVM 
NB 
SVM 
NB 

Accuracy 

F-measure 

88.847% 
84.266% 
77.117% 
74.951% 
84.894% 
82.454% 
70.561% 
66.594% 

93.100% 
90.016% 
86.242% 
84.878% 
89.768% 
87.894% 
81.852% 
79.062% 

Dari hasil perhitungan nilai akurasi, recall, presisi, dan 
f-measure  pada  setiap  kondisi,  dapat  dilihat  bahwa 
kombinasi  Antara  metode  preprocessing  menggunakan 
algoritma  python  dan  metode  klasifikasi  support  vector 
machine  menjadi  kombinasi  dengan  nilai  akurasi  dan  f-
measure  tertinggi  pada  masing-masing  kata  kunci  dengan 
akurasi  88.847%  dan  f-measure  93,100%  pada  kata  kunci 
demokrat dan akurasi 84,894% dan f-measure 89,768% pada 
kata  kunci  KPK.  Diikuti  dengan  kombinasi  antara  metode 
preprocessing  algoritma  python  dan  metode  klasifikasi 
Naïve  bayes  yang  memiliki  nilai  akurasi  dan  f-measure 

 7 / 8 

 
 
 
 
 
 
 
 
 
 
tertinggi kedua pada masing-masing kata kunci yaitu sebesar 
84,266% dan 90,016 untuk nilai akurasi dan f-measure pada 
kata  kunci  demokrat  dan  sebesar  82,454%  dan  87,894% 
untuk nilai akurasi dan f-measure pada kata kunci KPK. 

Kemudian dengan nilai akurasi dan f-measure tertinggi 
ketiga  yaitu  kombinasi  antara  metode  Preprocessing 
operator  rapidminer  dan  metode  analisis  support  vector 
machine  yaitu  sebesar  77,117%  dan  86,242%  untuk  nilai 
akurasi dan f-measure pada kata kunci demokrat dan sebesar 
70,561%  dan  81,852%  untuk  nilai  akurasi  dan  f-measure 
pada kata kunci KPK. Dengan nilai akurasi dan  f-measure 
antara  metode 
terendah  merupakan 
preprocessing rapidminer dan metode analisis Naïve bayes 
yang  hanya  memiliki  akurasi  dan 
f-measure  sebesar 
74,951%,  84,878%  dan  66,594%,  79,062%  untuk  masing-
masing kata kunci demokrat dan KPK. 

kombinasi 

 Hasil perhitungan akurasi dan f-measure setiap kondisi 

ini dapat divisualisasikan seperti pada gambar 8 berikut: 

S V M

N B

S V M

N B

S V M

N B

S V M

N B

P Y T H O N

R A P I D M I N E R

P Y T H O N

R A P I D M I N E R

D E M O K R A T

K P K

Acuracy

F-Measure

Gambar 8. Grafik Nilai Akurasi dan F-measure 

Dari hasil perhitungan nilai akurasi dan f-measure 
setiap kondisi diatas dapat dilihat juga pada kata kunci dan 
metode  preprocessing  yang  sama,  metode  klasifikasi 
Support  vector  machine  memiliki  nilai  akurasi,  recall, 
presisi, dan f-measure yang lebih tinggi dibandingkan nilai 
akurasi,  recall,  presisi,  dan 
f-measure  pada  metode 
klasifikasi Naïve bayes pada setiap kondisi. Selain itu pada 
kata  kunci  dan  metode  klasifikasi  yang  sama,  penggunaan 
algoritma  python  pada  preprocessing  menghasilkan  nilai 
akurasi,  recall,  presisi,  dan  f-measure  yang  lebih  tinggi 
dibandingkan  nilai  akurasi,  recall,  presisi,  dan  f-measure 
pada  kata  kunci  dan  metode  klasifikasi  yang  sama  namun 
dengan  metode 
proses 
preprocessing. 

rapidminer 

operasi 

pada 

VII. PENUTUP 

Dari  penelitian  yang  dilakukan  dapat  disimpulkan 
bahwa  data  tweet  yang  didapatkan  dari  proses  scrapping 
cenderung  didominasi  oleh  kata-kata  yang  berhubungan 
dengan kata kunci yang berkaitan dan kata-kata umum atau 
konjungsi. Selain itu dari hasil perhitungan yang dilakukan 
dapat  ditarik  kesimpulan  bahwa  metode  preprocessing 
lebih  efektif  dalam 
menggunakan  algoritma  python 
melakukan 
program 
rapidminer.  Hal  ini  dibuktikan  dari  hasil  penelitian  yang 
mana  pada  data  dan  metode klasifikasi  yang  sama  metode 
preprocessing algoritma python memiliki nilai akurasi yang 
tinggi  yaitu  berkisar  diantara  82,454%  hingga 
lebih 

preprocessing 

dibandingkan 

Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik 

pengklasifikasian 

dalam  melakukan 

88,847%,  dibandingkan  metode  operator  rapidminer  yang 
hanya  memiliki  nilai  akurasi  diantara  66,594%  hingga 
77,117%. Kesimpulan berikutnya yang dapat diambil adalah 
metode  pengklasifikasian  Support  vector  machine  lebih 
efektif 
Spam 
dibandingkan  metode  Naïve  bayes, hal ini ditunjukan oleh 
nilai  akurasi  pada  metode  pengklasifikasian  SVM  lebih 
tinggi  yaitu  berkisar  antara  70,561%  hingga  88,847% 
dibandingkan  metode  NB  menghasilkan  nilai  akurasi 
diantara  66,594%  hingga  84,266%  dan  pada  data  dan 
metode  preprocessing  yang  sama  metode  SVM  selalu 
memiliki  nilai  akurasi  dan  f-measure  yang  lebih  tinggi 
dibandingkan  metode  NB.  Dari  seluruh  kombinasi 
penggunaan  metode  preprocessing  dan  metode  klasifikasi 
dapat disimpulkan  kombinasi antara  metode prerpocessing 
menggunakan algoritma python dan metode klasifikai SVM 
memberikan nilai akurasi tertinggi pada masing-masing data 
yaitu sebesar 88,847% dan 84,894% 

DAFTAR PUSTAKA 
[1] Hidayat,  T.  (2019).  Trend  Teknologi  R evolusi  Industri  4.0. 
Diakses 4 Juni 2021, dari Universitas Djuanda Bogo r.  
[2] Beananda,  L.  (2019).  Mengenal  lebih  jauh  Revolusi  Industri  4.0. 

Diakses 4 Juni 2021, dari Universitas Bina Nusantara Jakarta. 

[3]  Asur, Sitaran dan Bernardo A. Huberman. Predicting The Future With 
Sosial  Media.  Proceeding  WI-IAT  ‘10  Proceeding  of  the  2010 
IEEE/WIC/ACM  International  Conference  on  Web  Intelligence  and 
Intelligent Agent Technology. Volume 01. Halaman 492-499. 2010. 
[4]  McCord  M  and  Chuah  M.  2011.  Spam  detection  on  Twitter  using 
traditional classifier. Di dalam: M Jose, editor. Autonomic and Trusted 
Computing:  8th  International  Conference;  Banff,  2011  Sep  2-4.  San 
Francisco (US): Springer. hlm: 175-186. 

[5]  Benevenuto,  Fabricio,  dkk.  2010.  Detecting  Spammers  on  Twitter. 
Universidade Federal de Minas Gerais, Belo Horizonte (TOMCCAP), 
5(4):1–25, 2009 

[6]  Wartaekonomi.co.id.  (2021,  3).  Pemerintah  Tolak  KLB  Demokrat 
Kubu Moeldoko, ""Kami Bersama AHY"" Trending di Twitter [Online]. 
Available:https://www.wartaekonomi.co.id/read334788/pemerintah-
tolak-klb-demokrat-kubu-moeldoko-kami-bersama-ahy-trending-di-
Twitter. 

[7]  Ajie  Poetra,  D,  Sistem  Prediksi  Spam  Account  Pada  Media  Sosial 
Twitter Dengan Menggunakan Algoritma C4.5. Universitas Pendidikan 
Indonesia, Bandung. 2016. 

[8]  P. Calais, D. Pires, D. Guedes, J. W. Meira, C. Hoepers, and K. Steding-
Jessen. A campaign-based characterization of Spamming strategies. In 
Conference on e-mail and anti-Spam (CEAS), 2008. 

[9]  Thomason. Blog Spam: A review. In Conference on Email and Anti-

Spam (CEAS), 2007 

[10] Statistical analysis to locate Spam web pages. In Int’l Workshop on the 

Web and Databases (WebDB), 2004. 

[11] F.  Benevenuto,  T.  Rodrigues,  V.  Almeida,  J.  Almeida, and  K.  Ross. 
Video interactions in online video sosial networks. ACM Transactions 
on Multimedia Computing, Communications and Applications 

[12] R. Chrismanto, Y. Lukito. Identifikasi Komentar Spam pada Instagram. 

Universitas Kristen Duta Wacana, Yogyakarta, 2017. 

[13] L  Satrionugroho.  Deteksi  Spammer  di  Twitter  menggunakan  metode 

Naive bayes. Universitas Telkom, Bandung 

[14] Fieldman,  R.  dan  Sanger,  J.  (2017).  The  teks  Mining  Handbook 
Advanced  Approaches  in  Analyzing  Unstructed  Data.  Cambridge 
Univercity Press. New York, United state of America. 

[15] Ni’matul  Maula,  S.  2019.  Kajian  Perbandingan  Algoritma 
Sentistrength  dan  Naïve  bayes  Analisis  Sentimen  Twitter.  Politeknik 
Statistika STIS. Jakarta 

[16] Nikmatun  Aliyah  Salsabila,  2018,  Kamus  Alay.  Website: 
https://github.com/nasalsabila/kamus-alay/blob/master/colloquial-
indonesian-lexicon.csv, diakses tanggal 21 maret 2021 

[17] Adikara  Putra  Pandu,  2012,  Kamus  Kata  Dasar  dan  Stopword  List 
Bahasa 
Website: 
Indonesia. 
http://static.hikaruyuuki.com/wpcontent/uploads/stopword_list_tala.tx
t, diakses tanggal 21 maret 2021 

 8 / 8 

 
 
 
 
 
"
221709848,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pembangunan Sistem Penarikan Sampel Pengganti 
dan Sampel Tambahan Survei Ubinan Komoditas 
Palawija Berbasis Web 

Muhammad Azmil Mubarrok (221709848, 4SI1) 
Dosen Pembimbing: Firdaus, M.B.A. 

Ringkasan—  Survei  Ubinan  Tanaman  Pangan  Komoditas 
Palawija  merupakan  salah  satu  survei  yang  dilaksanakan  oleh 
BPS  RI  dengan  tujuan  memperoleh 
informasi  mengenai 
produktivitas  tanaman  pangan  komoditas  palawija.  Dalam 
pelaksanaan survei terdapat beberapa kondisi dimana kerangka 
sampel  yang  telah  terpilih  menjadi  sampel  harus  mengalami 
penggantian  maupun  penambahan  sampel.  Proses  penggantian 
dan  penambahan  sampel  ini  dinilai  belum  efisien  dan  rentan 
terjadi human error baik dalam proses pemilihan usulan sampel, 
pengajuan  dan  pengiriman  dokumen,  hingga  penulisan  script 
untuk  memeriksa  usulan  sampel.  Untuk  mengatasi  masalah 
tersebut  peneliti  merancang  dan  membangun  sistem  penarikan 
sampel pengganti dan sampel tambahan survei ubinan komoditas 
palawija  berbasis  web  deengan  metode  Rapid  Application 
Development  (RAD).  Evaluasi  terhadap  sistem  yang  dibangun 
dilakukan  menggunakan  metode  black  box  testing  dan  System 
Usability Scale (SUS). Hasil evaluasi menunjukkan bahwa sistem 
yang  dibangun  dapat  diterima  dengan  baik  dan  sesuai  dengan 
kebutuhan pengguna. 

Kata  Kunci—  survei  ubinan,  palawija,  penarikan  sampel, 

pengajuan sampel, sistem. 

I.  LATAR BELAKANG 

Pangan  merupakan  kebutuhan  dasar  utama  bagi  manusia 
yang  harus  dipenuhi  setiap  saat.  Ketersediaan  pangan  dalam 
jumlah yang cukup, mutu bahan pangan yang baik, serta nilai 
gizi  yang  tinggi  memiliki  dampak  cukup  signifikan  pada 
perekonomian  dan  mutu  sumber  daya  manusia.  Sektor 
Pertanian merupakan sektor yang berkaitan erat dengan pangan, 
dan salah satu subsektor  pertanian yang mempunyai peranan 
yang  sangat  strategis  adalah  subsektor  tanaman  pangan. 
Peranan  penting  subsektor  tanaman  pangan  adalah  dalam  hal 
pemenuhan kebutuhan pangan masyarakat Indonesia [1]. 

Mengingat  peranan  penting  subsektor  tanaman  pangan, 
informasi mengenai produksi tanaman pangan yang akurat dan 
mampu  menggambarkan  kondisi  terkini  sangat  dibutuhkan. 
Untuk memenuhi kebutuhan akan data tanaman pangan, Badan 
Pusat Statistik Republik Indonesia (BPS RI) sebagai lembaga 
yang  bertanggung  jawab  atas  kegiatan  statistik  di  Indonesia 
bekerja  sama  dengan  Direktorat  Jenderal  Tanaman  Pangan, 
Kementerian Pertanian melaksanakan kegiatan Survei Ubinan 
Tanaman Pangan [2]. 

Survei ubinan tanaman pangan bertujuan untuk memperoleh 
informasi mengenai produktivitas komoditas tanaman pangan 
(hasil  per  hektar)  dan  informasi  pendukung  lainnya  seperti 
koordinat  lokasi  ubinan,  cara  penanaman,  sistem  penanaman 
jajar  legowo  (khusus  padi),  penggunaan  pupuk,  dan  lain-lain 

[1]. Pengumpulan data produktivitas survei ubinan mencakup 
seluruh wilayah Indonesia dan terdiri dari dua komoditas, yaitu 
komoditas  padi  dan  komoditas  palawija.  Fokus  penelitian  ini 
adalah  Survei  Ubinan  Tanaman  Pangan  Komoditas  Palawija 
(SUBP). 

rumah 

Kerangka  sampel  yang  digunakan  dalam  survei  ubinan 
komoditas palawija ada dua jenis, yaitu kerangka sampel blok 
tangga. 
sensus  dan  kerangka  sampel  berbasis 
Selanjutnya kerangka sampel yang dimaksud dalam penelitian 
ini  adalah  kerangka  sampel  blok  sensus.  Kerangka  sampel 
tersebut dibangun dari kecamatan yang memiliki informasi luas 
panen  palawija  hasil  pendataan  Survei  Pertanian  tahun 
sebelumnya  [1].  Informasi  mengenai  kerangka  sampel  yang 
didapatkan  oleh  petugas  pencacah  di  lapangan  diserahkan 
kepada 
kemudian 
pengawas/pemeriksa 
diserahkan  kepada  Kepala  Seksi  Statistik  Produksi  BPS 
kabupaten/kota. 

petugas 

yang 

Entri data hasil pendataan oleh petugas lapangan dilakukan 
di  BPS  kabupaten/kota,  sedangkan  untuk  penarikan  sampel 
blok sensus dilakukan di BPS RI yang dalam hal ini diwakilkan 
oleh  Subdirektorat  Pengembangan  Kerangka  Sampel  (Subdit 
PKS)  berdasarkan  data  yang  didapatkan  dari  entri  data  pada 
setiap  kabupaten/kota.  Proses  penarikan  sampel  oleh  Subdit 
PKS  dibantu  oleh  beberapa  aplikasi  seperti  R  Studio  dan 
Microsoft Visual Foxpro. Aplikasi R Studio digunakan untuk 
melakukan penyusunan kerangka sampel dan penarikan sampel, 
sedangkan aplikasi Microsoft Visual Foxpro digunakan untuk 
menyusun  dokumen  dari  daftar  sampel  terpilih  yang  meliputi 
Daftar Sampel Blok Sensus (DSBS) dan Pemutakhiran Daftar 
Rumah  Tangga  Survei  Ubinan  Tanaman  Pangan  (Dokumen 
SUB-P). 

Setelah  penarikan  sampel  dan  pengiriman  dokumen  DSBS 
serta  dokumen  SUB-P  oleh  Subdit  PKS,  selanjutnya  BPS 
identifikasi  dan  pencacahan 
kabupaten/kota  melakukan 
berdasarkan  sampel  yang 
terpilih.  Selama  proses 
telah 
identifikasi,  terdapat  beberapa  permasalahan  yang  terjadi  di 
lapangan  seperti  kejadian  alam  tak  terduga,  alih  fungsi  blok 
sensus,  biaya  transportasi  besar,  objek  yang  diteliti  tidak  ada 
(sudah  dipanen  terlebih  dahulu  oleh  petani  sehingga  petugas 
tidak  bisa  mendapatkan  informasi),  dan  alasan  lainnya.  Oleh 
karena 
itu  dibutuhkan  usulan  sampel  pengganti  untuk 
menggantikan  sampel  terpilih  yang  bermasalah.  Selain  untuk 
mengganti  sampel,  identifikasi  diperlukan  untuk  mengetahui 
apakah memerlukan usulan penambahan sampel. 

 1 / 8 

 
 
 
 
Usulan sampel pengganti dan sampel tambahan didapatkan 
oleh  BPS  kabupaten/kota  berdasarkan  hasil  pengamatan  atau 
identifikasi  dari  lapangan.  Usulan  sampel  tersebut  diajukan 
kepada BPS provinsi dengan menggunakan template pengajuan 
yang  disediakan  Subdit  PKS  dalam  format  Microsoft  Excel 
beserta dengan surat pengajuan usulan sampel yang kemudian 
dikirim  melalui  surat  elektronik  (surel).  BPS  provinsi 
bertanggung  jawab  untuk  mengumpulkan  dokumen  template 
dan  surat  yang  diterima  dari  BPS  kabupaten/kota  untuk 
diajukan  dan  dikirimkan  melalui  surel  kepada  Subdit  PKS. 
Proses  pengajuan  usulan  sampel  dapat  dilakukan  berkali-kali 
bergantung kebutuhan dari BPS kabupaten/kota.  

Subdit  PKS  bertanggung  jawab  untuk  memeriksa  usulan 
sampel  tersebut  apakah  tersedia  pada  frame  kerangka  sampel 
Subdit PKS dan belum terpilih menjadi sampel pada subround 
yang berjalan. Jika kedua syarat tersebut terpenuhi maka usulan 
sampel dapat disetujui menjadi sampel pengganti atau sampel 
tambahan.  Subdit  PKS  juga  harus  mengirimkan  dokumen 
DSBS  dan  dokumen  SUB-P  sampel  pengganti  dan  sampel 
tambahan  kepada  masing-masing  BPS  provinsi  dan  BPS 
kabupaten/kota. 

Dari  proses  pengajuan  sampel  yang  telah  dijabarkan, 
didapatkan  beberapa  permasalahan  dalam  proses  bisnisnya, 
antara lain : 
1.  Pemilihan usulan sampel oleh BPS kabupaten/kota. 

Tidak terdapat jaminan bahwa usulan sampel terpilih 
secara  acak  karena  usulan  sampel  hanya  berdasarkan 
pengamatan  oleh  petugas 
tidak  ada 
pengawasan selama proses penarikan sampelnya. 

lapangan  dan 

2.  Pengajuan usulan sampel. 

Ketika  melakukan  pengiriman  surat  dan  dokumen 
pengajuan sampel melalui surel, baik BPS kabupaten/kota 
maupun  BPS  provinsi  tidak  dapat  mengetahui  status 
perkembangan dari pengajuan apakah sudah diproses atau 
belum.  Begitu  juga  BPS  RI  dan  BPS  provinsi  yang 
kesulitan  dalam  mengontrol  mana  pengajuan  yang  sudah 
diproses  atau  belum.  Selain 
itu,  proses  pengajuan 
menggunakan  surel  ini  memakan  cukup  banyak  waktu 
untuk  membuka,  memeriksa,  dan  mengunduh  dokumen 
pengajuan  serta  bisa  saja  surel  pengajuan  tidak  terbaca 
diakibatkan masuknya surel lain yang tidak berhubungan 
dengan  pengajuan  dan  ditambah  proses  pengajuan  tidak 
hanya dilakukan satu kali dalam setiap subround sehingga 
menambah  kemungkinan  surel  bertumpuk  dan  rentan 
terjadi human error. 

3.  Pemeriksaan usulan sampel oleh Subdit PKS. 

Subdit  PKS  melakukan  pemeriksaan  usulan  sampel 
menggunakan  aplikasi  R  Studio  dengan  menuliskan  atau 
memodifikasi  script  pada  aplikasi.  Penulisan  script 
tersebut dilakukan berulang-ulang setiap penarikan sampel 
pengganti dan sampel tambahan. Hal tersebut dinilai tidak 
efisien karena memakan banyak waktu untuk menuliskan 
script dan memverifikasi dokumen dari pengajuan usulan 
sampel. Kegiatan menuliskan script secara berulang-ulang 
seharusnya  dapat  digantikan  oleh  program  yang  dapat 
berjalan  secara  otomatis,  hal 
juga  dapat 
meminimalisasi  kesalahan  yang  dibuat  oleh  manusia 
ketika menuliskan script. 

tersebut 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

II.  TUJUAN PENELITIAN 

Berdasarkan latar belakang di atas, secara umum penelitian 
ini  bertujuan  untuk  merancang  dan  membangun  sistem  yang 
dapat  mempermudah,  meningkatkan  efisiensi,  dan  mencegah 
terjadinya  human  error  dalam  proses  penarikan,  pengajuan, 
hingga  persetujuan  pada  sampel  pengganti  dan  sampel 
tambahan. 

Adapun tujuan khusus yang ingin dicapai dalam penelitian 

ini adalah : 
1.  Merancang sistem informasi yang dapat melakukan proses 
penarikan,  pengajuan,  hingga  persetujuan  pada  sampel 
pengganti dan sampel tambahan survei ubinan palawija. 
2.  Melakukan implementasi terhadap hasil rancangan sistem 
informasi  penarikan  sampel  pengganti  dan  sampel 
tambahan survei ubinan palawija. 

3.  Melakukan evaluasi terhadap sistem informasi yang telah 

diimplementasikan. 

III. PENELITIAN TERKAIT 

Penelitian terdahulu terkait sistem penarikan sampel. 
1.  Pembangunan  Aplikasi  Penarikan  Sampel  dalam 
Mengimplementasikan  Desain  Sampling  Survei  Rumah 
Tangga  Tanaman  Padi  2014  (SPD  2014)  oleh  Shintia 
Hafifah  dari  Sekolah  Tinggi  Ilmu  Statistik  tahun  2016. 
Permasalahan yang dibahas dalam penelitian di atas adalah 
penggunaan  aplikasi  Microsoft  Visual  Foxpro  dan  Stata 
yang  berbasis  command  line  dalam  penarikan  sampel 
sehingga  hanya  dapat  dilakukan  oleh  orang  yang  paham 
cara  menggunakannya  [3].  Tujuan  dari  penelitian  ini 
adalah  bagaimana  membangun  aplikasi  yang  dapat 
melakukan  penarikan  sampel  tanpa  harus  menuliskan 
script pada SPD 2014. Hasil akhir dari penelitian ini adalah 
aplikasi penarikan sampel SPD berbasis desktop. 

2.  Surveykit:  Modul  Penarikan  Sampel  Tahap  Akhir  pada 
CAPI-STIS  oleh  Gede  Ananda  Nartapradnyana  dari 
Sekolah  Tinggi  Ilmu  Statistik  tahun  2017.  Penelitian  ini 
membahas  tentang  pengembangan  modul  dalam  aplikasi 
Computer  Assisted  Personal  Interviewing  (CAPI)  untuk 
melakukan  penarikan  sampel  tahap  akhir.  Modul  yang 
dibangun memiliki fitur integrasi dan fitur multi kuesioner 
sehingga fitur penarikan sampel yang ada pada CAPI-STIS 
dapat digunakan pada kuesioner yang berbeda tanpa perlu 
melakukan  coding  tambahan  (reusable)  [4].  Hasil  akhir 
dari  penelitian  ini  berupa  subsistem  dari  aplikasi  CAPI-
STIS yang dapat memudahkan petugas dalam melakukan 
penarikan sampel. 

IV. METODE PENELITIAN  

A.  Ruang Lingkup Penelitian 

Ruang  lingkup  dalam  penelitian  ini  yaitu  mencakup 
perancangan  sistem  penarikan  sampel  pengganti  dan  sampel 
tambahan pada Survei Ubinan Komoditas Palawija. Kerangka 
sampel  yang  digunakan  adalah  kerangka  sampel blok  sensus. 
Sistem  yang 
telah  dirancang  diimplementasikan  dengan 
dan 
memperhatikan 
memperhatikan  fungsi  utama  dari  sistem  yaitu  sistem  dapat 
melakukan proses penarikan, pengajuan, dan persetujuan pada 
tambahan.  Hasil  dari 
sampel 
sampel  pengganti  dan 

subject  matter 

kebutuhan 

dari 

 2 / 8 

 
 
implementasi  dijadikan  sebagai  bahan  evaluasi  terhadap 
kegunaan  dan  kesesuaian  sistem  dengan  kebutuhan  subject 
matter. Subject matter dari penelitian ini adalah Subdirektorat 
Pengembangan  Kerangka  Sampel  (Subdit  PKS)  Badan  Pusat 
Statistik Republik Indonesia (BPS RI). 

B.  Tahapan Penelitian 

Secara  umum,  penelitian  ini  dibagi  menjadi  lima  tahap, 

yaitu : 

1)  Requirement analysis 

dilakukan 

Requirement 

analysis 
langsung  kepada 

cara 
dengan 
subject  matter  yang 
wawancara 
bersangkutan.  Tahap 
ini  akan  menghasilkan  daftar 
kebutuhan  sistem  yang  akan  digunakan  sebagai  batasan 
dalam perancangan sistem nantinya. 
2)  Studi Pendahuluan 

Studi  pendahuluan   dilakukan  untuk mencari  referensi 
dan mempelajari tools yang berkaitan dengan permasalahan 
yang berhubungan dengan perancangan sistem. Tahapan ini 
berguna untuk membandingkan berbagai macam tools yang 
berhubungan  dengan  sistem  dan  memutuskan  tools  mana 
yang sesuai dengan kebutuhan sistem.  
3)  Perancangan Sistem 

Dalam  perancangan  sistem  dibagi  menjadi  tiga  jenis, 
yaitu perancangan basis data (database), perancangan antar 
muka pengguna (user interface), dan perancangan program 
aplikasi (front-end dan back-end). 
4) 

Implementasi Sistem 
tahap  konstruksi  dalam 
Tahap 
pembangunan 
dengan  mengimplementasikan 
rancangan sistem yang telah dibuat pada tahap sebelumnya. 
Tahap ini mencakup coding dan testing. 
5)  Evaluasi Sistem 

ini  merupakan 

sistem 

Tahapan ini bertujuan untuk mengevaluasi kinerja dari 
sistem yang meliputi verifikasi dan validasi. Verifikasi yaitu 
memastikan  apakah  fungsi  dalam  sistem  sudah  berjalan 
dengan  benar  sesuai  dengan  kebutuhan  sistem.  Sedangkan 
validasi  yaitu  memastikan  sistem  yang  telah  dibangun 
apakah sudah sesuai dengan kebutuhan dari subject matter. 

C.  Metode Pengumpulan Data 

Dalam penelitian ini, salah satu data yang digunakan dalam 
pembangunan sistem adalah data survei ubinan palawija pada 
tahun  2020  subround  pertama  dari  Subdit  PKS  BPS  RI  yang 
meliputi  data  frame  dari  populasi  survei  ubinan,  data  rumah 
tersebut 
tangga,  dan  data  sampel  utama 
digunakan  sebagai  acuan  dalam  penarikan  sampel  pengganti 
dan sampel tambahan untuk menguji bagaimana perbandingan 
hasil  penarikan  sampel  menggunakan  aplikasi  R  Studio  yang 
selama  ini  digunakan  dengan  penarikan  sampel  oleh  sistem 
yang akan dibangun.  
Adapun  metode 

dalam 
pengumpulan data pada penelitian ini adalah sebagai berikut : 

terpilih.  Data 

digunakan 

peneliti 

yang 

1)  Wawancara 

Wawancara  dilakukan  kepada  subject  matter  secara 
virtual  meeting  melalui  aplikasi  video  conference. 
Wawancara digunakan untuk mendapatkan gambaran secara 
umum  mengenai  permasalahan  yang  dihadapi  dan 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

kebutuhan  dari  subject  matter  terhadap  sistem  yang  akan 
dibuat 
2)  Studi Pustaka 

Sumber  studi  pustaka  utama  yang  digunakan  peneliti 
adalah Buku Pedoman Survei Ubinan. Selain buku pedoman, 
peneliti  juga  menggunakan  sumber  lain  yang  berkaitan 
dengan  penelitian  seperti  laporan  penelitian  terkait  dengan 
pembangunan sistem. 
3)  Kuesioner 

Kuesioner  merupakan  salah  satu  pengumpulan  data 
dengan  menggunakan  instrumen  pertanyaan  yang  diajukan 
kepada responden. Pada penelitian ini kuesioner digunakan 
pada  tahap  evaluasi  sistem  melalui  System  Usability  Scale 
(SUS)  untuk  mengetahui  seberapa  diterima  sistem  yang 
dibangun oleh pengguna. 

D.  Metode Pembangunan Sistem 

Metodologi yang digunakan dalam pembangunan sistem ini 
adalah  Rapid  Application  Development  (RAD).  RAD  dapat 
mempersingkat  langkah-langkah  dari  pembangunan  sistem 
yang  konvensional  meliputi  proses 
analisis,  desain, 
membangun, dan menguji sistem menjadi proses yang iteratif. 
RAD sendiri terdiri dari tiga fase, yaitu : 

1)  Requirement Planning 

Dalam  fase  ini  dilakukan  identifikasi  terhadap  tujuan 
dari pembangunan sistem serta syarat-syarat informasi yang 
ditimbulkan  dari  tujuan  tersebut.  Dalam  penelitian  ini, 
tujuan yang ingin dicapai dari sistem yang akan dibangun 
adalah sistem dapat melakukan penarikan, pengajuan, dan 
persetujuan sampel pengganti dan sampel tambahan sesuai 
dengan metode yang telah ditetapkan BPS. 
2)  RAD Design Workshop 

Fase  ini  merupakan  fase  untuk  merancang  sekaligus 
melakukan  perbaikan  pada  sistem.  Peneliti  sebagai 
pembangun  sistem  dapat  bekerja  membangun  dan 
menunjukkan  representasi  visual  desain  dan  pola  kerja 
sistem  kepada  subject  matter.  Selama  proses  workshop 
desain,  subject  matter  merespon  rancangan  yang  ada  dan 
peneliti melakukan perbaikan modul-modul yang dirancang 
berdasarkan respon dari subject matter.   
3) 

Implementation 
Fase  implementasi  merupakan  fase  deployment  atau 
penyerahan sistem kepada subject matter termasuk uji coba 
terakhir,  pelatihan,  serta  implementasi  dari  sistem  itu 
sendiri. 
Dalam pembangunan sistem, RAD dinilai cocok digunakan 
karena  menekankan  kecepatan  dalam  pembangunan  sistem 
serta  dalam  proses  pembuatan  aplikasi  penarikan  sampel 
pengganti dan sampel tambahan ini memerlukan interaksi serta 
feedback yang berkesinambungan dari subject matter terhadap 
prototipe  sistem  agar  cepat  dapat  disesuaikan  dengan 
kebutuhan subject matter. 

E.  Analisis Sistem Berjalan 

Berdasarkan hasil wawancara dengan pegawai Subdit PKS, 
berikut  merupakan  alur  dalam  melakukan  penggantian  atau 
penambahan sampel. 

 3 / 8 

 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

b) 

Sistem  dapat  melakukan  penarikan 
sampel 
pengganti,  sampel  tambahan,  dan  pemeriksaan 
usulan sampel yang lebih mudah dan efisien. 
Information 
Sistem dapat memastikan bahwa sampel yang telah 
terpilih sebelumnya tidak akan terpilih lagi menjadi 
sampel pengganti atau sampel tambahan dan sistem 
juga dapat memastikan bahwa tidak ada pengajuan 
maupun  persetujuan  usulan  sampel  yang  terlewat 
dan berulang (redundant). 

c)  Economic 

Resource yang dibutuhkan untuk mengakses sistem 
hanya  memerlukan  perangkat  yang  mempunyai 
akses internet. 

d)  Control 

Sistem dirancang agar dapat mengurangi pemilihan 
sampel  secara  purposive  sehingga  dapat  menjaga 
nilai acak dari sampel, serta sistem memungkinkan 
pengguna  dapat  memantau  perkembangan  dari 
pengajuan maupun persetujuan usulan sampel. 

e)  Efficiency 

Sistem  secara  otomatis  melakukan  pengecekan 
apakah  sampel  yang  diusulkan  tersedia  dalam 
frame atau tidak. 

f)  Service 

Fasilitas  melakukan  penarikan  dan  pemeriksaan 
sampel  pengganti  dan  sampel  tambahan  dapat 
dilakukan  oleh  pengguna  tanpa  harus  menuliskan 
secara  berulang-ulang.  Sistem  dapat 
script 
digunakan  oleh  pengguna  yang  tidak  memiliki 
pengetahuan  mengenai  penulisan  script  pada 
aplikasi command line seperti R Studio. 

V.  KERANGKA PIKIR 

Untuk  menjelaskan  secara  garis  besar  alur  berjalannya 
penelitian  ini  dan  menjelaskan  permasalahan  serta  solusinya, 
dibutuhkan  sebuah  kerangka  pikir  yang  dapat  digambarkan 
dalam diagram berikut. 

Gambar 1. Proses bisnis sistem berjalan 

F.  Analisis Kebutuhan 

Berdasarkan analisis pada proses bisnis sistem berjalan dan 
hasil  wawancara  dengan  subject  matter,  maka  didapatkan 
solusi  untuk  mengatasi  permasalahan  yang  dituangkan  dalam 
analisis kebutuhan. Analisis kebutuhan sistem dikelompokkan 
menjadi kebutuhan fungsional dan kebutuhan non-fungsional. 

1)  Kebutuhan fungsional 

a)  Sistem dapat melakukan proses upload untuk data 

frame, data ruta, dan data sampel utama 

b)  Sistem  dapat  melakukan  penarikan 

sampel 

pengganti dan sampel tambahan 

c)  Sistem  dapat  melakukan  proses  pengajuan  dan 
persetujuan  usulan  sampel  pengganti  dan  sampel 
tambahan 

d)  Sistem dapat melakukan cetak dokumen DSBS dan 
dokumen  SUB-P  baik  sampel  utama,  sampel 
pengganti, maupun sampel tambahan 
e)  Sistem  dapat  melakukan  monitoring 

terhadap 

usulan sampel 

f)  Sistem  dapat  melakukan 

sampel  dan 
mengeluarkan  output  berupa  master  olah  untuk 
kebutuhan pengolahan 

rilis 

g)  Sistem dapat melakukan manajemen user  
h)  Sistem  dapat  menerapkan  Single  Sign-On  (SSO) 

milik BPS 

2)  Kebutuhan non-fungsional 

Kebutuhan  non-fungsional  dianalisis  menggunakan 

analisis PIECES. 

a)  Performance 

 4 / 8 

 
 
 
 
 
 
Gambar 2. Kerangka pikir penelitian 

Penelitian ini dilakukan atas dasar permasalahan yang ada 
pada proses bisnis yang berjalan selama ini. Dari permasalahan 
tersebut dilakukan pendekatan untuk mendapatkan solusi dari 
setiap 
pendekatan 
permasalahan.  Hasil 
permasalahan  akan  memberikan  alternatif  solusi  untuk 
memecahkan  permasalahan  secara  umum.  Solusi  yang 
diberikan  terhadap  permasalahan  yang  ada  yaitu  dengan 
membangun  sistem  yang  dapat  melakukan  penarikan, 
pemeriksaan,  pengajuan,  dan  persetujuan  pada  sampel 
pengganti dan sampel tambahan. 

solusi 

dari 

Solusi  diimplementasikan  dengan  metode  pembangunan 
sistem  yang  ditetapkan.  Setelah  solusi  sistem  berhasil 
diimplementasikan,  dilakukan  evaluasi  terhadap  sistem  untuk 
mengukur  tingkat  keberhasilan  sistem.  Apabila  hasil  evaluasi 
yang  didapatkan  berada  di  atas  ambang  batas,  maka  sistem 
tersebut  dapat  dikatakan  layak  digunakan.  Hasil  akhir  dari 
penelitian  ini  berupa  sistem  penarikan  sampel  pengganti  dan 
sampel tambahan survei ubinan palawija berbasis web. 

VI. HASIL DAN PEMBAHASAN 

A.  Proses Bisnis Sistem Usulan 

Berdasarkan  hasil  analisis  sistem  berjalan  dan  analisis 
kebutuhan,  solusi  yang  diusulkan  adalah  dengan  membuat 
sistem yang dapat mempermudah, meningkatkan efisiensi, dan 
mencegah  terjadinya  human  error  dalam  proses  penarikan, 
pengajuan,  hingga  persetujuan  pada  sampel  pengganti  dan 
sampel tambahan. Berikut analisis proses bisnis sistem usulan. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 3. Proses bisnis sistem usulan 

B.  Diagram Use Case Sistem Usulan 

Use  case  digunakan  untuk  menggambarkan  hubungan 
antara  aktor  dan  aktivitas  dalam  sistem  yang  akan  dibangun. 
Terdapat  4  aktor  dalam  sistem  yang  dibangun,  yaitu  Super 
Admin, Admin BPS RI, Admin BPS Provinsi, dan Admin BPS 
Kabupaten.  Berikut  penggambaran  peran  dari  aktor  tersebut 
berdasarkan use case diagram. 

Gambar 4. Diagram Use Case sistem usulan 

C.  Implementasi 

1)  Penarikan dan pengajuan sampel pengganti dan sampel 

tambahan 
Secara umum proses penarikan sampel pengganti dan 
sampel tambahan sama, namun pada sistem yang dibangun 
keduanya  diletakkan  pada  menu  yang  berbeda. Perbedaan 
antara  keduanya  terletak  pada  populasi  kerangka  sampel 
dan input yang dibutuhkan. Sampel pengganti membutuh-
kan informasi tentang sampel yang akan diganti, sedangkan 
sampel  tambahan  membutuhkan  informasi  tentang  strata 
dari sampel tambahan. Metode penarikan sampel pengganti 
dan  sampel  tambahan  menggunakan  Simple  Random 
Sampling  dari  populasi  kerangka  sampel  yang  tersedia 
dimana  setiap  kerangka  sampel  memiliki  peluang  yang 
sama untuk terpilih menjadi sampel pengganti atau sampel 
tambahan.  

Populasi  untuk  kerangka  sampel  pengganti  yaitu 
kerangka  sampel  yang  tersedia  pada  tahun,  subround, 
kecamatan, dan strata yang sama dengan sampel yang akan 
diganti serta belum terpilih sebagai sampel utama, sampel 
pengganti,  dan  sampel  tambahan  pada  subround  yang 
berjalan.  Jika  dalam  kecamatan  yang  sama  tidak  ada 
kerangka sampel yang memenuhi, maka populasi kerangka 
sampel  penggantinya  menggunakan  kerangka  sampel 
kabupaten  yang  tersedia.  Populasi  untuk  kerangka  sampel 
tambahan yaitu kerangka sampel yang tersedia pada tahun, 
subround, kabupaten, dan strata berdasarkan hasil input dari 
pengguna,  serta  belum  terpilih  sebagai  sampel  utama, 
sampel  pengganti,  dan  sampel  tambahan  pada  subround 

 5 / 8 

 
 
 
 
 
 
 
 
 
yang  berjalan.  Proses  penarikan  sampel  pengganti  atau 
sampel  tambahan  digambarkan  melalui  diagram  aktivitas 
berikut. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 7. Tampilan hasil penarikan sampel pengganti oleh sistem 

Gambar 8. Tampilan pemilihan sampel pengganti oleh pengguna 

Gambar 9. Tampilan form penarikan sampel tambahan 

Gambar 5. Activity diagram penarikan dan pengajuan sampel pengganti 
dan sampel tambahan 

Berdasarkan  diagram  aktivitas,  proses  penarikan 
sampel  pengganti/tambahan  dimulai  dari  sistem  memuat 
halaman  awal  penarikan  sampel  pengganti/tambahan. 
Pengguna  diharuskan  mengisi  input  pada  form  yang 
disediakan untuk melakukan penarikan sampel.  

Gambar 10. Tampilan hasil penarikan sampel tambahan oleh sistem 

Terdapat 4 kali percobaan dalam melakukan penarikan 
sampel  dimana  percobaan  1,  2,  dan  3  penarikan  sampel 
dilakukan secara otomatis dan acak oleh sistem berdasarkan 
populasi  yang  tersedia,  sedangkan  percobaan  4  dilakukan 
pemilihan usulan sampel oleh pengguna dari populasi yang 
tersedia. Setelah usulan sampel terpilih, pengguna diminta 
untuk  melakukan  konfirmasi  apakah  akan  mengajukan 
sampel atau tidak. Usulan sampel akan dikirimkan ke BPS 
proses 
provinsi 
pengajuannya. 

diidentifikasi 

alasan 

untuk 

dan 

Gambar 6. Tampilan form penarikan sampel pengganti 

Gambar 11. Tampilan pemilihan sampel tambahan oleh pengguna 

2)  Persetujuan usulan sampel 

Secara  umum  proses  persetujuan  usulan  sampel 
pengganti dan sampel tambahan sama, namun pada sistem 
yang  dibangun  keduanya  diletakkan  pada  menu  yang 
berbeda.  Proses  persetujuan  usulan  sampel  digambarkan 
melalui diagram aktivitas berikut. 

 6 / 8 

 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 14. Tampilan menolak usulan sampel 

3)  Cetak dokumen DSBS dan dokumen SUB-P otomatis 

oleh sistem 
Fungsi  cetak  dokumen  DSBS  dan  dokumen  SUB-P 
disediakan  agar  BPS  kabupaten  dapat  melakukan  cetak 
dokumen  tersebut  secara  mandiri  dan  Subdit  PKS  tidak 
perlu  mengirimkan  lagi  dokumen  tersebut  satu  persatu. 
Proses  cetak  dokumen  DSBS  dan  dokumen  SUB-P 
digambarkan melalui aktivitas diagram berikut. 

Gambar 15. Activity diagram cetak dokumen DSBS dan SUB-P 

Berdasarkan diagram aktivitas, proses cetak dokumen 
DSBS  dan  dokumen  SUB-P  dimulai  dari  sistem  memuat 
halaman  cetak  DSBS  dan  SUB-P.  Pengguna  diharuskan 
mengisi  informasi  pada  form  yang  disediakan  untuk 
melakukan cetak dokumen. Khusus untuk dokumen SUB-P 
pengguna dapat memilih beberapa atau semua sampel blok 
sensus  yang  tersedia.  Sistem  akan  melakukan  generate 
terhadap sampel blok sensus dan sampel rumah tangga yang 
berada  di  database  untuk  diubah  dalam  bentuk  dokumen 
PDF yang selanjutnya dapat diunduh oleh pengguna. 

Gambar 16. Tampilan cetak dokumen DSBS dan SUB-P 

Gambar 17. Tampilan dokumen DSBS hasil generate 

 7 / 8 

Gambar 12. Activity diagram persetujuan usulan sampel 
Berdasarkan  diagram  aktivitas,  proses  persetujuan  usulan 
sampel  dimulai  dari  sistem  memuat  halaman  persetujuan 
usulan  sampel  pengganti/tambahan.  Pengguna  diharuskan 
melakukan  identifikasi  terhadap  sampel  yang  diajukan  baik 
dari segi alasan dan dari segi proses bagaimana usulan sampel 
didapatkan. Setelah mengidentifikasi usulan sampel, pengguna 
memilih  beberapa  usulan  sampel  untuk  disetujui  atau  ditolak 
pengajuannya.  Pengguna  diharapkan  mengirim  notifikasi 
kepada BPS kabupaten agar BPS kabupaten dapat mengetahui 
secara  langsung  terhadap  perkembangan  pengajuan  usulan 
sampel yang diajukan. 

Gambar 12. Tampilan daftar persetujuan sampel 

Gambar 13. Tampilan menyetujui usulan sampel 

 
 
 
 
 
 
 
 
 
 
Gambar 18. Tampilan dokumen SUB-P hasil generate 

4)  Fungsi lainnya 

Fungsi  lainnya  yang  sudah  diimplementasikan  dalam 
sistem  yang  dibangun  meliputi  fungsi  upload  data  frame, 
upload  data  rumah  tangga,  upload  data  sampel,  riwayat 
pengajuan, rekap data DSBS dan SUB-P, rilis sampel dan 
ekspor  master  olah,  monitoring  sampel,  manajemen 
pengguna, dan pengaturan admin. 

D.  Evaluasi 

Untuk  mengukur  kesesuain  sistem  yang  dibangun  dengan 
kebutuhan  pengguna,  peneliti  menggunakan  dokumen 
Software Requirement Spesification (SRS) sebagai tolak ukur. 
Berdasarkan  dokumen  tersebut  dapat  disimpulkan  bahwa 
sistem  yang  dibangun  sudah  sesuai  dengan  kebutuhan  dari 
pengguna. Selain dokumen SRS, evaluasi terhadap sistem yang 
dibangun  juga  dilakukan  dengan  menggunakan  dua  metode 
yaitu black box testing dan System Usability Scale (SUS). 

1)  Black box testing 

Tujuan  black  box  testing  untuk  melihat  keberhasilan 
pada setiap fungsi dalam sistem. Black box testing sendiri 
merupakan  metode  pengujian  tanpa  mengetahui  struktur 
internal  kode  dari  sistem.  Pada  penelitian  ini,  black  box 
testing  dilakukan  oleh  salah  satu  pegawai  di  Subdit  PKS 
dan  salah  satu  mahasiswa  jurusan  Komputasi  Statistik  di 
Politeknik  Statistika  STIS.  Kesimpulan  yang  didapatkan 
dari  hasil  pengujian  ini  adalah  setiap  fungsi  pada  setiap 
menu  pada  sistem  yang  dibangun  sudah  berjalan 
sebagaimana yang diharapkan. 

2)  System Usability Scale (SUS) 

SUS  merupakan  salah  satu  metode  evaluasi  sistem 
dengan  tujuan  untuk  mengukur  persepsi  kegunaan  dari 
sistem.  SUS  dilakukan  dengana  memberikan  kuesioner 
kepada responden yang berisi 10 pertanyaan dengan skala 
jawaban  1  hingga  5.  Responden  SUS  pada  penelitian  ini 
berjumlah  10  orang  yang  terdiri  pegawai  BPS  yang  akan 
menggunakan  sistem  ini  nantinya.  Skor  yang  didapatkan 
dari  hasil  uji  SUS  ini  sebesar  75,00  dimana  skor  tersebut 
berada  pada  rentang  68-100  yang  menunjukkan  bahwa 
sistem yang dirancang dapat diterima baik oleh pengguna.  

VII. 

PENUTUP 

Kesimpulan yang didapatkan dari hasil penelitian mengenai 
penarikan  sampel  pengganti  dan  sampel  tambahan  Survei 
Ubinan Komoditas Palawija adalah sebagai berikut. 
1.  Telah  dirancang  sistem  informasi  yang  dapat  melakukan 
proses  penarikan,  pengajuan,  hingga  persetujuan  pada 
sampel  pengganti  dan  sampel  tambahan  survei  ubinan 
palawija. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

2. 

Implementasi yang dilakukan berdasarkan hasil rancangan 
telah  menghasilkan  sistem  penarikan  sampel  pengganti 
dan  sampel  tambahan.  Implementasi  dari  pembangunan 
sistem telah dilakukan dengan metode Rapid Application 
Development. 

3.  Evaluasi dilakukan dengan System Usability Scale, Black 
Box Testing, dan melihat kesesuaian sistem yang dibangun 
dengan  dokumen  Software  Requirement  Spesification. 
Berdasarkan  hasil  evaluasi  tersebut  didapatkan  bahwa 
sistem  informasi  yang  dibangun  dapat  diterima  dengan 
baik  oleh  pengguna,  setiap  fungsi  dalam  sistem  sudah 
berjalan  sebagaimana  yang  diharapkan,  dan  sistem  yang 
dibangun sudah sesuai dengan kebutuhan pengguna. 

Saran  yang  dapat  peneliti  berikan  yang  mungkin  berguna 

untuk pengembangan sistem dan penelitian selanjutnya. 
1.  Berdasarkan hasil SUS diketahui bahwa pada pernyataan 
nomor  4  dan  nomor  10  menunjukkan  bahwa  pengguna 
masih  membutuhkan  bantuan  dari  tekisi  serta  perlu 
ini. 
membiasakan  diri  untuk  menggunakan  sistem 
Sehingga  perlu  dilakukan  penambahan  fungsi  atau 
dokumen yang dapat membantu pengguna sistem. 

2.  Melakukan  sinkronisasi  dengan  sistem  penarikan  sampel 
Survei Ubinan Komoditas Padi berbasis Kerangka Sampel 
Area  (SUB-KSA)  karena  antara  pengguna  sistem  survei 
ubinan palawija dan SUB-KSA sebagian besar sama. 

DAFTAR PUSTAKA 

[1]  Badan  Pusat  Statistik,  “Pedoman  Pengumpulan  Data  Survei  Ubinan 

Tanaman Pangan 2018”, 2018. 

[2]  Badan  Pusat  Statistik.  (2016).  Survei  Ubinan  Tanaman  Pangan,  2016. 

[Online]. Available : 
https://sirusa.bps.go.id/sirusa/index.php/dasar/view?kd=47&th=2016 
[3]  S.  Hafifah,  “Pembangunan  Aplikasi  Penarikan  Sampel  Dalam 
Mengimplementasikan  Desain  Sampling  Rumah  Tangga  Tanaman  Padi 
2014 (SPD 2014)”, thesis, 2016. 

[4]  G. A. Nartapradnyana, “Surveykit: Modul Penarikan Sampel Tahap Akhir 

pada CAPI-STIS”, thesis, 2017. 

[5]  J.  Martin,  Rapid  Application  Development,  Macmillan  Publishing  Co., 

1991. 

[6]  J.L. Whitten and L.D. Bentley, System Analysis and Design Method ed.7. 

McGraw-Hill Irwin. 

[7]  IEEE  Std.,  “IEEE  Recommended  Practice  for  Software  Requirement 

Specifications”, 1993. 

[8]  M.  Babaeian,  “Comparison  of  software  testing  review  Black  Box  and 

White Box and Gray Box”, thesis, 2015. 

[9]  J. Brooke, “SUS-A quick and dirty usability scale”, Usability Evaluation 

in Industry, 1996. 

 8 / 8 

 
 
 
 
 
 
 
 
 
 
"
221709845,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pendeteksian Potensi Fraud dalam Pengadaan 
Barang/Jasa dengan Pendekatan Machine Learning 
Studi Kasus: Tahun Anggaran 2018-2020 

Muhammad Alif Taufiqulhakim (221709845, 4SD2) 
Dosen Pembimbing: Dr. Drs. Waris Marsisno, M.Stat. 

Ringkasan—  Kasus  tindak  pidana  korupsi,  penyalahgunaan 
aset, dan pernyataan palsu merupakan beberapa contoh bentuk 
terjadinya  fraud  dalam  lingkup  publik.  Berdasarkan  data  dari 
KPK,  sejak  tahun  2004  hingga  2020  terdapat  total  224  kasus 
Tindak  Pidana  Korupsi  (TPK)  yang  tergolong  dalam  perkara 
Pengadaan  Barang/Jasa.  Melihat  fakta,  bahwa  masih  banyak 
terjadi  TPK  dalam  perkara  Pengadaan  Barang/Jasa  di 
pemerintahan  meskipun  telah  diterapkannya  e-Procurement 
melalui  Layanan  Pengadaan  Secara  Elektronik  (LPSE)  yang 
dikelola  oleh  Lembaga  Kebijakan  Pengadaan  Barang/Jasa 
Pemerintah  (LKPP).  Dalam  penelitian  ini  penulis  melakukan 
scraping  pada situs web LPSE untuk memperoleh data kegiatan 
Pengadaan Barang/Jasa dari periode tahun 2018-2020. Data yang 
telah diperoleh untuk selanjutnya diolah dengan bantuan metode 
Machine  Learning  guna  mendeteksi  potensi  adanya  fraud. 
Berdasarkan  pada  hasil  pengerjaan  penelitian  yang  dilakukan 
penulis,  digunakan  model  Clustering,  Decision  Tree  dan 
Association  Rules  dengan  disertai  oleh  implementasi  metode 
scoring  oleh opentender.net terhadap beberapa indikator  dalam 
melakukan  pendeteksian  pada  potensi  adanya  fraud  dalam 
kegiatan Pengadaan Barang/Jasa. 

Kata  Kunci— 

fraud,  Pengadaan  barang/jasa,  Machine 

Learning. 

I.  LATAR BELAKANG 

Kasus  tindak  pidana  korupsi,  penyalahgunaan  aset,  dan 
pernyataan palsu merupakan beberapa contoh bentuk terjadinya 
fraud  dalam  lingkup  publik.  Dalam  Undang-Undang  No.  31 
tahun  1999  junto  Undang-Undang  No.  20  tahun  2001 
dinyatakan  bahwa  perbuatan  curang  dan  perbuatan  yang 
merugikan  keuangan  negara  merupakan  contoh  dari  berbagai 
jenis-jenis Tindak Pidana Korupsi (TPK). 

Berdasarkan Rule of Law Index tahun 2019 yang dikeluarkan 
oleh World Justice Project [1], Indonesia menempati urutan ke-
62 dari 126 negara dengan skor 0,52 dari skala 0-1. Sedangkan 
dalam  parameter  tidak  adanya  korupsi,  Indonesia  menempati 
urutan ke 97 dari 126 Negara dengan skor 0,38 dari skala 0-1. 
Posisi  tersebut  dapat  diartikan  bahwa  Indonesia  termasuk  ke 
dalam 30 besar negara di dunia dengan kasus korupsi terbanyak. 
Dengan  mengacu  pada  data  dari  KPK,  sejak  tahun  2004 
hingga  2020  terdapat  total  224  kasus  TPK  yang  tergolong 
dalam  perkara  Pengadaan  Barang/Jasa  [2].    Angka  ini  hanya 
lebih kecil dari jumlah TPK perkara Penyuapan yang mencapai 
total  708  kasus.  Hal  ini  cukup  menjadi  pertanyaan  apabila 
mengingat  bahwa  kegiatan  Pengadaan  Barang/Jasa  sudah 
mengalami  digitalisasi  dan  transparansi  dalam  bentuk  e-
Procurement. Di Indonesia telah diperkenalkan dan diterapkan 

e-Procurement sebagai sistem pengadaan barang dan jasa sejak 
tahun 2002 [3].  

Indonesia  Corruption  Watch  (ICW),  suatu  organisasi  non-
pemerintah  dan  non-profit  yang  memiliki  misi  untuk 
mengawasi dan melaporkan berbagai hal terkait dengan korupsi 
yang terjadi di Indonesia, melalui website mereka juga merilis 
publikasi  terkait  dengan  TPK  Pengadaan  Barang/Jasa  yang 
ditindak  dalam kurun  waktu  tahun  2019,  lebih  tepatnya  TPK 
yang  ditangani  aparat  penegak  hukum    dalam  kurun  waktu 
tanggal 1 Januari 2019 hingga 31 Desember 2019 [4]. 

Dalam  publikasinya,  ICW  menemukan  bahwa  sepanjang 
tahun 2019, telah terjadi 271 kasus TPK dimana sebanyak 64% 
atau  174  kasus  berkaitan  dengan  kegiatan  Pengadaan 
Barang/Jasa  di  pemerintahan.  Akibat  yang  ditimbulkan  dari 
beberapa  kasus  tersebut  diantaranya  mengakibatkan  kerugian 
bagi  negara  senilai  957,34  miliar  rupiah  oleh  389  orang 
tersangka  yang  sebagian  besar  berprofesi  sebagai  Aparatur 
Sipil  Negara  (ASN),  pihak  swasta  dan  Kepala  Desa  beserta 
jajarannya. Besarnya kerugian bagi negara tersebut dirasakan 
oleh beberapa sektor antara lain Anggaran Desa, Transportasi, 
Pemerintahan  dan  Pendidikan.  Dampak  korupsi  di  sektor 
pengadaan terletak pada buruknya kualitas barang publik, tidak 
dapat dimanfaatkannya barang publik, atau jika dipergunakan 
akan berpotensi untuk dapat membahayakan masyarakat. 

Menurut  Perpres  No.  70  Tahun  2012,  pengadaan  secara 
elektronik  terbagi  dua:  e-tendering  dan  e-purchasing.  E-
tendering adalah sistem tender secara elektronik dimana para 
penyedia berkompetisi untuk mendapatkan kontrak dari badan 
publik.  Beberapa  penelitian  mengkonfirmasi  bahwa  tender 
secara elektronik lebih mencegah terjadinya korupsi [5]. Badan 
publik akan memilih pemenang tender dengan tolak ukur: (i) 
penawar harga terendah yang memenuhi persyaratan; atau (ii) 
penawar dengan harga ekonomi terbaik. 

diterapkannya 

Melihat  fakta  bahwa  masih  terdapat  TPK  dalam  perkara 
Pengadaan  Barang/Jasa  di  pemerintahan  bersamaan  dengan 
telah 
e-Procurement  melalui  Layanan 
Pengadaan Secara Elektronik (LPSE) yang diperkenalkan sejak 
tahun  2008  namun  secara  efektif  dimanfaatkan  sejak  tahun 
2012,  yang  dikelola  oleh  Lembaga  Kebijakan  Pengadaan 
Barang/Jasa  Pemerintah 
tentu  menimbulkan 
pertanyaan  mengenai  bagaimana  TPK  dalam  perkara 
Pengadaan Barang/Jasa di pemerintahan masih dapat terjadi di 
tengah pemberlakuan e-Procurement yang diharapkan mampu 
memberikan 
termasuk 
masyarakat  dalam  memantau  dan  mengawasi  kegiatan 
Pemerintah dalam Pengadaan Barang/Jasa. Sesuai yang dimuat 

transparansi  bagi 

semua  pihak 

(LKPP), 

 1 / 7 

 
 
 
 
 
dalam Jurnal LKPP: Senarai, potensi keberadaan fraud dalam 
kegiatan  Pengadaan  Barang/Jasa  dapat  terjadi  pada  hampir 
semua tahap, mulai dari perencanaan pengadaan hingga tahap 
pemanfaatannya [6]. 

Status  dokumen  kontrak  dalam  Pengadaan  Barang/Jasa 
sejatinya  masih  menjadi  perdebatan,  apakah  seharusnya 
bersifat terbuka atau tertutup. Padahal dalam Undang-Undang 
No. 14 Tahun 2008 tentang Keterbukaan Informasi Publik (KIP) 
dan  Peraturan  Komisi  Informasi  Pusat  No.  1  Tahun  2010 
tentang  Standar  Layanan  Informasi  Publik  (SLIP)  telah 
menyebutkan  bahwa  perjanjian  antara  Pemerintah  dan  pihak 
ketiga dan informasi berkaitan dengan Pengadaan Barang/Jasa 
adalah  informasi  publik,  yang  artinya  dapat  diakses  dengan 
bebas oleh masyarakat. Perbedaan pandangan tersebut diduga 
terjadi karena di dalam aturan-aturan tersebut tidak disebutkan 
secara  terperinci  mengenai  informasi  jenis  apa  saja  yang 
dimaksud  dengan  informasi  yang  berkaitan  dengan  kegiatan 
Pengadaan  Barang/Jasa  yang  dapat  diakses  oleh  publik. 
Sehingga  masih  banyak  informasi  penting  yang  tersimpan 
namun seharusnya dirilis secara bebas. 

Dicanangkannya  kebijakan  “Satu  Data”  oleh  Pemerintah 
diharapkan mampu mendorong perbaikan dari kualitas data dan 
informasi  yang  tersedia  serta  terwujudnya  standardisasi  pada 
format  dan  konten  dari  data  yang  dimuat  berkaitan  dengan 
Pengadaan Barang/Jasa [7]. Dengan demikian akan membuat 
transparansi  data  lebih  baik  dan  memudahkan  bagi  seluruh 
pihak  untuk  dapat  melakukan  pengawasan  terhadap  kegiatan 
Pengadaan  Barang/Jasa  yang  dilakukan.  Disini  penulis 
mencoba  untuk  dapat  memanfaatkan  ketersediaan  data  pada 
LPSE  untuk  mencoba  melakukan  deteksi  tentang  potensi 
keberadaan  fraud  yang  ada  dalam  transaksi  Pengadaan 
Barang/Jasa  dengan  menerapkan  beberapa  metode  Machine 
Learning dan/atau Data Mining yang penulis ketahui.  

II.  TUJUAN PENELITIAN 

Berdasarkan 

latar  belakang  yang 

telah  disampaikan, 

penelitian bertujuan : 

1. Mendeteksi  potensi  fraud  pada  kegiatan  pengadaan 
barang/jasa  secara  elektronik  berdasarkan  indikator  dari 
opentender.net. 

2. Mendeteksi  fraud  dengan  menerapkan  metode  Machine 

Learning.  

3. Menerapkan  metode  Decision  Tree  untuk  mengetahui 

variabel yang paling berpengaruh. 

III. PENELITIAN TERKAIT 

dalam 

Supervised  Learning 

Dalam  sejumlah  penelitian  yang  dilakukan  sebelumnya, 
diketahui  dalam  mendeteksi  adanya  fraud  dalam  kegiatan 
Pengadaan  Barang/Jasa  terdapat  beberapa  metode  dalam 
Machine Learning yang dapat dimanfaatkan, baik metode yang 
atau  metode 
termasuk 
Unsupervised Learning. Dengan dimulai dengan proses Cluster 
Analysis,  selanjutnya  dapat  diterapkan  beberapa  metode 
lainnya seperti Decision Tree atau Random Forest [10][11][12]. 
Selain itu, beberapa metode Data Mining yaitu Apriori atau 
biasa  disebut  Association  Rules  [8],  Decision  Tree,  Bayesian 
Network dan Neural Network [9] juga dapat dimanfaatkan guna 
mendeteksi  adanya  fraud  dengan  menjelaskan  ada  atau 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

tidaknya  keterkaitan  antara  satu  pihak  dan  pihak  lain  yang 
terlibat  dalam  kegiatan  Pengadaan  Barang/Jasa.  Tentu 
diperlukan perlakuan dan kriteria khusus terhadap data sebelum 
dapat  diproses  lebih  lanjut  dengan  metode  yang 
telah 
ditentukan  yang  juga  membutuhkan  karakteristik  data  yang 
berbeda-beda pada setiap metode yang akan digunakan. 

Pendekatan lain yang mungkin dapat dipertimbangkan [13] 
dimana terdapat enam langkah tambahan yang dapat dilakukan 
guna  mendeteksi  adanya  fraud  dalam  kegiatan  Pengadaan 
Barang/Jasa antara lain (1) Pemeriksaan prosedur persetujuan 
lelang, (2) Pengecekan validitas jangka waktu penggunaan user 
id, (3) Verifikasi kontrol akses terhadap ULP, (4) Pengecekan 
adanya kolusi antara pemilik proyek dan penyedia layanan, (5) 
Pengecekan adanya kolusi antar penyedia layanan satu dengan 
yang  lain  dan  (6)  Pemeriksaan  adanya  fraud  dalam  sistem 
LPSE yang digunakan. 

No 
Judul 
[8]  Brazilian 

Government 
Procurements: 
an Approach 
to Find Fraud 
Traces in 
Companies 
Relationships 

TABEL I 
TABEL LITERATUR 

Data Mining 

Penulis, Publikasi  Metode 
Rebeca A. 
Baldomir, Gustavo 
C. G. Van Erven, 
Célia Ghedini 
Ralha, 

Tecnologias, 
Métodos e Teorias 
na Engenharia de 
Computação 
(pp.8-19) 

[9]  Data Mining 

Rekha Bhowmik, 

Data Mining 

Techniques in 
Fraud 
Detection 

[10]  Fraud 

Detection 
using 
Machine 
Learning and 
Deep 
Learning 

[11]  Automatic 

Machine 
Learning 
Algorithms 
For Fraud 
Detection in 
Digital 
Payment 
Systems 

Journal of Digital 
Forensics, 
Security and Law. 
Vol. 3(2) 

Pradheepan 
Raghavan, Neamat 
El Gayar, 

2019 International 
Conference on 
Computational 
Intelligence and 
Knowledge 
Economy 
(ICCIKE) 
O.  Kolodiziev, A. 
Mints, P. Sidelov, 
I. Pleskun, O. 
Lozynska, 

Eastern-European 
Journal of 
Enterprise 
Technologies 

Machine Learning 

Machine Learning 

IV. METODE PENELITIAN  

 2 / 7 

 
 
 
 
A.  Batasan Studi 

sebesar  56,7 

Dengan mengacu pada data yang dirilis oleh ICW [15][16], 
bahwa  sepanjang  periode  tahun  2018-2020  tercatat  kerugian 
material  yang  dialami  negara  akibat  kasus  TPK  selalu 
mengalami  peningkatan  dari  tahun  ke  tahun  dengan  cukup 
signifikan.  Peningkatan  tertinggi  terjadi  pada  tahun  2020 
rupiah 
dengan  kerugian  negara 
dibandingkan kerugian di tahun 2019 sebesar 8,4 triliun rupiah.  
Secara  garis  besar  selama  periode  tiga  tahun  tersebut 
kerugian  negara  terus  meningkat  setiap  tahunnya  akibat  dari 
terjadi.  Atas  pertimbangan  dengan 
kasus  TPK  yang 
memperhatikan  pada 
lonjakan  kerugian  yang 
dialami negara akibat dari kasus TPK yang terjadi di Indonesia 
selama  tiga  tahun  terakhir,  penulis  bermaksud  membatasi 
lingkup penelitian pada data tahun 2018-2020 guna mendeteksi 
potensi fraud dalam kegiatan Pengadaan Barang/Jasa. 

terjadinya 

triliun 

KERUGIAN NEGARA 
AKIBAT KORUPSI

56.70

2020

5.60

2018

8.40

2019

i

h
a
p
u
R
n
u

i
l
i
r
T

Gambar 1. Grafik Nominal Kerugian Negara Akibat Korupsi 2018-2020 

B. Pengumpulan Data 

Dalam penelitian ini, sumber data diperoleh dari situs LPSE 
lembaga  terkait  di  seluruh  Indonesia  dan  apabila  diperlukan, 
tambahan data  diperoleh dari situs opentender.net.  Data yang 
diambil  merupakan  kegiatan  Pengadaan  Barang/Jasa  yang 
berlangsung  pada  periode  tahun  2018-2020  dengan  status 
tahapan  “Tender  Telah  Selesai”  pada  situs  LPSE  masing-
masing lembaga serta situs opentender.net. Proses pengambilan 
data  dilakukan  dengan  metode  scraping  yaitu  suatu  proses 
pengambilan  data  dari  suatu  situs  web.  Dalam  penelitian  ini 
penulis  melakukan  scraping  dengan  memanfaatkan  bahasa 
pemrograman Python dengan bantuan  library Selenium untuk 
selanjutnya dapat dilakukan beberapa proses pengolahan data. 
Kemudian  data  yang  telah  berhasil  dikumpulkan  dengan 
bantuan  library  Selenium  untuk  selanjutnya  disatukan  dalam 
sebuah  tabel  untuk  kemudian  disimpan  dalam  bentuk  file 
dengan format csv. 

Pengambilan  data  yang  dirancang  untuk  dapat  berjalan 
secara  otomatis  dengan  menggunakan  bantuan 
library 
tidak  dapat  berjalan  secara  maksimal  karena 
Selenium 
menemui  beberapa  permasalahan.  Hal  ini  salah  satunya 
disebabkan  oleh  tidak  adanya  standardisasi  dan  keseragaman 
pada struktur penyajian data pada setiap situs LPSE di masing-

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

masing 
lembaga  menjadikan  banyak  perbedaan  pada 
kelengkapan  serta  tampilan  data  yang  tersaji  secara  tidak 
konsisten pada masing-masing halaman situs yang menyajikan 
informasi lengkap suatu proyek. 

Akibat  dari  hal  tersebut  menimbulkan  banyaknya  atribut 
data yang tidak terlengkapi dengan sempurna. Untuk mengatasi 
jika  memungkinkan,  dilakukan 
permasalahan 
penyesuaian  terhadap  metode  scraping  data  dari  situs  LPSE 
lembaga terkait sesuai kebutuhan. 

tersebut 

C. Pre-processing Data 

Agar  memperoleh  hasil  pengolahan  data  serta  model  yang 
diperoleh dapat lebih optimal, sebelum dilakukan pengolahan 
dengan  menggunakan  metode  Machine  Learning,  data  yang 
telah  berhasil  diperoleh  dari  situs  LPSE  dan  telah  ditabulasi 
terlebih  dahulu  dilakukan  kegiatan  pre-processing  pada  data 
yang ada. Hal ini penting untuk dilakukan demi memperoleh 
data  yang  bersih,  lengkap  dan  siap  untuk  diolah  dengan 
menggunakan metode tertentu. 

Tahapan dalam pre-processing data antara lain yaitu dengan 
melakukan  Data  Cleansing.  Proses  ini  merupakan  suatu 
metode untuk menganalisa  mengenai kualitas dan konsistensi 
dari  data  yang  dimiliki  dengan  mengubah,  mengoreksi, 
menyesuaikan  ataupun  bahkan  menghapus  data  tersebut  jika 
memang  tidak  diperlukan.  Data  yang  dibersihkan  tersebut 
biasanya  adalah  data  yang  salah,  rusak,  tidak  akurat,  tidak 
lengkap dan tidak sesuai dengan format yang diinginkan. 

Setelah  dirasa  data  sudah  dalam  kondisi  yang  optimal, 
langkah  selanjutnya  yaitu  dilakukan  metode  encoding  dan 
scaling  pada  variabel-variabel  dalam  data.  Proses  encoding 
diperlukan  untuk  mengubah  variabel  data  kategorik  menjadi 
angka  agar  dapat  masuk  ke  dalam  operasi  matematika. 
Sedangkan proses scaling berguna untuk melakukan konversi 
nilai variabel menjadi nilai dengan skala yang lebih kecil yang 
nantinya  berguna  dalam  menjaga  kualitas  model  yang 
dihasilkan  dari  operasi  metode  Machine  Learning  yang  pada 
umumnya didasarkan pada hukum Euclidean Distance. 

TABEL II 
HASIL DATA CLEANSING 
Jumlah Atribut 
37 

Jumlah Entitas 
276.111 

14 

273.839 

Status 
Dataset hasil 
scraping 
Dataset setelah 
cleansing 

D. Clustering 

Data yang telah melalui tahapan pre-processing selanjutnya 
mulai  dilakukan  pengolahan  dengan  menerapkan  metode 
clustering.  Tahapan 
ini  merupakan  suatu  proses  untuk 
mengelompokan data ke dalam beberapa cluster sehingga data 
dalam satu cluster memiliki karakteristik dan tingkat kemiripan 
yang  maksimum  dan  data  antar cluster memiliki  kemiripan 
yang  minimum.  Dengan  menerapkan  metode  ini  penulis 
bermaksud untuk mempersempit skala penglihatan data dengan 
membaginya  ke  dalam  kelompok  tertentu.  Metode  clustering 
yang  digunakan  penulis  dalam  penelitian  ini  yaitu  K-Means 
Clustering yang bertujuan untuk meminimalisir varians dalam 

 3 / 7 

 
 
 
 
 
 
 
 
 
suatu  cluster  atau  disebut  Within-Cluster-Sum-of-Squares 
(WCSS) yang dirumuskan: 

penelitian, penulis menggunakan rancangan alur kerja seperti 
yang terlihat pada Gambar 2. sebagai berikut. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

𝑛
𝑊𝐶𝑆𝑆 = ∑ (𝑥𝑖 − 𝑦𝑖)2
𝑖=1

(1) 

E. Decision Tree 

Penggunaan  metode  Decision  Tree  dimaksudkan  untuk 
menjadi  metode  pengambilan  keputusan  melalui  klasifikasi 
dengan  cara  menampilkan prediksi  dengan  struktur  tingkatan 
atau  hierarki.  Pada  metode  Decision  Tree,  data  yang  tersedia 
diubah dengan menerapkan aturan-aturan untuk menempatkan 
data-data  tersebut  ke  dalam  kelompok-kelompok  yang  lebih 
kecil  sehingga  diharapkan  proses  pengambilan  keputusan 
menjadi lebih sederhana.  

Sebelum dapat menggunakan metode Decision Tree, terlebih 
dahulu data perlu dipisahkan menjadi dua jenis data, yaitu data 
latih dan data tes.  Tidak ada aturan pasti mengenai persentase 
pembagian porsi dari besarnya data latih dan data tes yang akan 
digunakan  dalam  pembuatan  model.  Namun  pada  umumnya 
persentase jumlah data latih lebih besar dibandingkan dengan 
data  tes.  Pemisahan  ini  juga  bertujuan  agar  model  Machine 
Learning  yang  dihasilkan  nantinya  memiliki  akurasi  yang 
tinggi karena model tidak menjalankan atau menemui jenis data 
yang sama pada saat sedang belajar atau berlatih dengan pada 
saat membuat model yang sebenarnya. 

F. Association Rules 

Metode  Association  Rules  merupakan  salah  satu  metode 
yang  bermanfaat  guna  mempelajari  aturan  asosiasi,  mencari 
pola hubungan antar satu atau lebih objek dalam suatu dataset. 
Dalam  penerapannya,  metode  ini  dapat  dinilai  atau  diukur 
berdasarkan  beberapa  parameter,  support  (nilai  penunjang) 
yaitu  persentase  kombinasi  objek  tersebut  dalam  dataset  dan 
confidence  (nilai  kepastian)  yaitu  kuatnya  hubungan  antar 
objek  dalam  aturan  asosiatif.  Analisis  asosiasi  didefinisikan 
sebagai suatu proses untuk menemukan semua aturan asosiatif 
yang  memenuhi  syarat  minimum  untuk  support  (minimum 
support)  dan  syarat  minimum  untuk  confidence  (minimum 
confidence).  

V.  KERANGKA PIKIR 

Pengerjaan  penelitian  kali  ini  penulis  memanfaatkan  jenis 
data  sekunder  yang  berupa  data  kegiatan  Pengadaan 
Barang/Jasa  oleh  Lembaga  Negara  yang  dimuat  pada  situs 
LPSE  yang  dikelola  oleh  LKPP  guna  dimanfaatkan  untuk 
pelaksanaan e-procurement yang bertujuan untuk menciptakan 
transparansi dalam proses Pengadaan Barang/Jasa bagi semua 
pihak.  Pengambilan  data  dilakukan  dengan  menggunakan 
library 
metode  scraping  dengan  memanfaatkan  bantuan 
Selenium pada bahasa pemrograman Python. 

Dalam  melakukan  proses  pengecekan  potensi  keberadaan 
fraud,  penulis  mengacu  kepada  metode  analisa  data  yang 
dikeluarkan  oleh  opentender.net  serta  memanfaatkan  metode 
Machine Learning, dalam hal ini penulis menggunakan metode 
Decision Tree setelah sebelumnya dilakukan proses clustering 
terlebih dahulu. Secara garis besar dalam melaksanakan proses 

Gambar 2. Diagram Alir Kerangka Pikir 

VI. HASIL DAN PEMBAHASAN 

A. Pengumpulan Data 

Proses  pengumpulan  data  dengan  menggunakan  metode 
scraping dari situs LPSE lembaga terkait di seluruh Indonesia  
memanfaatkan  library  Selenium  pada  bahasa  pemrograman 
Python  memperoleh  hasil  dataset  sebanyak  276.111  entitas 
dengan  atribut  sebanyak  37.  Data  yang  telah  diperoleh 
kemudian ditabulasikan ke dalam file dengan format csv untuk 
selanjutnya dapat diolah lebih lanjut. 

B. Analisa Data 

Sebagai dasar dalam melakukan potensi adanya fraud dalam 
kegiatan  Pengadaan  Barang/Jasa,  penulis  menggunakan 
panduan  pemberian  skor  pada  indikator-indikator  yang  telah 
dirilis  oleh  opentender.net 
[17].  Berdasarkan  panduan 
pemberian skor yang terdapat pada Tabel III, apabila skor yang 
diperoleh  suatu  proyek  Pengadaan  Barang/Jasa  semakin 
mendekati 100  berarti  potensi  resiko kecurangannya  semakin 
tinggi.  Skor  ini  diperoleh  dari  perhitungan  jumlah  skor 
berdasarkan  indikator  dikurang  skor  minimal,  dibagi  dengan 
skor  maksimal  dikurangi  skor  minimal,  kemudian  hasilya 
dikalikan  100.  Untuk  lebih  jelas  dapat  dilihat  dalam  rumus 
sebagai berikut:  

𝑆𝑘𝑜𝑟 = (

𝐽𝑢𝑚𝑙𝑎ℎ 𝑠𝑘𝑜𝑟−𝑠𝑘𝑜𝑟 𝑚𝑖𝑛

𝑠𝑘𝑜𝑟 max − 𝑠𝑘𝑜𝑟 𝑚𝑖𝑛

) 𝑥 100   

(2) 

Setelah  melakukan  perhitungan  skor  pada  masing-masing 
proyek,  selanjutnya  hasil  skor  yang  telah  diperoleh  ini 
dijadikan  sebagai  dasar  atau  acuan  dalam  melakukan  proses 
clustering dengan menggunakan metode K-Means. Selanjutnya 
dibentuk  suatu  dataset  baru  yang  menampilkan  skor  dari 

 4 / 7 

 
 
 
     
 
 
 
 
     
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

TABEL III 
INDIKATOR POTENTIAL FRAUD ANALYSIS (PFA) OPENTENDER.NET 

No. 

Indikator 

Keterangan 

Skoring 

Ketersediaan 
pada Dataset 

Durasi antara tanggal 
pengumuman dengan 
penetapan pemenang 

Waktu yang lebih lama antara tanggal pengumuman dengan 
penetapan pemenang dapat mengindikasikan inefisiensi dalam 
proses pengadaan. 

1 

1 : 0-25 
2 : 26 - 35 
3 : 36 - 45 
4 : 46 - 70 
5 : > 70 

Nilai kontrak diatas nilai HPS tidak diperbolehkan berdasarkan 
peraturan pengadaan barang dan jasa 

5 : < 0 

Perbandingan Nilai 
Kontrak dan HPS 

2 

Jika nilai kontrak semakin mendekati HPS maka potensi 
penyimpangan lebih tinggi. 
Namun, semakin jauh nilai kontrak dibawah HPS juga 
mengindikasikan perencanaan kurang baik/ potensi 
penyimpangan tinggi 

3  Nilai Kontrak Tinggi 

Semakin tinggi nilai kontrak sebuah pengadaan maka potensi 
resikon kecurangannya pun semakin besar 

4 

5 

Pengadaan di kuartal  
ke-IV 

Pengadaan di kuartal ke empat dapat mengindikasikan 
pengadaan dalam rangka menghabiskan anggaran dengan proses 
yang terburu-buru, sehingga hasilnya tidak maksimal 

Pemenang berulang 

Menghitung berapa kali sebuah perusahaan memenangkan 
pengadaan pemerintah dalam satu tahun anggaran; semakin 
banyak sebuah perusahaan menang di tahun anggaran yang 
sama, maka potensinya akan semakin besar 

5 : 0 - 0.99 
4 : 1 - 1.99 
3 : 2 - 2.99 
2 : 3 - 3.99 
1 : 4 - 7.99 
2 : 8 - 11.99 
3 :12 - 15.99 
4 : 16 - 19.99 
5 : >=20 
1 : 0 - 500 Jt 
2 : >500 Jt -1 M 
3 : >1 - 5 M 
4 : >5-10 M 
5 : > 10 M 
1 : Januari-Juni 
2 : Juli-September 
3 : Oktober 
4 : November 
5 : Desember 
5 : > 4 kali 
4 : 4 kali 
3 : 3 kali 
2 : 2 kali 
1 : 1 kali 

Tersedia 

Tersedia 

Tersedia 

Tersedia 

Tersedia 

masing-masing  indikator  scoring  sebagai  variabel  baru  dan 
dilakukan scaling terlebih dahulu. 

Proses  clustering  dengan  menggunakan  metode  K-Means 
menghasilkan  rekomendasi  pembagian  data  berdasarkan 
kemiripan  karakteristiknya  ke  dalam  3  buah  cluster. 
Berdasarkan  yang  terlihat  pada  Gambar  3,  dimana  nampak 
bahwa  jika  dilihat  dari  grafik  rata-rata  skor  masing-masing 
indikator  dan  rata-rata  total  skor  pada  setiap  cluster  yang 
terbentuk dapat dijelaskan bahwa Cluster 0 memiliki nilai rata-
rata skor yang tertinggi dibandingkan dengan  cluster lainnya. 
Dimana  semakin  tinggi  skor  yang  diperoleh  maka  potensi 
terjadinya  fraud  juga  semakin  besar  pada  proyek  yang 
tergabung  pada  cluster  tersebut.  Hal  ini  turut  didukung  oleh 
variabel  indikator-indikator  scoring  lainnya  dimana  terlihat 
bahwa pada Cluster 0 memiliki nilai rata-rata skor yang lebih 
tinggi dibandingkan cluster lainnya dengan perbedaan dengan 
perbedaan yang cukup signifikan. 

Selanjutnya  dapat  dilihat  dimana  nilai  rata-rata  skor  pada 
Cluster 1 dan Cluster 2 nampak tidak berbeda secara signifikan. 
Nampak  pada  Cluster  1  bahwa  nilai  rata-rata  skor  pada 
indikator ‘Perbandingan Nilai Kontrak dan HPS’ relatif tinggi 
hingga  menyamai  nilai  rata-rata  skor  yang  terdapat  pada 
Cluster 0. Sedangkan pada Cluster 2 terlihat bahwa nilai rata-

rata skor pada indikator ‘Pengadaan di kuartal  ke-IV’ sangat 
rendah hingga di bawah skor 2. Hal ini dapat diartikan bahwa 
pada  Cluster  2,  rata-rata  proyek  yang  tergabung di  dalamnya 
dilaksanakan  dalam  kurun  waktu  Kuartal  I  hingga  Kuartal  II 
atau pada Bulan Januari-Juni di tahun tersebut. 

Gambar 3. Grafik Hasil Clustering (Mean) 

 5 / 7 

 
 
 
 
Setelah  memperoleh  hasil  dari  proses  clustering  hingga 
diperoleh  hasil  3  buah  cluster,  tahapan  selanjutnya  yaitu 
dengan mengembangkan atau menindaklanjuti cluster yang ada 
dengan  proses  penyusunan  Decision  Tree  untuk  memperoleh 
gambaran  mengenai 
indikator-indikator  yang  memiliki 
pengaruh terbesar dalam rangka digunakan untuk pengambilan 
keputusan  terbaik  untuk  dapat  diterapkan  dalam  mendeteksi 
adanya  fraud  dalam  data.  Guna  mengukur  kualitas  Decision 
Tree  yang  dihasilkan  secara  kuantitatif  dilakukan  Cross 
Validation pada data latih dan data uji. Berikut merupakan hasil 
dari  pengolahan  data  beserta  Decision  Tree  yang  dihasilkan 
dapat dilihat pada Gambar 4, Gambar 5 dan Gambar 6. Dimana 
nampak  bahwa  beberapa  variabel  yang  dinilai  berpengaruh 
cukup  signifikan  berdasarkan  hasil  dari  Decision  Tree  antara 
lain  ‘skor’,  ‘nilai_kontrak_per_hps’,  ‘menang_berulang’  dan 
‘nilai_kontrak’. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 6. Hasil Decision Tree, diolah 

Tahapan  selanjutnya  yaitu  dilakukan  pengolahan  data 
dengan  memanfaatkan  metode  Association  Rules  atau  biasa 
disebut sebagai  Market Basket Analysis.  Proses ini dilakukan 
guna  mengetahui  ada  atau  tidaknya  hubungan  atau  relasi 
tertentu  antara  Lembaga  Negara  yang  mengadakan  prosesi 
lelang Pengadaan Barang/Jasa dengan perusahaan-perusahaan 
penyedia barang/jasa tersebut. 

Gambar 4. Hasil Cross Validation Decision Tree 

Gambar 5. Hasil Akurasi Decision Tree 

Gambar 7. Hasil Association Rules 

Berdasarkan yang dapat dilihat pada Gambar 7 ditunjukkan 
beberapa nama perusahaan yang dianggap memiliki keterkaitan 
satu  sama  lainnya.  Hal  ini  terbentuk  berdasarkan  beberapa 
faktor,  salah  satu  diantaranya  yaitu  seberapa  sering  suatu 
perusahaan  memenangkan  tender  pada  suatu  lembaga  negara 
atau  di  beberapa  lembaga  negara.  Nampak  bahwa  beberapa 
nama  perusahaan  yang  muncul  merupakan  perusahaan  yang 
bergerak  di  bidang  konstruksi,  jasa  konsultan  konstruksi  dan 
telekomunikasi. 
Tingginya 

frekuensi  suatu  perusahaan  memenangkan 
banyak  tender  di  suatu  LPSE  pemerintahan  daerah,  lembaga 
negara  atau  beberapa  lembaga  negara  sekaligus,  berdasarkan 
pada indikator analisis potensi fraud oleh situs opentender.net 
dapat menjadi suatu indikasi potensi tentang keberadaan fraud 
yang  berkaitan  dengan  adanya  pemenang  berulang  pada 
kegiatan  pengadaan  barang/jasa  [17].  Namun  untuk  dapat 
memastikan dugaan potensi adanya fraud tentu saja diperlukan 
kajian  dan  penelitian  yang  lebih  mendalam  dengan  variabel-
variabel  lainnya  yang  tidak  dapat  diperoleh  hanya  dari 
penarikan data dari situs LPSE. 

 6 / 7 

 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

[11] Kolodiziev, Oleh and Mints, Aleksey and Sidelov, Pavlo and Pleskun, Inna 
and Lozynska, Olha, “Automatic Machine Learning Algorithms for Fraud 
Detection  in  Digital  Payment  Systems”  (October  27,  2020).  Eastern-
European Journal of Enterprise Technologies, 5(9 (107)), 14-26, 2020. 
[12] Phua, Clifton & Alahakoon, Damminda & Lee, Vincent. (2004). “Minority 
Report  in  Fraud  Detection:  Classification  of  Skewed  Data.”  SIGKDD 
Explorations. 6. 50-59. 

[13] Arumsari, Totok P., lswahyudi, Mucharor , and Akib P .. (2014) AUDIT 
ATAS PELAKSANAAN LELANG SECARA ELEKTRONIK DALAM 
PENGADAAN  BARANG  DAN  JASA  PEMERINT  AH.  [Online]. 
Available: 
https://www.bpkp.go.id/investigasi/berita/read/13521/01/AUD1T-ATAS-
PELAKSANAAN-LELANG-SECARA-ELEKTRONIK-DALAM-
PENGADAAN-BARANG-DAN-JASA-PEMERINTAH.bpkp 

[14] Phua, Clifton & Alahakoon, Damminda & Lee, Vincent. (2004). “Minority 
Report  in  Fraud  Detection:  Classification  of  Skewed  Data.”  SIGKDD 
Explorations. 6. 50-59. 

[15] ICW. (2020, 2) Tren Penindakan Kasus Korupsi 2019 [Online]. Available: 
https://www.antikorupsi.org/id/article/tren-penindakan-kasus-korupsi-
2019 

[16] Kompas.  (2021,  3)  Data  ICW  2020:  Kerugian  Negara  Rp  56,7  Triliun, 
Uang Pengganti dari Koruptor Rp 8,9 Triliun [Online]. Available: 
https://nasional.kompas.com/read/2021/03/22/19301891/data-icw-2020-
kerugian-negara-rp-567-triliun-uang-pengganti-dari-koruptor-rp 

[17] opentender.net. Metode OpenTender [Online]. Available: 

https://opentender.net/#/method/ 

VII. 

PENUTUP 

Dalam rangka mendeteksi potensi keberadaan  fraud  dalam 
kegiatan pengadaan barang/jasa  di pemerintahan dapat cukup 
terbantu  dengan  menerapkan  indikator-indikator  yang  dirilis 
oleh  ICW  dalam  platform  opentender.net.  Ditambah  lagi 
dengan cara dikolaborasikan dengan beberapa metode Machine 
Learning dan Data Mining yang tepat sehingga dapat dilakukan 
pengkajian  secara 
indikator-
indikator apa  saja  yang dapat dijadikan acuan dalam langkah 
awal  mendeteksi  potensi  fraud  tersebut.  Performa  yang 
dihasilkan  metode  Decision  Tree  yang  diterapkan  penulis 
sudah  cukup  memuaskan  dengan 
tingkat  akurasi  yang 
mencapai  97,42%,  hanya  saja  masih  banyak  hal  yang  dapat 
ditingkatkan  guna  memperoleh  hasil  yang  lebih  baik,  akurat 
dan  signifikan  untuk  dapat  mendeteksi  adanya  potensi  fraud 
dengan kajian dan penelitian yang lebih mendalam.  

lebih  mendalam  mengenai 

Proses penelitian penulis cukup terhambat dengan minimnya 
data riil untuk melakukan konfirmasi tentang fraud yang benar-
benar terjadi dalam kegiatan pengadaan barang/jasa pemerintah 
sehingga model belum dapat benar-benar dinyatakan valid. Hal 
ini semoga dapat lebih baik di masa depan bersamaan dengan 
pengembangan  yang  dapat  dilakukan  pada  penelitian 
mendatang dengan menerapkan model dan algoritma yang ada 
ke  dalam  suatu  sistem  informasi  yang  memberikan  suatu 
deteksi  dan  peringatan  awal  mengenai  adanya  potensi 
terjadinya fraud. 

DAFTAR PUSTAKA 

[1]  WJS. (2019, 3) WJP Rule of Law Index 2019 [Online]. Available: 

https://worldjusticeproject.org/our-work/research-and-data/wjp-rule-law-
index-2019 

[2]  KPK.  (2020, 6)  Statistik  TPK Berdasarkan  Instansi  [Online].  Available: 
https://www.kpk.go.id/id/statistik/penindakan/tpk-berdasarkan-jenis-
perkara 

[3]  A.  Ludya  &  S.  Candra.  (2017,  10)  Perkembangan  e-Procurement  di 

Indonesia (4) [Online]. Available:  
https://bbs.binus.ac.id/management/2017/11/perkembangan-e-
procurement-di-indonesia-4/  

[4]  ICW. (2020, 2) Tren Penindakan Kasus Korupsi Pengadaan Barang dan 

Jasa Tahun 2019 [Online]. Available: 
https://antikorupsi.org/sites/default/files/dokumen/Tren%20Pengadaan%
202019.pdf 

[5]  Haryati, D., Anditya, A. dan Wibowo, R.A. (2011) “Pelaksanaan 
Pengadaan Barang/Jasa Secara Elektronik (E-Procurement) Pada 
Pemerintah Kota Yogyakarta”, Jurnal Mimbar Hukum, Volume 23, 
Nomor 2 

[6]  LKPP,  ""Senarai  Pengadaan  Barang  1  Jasa  Pemerintah,""  Jurnal  LKPP: 

Senarai, vol. 1, 2011. 

[7]  ICW.  (2019,  4)  Mengawal  Pengadaan  Barang  dan  Jasa  di  Indonesia 

[Online]. Available: 
https://www.antikorupsi.org/id/article/mengawal-pengadaan-barang-dan-
jasa-di-indonesia 

[8]  Rebeca A. Baldomir, Gustavo C. G. Van Erven, and Célia Ghedini Ralha, 
“Brazilian Government Procurements: an Approach to Find Fraud Traces 
in Companies Relationships” in  W. Wijayanto and T. Murata, “Learning 
adaptive graph protection strategy on dynamic networks via reinforcement 
learning,”  in  Tecnologias,  Métodos  e  Teorias  na  Engenharia  de 
Computação, September 2020, pp.8-19 

[9]  Rekha Bhowmik, ""Data Mining Techniques in Fraud Detection,"" Journal 

of Digital Forensics, Security and Law, vol. 3, pp. 35-54,2008. 

[10] Pradheepan Raghavan, Neamat El Gayar, “Fraud Detection using Machine 
Learning  and  Deep  Learning,”  in  2019  International  Conference  on 
Computational Intelligence and Knowledge Economy (ICCIKE),  Dubai, 
United Arab Emirates, 2019, pp. 334-339 

 7 / 7 

 
 
 
 
"
221709837,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Sistem Informasi Manajemen Keuangan 
Monitoring Penggunaan Anggaran Politeknik Statistika STIS 

Moh. Rifki Arif (221709837, 4SI1) 
Dosen Pembimbing: Farid Ridho, MT 

Ringkasan—Selama ini aplikasi SIMAMOV digunakan hanya 
dikhususkan  untuk  membantu  pekerjaan  petugas  BAU. 
Penelitian 
ini  bertujuan  agar  aplikasi  SIMAMOV  dapat 
digunakan  oleh seluruh  pegawai  sehingga  dapat mempermudah 
dalam proses manajemen keuangan. Beberapa penyesuaian yang 
harus  dilakukan  sebelum  sistem  dapat  diimplementasikan  yaitu 
penambahan fitur e-loket, menampilkan POK menurut unit dan 
role  petugas,  dan  menambahkan  fitur  riwayat  detail.  Selain  itu 
juga dilakukan penyesuaian lainnya seperti penambahan pilihan 
tahun  anggaran  di  halaman  POK,  memperbaiki  bugs,  dan 
penyesuaian format POK. Namun fitur e-loket tidak akan dibahas 
melainkan hanya menambahkan fitur input realisasi pada e-loket 
bendahara. Pengembangan sistem ini menggunakan metode RAD 
dengan  metode  pengumpulan  data  wawancara.  Fitur  riwayat 
detail telah dapat menampilkan riwayat perubahan yang terjadi 
pada detail. POK telah dapat disesuaikan menurut unit dan role 
yang telah ditentukan. Input realisasi telah ditambahkan pada e-
loket  bendahara.  Bugs  unggah  telah  dapat  diperbaiki  diikuti 
dengan  penyesuaian  format  POK.  Hasil  pengujian  diketahui 
bahwa semua fungsi telah berjalan sesuai harapan namun perlu 
sosialisasi terlebih dahulu sebelum sistem digunakan. 

Kata Kunci— Sistem Informasi, Anggaran, Manajemen, POK 

I.  LATAR BELAKANG 

Politeknik Statistika STIS adalah perguruan tinggi kedinasan 
di  lingkungan  Badan  Pusat  Statistik.  Pembinaan  secara 
fungsional Politeknik Statistika STIS dilaksanakan oleh Kepala 
Badan  Pusat  Statistik,  namun  dalam  pembinaan  secara  teknis 
akademik  dilaksanakan  oleh  menteri  yang  menyelenggarakan 
urusan pemerintahan di bidang pendidikan tinggi dalam hal ini 
adalah  Menteri  Riset,  Teknologi,  dan  Pendidikan  Tinggi  [1]. 
Visi  dan  misi  dari  perguruan  tinggi  tersebut  yaitu  menjadi 
perguruan  tinggi  yang  berkualitas  dan  dapat  memberikan 
kontribusi  nyata  terhadap  sistem  statistik  nasional  maupun 
internasional dengan menyelenggarakan pendidikan, penelitian 
dan  pengabdian  terhadap  masyarakat  dalam  bidang  statistika 
terapan dan komputasi statistik [2]. 

Dalam  mewujudkan  pendidikan  yang  berkualitas  perlu 
adanya  pengelolaan  secara  menyeluruh  dan  profesional 
terhadap sumber daya yang ada dalam perguruan tinggi. Salah 
satu  sumber  daya  yang  perlu  dikelola  dengan  baik  dalam 
perguruan tinggi adalah masalah keuangan. Dalam pengelolaan 
keuangannya  selama  ini  Politeknik  Statistika  STIS  dikelola 
dalam  aplikasi  SIMAMOV  dengan  menggunakan  data 
Petunjuk  Operasional  Kegiatan  (POK)  yang  diperoleh  dari 
Sistem  Aplikasi  Keuangan  Tingkat  Instansi  (SAKTI)  dengan 
cara mengunggah output dari aplikasi SAKTI yang berekstensi 
XLSX  atau  Microsoft  Excel.  Sistem  Aplikasi  Keuangan 
Tingkat  Instansi  (SAKTI)  merupakan  integrasi  dari  berbagai 
aplikasi  yang  telah  digunakan  oleh  Satuan  Kerja  (Satker). 
Selain  integrasi  aplikasi  SAKTI  juga  dirancang  berdasarkan 

proses  bisnis  SPAN  yang  baru.  Proses  Bisnis  tersebut 
dituangkan dalam beberapa modul [3]. 

DIPA adalah dokumen pelaksanaan anggaran yang disusun 
oleh  Pengguna  Anggaran/Kuasa  Pengguna  Anggaran  dan 
disahkan  oleh  Direktur  Jenderal  Perbendaharaan  atau  Kepala 
Kantor Wilayah Direktorat Jenderal Perbendaharaan atas nama 
Menteri  Keuangan  selaku  Bendahara  Umum  Negara  (BUN). 
DIPA berlaku untuk satu tahun anggaran dan memuat informasi 
satuan-satuan 
sebagai  dasar 
terukur  yang  berfungsi 
pelaksanaan  kegiatan  dan  penggunaan  anggaran.  Selain  itu, 
DIPA  berfungsi  sebagai  alat  pengendali,  pelaksanaan, 
pelaporan,  pengawasan,  dan  sekaligus  merupakan  perangkat 
akuntansi  pemerintah.  Pagu  dalam  DIPA  merupakan  batas 
pengeluaran 
tidak  boleh  dilampaui  dan 
pelaksanaannya harus dapat dipertanggungjawabkan [4]. POK 
merupakan  penjabaran  lebih  lanjut  dari  DIPA  yang  berisikan 
uraian 
rencana  kerja  beserta  biaya  yang  dibutuhkan. 
DIPA/POK dapat diubah mengikuti kondisi yang dihadapi pada 
saat  pelaksanaannya  sehingga  perlu  adanya  revisi  terhadap 
DIPA/POK.  Oleh  karena  itu,  maka  perlu  adanya sistem  yang 
dapat memonitor anggaran yang sudah direalisasikan dan sisa 
pagu  yang  belum  digunakan  sehingga  dapat  membantu 
pengambilan keputusan dari stakeholder. 

tertinggi  yang 

SIMAMOV  adalah  sebuah  aplikasi  berbasis  web  yang 
dikembangkan oleh Muh. Shamad lulusan Politeknik Statistika 
STIS tahun 2017. SIMAMOV merupakan sebuah aplikasi yang 
digunakan  untuk  manajemen  administrasi,  monitoring,  dan 
evaluasi  anggaran  di  Politeknik  Statistika  STIS.  Aplikasi 
SIMAMOV  dilengkapi  dengan  beberapa  fitur  penting  yang 
dapat  memudahkan  dalam  pengelolaan  anggaran  yaitu 
pengelolaan  POK,  Pengelolaan  SPPD,  pengelolaan  SPJ,  dan 
pengelolaan pegawai. Pengelolaan POK merupakan fitur untuk 
mengelola data POK dan DIPA yang berupa upload data DIPA, 
revisi  atau  edit  POK,  dan  monitoring  realisasi.  Pengelolaan 
SPPD  adalah  fitur  untuk  membuat  Surat  Tugas,  Surat  Tugas 
Biasa,  dan  Surat  Perhitungan  tidak  dapat  digunakan  sebelum 
pengaturan  Penanda  Tangan  Surat  Tugas,  Penandatangan 
Legalitas, Pejabat Pembuat Komitmen (PPK), dan Bendahara 
ditentukan.  Pengelolaan  SPJ  digunakan  untuk  membuat  SPJ 
Honor Dosen dan SPJ Transport Dosen Non STIS yang hanya 
setelah  pengaturan  Pejabat  Pembuat 
dapat  digunakan 
Komitmen (PPK) dan Bendahara ditentukan pada Pengaturan 
SPPD. Dan untuk fitur pengelolaan pegawai digunakan untuk 
mengelola data pegawai mulai dari menambah dan menghapus 
data  pegawai  sampai  melihat  riwayat  penerimaan  dari  setiap 
pegawainya. 

Pada  aplikasi  SIMAMOV  yang  digunakan  memiliki 
beberapa  kekurangan  diantaranya  yaitu  hanya  dapat  melihat 
DIPA  pada  satu  tahun  anggaran  saja  yang  lebih  baik  jika 
pengguna  dapat  dengan  mudah  melihat  tahun  anggaran  yang 

 1 / 7 

 
 
 
 
mereka butuhkan. Selain itu, aplikasi SIMAMOV masih belum 
dapat  menampilkan  riwayat  perubahan  dari  detail  POK 
sehingga pengguna tidak dapat mengetahui perubahan apa saja 
yang  telah  terjadi  dan  sudah  berapa  kali  direvisi.  Proses 
pengajuan  dana  pada  saat  ini  masih  dilakukan  secara  manual 
oleh  Biro  Administrasi  Umum  (BAU)  Politeknik  Statistika 
STIS pada aplikasi SIMAMOV. Pemberitahuan tentang update 
dari pengajuan dana tersebut diberitahukan melalui email yang 
masih dilakukan secara manual oleh pegawai BAU. Sehingga, 
pegawai  BAU  sering 
lupa  untuk  mengirimkan  email 
pemberitahuan tersebut yang mengakibatkan proses pengajuan 
dana  tidak  dapat  terpantau.  Pada  aplikasi  SIMAMOV  yang 
digunakan  sekarang 
terdapat  beberapa  bugs  yaitu 
dikarenakan  adanya  perubahan  struktur  DIPA  dari  aplikasi 
SAKTI mengakibatkan error saat akan mengunggah data DIPA 
juga  pembacaan  berkas  yang  kurang  baik  yang 
dan 
mengikutsertakan baris kosong sehingga mengakibatkan error 
pada  saat  validasi  data.  Aplikasi  SIMAMOV  yang  sekarang 
belum dapat membedakan tampilan pada halaman POK untuk 
tiap unitnya yang mengakibatkan unit dapat melihat POK dari 
unit lain yang seharusnya tidak boleh terlihat. 

juga 

Penelitian 

ini  dilakukan  dalam  bentuk  proyek  yang 
dikerjakan  oleh  dua  orang  yang  di  mana  pada  penelitian  ini 
hanya  akan  membahas  sebagian  dari  masalah  yang  ada  pada 
aplikasi  SIMAMOV.  Masalah  yang  akan  dibahas  pada 
penelitian ini yaitu yang pertama tentang belum adanya akses 
untuk mengganti tahun anggaran pada saat di dalam aplikasinya 
dan  yang  kedua  yaitu  tentang  fitur  riwayat  perubahan  POK 
yang  belum  dapat  melihat  detail  dari  perubahan  yang  telah 
terjadi. 

II.  TUJUAN PENELITIAN 

Berdasarkan  latar  belakang  di  atas,  maka  tujuan  dari 

penelitian ini adalah sebagai berikut: 

1.  Menambahkan  fitur  riwayat  perubahan  detail  pada 

POK 

2.  Menambahkan  menu  pilihan  unit  pada  detail  POK  
yang  dilanjutkan  dengan  menyesuaikan  tampilan 
pengelolaan  POK  berdasarkan  unit  dan  memberikan 
akses  terhadap  pegawai  BAU  Politeknik  Statistika 
STIS berdasarkan role yang telah ditentukan. 

3.  Menambahkan fitur input realisasi ke loket bendahara 
4.  Memperbaiki bugs dan validasi dokumen pada upload 
POK yang dilanjutkan dengan menyesuaikan tampilan 
POK dengan format POK yang baru 

5.  Menambahkan  pilihan  tahun  anggaran  ke  halaman 

pengelolaan POK  

III. PENELITIAN TERKAIT 

SIMAMOV  merupakan  sebuah  sistem  informasi  yang 
dikembangkan  oleh  Muh.  Shamad  pada  tahun  2017  dalam 
skripsinya  yang  berjudul  Pembangunan  Sistem  Informasi 
Manajemen Administrasi, Monitoring, dan Evaluasi anggaran 
STIS [5].  Beberapa referensi lainnya yang saya gunakan yaitu 
penelitian  dari  Adiwarman  P.  Paputungan,  Yaulie  Deo  Y. 
Rindengan,  dan  Steven  R.  Sentinuwo  dalam  skripsinya  yang 
berjudul  Sistem  Monitoring  Dan  Evaluasi  Anggaran 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pendapatan  Belanja  Daerah  (APBD)  Di  Kabupaten  Bolaang 
Mongondow Selatan Berbasis Web [6], Yesita Astarina, S.E., 
M.Si  yang  berjudul  Perancangan  Sistem  Informasi  Anggaran 
Pada  Stie  Lembah  Dempo  Pagar  Alam  [7],  dan  Alam 
Rahmatulloh  dam  Husen  yang  berjudul  Sistem  Informasi 
Manajemen Anggaran (Simangga) Perguruan Tinggi Berbasis 
Web(Studi Kasus : Universitas Siliwangi) [8]. 

Referensi  [6]  menitikberatkan  pada  monitoring  terhadap 
penggunaan  anggaran  pendapatan  daerah  di  Bolaang 
Mongondow yang ditampilkan secara interaktif melalui grafik 
dan tabel realisasi pada aplikasi berbasis web. 

sudah 

Referensi  [7]  membahas  tentang  pembangunan  sistem 
pengelolaan  anggaran  berbasis  desktop.  Dimana  dalam 
pengelolaannya 
dapat 
mempermudah  pegawai  dalam  melakukan  kegiatan  rutin 
sehari-hari  yang  berhubungan  dengan  anggaran  sehingga 
meningkatkan  kinerja  di  STIE  Lembah  Dempo  [7].  Dengan 
dibangunnya  sistem  informasi  juga  dapat  meminimalisasi 
terjadinya human error dan menghindari over budget. 

terintegrasi 

sehingga 

Referensi  [8]  membahas  tentang  pembangunan  sistem 
manajemen  anggaran  berbasis  web  untuk  perguruan  tinggi 
yang  dimana  fitur  yang  dimuat  dalam  sistem  terbilang  cukup 
lengkap  mulai  dari  dashboard  yang 
informatif  sampai 
pengelolaan  anggaran  yang  cukup  lengkap.  Beberapa  fitur 
tersebut  relevan  dengan  apa  yang  akan 
dalam  sistem 
dikembangkan  oleh  penulis  yaitu  penganggaran  pada  sistem 
tersebut  sudah  dibagi  menurut  unitnya  masing-masing  dan 
proses  pengajuan 
revisi  yang 
dikembangkan oleh penulis akan sedikit berbeda karena proses 
pengelolaan POK akan dilakukan pada aplikasi yang berbeda 
sehingga  perlu  adanya  penyesuaian  secara  manual  oleh 
binagram. 

revisi.  Namun  untuk 

Referensi  [5]  menjelaskan  tentang  cara  kerja  SIMAMOV 
yang  digunakan  pada  saat  ini.  Referensi  tersebutlah  yang 
menjadi  acuan  utama  dalam  pengembangan  sistem  ini  yang 
dimana  penulis  akan  melakukan  pengembangan  terhadap 
sistem tersebut. Pada referensi [5] tersebut aplikasi SIMAMOV 
hanya  dikhususkan  untuk  penggunaan  internal  BAU  saja. 
Sedangkan,  untuk  penelitian  yang  penulis  lakukan  yaitu 
bertujuan  agar  sistem  tersebut  dapat  digunakan  oleh  seluruh 
unit  secara  terintegrasi.  Sehingga,  dapat  mempermudah  dan 
mempercepat dalam pengelolaan anggaran Politeknik Statistika 
STIS. 

IV. METODE PENELITIAN  
Metode yang akan digunakan dalam Pengembangan Sistem 
Informasi  Keuangan  yaitu  adalah  metode  RAD  (Rapid 
Application  Development).  Karena  dalam  pengembangan 
sistem ini dikerjakan dalam bentuk proyek yang terdiri dari dua 
orang sehingga pemilihan metode ini adalah yang paling tepat 
karena  salah  satu  keuntungan  dari  penggunaan  metode  RAD 
yaitu  setiap  fungsi  dapat  dimodulkan  dan  kemudian  dapat 
diintegrasikan  sehingga  waktunya  lebih  efisien.  Kelebihan 
lainnya  dari  metode  RAD  Berikut 
tahapan 
pengembangan sistem menggunakan metode RAD 

adalah 

 2 / 7 

 
 
 
1.  Requirement Analysis 

Gambar 1. Tahapan metode RAD 

Mengumpulkan  informasi  tentang  permasalahan  saat  ini, 
mengidentifikasi  persyaratan  proyek,  dan  penyelesaian 
yang disetujui oleh pemangku kepentingan 

2.  Studi Pendahuluan 

Mengumpulkan  beberapa  referensi  yang  berkaitan  dengan 
permasalahan yang ada. 

3.  Perancangan Sistem 

Perancangan sistem dengan metode RAD terdiri dari empat 
siklus  tahapan  yaitu  desain,  pembangunan  prototipe, 
demonstrasi terhadap pengguna, jika sistem yang dibangun 
tidak  sesuai  dengan  kebutuhan  pemangku  kepentingan 
maka  dilakukan 
terhadap  prototype  untuk 
selanjutnya diperbaiki namun jika sudah sesuai maka dapat 
dilanjutkan ke fase berikutnya yaitu testing 

review 

4.  Testing 

Testing  dilakukan  dengan  metode  black  box  dan  untuk 
pengujian usability dilakukan dengan metode SUS (System 
Usability Scale) 

V.  KERANGKA PIKIR 
Kerangka pikir yang penulis gunakan dalam penelitian ini 

adalah sebagai berikut: 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

VI. HASIL DAN PEMBAHASAN 

1.  Riwayat perubahan detail POK 

Rancangan flowchart dari unggah detail POK dapat kita 

lihat pada gambar di bawah ini. 

Gambar 3.  Flowchart unggah revisi POK 

Dari  flowchart  dijelaskan  bahwa  akan  adanya  input 
manual dari user yaitu binagram selaku sebagai user yang 
bertanggung  jawab  terhadap  revisi  detail  pada  POK.  Dari 
gambar tersebut dapat kita ketahui bahwa proses perubahan 
pada  POK  dimulai  dari  pengunggahan  berkas  revisi  POK 
pada  halaman  pengelolaan  POK  segmen  unggah.  Sistem 
akan  melakukan  pengecekan  atau  pencocokan  POK  baru 
dengan POK lama dan hasil perubahan yang terdeteksi akan 
ditampilkan pada tabel perubahan POK. Perubahan berupa 
item  atau  detail  baru  akan  ditambahkan  tombol  “timpa 
 yang  di  mana  tombol  tersebut  berfungsi  untuk 
detail” 
mengidentifikasi  bahwa  detail 
tersebut  merupakan 
perubahan dari detail yang ditimpa. Setelah itu perubahan 
baru  yang  terjadi  akan  ditampilkan  kembali  pada  tabel 
tersebut. Detail yang ditimpa tidak akan hilang kecuali yang 
memang  telah  dihapus  dan  setiap  detail  dapat  ditimpa 
berkali-kali.  Jika  perubahan  tidak  sesuai  atau  salah  maka 
dapat dikembalikan dengan menekan tombol “kembalikan 
dan  buat  baru” 
.  Menekan  tombol  tersebut  akan 
mengembalikan  perubahan  dan  mengembalikan  detail 
tersebut menjadi detail baru kembali. 

Gambar 2. Kerangka pikir 

Gambar 4.  Tampilan tabel perubahan POK 

 3 / 7 

 
 
 
 
 
 
 
Beberapa jenis revisi detail yang dapat dilakukan pada 

sistem tersebut adalah sebagai berikut: 
1.  Revisi terhadap detail secara keseluruhan 

Jika mengganti salah satu detail dengan detail yang baru. 
Detail yang akan diganti dengan detail yang baru dapat 
dihapus  pada  revisi  POK.  Detail  yang  ditambahkan 
dapat menimpa detail yang telah terhapus. 

2.  Pemecahan sebagian dari detail menjadi detail baru 

Jika ingin mengalihkan sebagian dari detail yang lama 
ke  detail  yang  baru.  Detail  yang  sebagian  dari 
volumenya  akan  digantikan  tersebut  pada  revisi  POK 
yang  akan  diunggah  volume  dari  detail  tersebut  dapat 
dikurangi. Detail yang baru dapat menimpa detail yang 
volumenya telah dikurangi. 

3.  Pemecahan  detail  menjadi  dua  atau  lebih  detail  yang 

berbeda 
Jika ingin mengganti detail detail dengan lebih dari satu 
detail  yang  baru.  Detail  yang  akan  dipecah  tersebut 
dihapus pada revisi POK. Setelah itu, detail yang baru 
dapat menimpa detail yang telah terhapus tersebut. 

4.  Penggabungan detail 

Jika  terdapat  dua  atau  lebih  detail  POK  yang  akan 
digantikan  dengan  detail  baru.  Maka  pada  saat  revisi 
detail  yang  akan  digantikan  tersebut  dapat  dihapus. 
Detail baru dapat menimpa satu persatu detail yang telah 
dihapus. 

5.  Penghapusan detail 

Detail yang tidak digunakan dapat dihapus pada revisi 
POK  dan  pagu  dari  detail  tersebut  dapat  ditambahkan 
pada detail lainnya. 
6.  Penambahan detail 

Jika  terdapat  masih  terdapat  pagu  yang  tersisa  dapat 
ditambahkan pada detail baru pada saat revisi POK. 
Riwayat  perubahan  dapat  kita  lihat  pada  menu  entri 

POK dengan flowchart sebagai berikut: 

Gambar 5.  Halaman beranda POK 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Sebelum dapat melihat riwayat detail POK maka perlu 
ditambahkan 
tombol  untuk  mengaksesnya  dengan 
menambahkan tombol riwayat pada setiap detail yang old-
nya tidak kosong atau memiliki panjang array lebih dari nol. 

Gambar 9.  Tombol riwayat pada detail POK 

Riwayat  dari  detail  target  ditampilkan  ketika menekan 
tombol  riwayat  yaitu  dengan  cara  menambahkan  revisi 
detail yang berupa perbandingan perubahan yang terjadi ke 
tabel  riwayat  yang  dimulai  dari  old  dengan  array  index 
terbesar ke terendah atau nol. Setelah itu tabel ditampilkan 
dalam bentuk modal seperti gambar di bawah ini. 

2.  Perbaikan Bugs dan penyesuaian format POK 

Gambar 6.  Modal tabel riwayat 

Bugs  yang  terdapat  pada  SIMAMOV  yaitu  ikut 
terbacanya baris kosong pada berkas diunggah. Oleh karena 
itu,  penulis  menambahkan  filter  terlebih  dahulu  untuk 
menghilangkan  baris  kosong  tersebut  sebelum  data  dapat 
divalidasi. Validasi juga mengalami kegagalan dikarenakan 
terjadinya  perubahan  terhadap  struktur  dari  POK.  Untuk 
mengatasi masalah tersebut maka struktur POK disesuaikan 
dengan struktur yang baru. Struktur yang baru mengalami 
perubahan  penamaan  yang  dimana  pada  awalnya  struktur 
POK-nya  adalah  program/  kegiatan/  output/  suboutput/ 
komponen/  subkomp/  detil  berubah  menjadi  program/ 
aktivitas/ kro/ ro/ komponen/ subkomp/ detil. 

Detil pada POK juga terdapat perubahan yang cukup 
terdapat 
signifikan  dimana  pada  POK  yang  baru 
pengelompokan  terhadap  detil.  Perubahan  pada  detil 
tersebut  mengakibatkan  perlu  adanya  penyesuaian  ulang 
terhadap  terhadap  tampilan  POK,  perhitungan  pagu,  dan 
perhitungan  realisasinya.  Untuk  contoh  dari  format  POK 
yang baru dapat dilihat pada gambar di bawah ini: 

3.  Tampilan pengelolaan POK menurut unit dan role masing 

Gambar 7.  Contoh format POK yang baru 

masing petugas BAU 

Detail  pada  POK  belum  bisa  dibedakan  untuk  tiap 
unitnya karena belum dilengkapi dengan atribut yang dapat 
digunakan  untuk  membedakannya.  Untuk  membedakan 

 4 / 7 

 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

view tersebut maka diperlukan form yang dapat diakses oleh 
satu orang sebagai editor untuk menentukan tiap detailnya 
dapat  dilihat  oleh  unit  apa  saja.  Flowchart  untuk  form 
tersebut dapat kita lihat pada gambar dibawah ini. 

Gambar 10.  Halaman beranda POK 
Tampilan  POK  disesuaikan  dengan  unit  dan  role  dari 
pengguna.  POK  hanya  akan  ditampilkan  menurut  unitnya 
saja dan tidak ada akses untuk mengedit apapun di halaman 
pengelolaan  POK  kecuali  untuk  beberapa  yang  memiliki 
role khusus seperti: 

1.  Bendahara  dapat  melihat  keseluruhan  POK  dan 
dapat  menginput  realisasi  langsung  pada  halaman 
POK entri 

2.  Binagram  dapat  melakukan  unggah  POK  jika 

terdapat revisi pada POK sebelumnya 
3.  Administrator  memiliki  akses  penuh 

terhadap 

pengelolaan POK 

4.  Pimpinan (direktur atau wakil direktur) dan petugas 
BAU  lainnya  dapat  melihat  keseluruhan  POK 
namun tidak memiliki akses mengedit POK 
4.  Menambahkan form realisasi pada e-loket bendahara 

Modal untuk form realisasi pada SIMAMOV yang lama 
terletak  pada  tabel  entri  POK  yang  berada  pada  halaman 
POK pada bagian entri seperti pada gambar di bawah ini. 

Gambar 8.  Halaman beranda POK 
Dari flowchart diatas sebelum menampilkan tabel POK 
perlu  menambahkan  button/anchor  terlebih  dahulu  pada 
kolom  unit  di  setiap  detail  dari  POKnya.  Button/anchor 
tersebut digunakan untuk mengakses modal dari form unit. 
Button bertuliskan set akan ditambahkan pada detail yang 
belum  terdapat  atribut  unit  di  dalamnya  dan  jika  sudah 
pernah diedit sebelumnya tombol tersebut akan digantikan 
dengan  anchor  yang  bertuliskan  unit  apa  saja  yang  telah 
dipilih  untuk  detail  tersebut.  Untuk  lebih  jelasnya  dapat 
dilihat pada gambar di bawah ini. 

Gambar 9.  Halaman beranda POK 
Jika  button/anchor  tersebut  di  klik  oleh  editor  maka 
akan  ditampilkan  modal  dari  form  unit  yang  di  mana  jika 
belum  terdapat  atribut  didalamnya  (button)  form  tersebut 
belum terisi dan jika sudah pernah diedit sebelumnya maka 
form  tersebut  akan  berisikan  pilihan  sebelumnya.  Untuk 
lebih  jelasnya  form  dapat  kita  lihat  pada  gambar dibawah 
ini. 

Gambar 11.  Form realisasi pada halaman POK 
Form tersebut seharusnya juga berada pada halaman e-
loket  bagian  bendahara  supaya  dapat  memudahkan 
bendahara dalam menginput realisasi dari permintaan dana 
yang  ada.  Sehingga,  bendahara  tidak  perlu  lagi  mencari 
detail  yang  dimaksudkan  dalam  permintaan  dana  pada 
halaman POK entri. Oleh karena itu, form realisasi tersebut 
perlu  ditambahkan  ke  halaman  loket  bagian  bendahara 
seperti pada gambar di bawah ini. 

 5 / 7 

 
 
 
 
  
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

TABEL I 
KESIMPULAN HASIL PENGUJIAN BLACKBOX 

Pernyataan 
Dapat mengubah tahun anggaran yang ditampilkan 

Pada halaman POK, Data yang ditampilkan sesuai 
dengan tahun anggaran yang dipilih 

Pada halaman POK entri, Tombol riwayat detail hanya 
muncul pada detail yang pernah direvisi 

Data yang ditampilkan pada tabel riwayat detail sudah 
sesuai dengan revisi yang dilakukan 

Binagram, bendahara, pimpinan, dan admin dapat 
melihat POK secara keseluruhan 

Binagram dan admin dapat mengakses unggah POK 

Binagram dan admin dapat melakukan revisi detail (1-
1 atau 1-2 dst) 

Tombol riwayat detail muncul pada detail yang telah 
direvisi setelah reload halaman pada halaman POK 
entri 

Bendahara dan admin dapat mengakses form input 
realisasi pada halaman POK entri 

Tombol input realisasi pada halaman POK entri hanya 
muncul pada tahun anggaran yang sedang berjalan 

Input realisasi pada halaman form bendahara ketika 
memproses permintaan dana, dapat berfungsi 
sebagaimana mestinya 

Admin dapat mengakses form unit pada halaman POK 
edit dan menentukan unit dari setiap detailnya 

Admin dan binagram dapat mengunggah POK (awal 
maupun revisi) 

Benar 

Salah 

✓ 

✓ 

✓ 

✓ 

✓ 

✓ 

✓ 

✓ 

✓ 

✓ 

✓ 

✓ 

✓ 

Dari tabel di atas diketahui bahwa semua fungsi telah 
dapat  berjalan  sebagaimana  mestinya.  Hasil  pengujian 
System  Usability  Scale  yang  diperoleh  adalah  sebagai 
berikut: 

TABEL II 
KESIMPULAN HASIL PENGUJIAN BLACKBOX 

Pertanya
an ke- 

1 

2 

P
o
i
n

d
a
r
i

r
e
s
p
o
n
d
e
n

k
e
-

1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 

3 
3 
4 
4 
2 
2 
2 
3 
2 
3 
3 
3 
4 
3 
3 

2 
2 
2 
3 
3 
2 
2 
2 
1 
2 
3 
4 
3 
4 
3 

3 

2 
2 
3 
3 
3 
3 
2 
2 
2 
2 
3 
4 
3 
4 
3 

4 

1 
3 
3 
3 
2 
2 
3 
4 
1 
2 
3 
4 
2 
4 
3 

5 

6 

3 
2 
2 
3 
3 
3 
3 
2 
3 
3 
2 
3 
2 
1 
3 
3 
1 
3 
3 
3 
3 
3 
3 
4 
2 
3 
4 
4 
3 
3 
Skor 

7 

3 
3 
3 
3 
3 
3 
2 
2 
2 
2 
3 
3 
3 
3 
3 

8 

3 
2 
3 
3 
3 
2 
1 
3 
1 
3 
3 
4 
3 
4 
3 

9 

3 
3 
3 
3 
2 
2 
1 
3 
2 
2 
3 
4 
3 
4 
3 

10 

Jumlah 

Poin 

0 
1 
0 
3 
0 
0 
0 
1 
0 
0 
3 
4 
1 
4 
3 

22 
24 
27 
30 
24 
21 
16 
26 
15 
22 
30 
37 
27 
38 
30 

55 
60 
67,5 
75 
60 
52,5 
40 
65 
37,5 
55 
75 
92,5 
67,5 
95 
75 
64,8 

 6 / 7 

Gambar 12.  Halaman beranda POK 

Tombol pada halaman tersebut hanya akan aktif jika 
form detail dan jumlah sudah terisi dan akan nonaktif jika 
keduanya belum terisi.  

5.  Tahun anggaran 

Berikut flowchart dari tahun anggaran sebelum (kiri) 

dan sesudah (kanan) dilakukan perubahan. 

flowchart 

Gambar 13. Flowchart Tahun Anggaran sebelum dan sesudah 
Dari 

tersebut  dapat  diketahui  bahwa 
SIMAMOV  yang  lama  hanya  meletakkan  pilihan  tahun 
anggaran  pada  halaman  login  sehingga  jika  ingin  melihat 
atau  berpindah  tahun  anggaran  user  diharuskan  untuk 
logout  terlebih  dahulu  dan  login  ulang  dengan  tahun 
anggaran yang diinginkan sehingga kurang fleksibel. Pada 
SIMAMOV yang sekarang pemilihan tahun anggaran juga 
terdapat  pada  halaman  POK  sehingga  user  dapat  dengan 
mudah  mengubah  tahun  anggaran  pada  halaman  tersebut 
tanpa harus logout. Untuk berpindah tahun anggaran pada 
saat  setelah  login  dapat  menggunakan  pilihan  tahun 
anggaran  yang  terletak  pada  halaman  POK  seperti  pada 
gambar di bawah ini. 

6.  Pengujian 

Gambar 14.  Halaman beranda POK 

Pengujian  dilakukan  pada  21  responden  dengan 
rincian 5 orang petugas BAAK, 12 orang petugas BAU dan 
4  orang  Mahasiswa.  Metode  pengujian  yang  digunakan 
pada  penelitian  ini  yaitu  blackbox  dan  System  Usability 
Scale  (SUS).  Dari  21  responden  tersebut  hanya  15 
responden  yang  memberikan  respon.  Kesimpulan  hasil 
yang  diperoleh  dari  pengujian  blackbox  ditampilkan  pada 
tabel dibawah ini: 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
              
Hasil  dari  pengujian  SUS  tidak  cukup  memuaskan 
yang dimana skor yang diperoleh hanya sebesar 64,8 yang 
dimana  berada  pada  rentang  marginal  high  dan  tergolong 
ok.  Skor  tersebut  belum  cukup  baik  dan  pengguna  masih 
mengalami  sedikit  kesulitan  dalam  menggunakan  sistem 
tersebut. Hal tersebut kemungkinan besar diakibatkan oleh 
sosialisasi akan sistem tersebut yang belum maksimal yang 
hanya  berupa  berkas  pdf 
tentang  petunjuk  cara 
penggunaannya saja. 

VII. 

PENUTUP 

Fitur  riwayat  perubahan  detail  pada  POK  telah  dapat 
diterapkan  dan  telah  dapat  menampilkan  perubahan  yang 
terjadi pada detail POK meskipun perubahan detail tidak dapat 
teridentifikasi  secara  otomatis  dan  masih  memerlukan 
penyesuaian  secara  manual  oleh  binagram  karena  perubahan 
tidak  dilakukan  pada  aplikasi  yang  sama.  Menu  pilihan  unit 
pada detail POK telah ditambahkan dan dapat digunakan oleh 
admin  sehingga  tampilan  pengelolaan  POK  sudah  dapat 
disesuaikan  berdasarkan  unit  dan  role  yang  telah  ditentukan. 
Input realisasi telah dapat ditambahkan ke loket bendahara dan 
dapat  berfungsi  dengan  baik  sehingga  bendahara  dapat 
langsung mengisi realisasi tanpa harus mencari detail tersebut 
pada  halaman  POK  entri.  Bugs  dan  validasi  dokumen  POK 
telah diperbaiki dan format POK telah disesuaikan dengan POK 
yang baru. 

Hasil pengujian blackbox yang dilakukan didapatkan bahwa 
semua  fungsi  sudah  dapat  berjalan  sebagaimana  mestinya 
sehingga  aplikasi  ini  sudah  layak  untuk  digunakan.  Namun, 
hasil  pengujian  SUS  yang  diperoleh  tidak  sesuai  dengan 
harapan. Dari hasil yang diperoleh diketahui bahwa kurangnya 
sosialisasi tentang sistem baru tersebutlah yang mengakibatkan 
kecilnya  skor  yang  diperoleh  yaitu  64,8.  Hal  tersebut  dapat 
diketahui  dari  pengguna  yang  mengalami  kesulitan  dalam 
memahami cara kerja dari sistem tersebut sehingga sebaiknya 
dilakukan sosialisasi terlebih dahulu sebelum diterapkan. 

DAFTAR PUSTAKA 

[1]  Politeknik  Statistika  STIS.  Sejarah  Singkat 

https://stis.ac.id/hal/16/sejarah-singkat 

[2]  Politeknik  Statistika  STIS.  Visi  dan  Misi 

https://stis.ac.id/hal/17/visi-dan-misi 

[Online].  Available: 

[Online].  Available: 

[3]  Ditjen  Perbendaharaan  Kemenkeu  RI.  Apa  itu  SAKTI???  [Online]. 
http://djpb.kemenkeu.go.id/kppn/majene/id/data-

Available: 
publikasi/berita-terbaru/2813-apa-itu-sakti.html  

[4]  Petunjuk Penyusunan dan Pengesahan Daftar Isian Pelaksanaan Anggaran, 
NOMOR  164 /PMK.05/2011, Peraturan   Menteri    Keuangan, Indonesia, 
2011 

[5]  M. Shamad, “Pembangunan  Sistem Informasi Manajemen Administrasi, 
Monitoring,  dan  Evaluasi  anggaran  STIS,”  Komputasi  Statistik  Sekolah 
Tinggi Ilmu Statistik. Jakarta, Indonesia, 2017. 

[6]  A.  P.  Paputungan,  Y.  D.  Y.  Rindengan,  and  S.  Sentinuwo,  “Sistem 
Monitoring Dan Evaluasi Anggaran Pendapatan Belanja Daerah (APBD) 
Di  Kabupaten  Bolaang  Mongondow  Selatan  Berbasis  Web,”  Teknik 
Informatika Universitas Sam Ratulangi. Manado, Indonesia, 2017. 
[7]  Y. Astarina, S.E., M.Si, “Perancangan  Sistem  Informasi Anggaran Pada 
STIE  Lembah  Dempo  Pagar  Alam,”  Jurnal  Sistem  Informasi Komputer 
dan Teknologi Informasi (SISKOMTI), vol. 1, no. 1, pp. 40-54, 2019 
[8]  A.  Rahmatulloh,  Husen,  “Sistem  Informasi  Manajemen  Anggaran 
(Simangga)  Perguruan  Tinggi  Berbasis  Web  (Studi  Kasus  : Universitas 
Siliwangi),” Jurnal Edukasi dan Penelitian Informatika (JEPIN), vol. 3, no. 
2, pp. 89-95, 2017 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

 7 / 7 

 
 
 
 
 
"
221709818,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Kajian Kesiapan Pemanfaatan Data Google Maps 
untuk Pemenuhan Variabel Jumlah dan Jarak 
Infrastruktur pada Data Potensi Desa 
(Studi Kasus Kota Yogyakarta) 

Masyitah Ayuning Setyo (221709818, 4SD2) 
Dosen Pembimbing: Dr. Drs. Waris Marsisno, M.Stat 

jumlah  dan 

Ringkasan— Salah satu Official Statistics yang dipublikasikan 
oleh  Badan  Pusat  Statistik  adalah  Statistik  Potensi  Desa 
(PODES).  Pada  kuesioner  PODES,  salah  satu  pertanyaan  yang 
banyak  ditanyakan  adalah  mengenai 
jarak 
infrastruktur  dalam  suatu  desa.  Variabel  ini  diperlukan  untuk 
penyusunan berbagai indeks sehingga di-update setiap tahunnya 
di luar tahun pendataan PODES. Tersedianya Big Data memiliki 
potensi  untuk  memudahkan  pemutakhiran  data  PODES.  Pada 
penelitian  sebelumnya,  Google  Maps  sebagai  suatu  Big  Data, 
disebutkan  dapat  menjadi  sumber  pengisian  data  PODES. 
Penelitian  ini  mengkaji  lebih  dalam  terkait  proses  yang  perlu 
diperhatikan  jika  menggunakan  Google  Maps  sebagai  sumber 
data  dari  Official  Statistics,  serta  menganalisis  keakuratan  data 
yang dihasilkan oleh Google Maps. Pada penelitian ini dibangun 
Web 
untuk 
mengumpulkan  data  infrastruktur  di  Kota  Yogyakarta  dari 
Google Maps. Pada penelitian yang dilakukan, ditemukan bahwa 
proses  pengumpulan  dan  pre-processing  data  membutuhkan 
waktu  dan  proses  yang  lama  dan  memiliki  akurasi  data  yang 
masih  sangat  rendah  dalam  mengestimasi  jumlah  infrastruktur 
per  desa.  Akurasi  dari  titik  koordinat  Google Maps  sudah  baik, 
namun  variabel  jarak  yang  diinformasikan  oleh  Google  Maps 
perlu  diteliti  lebih  lanjut  ke  lapangan.  Dari  hasil  penelitian, 
disimpulkan  bahwa  data  Google  Maps  belum  dapat  digunakan 
sebagai sumber data untuk pengisian variabel jumlah dan jarak 
infrastruktur pada PODES. 

Scraper  menggunakan 

Bahasa 

Python 

Kata  Kunci—  Big  Data,  Google  Maps,  Official  Statistics, 

PODES 

I.  LATAR BELAKANG 

(BPS)  merupakan 

Badan  Pusat  Statistik 

lembaga 
pemerintahan non-kementerian yang memiliki wewenang dan 
tugas  sebagai  penyelenggara  kegiatan  statistik  dasar  dan 
statistik  sektoral  di  Indonesia.  Salah  satu  statistik  yang 
disusun  oleh  BPS  adalah  Statistik  Potensi  Desa  Indonesia 
yang  menyajikan  hasil  pendataan  potensi  desa  (PODES) 
berupa  pendataan  terhadap  ketersediaan  infrastruktur,  potensi 
sosial  dan  ekonomi[1].  Data  PODES  dikumpulkan  melalui 
metode 
seluruh 
lengkap 
kabupaten/kota,  kecamatan,  dan  wilayah 
administrasi 
pemerintahan  terendah  setingkat  desa  (wilayah  administrasi 
setingkat  desa  yang  dimaksud,  yaitu:  desa,  kelurahan,  nagari 
di  Sumatera  Barat,  Unit  Permukiman  Transmigrasi  (UPT), 
dan  Satuan  Permukiman  Transmigrasi  (SPT)  yang  masih 
dibina oleh kementerian terkait)[2]. 

terhadap 

(sensus) 

survei 

Pada 

jarak 

jumlah  dan 

tingkat  desa,  aparatur  desa  bertindak  sebagai 
responden yang akan menjawab pertanyaan kuesioner PODES 
yang  terdiri  dari  total  118  pertanyaan  pada  pelaksanaan  di 
tahun  2018.  Variabel 
infrastruktur 
merupakan  variabel  yang  relatif  banyak  di  tanyakan  pada 
kuesioner  PODES.  Jika  suatu  infrastruktur  tidak  tersedia  di 
desa  tersebut,  selanjutnya  ditanyakan  variabel  jarak  yang 
diisikan dengan jarak antara kantor desa dengan infrastruktur 
sejenis di desa terdekat. Kedua variabel ini diperlukan dalam 
penyusunan  Indeks  Kesulitan  Geografis  (IKG)  yang  menjadi 
salah  satu  variabel  pengalokasian  Dana  Desa.  Selain  IKG, 
variabel  jumlah  dan  jarak  juga  diperlukan  untuk  menyusun 
Indeks  Pembangunan  Desa  serta  Daerah  Dalam  Angka. 
Dikarenakan  mendesaknya  kebutuhan  data  PODES  yang  ter-
update  setiap  tahunnya,  maka  BPS  melakukan  pemutakhiran 
data  PODES  yang  dilaksanakan  setiap  tahun  di  luar  tahun 
pendataan PODES. 
Perkembangan 

pesat 
mengakibatkan  terciptanya  triliunan  data  setiap  hari  melalui 
berbagai  sumber.  Data  yang  berjumlah  sangat  besar  dan 
kompleks  ini  mengacu  pada  sebuah  istilah  yang  disebut  Big 
Data. [3] mendefinisikan Big Data sebagai aset informasi yang 
dicirikan  memiliki  volume,  velocity,  dan  variety  yang  tinggi 
sehingga  untuk  mentransformasinya  menjadi  informasi  yang 
bernilai membutuhkan teknologi dan metode analisis tertentu. 
Selain tiga karakteristik itu,  [4] menyebutkan bahwa Big Data 
memiliki  enam  karakteristik:  Volume  (berukuran  sangat 
besar), Variety (datanya sangat beragam), Velocity (memiliki 
kecepatan  akses  data  yang  memadai),  Veracity  (memiliki 
bentuk  data  yang  dapat  dianalisis),  Variability  (keberagaman 
data),  dan  Visualization 
(kemudahan  memproses  dan 
menafsirkan  data).    Pada  45th  Session  of  the  United  Nation 
Statistical  Commission 
  disebutkan  bahwa  Big  Data 
mempunyai  potensi  untuk  menghasilkan  data  yang  lebih 
relevan dan tepat waktu dibandingkan sumber data tradisional 
dari  official  statistics  seperti  survei  dan  sumber  data 
administratif. 

informasi 

teknologi 

yang 

Tim  tugas  Big  Data  dari  United  Nations  Economic 
Commission for Europe (UNECE) pada bulan Juni tahun 2013 
menetapkan  klasifikasi  dari  tipe-tipe  Big  Data  menjadi  tiga 
jenis,  yaitu  social  networks  (human-sourced  information), 
traditional  business  systems  (process-mediated  data),  dan 
Internet  of  Things  (machine-generated  data)  [5].  Beberapa 
klasifikasi  yang  termasuk  pada  kelompok  Internet  of  Things 

 1 / 15 

 
 
 
 
 
(machine-generated  data)  adalah  mobile  phone  location 
(dengan kode 3121) dan satellite images (dengan kode 3123). 
Google  Maps  merupakan  aplikasi  peta  online  gratis  yang 
disediakan  oleh  Google.  Pada  Google  Maps  Platform 
disebutkan  bahwa  data  Google  Maps  diperbaharui  setiap 
harinya,  baik  melalui  citra  satelit,  street  view¸  ataupun  dari 
pengguna  Google  Maps.  Dimana  pengguna  memberikan 
kontribusi 20 juta informasi setiap harinya atau 200 kontribusi 
setiap  detiknya.  [6]  menyebutkan  bahwa  sumber  data  utama 
Google  Maps  berasal  dari  smartphone  para  penggunanya, 
yaitu data GPS. Hal ini memungkinkan Google Maps menjadi 
bagian  dari  salah  satu  sumber  Big  Data.  Berdasarkan  hal 
tersebut ada kemungkinan untuk memanfaatkan Google Maps 
sebagai  sumber  dari  Big  Data  pada  Official  Statistics,  yakni 
memberikan  kemudahan  dalam  melaksanakan  pemutakhiran 
data PODES yang harus dilaksanakan setiap tahunnya. 

Sejalan dengan hal itu, pada bulan Maret 2020, Mahasiswa 
Tingkat  Tiga  DIV  Politeknik  Statistika  STIS  Tahun  Ajaran 
2019/2020 telah melaksanakan Praktik Kerja Lapangan (PKL) 
dengan  salah  satu  kajiannya  adalah  “Pemanfaatan  Big  Data 
dalam  Pemenuhan  Variabel 
Jarak 
Infrastruktur pada Data Potensi Desa”.  

Infrastruktur  dan 

Dalam  [7]  disebutkan  bahwa  Big  Data  dari  informasi 
Google Maps dapat dijadikan sebagai rujukan dan pelengkap 
isian  yang  berkaitan  dengan  jumlah  dan  jarak  infrastruktur 
pada  pendataan  PODES.  Berdasarkan  hal 
tersebut, 
Subdirektorat  Statistik  Ketahanan  Wilayah  yang  menangani 
Statistik  Potensi  Desa  mencanangkan  pemanfaatan  Google 
jarak 
Maps  untuk  pemenuhan  variabel 
infrastruktur  pada  data  PODES.  Namun  kajian  pada  Praktik 
Kerja  Lapangan  tersebut  baru  mencakup  verifikasi  dan 
analisis  data  Google  Maps  terhadap  22  infrastruktur  dari  47 
isian  pendataan  PODES  yang  berkaitan  dengan  infrastruktur 
pada desa. 

jumlah  dan 

Google  Maps  merupakan  aplikasi  peta  online  gratis  yang 
disediakan  oleh  Google.  Google  Maps  membolehkan 
penggunanya  untuk  berkontribusi  dalam  menambahkan 
informasi  geografis  seperti  melakukan  penandaan  lokasi 
melalui  fitur  “Add  Place”,  melakukan  pengeditan  peta  agar 
tetap akurat melalui fitur “Edit Map”, dan menuliskan review 
ke  lokasi-lokasi  yang  dikunjungi.  Fitur  “Add  Place”  tersebut 
dapat  menyebabkan  adanya  satu  lokasi  yang  mendapat  dua 
atau  lebih  penandaan  yang  berbeda  karena  ditandai  oleh 
banyak  pengguna.  Hal  ini  akan  menyebabkan  adanya  double 
counting  jika digunakan dalam penghitungan  variabel jumlah 
infrastruktur  di  suatu  daerah.  Selain  double  counting,  juga 
terdapat  kemungkinan  infrastruktur  yang  lewat  cacah  karena 
tidak mendapat penandaan lokasinya di Google Maps.   

Dalam  melakukan  penandaan  lokasi,  pengguna  Google 
Maps  akan  mengira-ngira titik lokasi  yang akan dimasukkan, 
hal  ini  menyebabkan  lokasi  pada  Google  Maps  tidak  sesuai 
dengan  keadaan  sebenarnya,  sehingga  akan  mempengaruhi 
keakuratan  jika  digunakan  dalam  pemenuhan  variabel  jarak. 
Berdasarkan hal tersebut, perlu adanya kajian lebih mendalam 
terkait  keakuratan  data  yang  ada  di  Google  Maps  untuk 
mengetahui  seberapa  baik  data  yang  dihasilkan  oleh  Google 
Maps untuk dijadikan sebagai sumber data dalam pemenuhan 
variabel jumlah dan jarak infrastruktur pada data PODES. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

II.  TUJUAN PENELITIAN 

Tujuan dari penelitian ini adalah untuk 
1.  Mengetahui  tingkat  akurasi  data  Google  Maps  untuk 
dijadikan  sebagai  sumber  pemenuhan  variabel  jumlah 
infrastruktur pada data PODES 

2.  Mengetahui  kemampuan  data  pada  Google  Maps 
dalam  mengidentifikasi  perbedaan  antara  infrastruktur 
sejenis yang berbeda fasilitas/bangunan fisik 

3.  Mengetahui tingkat akurasi titik koordinat data Google 

Maps  

III. PENELITIAN TERKAIT 

Penelitian  ini  mengacu  pada  tiga  penelitian  terkait  yang 
telah  dilakukan  sebelumnya.  Pertama,  penelitian  yang 
dilakukan  mahasiswa  PKL  Politeknik  Statistika  STIS  tahun 
ajaran 2019/2020 dengan judul “Pemanfaatan Big Data dalam 
Pemenuhan  Variabel  Jarak  dan  Jumlah  Infrastruktur  pada 
Data  Potensi  Data”.  Penelitian  [7]  melakukan  perbandingan 
hasil  pengumpulan  data  Google  Maps  terhadap  data  PODES 
untuk  variabel  jumlah  dan  jarak  dari  empat  kelompok  jenis 
infrastruktur  yang  terdiri  dari  22  infrastruktur  di  Provinsi 
Daerah  Istimewa  Yogyakarta.  Pengumpulan  data  dalam 
penelitian  ini  menggunakan  metode  web  scraping  dan  akses 
API.  Hasil  deskriptif  menunjukkan  bahwa  dibandingkan  data 
hasil  PODES,  data  Google  Maps  menunjukkan  jumlah 
infrastruktur  yang  cenderung  lebih  kecil  untuk  kelompok 
infrastruktur  pendidikan  dan  kesehatan,  sedangkan  untuk 
kelompok  infrastruktur  lainnya  menunjukkan  jumlah  yang 
cenderung lebih besar. Penelitian ini juga menunjukkan hanya 
ada  19,51  persen  data  jarak  hasil  web  scraping  yang  dapat 
dikatakan  sama  atau  hanya  memiliki  jangkauan  perbedaan 
dari 0 sampai 100 meter dengan data PODES. Hasil penelitian 
ini  menyebutkan bahwa penggunaan  Big Data dari informasi 
Google  Maps  telah  cukup  baik  untuk  melakukan  pemenuhan 
isian jumlah infrastruktur dan jarak infrastruktur di kelompok 
infrastruktur  pendidikan  dan  kesehatan,  namun  belum  cukup 
baik  untuk  melakukan  pemenuhan  isian  pada  kelompok 
infrastruktur lain. 

Kedua,  penelitian  [8]  dengan  judul  “Modernizing  Official 
Statistics  With  Big  Data  :  A  Case  On  PODES”  melakukan 
perbandingan  data  PODES  dengan  Big  Data  serta  data  dari 
tiga 
Kementerian  Kesehatan  dan  Pendidikan 
infrastruktur,  yaitu  SMA,  puskesmas,  dan  rumah  sakit  pada 
Provinsi  DKI  Jakarta.  Berdasarkan  analisis  deskriptif  berupa 
distribusi margin dari selisih data PODES dengan data Google 
Maps,  didapatkan  bahwa  margin  didominasi  dengan  nilai  0 
yang artinya data Google Maps dan PODES bersesuaian, yaitu 
      desa untuk Puskesmas,      desa untuk Rumah Sakit, 
dan 100 desa untuk SMA. 

terhadap 

Penelitian  ketiga  adalah  penelitian  C.F.Annisa  dan 
S.Pramana  dengan  judul  “Kajian  Pemanfaatan  Data  Google 
Maps  dalam  Official  Statistics”.  Pada  penelitian  [9],  peneliti 
memanfaatkan  data  Google  Maps  untuk  mendapatkan  data 
usaha  penyedia  makan  minum  di  Pulau  Jawa  dan  Bali  untuk 
melihat  apakah  data  Google  Maps  dapat  digunakan  sebagai 
pelengkap  dalam  pemenuhan  statistik  usaha  penyediaan 
makan  minum  yang  ditangani  oleh  Subdirektorat  Pariwisata 
BPS.  Untuk  melihat  kecocokan  data  Google  Maps  dengan 

 2 / 15 

 
 
data  frame  BPS,  peneliti  menggunakan  kombinasi  algoritma 
Jaro  Winkler  dan  Haversine.  Hasil  pencocokan  yang 
dilakukan  penelitian  ini  menunjukkan  bahwa  68,22  persen 
data  Google  Maps  cocok  dengan  data  frame  yang  dimiliki 
BPS.  

Dalam penelitian ini peneliti akan melakukan perbandingan 
data  Google  Maps  untuk  47  jenis  infrastruktur  dengan  data 
hasil  pendataan  PODES  dan  data  Geotagging  dari 
Subdirektorat Pengembangan Pemetaan Statistik BPS dengan 
wilayah cakupan Kota Yogyakarta. 

IV. METODE PENELITIAN  

4.1 Cakupan Penelitian 

Penelitian  ini  ditujukan  untuk  melakukan  pengumpulan 
data  yang  berkaitan  dengan  variabel  jumlah  dan  jarak 
infrastruktur  di  dalam  kuesioner  PODES  dari  Google  Maps 
dan  melakukan  analisis  kendala  yang  terjadi  di  setiap  tahap 
pengumpulan  data  serta  melakukan  analisis  keakuratan  data 
yang  dihasilkan  dari  proses  pengambilan  data  dari  Google 
Maps  tersebut.  Unit  analisis  dalam  penelitian  ini  informasi 
detail  infrastruktur  dan  jarak  infrastruktur  dari  45  desa  pada 
Kota Yogyakarta. Infrastruktur yang dicakup dalam penelitian 
ini berasal dari 47 rincian pertanyaan pada kuesioner PODES 
yang  dikembangkan  menjadi  71  kata  kunci  untuk  pencarian 
pada Google Maps. 

4.2 Metode Pengumpulan Data 

Pengumpulan  data  dilakukan  melalui  beberapa  sumber 

yaitu web scraping dan pengambilan data sekunder. 

4.2.1 Web Scraping  
Web-scraping  digunakan  untuk  mengumpulkan  data 
sekunder  dari  Google  Maps  berupa  informasi  objek  dari  71 
infrastruktur  yang  bersesuaian  dengan 
kata  kunci 
kuesioner 
dengan 
menggunakan bahasa Python 3.7 pada IDE Jupyter Notebook 
dan  menggunakan  Selenium  sebagai  library  utama.  Web-
scraping terdiri dari tiga tahap: 

jenis 
PODES.  Web-scraper 

dijalankan 

1)  Pengumpulan URL Kantor Desa 

Alur  kerja  dari  tahap  ini  diawali  dengan  melakukan 
otomatisasi  web-scraper  untuk  mengakses  URL 
Google  Maps  yang  sudah  berisi  kata  kunci  desa  yang 
akan dicari, URL yang diakses memiliki format: 
“https://www.google.com/maps/search/kanto+desa+[n
amadesa]”  *[namadesa]  diisikan  menggunakan 
format nama_desa + kecamatan + kabupaten kota. 
Dari  hasil  penelusuran  URL  tersebut  dikumpulkan 
URL yang mengarahkan ke laman detail kantor desa di 
Google Maps. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 1. Flowchart Pengumpulan URL Kantor Desa 

2)  Pengumpulan URL Infrastruktur 

Pada  tahap  ini  web-scraper  membuka  URL  laman 
detail  kantor  desa  yang  dikumpulkan  dari  tahap 
sebelumnya,  kemudian  menekan  fitur  “nearby”  dan 
memasukkan  kata  kunci  jenis  infrastruktur  pada  field 
pencarian  Google  Maps  dan  menjalankan  fungsi 
tombol 
tersebut 
dikumpulkan  URL  yang  mengarah  ke  laman  hasil 
pencarian berdasarkan tiap kata kunci. 

enter.  Dari 

pencarian 

hasil 

Gambar 2. Flowchart Pengumpulan URL Infrastruktur 

3)  Pengumpulan Detail Informasi Penandaan 

Pada  tahap  ini  web-scraper  membuka  URL  yang 
dihasilkan  pada  tahap  dua,  kemudian  mengumpulkan 
detail  informasi  dari  40  objek  yang  terdapat  dalam 
hasil  pencarian 
informasi  yang 
tersebut.  Detail 
dikumpulkan  adalah  nama  penandaan,  rating,  jumlah 
ulasan,  kategori  infrastruktur,  alamat,  koordinat,  jarak 
& durasi ke kantor desa, serta URL laman pencarian. 

Gambar 3. Flowchart Pengumpulan Detail Informasi Penandaad 

4.2.2 Pengumpulan Data Sekunder  

 3 / 15 

 
 
 
 
 
 
Penelitian ini menggunakan 3 data sekunder yang diperoleh 
dari BPS, yaitu Peta Digital Wilkerstat Kota Yogyakarta, Data 
PODES 2018 dan 2020 untuk  wilayah Kota Yogyakarta, dan 
Data Geotagging Kota Yogyakarta. 

Dalam  pengumpulan  URL  45  kantor  desa  di  Kota 
Yogyakarta,  dilakukan  empat  kali  pengumpulan  data  yang 
menghasilkan data yang berbeda-beda dengan rincian sebagai 
berikut. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

4.3 Metode Analisis Data 

Metode analisis yang digunakan dalam penelitian ini adalah 
analisis  deskriptif.  Analisis  deskriptif  digunakan  untuk 
menggambarkan langkah serta kendala dalam pengolahan data 
Google  Maps,  menggambarkan  tingkat  akurasi  data  Google 
Maps berupa persentase kecocokan data Google Maps dengan 
PODES,  mendeskripsikan  kemampuan  data  Google  Maps 
dalam mengidentifikasi perbedaan antara infrastruktur sejenis 
yang  berbeda 
fisik,  mendeskripsikan 
infrastruktur yang dapat dipenuhi atau tidak oleh data Google 
Maps,  serta  menggambarkan  kesesuaian  titik  koordinat  data 
Google  Maps  berupa  persentase  kecocokan  dengan  data 
Geotagging. 

fasilitas/bangunan 

V.  KERANGKA PIKIR 

Kerangka  pikir  dalam  penelitian  ini  digambarkan  dari 

Gambar 4 berikut ini. 

Gambar 4. Bagan Kerangka Pikir 

Gambar  4  di  atas  menjelaskan  keseluruhan  proses 
penelitian yang peneliti lakukan. Sesuai tujuan penelitian yang 
ingin  dicapai  maka  dilakukan  pengumpulan  informasi  lima 
kelompok 
infrastruktur  yang  ditanyakan  pada  kuesioner 
PODES  pada  Google  Maps  yang  kemudian  dikembangkan 
menjadi  71  kata  kunci  yang  mewakili 
lima  kelompok 
infrastruktur  tersebut  dengan  membangun  Web  Scraper  yang 
dijalankan dengan menggunakan bahasa Python 3.7 pada IDE 
Jupyter  Notebook.  Selanjutnya  dari  data  yang  dikumpulkan 
tersebut 
dan 
pendokumentasian  terkait  karakteristik  dan  kendala  yang 
ditemukan.  Dari  analisis  dan  dokumentasi  tersebut  ditarik 
kesimpulan  terkait  peluang  pemanfaatan  data  Google  Maps 
sebagai sumber data pada pendataan PODES. 

pengolahan, 

dilakukan 

analisis, 

VI. HASIL DAN PEMBAHASAN 

6.1 Analisis dan Evaluasi Proses Pengumpulan Data 

6.1.1 Pengumpulan URL Kantor Desa  

TABEL I 
PERCOBAAN PENGUMPULAN URL DESA 

Pengumpulan 
ke- 

(1) 

1 

2 

3 

4 

Rata-rata 

Total 
Durasi 
(menit) 

(2) 

7,86 

7,82 

11,52 

14,21 

10,35 

Rata-rata durasi 
pengumpulan satu 
kantor desa 
(detik) 
(3) 

10,49 

10,43 

15,36 

18,94 

13,8 

Catatan 

(4) 
2 null 
3 URL salah 
2 null 
3 URL salah 
2 null 
5 URL salah 
2 null 
6 URL salah 

Pengumpulan  dilakukan  berulang  kali  dengan 

tujuan 
menemukan  penyebab  adanya  pencarian  bernilai  null  dan 
salah.  Pengumpulan  dilakukan  pada  dua  perangkat  dan 
kecepatan  koneksi  internet  yang  berbeda  kemudian  pada 
pengumpulan  kedua  dan  keempat  diberikan  kondisi  dimana 
komputer  memiliki  beban  kerja  yang  banyak  (multitask). 
Pengumpulan  pertama  dan  kedua  menggunakan  koneksi 
internet  dengan  kecepatan  110  Mbps  pada  komputer  Dell 
OptiPlex 7480 AIO dengan prosesor Intel Core i7-10700 dan 
RAM  16  GB,  sedangkan  pengumpulan  ketiga  dan  keempat 
menggunakan  koneksi  internet  dengan  kecepatan  1,8  Mbps 
pada  laptop ASUS X442URR dengan prosesor Intel Core i5-
8250U dan RAM 4 GB.  
dari 

menunjukkan 
ketidaklengkapan dengan jumlah dan desa yang sama, namun 
terdapat  perbedaan  durasi  pengumpulan  antara  pengumpulan 
pertama-kedua  dengan  ketiga-keempat.  Berdasarkan  tabel  1, 
terlihat  durasi  pengumpulan  pertama  dan  kedua  lebih  cepat 
dibandingkan  pengumpulan  lainnya,  ini  menunjukkan  bahwa 
kecepatan 
durasi 
pengumpulan data. Pada data pengumpulan  keempat terdapat 
lebih  banyak  kesalahan  dibanding  pengumpulan  lainnya,  ini 
juga 
menunjukkan 
mempengaruhi proses pengumpulan data. 

spesifikasi  perangkat  pengumpulan 

internet  mempengaruhi 

pengumpulan 

koneksi 

empat 

Dua 

Penyebab  adanya  nilai  “null”  pada  pengumpulan  URL 
kantor  desa  dikarenakan  nama  pada  penandaan  kantor  desa 
yang  sesuai  dianggap  salah  oleh  algoritma  web-scraper, 
sedangkan  untuk  URL  yang  salah  dan  mengarahkan  pada 
objek lain, disebabkan oleh beberapa alasan berikut ini: 

1)  Penandaan  yang  muncul  pada  urutan  pertama  pada 
hasil  pencarian  di  Google  Maps  adalah  objek  lain, 
namun  memenuhi  algoritma  dan  disimpulkan  oleh 
web-scraper sebagai kantor desa yang dicari  

2)  Koneksi  internet  yang  lambat  dan/atau  beban  kerja 

perangkat berat 

Berdasarkan  hasil  tersebut,  penggunaan  perangkat  dengan 
koneksi  internet  yang  lebih  stabil  sangat  disarankan,  karena 
dapat  menghemat  waktu  pengumpulan  sebanyak  lima  hingga 
delapan detik per desanya dan untuk total 45 desa menghemat 

 4 / 15 

 
 
 
 
 
 
waktu  empat  hingga  tujuh  menit.  Setelah  pengumpulan  data 
tahap  1  diperlukan  tahap  cleaning  data  berupa  pengecekan 
isian  URL  yang  kosong  atau  salah,  kemudian  diimputasi 
dengan  URL  yang  benar  dari  hasil  pencarian  secara  manual. 
Proses ini memakan waktu yang lama dan butuh ketelitian jika 
dilakukan untuk jumlah desa yang lebih banyak. 

6.1.2 Pengumpulan URL Infrastruktur  
Pengumpulan  URL  Infrastruktur  memiliki  durasi  10,15 
menit  untuk  45  desa  dengan  rata-rata  13,53  detik  untuk  satu 
desa.  Dari  pengumpulan  terdapat  total  3690  URL  yang 
menunjukkan URL yang benar, sehingga pada tahap ini tidak 
perlu dilakukan cleaning apapun. 

6.1.3 Pengumpulan Detail Informasi Penandaan  
Pengumpulan  detail  informasi  penandaan  menghasilkan 
data  yang  akan  diolah  dan  diharapkan  dapat  menjadi  sumber 
data  untuk  isian  variabel  jumlah  dan  jarak  infrastruktur  pada 
kuesioner PODES. Data ini berisi detail informasi penandaan 
di  sekitar  kantor  desa  yang  didapat  dari  Google  Maps 
terhadap  71  kata  kunci 
berdasarkan  hasil  pencarian 
ini 
menggunakan 
didapatkan  total  117.799  penandaan  untuk  45  desa  di  Kota 
Yogyakarta.  Pengumpulan  data  pada  tahap  ini  memakan 
waktu yang sangat banyak, dimana untuk satu desa memakan 
waktu  kurang  lebih  7  jam  sehingga  untuk  pengumpulan  45 
desa  membutuhkan  waktu  kurang  lebih  315  jam  atau  13 
hingga 14 hari. 

fitur  “nearby”.  Pada  pengumpulan 

6.2 Analisis Eksplorasi dan Pre-processing Data 

Dari 

total  117.799  penandaan  yang  didapatkan  dari 
pengumpulan  data  menggunakan  web-scraping,  selanjutnya 
dilakukan  eksplorasi  data  untuk  menentukan  tahap  pre-
processing  yang  akan  dilakukan  agar  data  siap  untuk 
digunakan.  

6.2.1 Penandaan yang berada di luar desa bersangkutan  
Dari  eksplorasi  data  yang  dilakukan  ditemukan  adanya 
penandaan yang berada di luar desa yang bersangkutan, hal ini 
dikarenakan  hasil  pencarian  pada  Google  Maps 
tidak 
terdekat  dari  kantor  desa. 
jarak 
diurutkan  berdasarkan 
Mengatasi hal ini, dilakukan penghapusan data yang berada di 
luar desa bersangkutan yang menjadi tahap pertama dari  pre-
processing data. Proses ini dilakukan dengan cara melakukan 
pengecekan  keberadaan  koordinat  data,  apakah  terletak  di 
dalam  wilayah  administratif  desa  atau 
tidak  dengan 
menggunakan  bantuan  library  “Shapely”  pada  Python.  Dari 
pre-processing tahap 1 didapatkan sebanyak 96,19 persen data 
yang dikumpulkan tidak berada pada desa yang bersangkutan 
dan hanya ada 3,81 persen (4.485 penandaan) yang benar. 

6.2.2 Pengelompokan Data Frame 
Pengelompokan  data  dilakukan  untuk  mempermudah 
tahapan  pre-processing  selanjutnya.  Pengelompokan 
ini 
didasarkan  pada  pertanyaan  yang  terdapat  pada  kuesioner 
dengan  total  47  pertanyaan.  Tetapi  pengelompokan  hanya 
dibagi  menjadi  43  kelompok,  dikarenakan  kata  kunci  yang 
berhubungan  dengan  puskesmas  dan  pasar  dijadikan  satu 
tersebut,  16  diantaranya 
kelompok.  Dari  43  kelompok 
merupakan  kelompok  yang  berisi  gabungan  kata  kunci  yang 
dapat  dilihat  pada  Error!  Reference  source  not  found., 
sedangkan 27 kelompok lainnya berisi satu kata kunci. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

EMPAT BELAS KELOMPOK PERTANYAAN PADA KUESIONER PODES 

TABEL II 

Rincian Pertanyaan 
(1) 

TK/RB/BA 

SD/MI 
SMP/MTs 
SMA/MA 
Akademi / Perguruan Tinggi 

Puskesmas 

Poliklinik/Balai Pengobatan 
Toko  khusus  obat/toko  khusus 
jamu 
Surau/langgar/musala 

Pasar 

Minimarket/swalayan 
Toko/warung kelontong 

Keywords 
(2) 

TK; bustanul athfal; 
raudatul athfal 
SD; madrasah ibtidaiyah 
SMP; madrasah tsanawiyah 
SMA; madrasah aliyah 
Akademi; Perguruan Tinggi 
puskesmas;  puskesmas  rawat 
inap;  puskesmas  tanpa  rawat 
inap; puskesmas pembantu 
Poliklinik; balai pengobatan 
Toko obat; Toko jamu 

pasar 

Surau; langgar; musala 
bangunan 
Pasar; 
permanen;  pasar  bangunan 
semi  permanen;  pasar  tanpa 
bangunan 
Minimarket; swalayan 
Toko 
kelontong 
Restoran; rumah makan 

kelontong;  warung 

Restoran/rumah makan 
Warung/kedai makanan minuman  Warung makanan minuman; 

Penginapan/hostel/motel/losmen/w
isma 
Bank Umum 

Kedai makanan minuman; 
Penginapan; hotel; motel; 
Losmen; wisma; 
Bank  BRI;  bank  BNI;  bank 
mandiri;  bank  daerah;  bank 
BTN 

6.2.3 Duplikasi Data  
Setelah  dilakukan  pre-processing  data  tahap  1  dan  2,  data 
kembali  dieksplorasi  dan  adanya  duplikasi  data,  dimana 
terdapat tiga kasus duplikasi yang berbeda, yaitu: 

1)  Duplikasi  penandaan  yang  sama  karena  terambil  lebih 
dari  sekali,  selanjutnya  disebut  sebagai  duplikasi  kasus 
A 

2)  Duplikasi  penandaan  yang  sama  karena  muncul  pada 
kata  kunci  yang  berbeda,  namun  masih  dalam  satu 
kelompok  pertanyaan  yang  sama  pada  kuesioner 
PODES, selanjutnya disebut sebagai duplikasi kasus B 

3)  Duplikasi  objek  yang  sama  namun  memiliki  banyak 
penandaan  yang  berbeda  yang  selanjutnya  disebut 
duplikasi kasus C 

Mengatasi  hal  ini,  dilakukan  penghapusan  duplikasi  data 

yang menjadi tahap ketiga dari pre-processing data.   

Duplikasi Kasus A  
Duplikasi  kasus  A  merupakan  duplikasi  yang  disebabkan 
karena adanya sebuah penandaan yang dikumpulkan lebih dari 
sekali oleh web scraper. Pada data ditemukan sebanyak 14,18 
persen  merupakan  duplikasi  kasus  A.  Penghapusan  duplikasi 
data pada kasus A dilakukan dengan melihat adanya data yang 
memiliki  kesamaan  jenis  infrastruktur,  nama,  dan  alamat 
penandaan.  Berikut  persentase  duplikasi  kasus  A  pada  5 
kelompok infrastruktur secara umum. 

 5 / 15 

 
 
 
 
29.14 

14.52 

8.81 

14.09 

10.07 

Pendidikan

Kesehatan

Tempat
Ibadah

Ekonomi

Keuangan

Gambar 5. Persentase Duplikasi Kasus A Menurut Kelompok Infrastruktur 

Duplikasi Kasus B  
Duplikasi  kasus B disebabkan karena adanya  pengambilan 
penandaan  yang  sama  pada  kata  kunci  yang  berbeda. 
Duplikasi  ini  terdapat  pada  16  kelompok  pertanyaan  yang 
memiliki  lebih  dari  satu  kata  kunci  seperti  pada  tabel  II. 
Contohnya  penandaan  perguruan  tinggi  bernama  “Kampus  2 
Akademi  Keperawatan  Al  Islam”  muncul  pada  pencarian 
menggunakan kata kunci “akademi” dan “perguruan tinggi”. 

total, 

Penghapusan  duplikasi  dari  kasus  B  dilihat  berdasarkan 
kesamaan  nama  dan  alamat  penandaan.  Karena  pada  isian 
kuesioner  PODES  hanya  dibutuhkan  data  agregat  berupa 
jumlah 
sehingga  untuk  menghapus  data  yang 
terduplikasi  tidak  perlu  melihat  apakah  penandaan  tersebut 
lebih  cocok  dengan  kata  kunci  yang  mana,  dikarenakan  kata 
kunci dalam satu kelompok pertanyaan  memiliki satu konsep 
definisi  yang  sama.  Contohnya  pada  penandaan  “Kampus  2 
Akademi Keperawatan Al Islam” diatas, hanya perlu dihapus 
salah  satu  baris  dan  tidak  perlu  mengidentifikasi  apakah 
penandaan  tersebut  termasuk  ke  TK,  bustanul  athfal,  atau 
raudatul athfal, karena ketiga kata kunci ini berada dalam satu 
kelompok pertanyaan. 

Pada  data  yang  dikumpulkan  ditemukan  sebanyak  13,67 
persen  duplikasi  kasus  B  yang  sebarannya  dapat  dilihat  pada 
gambar 6. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Duplikasi  kasus  C  merupakan  duplikasi  yang  disebabkan 
karena adanya objek yang memiliki lebih dari satu penandaan 
di  Google  Maps  dan  memiliki  penamaan  yang  berbeda-beda 
karena  ditandai  oleh  pengguna  yang  berbeda.  Contohnya 
terdapat  infrastruktur  taman  kanak-kanak  yang  beroperasi 
dengan  nama  “TK  ABA  Al-Ishlah”  memiliki  dua  penandaan 
berbeda  pada  Google  Maps, yaitu  ditandai  dengan  nama  “Tk 
Aba Al Ishlah” dan “TK Al-Ishlah”. Contoh lainnya, terdapat 
infrastruktur  penginapan  yang  beroperasi  dengan  nama 
“Wisma  Nendra  Hotel  Syari’ah”,  memiliki  dua  penandaan 
yang namanya cukup jauh berbeda, yaitu “Hotel Syariah” dan 
“Wisma Nendra Syariah”. 

Untuk  menghapus  duplikasi  data  pada  kasus  C  dilakukan 
pengecekan  data  satu  per  satu  dengan  cara  mengunjungi 
laman  Google  Maps  dan  mengidentifikasi  jika  terdapat 
penandaan  pada  objek  yang  sama.  Seperti  pada  contoh, 
terdapat kemungkinan objek terduplikasi yang memiliki nama 
yang  cukup  jauh  berbeda,  hal  ini  mengharuskan  pengecekan 
data  satu  per  satu  ke  laman  pencarian  Google  Maps  dan 
terdapat  duplikasi 
diidentifikasi  secara  subjektif  apakah 
melalui foto atau letak penandaan.  

Pada  data  yang  dikumpulkan  terdapat  113  objek  memiliki 
lebih  dari  satu  penandaan:  107  objek  dengan  2  penandaan,  5 
objek dengan 3 penandaan, dan 1 objek dengan 4 penandaan. 
Sehingga  terdapat  total  235  penandaan  terduplikasi  dan  121 
penandaan  dihapuskan.  Persentase  duplikasi  kasus  C  dapat 
dilihat pada gambar 7. 

9.41 

6.45 

4.07 

1.87 

1.29 

22.43 

Pendidikan

Kesehatan

Tempat
Ibadah

Ekonomi

Keuangan

Gambar 7. Persentase Duplikasi Kasus C Menurut Kelompok Infrastruktur 

Infrastruktur Pendidikan 
Pada  infrastruktur  pendidikan  terdapat  total  45  objek  yang 
terduplikasi (kasus C) dan dihapuskan sebanyak 48 penandaan 
dari  93  penandaan.  Gambar  8  menampilkan  banyaknya  baris 
yang dihapus per kelompok pertanyaan. 

11 

8 

2 

6 

6 

6 

3 

3 

1 

1 

1 

0 

3.77 

3.16 

Pendidikan

Kesehatan

0 

Tempat
Ibadah

5.09 

Ekonomi

Keuangan

Gambar 6. Persentase Duplikasi Kasus B Menurut Kelompok Infrastruktur 
tertinggi  dimiliki  oleh 
Pada  gambar  6,  persentase 
infrastruktur  ekonomi,  hal  ini  dikarenakan  adanya  duplikasi 
sebesar  51,42  persen  pada  kelompok  penginapan.  Persentase 
duplikasi  yang 
ini  disebabkan  karena  kelompok 
penginapan  merupakan  gabungan  dari  5  kata  kunci,  yakni 
“penginapan”,  “hostel”,  “motel”,  “losmen”,  dan  “wisma”, 
sehingga kemungkinan satu penandaan terambil berulang kali 
pada kata kunci yang berbeda semakin besar. 

tinggi 

Duplikasi Kasus C 

 6 / 15 

 
 
 
 
 
 
 
Gambar  8.  Jumlah  Penghapusan  Duplikasi  Kasus  C  pada  Infrastruktur 
Pendidikan 

13 

jumlah 

Kelompok  TK/RA/BA  memiliki 

duplikasi 
terbanyak,  dimana  5  dari  8  duplikasi  dikarenakan  adanya 
penamaan  yang  berbeda  untuk  “Aisyiyah  Bustanul  Athfal”, 
yang disingkat dengan “ABA” oleh masyarakat setempat, hal 
ini  menunjukkan  diperlukan  petugas  yang  mengenal  dengan 
baik  daerah  yang  ditanganinya  agar  tidak  terjadi  kesalahan 
saat proses pre-processing data.  

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

3 

0 

0 

0 

0 

0 

0 

dan 

Suryowijayan” 

Pola  lain  yang  ditemukan  adalah  terdapat  penandaan  yang 
berbeda  untuk  lokasi  satu  sekolah  yang  berbeda,  contohnya 
“SD 
“SD  Muhammadiyah 
Muhammadiyah  Suryowijayan  Unit  II”,  yakni  3  dari  8 
duplikasi  pada  kelompok  SD/MI  dan  1  dari  6 duplikasi  pada 
kelompok  SMA/MA.  Juga  terdapat  2  duplikasi  dikarenakan 
penandaan  yang  berbeda  untuk  fasilitas  di  dalam  sekolah, 
yaitu pada “Kantin Sehat SD Muhammadiyah Sokonadi” dan 
“Masjid  SMA  N  4  Yogyakarta”.  Sedangkan  untuk  kelompok 
Akademi/Perguruan  Tinggi,  ditemukan  3  dari  6  duplikasi 
disebabkan  karena  adanya  penandaan  dengan  nama  jurusan 
pada suatu akademi/perguruan tinggi. 

Infrastruktur Kesehatan 
Pada  infrastruktur  kesehatan  terdapat  total  18  objek  yang 
terduplikasi  (kasus  C)  dan  dihapuskan  sebanyak  20  dari  38 
penandaan yang terduplikasi. Gambar 9 menampilkan jumlah 
penandaan yang dihapuskan per kelompok pertanyaan. 

7 

5 

4 

1 

1 

0 

0 

0 

0 

0 

0 

0 

0 

2 

Gambar  9.  Jumlah  Penghapusan  Duplikasi  Kasus  C  pada  Infrastruktur 
Kesehatan 

Pada  gambar  9  terlihat,  Kelompok  Rumah  Sakit  memiliki 
jumlah duplikasi terbanyak. 6 dari 7 duplikasi pada Kelompok 
Rumah  Sakit  dikarenakan  adanya  penandaan  pada  fasilitas 
yang  ada  pada  rumah  sakit,  contohnya  “Siloam  Jogja  Sport 
Center”  sedangkan  terdapat  penandaan  “Siloam  Hospitals 
Yogyakarta”. Sedangkan untuk kelompok lain tidak memiliki 
pola tertentu. 

Infrastruktur Tempat Ibadah 
Pada  infrastruktur  tempat  ibadah  terdapat  total  15  objek 
yang terduplikasi (kasus C) dan dihapuskan sebanyak 16 dari 
31  penandaan  yang  terduplikasi.  Gambar  10  menampilkan 
jumlah penandaan yang dihapuskan per kelompok pertanyaan. 

Gambar 10. Jumlah Penghapusan Duplikasi Kasus C pada Tempat Ibadah 

dalam  mengidentifikasi 

Untuk  tempat  ibadah  tidak  ada  pola  tertentu,  namun  perlu 
penamaan  Masjid 
ketelitian 
menggunakan bahasa arab yang di Indonesia-kan. Contohnya 
penandaan  “Masjid  Baiturachim  Patangpuluhan  Jogja”  dan 
“Masjid Baiturrokhim” yang merujuk pada masjid yang sama. 

Infrastruktur Ekonomi 
Pada  infrastruktur  ekonomi  terdapat  total  30  objek  yang 
terduplikasi  (kasus  C)  dan  dihapuskan  sebanyak  31  dari  61 
penandaan  yang 
terduplikasi.  Gambar  11  menampilkan 
jumlah penandaan yang dihapuskan per kelompok pertanyaan. 

15 

7 

3 

0 

0 

0 

0 

2 

2 

1 

0 

Gambar  11.  Jumlah  Penghapusan  Duplikasi  Kasus  C  pada  Infrastruktur 
Ekonomi 

jumlah 

Kelompok  Penginapan  memiliki 

duplikasi 
terbanyak.  Pola  duplikasi  pada  kelompok  penginapan  adalah 
adanya  penandaan  penginapan  menggunakan  kata-kata 
“RedDoorz”,  “OYO”,  atau  “Airy”  yang  juga  menyulitkan 
pengidentifikasian  duplikasi  kasus  C.  Contohnya  ada 
penandaan  “RedDoorz  Hostel  @  Kotagede”  yang  ternyata 
merujuk  pada  objek  yang  sama  dengan  penandaan  “BHUMI 
Hostel”  atau  “Airy  Syariah  Stasiun  Yogyakarta  Tentara 
Rakyat  Mataram  10”  yang  merujuk  pada  objek  yang  sama 
dengan penandaan “Hinggil Homestay”. 
Infrastruktur Lembaga Keuangan 
Pada infrastruktur lembaga keuangan terdapat total 6 objek 
yang  terduplikasi  (kasus  C)  dan  dihapuskan  sebanyak  6  dari 
12  penandaan  yang  terduplikasi.  Gambar  12  menampilkan 
jumlah penandaan yang dihapuskan per kelompok pertanyaan. 

 7 / 15 

 
 
 
 
 
3 

3 

Bank Pemerintah

Bank Swasta

0 

BPR

Gambar  12.  Jumlah  Penghapusan  Duplikasi  Kasus  C  pada  Lembaga 
Keuangan 

Duplikasi  pada  Bank,  baik  itu  bank  pemerintah  maupun 
bank swasta, 4 dari 6 duplikasi ditemukan memiliki penamaan 
yang  general,  contohnya  “PT  Bank  Negara  Indonesia 
(Persero)” atau “BCA Syariah”. 

Penghapusan  data  pada  duplikasi  kasus  C  merupakan 
langkah  yang  membutuhkan  waktu  yang  lama  dan  ketelitian 
yang sangat tinggi, karena harus melakukan pemeriksaan data 
di  Google  Maps  yang  memakan  waktu  lama.  Langkah 
pemeriksaan  di  Google  Maps  tidak  bisa  dihilangkan,  karena 
terdapat  22,81  persen  data  yang 
terduplikasi  memiliki 
penamaan  yang  jauh  berbeda  dan  tidak  dapat  diidentifikasi 
hanya  dari  nama  penandaan,  namun  harus  diidentifikasi 
berdasarkan  foto  atau  letak  titik  koordinat  yang  terdapat  di 
Google Maps, dan hasilnya akan sangat subjektif berdasarkan 
kesimpulan petugas yang menangani. 

Namun, ditemukan beberapa pola yang dapat memudahkan 

proses pre-processing data, yakni  

1)  Ditemukan  sebanyak  10  persen  duplikasi  memiliki 
penamaan yang umum. Ini artinya jika ditemukan data 
ini 
yang  memiliki  penamaan  yang  umum,  data 
memiliki 
dengan 
besar 
penandaan  lainnya,  sehingga  harus  diperiksa  dengan 
baik penandaan lainnya. Berikut beberapa contoh yang 
ditemukan  pada  data;  “Warung  Kelontong”,  “Gereja 
Kristen Jawa”, “Toko Jamu”, dan sebagainya.  

terduplikassi 

peluang 

2)  Ditemukan  sebanyak  86,73  persen  duplikasi  yang 
dihapuskan  memiliki  ulasan  yang 
lebih  sedikit 
dibandingkan  dengan  penandaan  yang  tidak  dihapus. 
Sebanyak  77,19  persen  penandaan  yang  terduplikasi 
memiliki  penamaan  yang  mirip,  sehingga  untuk 
menghapus 
perlu  melakukan 
pengecekan  ke  Google  Maps,  cukup  menghapus  yang 
memiliki ulasan lebih sedikit. 

duplikasi 

tidak 

6.2.4 Ketidaksesuaian Penandaan dengan Kata Kunci  
Dari eksplorasi terhadap sisa data dari pre-processing data 
tahap tiga ditemukan sebanyak 38,4 persen (1.230 penandaan) 
penandaan  tidak  sesuai  dengan  kata  kunci  yang  diminta.  
Contohnya  penandaan  “SD Pangudi  Luhur I” pada pencarian 
menggunakan kata kunci Paud.  

Data  yang  tidak  sesuai  ini  harus  dihapuskan  agar  tidak 
terjadi  kesalahan  dalam  penghitungan  jumlah  infrastruktur  di 
suatu  desa,  penghapusan  penandaan  ini  dilakukan  bersamaan 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

dengan  identifikasi  duplikasi  kasus  C  karena  juga  harus 
dilakukan pemeriksaan secara manual. 

Infrastruktur Pendidikan 
Pada  infrastruktur  pendidikan  terdapat  total  17  penandaan 
yang  tidak  sesuai  yang  dapat  dilihat  pada  gambar  13  berikut 
ini. 

5 

2 

2 

2 

1 

1 

1 

1 

1 

1 

0 

0 

0 

Gambar 13. Jumlah Penandaan Pendidikan yang Tidak Sesuai 

Dari  17  total  penandaan  yang  tidak  sesuai,  sebanyak  64,7 
persen dikarenakan adanya salah pengidentifikasian tingkatan 
sekolah,  misalnya  SD  yang 
terkumpul  pada  kelompok 
TK/RA/BA, SMP yang terkumpul pada kelompok SMA/MA, 
dan sebagainya.  

Infrastruktur Kesehatan 
Pada  infrastruktur  kesehatan  terdapat  total  145  penandaan 
yang tidak sesuai dan dapat dilihat pada gambar 14 berikut ini. 

28 

25 

24 

17  15 

7 

4 

3  3 

14 

0  2  1  2 

Gambar 14. Jumlah Penandaan Kesehatan yang Tidak Sesuai 

82,76  persen  dari  data  yang  dihapuskan  dikarenakan  salah 
mengidentifikasi 
contohnya  untuk 
fasilitas  kesehatan, 
Kelompok  Rumah  Sakit  Bersalin  dari  28  penandaan  yang 
salah  13  merupakan  Rumah  Sakit,  4  merupakan  Rumah 
Bersalin,  6  merupakan  Praktik  Bidan,  4  merupakan 
Poliklinik/Balai  Pengobatan,  dan 
tidak  dapat 
diidentifikasi. 

sisanya 

Kemudian  6,2  persen  dari  145  kesalahan  merupakan 
penandaan  berkaitan  dengan  kantor  yang  berkaitan  dengan 
seperti  dinas  kesehatan  dan  11,03  persen 
kesehatan 
merupakan  penandaan  yang 
tidak  berhubungan  dengan 
kesehatan. 

Infrastruktur Tempat Ibadah 
Pada 

infrastruktur  Tempat  Ibadah 

total  17 
penandaan yang tidak sesuai, 17 kesalahan ini tidak memiliki 
pola  tertentu.  Jumlah  penandaan  yang  tidak  sesuai  untuk 
Infrastruktur  Tempat  Ibadah  dapat  dilihat  pada  gambar  15 
berikut ini. 

terdapat 

 8 / 15 

 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

7 

225 

2 

1 

1 

2 

1 

3 

0 

Gambar 15. Jumlah Penandaan Tempat Ibadah yang Tidak Sesuai 

Infrastruktur Ekonomi 
Pada  infrastruktur  Ekonomi  terdapat  total  737  penandaan 
yang  tidak  sesuai  yang  dapat  dilihat  pada  gambar  16  berikut 
ini. 

244 

144 

95 

113 

41  54 

14 

9  23 

Gambar 16. Jumlah Penandaan Ekonomi yang Tidak Sesuai 

terhadap  warung/kedai  makanan 

Pada  gambar  16  dapat  dilihat  Kelompok  Restoran/Rumah 
Makan  memiliki 
jumlah  kesalahan  paling  banyak,  dan 
kemudian  diikuti  oleh  kelompok  Minimarket/Swalayan. 
ini  disebabkan  adanya 
Kesalahan  yang  cukup  banyak 
kecenderungan  dari  pengguna  Google  Maps  saat  melakukan 
penandaan 
dengan 
memasukkannya pada kategori Restoran/Rumah Makan. Pada 
data ditemukan sebanyak 76,62 persen warung/kedai makanan 
atau  pedagang  kaki  lima  ditandai  sebagai  restoran/rumah 
makan  pada  Google  Maps.  Sedangkan  untuk  warung/toko 
yang  menjual  barang  keseharian,  pengguna  Google  Maps 
cenderung  memasukkannya  pada  kategori  Minimarket  atau 
Swalayan saat melakukan penandaan pada Google Maps, pada 
kelompok  Minimarket/Swalayan  hal  ini  ditemukan  sebanyak 
54,16 persen. 

Infrastruktur Lembaga Keuangan 
Untuk  lembaga  keuangan  terdapat  total  314  penandaan 
yang  tidak  sesuai  yang  dapat  dilihat  pada  gambar  17  berikut 
ini. 

84 

5 

Bank Pemerintah

Bank Swasta

BPR

Gambar 17. Jumlah Penandaan Lembaga Keuangan yang Tidak Sesuai 

Dari  314  penandaan  yang  tidak  sesuai  pada  Lembaga 
Keuangan 76,75 persen dikarenakan adanya penandaan ATM; 
21,66 persen dikarenakan adanya bank umum atau BPR yang 
masuk  pada  kelompok  bank  swasta  dan  sebaliknya  dan 
sisanya penandaan pada objek lain. 

6.3 Identifikasi Perbedaan untuk Infrastruktur Puskesmas dan 
Pasar 

Pada kuesioner PODES, infrastruktur puskesmas dan pasar 
ditanyakan  dengan  lebih  rinci,  untuk  puskesmas  ditanyakan 
keberadaan  puskesmas  dengan  fasilitas  rawat  inap,  tanpa 
rawat  inap,  dan  puskesmas  pembantu.  Sedangkan  untuk 
infrastruktur  pasar  ditanyakan  keberadaan  pasar  dengan 
bangunan  permanen,  semi  permanen,  dan  tanpa  bangunan. 
Pada  pengumpulan  data,  masing-masing  pertanyaan  tersebut 
diwakili  oleh  satu  kata  kunci  pencarian,  kemudian  peneliti 
semua 
menambahkan  kata  kunci  yang  menggambar 
infrastruktur  tersebut  secara  umum  yakni  dengan  kata  kunci 
“puskesmas”  dan  “pasar”  saja.  Hal  ini  ditujukan  untuk 
mengetahui  apakah  kata  kunci  yang  lebih  umum  dapat 
lain.  Karena  adanya 
mencakup  keseluruhan  kata  kunci 
penambahan  kata  kunci  ini,  proses  pre-processing  data 
dilakukan  secara  terpisah  untuk  infrastruktur  puskesmas  dan 
pasar. 

6.3.1 Puskesmas  
Pada pencarian menggunakan kata kunci “puskesmas rawat 
inap” didapatkan 8 penandaan setelah dilakukan penghapusan 
untuk  duplikasi  kasus  A.  Dari  8  penandaan  ini  fasilitas 
puskesmas  tidak  dapat  diidentifikasi  dari  penamaannya  saja, 
kemudian  setelah  dilakukan  penghapusan  untuk  duplikasi 
kasus  B  dan  C  serta  penandaan  yang  tidak  sesuai,  tersisa  1 
penandaan  yang  diidentifikasi  sebagai  puskesmas  yang 
memiliki pelayanan rawat inap.    

Untuk  pencarian  dengan  kata  kunci  “puskesmas  tanpa 
rawat  inap”  didapatkan  5  penandaan,  kemudian  setelah 
dilakukan tahap pre-processing data tersisa 2 penandaan yang 
diidentifikasi sebagai puskesmas tanpa rawat inap. Sedangkan 
untuk  pencarian  menggunakan  kata  kunci  “puskesmas 
pembantu” didapatkan 11 penandaan dan hanya 8 diantaranya 
diidentifikasi  sebagai  puskesmas  pembantu  setelah  dilakukan 
pengidentifikasian. 

Pada  pencarian  menggunakan  kata  kunci  “puskesmas” 
didapatkan  28  penandaan  dan  7  diantaranya  juga  ditemukan 
pada  pencarian  dengan  tiga  kata  kunci  lainnya.  Setelah 
dilakukan pre-processing data, tersisa 17 penandaan pada kata 
kunci “puskesmas”. 17 penandaan ini kemudian diidentifikasi 

 9 / 15 

 
 
 
 
 
 
dengan  mengunjungi  laman  Google  Maps  dengan  hasil  14 
merupakan  puskesmas  tanpa  rawat  inap  dan  3  sisanya 
merupakan puskesmas pembantu. 

 Berdasarkan  penjelasan  tersebut  memperlihatkan  bahwa 
semakin  umum  kata  kunci  yang  digunakan,  maka  hasil 
pencarian  akan  semakin  relevan.  Dan  dapat  disimpulkan 
tidak  dapat  mengidentifikasikan 
bahwa  Google  Maps 
puskesmas  berdasarkan  fasilitasnya,  dan  untuk  menggunakan 
data  Google  Maps  pada  rincian  pertanyaan  mengenai  jumlah 
puskesmas  pada  PODES  harus  diidentifikasi  melalui 
informasi yang terbatas pada Google Maps oleh petugas yang 
menanganinya dan hal ini akan bersifat subjektif. 

6.3.2 Pasar  
Dari  empat  kata  kunci  yang  digunakan  untuk  infrastruktur 
Pasar,  hanya  terdapat  data  untuk  kata  kunci  “pasar  tanpa 
bangunan”  dan  “pasar”.  Pencarian  menggunakan  kata  kunci 
“pasar  tanpa  bangunan”  mendapatkan  95  hasil  penandaan, 
namun  setelah  dilakukan  pre-processing  data  95  penandaan 
ini  tidak  merujuk  pada  pasar  tanpa  bangunan  di  lapangan, 
sebagai  berikut:  41  merujuk  pada 
dengan 
minimarket/supermarket, 19 pada kantor pos, 13 pada bank,  6 
pasar permanen/semi permanen, 6 pada warung/toko, 3 kantor, 
2 hotel, 2 asuransi, dan 3 merujuk ke objek lain. 

rincian 

Sedangkan,  kata  kunci  “pasar”  mendapatkan  53  hasil 
penandaan,  kemudian  3  penandaan  dihapuskan  karena 
duplikasi  kasus  C  dan  23  karena  penandaan  tidak  merujuk 
pada  pasar.  Sehingga  tersisa  31  penandaan  yang  kemudian 
diidentifikasikan seperti pada infrastruktur puskesmas. Setelah 
pengidentifikasian,  10  penandaan  diidentifikasi  sebagai  pasar 
dengan bangunan permanen, 11 penandaan sebagai pasar semi 
permanen, 7 penandaan sebagai pasar tanpa bangunan, dan 3 
penandaan tidak dapat diidentifikasi karena tidak foto maupun 
ulasan  pada  Google  Maps  untuk  mengidentifikasikan 
penandaan.  Berdasarkan  hasil  tersebut  disimpulkan  bahwa 
Google  Maps 
tidak  bisa  mengidentifikasikan  perbedaan 
bangunan dari infrastruktur pasar tanpa dilakukan identifikasi 
secara  manual  dan  subjektif,  serta  ada  kemungkinan  tidak 
berhasil diidentifikasi. 

6.4 Analisis Penggunaan Kata Kunci 

Dari  penggunaan  71  kata  kunci  ditemukan  beberapa 
penggunaan  yang  tidak  efektif.  Pertama,  pada  penggunaan 
kata  kunci “praktik bidan”  web-scraper  mengumpulkan 1283 
penandaan,  namun  setelah  melakukan  pre-processing  tahap 
satu (penghapusan penandaan di luar desa) hanya bersisa satu 
pre-processing 
dalam 
penandaan.  Sedangkan 
kelompok  “rumah  sakit  bersalin”  dan  “rumah  bersalin” 
ditemukan  enam  penandaan  tempat  praktik  bidan.  Tidak 
diketahui penyebab terjadinya hal ini, namun untuk pencarian 
selanjutnya dapat menggunakan kata kunci “bidan” saja. 

proses 

“pasar  bangunan 

Kedua,  penggunaan  kata  kunci  “kelompok  pertokoan”, 
“pasar  bangunan  permanen”, 
semi 
permanen”,  dan  “pasar  tanpa  bangunan”  sangat  tidak  efektif 
karena  pada  data  dari  Google  Maps 
tidak  ditemukan 
penandaan  yang  sesuai  menggunakan  kata  kunci  tersebut. 
Sehingga  untuk  penggunaannya  Google  Maps  tidak  dapat 
memenuhi  variabel 
infrastruktur  pada  kelompok 
“kelompok  pertokoan”  namun  untuk  pasar  masih  bisa 

jumlah 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

dipenuhi dengan penggunaan  kata  kunci “pasar”  seperti  yang 
sudah dijelaskan pada bagian 6.3. 

6.5Perbandingan  Data  Google  Maps,  PODES,  dan 
Geotagging 

Setelah  tahap  pre-processing  dilaksanakan,  data  sudah 
dapat  diolah  untuk  memenuhi  variabel  jumlah  infrastruktur 
pada  desa.  Berikutnya  data  dari  Google  Maps  akan 
dibandingkan  dengan  data  PODES  2018,  PODES  2020,  dan 
data  Geotagging.  Sebagai  catatan,  untuk  data  PODES  2020 
dan Geotagging beberapa infrastruktur tidak tersedia datanya.  
Perbandingan  dari  ketiga  sumber  data  tersebut  disajikan 
dalam  bentuk  grafik  batang  dari  jumlah  total  infrastruktur  di 
Kota  Yogyakarta 
kelompok 
infrastruktur. 

untuk  masing-masing 

Untuk  mengetahui  tingkat  akurasi  dari  data  Google  Maps, 
dihitung  jumlah  desa  yang  berhasil  diestimasi  dengan  benar 
oleh  data  Google  Maps.  Penghitungan  ini  dilakukan  dengan 
cara  mengurangi  jumlah  infrastruktur  per  desa  dari  PODES 
atau  Geotagging  dengan  data  Google  Maps,  jika  nilai 
perhitungan  selisih bernilai nol  ini artinya data  Google Maps 
mengestimasi  dengan  akurat.  Namun  jika  bernilai  lebih  dari 
nol artinya data Google Maps underestimate dan jika bernilai 
kurang  dari  nol  artinya  data  Google  Maps  overestimate. 
Contohnya  jumlah  PAUD  dari  hasil  pengumpulan  data 
menggunakan  Google  Maps  pada  Desa  Kricak  dicari 
selisihnya  dengan  jumlah  PAUD  pada  PODES  dan  data 
Geotagging,  persentase  yang  akan  disajikan  merupakan 
persentase banyaknya desa  yang  memiliki selisih  kurang dari 
nol, sama dengan nol, atau lebih dari nol. 

6.4.1 Infrastruktur Pendidikan  
Untuk  infrastruktur  pendidikan,  data  PODES  2020  dan 
Geotagging  tidak  tersedia  untuk  kelompok  SDLB,  SMPLB, 
SMALB, pondok pesantren, madrasah diniyyah, dan seminari, 
serta ditambah kelompok PAUD pada Geotagging. 

Big Data

Podes '18

Podes 20

Geotagging

8
3
5

7
1
5

5
3
2

7
2
2

7
4
2

1
2
1

7
6
1

9
6
1

5
7
1

2
9

PAUD

TK/RA/BA

SD/MI

5
7

9
6

0
7

2
5

8
4

3
5

4
3

9
2

9
2

5
3

2
3

3
4

2
4

SMP/MTs

SMA/MA

SMK

 10 / 15 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Big Data

Podes '18

Podes 20

Geotagging

PODES 18

PODES 20

GEO

BG

8
3

0
8

2
6

1
5

4
4

8

3

4

3

1
2

6
1

2
1

3
1

4

2

1

2

2

1

0

Akademi/PT

SDLB

SMPLB

Rumah Sakit

RSBersalin

Puskesmas Inap

6
1

6
1

6
1

4
1

Puskesmas
Tanpa Inap

3
2

8
1

5

1

9

4

0

0

SMALB

Ponpes

Mdiniyah

Seminari

Gambar 18. Perbandingan data hasil scraping, PODES, dan Big Data pada 

Infrastruktur pendidikan 

Dari  gambar  18,  terlihat  hasil  scraping  untuk  infrastruktur 
pendidikan  masih  memiliki  jarak  yang  cukup  jauh,  baik 
terhadap  data  PODES  maupun  data  Geotagging.  Dari 
penghitungan selisih antara data Google Maps dengan PODES 
2018,  PODES  2020,  dan  data  Geotagging  pada  infrastruktur 
pendidikan  yang  bernilai  nol  berturut-turut  sebesar  59,83 
persen,  37,46  persen,  dan  42,96  persen.  Dan  nilai  selisih 
mendominasi di atas nol terhadap PODES 2020 sebesar 52,06 
persen  dan  46,67  persen 
terhadap  data  Geotagging. 
Berdasarkan perbandingan dengan data PODES terbaru, yakni 
PODES  2020  dan  data  Geotagging  disimpulkan  infrastruktur 
pendidikan dari Google Maps cenderung underestimate. 

6.4.2 Infrastruktur Kesehatan  
Pada 

gambar 

19 

perbandingan 

jumlah 
disajikan 
infrastruktur  kesehatan  hasil  pengumpulan  melalui  Google 
Maps,  data  PODES,  dan  data  Geotagging.    Dari  gambar  19 
terlihat hasil data Google Maps untuk infrastruktur kesehatan 
pada  beberapa  kelompok  terlihat  cukup  baik  dan  memiliki 
selisih  yang  kecil,  yakni  untuk  infrastruktur  Puskesmas, 
Poskesdes, dan Apotek. Pada penghitungan selisih antara data 
Google  Maps  dengan  PODES  2018,  PODES  2020,  dan  data 
Geotagging  pada  infrastruktur  kesehatan  yang  bernilai  nol 
berturut-turut  terdapat  69,40  persen,  71,79  persen,  dan  69,26 
persen. Dari persentase ini dapat dikatakan data Google Maps 
cukup  baik  dalam  mengestimasi 
infrastruktur 
kesehatan. 

jumlah 

7
9
1

3
7
1

1
5
1

1
1

8

8

9

3
4

9
2

1
6

3
4

6
4

8

4

1

7

9
1
1

9
1
1

4
2
1

8
1
1

2
8

5
3

0
3

6
1

2
1

1

0

2

0

2

0

0

0

0

Praktik
Bidan

Poskeskes

Polindes

Apotek

Toko
Obat/Jamu

Gambar  19.  Perbandingan  data  hasil  scraping,  PODES,  dan  Big  Data  pada 

Infrastruktur Kesehatan 

Pada  Infrastruktur  Kesehatan,  data  PODES  2020  dan 

Geotagging tidak tersedia untuk kelompok toko obat/jamu. 

20 

tempat 

gambar 

disajikan 

perbandingan 

6.4.3 Infrastruktur Tempat Ibadah  
jumlah 
Pada 
ibadah  hasil  pengumpulan  melalui 
infrastruktur 
Google  Maps,  data  PODES,  dan  data  Geotagging.  Pada 
perbandingan  Tempat  Ibadah,  perhitungan  gereja  kristen  dan 
gereja 
untuk  mempermudah 
pembandingan  karena  pada  data  Geotagging  keduanya  tidak 
dibedakan.  Dari  gambar  20  terlihat  hasil  data  Google  Maps 
masih  cukup 
ibadah, 
jauh  dalam  mengestimasi 
terkhususnya  masjid  dan  surau/langgar/musala.  Pada  bagian 
ini data Geotagging tidak tersedia untuk kelompok kapel dan 
surau/langgar/musala. 

digabungkan 

katolik 

tempat 

 11 / 15 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
BG

PODES18

PODES 20

GEO

BG

PODES18

PODES 20

GEO

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

7
2
5

0
1
5

0
9
4

1
0
4

6
7
3

5
6
1

7

7
3

1
6

8
5

5
5

9
3
3
7

9
4
3
5

5
6
1
6

5
2
0
5

6
7
7
2

4
4

1
8
1

0

Toko/Warung Kelontong

Warung/Kedai Makanan
Minuman

5

5

5

7
2

5
2

4
2

3

3

3

0
1

1
1

2

2

2

2

8

7

9

6

4

5

2

1

1

1

1

0

Kapel

Pura

Wihara

Kelenteng

Gambar 20. Perbandingan data hasil scraping, PODES, dan Big Data pada 

Infrastruktur Tempat Ibadah 

Untuk  penghitungan  selisih  antara  data  Google  Maps 
dengan  PODES  2018,  PODES  2020,  dan  data  Geotagging 
pada tempat ibadah berturut-turut terdapat 62,22 persen, 61,90 
persen, dan 69,78 persen yang memiliki nilai nol. Berdasarkan 
persentase tersebut, dapat dikatakan data Google Maps cukup 
baik dalam mengestimasi jumlah infrastruktur tempat ibadah. 

Pasar Bangunan
Permanen

Pasar Bangunan
Semi Permanen

Pasar Tanpa
Bangunan

6
6
5

8
4
4

3
4
2

3
6
1

9
4
1

0
4
1

8
8

4
9

4
2
1

1
0
4

7
8
1

4
3
1

0

2
8
3

1
6
3

2
5
3

3
6
3

4
9
2

4
6
2

0
3
2

6.4.4 Infrastruktur Ekonomi  
Pada 

21 

gambar 

perbandingan 

jarak  yang  cukup 

jumlah 
disajikan 
infrastruktur  ekonomi  hasil  pengumpulan  melalui  Google 
Maps,  data  PODES,  dan  data  Geotagging.    Dari  gambar 
terlihat  hasil  data  Google  Maps  untuk  infrastruktur  ekonomi 
memiliki 
ini  dikarenakan 
banyaknya  infrastruktur  ekonomi  di  lapangan,  namun  pada 
pengumpulan  Google  Maps  hanya  diambil  40  pencarian 
teratas. Karena keterbatasan inilah pada infrastruktur ekonomi 
terlihat  jarak  yang  sangat  besar  antara  data  Google  Maps 
dengan  sumber  data  lainnya,  terlebih  pada  infrastruktur 
toko/warung kelontong dan warung/kedai makanan minuman. 

jauh,  hal 

Gambar 21. Perbandingan data hasil scraping, PODES, dan Big Data pada 

Infrastruktur Ekonomi 

Pada penghitungan selisih antara data Google Maps dengan 
PODES  2018,  PODES  2020,  dan  data  Geotagging  pada 
infrastruktur  ekonomi  yang  bernilai  nol  berturut-turut  
sebanyak  31,33  persen,  27,33  persen,  dan  33,09  persen. 
Sedangkan sebagian besarnya selisih berada diatas nol,  yakni 
53,11  persen  terhadap  PODES  2018,  63,11  persen  terhadap 
PODES 2020, dan 58,52 persen terhadap data Geotagging.  

Persentase  selisih  bernilai  nol  cukup  rendah,  namun  tidak 
dapat  diambil  kesimpulan  bahwa  data  Google  Maps  tidak 
akurat  dikarenakan  keterbatasan  penelitian  yang 
telah 
disebutkan  sebelumnya.  Namun  dapat  menjadi  pertimbangan 
terkait  durasi  pengambilan  data  oleh  web-scraper  untuk  40 
hasil  pencarian  pada  satu  infrastruktur  menghabiskan  waktu 
kurang lebih enam hingga delapan menit dan untuk 45 desa di 
Kota Yogyakarta menghabiskan waktu kurang lebih 11 jam. 

gambar 

6.4.5 Infrastruktur Lembaga Keuangan  
22 
Pada 

jumlah 
disajikan 
infrastruktur  lembaga  keuangan  hasil  pengumpulan  melalui 
Google  Maps,  data  PODES,  dan  data  Geotagging.    Dari 
gambar  terlihat  hasil  data  Google  Maps  untuk  infrastruktur 

perbandingan 

 12 / 15 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
lembaga  keuangan  tidak  memiliki  jarak  yang  cukup  jauh 
berbeda. 

BG

PODES18

PODES 20

GEO

2
0
1

2
9

6
8

6
7

6
7

5
5

6
5

4
4

4
3

7
2

7
1

0
2

Bank Pemerintah

Bank Swasta

BPR

Gambar 22. Perbandingan data hasil scraping, PODES, dan Big Data pada 

Infrastruktur Lembaga Keuangan 

Pada penghitungan selisih antara data Google Maps dengan 
PODES  2018,  PODES  2020,  dan  data  Geotagging  pada 
infrastruktur  kesehatan  yang  bernilai  nol  berturut-turut  
sebanyak  48,15  persen,  48,15  persen,  dan  48,89  persen. 
Sedangkan  untuk  selisih  bernilai  diatas  nol  berturut-turut 
26,67 persen, 36,30 persen, dan 33,33 persen 

6.6 Tingkat Akurasi Koordinat Titik Penandaan Data Google 
Maps 

Saat  melakukan  penandaan  di  Google  Maps  besar 
kemungkinan  pengguna  tidak  meletakkan  titik  penandaan 
pada  posisi  yang  tepat.  Hal  ini  tentunya  akan  mempengaruhi 
variabel  jarak  yang  diberikan  pada  Google  Maps.  Tingkat 
akurasi  dari  koordinat  titik  penandaan  di  Google  Maps 
dihitung  dengan  cara  mengukur  jarak  antara  koordinat  yang 
dikumpulkan  dari  Google  Maps  dengan  koordinat  pada  Data 
Geotagging  dengan  memanfaatkan  Algoritma  Haversine. 
Penghitungan  tingkat  akurasi  ini  dilakukan  pada  seluruh 
kelompok  pertanyaan,  kecuali  SDLB,  SMPLB,  SMALB, 
Pondok 
Seminari, 
Pesantren,  Madrasah  Diniyah, 
Surau/Langgar/Musala,  Kapel,  kelompok  pertokoan,  dan 
Warung/Kedai Kelontong  dikarenakan data  Geotagging atau 
Google  Maps 
tersedia.  Serta  Penginapan,  dan 
Warung/Kedai  Makanan  Minuman,  Bank  Umum  &  Swasta, 
dan  BPR  dikarenakan  data  yang  besar  dengan  tingkat 
kecocokan  yang  sedikit  serta  keterbatasan  waktu.  Sehingga 
perbandingan hanya dilakukan pada 32 kelompok pertanyaan 
dengan  total  data  dari  hasil  scraping  Google  Maps  sebanyak 
1.279 
sedangkan  untuk  data  geotagging 
sebanyak  2.805  infrastruktur.  Dari  1.279  infrastruktur  yang 
dikumpulkan  dari  scraping  Google  Maps  hanya  ditemukan 
sebanyak  756  infrastruktur  (59,10  persen)  yang  bersesuaian 
dengan data hasil Geotagging.  

infrastruktur 

tidak 

 Pada  gambar  23  berikut  ini,  ditampilkan  sebaran  jarak 
antara  koordinat  infrastruktur  data  Google  Maps  dengan 
Geotagging dalam satuan meter. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar  23.  Sebaran  jarak  antara  koordinat  data  Google  Maps  dengan 
Geotagging 

tersebut 

Dari  gambar 

jarak  antara  koordinat 
terlihat 
penandaan dari Google Maps dan Geotagging sebanyak 63,89 
persen berada pada rentang 20 meter. Pada referensi  [9], jika 
jarak  dari  perhitungan  haversine  kurang  dari  2  kilometer, 
maka dua titik yang diujikan merupakan objek yang sama, jika 
mengacu  pada  hal  tersebut  data  Google  Maps  100  persen 
sesuai  dengan  data  koordinat  dari  Geotagging.  Namun  jika 
dilihat,  banyaknya  infrastruktur  yang  berada  pada  rentang 
jarak nol hingga 100 meter ada sebanyak 94,44 persen, hal ini 
menunjukkan  koordinat  penandaan  dari  data  Google  Maps 
sudah cukup akurat. 

6.7 Jarak Infrastruktur 

tertentu 

selanjutnya  akan  ditanyai 

infrastruktur  bersangkutan.  Google  Maps 

Pada  kuesioner  PODES,  jika  suatu  desa  tidak  memiliki 
infrastruktur 
jarak 
infrastruktur sejenis terdekat di desa lain. Variabel jarak yang 
dimaksud  merupakan  jarak  yang  diukur  dari  kantor  desa 
menuju 
juga 
memberikan  informasi  jarak  dan  durasi  dari  kantor  desa 
menuju sebuah infrastruktur saat menggunakan fitur “nearby”. 
Informasi  ini  tentunya  memudahkan  pengisian  kuesioner. 
Untuk  mengetahui  tingkat  akurasi  dari  informasi  jarak  yang 
peneliti  melakukan 
disediakan 
perbandingan  antara  data  jarak  Google  Maps  dengan  jarak 
pada  data  Geotagging,  dimana  pada  Geotagging,  data  jarak 
dihitung  menggunakan 
seperti 
sebelumnya. 

oleh  Google  Maps, 

haversine 

algoritma 

Perbandingan  variabel 

jarak  dilakukan  dengan  cara 
mengurangi jarak dari data Geotagging dengan data jarak dari 
Google Maps yang sebarannya dapat dilihat pada gambar 24. 
Perbandingan  ini  dilakukan  terhadap  756  infrastruktur  pada 
bagian sebelumnya. 

 13 / 15 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

mengurutkan dari yang memiliki jarak terdekat. Berdasarkan 
keunggulan  ini,  proses  pre-processing  juga  akan  menjadi 
lebih  singkat,  yakni  dapat  menghindari  adanya  penandaan 
yang  berada  di  luar  desa  karena  data  dapat  diurutkan 
berdasarkan jarak serta dapat menghindari adanya duplikasi 
kasus A.   
2)  Jumlah data yang dikumpulkan 

Jumlah  data  yang  dikumpulkan  dapat  disesuaikan 
dengan jumlah infrastruktur pada  masing-masing  kelompok 
berdasarkan  pendataan  PODES  atau  data  Geotagging, 
terkhususnya  pada  Infrastruktur  Ekonomi  yang  jumlah 
infrastruktur  per  kelompok  pertanyaannya  pada  umumnya 
berjumlah ratusan. 
3)  Melakukan penandaan dan pengecekan ke lapangan 

Penandaan ke lapangan dapat dilakukan untuk mengatasi 
adanya  objek-objek  yang  tidak  mendapat  penandaan  di 
lapangan, meskipun akan memberikan beban kerja baru saat 
dilakukan  pertama  kalinya,  namun  penandaan  ke  lapangan 
akan  sangat  membantu  meningkatkan  akurasi  data.  Selain 
itu  pengecekan  ke  lapangan  juga  perlu  dilakukan  untuk 
melakukan validasi data yang dikumpulkan. 
4)  Petugas yang menangani pre-processing data 

Petugas  yang  menangani  proses  pre-processing  data 
sangat direkomendasikan berasal dari daerah yang sama. Hal 
ini  ditujukan  agar  petugas  waspada  pada  kebiasaan 
masyarakat 
yang  mungkin 
mempengaruhi penandaan pada Google Maps. 

tersebut 

daerah 

pada 

VII. 

PENUTUP 

Berdasarkan  hasil  dan  pembahasan  diatas,  sesuai  dengan 
tujuan  penelitian  untuk  mengetahui  tingkat  akurasi  data 
Google  Maps,  setelah  dilakukan  perbandingan  dengan  data 
PODES  dan  data  Geotagging  dapat  disimpulkan  bahwa 
akurasi  data  Google  Maps  dalam  mengestimasi  jumlah 
infrastruktur per desa secara umum masih rendah, yakni pada 
rentang  27  hingga  59  persen  kecuali  pada  Infrastruktur 
Kesehatan  dan  Tempat  Ibadah  yang  mencapai  akurasi  pada 
rentang 61 hingga 71 persen. Dalam proses pengumpulan data 
Google  Maps  membutuhkan  durasi  yang  cukup  lama  serta 
perangkat  dan  koneksi  internet  yang  memadai.  Proses  pre-
processing data untuk menjadikan data Google Maps fit to use 
membutuhkan waktu dan tenaga yang banyak karena beberapa 
tahapan  harus  dilakukan  secara  manual  dan  penuh  ketelitian. 
Proses  pre-processing  juga  menghasilkan  data  yang  bersifat 
subjektif  dari  peneliti  dalam  mengidentifikasi  infrastruktur 
pada Google Maps.  

Kemudian,  dalam  memenuhi  variabel  jumlah  infrastruktur 
puskesmas  dan  pasar  data  Google  Maps  belum  dapat 
mengelompokkan  secara  langsung  infrastruktur  berdasarkan 
fasilitas dan bangunannya, untuk menggunakannya diperlukan 
identifikasi  dari  peneliti  apakah 
infrastruktur 
puskesmas  yang  dikumpulkan  oleh  web-scraper  merupakan 
puskesmas  pembantu  atau  merupakan  puskesmas  dengan 
fasilitas  rawat  inap  atau  tidak  dan  mengidentifikasi  apakah 
data  pasar  yang  dikumpulkan  memiliki  bangunan  tetap  atau 
tidak.  Kemudian,  ditemukan  bahwa  titik  koordinat  data 
Google  Maps  sangat  akurat,  tetapi  untuk  informasi  variabel 

sebuah 

 14 / 15 

Gambar  24.  Sebaran  jarak  antara  koordinat  data  Google  Maps  dengan 
Geotagging 

. 
Berdasarkan gambar 24 terlihat bahwa sebaran selisih jarak 
dominan  berada  pada  rentang  1.500  hingga  3.000  meter  dan 
sebanyak  90,92  persen  hasil  selisih  antara  data  Geotagging 
dengan  data  Google  Maps  bernilai  positif,  yang  artinya  nilai 
jarak pada Google Maps cenderung lebih kecil daripada jarak 
pada  data  Geotagging.  Dan  hanya  terdapat  1,98  persen  yang 
memiliki selisih pada rentang 0-100 meter.  

Namun,  berdasarkan  penelitian  yang  dilakukan  oleh  [10] 
yang melakukan perbandingan jarak dari algoritma haversine, 
Google  Maps  API,  dan  jarak  sebenarnya  yang  dihitung 
menggunakan  odometer  kendaraan  bermotor  pada  6  objek, 
menemukan  bahwa  MAPE  (Mean  Absolute  Error)  dari 
Google  Maps  lebih  rendah  (11  persen)  dibandingkan  dengan 
MAPE  dari  algoritma  Haversine  (33 persen).  Dimana  hal  ini 
dikarenakan  pada  algortima  haversine  mengambil  garis  lurus 
dalam  perhitungan 
jarak  sedangkan  Google  Maps  API 
mengikuti  rute  jalan  terpendek  dan  hampir  sama  menyerupai 
perhitungan secara real [10]. 

Berdasarkan penelitian [10] tersebut, terdapat peluang data 
jarak  dari  Google  Maps  memiliki  akurasi  yang  cukup  baik, 
namun  harus  dilakukan  penelitian  ke 
lapangan  untuk 
membandingkan  jarak  yang  sebenarnya  dengan  jarak  pada 
Google  Maps  dikarenakan  penelitian  [10]  baru  dilakukan 
untuk 6 objek saja. 

6.8  Rekomendasi  untuk  Penggunaan  Data  Google  Maps 
dalam  Pemenuhan  Variabel  Jumlah  dan  Jarak  Infrastruktur 
pada PODES 

Berikut  ini  diuraikan  beberapa  rekomendasi  yang  bisa 
dilakukan  jika  ingin  menggunakan  data  dari  Google  Maps 
dalam  pemenuhan  variabel  jumlah  dan  jarak  infrastruktur 
pada PODES agar dapat meningkatkan akurasi data: 
1)  Penggunaan Google Maps API 

Berdasarkan  kajian  Praktik  Kerja  Lapangan  Politeknik 
Statistika  STIS  diketahui  bahwa  pengumpulan  data 
menggunakan  Google  Maps  API  menunjukkan  keunggulan 
yang  lebih  banyak  dibandingkan  metode  web-scraping  [7]. 
Keunggulan 
tersebut  diantaranya  adalah  durasi  waktu 
pengumpulan  data  yang  lebih  singkat  (5-10  detik  per 
infrastruktur), 
tidak  membutuhkan  komputer  dengan 
spesifikasi yang tinggi, tidak membutuhkan koneksi internet 
yang  cepat,  dan  data  dapat  dikumpulkan  dengan 

 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

jarak yang disediakan oleh Google Maps perlu penelitian lebih 
lanjut ke lapangan.  

Berdasarkan  temuan-temuan  tersebut,  dapat  disimpulkan 
bahwa  data  Google  Maps  belum  direkomedasikan  digunakan 
sebagai  sumber  data  dalam  pengisian  variabel 
jumlah 
infrastruktur,  meskipun  untuk  infrastruktur  kesehatan  dan 
tempat  ibadah  cukup  baik,  namun  persentase  kesesuaian 
tertinggi  hanya  71  persen,  sehingga  belum  disarankan. 
Sedangkan  untuk  variabel  jarak  infrastruktur,  karena  akurasi 
titik koordinat yang sangat baik, pengisian variabel jarak dapat 
diisi  melalui  penghitungan  jarak  menggunakan  algoritma 
Haversine. Untuk penelitian selanjutnya peneliti menyarankan 
untuk  mengkaji  perbandingan  tingkat  akurasi  data  Google 
Maps  untuk  pemenuhan  variabel 
jarak 
infrastruktur  PODES  antara  wilayah  Kabupaten  dan  Kota, 
mengkaji tingkat akurasi data Google Maps untuk pemenuhan 
variabel  jumlah  dan  jarak  infrastruktur  PODES  pada  wilayah 
dengan  penetrasi 
internet  yang  rendah,  dan  mengatasi 
keterbatasan penelitian untuk mengetahui tingkat akurasi data 
Google Maps dalam pemenuhan variabel jumlah infrastruktur 
pada  kelompok  yang  memiliki  jumlah  infrastruktur  diatas 
empat puluh per desanya. 

jumlah  dan 

BPS, 2018. 

DAFTAR PUSTAKA 
[1]  Badan Pusat Statistik. “Pedoman Pencacah Desa/Kelurahan Podes 2018”, 

[2]  Badan Pusat Statistik. “Statistik Potensi Desa 2018”. BPS, 2018. 
[3]  A.  De  Mauro,  M.  Greco,  and  M.  Grimaldi,  “What  is  big  data?  A 
consensual  definition  and  a  review  of  key  research  topics,”  in  AIP 
Conference  Proceedings,  2015,  vol.  1644,  no.  2015,  pp.  97–104,  doi: 
10.1063/1.4907823. 

[4]  S.  Shukla,  V.  Kukade,  and  S.  Mujawar,  “Big  Data:  Concept,  Handling 
and  Challenges:  An  Overview,”  Int. J.  Comput.  Appl.,  vol.  114, no. 11, 
pp. 6–9, 2015, doi: 10.5120/20020-1537. 

[5]  S.  Vale  and  T.  Gjaltema,  “Classification  of  Types  of  Big  Data  - 
Classification  of  Types  of  Big  Data  -  UNECE  Statswiki,”  2020. 
https://statswiki.unece.org/display/bigdata/Classification+of+Types+of+
Big+Data (accessed Jul. 05, 2021). 

[6]  Patrick, “The Big Data Driving Google Maps | Oceans of Data,”  Oceans 
of  Data  Institute,  2016.  http://oceansofdata.org/big-data-driving-google-
maps (accessed Jul. 05, 2021). 

[7]  PKL  59  Politeknik  Statistika  STIS.  “Pemanfaatan  Big  Data  dalam 
Pemenuhan  Variabel  Jarak  dan  Jumlah  Infrastruktur  pada  Data  Potensi 
Desa”, Laporan dari Riset 4 PKL 59 Politeknik Statistika STS, Jakarta : 
2020. 

[8]  C. Nnuryakin, N.A. Gumelar, M.D. Ul-Haq, R. Patonangi, A.P. Pratama, 
“Modernizing  Official  Statistics  With  Big  Data  :  A  Case  On  PODES”, 
LFPEM-FEBUI Working Paper,2020. 

[9]  C.  F.  Annisa  &  S.Pramana.  “Kajian  Pemanfaatan  Data  Google  Maps 
dalam  Official  Statistics,”  dalam  Seminar  Nasional  Official  Statistics, 
2020,pp.328-337 

[10] D.R.  Mukarromin  &  A.  Nilogiri.  “Perbandingan  Algortima  Haversine 
dan  Perhitungan  Jarak Menggunakan  Google  Maps  API  pada  Pemilihan 
Unit Pelaksana Teknis Dinas (UPTD) Terdekat Dengan Lokasi Pelaporan 
Kejadian  Masyarakat  Di  Kabupaten 
Jember”.  Universitas 
Muhammadiyah Jember, 2020. 

 15 / 15 

 
 
"
221709813,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Simulasi Mobilitas Penduduk DKI Jakarta 
Menggunakan Agent-Based Modeling  

Marko Januarta Putra Mulyowidodo (221709813, 4SD1) 
Dosen Pembimbing: Dr. Tiodora Hadumaon Siagian, M.Pop.Hum.Res. 

Ringkasan— Salah satu bentuk mobilitas non permanen yang 
berkembang pesat saat ini di kota-kota besar di Indonesia adalah 
kegiatan komuter atau nglaju. Banyaknya penduduk yang tinggal 
di  wilayah  peri  urban  menjadi  penglaju  menjadikan  kegiatan 
komuter ini berdampak pada pembangunan wilayah, khususnya 
untuk  penyediaan  transportasi  yang  beragam  dan  memadai. 
Sebagai  wilayah  ibukota,  Provinsi  DKI  Jakarta  tentu  memiliki 
banyak  penglaju  sehingga  dipandang  perlu  mewujudkan  smart 
mobility  sebagai  salah  satu  program  Smart  City di  DKI  Jakarta. 
Untuk itu penelitian ini melakukan simulasi mobilitas penduduk 
DKI Jakarta menggunakan Agent-Based Modeling. Simulasi yang 
dilakukan  adalah  simulasi  aktivitas  mobilitas  dari  rumah  ke 
kantor dan sekolah. Simulasi yang dibangun berhasil disesuaikan 
dengan  jenis  kegiatan  utama  penduduk  komuter  serta  waktu 
kegiatan  yang  terdapat  pada  publikasi  Statistik  Komuter 
Jabodetabek  2019.  Kemudian  dilakukan  perbandingan  dengan 
data empiris berupa data dari media sosial Twitter menunjukkan 
bahwa big data memiliki potensi untuk dimanfaatkan untuk 
studi  kependudukan.  Diharapkan  hasil  penelitian  ini  dapat 
membantu dalam perencanaan dan pengambilan dasar kebijakan 
penyediaan transportasi di DKI Jakarta yang lebih efisien, efektif 
dan memadai. 

Komunikasi,  Informatika  dan  Statistik  Provinsi  DKI  Jakarta, 
masih  banyak  warga  ibukota  yang  menganggap  memiliki 
kendaraan  pribadi  masih  jauh  lebih  efektif  dalam  menunjang 
mobilitas  sehari-hari,  walaupun  sudah  tersedia  banyak  moda 
transportasi massal. Sebagaimana dapat dilihat pada Gambar 1, 
jumlah kendaraan bermotor di provinsi DKI Jakarta tahun 2019 
naik  sebesar 0,7  persen,  tepatnya  sebanyak  77.158  kendaraan 
dibanding 
jumlah 
kendaraan  bermotor 
ini  menimbulkan  masalah  karena 
pertumbuhan  jumlah  kendaraan  tidak  seimbang  dengan  laju 
pertumbuhan  jalan.  Tentu  masalah  ini  berdampak  pada 
kelancaran aktivitas komuter bagai masyarakat di DKI Jakarta. 

sebelumnya.[3]  Peningkatan 

tahun 

11.762.763 

11.839.921 

11.274.597 

Kata  Kunci—  Mobilitas,  Transportasi,  Agent-Based  Modeling, 

Simulasi 

2017

2018

2019

I.  LATAR BELAKANG 

Perencanaan pembangunan membutuhkan informasi penting 
mengenai  keadaan  kependudukan  setempat  seperti  jumlah 
penduduk,  sebaran  penduduk,  aktivitas  penduduk  dan 
sebagainya.  Informasi-informasi  tersebut  bisa  diperoleh  dari 
berbagai  hasil  survei  atau  sensus  yang  sudah  ada.  Namun 
demikian  jumlah  penduduk  dalam  suatu  tempat  bersifat 
dinamis karena dapat berubah yang dipengaruhi oleh beberapa 
faktor. Salah satu faktornya adalah perpindahan atau mobilitas 
penduduk. 

Menurut  Badan  Pusat  Statistik  (BPS),  salah  satu  bentuk 
mobilitas non permanen yang mengalami perkembangan pesat 
di  kota-kota  besar  di  Indonesia  seperti  DKI  Jakarta  adalah 
kegiatan komuter. Komuter adalah pergerakan yang dilakukan 
dalam  waktu  satu  hari  dengan  melintasi  batas  wilayah  dan 
kembali  ke 
tempat  asalnya.  [1]  Hasil  Survei  Komuter 
Jabodetabek 2019  oleh  BPS  menunjukkan bahwa  11,1% dari 
29,3 juta penduduk Jabodetabek yang berumur 5 tahun ke atas 
merupakan komuter. [2] 

Aktivitas  komuter  berkaitan  erat  dengan  permasalahan  di 
sektor transportasi. Permasalahan tersebut selalu menjadi salah 
satu  perhatian  utama  bagi  Pemerintah  DKI  Jakarta  setiap 
tahunnya.  Menurut  Unit  Pengelola  Statistik  dari  Dinas 

Sumber: Dinas Perhubungan Provinsi DKI Jakarta, statistik.jakarta.go.id 

Gambar 1. Perkembangan Jumlah Kendaraan Bermotor di DKI Jakarta 

Tahun 2017-2019 

Diperlukan  sarana 

transportasi  yang  memadai  untuk 
menunjang  aktivitas  komuter  di  DKI  Jakarta.  Hal  ini  juga 
terkait  dengan  perlunya  Jakarta  untuk  menjadi  sebuah  smart 
city, yang ditunjang oleh smart mobility di mana adanya suatu 
sistem  transportasi  yang  memungkinkan  masyarakat  untuk 
bergerak  seminimal  dan  secepat  mungkin,  dengan  hambatan 
yang  rendah  untuk  mencapai  kebutuhannya.    [4]  Pemerintah 
telah  membuat  berbagai  kebijakan  dan  usaha  untuk 
menyediakan moda transportasi yang semakin beragam, mudah 
diakses, efisien, serta terintegrasi dengan teknologi, informasi, 
dan  komunikasi.  Misalnya  KRL  Commuter  Line,  bus 
Transjakarta,  Moda  Raya  Terpadu  Jakarta,  Jak  Lingko,  dan 
sebagainya.  Rencana  Induk  Transportasi  Jabodetabek  (RITJ) 
tahun 2018 – 2029 dilaksanakan dalam tiga tahap, yaitu tahap 
pertama  tahun  2018 –  2019,  tahap  kedua  tahun  2020 – 2024, 
dan  tahap  terakhir  tahun  2025  -  2029.  Beberapa  target  yang 
ingin  dicapai  dalam  RITJ  yaitu  pada  2029  diharapkan 
pergerakan  orang  dengan  menggunakan  angkutan  umum 
perkotaan mencapai enam puluh persen dari total pergerakan, 

 1 / 9 

 
 
 
 
 
 
 
 
 
 
cakupan  pelayanan  angkutan  umum  perkotaan  mencapai 
delapan puluh persen dari panjang jalan, serta akses jalan kaki 
ke angkutan umum maksimal 500 meter [5]. Upaya mengejar 
target  itu  pun  sudah  dilakukan  melalui  penyediaan  sarana 
transportasi umum yang lebih bervariasi. 

Beberapa  penelitian  sebelumnya  mengenai  mobilitas  DKI 
Jakarta antara lain  penelitian  Yusuf  dan  Buchori  (2018)  yang 
melakukan penelitian berupa simulasi pergerakan penduduk di 
Provinsi DKI Jakarta. Dalam penelitian ini simulasi dilakukan 
dengan  metode  pemodelan  berbasis  agen  (agent  based 
modelling atau ABM). [6] 

ABM  adalah  suatu  sistem  yang  dimodelkan  sebagai 
kumpulan  entitas  pembuat  keputusan  otonom  yang  disebut 
agen.  Model  disimulasikan  dalam  sebuah  sistem  yang 
kompleks dan dinamis, di mana agen-agen tersebut mempunyai 
sifat heterogen dan otonom serta dapat berinteraksi antar agen 
dan lingkungannya. [7] ABM dapat diterapkan untuk simulasi 
di  berbagai  bidang  atau  masalah,  seperti  di  bidang  sosial, 
ekonomi,  transportasi,  dan  lain  lain.  Masih  belum  banyak 
penelitian, khususnya di Indonesia, yang menggunakan ABM 
sebagai  metode  analisisnya.  Padahal  ABM  memberikan 
keuntungan  tersendiri  untuk  mempelajari  berbagai  fenomena 
yang bisa terjadi dalam masyarakat. 

Penelitian  ini  berusaha  mengembangkan  model  aktivitas 
penduduk  di  DKI  Jakarta  dengan  pendekatan  pemodelan 
berbasis  agen.  Model  dan  hasil  analisis  pada  penelitian  ini 
diharapkan  nantinya  bisa  memberikan  gambaran  mengenai 
kondisi lalu lintas jalan raya dan penggunaan moda transportasi 
ini  diharapkan  dapat  mendukung 
umum.  Pemodelan 
perencanaan dan pengambilan keputusan dalam pembangunan 
DKI Jakarta khususnya untuk mewujudkan Smart Mobility.  

II.  TUJUAN PENELITIAN 

Berdasarkan  permasalahan  yang  telah  diuraikan  di  atas, 

maka tujuan dari penelitian ini adalah: 
1.  Membangkitkan  populasi  sintesis  DKI  Jakarta  dengan 

menyesuaikan statistik komuter jabodetabek. 

2.  Melakukan  simulasi  pemodelan  mobilitas  penduduk 

berbasis agen di Provinsi DKI Jakarta 

3.  Melakukan  visualisasi  dan  analisis  hasil 

simulasi 

pemodelan mobilitas penduduk di Provinsi DKI Jakarta 
4.  Melakukan validasi dan verifikasi hasil simulasi pemodelan 
mobilitas  penduduk  di  Provinsi  DKI  Jakarta  melalui 
perbandingan dengan data dari media sosial Twitter.  

III. PENELITIAN TERKAIT 

Menurut  penelitian  Setyodhono,  lebih  dari  80  persen 
komuter  di  Jabodetabek  adalah  pekerja.  Sebanyak  80  persen 
komuter  menggunakan  sepeda  motor  untuk  mencapai  tempat 
kerjanya,  kemudian  15,25  persen  menggunakan  kendaraan 
umum.  Penelitian  tersebut  menggunakan  analisis  regresi 
logistik  multinomial  dan  menunjukkan  bahwa  faktor:  umur, 
jenis  kelamin,  tingkat  pendidikan,  status  pekerjaan,  rata-rata 
penghasilan  pekerja  komuter,  jarak  tempat  tinggal  ke  tempat 
kerja, serta lama dan biaya perjalanan memiliki pengaruh yang 
nyata  terhadap  pemilihan  moda  transportasi  yang  digunakan 
untuk melakukan kegiatan utama komuter tersebut.[1] 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

lintas,  dimana  dalam  sistem 

Semenetara penelitian yang dilakukan oleh Tari Saputri, dkk 
menunjukkan bahwa kompleksitas interaksi dalam suatu sistem 
tersebut  melibatkan 
lalu 
pergerakan  kendaraan, memerlukan  suatu  alat analisis  berupa 
model  simulasi  untuk  evaluasi  rancangan  sistem  lalu  lintas 
tersebut.[8]  Sejalan  dengan  hal  tersebut,  Grace  O.  Kagho 
menyatakan  bahwa  telah  banyak  kemajuan  telah  dibuat  pada 
pemodelan  berbasis  objek  terhadap  transportasi.  Suatu  teknik 
komputasi 
terus  dikembangkan,  beban  komputasi  dari 
pemodelan berbasis agen tersebut semakin berkurang, sehingga 
memungkinkan untuk semakin berkembangnya pengaplikasian 
model  berbasis  agen  tersebut.  [9]  Carlos  Llorca,  dkk  dalam 
penelitian  mereka  menyebutkan  bahwa  dalam  aplikasi  model 
berbasis 
simulasi, 
penggunaan  lahan  serta  ukuran  populasi  mempengaruhi  hasil 
dari model transportasi yang sudah dibangun.[10] 

tersebut,  perbedaan 

interval 

agen 

Penelitian yang dilakukan oleh Yusuf dan Buchori dilakukan 
dengan  melakukan  simulasi  komuter  terhadap  warga  DKI 
Jakarta melalui mode transportasi kendaraan pribadi. Penelitian 
ini menunjukkan bahwa kota Jakarta Pusat menjadi titik sentral 
kegiatan sebagian besar penduduk ibukota. Selain itu big data 
juga bisa menjadi pelengkap dalam analisisnya.[6] Penelitian-
penelitian yang telah disebutkan di atas dipetakan pada Gambar 
2. 

Gambar 2. Roadmap penelitian 

IV. METODE PENELITIAN  
Simulasi pemodelan mobilitas penduduk pada penelitian ini 
mencakup  semua  wilayah  DKI  Jakarta  kecuali  wilayah 
Kabupaten  Kepulauan  Seribu  dan  penduduk  DKI  Jakarta 
sebagai  unit  analisis.  Kabupaten  Kepulauan  Seribu  tidak 
dicakup  dalam  wilayah  penelitian  karena penelitian ini  hanya 
fokus  pada  pergerakan  di  satu  wilayah  daratan  saja,  tidak 
termasuk antar pulau. Subjek dari penelitian ini yaitu individu 
dengan  kegiatan  utama  bekerja  dan  bersekolah  atau  kursus 
berdasarkan  Statistik  Komuter  Jabodetabek  2019.  Kegiatan 
yang  disimulasikan  merupakan  mobilitas  individu  dalam 
melakukan kegiatan utama komuternya yang dilakukan selama 
jangka  waktu  24  jam.  Populasi  sintetis  yang  dibangkitkan 

 2 / 9 

 
 
  
 
 
 
menyesuaikan  Statistik  Komuter  Jabodetabek  2019.  Hasil 
simulasi  ditampilkan  dalam  bentuk  visualisasi  simulasi,  serta 
data-data hasil simulasi ditampilkan dalam bentuk grafik agar 
informasi  yang  diperoleh  mudah  untuk  dipahami.  Hasil 
simulasi yang diperoleh akan dianalisis untuk mencapai tujuan-
tujuan penelitian.  

Data  yang  digunakan  dalam  penelitian  ini  adalah  data 
sekunder berupa statistik yang diproduksi oleh BPS, tepatnya 
pada publikasi Statistik Komuter Jabodetabek 2019, digunakan 
dalam  pembentukan  populasi  sintetis  yang  dibangkitkan 
sebagai  agent  dalam  proses  simulasi.  Selain  itu  dilakukan 
scrapping  data  dari  platform  media  sosial  Twitter  untuk 
memperoleh data  berupa  tweet  bergeoreferensi atau berlokasi 
di DKI Jakarta dalam kurun waktu 24 jam yang akan digunakan 
dalam proses validasi terhadap hasil simulasi. 

Tahapan dalam membangun suatu pemodelan berbasis agen 
adalah  pertama  melakukan  suatu  studi  mengenai  fenomena 
sosial  yang  menarik  untuk  diteliti  serta  teori-teori  yang 
mendukung.  Tahap  kedua  meliputi  pemodelan  itu  sendiri,  di 
mana  peneliti  membangun  model  simulasi  dengan  cara 
eksperimental. Langkah ketiga meliputi kalibrasi model dengan 
data  empiris  atau  validasi  dengan  menggunakan  uji  statistik 
yang sesuai.[11] 

publikasi  mengenai 

Pada penelitian ini diawali dengan studi literatur mengenai 
konsep simulasi pemodelan berbasis agen, konsep pemodelan 
transportasi,  serta  memahami  penggunaan  Netlogo  sebagai 
framework  simulasi  komuter.  Peneliti  kemudian  melakukan 
pengunduhan 
komuter 
jabodetabek  melalui  situs  web  Badan  Pusat  Statistik  sebagai 
dasar  pembentukan  populasi  sintetis.  Lingkungan  simulasi 
berupa  tempat  tinggal,  tempat  kerja,  sekolah,  serta  jaringan 
jalan.  Pembentukan  lingkungan  simulasi  dimulai  dengan 
mengunduh tata guna lahan, serta jaringan-jaringan jalan DKI 
Jakarta  melalui  internet  dan  melalui  situs  OpenStreetMap. 
Kemudian dilakukan editing dan cleaning data. 

statistik 

Setiap  agen  pada  simulasi  memiliki  karakteristik  yang 
berbeda-beda  yang  disebut  atribut.  Agen  yang  dibangkitkan 
pada  simulasi  akan  dibuat  serealistis  mungkin  untuk  bisa 
mewakili keadaan populasi yang sebenarnya. Variabel atribut 
agen tersebut antara lain: 

1. 
Kegiatan utama komuter (Bekerja atau Sekolah) 
2. 
Lokasi rumah 
3. 
Lokasi aktivitas 
4. 
Pilihan rute 
5.  Waktu berangkat  
6.  Waktu mulai kegiatan, 
7.  Waktu berakhir kegiatan 
Nilai-nilai  pada  atribut  agen  ditentukan  di  awal  namun 
beberapa  atribut  bisa  berubah  nilainya  sejalan  dengan 
dilakukannya beberapa iterasi pada simulasi. Selain itu terdapat 
juga nilai-nilai variabel yang dimiliki oleh lingkungan tempat 
agen  disimulasikan,  tepatnya  pada  sistem  geografis  DKI 
Jakarta. 
1. 
2. 
3. 
Jika  terjadi  kepadatan  pada  suatu  jalur.  Maka  kecepatan 
perpindahan akan berkurang. Hal tersebut juga memungkinkan 

Koordinat rumah agen 
Koordinat lokasi kegiatan 
Panjang lintasan agen 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

adanya kemacetan dan agen bisa memilih alternatif jalur yang 
belum terisi.  

Simulasi dilakukan  dengan  menggunakan aplikasi  Netlogo 
versi 6.0.1 dengan memanfaatkan beberapa ekstensi seperti gis, 
nw, dan lain-lain untuk menyesuaikan ekstensi file penunjang 
seperti  shapefile  peta  DKI  Jakarta  serta  tata  guna  lahan  dan 
jaringan  jalan.  Pada  simulasi  ini,  pengguna  bisa  menentukan 
nilai parameter sesuai kebutuhannya selama masih dalam batas 
interval  masing-masing  parameter.  Hasil  simulasi  berupa 
visualisasi  dan  dapat  diperoleh  grafik-grafik  yang  memuat 
informasi penting dari hasil simulasi tersebut 

Beberapa  kebutuhan 

sistem  menjadi  dasar  dalam 
menentukan  platfotm  atau  tools  yang  akan  digunakan  untuk 
menunjang  analisis  simulasi.  Diantaranya  membutuhkan  data 
statistik  sebagai  dasar  pembentukan  agen  sehingga  bisa 
menggambarkan  keadaan  DKI  Jakarta  di  dalam  simulai 
sehingga  mendekati  keadaan  yang  sebenarnya.  Dalam 
penelitian  ini  dibutuhkan  statistik  yang  diproduksi  BPS. 
Platform  yang  digunakan  sebagai  sarana  simulasi  perlu 
mendukung  simulasi  dalam  skala  menengah  dan  besar 
mengingat simulasi yang dibangun meliputi area DKI Jakarta. 
Dibutuhkan juga file dengan format shp yang merupakan data 
geospasial  sebagai  pemodelan  lingkungan  dalam  simulasi. 
Untuk  kebutuhan  validasi hasil  simulasi,  data  empiris  berupa 
data tweet yang memiliki atribut georeferensi. 

Visualisasi  hasil  simulasi  sebagai  gambaran  keadaan 
komuter penduduk DKI Jakarta juga disajikan dalam penelitian 
ini.  Nantinya  dalam  visualisasi 
tersebut  bisa  diperoleh 
mengenai pergerakan agen-agen dalam simulasi, serta melihat 
bagaimana  keberadaan  penduduk  komuter  di  daerah  DKI 
Jakarta  setiap  waktunya  dalam  tampilan  peta.  Kemudian 
dilakukan  analisis  deskriptif  yang  bertujuan  untuk  mendapat 
gambaran  umum  mengenai  aktivitas  komuter  penduduk  DKI 
Jakarta.  Hal  ini  dijelaskan  dengan  adanya  grafik-grafik 
mengenai  persentase  penduduk  komuter  DKI  Jakarta  setiap 
kecamatan setiap jamnya.  

Dilakukan  perbandingan  dengan  persentase  penduduk  tiap 
kecamatannya  yang  diperoleh  dari  hasil  scrapping  data  dari 
media  sosial  Twitter.  Perbandingan  dilakukan  dengan 
menghitung persentase tweet dalam satu kecamatan dibagi total 
tweet,  dengan  persentase  penduduk  komuter  dalam  satu 
kecamatan  per  total  penduduk  komuter.  Selain  itu  juga 
dilakukan  visualisasi  ke  dalam  bentuk  peta  untuk  kepadatan, 
baik tweet maupun agen simulasi pada jam-jam tertentu. 

V.  KERANGKA PIKIR 

Suatu model  simulasi  diperlukan  dalam  perancangan  suatu 
sistem  yang  akan  dibuat  untuk  kepentingan  analisis  dan 
evaluasi.  Untuk  itu  berdasarkan  teori-teori  yang  telah  dikaji 
sebelumnya maka dibangun model simulasi berbasis agen dari 
aktivitas komuter DKI Jakarta dengan memperhatikan atribut-
atribut  tiap  agennya,  lingkungan,  serta  interaksi  antar  agen. 
Adapun kerangka pikir pembentukan model dapat dilihat pada 
Gambar 3. 

 3 / 9 

 
 
 
Gambar 3.Kerangka Pikir. 

VI. HASIL DAN PEMBAHASAN 

Model berbasis agen memiliki tiga elemen. Elemen pertama 
yaitu  seperangkat agen, atribut  serta  perilaku mereka.  Kedua, 
seperangkat  hubungan  agen  dan  metode  interaksi.  Elemen 
terakhir  merupakan  lingkungan  agen.  [12]  Penelitian  ini 
menggunakan  beberapa  asumsi  untuk  menentukan  atribut-
atribut agen yang dibutuhkan dalam simulasi, yaitu: 

1. 

2. 

3. 

4. 

5. 

6. 

7. 

Pemilihan  moda:  menggunakan  moda  transportasi 
kendaraan bermotor 
Pemilihan  lokasi  rumah:  dilakukan  secara  acak  dan 
menempati atribut tata guna lahan residential 
Pemilihan rute: otomatis pada proses iterasi di dalam 
simulasi  menggunakan  ekstensi  nw  pada  aplikasi 
Netlogo 
Pemilihan  jenis  kegiatan:  persentase  agen  terhadap 
pemilihan jenis kegiatan tertentu disesuaikan dengan 
publikasi Statistik Komuter Jabodetabek 
Pemilihan lokasi kegiatan: dilakukan secara acak dan 
menyesuaikan dengan jenis kegiatan agen 
Pemilihan  waktu  mulai  kegiatan:  berdasarkan 
informasi 
Statistik  Komuter 
Jabodetabek 
Pemilihan  durasi  kegiatan:  menggunakan  asumsi  7 
hingga 8 jam kerja 

publikasi 

dari 

Simulasi  komuter  DKI  Jakarta  dilalukan  melalui  beberapa 
tahap. Beberapa tahapan harus dilakukan secara runtun, karena 
output yang dihasilkan akan dibutuhkan untuk input pada tahap 
selanjutnya. Gambar 4 menunjukkan tahapan persiapan hingga 
pelaksanaan simulasi komuter DKI Jakarta. Tahap awal dalam 
membangun  simulasi  ini  merupakan  pengumpulan  data  yang 
diperlukan  dalam  simulasi,  seperti  data  yang  mendukung 
atribut agen,  dan  data  untuk  pembentukan  lingkungan  tempat 
agen  disimulasikan.  Dari  publikasi  Statistik  Komuter 
Jabodetabek  2019  beberapa 
statistik  yang  diperoleh 
dimanfaatkan  untuk  melengkapi  atribut  agen  dalam  simulasi. 
Karakteristik  agen  seperti  jenis  dan  waktu  kegiatan  akan 
disesuaikan dengan publikasi tersebut.  

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 4. Tahapan persiapan hingga simulasi menggunakan Netlogo 

Lingkungan agen dalam ABM untuk penelitian ini dibangun 
dari shapefile batas administrasi, tata guna lahan (land use), dan 
jaringan  jalan  DKI  Jakarta.  Shapefile  ini  berperan  penting 
untuk  membentuk  lingkungan  agen  agar  simulasi  bisa  dibuat 
serealistis  mungkin  sesuai  keadaan  wilayah  DKI  Jakarta. 
Ketiga shapefile tersebut dapat diperoleh dari OpenStreetMap, 
yang  bisa  diunduh  dengan  menggunakan  HOT  Export  Tool 
melalui  halaman  https://export.hotosm.org/en/v3/.  Penulis 
menggunakan  aplikasi  QGIS  3.16  untuk  melakukan  edit  dan 
cleaning data pada shapefile yang telah diunduh.  

Shapefile  batas  administrasi  mencakup  batas-batas  hingga 
tingkat  kecamatan  di  DKI  Jakarta.  Batas  wilayah  kecamatan 
akan  digunakan  untuk  melacak  pergerakan  agen  pada  saat 
simulasi  berlangsung.  Shapefile  batas  administrasi  bisa  diliat 
pada gambar 5. 

Gambar 5. Shapefile DKI Jakarta dan Batas Wilayah Kecamatan 

Sumber : OpenStreetMap, diolah 

Shapefile  tata  guna  lahan  berupa  polygon  features  yang 
memiliki atribut ruang terbuka, pemukiman, tempat kerja, dan 
sekolah. Tata  guna  lahan  ini  dibutuhkan  untuk  pengalokasian 

 4 / 9 

 
 
 
 
 
 
 
 
 
 
lokasi rumah (pemukiman) dan tempat aktivitas agen (tempat 
kerja dan sekolah). Shapefile tata guna lahan bisa dilihat pada 
Gambar 6. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Sumber: OpenStreetMap, diolah 
Gambar 6. Shapefile tata guna lahan DKI Jakarta 

Sumber : https://github.com/bugyardhytio 
Gambar 9. Shapefile Jaringan Jalan DKI Jakarta 

Shapefile  di  atas  dilakukan  import  ke  dalam  Netlogo 
menggunakan  ekstensi  GIS.  Dilakukan  apply  coverage  untuk 
atribut  batas kecamatan  dan  landuse  ke masing-masing  patch 
terkait.  Kemudian  patch-patch tersebut  didefinisikan  menurut 
jenis 
lahannya.  Potongan  kode  implementasi 
shapefile ke dalam Netlogo bisa dilihat pada gambar 7 

tata  guna 

Gambar 7. Potongan code untuk memanggil shapefile ke dalam 

Netlogo 

Kemudian  dilakukan  import  jaringan  jalan  menggunakan 
ekstensi  nw  pada  Netlogo.  Potongan  kode  untuk  import 
jaringan jalan ke dalam Netlogo dapat dilihat pada gambar 8. 
Jaringan  jalan  DKI  jakarta  dibutuhkan  untuk  tempat  agen 
melakukan perpindahan dari tempat tinggal ke tempat aktivitas 
kemudian  kembali  lagi  ke  rumah.  Jaringan  jalan  bisa  dilihat 
pada gambar 9. 

Kemudian dilakukan coding untuk membangkitkan populasi 
sintetis  sebagai  agen  dalam  simulasi.  User  bisa  mengatur 
berapa  banyak  populasi  yang  akan  dibangkitkan.  Agen  yang 
dibangkitkan akan menempati patch residential dan pemilihan 
lokasi  rumah  dilakukan  secara  acak  di  seluruh  DKI  Jakarta. 
Pada  bagian  ini  juga  dilakukan  coding  untuk  menentukan 
waktu berangkat dan waktu aktivitas agen serta jenis aktivitas 
agen.  Kemudian  setelah  ditentukan  aktivitasnya,  agen  akan 
menentukan lokasi aktivitas secara acak di seluruh DKI Jakarta, 
namun tetap menyesuaikan aktivitas agen tersebut. Pemilihan 
rute dari rumah ke tempat aktivitas memanfaatkan ekstensi nw 
untuk mencari rute terdekat dari rumah ke tempat aktivitas agen.  
Mekanisme  pergerakan agen  diatur  sedemikian  rupa  untuk 
bisa menggambarkan keadaan nyata serealistis mungkin. Agen 
mulai bergerak sesuai waktu berangkat menuju tempat aktivitas. 
Selama perjalan terdapat beberapa aturan mengenai kecepatan 
kendaraan  dan  bagaimana  agen  merespon  agen  lain  yang 
berada  di  jalan.  Pergerakan  kendaraan  di  jalan  mengadaptasi 
dari  car  following  algorithm  yang  berdasarkan  pada  Traffic 
Basic  Model,  yaitu  model  contoh  yang  terdapat  pada  library 
Netlogo  mengenai  lalu  lintas.  Hal  ini  meliputi  kecepatan, 
akselerasi, deselereasi, serta respon agen jika ada kendaraan di 
depannya.  

Ticks  atau  satuan  waktu  dalam  Netlogo  pada  simulasi 
komuter  DKI Jakarta  ini ditentukan  sebesar  1  menit per  tick-
nya.  Kemudian  1  iterasi  simulasi  dilakukan  selama  24  jam 
waktu simulasi. Menurut Statisik Komuter Jabodetabek 2019, 
komuter  berkegiatan  utama  bekerja  sebesar  80,5%,  kemudian 
sebesar  56,8%  komuter  berangkat  dari  rumah  pukul  06.00-
07.59. Maka pada interface netlogo ditentukan persentase jenis 
aktivitas komuter serta waktu aktivitas komuter seperti terlihat 
pada gambar 10. 

Gambar 8. Potongan code untuk jaringan Jalan DKI Jakarta 

Gambar 10. Interface menentukan aktivitas dan waktu aktivitas 

 5 / 9 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
Simulasi  pada  netlogo  dimulai  dengan  meng-click  tombol 
setup  pada  interface  untuk  memanggil  fungsi-fungsi  yang 
membentuk  lingkungan  agen  dan  populasinya.  Kemudian 
tombol go untuk memulai jalannya simulasi. 

Gambar 11. Tombol setup dan go pada interface Netlogo 

Tampilan  peta  DKI  Jakarta,  dilengkapi  dengan  tata  guna 
lahan  dan  jaringan  jalannya  akan  muncul  pada  kotak  yang 
tesedia.  Area  pemukiman  diwakili  oleh  warna  kuning,  area 
perkantoran diwakili warna merah, area sekolah diwakili warna 
merah  muda,  serta  lahan  terbuka  diwakili  warna  hijau.  Agen 
berbentuk  segitiga  kemudian  akan  berjalan  ke  jalan  terdekat 
kemudian  mengendarai  kendaraan  bermotor  untuk  menuju 
tempat aktivitas utamanya 

Gambar 12. Running simulasi komuter DKI Jakarta pada Netlogo 

Dari  hasil running  simulasi  bisa  diperoleh  plot  dari model. 
Plot  di  ekspor  ke dalam  format  csv  untuk kemudia  dilakukan 
analisis.  Ekspor  dilakukan  pada  tick  tertentu,  artinya  pada 
waktu yang diinginkan model simulasi perlu di pause terlebih 
dahulu  untuk  melakukan  ekspor  plot  ke  dalam  csv.  Penulis 
melakukan ekspor pertama pada tick ke-359, atau pada pukul 
5.59. pada waktu tersebut seluruh agen masih berada di rumah 
masing  masing.  Gambar 13 menunjukkan potongan  csv hasil 
ekspor dari model yang dibuat.  

Gambar 13. Potongan plot agen hasil ekspor dari netlogo 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Agen penduduk ditandai dengan atribut “breed people”. Dari 
plot  tersebut  bisa  diperoleh  data-data  seperti  posisi  koordinat 
agen  dan  status  kegiatan  pada  saat plot  diekspor.  Data  diolah 
menggunakan  Microsoft  Excel  untuk  memperoleh  persentase 
aktivitas  utama  komuter.  Kemudian  dihitung  jumlah  agen 
setiap kecamatan. Angka tersebut diagregat hingga tingkat kota 
administrasi dan dibandingkan dengan data dari hasil scrapping 
media sosial twitter.  

Untuk  perbandingan  persentase  aktivitas  komuter  hasil 
simulasi dengan data Statistik Komuter 2019 bisa dilihat pada 
gambar  14.  Berdasarkan  diagram  tersebut,  populasi  yang 
dibangkitkan sudah menyesuaikan perbandingan antar aktivitas 
utama komuter dari data bangkitan dan data BPS. 

100

80

60

40

20

0

Pelajar

Pekerja

Simulasi

Statistik Komuter

Gambar 14. Perbandingan persentase aktivitas utama komuter dari 

populasi yang dibangkitkan dan data BPS 

Selama jangka  waktu  24 jam  agen melakukan aktivitasnya 
masing-masing, baik mulai dari rumah hingga ke tempat kerja, 
dan kemudian kembali lagi ke rumah. Diperoleh hasil berupa 
grafik  pergerakan  penduduk  selama  1 iterasi  simulasi.  Grafik 
pergerakan penuduk bisa dilihat pada Gambar 15. 

Gambar 15. Grafik mobilitas penduduk selama 1 iterasi simulasi 

Dapat  dilihat  pada  grafik  di  atas  semua  agen  pada  pukul 
06.00  mulai  melakukan  mobilitas  menuju  tempat  kegiatan 
utamanya.  Garis  merah  menandakan  jumlah  agen  yang 
melakukan kegiatannya terlihat pada pukul 09.00 semua agen 
sudah mencapai  tempat kegiatan  kemudian  pada  pukul 15.00 
mulai melakukan mobilisasi  untuk menuju kembali ke  rumah 

 6 / 9 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
masing-masing.  Waktu  agen  tiba  di  rumah  sangat  bervariasi 
dan semua dipastikan sudah tiba di rumah pada pukul 21.00 

Pada  tick  ke-540,  atau  pada  pukul  09.00  semua  agen 
dipastikan sudah berada di tempat aktivitasnya masing-masing. 
Plot pada tick tersebut diekspor kemudian dianalisis. Informasi 
yang  bisa  diperoleh  adalah  memperoleh  matriks  asal-tujuan 
dengan  memanfaatkan  pivot  table  pada  Microsoft  Excel. 
Matriks tersebut kemudian disimpan kembali dalam bentuk csv. 
Kemudian  memanfaatkan  modul  pandas  dan  holoviews, 
matriks asal-tujuan ini diolah untuk memperoleh chord diagram 
yang bisa dilihat pada Gambar 16. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Scrapping data dari media sosial Twitter dilakukan sebagai 
pengumpulan  data  untuk  melakukan  tahap  validasi  hasil 
simulasi.  Penulis  menggunakan  modul  twint  pada  bahasa 
pemrograman  python  untuk  melakukan  scraping  data.  Modul 
ini  memungkinkan 
untuk  melakukan 
penggunanya 
pengambilan  data  twitter  tanpa  membutuhkan  Application 
Programming  Interface  (API),  tidak  ada  batasan  mengenai 
tweet yang bisa ditarik, baik dari segi jumlah, jenis, waktu, dan 
sebagainya.  

Data  yang  dibutuhkan  berupa  tweet  bergeoreferensi  yang 
menunjukkan  dimana  tweet  tersebut  dikirimkan.  Namun 
dengan  menggunakan  modul  twint  data  yang  diperoleh  tidak 
langsung berisi atribut georeferensi, sehingga perlu dilakukan 
konfigurasi 
twint  yang 
secara  manual  pada  modul 
menunjukkan  koordinat  pusat  (longitude  dan  latitude)  dari 
kecamatan-kecamatan  di  DKI  Jakarta  serta  radius  dari  titik 
koordinat  pusat  kecamatan  yang  bersangkutan  sehingga 
menunjukkan area berupa lingkaran. Data koordinat kecamatan 
di  DKI  Jakarta  diperoleh  dari  fungsi  centroids  dari  geometry 
tools, serta luas area kecamatan di DKI Jakarta diperoleh dari 
fungsi  add  geometry  attributes  yang  terdapat  pada  QGIS. 
Estimasi 
dengan 
memanfaatkan formula luas lingkaran. Potongan data koordinat, 
area, dan radius tersebut bisa dilihat pada TABEL I. 

kecamatan 

dilakukan 

radius 

area 

TABEL I 
POTONGAN DATA GEOGRAFIS KECAMATAN DI PROVINSI DKI JAKARTA 

Gambar 16. Chord diagram dari matrik tempat tinggal dan tujuan 

dari populasi yang dibangkitkan 

Kemudian  dilakukan  analisis  deskriptif  terhadap  beberapa 
tick  pada  saat  agen  melakukan  mobilitas  untuk  berangkat 
menuju  tempat  aktivitasnya  untuk  setiap  kecamatan.  Data  di 
ekspor  pada 
interval  30  menit  dimulai  dari 
keberangkatan  agen.  Gambar  16  menunjukkan  grafik 
kepadatan penduduk per kecamatan di wilayah Jakarta Timur. 

setiap 

Kecamatan 

Latitude 

Longitude 

Jakarta Pusat 

Cempaka 
Putih 

-6,1808330 

106,8680932 

   Gambir 

-6,1714686 

106,8180484 

Johar Baru 

-6,1821000 

106,8545919 

   Kemayoran 

-6,1626075 

106,8557400 

   Menteng 

-6,1959273 

106,8353824 

Sawah Besar 

-6,1540443 

106,8331460 

Senen 

-6,1843209 

106,8459536 

   Tanah Abang 
Sumber: OpenStreetMap, diolah 

-6,2053622 

106,8094047 

Luas 
Wilayah 
(km2) 

Estimasi 
radius 
(km) 

4,71 

7,49 

2,38 

7,16 

6,44 

5,40 

4,35 

10,05 

1,22 

1,54 

0,87 

1,51 

1,43 

1,31 

1,17 

1,79 

Gambar 15. Grafik jumlah penduduk komuter di Jakarta Timur 

menurut waktu 

Data tweet digunakan untuk dihitung untuk membandingkan 
hasil simulasi terhadap data real, dalam hal ini penulis memilih 
memanfaatkan  big  data  sebagai  pembanding  statistik  hasil 
simulasi  dan  statistik  yang  diproduksi  BPS.  Terkait  tujuan 
penelitian,  data  tweet yang  sudah  diperoleh digunakan dalam 
tahapan validasi dalam rangkaian ABM. Salah satu output dari 
simulasi  yang  dibangun  adalah  statistik  kepadatan  penduduk 
tiap  satuan  waktunya.  Hal  ini  bersesuaian  dengan  data  tweet 
yang  memiliki  atribut  waktu  kapan  tweet  tersebut  dibagikan.  
Data tweet ini bisa dipetakan ke dalam shapefile untuk melihat 
kepadatan dari tweet-tweet yang dibagikan penggunanya dalam 
satu  kecamatan,  kemudian 
tweet  bisa  dikelompokkan 
berdasarkan  jam  untuk  melihat  kepadatan  tweet  pada  waktu 
waktu tertentu.  

 7 / 9 

 
 
 
 
 
 
 
 
  
  
  
  
  
  
  
  
 
Angka yang digunakan sebagai perbandingan adalah angka 
persentase setiap kota administrasi dan bisa dilihat pada Tabel 
2. Sedangkan grafik perbandingan bisa diliat pada gambar 17. 
Berdasarkan  diagram  batang  tersebut  bisa  terlihat  persentase 
tempat 
tinggal  berdasarkan  kota  administrasi  memiliki 
distribusi  yang  sama  antara  data  hasil  simulasi  dengan  data 
tweet. 

TABEL II 
DISTRIBUSI TEMPAT TINGGAL PENDUDUK DKI JAKARTA HASIL SIMULASI DAN 

TWEET 

Jumlah Penduduk 

Persentase Penduduk 

Simulasi 

Tweet 

Simulasi 

Tweet 

30 

18 

15 

31 

5 

99 

28 

30,30303 

18,5 

18,18182 

20 

15,15152 

25,5 

31,31313 

8 

100 

5,050505 

100 

Kota 
Administrasi 

Jakarta Timur 

Jakarta Utara 

Jakarta Barat 
Jakarta 
Selatan 
Jakarta Pusat 

56 

37 

40 

51 

16 

Jumlah 

200 
Sumber: Twitter, diolah 

35
30
25
20
15
10
5
0

Jakarta
Timur

Jakarta
Utara

Jakarta
Barat

Jakarta
Selatan

Jakarta
Pusat

Simulasi

Tweet

Gambar 17. Perbandingan persentase tempat tinggal antara 

populasi yang dibangkitkan dan data tweet 

Penulis  mengakui  masih  terdapat  kelemahan  dari  model 
simulasi  yang  dibangun  ini,  antara  lain  mengasumsikan 
populasi  DKI  Jakarta  yang  selalu  tetap,  padahal  sebenarnya 
tentu banyak mobilitas dari luar DKI Jakarta terutama kota-kota 
penyangganya  seperti  Depok,  Bekasi,  Tangerang.  Kemudian 
skala yang digunakan masih cukup kecil. Karena 1 patch-nya 
mewakili 255,52m x 255,52m. Sehingga mungkin ada beberapa 
fasilitas yang tidak terbaca pada applikasi Netlogo karena area 
yang  tidak  memenuhi  patch.  Menurut  Chattoe  (2003)  Salah 
satu  kesulitan  dalam  membangun  pemodelan  berbasis  agen 
adalah  dari  sisi  pengumpulan  data  yang  relevan  untuk 
membangun dan menguji ABM. Begitu pun data tweet untuk 
tahap  validasi  juga  memiliki  banyak  kelemahan  mengingat 
jenis datanya berupa big data. Ada kemungkinan tidak merata 
nya  pengguna  twitter  ataupun  masalah  dari  perangkat  yang 
digunakan  untuk  merekam  data  geografis.  Namun  big  data 
dianggap  memiliki  potensi  untuk  dimanfaatkan  dalam  studi 
kependudukan. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

VII. 

PENUTUP 

2. 

3. 

Kesimpulan yang diperoleh dari penelitian ini yaitu  
1. 

Simulasi  berhasil  dilakukan  dengan  menyesuaikan 
Statistik  Komuter  Jabodetabek  2019,  yaitu  pada 
sebaran  jenis  aktivitas  dan  waktu  kegiatan  aktivitas 
utama komuter. 
Data  hasil  pemodelan  mobilitas  penduduk  DKI 
Jakarta  ditampilkan  dengan  grafik-grafik.  Diperoleh 
bahwa  jumlah  penduduk  komuter  tertinggi  selama 
mobilitas  agen  menuju  tempat  aktivitas  berlangsung 
terdapat pada kecamatan Jagakarsa. 
Validasi  dengan  memanfaatkan  big  data  berhasil 
dilakukan  pada  data  tempat  tinggal  agen  dengan 
membandingkan data hasil simulasi dengan data dari 
media  sosial  Twitter.  Diperoleh  bahwa  distribusi 
tempat tinggal agen menyerupai distribusi kepadatan 
tweet  pada  masing-masing  kota  administrasi  di  DKI 
Jakarta. 

Saran yang bisa diberikan penulis yaitu 
1.  Untuk  pemerintah,  agar  dapat  meningkatkan  fasilitan 
serta  sarana  dan  prasarana  transportasi  mengingat 
pentingnya  hal  tersebut  bagi  kegiatan  komuter  untuk 
melakukan  mobilitas  ke  tampat  aktivitasnya  masing-
masing.  Hal  ini  untuk  mendorong  efektivitas  dan 
efisiensi setiap moda transportasi terutama di kota besar 
seperti DKI Jakarta. 

keadaan  mobilitas 

2.  Untuk  penelitian  selanjutnya  agar  dapat  memasukkan 
input  data  moda  transportasi  yang  bervariasi  untuk 
penduduk 
menggambarkan 
serealistis  mungkin.  Selain  itu  penelitian  selanjutnya 
dapat menggunakan data validasi yang bersumber dari 
statistic resmi (official statistics) guna memastikan data 
hasil  simulasi  memang  sudah  mewakili  keadaan  atau 
fenomena yang sebenarnya 

DAFTAR PUSTAKA 
[1]  S.  Setyodhono,  Faktor  yang  Mempengaruhi  Pekerja  Komuter  di 
Jakarta: 
Jabodetabek  Menggunakan  Moda  Transportasi  Utama. 
Puslitbang Ketenagakerjaan, Kementerian Ketenagakerjaan RI, 2017, pp. 
1-12. 

[2]  Badan Pusat Statistik. Statistik Komuter Jabodetabek 2019. Jakarta: Badan 

Pusat Statistik, 2019 

[3]  Unit  Pengelola  Statistik.  (2020,  8)  Peningkatan  Jumlah  Kendaraan 
Available: 

Bermotor 
https://statistik.jakarta.go.id/peningkatan-jumlah-kendaraan-bermotor-di-
dki-jakarta/ 

[Online]. 

Jakarta 

Dki 

Di 

[4]  Dinas  Perhubungan  Kabupaten  Banjar.  (2019,  11)  Menhub  Bicara 
Pentingnya  Pengembangan  Sistem  Angkutan  Massal  Di  Kawasan 
Available: 
[Online]. 
Mahasiswa 
Perkotaan, 
https://dishub.banjarkab.go.id/menhub-bicara-pentingnya- 
pengembangan-sistem-angkutan-massal-di-kawasan-perkotaan-
dihadapan-mahasiswa/ 

Dihadapan 

[5]  Republik  Indonesia.  2018.  Peraturan  Presiden  Nomor  55  Tahun  2018 
tentang  Rencana  Induk  Transportasi  Jakarta,  Bogor,  Depok,  Tangerang, 
dan Bekasi tahun 2018 – 2029. 

[6]  M.  B.  A.  Yusuf  dan  I.  Buchori,  Model  Simulasi  Aktivitas  Pergerakan 
Penduduk Berbasis Agen (Studi Kasus: Provinsi DKI Jakarta). Semarang: 
Universitas Diponegoro, 2015, pp. 1–10. 

[7]  E. Bonabeau. (2002, 5) Agent-based modeling: Methods and techniques 
Available: 

[Online]. 

simulating 

for 
https://www.pnas.org/content/99/suppl_3/7280 

[8]  T. Saputri, C. Nugraha, dan K. Amila, Model Simulasi Untuk Pergerakan 
Kendaraan  Pada  Ruang  Dua  Dimensi  Kontinu  Dengan  Pendekatan 

systems 

human 

 8 / 9 

 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pemodelan Berbasis Agen. Bandung: Institut Teknologi Nasional, 2014, 
pp. 1–13. 

[9]  Kagho  G.O.,  Balaca  M.,  and  Axhausen  K.W.  “Agent-Based  Models  in 
Transport  Planning:  Current  State,  Issues,  and  Expectations”  in  The  9th 
International  Workshop  on  Agent-based  Mobility,  Traffic  and 
Transportation  Models,  Methodologies  and  Applications  (ABMTRANS) 
April 6 - 9, 2020. Warsaw, Poland: Elsevier B.V, 2020, pp. 726-732. 
[10] C.  Llorcaa,  N.  Kuehnel,  dan  R.  Moeckel,  Agent-based  integrated  land 
use/transport  models:  a  study  on  scale  factors  and  transport  model 
simulation intervals. Munich: Technical University of Munich, 2020, pp. 
1–6. 

[11] Salgado  M.  dan  Gilbert  N.,  “Agent  Based  Modelling”  in  Handbook  of 
Quantitative  Methods  for  Educational  Research.  Rotterdam:  Sense 
Publishers, 2013, pp. 247-265. 

[12] Macal,  C.  M.,  North,  M.  J.,  “Tutorial  on  agent-based  modelling  and 
simulation” in Journal of Simulation. Chicago, Universit of Chicago, pp. 
151 – 12. 

[13] Chattoe,  E.  The  Role  of  Agent-Based  Modelling  in  Demographic 

Explanation. Oxford: University of Oxford, 2003, pp. 1–13. 

 9 / 9 

 
 
"
221709806,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Perancangan Arsitektur Penyimpanan Data pada 
Infrastruktur Big Data untuk Official Statistik di BPS 

Malkan Ihwani (221709806, 4SI1) 
Dosen Pembimbing: Takdir, SST., M.T. 

dua, 

bagian 

penyimpanan  menjadi 

Ringkasan—    BPS  sudah  mulai  menerapkan  big  data  dalam 
memperoleh  official 
satunya  dengan 
statistiknya.  Salah 
melakukan  crawling/scrapping  data  pada  beberapa  web.  BPS 
sudah memiliki infrastruktur big data pada bagian pengumpulan 
(collection), namun perlu ditemukan arsitektur dengan teknologi 
yang  tepat  pada  bagian  penyimpanannya.  Tim  terkait  di  BPS 
merancang 
yaitu 
penyimpanan  untuk  OLTP  dan  OLAP.  Penelitian  ini  berfokus 
pada  bagian  penyimpanan  untuk  OLTP.  Teknologi  usulan 
peneliti  adalah  Apache  Ignite,  namun  teknologi  tersebut  masih 
implementasinya  untuk  bisa  memenuhi 
harus  dicari  pola 
kebutuhan  pada  bagian  penyimpanan  untuk  OLTP  di  BPS. 
Berdasarkan hal tersebut, maka dilakukanlah penelitian ini yang 
memiliki tujuan utama untuk mendapatkan gambaran mengenai 
bagaimana arsitektur dengan menggunakan Apache Ignite dalam 
memenuhi kebutuhan pada bagian penyimpanan untuk OLTP di 
BPS.  Terdapat  empat  skenario  arsitektur  yang  dirancang  pada 
penelitian  ini.  Keempat  skenario  tersebut  diimplementasikan 
pada  prototype  infrastruktur.  Hasil  evaluasi  keempat  skenario 
menunjukkan  bahwa  arsitektur  dengan  menggunakan  Apache 
Ignite  bisa  memenuhi  kebutuhan  pada  bagian  penyimpanan 
untuk OLTP di BPS. 

Kata  Kunci—  Big  Data,  Penyimpanan  Big  Data,  Arsitektur 

Penyimpanan Data, OLTP. 

I.  LATAR BELAKANG 

BPS sudah mulai menerapkan big data dalam memperoleh 
official  statistiknya.  Salah  satunya  dengan  melakukan 
crawling/scrapping  pada  beberapa  web.  Data  yang 
dikumpulkan dari kegiatan tersebut merupakan big data karena 
memenuhi  tiga  karakteristik  big  data  yaitu  volume  datanya 
besar,  aliran  datanya  cepat,  dan  variasi  datanya  beragam  ada 
yang terstruktur maupun tidak terstruktur [1].  

BPS  sudah  memiliki  infrastruktur  big  data  pada  bagian 
pengumpulan  (collection),  namun  pada  bagian  penyimpanan 
perlu  ditemukan  arsitektur  yang  tepat  untuk  bisa  menyimpan 
big data tersebut. Saat ini, tim terkait di BPS masih melakukan 
penelitian  untuk  mewujudkannya.  Pada  rancangan  kebutuhan 
yang sudah dibuat oleh tim terkait infrastruktur big data di BPS, 
bagian  penyimpanan  dibagi  menjadi  dua,  yaitu  penyimpanan 
untuk  OLTP 
dan 
penyimpanan  untuk  OLAP  (Online  Analytical  Processing). 
Penyimpanan  untuk  OLTP  merupakan  penyimpanan  yang 
difokuskan 
dan 
penyimpanan  untuk  OLAP  merupakan  penyimpanan  yang 
difokuskan untuk menangani proses analisis. 

(Online  Transaction  Processing) 

untuk  menangani 

transaksi, 

proses 

Penelitian  ini  berfokus  pada  bagian  penyimpanan  untuk 
OLTP.  Bagian  ini  merupakan  tempat  pertama  data  hasil 
crawling/scrapping  disimpan.  Setelah  itu,  data  yang  telah 
transformasi  dan 
disimpan  dilakukan  proses  ekstraksi, 

data 

hasil 

crawling/scrapping, 

dimasukkan pada bagian untuk OLAP. Sebagai tempat pertama 
penyimpanan 
bagian 
penyimpanan untuk OLTP harus bisa menyimpan data dengan 
karakteristik  big  data 
ini  harus  bisa 
menyimpan  data  dengan  volume  yang  besar,  memiliki 
kemampuan  untuk  menyimpan  data  (performa  write)  secara 
cepat agar bisa mengimbangi aliran data yang masuk, dan bisa 
menyimpan data yang bervariasi baik terstruktur maupun tidak 
terstruktur. 

tersebut.  Bagian 

Big  data  sulit  untuk  diproses  menggunakan  teknologi 
konvensional  dan  perlu  teknologi  yang  bisa  menyimpan  dan 
memproses  data  dalam  jumlah  terabytes,  petabytes,  atau 
bahkan  exabytes  [2].  Salah  satu  pendekatan  yang  dilakukan 
untuk  bisa  memproses  big  data  yaitu  menggunakan  sistem 
terdistribusi.  Sistem 
terdistribusi  merupakan  kumpulan 
komputer  independen  yang  dilihat  oleh  penggunanya  sebagai 
satu  sistem  yang  saling  berhubungan  [3].  Sekarang  sudah 
banyak  teknologi  untuk  menunjang  penyimpanan  big  data 
seperti  Apache  Hadoop,  MongoDB,  Apache  Ignite,  dll. 
Teknologi  tersebut  umumnya  masih  terbilang  baru  sehingga 
perlu dicari pola implementasi yang tepat agar bisa menunjang 
penyimpanan big data ini. 

Hasil  penelitian  [4]  menunjukkan  bahwa  performa  write 
Hadoop Cluster Multi Node akan mengalami penurunan ketika 
menambahkan  komputer  pada  sistem  terdistribusinya.  Hal 
tersebut 
tidak  sesuai  dengan  kebutuhan  pada  bagian 
penyimpanan  untuk  OLTP,  dimana  bagian  ini  memerlukan 
performa write yang cepat. Berdasarkan dokumentasi yang ada, 
diketahui  bahwa  MongoDB  merupakan  penyimpanan  untuk 
data tidak terstruktur, dimana data disimpan sebagai dokumen-
dokumen dengan menggunakan format JSON [5]. Hal ini juga 
tidak sesuai dengan kebutuhan pada bagian penyimpanan untuk 
OLTP,  dimana  bagian  ini  harus  bisa  menyimpan  data  yang 
bervariasi  baik 
terstruktur. 
terstruktur  maupun 
Berdasarkan  dokumentasi  yang  ada,  Apache  Ignite  memiliki 
performa write yang cepat karena ia menyimpan datanya pada 
RAM  (Random  Access  Memory)  [6]  dan  sebagaimana  kita 
ketahui  bahwa  RAM  memiliki  performa  write  yang  sangat 
cepat. Apache Ignite juga bisa menyimpan data dengan volume 
yang  besar  karena  ia  merupakan  sistem  penyimpanan  yang 
[6],  dimana  untuk  menambah  kapasitas 
terdistribusi 
penyimpanan bisa dilakukan dengan  menambahkan komputer 
pada  sistem 
juga  bisa 
menyimpan data terstruktur maupun data tidak terstruktur [7]. 
Berdasarkan  hal  tersebut,  Apache  Ignite  diperkirakan  bisa 
memenuhi kebutuhan pada bagian penyimpanan untuk OLTP. 
Oleh karena itu, teknologi yang digunakan pada penelitian ini 
adalah Apache Ignite. 

terdistribusinya.  Apache  Ignite 

tidak 

 1 / 8 

 
 
 
 
Berdasarkan  penjelasan  di  atas,  maka  dilakukanlah 
penelitian mengenai perancangan arsitektur penyimpanan data 
pada infrastruktur  big data untuk  official statistik di BPS ini. 
Penelitian ini berfokus pada bagian penyimpanan untuk OLTP 
dan  teknologi  yang  digunakan  pada  penelitian  ini  adalah 
Apache  Ignite.  Penelitian  ini  akan  merancang  skenario-
skenario  arsitektur  menggunakan  teknologi  Apache  Ignite, 
mengimplementasikannya, 
lalu  mengevaluasinya.  Hasil 
penelitian  ini  merupakan  gambaran  mengenai  bagaimana 
arsitektur  menggunakan 
teknologi  Apache  Ignite  dalam 
memenuhi kebutuhan pada bagian penyimpanan untuk OLTP 
di BPS. 

II.  TUJUAN PENELITIAN 

Tujuan  utama  penelitian  ini  yaitu  untuk  memberikan 
teknologi 
gambaran  bagaimana  arsitektur  menggunakan 
Apache  Ignite  dalam  memenuhi  kebutuhan  pada  bagian 
penyimpanan untuk OLTP di BPS. Tujuan tersebut kemudian 
diuraikan menjadi lebih rinci menjadi: 

  Merancang  skenario-skenario  arsitektur  pada  bagian 
penyimpanan  untuk  OLTP  di  BPS  menggunakan 
teknologi Apache Ignite. 

  Mengimplementasikan 

skenario-skenario  arsitektur 

yang telah dirancang. 

  Mengevaluasi  skenario-skenario  arsitektur  yang  telah 

diimplementasikan 

III. PENELITIAN TERKAIT 
TABEL I 
TABEL LITERATUR 

Penulis 

M. Hana et al. [4] 

R. Faisal et al. [8] 

Y. H. Partogi et al. [9] 

Keterangan 
Mencoba untuk membangun infrastruktur big data 
berbasis Hadoop pada Universitas Jambi 
Mencoba  untuk  mengimplementasikan  Hadoop 
Cluster  Multi  Node  untuk  membangun  model 
infrastruktur big data. 
Mencoba 
infrastruktur 
pemrosesan  big  data  menggunakan  Apache  Drill 
pada perusahaan e-commerce SIRCLO 

untuk  merancang 

Terdapat penelitian terkait, seperti yang tertera pada Tabel I. 
Penelitian  [4]  mencoba  untuk  membangun  infrastruktur  big 
data  berbasis  Hadoop  pada  Universitas  Jambi.  Data  yang 
digunakan  dalam  pengujian  bersumber  dari  bank  data  UCI 
Machine  Learning  dan  dalam  pengujiannya  dilakukan 
komputasi  K-Means  pada  data.  Hasil  dari  penelitian  tersebut 
menunjukkan  pembangunan 
infrastruktur  big  data  pada 
Universitas Jambi bisa direkomendasikan untuk menggunakan 
Hadoop.  Selanjutnya  ada  penelitian  [8].  Penelitian  tersebut 
mencoba  untuk  mengimplementasikan  Hadoop  Cluster  Multi 
Node  untuk  membangun  model 
infrastruktur  big  data. 
Penelitian  tersebut  menggunakan  total  lima  buah  node. 
Pengujiannya  menggunakan  tools  TestDFSIO  dengan  data 
berukuran 1 GB, 2 GB, 4 GB, 8 GB, dan 16 GB untuk menguji 
performa write dan read dari infrastruktur yang dibangun. Hasil 
dari penelitian tersebut menunjukkan pada proses write, jumlah 
node  yang  sedikit  menghasilkan  waktu  yang  lebih  cepat. 
Sedangkan  pada  proses  read,  penambahan  jumlah  node 
menghasilkan waktu yang lebih cepat. Lalu ada penelitian [9] 
yang mencoba untuk merancang infrastruktur pemrosesan  big 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

tempat  penyimpanan  secara 

data menggunakan Apache Drill pada perusahaan e-commerce 
SIRCLO.  Pada  penelitian 
tersebut  menggunakan  HDFS 
terdistribusi.  Hasil 
sebagai 
pengujian  pada  bagian  penyimpanan  menunjukkan  proses 
pengiriman  ke  HDFS  dengan  jumlah  node  yang  sedikit  lebih 
cepat.  Sedangkan  pada  proses  query  dari  Drill  ke  HDFS 
penambahan  jumlah  node  menghasilkan  waktu  yang  lebih 
cepat. 

IV. METODE PENELITIAN  

Gambar 1. Alur metode penelitian 

 Langkah-langkah  yang  dilakukan  untuk  menyelesaikan 
penelitian ini, secara keseluruhan bisa dilihat pada Gambar 1. 
Penjelasan  mengenai  langkah  yang  dilakukan  adalah  sebagai 
berikut: 

A. 

Identifikasi  kebutuhan  pada  bagian  penyimpanan  untuk 
OLTP di BPS 

Identifikasi kebutuhan dilakukan dengan  mempelajari 
paparan yang diberikan tim terkait infrastruktur big data 
di BPS dan melakukan diskusi dengan dosen pembimbing. 
Pada  bagian 
juga  sudah  dijelaskan 
beberapa hasil dari identifikasi kebutuhan yang dilakukan, 
lengkap.  Berikut  merupakan 
namun  masih  belum 
kebutuhan pada bagian penyimpanan untuk OLTP di BPS 
versi lengkapnya: 

latar  belakang 

1)  Perlu performa write yang cepat 
2)  Harus  bisa  menyimpan  data  dengan  volume  yang 

besar 

3)  Harus  bisa  menyimpan  data  terstruktur  maupun 

tidak terstruktur 

4)  Harus bisa menjamin ketersediaan data 
5)  Harus bisa menangani komputer dengan spesifikasi 

yang berbeda pada sistem terdistribusinya 

Karakteristik  OLTP  itu  perlu  waktu  respon  dan 
throughput yang cepat untuk bisa memproses banyaknya 
transaksi  yang  dilakukan  secara  bersama-sama  [10]. 
Begitu  juga  dengan  bagian  penyimpanan  untuk  OLTP 
dimana  ia  perlu  performa  write  yang  cepat  untuk  bisa 
mengimbangi cepatnya aliran data yang masuk dari hasil 
crawling/scrapping web yang dilakukan BPS.  

Bagian  penyimpanan  untuk  OLTP  juga  harus  bisa 
menyimpan  data  dengan  volume  yang  besar,  karena 
dalam  sekali  proses  crawling/scrapping  web  yang 
dilakukan  BPS,  besarnya  data  yang  dikumpulkan  bisa 
mencapai  1  terabytes  dengan  jumlah  mencapai  425  juta 
data. 

 2 / 8 

 
 
 
 
Bagian  penyimpanan  untuk  OLTP  juga  harus  bisa 
menyimpan  data  terstruktur  maupun  tidak  terstruktur, 
proses 
karena 
crawling/scrapping  web  yang  dilakukan  BPS  ada  yang 
berupa data terstruktur maupun tidak terstruktur. 

dikumpulkan 

yang 

pada 

data 

Big data sulit untuk diproses menggunakan teknologi 
konvensional [2]. Salah satu pendekatan yang dilakukan 
untuk  bisa  memproses  big  data  yaitu  dengan 
menggunakan  sistem  terdistribusi.  Sistem  terdistribusi 
biasanya akan terus tersedia meski ada beberapa komputer 
yang  mengalami  masalah  [3].  Begitu  juga  pada  bagian 
penyimpanan  untuk  OLTP,  dimana  ia  harus  menjamin 
ketersediaan  data  meski  ada  beberapa  komputer  yang 
mengalami masalah pada sistem terdistribusinya. 

Proses bisnis yang terjadi di BPS saat ini ialah, ketika 
unit  big  data  perlu  infrastruktur,  maka  ia  menyusun 
spesifikasi  dari  infrastruktur  yang  diperlukan  sesuai 
dengan  kebutuhannya  saat  itu,  lalu  menyerahkannya  ke 
subdit JKD  (Jaringan Komunikasi Data). JKD  lalu akan 
membuatkan infrastruktur yang berisikan beberapa virtual 
machine  sesuai  dengan  spesifikasi  yang  diajukan. 
Masalahnya ketika di kemudian hari kebutuhan dari unit 
big  data  tersebut  meningkat  dan  infrastruktur  yang  ada 
tidak  mampu  menangani  kebutuhan  tersebut.  Ia  perlu 
mengajukan  kembali  infrastruktur  dengan  spesifikasi 
sesuai  dengan  kebutuhan  yang  baru.  Supaya  tidak  perlu 
membuat infrastruktur dari awal, maka sebaiknya hanya 
perlu  menambahkan  virtual  machine  dengan  spesifikasi 
sesuai  dengan  selisih  kebutuhannya.  Oleh  karena  itu, 
untuk mendukung hal tersebut, maka bagian penyimpanan 
ini  harus  menangani  komputer  dengan  spesifikasi  yang 
berbeda pada sistem terdistribusinya. 

B.  Perancangan 

skenario-skenario 

arsitektur 

dengan 

menggunakan teknologi Apache Ignite 

Perancangan  skenario-skenario  arsitektur  dilakukan 
dengan  menentukan  fitur-fitur  Apache  Ignite  yang 
diperkirakan  bisa  memenuhi  kebutuhan  pada  bagian 
penyimpanan  untuk  OLTP  di  BPS.  Lalu  dirancanglah 
skenario-skenario  arsitektur  dengan  menggunakan  fitur-
fitur yang sudah ditentukan. Keterangan dari istilah-istilah 
teknologi Apache Ignite yang digunakan pada penelitian 
ini, bisa dilihat pada Tabel II. 

TABEL II 
TABEL KETERANGAN ISTILAH-ISTILAH APACHE IGNITE YANG DIGUNAKAN 
PADA PENELITIAN INI 

Istilah 

Node server 

Cluster 

Node client 

Cache 

Table 

Keterangan 
Unit komputer yang bertugas untuk melakukan komputasi 
dan penyimpanan data 
Sekelompok node server yang saling terhubung yang 
menyediakan sumber daya bersama seperti RAM, disk, 
dan CPU 
Gerbang antara pengguna dengan cluster 
Kumpulan dari entri data tidak terstruktur dalam bentuk 
key-value 
Kumpulan dari entri data terstruktur dalam bentuk SQL 
table 

Penentuan fitur-fitur yang diperkirakan bisa memenuhi 
kebutuhan  pada  bagian  penyimpanan  untuk  OLTP 
literatur  pada 
dilakukan  dengan  melakukan  studi 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

dokumentasi Apache Ignite. Hasil studi literatur mengenai 
fitur-fitur  yang  diperkirakan  bisa  memenuhi  kebutuhan 
pada bagian penyimpanan untuk OLTP, bisa dilihat pada 
Tabel III. 

TABEL III 
TABEL HUBUNGAN ANTARA FITUR TERPILIH DENGAN KEBUTUHAN PADA 
BAGIAN PENYIMPANAN UNTUK OLTP DI BPS 

Fitur terpilih 

Keterangan 

Efek terhadap kebutuhan 

Native 
persistence 

Cache/table 
dengan mode 
partitioned dan 
jumlah backup n 

Menyimpan seluruh 
data pada disk dan 
memuat sebanyak 
mungkin data pada 
RAM untuk 
meningkatkan 
performa 
Keseluruhan data 
dipartisi dan 
didistribusikan hampir 
secara merata pada 
setiap node server, 
partisi data yang 
disimpan suatu node 
server dibuat 
salinannya sejumlah n 
dan disimpan pada 
node server yang 
berbeda 

Node filter 

Mengatur 
pendistribusian data 
dari suatu cache/table 

-  Meningkatkan 

ketersediaan data 

-  Meningkatkan 
kapasitas 
penyimpanan data 

- 

-  Meningkatkan 
performa write 
Ketersediaan data 
terjamin selama tidak 
melebihi jumlah 
backup 
-  Meningkatkan 
kapasitas 
penyimpanan data 

- 

Bisa melakukan 
pendistribusian sesuai 
kebutuhan ketika 
cluster yang 
digunakan terdiri dari 
node server dengan 
spesifikasi yang 
beragam 

Skenario-skenario  yang  dirancang  pada  penelitian  ini 
mencakup  skenario  awal  sebagai  pembanding,  skenario 
saat ada penambahan node server dengan spesifikasi sama, 
skenario  saat  ada  penambahan  node  server  dengan 
spesifikasi  yang  lebih  rendah,  dimana  skenario  ini 
bertujuan  untuk  mendapatkan  gambaran  bagaimana 
arsitektur  dengan  menggunakan  Apache  Ignite  dalam 
menangani  komputer  dengan  spesifikasi  yang  berbeda 
pada 
saat 
terdistribusinya,  dan 
menggunakan node filter untuk mengatur pendistribusian 
data. 

skenario 

sistem 

C. 

Implementasi  skenario-skenario  arsitektur  yang  telah 
dirancang 

Skenario-skenario  tersebut  diimplementasikan  pada 
prototype 
infrastruktur  dengan  menggunakan  satu 
komputer  fisik  yang  akan  dipecah  menjadi  beberapa 
virtual machine. 

D.  Evaluasi 

skenario-skenario 

arsitektur  yang 

telah 

diimplementasikan 

Evaluasi  skenario-skenario  arsitektur  bertujuan  untuk 
mendapatkan  gambaran  bagaimana  arsitektur  dengan 
menggunakan Apache Ignite dalam memenuhi kebutuhan 
pada  bagian  penyimpanan  untuk  OLTP  di BPS.  Berikut 
merupakan  evaluasi  yang  akan  dilakukan  beserta 
penjelasannya: 

  Evaluasi performa write 

 3 / 8 

 
 
Evaluasi 

ini  bertujuan  untuk  mendapatkan 
gambaran  mengenai  bagaimana  arsitektur  dengan 
menggunakan  Apache  Ignite  dalam  memenuhi 
kebutuhan performa write pada bagian penyimpanan 
untuk  OLTP  di  BPS.  Evaluasi  ini  juga  bertujuan 
untuk  mendapatkan  gambaran  mengenai  apakah 
arsitektur dengan menggunakan  Apache Ignite bisa 
menyimpan data terstruktur dan tidak terstruktur. 

throughput 

transaction 

Kecepatan  suatu  sistem  database  dihitung 
menggunakan 
yang 
dinyatakan dalam banyaknya transaksi per detik [11]. 
Pada penelitian ini, performa write dihitung dengan 
menggunakan  besarnya  volume  data  yang  ditulis 
pada  database  dalam  periode  waktu  tertentu  yang 
dinyatakan dalam megabytes per detik. Evaluasi ini 
dilakukan  dengan  mencatat  waktu  eksekusi  dari 
suatu proses entri data, setelah itu dihitung performa 
write-nya  dengan  membagi  besarnya  volume  data 
yang dientri dengan waktu eksekusi.  

Proses entri data dilakukan dengan memasukkan 
masing-masing 2.000.000 data dummy dengan besar 
volume sekitar 2.8 gigabytes untuk data terstruktur 
maupun  tidak  terstruktur  pada  setiap  skenario 
arsitektur.  Proses  entri  juga  dilakukan  dengan 
menggunakan  beberapa  cara/fungsi  yang  berbeda. 
Cara pertama yaitu memasukkan data secara satu per 
satu.  Cara  kedua  yaitu  memasukkan  sejumlah  data 
secara  sekaligus  (batch).  Cara  ketiga  yaitu  dengan 
mengalirkan  data  (stream).  Data  dientri  dengan 
menggunakan API dan fungsi sebagai berikut: 

  Key-value API (untuk data tidak terstruktur) 

menggunakan thick client 
o  Put – Memasukkan data satu per satu 
o  PutAll(1000) – Memasukkan sekaligus 

1000 data 

o  DataStream – Mengalirkan data 

  SQL API (untuk data terstruktur) 

menggunakan JDBC-thin 
o 
o 

Insert – Memasukkan data satu per satu 
InsertBatch(1000) – Memasukkan 
sekaligus 1000 data 
InsertStream – Mengalirkan data 

o 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

setiap  node  server  yang  ada.  Informasi  mengenai 
jumlah data yang disimpan pada setiap node server 
diperoleh menggunakan Apache Ignite Visor CMD. 

  Evaluasi ketersediaan data 

evaluasi 

Evaluasi 

ini  bertujuan  untuk  mendapatkan 
gambaran  mengenai  bagaimana  arsitektur  dengan 
menggunakan  Apache  Ignite  dalam  menjamin 
ketersediaan  datanya.  Sebelum 
ini 
dilakukan,  terlebih  dahulu  dimasukkan  2.000.000 
data tidak terstruktur pada setiap skenario arsitektur. 
Evaluasi  ini  dilakukan  dengan  melakukan  simulasi 
ada  satu  node  server  yang  tidak  aktif  pada  suatu 
cluster.  Setelah 
pengecekan 
ketersediaan 
cara  mengambil 
dengan 
keseluruhan data. Pengecekan dikatakan sukses bila 
tidak ada data yang hilang ataupun terjadi error. 

dilakukan 

data 

itu, 

  Evaluasi keberlanjutan proses entri data 

Evaluasi 

ini  bertujuan  untuk  mendapatkan 
gambaran  mengenai  bagaimana  perilaku  arsitektur 
dengan menggunakan Apache Ignite ketika ada node 
server  yang  penuh  terlebih  dahulu  saat  dilakukan 
proses  entri  data,  apakah  proses  entri  data  bisa 
berlanjut  atau  tidak.  Evaluasi  ini  hanya  dilakukan 
yang  melakukan 
pada 
penambahan  node  server  dengan  spesifikasi  yang 
lebih rendah.  

arsitektur 

skenario 

Evaluasi  ini  dilakukan  dengan  cara  membuat 
penyimpanan  pada  node  server  yang  memiliki 
rendah  hampir  penuh.  Lalu 
spesifikasi  yang 
dilakukan  proses  entri  data 
terstruktur 
tidak 
sebanyak 1.000.000 data. Setelah itu dilihat apakah 
proses  entri  data  masih  bisa  berlanjut  atau  tidak 
ketika ada node server yang penuh terlebih dahulu. 
Dilihat juga  apakah data  yang disimpan pada  node 
server yang penuh bisa diakses kembali atau tidak. 

Untuk bisa mendukung evaluasi yang akan dilakukan, 
dibuatlah  program  sederhana  yang  akan  berjalan  pada 
node client dengan menggunakan API yang sesuai dengan 
evaluasi yang akan dilakukan. Source code dari program 
sederhana 
pada 
https://git.stis.ac.id/televisi/ignite-client. 

dilihat 

dibuat 

yang 

bisa 

  Evaluasi persebaran data 

V.  KERANGKA PIKIR 

Evaluasi 

arsitektur 

ini  bertujuan  untuk  mendapatkan 
dengan 
bagaimana 
gambaran 
menggunakan  Apache  Ignite  bisa  menyimpan  data 
dengan  volume  yang  besar  dan  bagaimana 
pendistribusian  datanya.  Evaluasi  ini  sendiri  hanya 
menghasilkan  jumlah  data  yang  disimpan  pada 
setiap  node  server  pada  suatu  skenario  arsitektur. 
Tetapi ketika dilakukan analisis perbandingan hasil 
evaluasi ini dengan skenario arsitektur yang lainnya, 
bisa  didapatkan  gambaran  mengenai  bagaimana 
terhadap 
pengaruh  penambahan  node 
penambahan  kapasitas  penyimpanan  datanya. 
Penambahan kapasitas penyimpanan data diketahui 
dari  penurunan  jumlah  data  yang  disimpan  pada 

server 

Gambar 2. Kerangka Pikir 

Gambar 2 menjelaskan mengenai bagaimana alur pemikiran 
peneliti  dalam  melaksanakan  penelitiannya.  Berawal  dari 
masalah  penelitian  dimana  belum  ditemukan  arsitektur  dan 

 4 / 8 

 
 
 
teknologi  yang  tepat  pada  infrastruktur  big  data  bagian 
penyimpanan  untuk  OLTP  di  BPS,  dan  teknologi  usulan 
peneliti  yaitu  Apache  Ignite  juga  masih  perlu  dicari  pola 
implementasinya untuk bisa memenuhi kebutuhan pada bagian 
penyimpanan  untuk  OLTP  di  BPS.  Dari  masalah  penelitian 
tersebut,  ditetapkanlah 
tujuan  penelitian.  Tujuan  utama 
ini  yaitu  memberikan  gambaran  mengenai 
penelitian 
bagaimana  arsitektur dengan  menggunakan  teknologi  Apache 
Ignite dalam memenuhi kebutuhan pada bagian penyimpanan 
untuk  OLTP  di  BPS.  Tujuan  utama  tersebut  kemudian 
diuraikan  menjadi  tiga  tujuan  yang  lebih  terperinci.  Tujuan 
pertama  yaitu  merancang  skenario-skenario  arsitektur  pada 
bagian  penyimpanan  untuk  OLTP  di  BPS  menggunakan 
teknologi 
yaitu 
mengimplementasikan skenario-skenario arsitektur yang sudah 
dirancang.  Tujuan  ketiga  yaitu  melakukan  evaluasi  terhadap 
skenario-skenario arsitektur yang sudah diimplementasikan.  

Apache 

Tujuan 

Ignite. 

kedua 

Langkah  yang  dilakukan  peneliti  untuk  mencapai  tujuan 
pertama adalah dengan melakukan identifikasi kebutuhan pada 
bagian  penyimpanan  untuk  OLTP  di  BPS,  lalu  merancang 
skenario-skenario  arsitektur  menggunakan  teknologi  Apache 
Ignite  yang  diperkirakan  bisa  memenuhi  kebutuhan  tersebut. 
Setelah  mendapatkan  skenario-skenario  arsitektur,  langkah 
yang  dilakukan  peneliti  untuk  mencapai  tujuan  kedua  adalah 
dengan  mengimplementasikan  skenario-skenario  arsitektur 
tersebut.  Langkah  terakhir  yang  dilakukan  peneliti  untuk 
mencapai tujuan ketiga adalah melakukan evaluasi pada semua 
skenario yang telah diimplementasikan. Hasil evaluasi tersebut 
merupakan hasil dari tujuan utama penelitian ini yaitu berupa 
gambaran  mengenai 
dengan 
menggunakan  teknologi  Apache  Ignite  dalam  memenuhi 
kebutuhan pada bagian penyimpanan untuk OLTP di BPS. 

bagaimana 

arsitektur 

VI. HASIL DAN PEMBAHASAN 

A.  Skenario-skenario Arsitektur yang Dirancang 

1.  Skenario satu 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 4. Skenario arsitektur 2 

Gambar  4  merupakan  gambaran  dari  skenario 
arsitektur dua. Skenario ini  menambahkan satu  node 
server  dengan  spesifikasi  yang  sama  pada  skenario 
satu.  Jumlah  node  client  serta  fitur  yang  digunakan 
masih sama seperti pada skenario satu. 

3.  Skenario tiga 

Gambar 5. Skenario arsitektur 3 

Gambar  5  merupakan  gambaran  dari  skenario 
arsitektur tiga. Skenario ini menambahkan satu  node 
server  dengan  spesifikasi  yang  lebih  rendah  pada 
skenario  satu.  Jumlah  node  client  dan  fitur  yang 
digunakan masih sama seperti pada skenario satu. 

4.  Skenario empat 

Gambar 3. Skenario arsitektur 1 

Gambar  3  merupakan  gambaran  dari  skenario 
arsitektur satu. Skenario ini merupakan skenario awal 
yang nantinya digunakan sebagai pembanding dengan 
skenario  yang  lainnya.  Skenario  ini  menggunakan 
cluster  yang  terdiri  dari  dua  node  server  dengan 
spesifikasi  yang  sama,  dan  menggunakan  satu  node 
client.  Fitur  yang  digunakan  pada  skenario  ini  yaitu 
native  persistence,  dan  cache/table  dengan  mode 
partitioned dengan jumlah backup satu. 

2.  Skenario dua 

Gambar 6. Skenario arsitektur 4 

Gambar  6  merupakan  gambaran  dari  skenario 
arsitektur  empat.  Skenario  ini  menambahkan  fitur 
node filter pada skenario tiga. Pada skenario ini, dua 
node  server  dengan  spesifikasi  yang  sama  diberi 
atribut  “bigCache”.  Pada  skenario  ini  terdapat  dua 
cache  yaitu  bigCache  dan  smallCache.  Fitur  node 
filter  mengarahkan  bigCache  hanya  disimpan  node 
server yang memiliki atribut “bigCache”, dan untuk 
smallCache disimpan secara default pada setiap node 
server yang ada. 

 5 / 8 

 
 
 
 
 
 
B.  Implementasi Skenario-skenario Arsitektur 

Implementasi 

skenario  dilakukan  pada  prototype 
infrastruktur  dengan  menggunakan  beberapa  virtual 
machine  sebagai  node  server.  Pada  penelitian  ini,  semua 
virtual machine dibuat pada satu komputer host dan client 
akan berjalan pada komputer host, dimana komputer yang 
lab-241  Politeknik 
digunakan  merupakan  komputer 
Statistika  STIS  yang  memiliki  spesifikasi  seperti  yang 
tertera pada Tabel IV. 

TABEL IV 
TABEL SPESIFIKASI KOMPUTER HOST 

CPU 
RAM 
Disk 

OS 
Connection 

Intel Core i7-10700 @ 2.90Ghz (16 Core) 
16 Gb Single 
C: -> SSD 500Gb 
D: -> HDD 1Tb 
Windows 10 Enterprise 64-bit 
Ethernet (stis.local) 

Berdasarkan  pertimbangan  spesifikasi  pada  Tabel  IV, 
node server yang identik memiliki spesifikasi seperti yang 
tertera pada Tabel V. Node server dengan spesifikasi yang 
lebih  rendah  memiliki  rincian  spesifikasi  seperti  yang 
tertera pada Tabel VI. 

TABEL V 
TABEL SPESIFIKASI NODE SERVER 1,2, dan 3 

CPU 
RAM 
Java Heap 
Disk 
OS 
Connection 
Ignite Ver. 
Java Ver. 

CPU 
RAM 
Java Heap 
Disk 
OS 
Connection 
Ignite Ver. 
Java Ver. 

2 Core @ 2.90Ghz 
3 GB 
1.5 Gb 
SSD -> 20 GB 
Ubuntu Server 64-bit 
Bridged Connection 
2.9.1 
Open JDK 1.8 

TABEL VI 
TABEL SPESIFIKASI NODE SERVER 3 LOW 

1 Core @ 2.90Ghz 
2 GB 
1 GB 
SSD -> 10 GB 
Ubuntu Server 64-bit 
Bridged Connection 
2.9.1 
Open JDK 1.8 

C.  Evaluasi Skenario-skenario Arsitektur 

  Hasil evaluasi performa write 

)
s
/
B
M

(
e
t
i
r

W
a
m
r
o
f
r
e
P

41.26

39.59

34.88

32.55

65.1

51.4

41.85
40.13

Put

PutAll(1000)

DataStream

Skenario 1

Skenario 2

Skenario 3

Skenario 4

Gambar  7.  Grafik  performa  write  data  menggunakan  key-value  API  untuk 
skenario 1,2,3 dan 4 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 7 menunjukkan hasil evaluasi performa  write key-
value API (untuk data tidak terstruktur), dimana fungsi dengan 
performa write tertinggi adalah DataStream, sedangkan fungsi 
dengan performa  write terendah adalah  Put. Pada skenario 1, 
performa  write  DataStream  sebesar  41.85  MB/s.  Dengan 
menambahkan satu node server, baik dengan spesifikasi yang 
sama maupun yang lebih rendah, terjadi peningkatan performa 
write,  dimana  peningkatan  tersebut  terlihat  cukup  jelas  pada 
fungsi DataStream. Pada skenario 2, dimana ia menambahkan 
satu node server dengan spesifikasi yang sama, performa write 
fungsi  DataStream  meningkat  sebesar  23.25  MB/s  menjadi 
65.1 MB/s. Pada skenario 3, dimana ia menambahkan satu node 
server  dengan  spesifikasi  yang  lebih  rendah,  performa  write 
fungsi  DataStream  juga  meningkat,  meskipun  tidak  sebesar 
skenario  2,  peningkatan  yang  terjadi  sebesar  9.55  MB/s 
menjadi  51.4  MB/s.  Pada  skenario  4,  dimana  ia  menerapkan 
node filter sehingga data hanya disimpan pada dua node server 
dengan  spesifikasi  yang  besar  saja,  performa  write  secara 
keseluruhan  fungsi tidak jauh berbeda dengan skenario 1. 

tidak 

terstruktur  pada  skenario  3.  Hal 

Gambar 7 juga menunjukkan hasil evaluasi performa  write 
untuk  data 
ini 
menunjukkan  bahwa,  Apache  Ignite  bisa  berjalan  pada 
komputer  dengan  spesifikasi  yang  berbeda-beda  dan  bisa 
terhubung  pada  cluster,  serta  menunjukkan  bahwa  data  tidak 
terstruktur berhasil dientri pada cluster tersebut. 

)
s
/
B
M

(

e
t
i
r

W
a
m
r
o
f
r
e
P

48.83

48.03
46.5
39.59

Insert

InsertBatch(1000)

InsertStream

Skenario 1

Skenario 2

Skenario 3

Skenario 4

Gambar 8. Grafik performa write data menggunakan SQL API untuk skenario 
1,2,3 dan 4 

Gambar 8 menunjukkan hasil evaluasi performa write SQL 
API  (untuk  data  terstruktur),  dimana  fungsi  dengan  performa 
write tertinggi adalah InsertStream, sedangkan fungsi dengan 
performa  write  terendah  adalah  Insert.  Pada  skenario  1, 
performa  write  InsertStream  sebesar  39.59  MB/s.  Dengan 
menambahkan satu node server, baik dengan spesifikasi yang 
sama maupun yang lebih rendah, terjadi peningkatan performa 
write  pada  fungsi  InsertStream.  Pada  skenario  2,  dengan 
menambahkan satu node server dengan spesifikasi yang sama, 
performa  write  fungsi  InsertStream  meningkat  sebesar  8.44 
MB/s  menjadi  48.03  MB/s.  Pada  skenario  3,  dengan 
menambahkan satu node server dengan spesifikasi yang lebih 
rendah,  performa  write  fungsi  InsertStream  juga  meningkat, 
meskipun  tidak  sebesar  skenario  2,  peningkatan  yang  terjadi 
sebesar 6.91 MB/s menjadi 46.5 MB/s. Pada skenario 4, dengan 
menerapkan node filter sehingga data hanya disimpan pada dua 
node  server  berspesifikasi  besar  saja,  performa  write  fungsi 

 6 / 8 

 
 
 
 
 
 
 
 
 
InsertStream  mengalami  peningkatan  paling  tinggi  sebesar 
9.24 MB/s menjadi 48.83 MB/s. 

Gambar 8 juga menunjukkan hasil evaluasi performa write 
untuk  data  terstruktur  pada  skenario  3.  Hal  ini  menunjukkan 
bahwa,  Apache  Ignite  bisa  berjalan  pada  komputer  dengan 
spesifikasi yang berbeda-beda dan bisa terhubung pada cluster, 
serta menunjukkan bahwa data terstruktur berhasil dientri pada 
cluster tersebut. 

  Hasil evaluasi ketersediaan data 

TABEL VII 
TABEL HASIL EVALUASI KETERSEDIAAN DATA 

Skenario 

Hasil 

Skenario 1 
Skenario 2 
Skenario 3 
Skenario 4 

Sukses 
Sukses 
Sukses 
Sukses 

Semua skenario menggunakan jumlah backup satu dan Tabel 
VII menunjukkan bahwa data yang disimpan pada skenario satu, 
dua, tiga maupun empat, masih tersedia secara utuh meskipun 
ada  satu  node  server  yang  tidak  aktif.  Hal  ini  menunjukkan 
bahwa Apache Ignite bisa menjamin ketersediaan data selama 
jumlah  node  server  yang  tidak  aktif  tidak  melebihi  jumlah 
backup. 

  Hasil evaluasi skalabilitas 

2000000

1000000

0

1013682

986318

705073

654296

1013682

640634

675790

654293

669914

986318

Skenario 1

Skenario 2

Skenario 3

Skenario 4

Node 1

Node 2

Node 3

Gambar 9. Grafik perbandingan persebaran data pada cluster antar semua 
skenario 

Gambar 9 menunjukkan bahwa dengan menambahkan node 
server  pada  cluster  akan  mengurangi  jumlah  data  yang 
disimpan  pada  setiap  node  server  sehingga  kapasitas  untuk 
menyimpan  data  pun  meningkat.  Seperti  pada  perbandingan 
antara skenario 2 dan skenario 1, awalnya  setiap  node server 
menyimpan hampir 1.000.000 data, setelah ditambahkan satu 
node  server,  itu  berkurang  menjadi  sekitar  650.000  sampai 
700.000  data  sehingga  kapasitas  penyimpanan  data  untuk 
skenario 2 akan meningkat. 

Gambar  9 

juga  menunjukkan  bahwa  data 

akan 
didistribusikan secara  hampir  merata  pada setiap  node server 
yang ada di suatu cluster. Hal ini berlaku juga pada cluster yang 
memiliki node server dengan spesifikasi yang lebih rendah dari 
yang  lainnya.  Seperti  pada  skenario  3,  dimana  node  server 
dengan  spesifikasi  yang  lebih  rendah  (node  3)  menyimpan 
sekitar 650.000 data, hampir sama dengan yang disimpan node 
server dengan spesifikasi yang lebih tinggi. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar  9  juga  menunjukkan  dengan  menggunakan  node 
filter,  cache/table  bisa  dikontrol  untuk  disimpan  hanya  pada 
node  server  tertentu.  Seperti  pada  skenario  4,  dimana  data 
hanya  disimpan  pada  node  server  dengan  spesifikasi  yang 
tinggi (node 1 dan 2). 

  Hasil evaluasi keberlanjutan proses entri data 

Gambar 10. Informasi penyimpanan node server dengan spesifikasi yang 
lebih rendah setelah dibuat hampir penuh. 

Gambar  10  menunjukkan  informasi  penyimpanan  node 
server  dengan  spesifikasi  yang  lebih  rendah  setelah  dibuat 
hampir penuh, dimana penyimpanan yang tersisa hanya sekitar 
14 MB. 

Gambar 11. Output proses entri data pada skenario 3 ketika node server 
dengan spesifikasi yang lebih rendah penuh duluan 

Gambar 12. Output tes ketersediaan data saat node server yang penuh duluan 
diaktifkan kembali 

Gambar 11 dan 12 menunjukkan ketika ada node server yang 
penuh terlebih dahulu pada suatu cluster, proses entri data tetap 
bisa  dilakukan.  Dan  ketika  suatu  node  server  penuh,  ia  akan 
menjadi nonaktif. Tetapi, ia bisa diaktifkan kembali sehingga 
keseluruhan  data  yang  disimpan  pada  cluster  tersebut  masih 
utuh. 

VII. 

PENUTUP 

hasil 

didapatkan 

Berdasarkan 

beberapa 
penelitian, 
kesimpulan untuk menjawab tujuan penelitian yang dijabarkan 
sebagai berikut: 
1.  Terdapat empat skenario arsitektur  yang dirancang pada 
penelitian  ini  untuk  memberikan  gambaran  bagaimana 
arsitektur dengan menggunakan teknologi Apache Ignite 
dalam  memenuhi  kebutuhan  pada  bagian  penyimpanan 
untuk OLTP di BPS. 

 7 / 8 

 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

[9] 

[10] 

[11] 

Data,” vol. 7, no. 2, 2018. 
Y. H. Partogi, A. Bhawiyuga, and A. Basuki, “Rancang Bangun 
Infrastruktur Pemrosesan Big Data Menggunakan Apache Drill 
( Studi Kasus : SIRCLO ),” J. Pengemb. Teknol. Inf. dan Ilmu 
Komput. Univ. Brawijaya, vol. 2, no. 3, pp. 951–957, 2018. 
Oracle, “What is Online Transaction Processing (OLTP) | Oracle.” 
https://www.oracle.com/database/what-is-oltp/ (accessed Jul. 02, 
2021). 
Oracle, “Transaction throughput.” 
https://docs.oracle.com/cd/E17276_01/html/programmer_reference/
transapp_throughput.html (accessed Jul. 02, 2021). 

2.  Skenario-skenario  tersebut  berhasil  diimplementasikan 
pada  prototype  infrastruktur  dengan  menggunakan  satu 
komputer  fisik  yang  dipecah  menjadi  beberapa  virtual 
machine. 

3.  Hasil  evaluasi  skenario-skenario  tersebut  menunjukkan 
bahwa arsitektur dengan menggunakan teknologi Apache 
Ignite  bisa  memenuhi  kebutuhan  pada  bagian 
penyimpanan  untuk  OLTP  di  BPS.  Berikut  merupakan 
gambaran  mengenai  bagaimana  arsitektur  dengan 
menggunakan Apache Ignite dalam memenuhi kebutuhan 
pada bagian penyimpanan untuk OLTP di BPS: 
  Performa  write 
ditingkatkan 

dengan 
menambahkan  node  server  pada  cluster,  sehingga 
untuk mendapatkan performa write yang diinginkan 
bisa  dilakukan  dengan  menambahkan/mengurangi 
jumlah node server pada suatu cluster. 

bisa 

  Kapasitas  penyimpanan  data  bisa  ditingkatkan 
dengan  menambah  node  server  pada  cluster, 
sehingga 
kapasitas 
penyimpanan  data  yang  diinginkan  bisa  dilakukan 
dengan  menambahkan/mengurangi  jumlah  node 
server pada suatu cluster. 

mendapatkan 

untuk 

  Data  terstruktur  maupun  tidak  terstruktur  bisa 

disimpan pada Apache Ignite. 

  Ketersediaan  data  terjamin  selama  jumlah  node 
server  yang  bermasalah  pada  suatu  cluster  tidak 
melebihi jumlah backup. Jaminan ketersediaan data 
bisa  diatur  dengan  menambah/mengurangi  jumlah 
backup. 

  Apache  Ignite  bisa  menangani  node  server  dengan 
spesifikasi  yang  berbeda-beda  pada 
sistem 
terdistribusinya. Pendistribusian datanya bisa diatur 
dengan  menggunakan  node  filter.  Proses  entri  data 
tetap  bisa  berlanjut  ketika  ada  node  server  yang 
penuh terlebih dahulu. Node server yang penuh akan 
menjadi  tidak  aktif,  tetapi  bisa  diaktifkan  kembali 
dan  keseluruhan  data  yang  telah  disimpan  tetap 
tersedia secara utuh. 

DAFTAR PUSTAKA 

IBM, “Big Data Analytics | IBM.” 
https://www.ibm.com/analytics/hadoop/big-data-analytics (accessed 
Nov. 06, 2020). 
K. S. Raste, “Big Data Analytics-Hadoop Performance Analysis,” 
2014. 
A. S. Tanenbaum and M. van Steen, Distributed Systems: 
Principles and Paradigms, Second Edi. Amsterdam: Prentice Hall, 
2006. 
M. Hana, J. Marzal, and Mauladi, “Pembangunan Infrasturktur Big 
Data Berbasis Hadoop Pada Universitas Jambi,” J. Sains dan Sist. 
Inf., vol. 1, no. 1, pp. 13–19, 2018. 
MongoDB Inc., “Structure your Data for MongoDB.” 
https://docs.mongodb.com/guides/server/introduction/ (accessed 
Jun. 30, 2021). 
Apache Ignite, “In-Memory Database - Apache Ignite.” 
https://ignite.apache.org/use-cases/in-memory-database.html 
(accessed Mar. 26, 2021). 
Apache Ignite, “What is Apache Ignite.” 
https://ignite.apache.org/whatisignite.html (accessed Mar. 27, 
2021). 
R. Faisal, D. Nurmalasari, and P. Sari, “Implementasi Hadoop 
Cluster Multi Node Untuk Membangun Model Infrastruktur Big 

[1] 

[2] 

[3] 

[4] 

[5] 

[6] 

[7] 

[8] 

 8 / 8 

 
 
 
"
221709801,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pengembangan Package R Seasonal Generalized 
Space Time Autoregressive dan Impelementasinya 
pada Peramalan Produksi Padi di Sumatera Selatan 

Ringkasan—space 

M. Yoga Satria Utama (221709801, 4SD2) 
Dosen Pembimbing: Dr. Ernawati Pasaribu 
suatu  model  yang 
time  merupakan 
menggabungkan unsur dependensi waktu dan lokasi pada deret 
waktu.  Dewasa  ini  pemodelan  space  time  telah  dapat  dilakukan 
dengan  aplikasi  R.  Salah  satu  library  R  yang  dapat  digunakan 
untuk pemodelan space time adalah package “gstar” yang dapat 
mempermudah pemodelan Generalized Space Time Autoregressive 
(GSTAR).  Namun,  package  tersebut  belum  mencakup  unsur 
musiman seperti pada model SGSTAR yang diperkenalkan oleh 
Prastuti  dan  Suhartono.  Oleh  karena 
ini 
mengembangkan  package  R  untuk  pemodelan  SGSTAR. 
ini  mengimplementasikan  package  R 
Kemudian  penelitian 
SGSTAR  pada  pemodelan  produksi  padi  di  Sumatera  Selatan. 
Data  yang  digunakan  berupa  produksi  padi  berdasarkan 
kabupaten/kota  di  Sumatera  Selatan  pada  Januari  2018  hingga 
Desember  2020.  Model  SGSTAR  terbaik  yang  terpilih  pada 
produksi  padi  menurut  kabupaten/kota  di  Sumatera  Selatan 
adalah SGSTAR(2)(1)12. Berdasarkan hasil pemodelan, produksi 
padi di Sumatera Selatan pada tahun 2021 diramalkan mencapai 
2.504.820  ton  GKG.  Selanjutnya,  penelitian  ini  membangun 
dashboard informasi berbasis web menggunakan framework Shiny 
telah 
untuk  memvisualisasikan  hasil  dari  model  yang 
dikembangkan. 

itu,  penelitian 

Kelemahan model STAR telah diperbaiki oleh Borovkova,dkk 
[4]  melalui  model  Generalized  Space  Time  Autoreggresive 
(GSTAR).  

Banyak  penelitian  yang  berkaitan  dengan  kajian  terapan 
model GSTAR, antara lain Ruchjana [5] melakukan pemodelan 
GSTAR pada kurva produksi minyak bumi. Minfeng Deng dan 
George Athanasopoulos [6] memodelkan data turis domestik di 
Australia  menggunakan  model  Space  Time  Autoregressive 
Integrated  Moving  Average 
[7] 
memodelkan  data  pariwisata  dengan  menggunakan  model 
VAR-GSTAR.  

(STARIMA).  Wutsqa 

Dari  beberapa  kajian  yang  telah  dilakukan  masih  terbatas 
pada data deret waktu multivariat yang stasioner, tetapi belum 
melibatkan  pola  musiman  atau  seasonal.  Prastuti  dan 
Suhartono  [8]  menambahkan  unsur  musiman  kedalam  model 
umum  GSTAR  dan  mengestimasi  parameter  menggunakan 
metode  Generalized  Least  Square  (GLS)  yang  biasanya 
digunakan  dalam  model  Seemingly  Unrelated  Regression 
(SUR)  melalui  model  S-GSTAR-SUR  atau  lebih  dikenal 
sebagai model SGSTAR. 

Kata Kunci— SGSTAR, Package R, Produksi Padi, Dashboard  

I.  LATAR BELAKANG 

Peramalan  (forecasting)  merupakan  prediksi  nilai-nilai 
sebuah  variabel  kepada  nilai  yang  diketahui  dari  variabel 
tersebut  atau  variabel  yang  berhubungan.  Peramalan  dapat 
dilakukan  dengan  analisis  deret  waktu  dalam  bentuk 
pemodelan data time series atau deret waktu. Hal ini didukung 
oleh  sebuah  metode  peramalan  Box-Jenkins  lebih  dikenal 
dengan  model  Autoregressive  Integrated  Moving  Average 
(ARIMA).  Model  ARIMA  adalah  model  yang  secara  penuh 
mengabaikan  variabel  penjelas  dalam  membuat  peramalan. 
ARIMA  menggunakan  nilai  masa  lalu  dan  sekarang  dari 
variabel respon untuk menghasilkan peramalan jangka pendek 
yang akurat [1].  

Dalam aplikasinya, karakteristik suatu daerah tidak hanya 
berkaitan antar waktu tetapi juga dipengaruhi oleh karakteristik 
dengan lokasi atau tempat lain yang berdekatan. Tobler dalam 
Arnanda [2] mengemukakan hukum pertama tentang geografi, 
yaitu  kondisi  pada  salah  satu  titik  atau  area  berhubungan 
dengan kondisi pada salah satu titik atau area yang berdekatan.  
Salah satu metode space time yang dapat digunakan adalah 
model Space Time Autoregressive (STAR) yang pertama kali 
diperkenalkan  oleh  Pfeifer  dan  Deutsch  [3],  Model  ini 
menggabungan  model  Autoregressive  (AR)  dari  model  Box-
Jenkins dan model spasial. Akan tetapi, model STAR memiliki 
kelemahan  pada  fleksibilitas  parameter  yang  mengasumsikan 
bahwa  lokasi-lokasi  yang  diteliti  memiliki  karakteriktik  yang 
sama  (homogen)  sehingga  parameter  autoregresive  dan 
parameter  space  time  bernilai  sama  untuk  semua  lokasi. 

Data  pertanian  adalah  salah  satu  data  runtun  waktu  yang 
seringkali  menunjukkan  siklus  musiman  karena  sangat 
dipengaruhi  oleh  iklim  dan  lingkungan.  Di  Indonesia,  Padi 
menjadi komoditas paling banyak dibudidayakan dan menjadi 
sumber  pangan  utama  bagi  masyarakat.  Produksi  padi  sangat 
pengaruhi  oleh  geografis  daerah  lahan  pertanian  tersebut, 
sehingga  suatu  daerah  akan  berkaitan  dengan  daerah  yang 
berdekatan  dikarenakan  memiliki  karakteristik  geografi  yang 
mirip. 

Rencana  Pembangunan  Jangka  Menengah  Nasional 
(RPJMN)  2020  –  2025  menyatakan  bahwa  pengembangan 
wilayah  Sumatera  diarahkan  untuk  memantapkan  perannya 
dalam  perekonomian  nasional  produksi  komoditas  serta 
sebagai  salah  satu  lumbung  pangan  nasional  [9].  Selain  itu, 
Pemerintahan  Provinsi  Sumatera  Selatan  didalam  Rencana 
Pembangunan Jangka Panjang Nasional (RPJPD) Tahun 2005 
– 2025 menargetkan produksi beras sebesar 3.200.000 ton atau 
setara dengan 5.000.000 ton padi [10]. 

Sumatera Selatan memiliki topografi wilayah yang beragam 
dengan  karakteristik  pedesaan  dan  perkotaan  sehingga 
produksi padi di Sumatera Selatan sangat  beragam antar satu 
daerah dan daerah lainnya. Sebaran produksi padi berdasarkan 
kabupaten/kota  di  Sumatera  Selatan  dapat  dilihat  pada  peta 
tematik berikut. 

 1 / 8 

 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

III. PENELITIAN TERKAIT 

studi 

Setiawan  dan  Prastuti  [11]  mengimplementasikan  metode 
SGSTAR  dalam  penelitiannya  pada  data  jumlah  kedatangan 
turis  pada  empat lokasi pariwisata  di Indonesia  yaitu Jakarta, 
Bali,  Surabaya,  dan  Surakarta.  Hasil 
simulasi 
menunjukkan bahwa S-GSTAR-SUR menghasilkan estimator 
yang  lebih  efisien  daripada  S-GSTAR-OLS  ketika  residu  di 
antara  keduanya  berkorelasi.  Hal  ini  ditunjukkan  dengan 
standard  error  S-GSTAR-SUR  yang  lebih  kecil.  Selain  itu, 
perbandingan  akurasi 
ramalan  dengan  metode  Vector 
Autoregressive  Integrated  Moving  Average  (VARIMA),  S-
GSTAR-OLS dan S-GSTAR-SUR menunjukkan bahwa model 
S-GSTAR-SUR dengan bobot spasial berdasarkan normalisasi 
korelasi  silang  parsial  menghasilkan  RMSE  terkecil  untuk 
meramalkan  jumlah  kunjungan  wisatawan  di  empat  lokasi 
pariwisata di Indonesia. 

Penelitian  dengan  menggunakan  metode  yang  sama  oleh 
Yudistira  [12],  melakukan  pengelompokan  stasiun  hujan 
menggunakan  variabel  geografis.      Pemodelan  space  time 
dengan  metode  Generalized  Space  Time  Autoregressive 
(GSTAR) digunakan untuk melakukan peramalan curah hujan 
di  Kabupaten  Jember.  Pemodelan  masing-masing  dilakukan 
sebanyak dua  kali  yaitu dengan  model  Seasonal GSTAR dan 
orde 
Non 
autoregressive 1 dan periode seasonal 12 pada model musiman. 
Penelitian  ini  menunjukkan  hasil  bahwa  model  musiman 
memberikan  nilai  RMSE  yang 
lebih  kecil  baik  pada 
pengelompokan BPS maupun K-Means.  

dengan  menggunakan 

Seasonal  GSTAR 

Mansoer  [13]  melakukan  pemodelan  dan  peramalan  hasil 
produksi padi pada tiga daerah di Jawa Tengah yaitu Kabupaten 
Demak,  Kabupaten  Boyolali  dan  Kabupaten  Grobongan. 
Penelitian  ini  menyimpulkan  bahwa  model  terbaik  yang 
dihasilkan  adalah  model  SGSTAR  (3)(1)3  dengan  bobot 
normalisasi  korelasi  silang,  karena  memiliki  nilai  rata-rata 
RMSE yang lebih kecil dan residual yang white noise. 

Namun  pada  penelitian  terdahulu  tersebut,  pemodelan 
dengan  metode SGSTAR ini  dilakukan dengan  mengestimasi 
parameter  secara  manual  baik  menggunakan  software  R 
maupun  aplikasi  stastistik  lainnya  sehingga  membutuhkan 
waktu  yang  lama.  Oleh  karena  itu,  penelitian  ini  akan 
membangun  sebuah  package  R  SGSTAR  sehingga  dapat 
mempermudah  penelitian  selanjutnya  yang  menggunakan 
metode ini dalam membangun model peramalan. 

IV. METODE PENELITIAN  
Pengembangan  package  R  dalam  penelitian  ini  disusun 
mengikuti metode Design Science Research (DSR) yang terdiri 
dari langkah-langkah berikut: 
1.  Identifikasi masalah 
2.  Tujuan untuk solusi 
3.  Desain dan pengembangan 
4.  Evaluasi 
5.  Kesimpulan 
Package  R  SGSTAR  yang  dikembangkan  kemudian 
diimplementasikan  pada  data  simulasi  dan  studi  kasus.  Data 
simulasi  adalah  data  yang  dibangkitkan  secara  manual  dan 
digunakan sebagai tiruan untuk merepresentasikan kondisi real 

 2 / 8 

Gambar 1. Produksi Padi Kab/Kot Provinsi Sumatera Selatan Tahun 2020 
Berdasarkan  peta  tematik  diatas,  dapat  dilihat  bahwa 
kabupaten dengan produksi padi tertinggi  di Sumatra Selatan 
yaitu  Banyu  Asin,  OKU  Timur,  OKI  dan  Musi  Banyuasin 
merupakan  daerah  yang  bertetangga  atau  berdekatan.  Jumlah 
produksi  padi  bersifat  tidak  tetap  dan  memiliki  karakteristik 
yang  berbeda  pada  setiap  daerahnya,  sehingga  sehingga 
pemodelan  SGSTAR  dapat  diimplementasikan  untuk 
meramalkan  produksi padi pada masing-masing kabupaten di 
Provinsi Sumatera Selatan. 

Penggunaan tools untuk statistik sangat bermanfaat karena 
memudahkan proses komputasi. Dewasa ini, pemodelan space 
time  telah  dapat  dilakukan  dengan  aplikasi  R.  Meskipun 
penggunaan  aplikasi  R  masih  berbentuk  command  line 
interface,  hal  tersebut  dapat  dipermudah  dengan  penggunaan 
package  yang  disediakan  dalam  R.  Salah  satu  library  yang 
dapat  digunakan  untuk  pemodelan  space  time  yaitu  package 
“gstar”.  Namun  package  ini  tidak  dapat  mencakup  unsur 
musiman  dalam  pemodelan  space  time.  Oleh  karena  belum 
adanya  package  yang  dapat  menerapkan  model  Seasonal 
GSTAR, penelitian ini membangun package untuk pemodelan 
SGSTAR sesuai dengan kaidah yang ada pada Comprehensive 
R  Archive  Network  (CRAN).  Kemudian  package  SGSTAR 
yang  dikembangkan  dalam  penelitian  ini  digunakan  untuk 
pemodelan pada produksi padi tiap kabupaten/kota di Sumatera 
Selatan. 

II.  TUJUAN PENELITIAN 
Berdasarkan latar belakang, maka tujuan penelitian ini 

adalah: 

1.  Membangun  package  pada  bahasa  pemrograman  R 
untuk  pemodelan  Seasonal  Generalized  Space  Time 
Autoreggresive (SGSTAR)  

2.  Mengimplementasikan  package  R  SGSTAR  untuk 
pemodelan  produksi  padi  menurut  kabupaten/kota  di 
Sumatera Selatan 

3.  Meramalkan produksi padi berdasarkan kabupaten/kota 
di  Sumatera  Selatan  tahun  2021  menggunakan  model 
peramalan terbaik. 

4.  Membangun  dashboard  visualisasi  sederhana  yang 

menampilkan hasil pemodelan. 

 
 
 
dalam memvalidasi pengoperasian dari package R. Data studi 
kasus  yang  digunakan  adalah  produksi  padi  berdasarkan 
kabupaten/kota pada Januari 2018 – Desember 2020. Data ini 
merupakan  data  sekunder,  bersumber  dari  hasil  pengolahan 
survei Kerangka Sampel Area (KSA) yang dilakukan oleh BPS 
Provinsi Sumatera Selatan.  

uji 

spasial 

dengan 

autokorelasi 

Dalam  penerapannya,  pemodelan  SGSTAR  ini  terlebih 
dahulu melakukan uji stasioneritas data dengan uji Augmented 
(ADF),  dilanjutkan  dengan  pengujian 
Dickey  Fuller 
spasial 
dependensi 
menggunakan metode Moran’s Index. Pemodelan SGSTAR ini 
menggunakan  matriks invers jarak sebagai  matriks pembobot 
spasial.  Model  SGSTAR  terbaik  dipilih  dengan  identifikasi 
tentatif  melalui  melalui  nilai  AIC,  RMSE  dan  Rsquared. 
Kemudian dilakukan uji kesuaian model dengan uji Ljung-Box 
untuk  mengetahui  sisaan  model  bersifat  white  noise. 
Selanjutnya  model  terbaik  yang  didapatkan  digunakan  untuk 
meramalkan produksi padi kabupaten/kota di Sumatera Selatan 
tahun 2021.  

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

2. Tujuan untuk Solusi 

Gambar 3. Diagram Ishikawa 

yang 

telah 

Pembangunan  package  R  dilakukan  karena  beberapa  hal 
yaitu: 
  Package  R 

dipublikasikan  melauli 
Comprehensive R Archive Network (CRAN) dapat diakses 
dan digunakan oleh pengguna manapun diseluruh dunia. 
  Package  R  merupakan  sebuah  alat  yang  mempermudah 
pengguna  dalam  memahami  suatu  script  karena  dapat 
digunakan secara langsung. 

V.  KERANGKA PIKIR 

Gambar 2 menjelaskan bagaimana kerangka pikir penelitian 
ini.  Peneliti  mendokumentasikan  package  R  yang  digunakan 
dalam  melakukan 
kedalam 
pemodelan 
Comprehensive  R  Archive  (CRAN).  Kemudian  package 
tersebut  diimplementasikan  pada  produksi  padi  kabupaten  di 
Sumatera  Selatan  serta  membangun  dashboard  visualisasi 
sederhana. 

SGSTAR 

  Package  R  dapat  memberikan  efisiensi  waktu  bagi 
pengguna  dalam  melakukan  pemodelan  tanpa  perlu 
menerjemahkan suatu rumus atau prosedur secara manual 
kedalam script. 

Pada package ini, terdapat tiga fungsi utama. Pertama, fungsi 
untuk  menduga ∅̂ sebagai  parameter  dari  model  SGSTAR 
pada setiap lokasi dengan menggunakan metode Generalized 
Least  Square  (GLS).  Dalam  menduga ∅̂ , model  SGSTAR 
yang digunakan mengikuti persamaan berikut. 

𝑝

𝑍𝑖(𝑡) = ∑[∅𝑘0 𝑍𝑖(𝑡 − 𝑘) +  ∅𝑘1𝑊 𝑍(𝑡 − 𝑘)]

𝑘=1

𝑝𝑠
   + ∑[∅𝑠
𝑘=1

𝑘0 𝑍𝑖(𝑡 − 𝑘 ∗ 𝑠) + ∅𝑠

𝑘1𝑊 𝑍𝑖(𝑡 − 𝑘 ∗ 𝑠)] + 𝑒(𝑡)               (1)

dengan  Zi(t)  menyatakan  pengamatan  pada  t  =  0,1,2,…,T 
waktu      dan  i=0,1,2,…N  lokasi.  Persamaan  diatas  serupa 
dengan bentuk linier 𝑌𝑖 = 𝑋𝑖𝛽𝑖 + 𝜀𝑖 dengan  

Gambar 2. Kerangka Pikir Penelitian 

VI. HASIL DAN PEMBAHASAN 

A.  Pengembangan Package R “sgstar” 
Berikut ini tahapan pengembangan package R “sgstar”. 
1. Identifikasi Masalah 

Identifikasi  masalah  dilakukan  dengan  studi 
literatur 
terhadap  perkembangan  dan  implementasi  metode  analisis 
runtun waktu dengan efek spasial atau space time. Masalah 
teridentifikasi  disajikan  dalam  bentuk  diagram 
yang 
ishikawa yang dapat dilihat pada Gambar 3 berikut. 

𝑌𝑖 = [

] 

𝑍𝑖(1)
𝑍𝑖(2)
⋮
𝑍𝑖(𝑇)
  𝑍𝑖(1 − 𝑠) …  𝑍𝑖(1 − 𝑝𝑠 ∗ 𝑠) 𝑉𝑖(1 − 𝑠) … 𝑉𝑖(1 − 𝑝𝑠 ∗ 𝑠)
  𝑍𝑖(2 − 𝑠) … 𝑍𝑖(2 − 𝑝𝑠 ∗ 𝑠) 𝑉𝑖(2 − 𝑠) … 𝑉𝑖(2 − 𝑝𝑠 ∗ 𝑠)

⋮           
 𝑍𝑖(𝑇 − 𝑠)

          ⋮        
   ⋱                       
  …  𝑍𝑖(𝑇 − 𝑝𝑠 ∗ 𝑠) 𝑉𝑖(𝑇 − 𝑠) … 𝑉𝑖(𝑇 − 𝑝𝑠 ∗ 𝑠)

   ⋱             ⋮       

 ⋮

] 

𝑋𝑖 = [

𝑍𝑖(1 − 1) … 𝑍𝑖(1 − 𝑝)
𝑍𝑖(2 − 1) ⋯ 𝑍𝑖(2 − 𝑝)

     ⋮        ⋱        

    ⋮

𝑉𝑖(1 − 1) … 𝑉𝑖(1 − 𝑝)
𝑉𝑖(2 − 1) … 𝑉𝑖(2 − 𝑝)
⋮ ⋱ ⋮

𝑍𝑖(𝑇 − 1) … 𝑍𝑖(𝑇 − 𝑝) 𝑉𝑖(𝑇 − 1) … 𝑉𝑖(𝑇 − 𝑝) 

𝑉𝑖 =

 ∑ 𝑊𝑖𝑗 ∗ 𝑍𝑗(1)

𝑗

⋮
∑ 𝑊𝑖𝑗 ∗ 𝑍𝑗(𝑇)
[

𝑗

]

 ; 𝜀𝑖 =   [

𝑒𝑖(1)
𝑒𝑖(2)
⋮
𝑒𝑖(𝑇)

] ;  𝛽𝑖 =

𝑝1

∅̂
10
⋮
∅̂
𝑝0
∅̂
11
⋮
∅̂
∅̂𝑠
10
⋮
∅̂𝑠
∅̂𝑠
11
⋮
∅̂𝑠
𝑝𝑠1]
[

𝑝𝑠0

 3 / 8 

 
 
 
 
 
         
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Perhitungan dimulai dengan meregresikan variabel X dan Y 
kemudian mengestimasi parameter dengan metode Ordinary 
Least Square (OLS) yaitu dengan meminimumkan fungsi  

𝜀′𝜀 = (𝑌 − 𝛽̂𝑋)′(𝑌 − 𝛽̂𝑋)       

(2) 

sehingga  

         (3) 
Residual pada model SGSTAR berkorelasi antar persamaan 
atau lokasi sehingga matriks varian-kovariannya adalah 

𝛽̂ = (𝑋′𝑋)−1(𝑋′𝑌)   

Fungsi  yang  ketiga  dari  package  ini  untuk  membuat 
visualisasi dari dari pemodelan SGSTAR yang telah dibuat. 
Alur  pembuatan  diagram  dimulai  dengan  memasukkan 
formula  model  yang  telah  dibuat,  lalu  menyimpan  nilai 
Z , 𝑍 ̂  𝑑𝑎𝑛 , 𝑍̂𝑝𝑟𝑒𝑑 untuk semua lokasi pengamatan, kemudian 
dengan i = 1 dan N = jumlah lokasi pengamatan pada model 
dilakukan iterasi untuk membuat diagram garis runtun waktu 
dengan  ggplot  hingga  nilai  i=N.  kemudian  setiap  diagram 
disimpan pada masing-masing plot lokasi pengamatan.  

𝐸(𝜀′𝜀) =   𝜎𝑖𝑗⨂𝐼𝑇 = [

⋯

𝜎11 𝜎12
𝜎21 𝜎22
⋮

𝜎1𝑁
𝜎2𝑁
⋮
𝜎𝑁1 𝜎𝑁2 ⋯ 𝜎𝑁𝑁

⋱

] ⨂𝐼𝑇 = Ω   

(4)

3. Desain dan Pengembangan 
  Pemodelan 

Kemudian matriks varian-kovarian tersebut digunakan dalam 
estimasi  parameter  SGSTAR  dengan  metode  Generalized 
Least  Square 
(GLS)  yaitu  dengan  meminimumkan 
generalized sum square 𝜀′Ω−1𝜀  [11] yaitu  

 𝛽̂ = (𝑋′Ω−1𝑋)−1(𝑋′Ω−1𝑌)   

(5) 

dan 

𝑐𝑜𝑣(𝛽̂) = (𝑋′Ω−1𝑋)−1  

          (6) 

Secara  sistematis,  pemodelan  SGSTAR  dapat  dilihat  pada 
diagram alir berikut 

Package R Seasonal Generalized Space Time Autoregressive 
(SGSTAR) dapat diakses melalui aplikasi R dengan memuat 
package menggunakan fungsi “library(sgstar)”. Pemodelan 
dari interaksi pengguna (actor) dengan Package R SGSTAR 
melalui aplikasi R digambarkan dengan diagram use case dan 
activity diagram. Diagram use case dari package “sgstar” ini 
dapat dilihat pada Gambar 5 berikut. 

Gambar 5. Diagram use case 
Pada  package  R  yang  dikembangkan,  hanya  ada  satu 
pengguna yang berinteraksi dengan package melalui aplikasi 
R.  Activity  diagram  untuk  memuat  package  “sgstar”  dapat 
dilihat pada Gambar 6 berikut. 

Gambar 4. Diagram Alir Pemodelan SGSTAR 
Fungsi  selanjutnya  adalah  pendugaan  nilai  prediksi  Z  di 
beberapa  waktu  yang akan datang (𝑍̂𝑝𝑟𝑒𝑑) dari model  yang 
telah dibentuk. Pertama, masukkan nilai Zi(t), 𝛽̂ dan n_time 
yang  akan  diprediksi  dengan  model  SGSTAR  yang  telah 
dibentuk.  Kemudian  dengan  n_time  =  jumlah  waktu  yang 
akan  diprediksi  dan  j  =1  lakukan  iterasi  untuk  menghitung 
nilai  𝑍̂𝑝𝑟𝑒𝑑(𝑗)  menggunakan  rumus  𝑍̂𝑖(𝑗) = 𝑋𝑖𝛽𝑖  untuk 
masing-masing  lokasi  data  pengamatan  (Zi)  sampai  pada 
nilai j = n_time. 

Gambar 6. Diagram use case 

 4 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  Perancangan 

Berikut  struktur  struktur  file  dan  folder  yang  terdapat  pada 
package “sgstar”. 

Gambar 7. Struktur package R “sgstar” 

package 

  Implementasi 
Pembangunan 
bahasa 
pemrograman  R  versi  4.0.0  dan  Rstudio  versi  1.2.1335.0 
sebagai IDE. Berikut Gambar 8 adalah dokumentasi bantuan 
package  R  “sgstar”  yang  dapat  diakses  pada 
laman 
https://www.rdocumentation.org/  

dilakukan 

dengan 

Gambar 8. Dokumentasi package R “sgstar” 

4. Evaluasi 
Evaluasi  package  R  “sgstar”  dilakukan  dalam  beberapa  hal, 
yaitu: 
  Uji Validitas 

Uji  validitas  package  ini  menggunakan  data  simulasi  yang 
telah dibangkitkan sebelumnya. Data bangkitan ini memiliki 
pola musiman yaitu dengan distribusi N(100,5) pada t=1, dan 
N(80,5) pada t=2,…,12 yang berulang secara periodik pada t 
berikutnya. Data bangkitan terdiri atas 3 lokasi dan 5 lokasi 
ini,  model 
sebanyak  15  periode  musim.  Dalam  hal 
SGSTAR(1)(1)12  yang  dibangkitkan    dapat  ditulis  dalam 
bentuk sebagai berikut. 
                       𝑍(𝑡) = (∅10 +  ∅11𝑊) 𝑍(𝑡 − 1) 

          +(∅12

10   + ∅12

11𝑊 )𝑍(𝑡 − 12) + 𝑒(𝑡)     (7) 

Jika diuraikan dalam bentuk matriks adalah sebagai berikut. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

𝑍1,𝑡
⋮
[
𝑍𝑁,𝑡
∅12

] = ([

0
⋮

∅10.𝑍1 …
⋱
… ∅10.𝑍𝑁
0
⋮

⋮
0
10.𝑍1 …
⋱
⋮
… ∅12
0

] + [

10.𝑍𝑁

] + [

∅12

0
⋮

∅11.𝑍1 …
⋱
… ∅11.𝑍𝑁
0
⋮

⋮
0
11.𝑍1 …
⋱
⋮
… ∅12
0

11.𝑍𝑁

] 𝑊) [

] 𝑊) [

𝑍1,𝑡−1
⋮
𝑍𝑁,𝑡−1
𝑍𝑖,𝑡−12
⋮
𝑍𝑁,𝑡−12

] 

] + [

𝑒1,𝑡
⋮
𝑒𝑁,𝑡

]       (8) 

                  + ([

Besar  koefisien  yang  digunakan  dalam  simulasi 
ditampilkan pada Tabel I. 

ini 

TABEL I 
KOEFISIEN DATA SIMULASI 

Data 

3 
Lokasi 

5 
Lokasi 

i 
Z1 

Z2 

Z3 

Z1 

Z2 

Z3 

Z4 

Z5 

∅10 
0,20 

0,15 

0,25 

0,20 

0,15 

0,25 

0,15 

0,25 

∅11 
-0,20 

-0,35 

-0,20 

-0,10 

-0,15 

-0,10 

-0,15 

-0,10 

∅10.12  ∅11.12 
0,20 
0,80 

0,75 

0,65 

0,80 

0,75 

0,60 

0,75 

0,60 

0,40 

0,40 

0,10 

0,20 

0,20 

0,20 

0,20 

Bobot  yang  digunakan  pada  simulasi  ini  adalah  bobot 
seragam dan ei,t dibangkitkan berdistribusi N(0,5). Kemudian 
simulasi  pemodelan  dengan  dengan  package  R  SGSTAR 
terhadap  data  bangkitan  pada  beberapa  rentang  waktu. 
Berikut ini hasil performa pemodelan yang dirangkum dalam 
tabel perbandingan MSE empiris terhadap model bangkitan.  

TABEL II 
HASIL UJI VALIDASI 

N(lokasi) 

3 

5 

t 
84 
120 
180 
84 
120 
180 

MSE 
60,08 
54,94 
53,52 
54,25 
50,49 
47,21 

RMSE 
7,75 
7,41 
7,31 
7,36 
7,11 
6,87 

Pada kondisi N = 3, pemodelan SGSTAR ini menghasilkan 
MSE  sebesar  53,52  pada  t  =  180  yaitu  paling  rendah  jika 
dibandingkan dengan t = 120 sebesar 54,94 dan t = 84 sebesar 
60,08. Hal ini menunjukkan bahwa estimator yang dihasilkan 
konsisten,  ketika  ukuran  pengamatan  meningkat  maka 
estimator  menjadi  semakin 
terkonsentrasi  pada  nilai 
parameter  sebenarnya.  Hal  ini  juga  terjadi  pada  percobaan 
kedua yaitu pada kondisi 5 lokasi, yang menghasilkan MSE 
terkecil pada saat t = 180 yaitu sebesar 47,21.  

  Uji Usability 

Berikut  adalah  hasil  dari  uji  usability  yang  dilakukan 
terhadap  10  pengguna  yang  dilakukan  dengan  instrumen 
kuesioner System Usability Scale (SUS). 

 5 / 8 

 
 
 
 
 
 
 
 
 
 
TABEL III 
HASIL UJI USABILITY 
Pertanyaan 
4 

6  7  8 

2 

3 

1 
9 
5 
3  4  4  3  3  3  4  4  3 
4  3  3  3  3  3  4  4  3 
3  3  3  3  3  3  3  3  4 
4  4  4  3  4  3  4  3  4 
4  3  3  3  3  4  4  4  3 
3  4  3  2  3  3  4  3  2 
4  3  3  4  3  4  3  4  3 
3  4  3  3  3  1  3  4  3 
2  1  4  1  3  3  3  3  2 
4  3  3  4  3  4  3  4  3 
3
3
3
0 
1 
4 

3
3 

3
5 

3
2 

3
6 

2
9 

3
1 
Rata-rata 

10 
3 
2 
3 
2 
1 
0 
4 
3 
2 
4 

24 

Use
r 

1 
2 
3 
4 
5 
6 
7 
8 
9 
10 

Jml 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

TABEL V 

HASIL UJI PERFORMA 

Jml 

34 
32 
31 
35 
32 
27 
35 
30 
24 
35 
315 

Jml 
*2,5 
85 
80 
77,5 
87,5 
80 
67,5 
87,5 
75 
60 
87,5 
787,5 

78,75 

Tabel III menyajikan hasil dari uji usability dan didapatkan 
skor  rata-rata  dari  seluruh  responden  sebesar  78,75  artinya 
package R yang dibangun dapat digunakan dengan baik oleh 
pengguna. 
  Uji Performa 

Tabel  IV  berikut  ini  menampilkan  hasil  dari  pencatatan 
waktu yang dibutuhkan antar fungsi: 

TABEL IV 
HASIL UJI PERFORMA 
p 

fungsi 

t(waktu) 

20 

100 

1 

2 

1 

2 

sgstar 
predict_sgstar 
plot_sgstar 
sgstar 
predict_sgstar 
plot_sgstar 
sgstar 
predict_sgstar 
plot_sgstar 
sgstar 
predict_sgstar 
plot_sgstar 

waktu 
0,23 detik 
0,28 detik 
0,86 detik 
0,25 detik 
0,39 detik 
0,91 detik 
0,28 detik 
1,46 detik 
1,2 detik 
0,30 detik 
1,58 detik 
1,51 detik 

Berdasarkan  tabel  diatas,  dapat  dilihat  bahwa  semakin 
panjang  rentang  waktu  yang  terdapat  pada  data  akan 
memakan  waktu  yang  lebih  lama  baik  pada  fungsi  sgstar, 
predic_sgstar  maupun  plot_sgstar.  Selain  itu,  banyaknya 
orde  yang  digunakan  juga  mempengaruhi  waktu  yang 
dibutuhkan untuk memproses ketiga fungsi tersebut. 

B.  Analisis Studi Kasus 
Berikut ini tahapkan analisis pemodelan SGSTAR 
1. Identifikasi Data 

Data produksi padi diperoleh sebanyak 612 data yaitu bulan 
Januari 2018 hingga Desember 2020 pada 17 kabupaten/kota 
yang ada di Provinsi Sumatera Selatan. Berdasarkan analisis 
deskriptif  yang  dirangkum  pada  Tabel  V  terlihat  bahwa 
dalam 3 tahun terakhir Kabupaten Banyu Asin, OKU Timur 
dan OKI memiliki produksi padi tertinggi,  sedangkan Kota 
Prabumulih  dan  Lubuklinggau  dengan  produksi  padi 
terendah. 

Selanjutnya  pengujian  autokorelasi  spasial  melalui  Indeks 
Moran’s  yang  menunjukkan  ada  tidaknya  ketergantungan 
spasial  antar  kabupaten/kota.  Berikut  ini  hasil  pengujian 
autokorelasi spasial dengan pembobot invers jarak.  

Gambar 9. Hasil Uji Morans 
Berdasarkan hasil diatas, dapat dilihat bahwa nilai P-value < 
5 % yang berarti tolak H0 sehingga dapat disimpulkan bahwa 
terjadi autokorelasi pada produksi padi antar kabupaten/kota 
di Sumatera Selatan. Berikut  ini disajikan pola signifikansi 
hubungan  antar  lokasi  dengan  Local  Indicator  Spatial 
Autocorrelation (LISA). 

Gambar 10. Peta Signifikansi LISA 
Berdasarkan hasil diatas, terlihat bahwa terdapat pola  high-
high  pada  kabupaten  Banyu  Asin  dan  OKI  yang  keduanya 
merupakan  daerah  dengan  produksi  tertinggi  di  Sumatera 
Selatan. Selain itu terdapat hubungan yang signifikan antar 
lokasi dengan produksi padi rendah seperti Kabupaten/Kota 
Lubuklinggau,  Lahat,  Empat  Lawang,  Pagar  Alam  dan 
sebagainya.  

 6 / 8 

Kab/KotTotalMinMeanMedianMaxSdPrabumulih49801407726Lubuklinggau2401716673662477708Musi Rawas Utara35256097939273361631Pagar Alam4241624011789323816739OKU482300134065852361352Pali497370138255777572041Palembang5145801429302103992281OKU Selatan1080573723002262573331719Empat Lawang17669631049084310155953152Lahat218768132160775347167283390Muara Enim219199060893230329487555Ogan Ilir3390100941730688190015655Musi Rawas34965922397138009285987042Musi Banyuasin4700444381305790233720110880OKI1493946595414994073414001931470OKU Timur18491672143513661991917555256247Banyu Asin28614921585794865747726620171800 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

TABEL VII 
EVALUASI MODEL SGSTAR(2)(1)12 

Kemudian  dilakukan  Uji  Augmented  Dickey  Fuller  (ADF) 
untuk  mengetahui  kestasioneran  data  pada  sebelum 
melakukan  pemodelan.  Hasil  dari  pengujian  menunjukkan 
bahwa  nilai  P-value  <  5  %  yang  berarti  tolak  H0  pada 
masing-masing  kabupaten/kota.  Sehingga  disimpulkan 
bahwa  data  telah  stasioner  dan  dapat  digunakan  untuk 
melakukan pemodelan SGSTAR. 

2. Pemodelan SGSTAR 

Pemodelan  diawali  dengan  mengidentifikasi  model  time 
series  melalui  plot  Autocorrelation  Function  (ACF)  dan 
Partial Autocorrelation Function (PACF). Berdasarkan pola 
ACF dan PACF, didapatkan bahwa terdapat cut off  pada lag 
1 dan 2 serta terindikasi pola musiman pada lag 6 dan 12 pada 
demikian, 
sebagian 
identifikasi  tentatif  dapat  dilakukan  pada  kandidat  model 
SGSTAR(1)(1)6,  SGSTAR(2)(1)6,  SGSTAR(1)(1)12  dan 
SGSTAR(2)(1)12.  Selanjutnya  dilakukan  perhitungan 
matriks  pembobot  invers  jarak  dengan  metode  euclidean 
dalam  menghitung  jarak  antar  lokasi.  Berikut  hasil  dari 
perhitungan matriks pembobot invers jarak. 

kabupaten/kota.  Dengan 

besar 

      Wij  =  

Setelah  matriks  pembobot  spasial  dibentuk,  kemudian 
dilakukan  estimasi  parameter  model  dengan  menggunakan 
fungsi  “sgstar(data,w,p,ps,s)”.  Tabel  VI  berikut 
ini 
menunjukkan  nilai  MSE,  RMSE,  AIC  dan  Rsquared  dari 
model tentatif. 

TABEL VI 
EVALUASI MODEL TENTATIF 
AIC 

RMSE 

MSE 

238029842 

15428 

271980231 

206005575 

14352 

251615778 

111488502 

10558 

131708102 

98647712 

9932 

126666170 

Model 
SGSTAR(1)(1)6 
SGSTAR(2)(1)6 
SGSTAR(1)(1)12 
SGSTAR(2)(1)12 

R^2 

0,53 

0,60 

0,80 

0,82 

Berdasarkan  hasil  diatas,  dapat  disimpulkan  bahwa  model 
SGSTAR(2)(1)12  sebagai  model 
ini 
ringkasan  performa  model  SGSTAR(2)(1)12  pada  masing-
masing Kabupaten/Kota. 

terbaik.  Berikut 

Berdasarkan  hasil  diatas,  dapat  dilihat  bahwa  Kabupaten 
Banyu  Asin dan OKU Timur dengan nilai performa paling 
baik  dengan  Rsquare  yang  tertinggi  yaitu  85%,  sedangkan 
Rsquare terkecil adalah Kabupaten Muara Enim yaitu hanya 
13%.    Kemudian  dilakukan  uji  kesuaian  model  dengan  uji 
Ljung-Box. Berikut ini ditampilkan hasil uji kesuaian model 
pada masing-masing lokasi. 

TABEL VIII 
EVALUASI MODEL SGSTAR(2)(1)12 
Statistic 
2,45 
0,48 
0,06 
0,87 
0,02 
0,61 
0,87 
0,28 
2,34 
5,81 
0,26 
0,11 
1,83 
0,61 
8,88 
2,56 
0,23 

Kabupaten/Kota 
Palembang 
Lubuklinggau 
OKU Selatan 
OKU 
OKI 
Musi Rawas 
Ogan Ilir 
Pagar Alam 
Banyu Asin 
OKU Timur 
Empat Lawang 
Prabumulih 
Lahat 
Musi Rawas Utara 
PALI 
Muara Enim 
Musi Banyuasin 

P-Value 
0,29 
0,79 
0,97 
0,65 
0,99 
0,74 
0,65 
0,87 
0,31 
0,05 
0,88 
0,95 
0,40 
0,74 
0,01 
0,28 
0,89 

 7 / 8 

Kab/KotMSERMSEAICR^2Palembang19282251388.624758900.31Lubuklinggau336699580.34323300.43OKU Selatan11762951084.615103920.7OKU781926884.310040130.66OKI29714576717237.93815427180.73Musi Rawas171256984138.3219898310.74Ogan Ilir591694927692.2759751320.34Pagar Alam195127441.72505480.49Banyu Asin76637652227683.59840469330.85OKU Timur45166485221252.45799491500.85Empat Lawang38294481956.949171080.5Prabumulih11010.51410.78Lahat62360032497.280071870.54Musi Rawas Utara251290501.33226620.23PALI17804631334.322861600.57Muara Enim492015857014.4631760860.13Musi Banyuasin198116034451.0254386010.8Total986477129932.21266661700.8200.0340.0350.0490.0830.0390.1390.0340.1380.0570.0330.0920.0400.0320.0810.0550.0600.03100.0350.0400.0230.2150.0340.0630.0300.0310.1180.0430.0770.1090.0530.0520.0470.0410.04500.1420.0390.0460.0550.0930.0340.0890.0590.0630.0810.0340.0540.0900.0350.0420.0370.10400.0370.0410.0640.0660.0330.1170.0460.0820.0720.0290.0610.1360.0330.1200.0370.0480.06200.0410.1160.0410.0830.0850.0370.0790.0450.0340.0650.0590.0510.0350.2060.0340.0420.02500.0370.0590.0330.0330.0910.0490.0790.0970.0650.0590.0560.1210.0320.0400.0650.0700.03700.0370.0620.0840.0330.1480.0430.0280.0860.0690.0450.0310.0630.0720.0700.0260.0610.03800.0270.0450.1240.0500.1890.0400.0500.0810.0330.1700.0400.0350.0460.0710.0460.0870.03700.0500.0370.0760.0420.0400.0800.0530.0900.0590.0350.0780.1390.0600.0390.0990.0500.04200.0400.0920.0540.0290.0630.0870.0370.0310.1210.0470.0500.0240.0980.0360.1290.0280.03700.0460.1380.0580.0520.0650.0390.0700.0360.0410.0730.0410.0430.1300.0410.0470.0680.03700.0520.0290.1430.1040.0450.0320.0670.0550.0670.0250.0720.0400.1650.0280.0420.1170.05500.0400.0590.1010.0350.0410.1520.0370.0430.0300.1410.0410.0560.0420.0360.0790.0500.06500.0610.0530.0730.0670.0470.0380.0580.0370.0610.0810.0450.0530.0500.0450.1540.0610.03900.0940.0680.0430.0440.0590.1230.0320.0520.0620.0690.0330.0660.0530.1060.0990.0320.08800.0380.0750.0630.0370.0470.0430.0800.0640.0450.0900.0440.0500.0740.0550.0700.1020.0600 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Berdasarkan Tabel VIII diatas, dengan tingkat signifikansi 5% 
gagal  tolak  H0  pada  setiap  lokasi  penelitian  kecuali  pada 
kabupaten Penukal Abab Lematang Ilir (PALI). Oleh karena 
itu, dapat disimpulkan bahwa asumsi  white noise terpenuhi 
kecuali pada Kabupaten PALI.  

3. Peramalan 

padi 

produksi 

Berdasarkan  model  SGSTAR(2)(1)12  yang  telah  dibentuk, 
peramalan 
bulanan  masing-masing 
kabupaten/kota  di  Sumatera  Selatan  dapat  dilakukan. 
Berdasarkan  hasil  peramalan  pada  tahun  2021,  secara 
keseluruhan  produksi  padi  terendah  di  Sumatera  Selatan 
terjadi  pada  bulan  Januari.  Sedangkan  produksi  tertinggi 
terjadi pada bulan Maret dan April. Ringkasan perbandingan 
data  produksi  padi  aktual  dan  pemodelan  dalam  tahunan 
ditampilkan pada Tabel XI berikut. 

TABEL XI 
PERBANDINGAN DATA AKTUAL DAN MODEL 

Berdasarkan peramalan pada tahun 2021, secara keseluruhan 
Provinsi Sumatera Selatan memproduksi 2.504.820 ton GKG. 
Banyu  Asin  sebagai  kabupaten  dengan  produksi  tertinggi 
yaitu sebesar 860.688 ton GKG, sedangkan Kota Prabumulih 
dengan produksi padi terendah yaitu hanya sebesar 106 ton 
GKG.  
4. Visualisasi 

Penelitian  ini  membangun  dashboard  interaktif  dengan 
framework  R  Shiny  untuk  memvisualisasikan  hasil  dari 
pemodelan  metode  SGSTAR.  Pada  dashboard  ini  terdapat 
tiga menu yaitu tabel data, line chart, dan peta tematik. Menu 
table  data  dan  line  chart  menampilkan  produksi  padi 
berdasarkan  kabupaten/kota  bulanan  baik  data  observasi 
maupun  hasil  dari  pemodelan.  Selain  itu,  peta  tematik 
ditampilkan  dengan  slider  time  sehingga  memungkinkan 
pengguna  untuk  memilih  menampilkan  produksi  padi 
menurut kabupaten/kota pada bulan tertentu. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

VII. 

PENUTUP 

Package R untuk mengimplementasikan model SGSTAR telah 
tersedia  melalui  situs  CRAN 
berhasil  dibangun  dan 
https://cran.r-project.org/package=sgstar.  Berdasarkan 
uji 
validitas, package R yang dibangun telah menghasilkan output 
yang  sesuai  dengan  pemodelan  SGSTAR.  Package  ini  dapat 
mempermudah dan mempercepat pengguna dalam melakukan 
pemodelan dengan metode SGSTAR. Berdasarkan uji usability, 
package R telah dapat digunakan dengan baik oleh pengguna 
dengan  skor  uji  SUS  78,75.  Sedangkan  dari  segi  performa, 
package  ini  dapat  berjalan  dengan  baik  dalam  waktu  yang 
relatif  singkat  untuk  semua  fungsi  yang  tersedia.  Pemodelan 
produksi  padi  menurut  kabupaten/kota  di  Sumatera  Selatan 
dengan  SGSTAR(2)(1)12  merupakan  model  SGSTAR  terbaik 
untuk  melakukan  peramalan.  Produksi  padi  di  Sumatera 
Selatan  pada  tahun 2021 diramalkan  mencapai 2.504.820 ton 
GKG. 

DAFTAR PUSTAKA 
[1]  A. Pankratz, Forecasting with Univariate Box-Jenkins Models, New York : 

Wiley & Sons Inc,1983. 

[2]  F.  Arnanda  dan  A.  Karim,  Pemodelan  Produksi  Padi  di  Provinsi  Jawa 
Tengah Dengan Pendekatan Spatial Econometrics, Jurnal Statistika, 2016.  
[3]  P.   E.  Pfeifer  dan  S.  J.  Deutsch,  A  Three Stage  Iterative  Procedure  For 

Space-Time Modeling. Technometrics, 1980.  

[4]  S. Borovkova., B.N. Rucjhana. dan H. P Lopuhaä, Generalized STAR with 
Random Weights. Greece: Proceeding of the 17th International Workshop 
on Statistical Modeling, 2002.  

[5]  B. N. Ruchjana, Pemodelan Kurva Produksi Minyak Bumi Menggunakan 
Model Generalisasi S-TAR. Bogor: Forum Statistika dan Komputasi IPB, 
2002.  

[6]  M.  Deng.  dan  G.  Athanasopoulos,  Modelling  Australia  Domestical 
International Inbound Travel: A Spartial-Temporal Approach, Australia : 
Departement of Econometrics and Business Statistic Monash Univeristity, 
2009. 

[7]  D.  U.  Wutsqa  dan  Suhartono,  Seasional  Multivariate  Time  Series 
Forecasting on Tourism Data by Using VAR-GSTAR Model, Jurnal Ilmu 
Dasar, 2010. 

[8]  M. Prastuti dan Suhartono, Model GSTAR-SUR Musiman untuk Peramalan 
Jumlah Wisatawan  Mancanegara di Empat  Lokasi Wisata  di  Indonesia. 
Surabaya: Institut Teknologi Sepuluh Nopember, 2014. 

[9]  Badan  Perencanaan  Pembangunan  Nasional,  ”Rencana  Pembangunan 
Jangka  Menengah  Nasional  (RPJMN)  2020-2024”,  Jakarta:  Kementrian 
Perencanaan Pembangunan Nasional/Bappenas, 2020. 

[10] Badan  Perencanaan  Pembangunan  Daerah  Provinsi  Sumatera 
Selatan, ”Rencana Pembangunan Jangka Panjang Daerah (RPJPD) 2005-
2025”, Bappeda Sumsel, 2005. 

[11] Setiawan  dan  M.  Prastuti,  S-GSTAR-SUR  model  for  seasonal  spatio 
temporal data forecasting, Malaysia: Malaysian Journal of Mathematical 
Sciences 10, 2016. 

[12] Yudistira.  I,  Pengelompokan  Stasiun  Hujan  Melalui  Variabel  Geografis 
Pada  Pemodelan  GSTAR  Musiman  Untuk  Peramalan  Curah  Hujan  di 
Kabupaten Jember, Jember: Fakultas MIPA, Universitas Jember [thesis], 
2017. 

[13] Mansoer.  A.S,  Pemodelan 

Space-Time 
Autoregressive  (SGSTAR),  Semarang:  Jurnal  Gaussian,  Universitas 
Diponegoro, 2016. 

Seasonal  Generalized 

 8 / 8 

2021AktualSGSTARAktualSGSTARSGSTARPalembang126821355814305128259951Lubuklinggau900179368532101378491OKU Selatan3741838474385103519334493OKU1774018091163661868716969OKI484605530131525218460009455052Musi Rawas103512115545123934109098115451Ogan Ilir7184675988820736925152554Pagar Alam1273513827147991300212942Banyu Asin905846966585917157858444860688OKU Timur575340616000635628581492604537Empat Lawang5592056218607315602356086Prabumulih136161138116106Lahat7312972102702787356469386Musi Rawas Utara64967496792459295973PALI1722116023155861615514558Muara Enim8312660336518666691952392Musi Banyuasin136643149881157016141635135193Kab/Kot20192020 
 
 
 
 
 
 
"
221709800,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pembangunan Sistem Informasi Pencarian dan 
Penemuan Barang Hilang di Lingkungan 
Kampus Politeknik Statistika STIS 

M. Meiru Panca Rezki (221709800, 4SD1) 
Dosen Pembimbing: Firdaus, M.B.A. 

Ringkasan— Lost-and-found adalah fasilitas yang dimiliki oleh 
sebuah  bangunan  atau  area  publik  di  mana  orang-orang 
mengambil  barang  hilang  miliknya  yang  mungkin  ditemukan 
orang lain. Politeknik Statistika STIS memiliki fasilitas pencarian 
dan penemuan barang hilang yang dijalankan oleh Fungsi Tata 
Usaha dan Rumah Tangga (Fungsi TU&RT) Bagian Administrasi 
Umum  (BAU).  Fasilitas  tersebut  diperuntukkan  bagi  civitas 
academica  Politeknik  Statistika  STIS  untuk  melaporkan 
kehilangan  atau  temuan  barang  di  area  kampus.  Berdasarkan 
observasi,  banyak  civitas  academica  yang  kehilangan  dan 
menemukan  barang  di  area  kampus.  Namun  nyatanya  fasilitas 
tersebut tidak terlalu banyak menerima laporan kehilangan dan 
temuan  barang.  Masih  terdapat  beberapa  kekurangan  dan 
masalah  yang  perlu  diperbaiki  agar  fasilitas  tersebut  dapat 
berfungsi  dengan  semestinya.  Maka  penulis  mengembangkan 
fasilitas 
dan 
mengimplementasikannya ke dalam bentuk aplikasi android dan 
web  administrator.  Dari  hasil  evaluasi  SUS,  didapatkan  bahwa 
aplikasi  android  tersebut  dapat  diterima  dengan  baik  dan  siap 
digunakan.   

tersebut  menjadi 

informasi 

sistem 

Kata  kunci—  sistem  informasi,  pencarian,  penemuan,  android, 

web 

I.  LATAR BELAKANG 

Menurut Kamus Bahasa Inggris Cambridge, lost-and-found 
atau  bisa  diartikan  sebagai  fasilitas  pencarian  dan  penemuan 
barang  hilang  adalah  sebuah  tempat  di  area  atau  bangunan 
publik  di  mana  temuan  barang  hilang  dari  orang-orang 
disimpan  [1].  Ketentuan  mengenai  temuan  barang  hilang  ini 
diatur dalam Pasal 1997 ayat (1) dan (2) KUHPerdata. Menurut 
Pasal  1977  ayat  (1)  KUHPerdata,  orang  yang  menemukan 
barang  hilang  dianggap  sebagai  pemiliknya,  kecuali  dapat 
dibuktikan  sebaliknya  dalam  batas  waktu  tenggang  selama  3 
tahun (Pasal 1977 ayat (2) KUHPerdata) [2]. Sehingga peran 
fasilitas pencarian dan penemuan barang hilang di sini adalah 
sebagai perantara antara barang hilang dengan pemilik aslinya. 
Di  mana  pemilik  yang  mengaku  harus  menyertakan  bukti 
kepemilikan  sebelum  dapat  memiliki  barang  hilang  tersebut. 
Keberadaan fasilitas pencarian dan penemuan barang hilang di 
area atau bangunan publik dianggap penting bagi orang yang 
kehilangan  barang  agar  dapat  mudah  menemukan  kembali 
barangnya tanpa harus mencari dan mengelilingi seluruh area 
atau bangunan di mana barang tersebut hilang. 

Politeknik Statistika  STIS memiliki fasilitas pencarian dan 
penemuan barang hilang yang dikelola oleh Fungsi Tata Usaha 
dan  Rumah  Tangga  (Fungsi  TU&RT)  Bagian  Administrasi 

Umum  (BAU)  Politeknik  Statistika  STIS.  Fasilitas 
ini 
dijalankan  oleh  Staf  Fungsi  TU&RT  dengan  dibantu  oleh 
cleaning  service  dan  mitra  yang  tersebar  di  seluruh  area  dan 
bangunan Politekik Statistika STIS. Staf Fungsi TU&RT dalam 
pencarian  dan  penemuan  barang  hilang  bertugas  menerima 
temuan barang hilang, menyimpannya, mengembalikan kepada 
pemilik yang mencarinya, dan mengkoordinir cleaning service 
dan  mitra  dalam  pencarian  dan  penemuan  barang  hilang. 
Sedangkan  cleaning  service  dan  mitra  bertugas  untuk 
menyerahkan  temuan  barang  hilang  dan  membantu  mencari 
barang  hilang  yang  hilang  di area  cleaning  service  dan  mitra 
tersebut bekerja. 

Fasilitas yang seharusnya banyak digunakan orang banyak 
dalam  melakukan  pencarian  dan  penemuan  barang  hilang  ini 
nyatanya tidak terlalu banyak menerima laporan pencarian dan 
penemuan  barang  hilang  berdasarkan  data  dari  staf  Fungsi 
TU&RT  dan  observasi  dari  media  sosial.  Berdasarkan  data 
yang  didapat  dari  staf  Fungsi  TU&RT,  jumlah  rata-rata  per 
bulan barang yang ditemukan hanya sejumlah 5 barang, orang 
yang melapor kehilangan barang sejumlah 3 orang, dan barang 
yang  dikembalikan  ke  pemiliknya  sejumlah  3  barang. 
Berdasarkan  observasi  pada  salah  satu  grup  chatting 
mahasiswa  yang berjumlah 256 anggota, jumlah rata-rata per 
bulan  pengumuman  kehilangan  antara  8 
sampai  10 
pengumuman. Hal ini salah satunya disebabkan karena belum 
banyak mahasiswa Politeknik Statistika STIS yang tahu tentang 
keberadaan fasilitas pencarian dan penemuan barang hilang ini 
berdasarkan  data  survei  penelitian.  Berdasarkan  pada  survei 
penelitian  yang  penulis  lakukan  kepada  mahasiswa  yang 
pernah kehilangan barang di mana hanya 58% mahasiswa yang 
tahu  bahwa  kehilangan  barang  diproses  oleh  Fungsi  TU&RT 
dan dari 58% mahasiswa yang tahu tersebut, hanya 70% yang 
melaporkan kehilangan barang ke Fungsi TU&RT. 

Ketidaktahuan  mahasiswa  akan  fasilitas  ini  kemungkinan 
disebabkan karena kurangnya informasi yang ditampilkan oleh 
Fungsi  TU&RT  seperti  belum  adanya  media  pengumuman 
barang hilang seperti papan pengumuman atau mading. Selain 
belum  adanya  media  pengumuman  barang  hilang,  Fungsi 
TU&RT  juga  belum  melakukan  pencatatan  dan  pendataan 
lengkap  barang  hilang  yang  ditemukan  atau  dilaporkan 
pemiliknya.  Lalu  cara  lain  yang  biasanya  dipilih  mahasiswa 
adalah melalui media sosial seperti Whatsapp, Line, Instagram, 
Facebook,  dan  sebagainya.  Sebanyak  51%  mahasiswa  yang 
pernah  kehilangan  barang,  menggunakan  media  sosial  untuk 
hilang.  Walaupun 
mengumumkan 

pencarian 

barang 

 1 / 8 

 
 
 
 
 
tetap 

penggunaan media sosial untuk mengumumkan barang hilang 
dan barang temuan ini lebih mudah digunakan, jangkauan yang 
sangat  luas,  dan  efisiensi  waktu  dalam  membuat  laporan, 
namun 
sosial  untuk 
saja  penggunaan  media 
mengumumkan  barang  hilang  dan  temuan  ini  memiliki 
kekurangan 
sasaran,  dapat 
menimbulkan  isu  keamanan,  dan  keterbatasan  layanan  dari 
media sosial itu sendiri yang tidak dirancang untuk menangani 
kasus kehilangan atau temuan barang hilang [3]. 

seperti 

target 

tidak 

tepat 

Upaya  untuk  mengatasi  permasalahan  dan  kekurangan 
tersebut adalah dengan mengembangkan sistem yang terdapat 
pada fasilitas pencarian dan penemuan barang hilang Politeknik 
Statistika  STIS  menjadi  sebuah  sistem 
informasi  yang 
menggunakan  media  yang  lebih  terkini  dan  interaktif  yaitu 
membangun  suatu  aplikasi  mobil  yang  didukung  oleh  sistem 
operasi  ponsel  cerdas  yang  banyak  digunakan  oleh  civitas 
academica  dan  membangun  web  administrator  yang  akan 
digunakan  oleh  Staf  Fungsi  TU&RT  untuk  membuat 
pengumuman  sekaligus  pencatatan  dan  pendataan  barang 
hilang.  Nantinya  aplikasi  mobil  dan  web  administrator  ini 
saling 
yang 
terkomputerisasi dan terintegrasi.  

untuk  menciptakan 

terhubung 

sistem 

II.  TUJUAN PENELITIAN 

Tujuan dari penelitian ini adalah melakukan pembangunan 
sistem  informasi  pencarian  dan  penemuan  barang  hilang  di 
lingkungan  kampus  Politeknik  Statistika  STIS 
agar 
menciptakan  sistem  yang  terkomputerisasi  dan  terintegrasi 
dengan  memperhatikan  permasalahan  pada  sistem  berjalan, 
yaitu: 
a) Menyediakan  media  informasi  untuk  pengumuman  barang 

hilang. 

b) Menyediakan  media  untuk  Fungsi  TU&RT  mencatat  dan 

mendata barang hilang. 

c) Menyediakan  media  yang  terkini  dan  interaktif  untuk 
memudahkan civitas academica dalam melakukan pencarian 
dan  penemuan  barang  hilang  di 
lingkungan  kampus 
Politeknik Statistika STIS.  

III. PENELITIAN TERKAIT 

Penelitian yang terkait dengan penelitian ini adalah sebagai 

berikut: 
a) Penelitian  berjudul  “Sistem  Informasi  Pencarian  Barang 
Hilang  “Lost  and  Found”  Pada  Kampus  3  Universitas 
Muhammadiyah  Malang”  oleh  Aminudin,  dkk  dari 
Universitas Muhammadiyah Malang tahun 2020 membahas 
pembuatan  sistem  informasi  pencarian  barang  hilang  pada 
kampus 3 Universitas Muhammadiyah Malang berbasis web. 
Metode  pengembangan  sistem  yang  digunakan  adalah 
metode  Waterfall  dan  metode  pendekatan  yang  digunakan 
adalah  metode  pendekatan  pola  desain  Model-View-
Controller  (MVC).  Sistem  yang  dibuat  dalam  penelitian 
tersebut adalah berbasis web, sedangkan sistem yang dibuat 
penulis  adalah  berbasis  android  dan  web.  Hasil  dari 
tersebut 
penelitian  menunjukkan 
memudahkan  mahasiswa  dalam  mengumumkan  dan 
mencari  informasi  barang  hilang  dengan  lebih  mudah  dan 

bahwa 

sistem 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

cepat, 
memudahkan pengguna [4]. 

sistem  yang  dibangun  dapat  diterima  dan 

b) Penelitian berjudul “Implementasi Layanan Pengaduan dan 
Kehilangan  Barang  Melalui  Aplikasi  L&F”  oleh  Rommy 
Trendy  Kharisma  dan  Andri  Sahata  dari  Universitas 
Komputer  Indonesia  tahun  2018  membahas  pembuatan 
sistem  pencarian  dan  pengumuman  barang  hilang  berbasis 
android.  Metode  pengembangan  sistem  yang  digunakan 
adalah metode pengembangan sistem prototype dan metode 
pendekatan  yang  digunakan  adalah  metode  pendekatan 
perancangan  berbasis  Objek  atau  OOP  (Object  Oriented 
Programming). Sistem yang dibuat dalam penelitian tersebut 
adalah  berbasis  android,  sedangkan  sistem  yang  dibuat 
penulis  adalah  berbasis  android  dan  web.  Hasil  dari 
penelitian  menunjukkan 
tersebut 
memudahkan  proses  pengaduan  laporan  barang  hilang 
menjadi  lebih  efisien,  pencarian  informasi  menjadi  lebih 
mudah, dan area pencarian menjadi semakin besar [5]. 

bahwa 

sistem 

IV. METODE PENELITIAN  

A. Ruang Lingkup Penelitian 

Penelitian  ini  merupakan  studi  yang  dilakukan  terhadap 
sistem  pencarian  dan  penemuan  barang  hilang  di  lingkungan 
kampus  Politeknik  Statistika  STIS.  Ruang  lingkup  dari 
penelitian 
ini  adalah  melakukan  pembangunan  sistem 
informasi  pencarian  dan  penemuan  barang  hilang  di  kampus 
Politeknik  Statistika  STIS  menjadi  sebuah  sistem  informasi 
yang  memiliki  fasilitas  pengumuman  dan  pencatatan  data 
terkomputerisasi serta terintegerasi. 

B. Metode Pengumpulan Data 

Dalam  melakukan  pengumpulan  data,  penelitian 

ini 

menggunakan beberapa metode yaitu: 
1. Wawancara 

Wawancara  merupakan  metode  pengumpulan  data  untuk 
memperoleh informasi dengan cara melakukan tanya jawab 
dengan  narasumber.    Narasumber  dalam  penelitian  ini 
adalah Subkoordinator Fungsi TU&RT. Pada penelitian ini, 
wawancara  dilakukan  untuk  mengetahui  dan  menganalisis 
proses  bisnis,  masalah  dan  kebutuhan  dari  sistem  yang 
sedang berjalan. 

2. Observasi 

Observasi  merupakan  metode  pengumpulan  data  untuk 
memperoleh  informasi  dengan  cara  melakukan  observasi 
atau  pengamatan  pada  objek-objek  yang  menjadi  sasaran 
penelitian.  Observasi  pada  penelitian  ini  adalah  dengan 
mengamati  jumlah  pengumuman  kehilangan  dan  temuan 
barang yang dilakukan mahasiswa pada media sosial. 

3. Studi Literatur 

Studi literatur merupakan metode pengumpulan data untuk 
memperoleh informasi yang berasal dari buku-buku, jurnal 
penelitian,  laporan  penelitian,  artikel  ilmiah,  dan  sumber 
lainnya  baik  dalam  bentuk  cetak  maupun  elektronik.  Pada 
penelitian  ini,  studi  literatur  dilakukan  dengan  mencari 
sumber  referensi  yang  memiliki  kesamaan  topik  dalam 
penelitian ini, yaitu berkaitan dengan pembangunan sistem 
informasi pencarian dan penemuan barang hilang. Informasi 

 2 / 8 

 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

barang  yang  hilang  dan  ditemukan  di  lingkungan  kampus 
Politeknik  Statistika  STIS.  Mekanisme  proses  pencarian  dan 
penemuan barang hilang yang dilakukan oleh sistem berjalan 
selama ini dapat dilihat pada flowchart berikut. 

yang  diperoleh  dari  studi  pustaka  akan  menjadi  landasan 
teori dan panduan pembangunan sistem informasi. 

4. Kuesioner  

Kuesioner  merupakan  metode  pengumpulan  data  untuk 
memperoleh  informasi  dengan  menggunakan  isian  daftar 
pertanyaan  yang  diisi  oleh  responden  dalam  hal  ini  adalah 
pengguna  sistem,  yaitu  civitas  academica.  Pada  penelitian 
ini,  kuesioner  dilakukan  untuk  mengetahui  tanggapan 
responden  mengenai  sistem  yang  sedang  berjalan  untuk 
mengetahui  permasalahan  yang  dimiliki  sistem  dan 
mengenai  sistem  hasil  pengembangan  untuk  dilakukan 
evaluasi  dengan  menggunakan  metode  System  Usability 
Scale (SUS). 

C. Metode Pengembangan Sistem 

pengembangan 

Pada  penelitian 

ini,  pembangunan  sistem 

informasi 
pencarian  dan  penemuan  barang  hilang  diimplementasikan 
menggunakan  metode 
System 
Development  Life  Cycle  (SDLC)  model  waterfall.  Metode 
pengembangan  sistem  SDLC  model  waterfall  adalah  urutan 
dari  beberapa  proses  secara  bertahap  dalam  merancang  dan 
mengembangkan sistem yang prosesnya menyerupai aliran air 
terjun  (waterfall)  [6].  Tahap-tahap  SDLC  model  waterfall 
dalam penelitian ini adalah sebagai berikut. 

sistem 

Gambar 1. Alur metode SDLC 

D. Metode Pembangunan Aplikasi 

Pada penelitian ini, pembangunan aplikasi sistem informasi 
pencarian  dan  penemuan  barang  Politeknik  Statistika  STIS 
menggunakan metode MVC (Model, View, Controller). MVC 
adalah pendekatan pembangunan aplikasi dengan pola desain 
yang  membagi  aplikasi  menjadi  tiga  komponen  [7].  Ketiga 
komponen tersebut adalah model, view, dan controller. Model 
adalah komponen yang berfungsi mengatur penyimpanan data, 
pergerakan  data,  dan  fungsi-fungsi  umum.  View  adalah 
komponen  yang  berfungsi  mengatur  tampilan  dari  aplikasi. 
Controller  adalah  komponen  yang  berfungsi  untuk  mengatur 
fungsi-fungsi  ketika  pengguna  melakukan 
aksi  dan 
penghubung aplikasi dengan database. 

E. Analisis Sistem Berjalan 
1. Proses Bisnis 

Selama  ini  fasilitas  pencarian  dan  penemuan  barang  di 
lingkungan kampus Politeknik Statistika STIS dijalankan oleh 
Fungsi  TU&RT.  Fasilitas  tersebut  melayani proses  pencarian 
dan  penemuan  barang  hilang  oleh  civitas  academica  untuk 

Gambar 2. Proses bisnis berjalan 

2. Analisis Permasalahan 

Saat  ini  proses  pencarian  dan  penemuan  barang  hilang  di 
lingkungan kampus Politeknik Statistika  STIS sudah berjalan 
dengan  baik.  Namun  masih  terdapat  beberapa  permasalahan 
dan kekurangan yang membuat kinerja Fungsi TU&RT dalam 
proses tersebut menjadi kurang efektif dan efisien. Hal tersebut 
pastinya  harus  diperbaiki  dan  ditingkatkan  lagi  agar  proses 
tersebut  bisa  berjalan  lebih  efektif  dan  efisien.  Beberapa 
permasalahan  dan  kekurangan  yang  terdapat  pada  proses 
pencarian dan penemuan barang hilang di lingkungan kampus 
Politeknik Statistika STIS adalah sebagai berikut: 
a) Pengumuman barang hilang 

Belum adanya media pengumuman barang hilang di kampus 
Politeknik  Statistika  STIS.  Civitas  academica  harus 
mendatangi langsung kantor BAU tiap kali untuk mengecek 
barang  hilang.  Hal  tersebut  tentunya  menyulitkan  civitas 
academica  dalam  mengetahui  informasi  mengenai  barang 
hilang.  

b) Pencatatan barang hilang 

Pencatatan  barang  hilang  saat  ini  belum  dilakukan  oleh 
Fungsi  TU&RT.  Hal  tersebut  menyebabkan  data  terkait 
barang hilang di Politeknik Statistika sulit untuk dianalisis. 
Tidak  adanya  daftar  barang  hilang  juga  bisa  menyulitkan 
pengecekan terhadap barang hilang. 

c) Pelaporan tidak melalui BAU 

Pelaporan  yang  tidak  melalui  Fungsi  TU&RT  contohnya 
seperti melalui media chatting dan media sosial. Hal tersebut 
dinilai  tidak  cukup  aman  dan  target  tidak  sesuai  sasaran 
dimana  data  pribadi  pemilik  atau  penemu  barang  hilang 
dapat  disalahgunakan  oleh  pihak  lain  untuk  kepentingan 
pribadi atau kelompok. 
Permasalahan  yang  telah  dijelaskan  di  atas  digambarkan 

melalui fishbone diagram berikut. 

 3 / 8 

 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

V.  KERANGKA PIKIR 

Gambar 3. Permasalahan sistem berjalan 

3. Analisis Kebutuhan 

Analisis kebutuhan diperlukan untuk menyeseuaikan solusi 
permasalahan  berdasarkan  analisis  permasalahan  yang  telah 
dilakukan. Untuk mengetahui kebutuhan sistem adalah dengan 
menilai sistem yang sedang berjalan itu sendiri. Kebutuhan dari 
sistem  terbagi  menjadi  dua,  yaitu  kebutuhan  fungsional  dan 
kebutuhan non fungsional. 
a) Kebutuhan fungsional 

Kebutuhan fungsional dari sistem pencarian dan penemuan 
barang hilang Politeknik Statistika STIS yang sedang berjalan 
adalah sebagai berikut: 

•  Sistem  pencarian  dan  penemuan  barang  hilang  official 

yang terkomputerisasi dan terintegrasi. 

•  Sistem  yang  mampu  mengakomodir  proses  pencarian 
dan  penemuan  barang  hilang  di  lingkungan  kampus 
Politeknik Statistika STIS. 

•  Sistem  pencarian  dan  penemuan  barang  hilang  yang 
memiliki  sarana  pengumuman  dan  pencatatan  terhadap 
barang hilang. 

•  Sistem  pencarian  dan  penemuan  barang  hilang  yang 
terdapat  pada  aplikasi  ponsel  cerdas  yang  dapat 
digunakan oleh civitas academica. 

b) Kebutuhan non fungsional 

Dalam  penelitian  ini  digunakan  metode  analisis  PIECES 
untuk menentukan kebutuhan non fungsional dari sistem yang 
sedang  berjalan  dan  menganalisis  sistem  usulan  yang  akan 
dibuat untuk memenuhi kebutuhan non fungsional tersebut. 

TABEL I 
KEBUTUHAN NON FUNGSIONAL 

No. 
(1) 

Aspek 
(2) 

Kebutuhan dalam Perbaikan Sistem 
(3) 

1 

2 

3 

4 

5 

6 

Performance 

Information 

Economy 

Control 

Efficiency 

Service 

Mampu meningkatkan kinerja proses pencarian 
barang hilang 
Mampu meningkatkan informasi tentang barang 
hilang menjadi lebih baik 
Mampu mempersingkat waktu kerja pegawai 
dalam memproses pencarian dan penemuan barang 
Mampu meningkatkan kontrol sistem dalam hal 
kepemilikan barang hilang 
Mampu meningkatkan keefisienan waktu bagi 
civitas academica dalam proses pencarian barang 
hilang 
Mampu meningkatkan jangkauan sistem dalam hal 
tempat dan waktu 

Gambar 4. Kerangka pikir 

tersebut.  Solusi  yang  penulis 

Kerangka pikir pada penelitian ini terdiri atas 4 bagian, yaitu 
masalah, solusi, evaluasi, dan hasil. Pertama, penulis mencari 
permasalahan yang  terjadi pada  sistem  yang  sedang  berjalan. 
Terdapat  3  permasalahan  yaitu  civitas  academica  kesulitan 
mengetahui informasi barang hilang, data barang hilang tidak 
tersedia,  dan  pencarian  dan  penemuan  barang  hilang  melalui 
media  sosial  memiliki  kekurangan.  Kedua,  setelah  masalah 
telah  diidentifikasi  maka  selanjutnya  membuat  solusi  untuk 
masalah 
lakukan  adalah 
membangun sistem informasi pencarian dan penemuan barang 
hilang di lingkungan kampus Politeknik Statistika STIS. Ketiga, 
setelah solusi berhasil dilakukan maka selanjutnya melakukan 
evaluasi  dari  solusi  tersebut.  Untuk  melakukan  evaluasi, 
penulis  menggunakan  2  metode  yaitu  black-box  testing  dan 
System Usability Scale (SUS). Apabila evaluasi telah dilakukan 
dan  hasil  evaluasi  berada  di  atas  ambang  batas  maka  dapat 
dikatakan hasil tersebut layak untuk digunakan. Keempat, hasil 
dari  penelitian  ini  adalah  berupa  aplikasi  android  dan  web 
administrator Sistem Informasi Pencarian & Penemuan Barang 
Politekenik Statistika STIS (SIPPBar STIS). 

VI. HASIL DAN PEMBAHASAN 

A. Rancangan dan Implementasi Sistem Usulan 

1. Proses Bisnis Sistem Usulan 

Proses  bisnis  sistem  informasi  pencarian  dan  penemuan 
barang  Politeknik  Satistika  STIS  yang  diusulkan  pada  dapat 
dilihat pada gambar berikut ini. 

 4 / 8 

 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

web service. Di dalam web service, record tersebut diubah ke 
dalam  bentuk  JSON  agar  dapat  diolah  dan  ditampilkan  pada 
aplikasi  android  civitas  academica.  Client-device  adalah 
aplikasi  native  android  yang  dibangun  menggunakan  bahasa 
pemrograman Kotlin dan XML. Web administrator adalah web 
yang dibangun menggunakan salah satu framework PHP, yaitu 
Laravel 8. 

Gambar 5. Proses bisnis usulan 

2. Diagram Use-Case Sistem Usulan 

Diagram  Use-case  dari  sistem  informasi  pencarian  dan 
penemuan  barang  Politeknik  Statistika  STIS  yang  diusulkan 
dapat dilihat pada gambar berikut ini. 

Gambar 7. Rancangan arsitektur 

4. Implementasi Database 

Gambar 6. Use-case sistem usulan 

3. Rancangan Arsitektur 

Arsitektur yang diusulkan untuk sistem informasi pencarian 
dan  penemuan  barang  Politeknik  Statistika  STIS  adalah 
berbentuk client-server dengan menggunakan jaringan internet 
untuk menghubungkan server database ke client device milik 
civitas  academica.  Web  administrator  dapat  menyimpan  atau 
menerima data pada server database. Antara client device dan 
server database dihubungkan oleh web service yang berfungsi 
dalam  mengeksekusi  kueri-kueri  select,  update,  insert,  atau 
delete  yang  parameternya  dikirim  oleh  civitas  academica 
melalui  jaringan  internet  secara  POST  atau  GET  melalui 
aplikasi  pencarian  dan  penemuan  barang  hilang.  Kueri  yang 
dikirimkan  client-device  akan  diterima  web  service  dan 
diteruskan  ke  server  database.  Kueri  yang  diterima  server 
database  akan  diolah  menjadi  record  data  yang  dibutuhkan. 
Record  tersebut  dikirimkan  kembali  ke  client-device  melalui 

Gambar 8. Implementasi database tabel barang_hilang 

Gambar 9. Implementasi database tabel barang_temu 

 5 / 8 

 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

6. Implementasi Antarmuka Pengguna 

Implementasi antarmuka pengguna atau user interface (UI) 
menjelaskan hasil dari tampilan aplikasi yang dibuat. UI yang 
dibuat oleh penulis dibagi menjadi UI aplikasi android dan UI 
web administrator. 
a) Implementasi UI aplikasi android 

•  Halaman login 

Pada  halaman  ini  pengguna  harus  mengisi  email  dan 
password yang telah didaftarkan. Jika pengguna berhasil 
login, pengguna akan diarahkan ke halaman utama yang 
berisi daftar pencarian dan penemuan barang hilang. Jika 
pengguna  belum  mendaftar,  maka  akan  ditampilkan 
peringatan  bahwa  email  tidak  terdaftar.  Jika  pengguna 
salah  memasukkan  password,  maka  akan  ditampilkan 
peringatan bahwa password yang dimasukkan salah. 

•  Halaman daftar pencarian/penemuan 

Pada  halaman 
ini  pengguna  dapat  melihat  daftar 
pencarian/penemuan  barang  hilang  yang  dibuat  oleh 
civitas  academica.  Pengguna  dapat  memperbarui  daftar 
toolbar  refresh  (🔄). 
pencarian  dengan  mengklik 
Pengguna  dapat  mencari  pencarian/penemuan  dengan 
mengklik  toolbar  search  (🔎).  Pengguna  juga  dapat 
menambahkan  daftar  pencarian  baru  dengan  mengklik 
icon  tambah  (➕).  Jika  pengguna  mengklik  salah  satu 
daftar pencarian/penemuan, maka aplikasi akan membuka 
halaman detail pencarian/penemuan. 
•  Halaman detail pencarian/penemuan 

Pada  halaman  ini  pengguna  dapat  melihat  secara  detail 
pencarian/penamuan  barang  hilang  yang  diklik  dari 
halaman  daftar  pencarian/penemuan.  Jika  pengguna 
mengklik  foto  barang,  maka  foto  akan  diperbesar  satu 
layar penuh. 

Gambar 10. Implementasi database tabel user_app 

Gambar 11. Implementasi database tabel lokasi_kampus 

Gambar 12. Implementasi database tabel jenis_barang 

5. Implementasi Kode Program 

Implementasi  kode  program  yang  dibuat  penulis  terbagi 
menjadi  kode  program  aplikasi  android,  kode  program  web 
service, dan kode program web administrator. 
a) Implementasi kode program aplikasi android 

Gambar 13. Implementasi kode program aplikasi android 

b) Implementasi kode program web service 

Gambar 14. Implementasi kode program web service 

(a)                                       (b)                                       (c) 

c) Implementasi kode program web administrator 

Gambar 15. Implementasi kode program web administrator 

Gambar 16. (a) Tampilan login aplikasi android, (b) Tampilan daftar 
pencarian/penemuan aplikasi android, (c) Tampilan detail 
pencarian/penemuan aplikasi android 

•  Halaman tambah pencarian 

Pada  halaman  ini  pengguna  dapat  menambahkan  daftar 
pencarian  baru  dengan  cara  mengisi  formulir  yang 
disediakan. Formulir tersebut berisi foto barang (jika ada), 
judul pencarian, jenis barang, lokasi hilang, waktu hilang, 
dan ciri barang. Jika sudah terisi semua, maka pengguna 

 6 / 8 

 
 
 
 
 
 
 
 
 
 
 
tinggal mengklik tombol tambah pencarian dan pencarian 
baru akan ditambahkan. 

•  Halaman sidebar 

Pada halaman ini ditampilkan header dari email dan nama 
pengguna.  Pengguna  dapat  mengklik  menu  lain  dari 
aplikasi  seperti  profil  pengguna,  pencarian  pengguna, 
notifikasi,  dan umpan  balik.  Jika  pengguna  ingin  keluar 
dari  aplikasi,  maka  pengguna  hanya  perlu  menekan 
tombol logout dan melakukan konfirmasi ingin logout. 

•  Halaman detail pencarian saya 

Pada  halaman  ini  pengguna  dapat  melihat  secara  detail 
pencarian  barang  hilang  pengguna  yang  diklik  dari 
halaman  daftar  pencarian 
saya.  Pengguna  dapat 
melakukan edit detail dari pencarian yang dibuat dengan 
mengklik 
tombol  edit  pencarian.  Pengguna  dapat 
menyelesaikan  pencarian  yang  dibuat  dengan  mengklik 
tombol selesaikan. 

(a)                                       (b)                                       (c) 

Gambar 17. (a) Tampilan tambah pencarian aplikasi android, (b) Tampilan 
sidebar aplikasi android, (c) Tampilan detail pencarian saya aplikasi android 

b) Implementasi UI web administrator 

•  Halaman login 

Pada  halaman  ini  pengguna  harus  mengisi  email  dan 
password yang telah didaftarkan. Jika pengguna berhasil 
login, pengguna akan diarahkan ke halaman  dashboard. 
Jika  pengguna  salah  memasukkan  email  atau  password, 
maka  akan  ditampilkan  peringatan  bahwa  email  atau 
password yang dimasukkan salah. 

•  Halaman menu dashboard 

Pada  halaman  ini  ditampilkan  grafik-grafik  dari  data 
pencarian dan penemuan barang. Grafik yang ditampilkan 
adalah  grafik  jumlah  pencarian  dan  penemuan  bulanan 
dalam  tahun  berjalan  dan  grafik  jumlah  pencarian  dan 
penemuan selesai bulanan dalam tahun berjalan. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

•  Halaman menu penemuan 

Pada  halaman  ini  ditampilkan  daftar  penemuan  barang 
temuan beserta detailnya. Pengguna dapat mencari barang 
pada  kotak  pencarian  berdasarkan  judul,  ciri-ciri,  jenis 
barang,  lokasi  temu,  dan  waktu  temu.  Pengguna  dapat 
menambahkan penemuan baru dengan mengklik tombol 
tambah  baru.  Pengguna  dapat  menambahkan  penemuan 
selesai  dengan  mengklik  tambah  selesai  jika  barang 
terdapat 
sudah 
identitas  pemiliknya  dan  barang 
dikembalikan  ke  pemiliknya.  Pengguna  dapat  mengedit 
detail  penemuan  dengan  mengklik  tombol  edit  detail. 
Pengguna  dapat  menyelesaikan  penemuan  dengan 
mengklik tombol belum selesai. 
•  Halaman menu tambah penemuan 

Pada halaman ini ditampilkan formulir tambah penemuan 
baru untuk menambahkan daftar penemuan baru. Seluruh 
isian dalam formulir wajib untuk diisi. Sehingga jika ada 
salah  satu  isian  yang  kosong,  maka  akan  ditampilkan 
pesan  jika  pengguna  mengklik  tombol  submit  tambah 
baru.  Jika  penemuan  berhasil  ditambahkan,  maka  akan 
ditampilkan pesan bahwa proses berhasil. 

(a)                                                               (b) 

Gambar 19. (a) Tampilan menu penemuan web administrator, (b) Tampilan 
menu tambah penemuan web administrator 

•  Halaman menu pencarian 

Pada  halaman  ini  ditampilkan  daftar  pencarian  barang 
hilang beserta detailnya. Pengguna dapat mencari barang 
pada kotak pencarian berdasarkan identitas pemilik, judul, 
ciri-ciri,  jenis  barang,  lokasi  hilang,  dan  waktu  hilang. 
Pengguna  dapat  menambahkan  pencarian  baru  dengan 
mengklik tombol tambah baru jika pemilik barang hilang 
tidak dapat menggunakan aplikasi android SIPPBar STIS 
pada 
cerdasnya.  Pengguna  dapat 
memberitahu  pemilik  barang  jika  barang  yang  hilang 
sudah ditemukan.  

ponsel 

pada 

•  Halaman menu unduh data 

Pada halaman ini ditampilkan formulir untuk mengunduh 
data  pencarian  dan  penemuan  barang  hilang.  Pengguna 
memilih  jenis  data,  memilih  status  selesai,  memilih 
tanggal mulai dan tanggal selesai, lalu mengklik tombol 
submit unduh. Data yang terunduh dalam format csv. 

(a)                                                               (b) 

(a)                                                               (b) 

Gambar 18. (a) Tampilan login web administrator, (b) Tampilan menu 
dashboard web administrator 

Gambar 20. (a) Tampilan menu pencarian web administrator, (b) Tampilan 
menu unduh data web administrator 

 7 / 8 

 
 
 
 
 
  
 
  
  
  
 
B. Pengujian Sistem Usulan 

1. Black-Box Testing 

Berdasarkan  kerangka  pikir  yang  ada  di  Bab  II,  maka 
penelitian ini menggunakan pengujian black-box untuk melihat 
apakah  aplikasi  yang  dibangun  sudah  bekerja  sesuai  yang 
diharapkan.  Pengujian  black-box  terbagi  menjadi  pengujian 
black-box  aplikasi  android  dan  pengujian  black-box  web 
administrator. 
a) Black-box aplikasi android 

TABEL II 
HASIL PENGUJIAN BLACK-BOX APLIKASI ANDROID 

Skenario 

(1) 

Pengguna: Civitas Academica 
Melakukan registrasi akun 
Melakukan login akun 
Mengganti nomor telepon 
Mengganti password 
Melihat detail pencarian 
Melihat detail penemuan 
Mencoba fitur cari (🔎) 
Menambahkan pencarian (➕) 
Mengedit pencarian 
Menyelesaikan pencarian 
Melakukan logout akun 

Hasil Kenyataan 
(berhasil/gagal) 
(2) 

Berhasil 
Berhasil 
Berhasil 
Berhasil 
Berhasil 
Berhasil 
Berhasil 
Berhasil 
Berhasil 
Berhasil 
Berhasil 

Berdasarkan hasil pengujian black-box aplikasi android pada 
tabel  II,  fungsi  yang  dibangun  oleh  penulis  sudah  berjalan 
sesuai  di  beberapa  perangkat  ponsel  cerdas  android  yang 
dijadikan pengujian. 
b) Black-box web administrator 

TABEL III 
HASIL PENGUJIAN BLACK-BOX WEB ADMINISTRATOR 

Skenario 

(1) 

Hasil Kenyataan 
(berhasil/gagal) 
(2) 

Pengguna: Staf Fungsi TU&RT 
Melakukan login akun 
Membuka menu dashboard 
Membuka menu penemuan 
Menambahkan penemuan 
Mengedit penemuan 
Menyelesaikan penemuan 
Membuka menu pencarian 
Memberi notifikasi pencarian ke pemilik 
Membuka menu unduh data 
Mengunduh data 
Melakukan logout akun 

Berhasil 
Berhasil 
Berhasil 
Berhasil 
Berhasil 
Berhasil 
Berhasil 
Berhasil 
Berhasil 
Berhasil 
Berhasil 

Berdasarkan  hasil  pengujian  black-box  web  administrator 
pada  tabel  III,  fungsi  pada  web  administrator  yang  dibangun 
oleh penulis sudah berjalan sesuai di beberapa jenis peramban 
web yang dijadikan pengujian. 
2. Evaluasi SUS 

Berdasarkan  kerangka  pikir  yang  ada  di  Bab  II,  maka 
penelitian  ini  menggunakan  evaluasi  system  usability  scale 
(SUS)  untuk  melihat  apakah  aplikasi  yang  dibangun  layak 
untuk diterapkan pada sistem. Evaluasi SUS dilakukan kepada 
pengguna aplikasi android, yaitu civitas academica. Hasil dari 
evaluasi SUS adalah sebagai berikut. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

TABEL IV 
HASIL EVALUASI SUS APLIKASI ANDROID 

Responden 

(1) 
1 
2 
3 
… 
27 
28 
29 

6 
(7) 
2 
1 
3 

4 
(5) 
4 
3 
4 

3 
(4) 
4 
3 
3 

2 
(3) 
3 
1 
3 

Pertanyaan 
9 
5 
1 
(10) 
(6) 
(2) 
3 
3 
2 
3 
3 
1 
3 
4 
1 
…  …  …  …  …  …  …  …  … 
3 
3 
3 
3 
3 
3 
3 
4 
4 
3 
3 
4 
Rerata 

8 
(9) 
3 
3 
3 

7 
(8) 
4 
3 
3 

2 
2 
4 

3 
3 
4 

2 
3 
4 

3 
3 
4 

3 
3 
4 

10 
(11) 
3 
2 
4 
… 
2 
3 
3 

Score 

(12) 
77,50 
57,50 
77,50 
… 
67,50 
75,00 
92.50 
76,81 

Berdasarkan  tabel  IV  dapat  disimpulkan  bahwa  aplikasi 
android  yang  dibangun  sudah  layak  untuk  digunakan  oleh 
pengguna. Hal ini terlihat dari hasil evaluasi lebih lanjut dari 29 
responden. Hasil dari score rerata  76,81 menunjukkan bahwa 
aplikasi android ini masuk dalam grade B yang artinya aplikasi 
android  yang  dibangun  sudah  layak  untuk  digunakan  oleh 
pengguna. 

VII. 

PENUTUP 

Berdasarkan  hasil  penelitian  yang 

telah  dilakukan, 

didapatkan beberapa kesimpulan sebagai berikut: 
a) Berhasil dibangun sistem informasi pencarian dan penemuan 
barang  hilang  di  lingkungan  kampus  Politeknik  Statistika 
STIS. 

b) Telah dibangun aplikasi android Sistem Informasi Pencarian 
&  Penemuan  Barang  Politeknik  Statistika  STIS  (SIPPBar 
STIS). 

c) Telah  dibangun  web  administrator  Sistem  Informasi 
Pencarian  &  Penemuan  Barang  Politeknik  Statistika  STIS 
(SIPPBar STIS) menggunakan framework Laravel 8. 

d) Hasil  uji  black-box  testing  menunjukkan  dari  seluruh 
skenario  aplikasi  android  dan  web  administrator  semua 
berhasil dilakukan. 

e) Hasil evaluasi aplikasi android menggunakan SUS mendapat 
skor  76,81  yang  artinya  aplikasi  android  dapat  diterima 
dengan baik dan siap digunakan. 

DAFTAR PUSTAKA 
lost-and-found. 

[1]  Cambridge  Dictionary. 

[Online].  Available: 

https://dictionary.cambridge.org/dictionary/english/lost-and-found   
[2]  J. Satrio. (2018, 3). Benda yang Dicuri atau Hilang Berada dalam Tangan 
Available:   

Pihak 
https://new.hukumonline.com/berita/baca/lt5a9f78bf85976/benda-yang-
dicuri-atau-hilang-berada-dalam-tangan-pihak-ketiga-pasal-1977-ayat-2-
bw/?page=all  

[Online]. 

Ketiga. 

[3]  A.  A.  Razi,  I.  R.  Mutiaz,  and  P.  Setiawan,  “Penerapan  Metode  Design 
Thinking Pada Model Perancangan UI/UX Aplikasi Penanganan Laporan 
Kehilangan  dan  Temuan  Barang  Tercecer”,  Jurnal  Desain  Komunikasi 
Visual,  Manajemen  Desain  dan  Periklanan,  vol.03,  no.02,  pp.  75-93, 
September 2018 

[4]  A. Aminuddin, I. Nuryasin, and S. Budianti, “Sistem Informasi Pencarian 
Barang  Hilang 
‘Lost  &  Found’  Pada  Kampus  3  Universitas 
Muhammadiyah Malang”, Jurnal REPOSITOR, vol. 2, no. 5, pp. 591-599, 
Mei 2020. 

[5]  R.T. Kharisma. 2018. Implementasi Layanan Pengaduan dan Kehilangan 
Barang Melalui Aplikasi L&F. Skripsi, Universitas Komputer Indonesia. 
[6]  J.L. Whitten and L.D. Bentley. System Analysis and Design Methods, 7th 

ed. McGraw-Hill/Irwin: New York, 2007, pp 89-92. 

[7]  F.  Buschmann,  R.  Meunir,  H.  Rohnert,  P.  Sommerlad,  and  M.  Stal. 
Pattern-Oriented Software Architecture: A System of Patterns. John Wiley 
& Sons: England, 1996, pp 125-143. 

 8 / 8 

 
 
 
"
221709798,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Implementasi Rule-Based Information Extraction
Untuk Membentuk Daftar Metadata SDI dan
Analisis Penggunaan Data BPS pada Publikasi
Kementerian

M Yais (221709798, 4SD2)

Dosen Pembimbing: Budi Yuniarto, S.S.T, M.Si

file

dari

informasi

Ringkasan— Ekstraksi

publikasi
kementerian dapat dilakukan untuk membentuk daftar
metadata. Daftar metadata dapat menjadi dasar untuk
melengkapi data pada Portal Satu Data Indonesia dan menjadi
perbandingan untuk memeriksa kelengkapan daftar metadata
yang sudah ada. Dari daftar metadata dapat dilakukan beberapa
analisis lanjutan sebagai berikut. Kemandirian Kementerian
Pertanian memproduksi data sektoral cenderung meningkat
ditunjukkan oleh rasio penggunaan data non BPS yang
cenderung meningkat meskipun
fluktuatif. Kemandirian
Kementerian Pariwisata memproduksi data sektoral relatif
rendah namun menunjukkan tren meningkat. Kemandirian
Kementerian Kesehatan dalam memproduksi data sektoral
relatif tinggi dan fluktuatif. Namun nilai tersebut cenderung
terpengaruh jumlah informasi tabel yang berhasil diekstrak.
Tren penggunaan data BPS pada Kementerian Pertanian,
Kementerian Pariwisata dan Kementerian Kesehatan cenderung
fluktuatif dan tidak selalu sesuai dengan perbedaan jumlah file
publikasi yang ditemukan pertahun.

Kata Kunci— Rule-based information extraction, text clustering,

feature engineering.

I. LATAR BELAKANG

program khusus

Pemerintah mengeluarkan

terkait
penyelenggaraan data untuk mengoptimalkan pemanfaatan
data-data pada semua sektor, yaitu melalui program SDI (Satu
Data Indonesia). Realisasi program SDI dilakukan dengan
membentuk Portal SDI (data.go.id). Terdapat permasalahan
kelengkapan data pada portal SDI. Dalam program SDI, BPS
memiliki peran sebagai pembina data statistik. Salah satu
tugas BPS sebagai pembina data statistik adalah memberikan
rekomendasi dalam perencanaan pengumpulan data. Masalah
kelengkapan data menjadi masalah yang harus diselesaikan
oleh BPS.

Data yang belum lengkap menghambat

tercapainya
tujuan-tujuan program Satu Data. Salah satu tujuan program
Satu Data menurut perpres nomor 39 tahun 2019 adalah
sebagai evaluasi. Untuk BPS, perlu dilakukan evaluasi
penggunaan datanya pada kementerian. Untuk kementerian,
evaluasi data yang mereka gunakan dan mereka produksi
dapat menjadi suatu ukuran kinerja dari kementerian yang
bersangkutan. Hal
sesuai dengan penjelasan dalam
Peraturan Pemerintah Republik Indonesia Nomor 51 Tahun
1999 bahwa statistik sektoral diselenggarakan oleh instansi

ini

pemerintah. Maka kementerian memiliki
dalam penyelenggaraan statistik sektoral.

tanggung jawab

file

langsung

Pemilihan

lebih cepat

pemanfaatan

Sebagai solusi untuk mempercepat upaya melengkapi
ketersediaan data pada Portal SDI, peneliti merekomendasikan
untuk memprioritaskan data yang ada pada publikasi
publikasi
kementerian.
tanpa harus
kementerian karena metode ini
berhubungan
yang
bersangkutan. Permasalahannya adalah data pada publikasi
yang pada umumnya dalam format pdf menjadi data yang
tidak terstruktur. Pada penelitian ini peneliti mengembangkan
suatu metode ekstraksi informasi untuk mengekstrak daftar
metadata dari data-data yang ada pada publikasi kementerian
sebagai langkah awal untuk melengkapi ketersediaan data. Isi
daftar metadata yang dibentuk oleh peneliti hanya sebatas
judul tabel dan sumber instansinya.

kementerian

dengan

Pembentukan daftar metadata sesuai dengan perpres nomor
39 tahun 2019 pasal 26, bahwa salah satu tahap perencanaan
pengumpulan data pada program Satu Data Indonesia adalah
penentuan daftar metadata yang akan dikumpulkan di tahun
selanjutnya. Pembentukan daftar metadata ini diharapkan
dapat menjadi
langkah awal untuk upaya melengkapi
ketersediaan data pada portal satu data Indonesia dengan lebih
cepat. Selain sebagai dasar pengumpulan data yang baru,
daftar metadata ini juga dapat dijadikan perbandingan untuk
memeriksa kelengkapan dari daftar metadata yang sudah ada
sebelumnya. Kemudian dari daftar metadata yang terbentuk,
beberapa
lanjutan dilakukan untuk mengamati
penggunaan data pada kementerian yang bersangkutan.

analisis

II. TUJUAN PENELITIAN

Berdasarkan masalah yang telah diidentifikasi, kemudian

dirumuskan beberapa tujuan penelitian sebagai berikut:

1. Membandingkan metode ekstraksi teks menggunakan
library PDFMiner, dan website
situs

menjadi

library PyPDF,
pdf
konversi
https://pdftotext.com/.

pada

txt

2. Membentuk daftar metadata berdasarkan publikasi
Kementerian Pertanian, Kementerian Pariwisata dan
Kementerian Kesehatan.

3. Mengevaluasi

penggunaan

publikasi Kementerian
Pariwisata dan Kementerian Kesehatan.

data BPS melalui
Pertanian, Kementerian

1 / 8

4. Mengevaluasi produksi data sektoral dari sumber
non-BPS pada Kementerian Pertanian, Kementerian
Pariwisata dan Kementerian Kesehatan.

III. PENELITIAN TERKAIT

Referensi [1] mengajukan metode eksplorasi teks dengan
menggunakan clustering kemudian memberikan label pada
setiap cluster yang terbentuk. Referensi [2] mengembangkan
suatu sistem analisis dan eksplorasi teks yang disebut TIARA
(Text Insight via Automated Responsive Analytics). Salah satu
fokus utama dalam penelitian ini
adalah penggunaan
unsupervised topic modelling dengan document clustering.

Referensi

[3] melakukan penelitian untuk mengekstrak
informasi dari data medis di Polandia dengan menggunakan
rule-based IE (information extraction). Rule-based IE dipilih
dibanding IE berbasis machine learning karena kompleksitas
dari dataset yang cenderung sulit
jika dibentuk model
machine learning ketika tidak tersedia cukup banyak data
untuk training dan testing serta ketersediaan dataset yang
sudah teranotasi secara semantik dalam bahasa Polandia masih
sangat sedikit.

IE

baik

seberapa

penerapan

rule-based.

menerapkan

Referensi [4] meneliti tentang perbedaan implementasi IE
pada dunia komersial dan dunia akademik. Sebesar 75% paper
NLP mengkaji implementasi IE berbasis machine learning.
Sebaliknya pada industri komersial vendor besar, 67%
implementasi
Peneliti
mengemukakan dua alasan tingginya implementasi rule-based
IE pada industri komersial sebagai berikut. Alasan pertama
adalah pada paper penelitian tentang IE berfokus pada akurasi
sedangkan pada dunia industri, ukuran akurasi saja tidak bisa
menilai
IE tersebut untuk
industrinya, karena akurasi untuk ekstraksi suatu entitas
tertentu bisa saja lebih penting dari akurasi untuk ekstraksi
entitas keseluruhan. Misalnya ekstraksi nama dari orang-orang
eksekutif suatu perusahaan lebih penting daripada ekstraksi
nama orang pada umumnya. Alasan kedua adalah biaya
pengembangan untuk mengembangkan IE dengan basis
machine learning relatif lebih mahal dibanding rule-based IE
yang kadang dinilai kurang sesuai dengan manfaat yang
diterima dari sistem IE yang terbentuk.
[5] meneliti

tentang klasifikasi berita hoaks
terkait covid-19 dengan feature engineering menggunakan
beberapa algoritma klasifikasi. Algoritma random forest
dengan feature engineering menghasilkan presisi, sensitivitas,
dan f1-score terbaik yaitu 92.31%, 100%, dan 96%.

Referensi

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

unggah,

tanggal
dan link unduh. Scraper kemudian
mengunduh otomatis file publikasi dan mengelompokkannya
berdasarkan tahun unggah.

File publikasi kemudian dikonversi menjadi file txt untuk
ekstraksi teks dalam file pdf. Peneliti mencoba tiga metode
untuk mengekstrak teks dari file pdf. Perbandingan antara
tersebut berdasarkan implementasi yang
ketiga metode
peneliti uji dapat dilihat pada tabel berikut.

TABEL I
PERBANDINGAN METODE EKSTRAKSI TEKS
Metode
Ekstraksi

Kelebihan

Kelemahan

No

1 Menggunakan
Library
PyPDF

1. Waktu ekstraksi setiap

1. Kualitas hasil

file sangat cepat,

2. Sudah dapat diotomasi

dengan python.

ekstraksi sangat
rendah.

2 Menggunakan
Library
PDFMiner

1. Sudah dapat diotomasi

1. Waktu ekstraksi

dengan python,

setiap file lambat.

2. Kualitas hasil ekstraksi

cukup baik.

3 Website

1. Waktu ekstraksi setiap

1. Belum dapat

konversi pdf
menjadi txt
(https://pdf
totext.com/
id/.)

file cepat,

2. Kualitas hasil ekstraksi
lebih baik dibanding
dua metode lainnya.

diotomasi dengan
python,
2. Data harus
diunggah
kemudian
hasilnya harus
diunduh,

3. Maksimal 20 file
untuk sekali
proses ekstraksi.

Metode yang peneliti pilih untuk digunakan dalam penelitian
lebih lanjut adalah metode ekstraksi dengan menggunakan
website. Karena pada penelitian ini fokus utama peneliti adalah
pada kualitas hasil ekstraksi teks.

judul

Rule-based IE kemudian dilakukan untuk memperoleh
tabel dan sumber data. Hasilnya akan
informasi
membentuk suatu daftar metadata. Rule-based IE adalah
informasi yang berdasarkan pada aturan
metode ekstraksi
tertentu yang dibuat secara manual. Ekstraksi
informasi
dimulai dengan memuat suatu file txt publikasi kementerian
menjadi
suatu dataframe dengan pemisah “\\n”. Maka
diperoleh suatu dataframe dengan satu kolom yang setiap
baris merupakan baris
file txt publikasi
kementerian. Kemudian setiap baris diberi label dengan alur
yang digambarkan pada flowchart.

tersendiri di

IV. METODE PENELITIAN
Data yang digunakan pada penelitian ini adalah file
publikasi pdf Kementerian Pertanian, Kementerian Pariwisata
dan Kementerian Kesehatan. Kementerian yang dipilih hanya
tiga untuk menghindari pembahasan yang terlalu banyak
sehingga dapat
fokus pada informasi setiap kementerian
secara lebih spesifik. Pemilihan kementerian berdasarkan
ketersediaan publikasi yang tepat untuk penelitian ini, yaitu
publikasi yang memuat data dalam bentuk tabel dan diketahui
juga memuat data-data dari BPS. Pengumpulan data
menggunakan scraper yang mengambil informasi judul file,

Gambar 1. Flowchart alur pemberian label.

2 / 8

Kemudian baris dengan label

tabel dipasangkan
dengan baris dengan label sumber terdekat yang terletak di
bawahnya. Alur pemasangan judul tabel dan sumber data
digambarkan pada flowchart berikut.

judul

Gambar 2. Flowchart alur pembentukan daftar metadata.

kemudian

Maka diperoleh pasangan judul tabel dan sumber data yang
membentuk suatu daftar metadata. Daftar metadata yang telah
terbentuk
serta
kejelasan
kelengkapan judul
tabel dan sumber data yang berhasil
diekstrak. Performa rule-based IE diamati dengan histogram
distribusi jumlah pasangan judul tabel dan sumber data yang
berhasil diekstrak per file publikasi.

dievaluasi

isinya

tren

penggunaan

Beberapa analisis lanjutan dilakukan dari daftar metadata
sebagai berikut : menghitung rasio penggunaan data non BPS,
data BPS, mengamati
menghitung
kelompok-kelompok data yang digunakan dengan clustering
tabel, mengamati sumber data yang dominan,
pada judul
tahun, dan
mengamati perubahan penggunaan data per
mengamati perubahan sumber data dominan per
tahun.
Sebelum dianalisis lebih lanjut dilakukan preprocessing untuk
mengubah semua huruf kapital menjadi huruf kecil,
menghapus stopword dan beberapa kata tambahan yang tidak
diperlukan, dan menghapus simbol, angka, dan multiple white
space.

ini

Rasio Penggunaan Data Non BPS
bertujuan
Rasio

untuk mengamati kemandirian
kementerian dalam menghasilkan data sektoralnya sendiri. Hal
ini sesuai amanat Peraturan Pemerintah Republik Indonesia
sektoral
Nomor
1999
diselenggarakan oleh instansi pemerintah. Dalam hal
ini
kementerian merupakan instansi pemerintah yang bertanggung
tertentu. Rasio ini dirumuskan
jawab pada suatu sektor
sebagai berikut.

statistik

bahwa

Tahun

51

𝑅𝑎𝑠𝑖𝑜 𝑃𝑒𝑛𝑔𝑔𝑢𝑛𝑎𝑎𝑛 𝐷𝑎𝑡𝑎 = 𝐽𝑢𝑚𝑙𝑎ℎ 𝑑𝑎𝑡𝑎 𝑠𝑢𝑚𝑏𝑒𝑟 𝑛𝑜𝑛 𝐵𝑃𝑆

𝐽𝑢𝑚𝑙𝑎ℎ 𝑑𝑎𝑡𝑎 𝑠𝑢𝑚𝑏𝑒𝑟 𝐵𝑃𝑆 𝑥100(1)

tabel dan sumber data yang memiliki

Jumlah data dengan sumber BPS dihitung dari pasangan
judul
label BPS.
Pemberian label BPS diberikan pada sumber data yang
memuat kata “bps” atau frasa “badan pusat statistik” dan
sebaliknya diberi label non BPS.

Aturan pemberian label BPS ini mengakibatkan sumber
data yang merupakan kerja sama antara BPS dan non BPS
hanya diidentifikasi sebagai sumber BPS. Identifikasi kerja
sama dapat diketahui dari keterangan sumber datanya. Jika
ditemukan BPS dan ada nama instansi lain pada keterangan
sumber datanya, ini menunjukkan adanya kerja sama antara
BPS dan instansi lain. Hal ini dapat menyebabkan nilai rasio

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

menjadi cenderung rendah. Untuk itu dilakukan klasifikasi
menggunakan algoritma
random forest dengan feature
engineering untuk mengklasifikasikan sumber BPS menjadi
dua kategori yaitu sumber BPS yang merupakan kerja sama
antara sumber BPS dan non BPS yang kemudian diberi label
sumber kerja sama dan sumber data yang seutuhnya hanya
diproduksi oleh BPS yang kemudian diberi label sumber non
kerja sama.

feature engineering adalah proses memilih dan mengubah
variabel saat membuat model prediktif menggunakan machine
learning atau pembuatan model statistik. Beberapa feature
yang dibentuk yaitu: jumlah karakter, ditemukan atau tidaknya
kata “dan”,
jumlah kata “dan”, ditemukan atau tidaknya
karakter “-”, ditemukan atau tidaknya karakter “,”, jumlah
karakter “,”, serta ditemukan atau tidaknya kata “ditjen” dan
“direktorat jenderal”. Klasifikasi dengan algoritma random
forest dipilih berdasarkan pada penelitian terdahulu [5]
melakukan klasifikasi berita hoaks dengan feature engineering
dan algoritma random forest memberikan presisi, sensitivitas,
dan f1-score terbaik.

Dataset untuk klasifikasi sumber kerja sama dibagi menjadi
train set dan test set sebesar 20% untuk test set dan dilakukan
dengan k-fold sebanyak 10 kali. Rata-rata presisi, recall, dan
f1-score yang diperoleh berturut-turut yaitu 0.97, 0.95, dan
0.96. Dari hasil klasifikasi, sumber BPS yang diklasifikasikan
sebagai sumber kerja sama akan turut menambah jumlah data
sumber non BPS.

Tren Penggunaan Data BPS
Tren penggunaan data BPS dibentuk dari

jumlah data
dengan sumber BPS. Penghitungan jumlah data berdasarkan
daftar metadata. Jumlah data dengan label BPS divisualisasi
dengan line chart membentuk tren penggunaan data.

Analisis Penggunaan Data Dominan
Analisis penggunaan data dominan dilakukan untuk
mengetahui gambaran ringkas data apa saja yang digunakan
oleh suatu kementerian. Analisis ini dilakukan dengan
clustering judul tabel dari daftar metadata dengan algoritma
k-means clustering. Peneliti memodifikasi ukuran kesamaan
dengan mengimplementasikan pre-trained model word2vec
dan mengukur jarak antar kata berdasarkan model word2vec
sebagai ukuran kesamaannya. Pre-trained model diperoleh
https://github.com/deryrahman/
dari
repository GitHub
word2vec-bahasa-indonesia.
jarak
Penggunaan
berdasarkan word2vec dipilih karena judul tabel memuat teks
yang relatif sangat pendek. Clustering pada teks yang pendek
membutuhkan tambahan informasi eksternal untuk lebih
memahami kosa kata yang ada [6].

ukuran

Tahapan dalam clustering teks judul

tabel diantaranya
adalah membentuk similarity matrix, membentuk cluster, dan
profiling cluster dengan word cloud. Ukuran kedekatan antara
dua judul tabel adalah rata-rata dari ukuran kesamaan untuk
seluruh kombinasi kata antara dua judul
tabel. Untuk
mengamati perbedaan data yang digunakan antara sumber
BPS dan non BPS maka clustering dilakukan secara terpisah.

3 / 8

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

berisi “tabel 6.3 No” dan sumber data yang hanya berisi
“sumber :”. Hal ini membuktikan bahwa metode ekstraksi
informasi yang digunakan masih memiliki kekurangan dari
kualitas informasi yang diekstrak. Evaluasi lainnya dilakukan
dengan mengamati penomoran tabel. Ditemukan suatu
publikasi yang hanya memuat satu pasang judul tabel dan
sumber data dengan penomoran “Tabel 5.4”. Dari penomoran
ini diketahui bahwa ada tabel
lain yang tidak berhasil
diekstrak dengan penomoran di bawahnya. Kemudian
performa hasil ekstraksi judul tabel dan sumber data dari file
publikasi Kementerian Pertanian, Kementerian Pariwisata, dan
Kementerian Kesehatan secara keseluruhan dapat diamati
pada histogram berikut.

Analisis Sumber Data Dominan
Analisis sumber data dominan dilakukan menggunakan
word cloud. Word cloud dibentuk dari seluruh sumber data
yang berhasil diekstrak dari seluruh file publikasi suatu
kementerian.

dua

dari

tabel

Fungsi

difflib.

library

menggunakan

Perubahan Penggunaan Data
Perubahan penggunaan data diidentifikasi dari nilai
tabel antar tahun. Pemeriksaan kesamaan
kesamaan judul
fungsi
judul
antara
SequenceMatcher()
ini
menyamakan setiap karakter antara dua string dengan
mengabaikan white
posisi
ditemukannya karakter. Jika untuk suatu judul pada tahun ke-t
memiliki nilai kesamaan kurang dari 50% dibanding semua
judul lainnya pada tahun ke-t-1, maka judul tabel tersebut
diidentifikasi sebagai data yang baru digunakan. Sedangkan
jika suatu judul pada tahun ke-t memiliki nilai kesamaan
kurang dari 50% dibanding semua judul lainnya pada tahun
ke-t+1, maka judul tersebut diidentifikasi sebagai data yang
sudah tidak lagi digunakan.

serta mengabaikan

space

Perubahan Sumber Data Dominan
Perubahan sumber data dominan diamati dengan word
cloud dari sumber data per tahun. Analisis ini dapat menjadi
informasi tambahan untuk menjelaskan rasio penggunaan data
non BPS.

V. KERANGKA PIKIR

Permasalahan pada penelitian ini adalah tidak lengkapnya
data pada portal SDI dan pentingnya evaluasi data yang
digunakan kementerian. Maka peneliti merekomendasikan
melengkapi data untuk portal SDI dengan prioritas data pada
publikasi kementerian. Selain itu peneliti merekomendasikan
beberapa analisis lanjutan untuk mengamati penggunaan data
pada kementerian sebagai suatu evaluasi penyelenggaraan
data sektoral.

Gambar 3. Flowchart alur penelitian

VI. HASIL DAN PEMBAHASAN

Evaluasi Daftar Metadata Kementerian Pertanian
Pada daftar metadata yang terbentuk dari file publikasi
Kementerian Pertanian, ditemukan judul tabel yang hanya

Gambar 4. Histogram distribusi judul tabel yang berhasil diekstrak per file
publikasi Kementerian Pertanian, Kementerian Pariwisata, dan Kementerian
Kesehatan.

Dari histogram diatas dapat dilihat bahwa judul file yang
berhasil diekstrak per file publikasi Kementerian Pertanian
mayoritas antara 0-40 dan ditemukan outlier dengan lebih dari
175 judul tabel per file publikasi. Pada publikasi Kementerian
Pariwisata judul file yang berhasil diekstrak per file publikasi
mayoritas antara 0-10 dan ditemukan outlier dengan sekitar 30
file publikasi. Sedangkan pada publikasi
judul
Kementerian Kesehatan judul file yang berhasil diekstrak per
file publikasi mayoritas antara 0-25 dan ditemukan outlier
dengan lebih dari 100 judul tabel per file publikasi.

tabel per

4 / 8

Rasio Penggunaan Data Non BPS
Nilai rasio penggunaan data non BPS dari tahun ke tahun
baik untuk publikasi Kementerian Pertanian, Kementerian
Pariwisata, dan Kementerian Kesehatan dapat dilihat pada line
chart berikut.

Gambar 5. Line chart rasio penggunaan data non BPS pada publikasi

Kementerian Pertanian, Kementerian Pariwisata, dan Kementerian Kesehatan.

Berdasarkan line chart, tampak bahwa seiring berjalannya
waktu Kementerian Pertanian telah menjadi semakin mandiri
dalam memproduksi data yang dibutuhkan dengan asumsi
mayoritas data dengan keterangan sumber non BPS berasal
dari Kementerian Pertanian itu sendiri. Asumsi
ini akan
dibuktikan selanjutnya pada analisis sumber data yang
digunakan. Pada Kementerian Pariwisata nilai kemandirian
memproduksi data sektoral juga semakin meningkat seiring
berjalannya waktu. Sempat terjadi penurunan pada tahun 2018
namun hal ini disebabkan nilai rasio yang diatur menjadi 0
tabel yang berhasil diekstrak.
karena
Sedangkan pada Kementerian Kesehatan nilai
rasionya
tahun ke tahun. Nilai rasio
tampak sangat fluktuatif dari
rendah terjadi disaat jumlah judul tabel dan sumber data yang
berhasil diekstrak sangat sedikit pada tahun tersebut.

tidak ada judul

Tren Penggunaan Data BPS

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

tersebut. Pada line chart berwarna hijau menunjukkan jumlah
data yang bersumber dari beberapa event besar yang
diselenggarakan BPS yaitu sensus penduduk, sensus ekonomi,
sensus pertanian, supas, sutas, susenas dan sakernas. Pada
Kementerian Pertanian, hampir semua data yang diidentifikasi
berasal dari event besar merupakan hasil susenas. Pada tahun
2014, 2015, 2016, dan 2017 semua data yang diidentifikasi
berasal dari event besar merupakan data susenas. Sedangkan
pada tahun 2018, 2019, dan 2020 selain data susenas,
ditemukan pula sedikit data sakernas. Fluktuasi penggunaan
data dari event besar tidak menunjukkan pola khusus baik
tren penggunaan data BPS cenderung tinggi
disaat nilai
maupun rendah. Maka waktu rilis data-data dari event besar
tidak terlalu berdampak pada tren penggunaan data BPS pada
publikasi Kementerian Pertanian.

Gambar 7. Tren penggunaan data BPS pada publikasi Kementerian

Pariwisata yang ditunjukkan oleh line chart dan perbandingannya terhadap
jumlah file publikasi yang digunakan yang ditunjukkan oleh bar chart.

Pada line chart diatas, penggunaan data BPS pada publikasi
Kementerian Pariwisata cenderung meningkat. Pada tahun
2018 nilai penggunaan data BPS adalah 0, karena memang
tidak ada judul tabel yang berhasil diekstrak Padahal banyak
file publikasi yang ditemukan. Penggunaan data BPS paling
banyak pada tahun 2019, sejalan dengan banyaknya file
publikasi yang ditemukan pada tahun 2019 tersebut. Untuk
penggunaan data dari event besar yang diselenggarakan oleh
BPS hanya ditemukan sedikit data sakernas pada tahun 2016
dan 2017.

Gambar 6. Tren penggunaan data BPS pada publikasi Kementerian

Pertanian yang ditunjukkan oleh line chart dan perbandingannya terhadap
jumlah file publikasi yang digunakan yang ditunjukkan oleh bar chart.

Pada line chart, terlihat tren penggunaan data BPS pada
Kementerian Pertanian sempat mengalami penurunan dengan
nilai terendah pada tahun 2018. Penurunan ini juga diiringi
penurunan jumlah file publikasi yang diperoleh untuk tahun

Gambar 8. Tren penggunaan data BPS pada publikasi Kementerian

Kesehatan yang ditunjukkan oleh line chart dan perbandingannya terhadap
jumlah file publikasi yang digunakan yang ditunjukkan oleh bar chart.

5 / 8

Pada line chart tampak bahwa tren penggunaan data BPS
dalam file publikasi Kementerian Kesehatan pada tahun 2014
jauh lebih tinggi daripada pada tahun-tahun lain. Tingginya
nilai tren bersesuaian dengan banyaknya jumlah file publikasi
yang ditemukan pada tahun tersebut. Banyaknya jumlah file
publikasi pada
tahun 2014 disebabkan karena metode
pengelompokan file publikasi ke dalam tahun tertentu
berdasarkan tanggal unggah yang ada pada website. Sehingga
untuk file publikasi sebelum tahun 2014 yang baru diunggah
pada tahun 2014 akan dikelompokkan dalam tahun 2014. Nilai
tren relatif rendah pada tahun 2015, 2016, dan 2017. Pada
tahun 2018 dan 2019 nilai tren cenderung tinggi kemudian
kembali menurun pada tahun 2020. Namun scraping file
publikasi pada tahun 2020 hanya sampai bulan september
2020. Maka nilai
tren untuk tahun 2020 tidak mewakili
seluruh file publikasi pada tahun tersebut.

Untuk data yang diproduksi dari event besar nilai tertinggi
pada tahun 2014 namun file publikasi yang dikelompokkan
pada tahun 2014 juga mencakup file-file publikasi pada
tahun-tahun sebelumnya yang baru diunggah pada tahun 2014.
Antara tahun 2015 sampai 2020 nilai penggunaan data dari
event besar yang dilakukan BPS hanya mengalami sedikit
fluktuasi dari waktu ke waktu.

Analisis Penggunaan Data Dominan

Gambar 9. Word cloud cluster judul tabel dari sumber BPS pada

Kementerian Pertanian.

Dari cluster diatas diketahui beberapa kelompok data pada
publikasi Kementerian Pertanian dari sumber BPS sebagai
berikut : data perkembangan konsumsi rumah tangga untuk
komoditas-komoditas pertanian pada cluster 0, data ekspor
impor komoditas pertanian pada cluster 1, data komoditas
pertanian pada sub sektornya pada cluster 2, data dependency
ratio komoditas pertanian pada cluster 3, data indeks
keunggulan komparatif indonesia untuk sektor pertanian pada
cluster 4, data komoditas pertanian menurut kode hs pada
cluster 5, serta data terkait penduduk miskin dan komoditas
pertanian pada cluster 6.

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Gambar 10. Word cloud cluster judul tabel dari sumber non BPS pada

Kementerian Pertanian.

Dari cluster diatas diketahui beberapa kelompok data pada
publikasi Kementerian Pertanian dari sumber non BPS sebagai
berikut : Data tentang realisasi penyaluran pupuk bersubsidi
pada cluster 0, data produksi komoditas pertanian tertentu di
dunia pada cluster 1, data rata-rata, prediksi dan perbandingan
harga komoditas pertanian pada cluster 2, data serangan hama
pada cluster 3, cluster 5 dan cluster 7, data kelompok tani,
pegawai Kementerian Pertanian, dan alat mesin pertanian
pada cluster 4, serta data penyediaan dan penggunaan alat
mesin pertanian dan komoditas pertanian pada cluster 6.

Gambar 11. Word cloud cluster judul tabel dari sumber BPS pada

Kementerian Pariwisata.

Dari cluster diatas diketahui beberapa kelompok data pada
publikasi Kementerian Pariwisata dari sumber BPS sebagai
berikut
: Pada cluster 0 berisi data jumlah perjalanan
wisatawan. Pada cluster 1 berisi data struktur pengeluaran
wisnus dan data pekerja pada sektor pariwisata. Pada cluster
2, 3, dan 4 berisi data kunjungan wisman pada pintu-pintu
masuk yang ada.

Gambar 12. Word cloud cluster judul tabel dari sumber non BPS pada

Kementerian Pariwisata.

Dari cluster diatas diketahui beberapa kelompok data pada
publikasi Kementerian Pariwisata dari sumber non BPS
sebagai berikut : pada cluster 0 berisi data tentang wisatawan

6 / 8

mancanegara menurut jenis kelamin, pada cluster 1 berisi
informasi seperti sepuluh negara tujuan wisata utama di dunia,
pada cluster 2 hanya berisi satu judul tabel yaitu daftar
pustaka lampiran. Cluster ini berisi judul tabel yang tidak
wajar, pada
tentang wisatawan
mancanegara menurut kelompok usia, pada cluster 4 berisi
data tentang pengeluaran wisman.

cluster 3 berisi data

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

tenaga kesehatan dan fasilitas kesehatan seperti posyandu, unit
transfusi darah, dll.

Analisis Sumber Data Dominan

Gambar 13. Word cloud cluster judul tabel dari sumber BPS pada

Kementerian Kesehatan .

Gambar 15. Word cloud sumber data Kementerian Pertanian tahun

2014-2020.

Dari cluster tersebut dapat diketahui beberapa kelompok
data yang digunakan pada publikasi Kementerian Kesehatan
yang bersumber dari BPS. Pada cluster 0 berisi data kondisi
penduduk terutama kemiskinan. Pada cluster 1 berisi data
tentang partisipasi penduduk dalam pendidikan serta tentang
angkatan kerja maupun penduduk yang bekerja. Sedangkan
pada cluster 2 berisi tabel yang mayoritas berjudul “tabel
numerik yang merupakan asal data peta”. Disini informasi
judul tabel tidak berhasil diekstrak dengan baik sehingga tidak
dapat dipahami dengan jelas.

Pada word cloud diatas terlihat bahwa pusdatin, direktorat
jenderal, dan bps adalah kata yang paling dominan. Pusdatin
adalah unit Pusat Pengolahan Data dan Informasi Kementerian
Pertanian. Direktorat Jenderal disini merujuk pada direktorat
jenderal yang ada pada Kementerian Pertanian itu sendiri.
Dapat disimpulkan bahwa Kementerian Pertanian dan BPS
merupakan sumber data yang dominan pada publikasi
Kementerian Pertanian.

Gambar 16. Word cloud sumber data Kementerian Pariwisata.

Gambar 14. Word cloud cluster judul tabel dari sumber non BPS pada

Kementerian Kesehatan.

Dari cluster tersebut dapat diketahui beberapa kelompok
data yang digunakan pada publikasi Kementerian Kesehatan
yang bersumber dari Non BPS. Pada cluster 0 berisi data
terkait penderita penyakit
terutama yang banyak dibahas
adalah hiv aids. Pada cluster 1 berisi data tentang jumlah
tenaga kesehatan dan pendidikan untuk tenaga kesehatan.
Pada cluster 2 berisi data tentang pasien di Rumah Sakit
Indonesia. Pada cluster 3 berisi data tentang jumlah kasus
penyakit tertentu. Pada cluster 4 berisi data tentang jumlah
kasus penyakit tertentu dan status gizi balita. Pada cluster 5
berisi data yang beragam dan tidak ada suatu kelompok data
tertentu yang dominan. Pada cluster 6 berisi data dengan judul
tabel lanjutan. Sebanyak 6 tabel ditemukan dengan judul tabel
“lanjutan”. Pada kasus ini judul tabel tidak ditulis ulang dan
hanya ditulis lanjutan saja sehingga untuk penulisan judul
tabel seperti ini mengakibatkan informasi tentang data pada
tabel tidak dapat dijelaskan. Pada cluster 7 berisi data tentang

data

penggunaan

Pada word cloud diatas

terlihat bahwa BPS dan
Kementerian Pariwisata itu sendiri merupakan sumber data
yang dominan. Hal ini sejalan dengan perkembangan rasio
jumlah
non BPS terhadap jumlah
penggunaan data BPS yang terus meningkat terutama pada
tahun 2019 dan 2020. Pada tahun-tahun tersebut mayoritas
data merupakan hasil kerja
antara Kementerian
Pariwisata dan BPS yang dapat diidentifikasi sebagai sumber
kerja sama sehingga menambah jumlah penggunaan data BPS
maupun non BPS.

sama

Gambar 17. Word cloud sumber data Kementerian Kesehatan.

7 / 8

Pada word cloud diatas tampak bahwa Kementerian
Kesehatan itu sendiri merupakan sumber data yang dominan.
ini ditunjukkan dari kata Kemenkes ri yang berarti
Hal
Kementerian Kesehatan republik indonesia, kata depkes ri
yang berarti departemen kesehatan republik indonesia, dan
kata-kata lainnya yang hampir seluruhnya merujuk kepada
Kementerian Kesehatan.

sedikit

Perubahan Penggunaan Data
Pada Kementerian Pertanian, perubahan penggunaan data
BPS cenderung
sebaliknya banyak perubahan
penggunaan data yang bersumber dari non BPS. Perubahan
yang signifikan untuk data yang bersumber dari non BPS yaitu
tahun 2015 data kebutuhan dan realisasi penyaluran pupuk
bersubsidi mulai digunakan, pada tahun 2016 data tentang
serangan hama dan ketersediaan alat mesin pertanian mulai
digunakan, pada tahun 2019 data produksi dan distribusi
semen beku balai besar inseminasi buatan mulai digunakan,
dan pada tahun 2020 data tentang unit-unit pelaksana teknis
pada Kementerian Pertanian mulai digunakan.

Pada Kementerian Pariwisata, penggunaan data cenderung
konstan baik data yang bersumber dari BPS maupun non BPS.
Untuk data-data yang bersumber dari non BPS, ditemukan
sedikit perubahan penggunaan data yaitu pada tahun 2020
distribusi wisatawan
ditemukan
mancanegara berdasarkan jenis kelamin dan kelompok usia.

tentang

baru

data

ditemukan

didominasi

Pada Kementerian Kesehatan

beberapa
perubahan penggunaan data. Penggunaan data yang berubah
tertentu. Perubahan
kelompok
tidak
data
penggunaan data juga sangat
terkait dengan tahun-tahun
dimana jumlah judul tabel dan sumber data yang berhasil
diekstrak berbeda secara signifikan dengan tahun sebelum
atau sesudahnya.

Perubahan Sumber Data Dominan
Pada Kementerian Pertanian, sumber data yang dominan
adalah BPS dan Kementerian Pertanian itu sendiri. Namun
pada tahun 2014 dan 2015 relatif banyak data yang diperoleh
dari kementerian perdagangan.

Pada Kementerian Pariwisata tahun 2016 sumber data yang
digunakan adalah BPS, UNWTO (United Nation World Tour
Organization), dan Kementerian Pariwisata itu sendiri. Pada
tahun 2017 sumber data dominan berasal dari BPS dan
Kementerian Pariwisata. Pada tahun 2018 tidak diketahui
sumber data yang digunakan karena tidak ada satupun sumber
data yang berhasil diekstrak pada tahun tersebut. Pada tahun
2019 hampir seluruh data yang ditemukan bersumber dari
hasil kerja sama antara BPS dan Kementerian Pariwisata itu
sendiri yaitu melalui ditjen imigrasi. Pada tahun 2020
dominasi
secara mandiri oleh
diproduksi
yang
Kementerian Pariwisata semakin meningkat.

data

Pada Kementerian Kesehatan sumber data yang dominan
adalah Kementerian Kesehatan itu sendiri. Namun pada tahun
2016, 2017, dan 2020 hanya berhasil diekstrak sedikit data
dan pada tahun-tahun ini sumber data dari BPS menjadi cukup
dominan.

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

VII.

PENUTUP

Eksplorasi penggunaan data berdasarkan file publikasi
kementerian dimulai dari ektraksi teks pada file publikasi.
Metode ekstraki yang dipilih menggunakan layanan dari
website pdf2text dengan pertimbangan utama pada kualitas
hasil ekstraksi yang paling baik dibanding metode lainnya.
Daftar metadata dari file publikasi Kementerian Pertanian,
Kementerian Pariwisata, dan Kementerian Kesehatan telah
berhasil dibentuk dengan beberapa keterbatasan pada kualitas
informasi dan kelengkapan tabel yang berhasil diekstrak.
Kemudian dari daftar metadata dibentuk tren penggunaan data
BPS untuk evaluasi penggunaan data BPS pada Kementerian
Pertanian, Kementerian
dan Kementerian
Kesehatan. Pada tahun tertentu ditemukan keterkaitan dimana
tren penggunaan data tinggi dan jumlah file publikasi pada
tahun tersebut juga tinggi. Namun tren penggunaan data BPS
per
tahun tidak selalu bersesuaian dengan jumlah file
publikasi yang ditemukan per tahun. Nilai tren ini masih
belum dapat menjadi ukuran kinerja BPS per tahun. Namun
tren ini dapat menjadi gambaran penggunaan data BPS dari
tahun ke tahun. Dari daftar metadata juga dibentuk rasio
penggunaan data non BPS. Rasio ini dapat menjadi ukuran
kinerja kementerian yang bersangkutan dalam memproduksi
data sektoral.

Pariwisata,

DAFTAR PUSTAKA
[1] Q. Han, et al. ""Labeltransfer-integrating static and dynamic label
representation for focus+ context text exploration."" 2018 International
Symposium on Big Data Visual and Immersive Analytics (BDVA). IEEE,
2018.

[2] F. Wei, et al. ""Tiara: a visual exploratory text analytic system.""
Proceedings of the 16th ACM SIGKDD international conference on
Knowledge discovery and data mining. 2010.

[3] A. Mykowiecka, M. Marciniak, and A. Kupść. ""Rule-based information
extraction from patients’ clinical data."" Journal of biomedical informatics
42.5 (2009): 923-936.

[4] L. Chiticariu, Y. Li, and F. Reiss. ""Rule-based information extraction is
dead! long live rule-based information extraction systems!."" Proceedings
of
language
processing. 2013.

the 2013 conference on empirical methods in natural

[5] A. T. B. Panjaitan, dan I. Santoso. Deteksi Hoaks Pada Berita Berbahasa
Indonesia Seputar COVID-19. Format : Jurnal Ilmiah Teknik Informatika.
2021.

[6] C. C. Aggarwal, and C. K. Reddy. “Data Clustering: Algorithms and
Applications.” United States of America : Chapman and Hall/CRC, 2013.

8 / 8

"
221709797,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pembangunan Sistem Informasi Manajemen dan 
Penilaian Kinerja Mitra Statistik 
Studi Kasus : BPS Kabupaten Jember 

M. Sonny Aditama (221709797, 4SI1) 
Dosen Pembimbing: Firdaus, MBA 

Ringkasan— BPS kabupaten/kota dalam melakukan tugasnya 
membutuhkan  tenaga  kerja  tambahan  yang  disebut  Mitra 
Statistik,  dalam  kegiatan  tersebut  mitra  dapat  melaksanakan 
berbagai  tugas,  diantaranya  yaitu  menjadi  petugas  pendataan 
lapangan,  pengolahan  data  dan  pemetaan.  Untuk  menjaga 
kualitas data yang dihasilkan, upaya yang dapat dilakukan adalah 
menggunakan  mitra  yang  berkualitas  dan  melakukan  evaluasi 
kinerja  mitra.  Selama  ini  data  mitra  di  tiap  seksi  masih  belum 
terintegrasi sehingga terjadi perbedaan data mitra, selain itu ada 
peraturan  baru  mengenai  keikutsertaan  mitra  pada  suatu 
kegiatan  sehingga  diperlukan  sistem  agar  lebih  mudah  dalam 
memilih mitra pada suatu kegiatan. Belum ada kriteria yang jelas 
pada  sistem  penilaian  kinerja  sehingga  penilaian  masih  secara 
subjektif.  Berdasarkan  permasalahan  tersebut,  dilakukanlah 
pembangunan sistem informasi manajemen dan penilaian kinerja 
mitra statistik di BPS Kabupaten Jember. Metode pembangunan 
sistem  yang  digunakan  adalah  SDLC  Waterfall  serta  pengujian 
dan evaluasi menggunakan Black Box Testing dan System Usability 
Scale (SUS). Hasil yang didapatkan dari penelitian ini yaitu sistem 
yang  dibangun  telah  berhasil  semua  fungsinya  sesuai  yang 
diharapkan  dan  dapat  diterima  dengan  baik  oleh  pengguna 
dengan nilai 75,00. 

Kata  Kunci—  Mitra  statistik,  manajemen,  penilaian  kinerja, 

SMARTER. 

I.  LATAR BELAKANG 
Badan Pusat Statistik merupakan lembaga pemerintah non-
kementrian yang bertanggungjawab langsung kepada Presiden. 
Tugas dari Badan Pusat Statistik sendiri adalah melaksanakan 
tugas  pemerintahan  di  bidang  statistik  sesuai  peraturan 
perundang-undangan  [9].  Struktur  organisasinya  terdiri  dari 
kepala  Badan  Pusat  Statistik,  Sekretaris  Utama,  Inspektur 
Utama,  Deputi  Bidang  Statistik  Distribusi,  Deputi  Bidang 
Statistik  Produksi,  Deputi  Bidang  Statistik  Sosial,  Deputi 
Bidang Neraca Wilayah dan Analisis Statistik, Deputi Bidang 
Metodologi Informasi Statistik, dan Instansi Vertikal yaitu BPS 
provinsi  dan  BPS  kabupaten/kota  yang  tersebar  di  seluruh 
Indonesia [8].  

Dalam melaksanakan tugas-tugasnya, instansi vertikal yaitu 
BPS  Kabupaten/Kota  membutuhkan  tenaga  kerja  tambahan 
yang  biasa  disebut  dengan  Mitra  Statistik.  Mitra  Statistik 
adalah tenaga kerja yang direkrut oleh BPS untuk menunjang 
kegiatan  statistik  di  suatu  wilayah.  Kegiatan  tersebut  yaitu 
sensus  dan  survei.  Dalam  kegiatan  tersebut,  mitra  dapat 
tugas,  misalnya  menjadi  petugas 
melakukan  berbagai 
pendataan  lapangan,  pengolahan  data,  dan  pemetaan.  Untuk 
pemberian honor kepada mitra, terdapat beberapa satuan yang 
digunakan,  salah  satunya  yaitu  Orang-Bulan  (O-B)  [2]. 

Berdasarkan Acuan Anggaran Jawa Timur T.A 2021, di BPS 
Kabupaten  Jember,  satuan  tersebut  juga  berpengaruh  pada 
pemberiaan  honor  pada  mitra  yang  juga  dapat  berpengaruh 
pada keikutsertaan pada suatu kegiatan. 

Banyaknya  kegiatan,  terbatasnya  jumlah  sumber  daya 
manusia  yang  berkualitas,  dan  adanya  aturan 
terkait 
keikutsertaan  mitra  dalam  suatu  kegiatan,  merupakan  suatu 
tantangan  tersendiri  bagi  BPS  Kabupaten  Jember  untuk  tetap 
menjaga kualitas data yang dihasilkan dari survei dan sensus. 
Aturan yang dimaksud yaitu jika mitra mengikuti kegiatan yang 
pembayarannya dalam satuan O-B, maka dalam rentang 1 (satu) 
bulan  tidak  dapat  menerima  honor  dari  kegiatan  lainnya  [4]. 
Satuan  O-B  tidak  harus  1 (satu)  bulan  kalender,  periode  satu 
bulan  disebutkan  tanggalnya  di  kontrak.  Kegiatan  yang 
menggunakan  satuan  O-B  untuk  pembayaran  yaitu  Sakernas, 
Susenas, dan Sensus Penduduk 2020. Dan untuk kegiatan yang 
satuannya selain O-B, mitra boleh mengikuti lebih dari 1 (satu) 
kegiatan secara bersamaan [4]. Kualitas data hasil sensus dan 
survei  ditentukan  oleh  kualitas  pencacah  (atau  dalam  hal  ini 
mitra yang menjadi pencacah) [3]. Untuk mengevaluasi kinerja 
mitra  tentunya  membutuhkan  sistem  penilaian  kinerja  yang 
dapat mengukur kualitas dari mitra tersebut [6]. 

Selama 

ini  belum 

tersebut  memenuhi 

terdapat  sistem  untuk  membantu 
pemilihan mitra yang akan diikutsertakan pada suatu kegiatan 
yang  otomatis  mengikuti  aturan  yang  ada, 
sehingga 
pemeriksaan terkait kegiatan apa saja yang diikuti oleh mitra, 
dan  apakah  mitra 
syarat  untuk 
diikutsertakan  dalam  suatu  kegiatan,  harus  dilakukan  secara 
manual.  Selain  itu,  data  mitra  juga  belum  terintegrasi  antar 
seksi  yang  dapat  menyebabkan  perbedaan  data  mitra  di  tiap 
seksi, contohnya yaitu nama satu mitra tidak sama di tiap seksi. 
Hal  tersebut  tentunya  akan  menambah  pekerjaan  dalam 
pemilihan  mitra  untuk  mengikuti  suatu  kegiatan.  Selain  itu, 
terdapat mitra yang terlalu banyak mengikuti kegiatan secara 
bersamaan 
tidak 
menggunakan  satuan  O-B).  Terlalu  banyak  kegiatan  yang 
diikuti secara bersamaan dapat berpengaruh pada kualitas data 
yang  didapatkan.  Untuk  penilaian  kinerja  sendiri  belum 
memiliki sistem yang kriterianya jelas, sehingga penilaian yang 
dilakukan  adalah  secara  subjektif  atau  kebetulan  saja,  dan 
hasilnya pun kurang dapat dipertanggungjawabkan. 

ini  yaitu  kegiatan  yang 

(dalam  hal 

Dari  penjelasan  di  atas,  peneliti  akan  membangun  Sistem 
Informasi Manajemen dan Penilaian Kinerja Mitra. Manajemen 
akan  berfokus  pada  pengintegrasian  data  mitra,  memberikan 
daftar mitra yang tersedia dan memenuhi syarat sesuai aturan 
yang ada untuk diikutsertakan pada suatu kegiatan. Sedangkan 

 1 / 8 

 
 
 
 
penilaian  kinerja  dilakukan  pada  beberapa  kriteria  penilaian. 
Selanjutnya akan dilakukan penghitungan hasil penilaian yang 
dapat menghasilkan ranking dari mitra disuatu kegiatan. Serta 
terdapat  output  berupa  laporan  kinerja  mitra.  Diharapkan 
penelitian ini dapat membantu BPS Kabupaten Jember dalam 
menyelesaikan permasalahan yang berkaitan dengan mitra. 

II.  TUJUAN PENELITIAN 

Berdasarkan latar belakang, tujuan dilakukannya penelitian 
yaitu membangun sistem informasi manajemen dan penilaian 
kinerja  mitra  statistik  yang  mampu  mengintegrasikan  data 
mitra, memberikan daftar mitra yang tersedia untuk mengikuti 
kegiatan sesuai aturan, memberikan informasi kegiatan apa saja 
yang  diikuti  oleh  mitra,  mampu  melakukan  penilaian  kinerja 
mitra  berdasarkan  kriteria  yang 
telah  ditentukan  dan 
menghasilkan  laporan  penilaian  kinerja  dan  ranking  mitra  di 
tiap kegiatan. 

sistem 

III. PENELITIAN TERKAIT 
Terdapat beberapa penelitian terkait, yaitu: 
1.  Pembangunan  Sistem  Informasi  Penilaian  Kinerja  dan 
Rekomendasi  Mitra  BPS  Berbasis  Web  Menggunakan 
Metode Analytic Hierarchy Proccess yang dilakukan oleh 
Raden  Rara  Nuraziza  Rahmadhanty  dari  Politeknik 
Statistik STIS pada tahun 2019 [7]. Sistem informasi yang 
dihasilkan  dapat  melakukan  penilaian  terhadap  mitra 
pencacahan  maupun  pengolahan  dengan  indikator  yang 
dapat  dipertanggungjawabkan, 
juga  dapat 
memberikan  rekomendasi  berdasarkan  nilai  kualitas 
kinerja mitra yang dapat digunakan sebagai pertimbangan 
dalam  penentuan  alokasi  mitra  ke  kegiatan  tertentu. 
Persamaan dengan penelitian yang dilakukan yaitu sama-
sama membuat sistem untuk melakukan penilaian kinerja 
mitra.  Sedangkan  perbedaannya  yaitu  indikator  atau 
kriteria  dan  sistem  penilaian  yang  lebih  disesuaikan, 
metode yang digunakan, dan output yang akan dihasilkan, 
penelitian yang dilakukan oleh peneliti akan menghasilkan 
output berupa laporan kinerja mitra dan  ranking mitra di 
tiap  kegiatan.  Selain  itu  juga  terdapat  history  penilaian 
mitra  dan  beberapa  fitur  penunjang  yang  tidak  terdapat 
pada penelitian terkait. 
Information System of Performance Assesment on Startup 
Business  using  Simple  Multi-Attribute  Rating  Technique 
Exploiting Ranks (SMARTER) yang dilakukan oleh Alen 
Adriyanti  Tangkesalu  dan  Jatmiko  Endro  Suseno  pada 
tahun  2018  [11].  Pada  penelitian  tersebut  dilakukan 
penilaian  kinerja  pada  bisnis  startup  menggunakan  4 
perspektif balanced scorecard dengan 14 indikator kinerja 
dan  metode  Simple  Multi-Attribute  Rating  Technique 
Exploiting  Ranks  (SMARTER)  untuk  menghitung  bobot 
penilaian  dari  setiap  indikator.  Hasil  dari  penelitian 
tersebut  yaitu  penggunaan 
instrumen  diatas  dapat 
diterapkan  pada  proses  evaluasi  bisnis  startup  karena 
diperoleh  hasil  kinerja  tertinggi  dari  perspektif  proses 
tersebut 
internal  dan  pelanggan.  Penelitian 
bisnis 
menunjukkan bahwa metode SMARTER dapat digunakan 
untuk  melakukan 
penilaian. 
penghitungan 
Persamaan dengan penelitian yang dilakukan peneliti yaitu 

bobot 

2. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

sama-sama  menggunakan  metode  SMARTER  untuk 
penghitungan  bobot  hasil  penilaian  kinerja.  Sedangkan 
perbedaannya yaitu objek yang akan dilakukan penilaian. 

IV. METODE PENELITIAN  

A.  Ruang Lingkup Penelitian 

lingkup  dari  penelitian 

ini  adalah 
Ruang  Ruang 
pembangunan  sistem  informasi  manajemen  dan  penilaian 
kinerja  mitra  statistik  di  BPS  Kabupaten  Jember.  Mitra 
statistik  yang  tercakup  yaitu mitra  pencacahan.  Penelitian 
ini membangun sistem untuk mengintregasikan data mitra 
di  tiap  seksi  dan  mengalokasikan  petugas  untuk  tiap 
kegiatan sesuai aturan yang ada. Selain itu juga dilakukan 
pembangunan  sistem  untuk  melakukan  penilaian  kinerja 
mitra statistik yang memiliki  output laporan kinerja mitra 
dan  ranking  mitra  di  setiap  kegiatan.  Penilaian  kinerja 
berdasarkan  beberapa  kriteria  yang  dibentuk  berdasarkan 
wawancara dengan staf BPS Kabupaten Jember. 

B.  Metode Pengumpulan Data 

Metode  pengumpulan  data  yaitu  kegiatan  yang  dilakukan 
untuk mendapatkan informasi mengenai sistem yang yang 
akan  dibangun.  Dari  pengumpulan  data,  informasi  yang 
didapat digunakan untuk mencari solusi dari permasalahan. 
Metode yang dilakukan yaitu: 
1.  Studi Pustaka 

Studi  pustaka  yaitu  metode  dalam  pengumpulan  data 
dengan  melakukan  analisis  terhadap  buku,  literatur, 
catatan,  dan 
laporan  yang  berkaitan  dengan 
permasalahan.  Hasil  dari  pengumpulan  data  dengan 
studi pustaka yaitu  landasan teori dan panduan untuk 
membangun sistem informasi. 

2.  Wawancara 

Wawancara  merupakan  suatu  metode  pengumpulan 
data  yang  dilakukan  dengan  bertanya  kepada 
narasumber, baik secara langsung maupun secara tidak 
langsung.  Wawancara  yang  dilakukan  yaitu  semi 
terstruktur, 
pewawancara  menanyakan 
mengenai permasalahan hanya secara  garis besar dan 
banyak  mendengarkan  pendapat  dan  ide-ide  dari 
narasumber.  

dimana 

3.  Kuesioner 

Kuesioner  yang  terdiri  dari  beberapa  pertanyaan. 
Kuesioner yang diberikan yaitu System Usability Scale 
(SUS),  yang  memiliki  tujuan  untuk  mengetahui  dan 
mengevaluasi kelayakan sistem yang dikembangkan. 

C.  Metode Pembangunan Sistem 

Metode  pembangunan  sistem 
informasi  yang  akan 
digunakan  adalah  metode  SDLC  Waterfall.  Metode 
Waterfall  merupakan  suatu  metode  pembangunan  sistem 
yang  setiap 
tahapannya  dilakukan  secara  berurutan, 
tahapannya  dimulai  dari  requirements  analysis,  system 
design,  implementation,  integration  and 
testing,  dan 
operation and maintenance [10]. Metode ini dipilih karena 
subject matter telah menguasai rancangan sistem yang akan 

 2 / 8 

 
 
 
 
dikembangkan,  sehingga  kemungkinan  perubahan  desain 
ditengah pengerjaan kecil. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 1. Tahapan metode waterfall 

1.  Requirement Analysis 

Pada tahap ini dilakukan pengumpulan kebutuhan dari 
sistem  yang 
akan  dibangunan.  Pengumpulan 
kebutuhan  dilakukan  dengan  studi  literatur  dan 
wawancara,  wawancara  dilakukan  terhadap  kasub 
bagian umum, kasi, dan staf BPS Kabupaten Jember. 
Selanjutnya akan dibentuk dokumen kebutuhan sistem. 

2.  System Design 

3. 

4. 

Pada tahapan ini, dilakukan penerjemahan dari tahap 
sebelumnya, berfokus pada proses bisnis, aliran data, 
basis data, dan tampilan antar muka. 
Implementation 
Tahap  ini  merupakan  penerjemahan  dari  tahapan 
desain, 
bahasa 
diterjemahkan  menggunakan 
pemograman  yang 
telah  ditentukan.  Termasuk 
implementasi  basis  data  dan  tampilan  antar  muka. 
Bahasa  pemograman  utama  yang  digunakan  yaitu 
PHP  dengan  framework  CodeIgniter.  DBMS  yang 
digunakan  yaitu  MySQL.  Template  yang  digunakan 
yaitu SB Admin 2. 
Integration and Testing 
Selanjutnya  dilakukan  pengujian  terhadap  sistem, 
pada 
testing. 
Blackbox testing berdasarkan output yang dihasilkan 
setelah  melakukan  input,  sehingga  tidak  melihat 
itu  bisa  dihasilkan. 
bagaimana  proses  output 
Pengujian  ini  dilakukan  untuk  mengetahui  apakah 
sistem telah berjalan sesuai dengan yang diinginkan. 
Untuk evaluasi, menggunakan System Usability Scale 
(SUS),  kuesioner  SUS  akan  diberikan  kepada 
pengguna  untuk  mengetahui  kelayakan  sistem  yang 
dikembangkan. 

ini  menggunakan  blackbox 

tahap 

5.  Operation and Maintenance 

Sistem  yang 
telah  dijalankan  perlu  dilakukan 
pemeliharaan agar sistem yang dijalankan tetap stabil. 

V.  KERANGKA PIKIR 

Untuk menjelaskan alur penelitian, dibuatlah kerangka pikir 

yang dapat digambarkan pada diagram berikut. 

Gambar 2. Kerangka pikir 
diawali 
dilakukan 

ini 

dengan 

Penelitian 

adanya 
permasalahan  yang  berkaitan  dengan  mitra  statistik,  yaitu 
adanya perbedaan data mitra di setiap seksi, berlebihnya beban 
kerja yang didapatkan oleh satu atau beberapa mitra (dalam hal 
ini  yang  dimaksud  yaitu  banyaknya  kegiatan  yang  diikuti 
secara bersamaan), belum adanya sistem penilaian kinerja yang 
memiliki  kriteria  yang  jelas.  Solusi  dari  permasalahan  yaitu 
membangun sistem informasi manajemen dan penilaian kinerja 
mitra statistik.  

Penelitian  dilakukan  dengan  pendekatan  metode  System 
Development  Life  Cycle  (SDLC)  Waterfall.  Alat-alat  yang 
digunakan  yaitu  PIECES,  Ishikawa  Diagram,  Data  Flow 
Diagram (DFD), Use Case Diagram, Activity Diagram, Entity-
Relationship  Diagram  (ERD).  Hasil  yang  diharapkan  yaitu 
terbangunnya  sistem  informasi  manajemen  dan  penilaian 
kinerja  mitra  statistik.  Dan  pada  akhir  penelitian  akan 
dilakukan  evaluasi  menggunakan  Black  Box  Testing  dan 
System Usability Scale. 

VI. HASIL DAN PEMBAHASAN 

A.  Sistem Penilaian 

Berdasarkan diskusi dengan subject matter, dihasilkan 
beberapa kriteria untuk penilaian kinerja mitra dan bentuk 
penilaiannya.  Kriteria  yang  dihasilkan  dianggap  dapat 
mewakili dari kinerja dari mitra, dan bentuk penilaian yang 
digunakan  dapat  dengan  mudah  untuk  diterapkan,  serta 
output dari hasil penilaian mudah dibaca dan dipahami oleh 
mitra. Terdapat 10 kriteria yang dihasilkan dengan bentuk 
penilaian skala 1-5. 

Selanjutnya untuk memperoleh nilai akhir mitra, akan 
dilakukan  penghitungan  dengan  pembobotan  pada  setiap 
kriteria.  Metode  yang  digunakan  yaitu  Simple  Multi-
Attribute Rating Technique Exploiting Rank (SMARTER). 
Langkah-langkahnya sebagai berikut. 
a.  Mengidentifikasi masalah. 
b.  Menetapkan kriteria dan subkriteria. 
c.  Memberi  peringkat  untuk  masing-masing  kriteria  dan 

subkriteria. 

d.  Hitung  bobot  kriteria  dan  subkriteria  menggunakan 

bobot Rank Order Centroid (ROC). 
Secara umum pembobotan ROC dapat dirumuskan pada 
persamaan: 

 3 / 8 

 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

C.  Analisis Masalah 

permasalahan.  Dalam 

Berdasarkan  analisis  sistem  berjalan,  dapat  identifikasi 
ini, 
beberapa 
menggunakan  diagram  ishikawa  atau  fishbone  untuk 
memvisualisasikan permasalahan yang akan disajikan pada 
gambar dibawah ini.  Permasalahan dikategorikan menjadi 
5  kategori  yaitu  man,  machine,  material,  method,  dan 
measurement. 

penelitian 

𝑊𝑘 =

1

𝐾

𝐾
∑ (
𝑖=𝑘

1

𝑖

)

(1) 

Keterangan: 
Wk : nilai bobot kriteria ke-k  
K   : 1, 2, … ,K 
K   : jumlah kriteria 
i     : nilai alternatif. 

e.  Untuk mendapatkan nilai akhir, dilakukan penjumlahan 
dari  bobot  kriteria  dikalikan  dengan  nilai  utility.  Dari 
hasil  penjumlahan  tersebut,  diurutkan  dari  terbesar  ke 
terkecil  untuk  mendapatkan  ranking  mitra.  Rumus 
untuk  menghitung  bobot  akhir  dapat  dilihat  pada 
persamaan: 

𝑈ℎ = ∑ 𝑊𝑘𝑈ℎ(𝑋ℎ𝑘)

𝐾
𝑘=1

(2) 

Keterangan: 
H 

K 
K 

𝑈ℎ 
𝑊𝑘 
𝑈ℎ(𝑋ℎ𝑘) 

: indeks untuk mengidentifikasi objek 
  evaluasi (dalam penelitian ini yaitu mitra), 
  dimana h = 1, 2, …, H. 
: jumlah kriteria 
: indeks untuk mengidentifikasi kriteria, 
  dimana k = 1, 2, …, K. 
: nilai akhir 
: bobot dari kriteria ke-k 
: nilai utility kriteria ke k untuk alternatif ke-
  k 

Berikut persamaan untuk menghitung nilai utility: 

𝑈𝑛(𝑋𝑛𝑘) = 100% 𝑥 

(𝐶𝑖−𝐶𝑚𝑖𝑛)
(𝐶𝑚𝑎𝑥−𝐶𝑚𝑖𝑛)

(3) 

Dimana, 
𝐶𝑖 
𝐶𝑚𝑖𝑛 
𝐶𝑚𝑎𝑥 

: Nilai kriteria ke-i 
: Nilai kriteria minimal 
: Nilai kriteria maksimal 

B.  Analisis Sistem Berjalan 

Secara  garis  besar,  berikut  proses  bisnis  sistem  yang 
berjalan. Penelitian akan berfokus pada tahap yang dibatasi 
dengan garis berwarna biru. 

Information 

Gambar 4. Analisis masalah sistem berjalan 

D.  Analisis Kebutuhan 

Analisis  kebutuhan  sistem  dilakukan  berdasarkan  hasil 
wawancara kepada subject matter. Berdasarkan wawancara 
terdapat beberapa permintaan mengenai sistem yang akan 
dikembangkan.  Pada  penelitian  ini,  analisis  kebutuhan 
sistem  akan  menggunakan  PIECES,  yang  terdiri  dari 
Performance,  Information,  Economy,  Control,  Efficiency, 
dan Services. 

TABEL I 
TABEL ANALISIS KEBUTUHAN 

Faktor 

Sistem berjalan 

Performance  Kinerja  mitra 

tidak 
terukur  dan  masih 
menggunakan  mitra 
sebenarnya 
yang 
kurang 
berkinerja 
baik. 
Belum  ada  informasi 
berupa  laporan/report 
mengenai 
kinerja 
mitra  baik  di  setiap 
kegiatan 
maupun 
secara keseluruhan. 

Economy 

Control 

Efficiency 

Services 

Membutuhkan  waktu 
memeriksa 
untuk 
boleh  atau 
tidaknya 
mitra  ikut  ke  suatu 
kegiatan. 
Belum  terintegrasinya 
data mitra antar seksi. 
memeriksa 
Masih 
secara 
manual 
mengenai  boleh  atau 
tidaknya  mitra ikut  ke 
suatu kegiatan. 
- 

Sistem usulan 
Adanya  sistem  informasi 
penilaian  kinerja  mitra 
sehingga  mitra 
yang 
berkinerja  kurang  baik 
dapat terdeteksi. 

tersedia 

Kinerja  mitra 
dinilai 
setiap  kegiatan,  sehingga 
akan 
laporan 
yang  dapat  digunakan 
untuk  evaluasi,  serta  ada 
informasi 
tambahan 
berupa  ranking  mitra  di 
setiap kegiatan. 
Sistem  sudah  otomatis 
mengikuti peraturan yang 
ada, sehingga lebih cepat 
mengetahui  ketersediaan 
mitra. 
Data  mitra  antar  seksi 
akan terintegrasi. 
Sistem  secara  otomatis 
memvalidasi  mitra  yang 
tersedia 
berdasarkan 
peraturan yang berlaku. 

- 

Gambar 3. Proses bisnis sistem berjalan 

E.  Proses Bisnis Sistem Usulan 

Berikut  proses  bisnis  sistem  usulan,  dimana  hanya 
menggambarkan  sistem  utama,  yaitu  manajemen  dan 
penilaian  kinerja  mitra.  Sebenarnya  terdapat  4  aktor  pada 
sistem, yaitu Superadmin, Operator Seksi, Pengawas, Mitra, 
tetapi  karena  fitur  untuk  Superadmin  tidak  berhubungan 

 4 / 8 

 
 
 
 
   
   
 
 
 
 
 
 
 
 
dengan sistem utama maka tidak digambarkan pada gambar 
proses bisnis usulan. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

G.  Activity Diagram 

Proses  bisnis  dari  suatu  aktivitas  digambarkan  melalui 
activity  diagram,  berikut  beberapa  activity  diagram  dari 
sistem utama. 

1.  Tambah 

2.  Tambah 

master 
data mitra 

master 
data pegawai 

3.  Tambah 
kegiatan 

Gambar 5. Proses bisnis sistem usulan 

F.  Use Case Diagram 

Use case diagram menggambarkan aktor dan aktivitas yang 
dapat dilakukannya. 

Gambar 7. Aktivitas 
tambah master data 
mitra  

Gambar 8. Aktivitas 
tambah master data 
mitra 

Gambar 9. Aktivitas 
tambah kegiatan 

4.  Tambah pencacah 

5.  Tambah pengawas 

Gambar 10. Aktivitas tambah 
pencacah    

Gambar 11. Aktivitas ambah 
pengawas 

6.  Plotting pencacah tiap pengawas 

Gambar 6. Use case diagram 

Gambar 12. Aktivitas plotting pencacah tiap pengawas    

 5 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
    
   
 
 
              
 
 
 
 
 
7.  Lihat jadwal mitra 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 13. Aktivitas lihat jadwal mitra    

8.  Mengisi 
penilaian 

9.  Lihat ranking 
mitra tiap 
kegiatan 

Gambar 17. Entity relationship diagram 

I.  Implementasi Tampilan 

1.  Halaman tambah master data mitra 

lainnya  dapat  dilakukan  oleh 

tambah  master  data  mitra  dan  aktivitas 
Untuk 
mengelola 
role 
superadmin  dan  operator  seksi.  Menambahkan  data 
mitra  dapat  dilakukan  secara  satu  per  satu  dan  batch 
dengan mengimpor file excel. Template file excel telah 
disediakan dan dapat diunduh. 

Gambar 18. Halaman tambah data mitra 

2.  Halaman tambah master data pegawai 

Untuk  tambah  master  data  pegawai  dan  aktivitas 
mengelola 
role 
superadmin  dan  operator  seksi.  Menambahkan  data 
pegawai hanya bisa dilakukan satu per satu. 

lainnya  dapat  dilakukan  oleh 

Gambar 14. Aktivitas mengisi 
penilaian    

Gambar 15. Aktivitas lihat 
ranking mitra tiap kegiatan    

10.  Unduh hasil penilaian 

Gambar 16. Aktivitas unduh hasil penilaian    

H.  Rancangan Database 

Rancangan 
Relationship Diagram berikut. 

database 

digambarkan  melalui  Entity 

Gambar 19. Halaman tambah data pegawai 

3.  Halaman tambah kegiatan 

Untuk menambahkan kegiatan hanya dapat dilakukan 
oleh  role  operator  seksi.  Kegiatan  yang  ditambahkan 
operator  seksi  tertentu  tidak  akan  muncul  pada  seksi 
lainnya. 

 6 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 20. Halaman tambah kegiatan 

4.  Halaman tambah pencacah 

Gambar 24. Halaman pengawas terpilih 

7.  Halaman plotting pencacah tiap pengawas 

Halaman 
ini  hanya  berisi  daftar  mitra,  karena 
penelitian  ini  memiliki  batasan  untuk  menilai  mitra. 
Tersedia  juga  halaman  untuk  melihat  mitra  yang 
terpilih menjadi pencacah. 

Plotting ini berpengaruh pada role pengawas, sehingga 
untuk  melakukan  penilaian,  hanya  tersedia  daftar 
pencacah yang telah di-plot dengan pengawas tersebut. 
Tersedia  juga  halaman  untuk  melihat  pencacah  yang 
terpilih. 

Gambar 21. Halaman tambah pencacah 

5.  Halaman lihat jadwal mitra 

Halaman  ini  dapat  menampilkan  jadwal  mitra  dalam 
bentuk timeline dan juga tabel. 

Gambar 25. Halaman plotting pencacah tiap pengawas 

8.  Halaman isi penilaian 

Penilaian  hanya  dapat  dilakukan  oleh  role  pengawas, 
terdiri dari 10 kriteria penilaian dengan skala 1 s/d 5. 

Gambar 22. Halaman lihat jadwal mitra 

6.  Halaman tambah pengawas 

Pengawas dapat berasal dari pegawai dan mitra, tetapi 
lebih  diutamakan  pegawai.  Sehingga  un tuk  tambah 
pengawas, secara default menampilkan daftar pegawai, 
dan  tersedia  button  untuk  menambahkan  dari  daftar 
mitra  serta  disediakan  halaman  untuk  melihat 
pengawas terpilih. 

Gambar 26. Halaman isi penilaian 

9.  Halaman unduh hasil penilaian 

Halaman ini dapat dilakukan oleh role operator seksi, 
pengawas  dan  mitra.  Untuk  operator  seksi  dapat 
mengunduh  hasil  penilaian  dari  mitra  yang  ikut 
kegiatannya, untuk pengawas hanya dapat mengunduh 
hasil  penilaian  pencacah  yang  dinilainya,  dan  untuk 
mitra hanya bisa mengunduh nilai diri sendiri sebagai 
pencacah. 

Gambar 23. Halaman tambah pengawas 

 7 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
Gambar 27. Halaman unduh hasil penilaian 

10.  Halaman lihat ranking mitra di suatu kegiatan 

Halaman  ini  hanya  dapat  diakses  oleh  role  operator 
seksi. Hasil ranking dapat dilihat jika seluruh penilaian 
telah  dilakukan  disuatu  kegiatan.  Untuk  perhi tungan 
ranking, menggunakan metode SMARTER. 

Gambar 28. Halaman lihat ranking mitra di suatu kegiatan 

J.  Pengujian 

Pengujian sistem dilakukan dengan  Black box testing dan 
System Usability Scale (SUS). 
1.  Black box testing 

Pengujian  dengan  black  box  testing  bertujuan  untuk 
menguji fungsi-fungsi yang telah dibuat telah berhasil. 
Dari pengujian ini didapatkan bahwa setiap fungsi telah 
berhasil dijalankan dengan hasil sesuai harapan. 

2.  System Usability Scale (SUS)  
TABEL II 
TABEL HASIL PENGUJIAN SUS 
Pertanyaan 

Responden 

1 

7,5 

10 

7,5 

7,5 

10 

5 

10 

2 

7,5 

7,5 

5 

7,5 

7,5 

7,5 

5 

3 

10 

10 

10 

10 

7,5 

7,5 

7,5 

4 

7,5 

7,5 

2,5 

7,5 

2,5 

7,5 

2,5 

5 

7,5 

10 

7,5 

7,5 

10 

7,5 

7,5 

6 

7,5 

10 

5 

7,5 

7,5 

7,5 

7,5 

7 

7,5 

10 

7,5 

2,5 

7,5 

5 

7,5 

8 

10 

10 

7,5 

10 

7,5 

7,5 

10 

9 

7,5 

10 

7,5 

7,5 

7,5 

7,5 

2,5 

10 

10 

7,5 

7,5 

10 

7,5 

5 

2,5 

1 

2 

3 

4 

5 

6 

7 

Total 

82,5 

92,5 

67,5 

77,5 

75 

67,5 

62,5 

Rata-rata 

8,21 

6,79 

8,93 

5,36 

8,21 

7,50 

6,79 

8,93 

7,14 

7,14 

75,00 

Min 

Max 

5 

5 

7,5 

2,5 

7,5 

5 

2,5 

7,5 

2,5 

2,5 

62,5 

10 

10 

10 

10 

10 

10 

10 

7,5 

7,5 

92,5 

10 
Pengujian  dengan  SUS  bertujuan  untuk  mengetahui 
kelayakan  sistem  yang  dibangun,  apakah  dapat 
diterima oleh pengguna atau tidak. SUS terdiri dari 10 
pertanyaan  dengan  skala 
jawaban  1  sampai  5. 
Responden dari SUS ini berjumlah 7 orang [1], yang 
terdiri  dari  pegawai  dan  mitra  di  BPS  Kabupaten 
Jember. Skor yang didapatkan dari SUS ini sebesar 75, 
yang berarti di rentang 71,4 – 85,5, menunjukkan jika 
sistem  yang  di  rancang  masuk  kategori  good  [5]  dan 
dapat diterima oleh pengguna. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

A.  Kesimpulan 

VII. 

PENUTUP 

Berdasarkan  hasil  penelitian  yang  telah  dilakukan,  dapat 
ditarik beberapa kesimpulan sebagai berikut. 
1.  Telah  berhasil  dikembangkan 

informasi 
manajemen dan penilaian kinerja mitra statistik di BPS 
Kabupaten Jember. 

sistem 

2.  Hasil pengujian dengan black box testing, semua fitur 
atau fungsi pada sistem telah berhasil dijalankan sesuai 
dengan yang diharapkan. 

3.  Hasil  evaluasi  dengan  System  Usability  Scale  (SUS) 
mendapatkan nilai rata-rata 75,00 yang berarti sistem 
dapat diterima dengan baik oleh pengguna 

B.  Saran 

Dari  penelitian  yang  telah  dilakukan,  terdapat  beberapa 
saran  dari  peneliti  yang  dapat  digunakan  untuk  penelitian 
selanjutnya. 
1.  Perlu 

untuk 
penelitian 
menentukan  kriteria  penilaian  kinerja  mitra  statistik 
sehingga  terdapat  standar  yang  dapat  digunakan  oleh 
seluruh BPS kabupaten/kota.  

dilakukan 

tersendiri 

2.  Penilaian  kinerja  dilakukan  tidak  hanya  pada  mitra 

pencacahan. 

3.  Berdasarkan  pengujian  SUS,  item  pernyataan  yang 
mendapatkan  skor 
terendah  yaitu  diperlukannya 
bantuan  teknis  untuk  menggunakan  sistem,  sehingga 
diperlukan  sosialisasi  mengenai  sistem  sebelum 
diterapkan. 

DAFTAR PUSTAKA 
[1]  Alroobaea,  R.,  and  Mayhew,  P.  J.  (2014).  How  many  participants  are 
really  enough  for  usability  studies  ?.  Proceedings  of  2014  Science  and 
Information Conference. SAI 2014. 48–56. 

[2]  Badan Pusat Statistik. Peraturan Kepala Badan Pusat Statistik Nomor 102 
Tahun 2020 tentang Harga Satuan Pokok Kegiatan untuk Pegawai Negeri 
Sipil Tahun Anggaran 2021 Pasal 1 Ayat 12. 

[3]  Badan  Pusat  Statistik.  Buku  3  Manajemen  Lapangan  Survei  Sosial 
Ekonomi Nasional (SUSENAS) Modul Kesehatan dan Perumahan 2019. 
Jakarta: Badan Pusat Statistik. 

[4]  Badan Pusat Statistik Provinsi Jawa Timur. Acuan Anggaran Jawa Timur 
Tahun Anggaran 2021: Penjelasan POK Program PPIS Tahun Anggaran 
2021. Surabaya: Badan Pusat Statistik Provinsi Jawa Timur. 

[5]  Bangor, A., Kortum, P. and Miller, J. (2009). Determining What Individual 
SUS Scores Mean: Adding an Adjective Rating Scale. Journal of Usability 
Studies, 4(3), hal. 114–123. 

[6]  Priyono  dan  Marnis.  Manajemen  Sumber  Daya  Manusia.  Sidoarjo: 

Zifatama Publisher. 

[7]  Rahmadhanty,  Raden  Rara  Nuraziza.  (2019).  Pembangunan  Sistem 
Informasi Penilaian Kinerja dan Rekomendasi Mitra BPS Berbasis Web 
Menggunakan  Metode  Analytic  Hierarchy Processs  [SKRIPSI].  Jakarta: 
Politeknik Statistika STIS. 

[8]  Republik  Indonesia.  Peraturan  Presiden  Nomor  86  Tahun  2007  tentang 
Badan Pusat Statistik dan Peraturan Kepala Badan Pusat Statistik Nomor 
116 Tahun 2014 tentang Organisasi dan Tata Kerja Badan Pusat Statistik. 
[9]  Republik  Indonesia.  Undang-Undang  Nomor  16  Tahun  1997  tentang 

Statistik. 

[10] Royce, Winson W. Managing The Development of Large Software Systems: 
Concepts and Techniques. Proceedings of the 9th International Conference 
on Software Engineering (pp. 328-338). 

[11] Tangkesalu,  A.  A.,  &  Suseno,  J.  E.  (2018).  Information  System  of 
Performance Assesment on Startup Business using Simple Multi-Attribute 
Rating  Technique  Exploiting  Ranks  (SMARTER).  In E3S  Web  of 
Conferences (Vol. 73). EDP Sciences. 

 8 / 8 

 
 
 
 
 
 
 
 
"
221709776,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pengembangan Metode Geoadditive Small Area 
Model Pada Software R 

Ketut Karang Pradnyadika (221709776, 4SD1) 
Dosen Pembimbing: Ika Yuni Wulansari, SST., M.Stat 

Ringkasan— Metode  pengumpulan  data yang  dilakukan BPS 
masih  berfokus  pada  sensus  dan  survei.  Namun  kenyataannya 
data  yang  dikumpulkan  memiliki  keterbatasan  dalam  estimasi. 
Untuk  melakukan  estimasi  level  area  kecil,  solusinya  dengan 
mengestimasi  tidak 
langsung  dengan  metode  Small  Area 
Estimation  (SAE).  Metode  SAE  memerlukan  variabel  penyerta 
yang  tersedia  hingga  level  yang  hendak  diestimasi.  Model  SAE 
merupakan model campuran dengan varians dalam subpopulasi 
dapat dijelaskan dengan efek tetap dan acak. Asumsi yang harus 
terpenuhi dalam estimasi model mixed linear adalah normalitas. 
Namun banyak data yang ditemukan tidak berdistribusi normal, 
sehingga  diperlukan  metode  alternatif  yang  dapat  digunakan 
sebagai  solusi  dalam  estimasi  statistik  area  kecil,  yaitu 
Geoadditive  Model.  Dalam  penerapannya,  metode  ini  belum 
dikembangkan 
ingin 
mengembangkan Geoadditive Small Area Model  pada software R 
dalam  melakukan  estimasi  pada  area  kecil.  Pengembangan 
metode 
ini  menghasilkan  suatu  package  R  dengan  nama 
‘geoSAE’,  yang  dapat  digunakan  sebagai  analisis  SAE  dengan 
metode  Geoadditive,  sehingga  dapat  digunakan  oleh  pengguna 
pada berbagai data yang memiliki keterbatasan asumsi statistik.  
Kata  Kunci—  Small  Area  Estimation,  Geoadditive  model, 

secara  maksimal, 

sehingga  peneliti 

Software R 

I.  LATAR BELAKANG 

Selama  ini,  metode  pengumpulan  data  yang  dilakukan 
BPS  sering  dilakukan  dengan  sensus  dan  survei.  Sensus 
mengkaji seluruh populasi sehingga data yang disajikan dalam 
sensus sampai  satuan  wilayah terkecil. Namun, pengumpulan 
data  dengan  sensus  memiliki  keterbatasan  pada  variabelnya, 
dan juga  membutuhkan  waktu, tenaga, dan biaya  yang besar. 
Di  sisi  lain,  kekurangan  tersebut  dapat  diatasi  dengan  survei, 
dimana  survei  dapat  mencakup  banyak  variabel  serta  lebih 
tenaga,  dan  biaya.  Meskipun 
efisien  dari  segi  waktu, 
demikian,  survei  memiliki  keterbatasan  dalam  estimasi  yang 
disebabkan  permasalahan  kecukupan 
sampel.  Apabila 
melakukan  estimasi  untuk  tingkat  administrasi  di  bawah 
provinsi  (area  kecil)  akan  memiliki  nilai  standard  error  yang 
besar  sehingga 
tidak  dapat 
diandalkan.  Maka  dari  itu,  dibutuhkan  suatu  pendugaan 
dengan  metode  tidak  langsung  yang  dapat  memberikan  hasil 
estimasi parameter pada level unit kecil dengan standar error 
yang  kecil  dan  jumlah  sampel  yang  terbatas.  Salah  satu 
metode  tidak  langsung  dapat  dijadikan  solusi  adalah  Small 
Area  Estimation  (SAE),  yaitu  metode  pendugaan  tidak 
langsung  yang  “meminjam  kekuatan”  dari  informasi  wilayah 

statistik  yang  dihasilkan 

yang  bersesuaian  atau  informasi  variabel  penyerta  yang 
mempunyai hubungan kuat dengan variabel amatan [1]. 

Model  SAE  merupakan  model  mixed  linear  dengan 
keragaman  dalam  subpopulasi  dapat  dijelaskan  dengan  efek 
tetap dan acak [2]. Efek tetap diterangkan oleh keragaman dari 
peubah  penyerta,  sedangkan  efek  acak  diterangkan  oleh 
keragaman  khusus  yang  tidak  dapat  diterangkan  oleh  peubah 
penyerta. Peneliti pertama yang mengembangkan statistik area 
kecil  berdasarkan  model  campuran  linear  dengan  Empirical 
Best  Linear  Unbiased  Prediction  (EBLUP)  sebagai  metode 
solusi  adalah  Fay  dan  Herriot  (1979)  [3].  Metode  EBLUP 
membutuhkan  asumsi  normalitas,  namun  pada  kenyataannya 
banyak ditemukan bahwa data tidak berdistribusi normal.  

area 

kecil 

dengan 

statistik 

Dengan  terlanggarnya  asumsi  tersebut,  maka  dibutuhkan 
suatu  pendekatan  semi  ataupun  nonparametrik.  Salah  satu 
metode  alternatif  yang  dapat  digunakan  sebagai  solusi  dalam 
pendekatan 
estimasi 
semiparametrik  adalah  Geoadditive  Model.  Metode 
ini 
merupakan metode penggabungan model kriging dan additive 
yang  disajikan  dalam  bentuk  model  mixed  linear.  Model  ini 
memiliki  fleksibilitas  dalam  menentukan  bentuk  hubungan 
antara  respon  dengan  informasi  spasial,  karena  merupakan 
fungsi  pemulusan.  Karena  teknik  pendugaan  area  kecil 
tetangga  sehingga  SAE 
“meminjam  kekuatan”  di  area 
dikembangkan dengan  mengintegrasikan informasi  spasial  ke 
dalam model, yang dikenal sebagai SAE spasial (spasial Fay-
Herriot). Baik model Fay-Herriot maupun model spasial Fay-
Herriot  memerlukan  pemenuhan  asumsi  linearitas  kovariat 
serta  normalitas  distribusi  respons  yang  terkadang  dilanggar, 
maka  Geoadditive  Model  menawarkan  penanganan  tersebut 
dengan menggunakan fungsi smoothing [2]. 

ini, 

teknologi  saat 

Dengan  perkembangan 

terdapat 
beberapa  perangkat  lunak  (software)  yang  dapat  digunakan 
untuk analisis SAE, salah satunya adalah R. Pada software ini, 
belum  terdapat  package  dengan  metode  Geoadditive  Small 
Area,  sehingga  pengguna  harus  mengetikkan  kode  script 
dalam  melakukan  estimasi.  Hal  ini  akan  berdampak  pada 
ketidak efisiensinya pengolahan data  dan pada akhirnya akan 
menguras banyak waktu dan tenaga.  

Berdasarkan permasalahan di atas, pada penelitian ini akan 
dilakukan  pengembangan  metode  Geoadditive  Small  Area 
Model pada software R.   

1 / 8 

 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

TABEL I 
TABEL LITERATUR 
Penulis, 
Publikasi 

Tertulis 

Komentar 

II.  TUJUAN PENELITIAN 

Berdasarkan  latar  belakang  yang  telah  dijelaskan,  maka 

tujuan dari penelitian ini adalah sebagai berikut. 

No 

Judul 

1.  Melakukan  simulasi  data  dengan  script  R  Geoadditive 

Small Area Model yang dibangun. 
package 

2.  Membangun 

sebuah 

dengan  metode 

Geoadditive Small Area Model  pada software R. 

3.  Menguji performa dari pembangunan package R dengan 
metode  Geoadditive  Small  Area  Model  yang  telah 
dibangun menggunakan data publikasi dari BPS sebagai 
studi kasus. 

III. PENELITIAN TERKAIT 

Kammann  dan  Wand  (2003)  merupakan  peneliti  pertama 
yang  mengemukakan  metode  geoadditive  [5],  dimana  studi 
kasus  yang  dilakukan 
tentang  kesehatan  reproduksi  di 
Amerika Serikat. Hasilnya menunjukkan hubungan non-linier 
dari variabel respon sehingga menggabungkan kriging dengan 
model  aditif.  penggabungan  model  tersebut  disebut  dengan 
Geoadditive  Model.  Terdapat  beberapa  peneliti  yang  sudah 
melakukan  penelitian  dengan  metode  Geoadditive,  seperti 
Kandala,  et.al  (2009)  untuk  memodelkan  kekurangan  gizi 
anak-anak 
(2011)  untuk 
memodelkan penyakit malaria [9]. 

[8],  dan  Nkurunziza,  et.  al 

Djuraidah  dan  Aunuddin  (2006)  melakukan  penelitian 
menggunakan  metode  kriging  dan  spline-2  pada  data 
pencemaran  ozon  [10].  Kriging  dan  spline-2  dinyatakan 
sebagai  kombinasi  linear  dari  fungsi  basis  radial,  sehingga 
kedua  model  tersebut  dapat  didekati  dengan  model  linear 
campuran.  Hasilnya  menunjukkan  prediksi  spasial  dengan 
kriging  menghasilkan  kurva  permukaan  yang  kasar, 
sedangkan  spline-2  menghasilkan  kurva  permukaan  yang 
halus.  

Salvati,  et.  al  (2008)  melakukan  estimasi  area  kecil 
menggunakan  direct  estimator  berbasis  model  nonparametrik 
[11].  Konsep  estimasi  langsung  berbasis  model  digunakan 
untuk  mengembangkan  pendekatan  nonparametrik  alternatif 
untuk 
(2009) 
menggunakan  pendekatan  SAE  dengan  nonparametrik, 
dimana menggabungkan konsep spline dan model SAE karena 
dapat dipandang sebagai model dengan efek acak [4]. 

area  kecil.  Kurnia 

rata-rata 

estimasi 

1 

Small Area 
Estimation, 2nd 
Edition 

J.N.K. Rao and 
Isabel Molina, 
New Jersey: 
John Wiley and 
Sons Inc. 

2 

Small Area 
Estimation Using 
A Nonparametric 
Model-Based 
Direct Estimator 

Salvati, N., 
Chandra, H., 
Ranalli, M. G. 
and Chambers, 
R. (2008).  
Submitted for 
publication. 

3 

Geoadditive 
Models in Small 
Area Estimation 
of Poverty 

Novi Hidayat 
Pusponegoro, 
Anik Djuraidah, 
Anwar 
Fitrianto, I 
Made 
Sumertajaya, 
Journal of Data 
Science and Its 
Applications, 
VOL. 2, No. 1, 
PP.11-18, 
January 2019 

konsumsi 

pengeluaran 

di  Albania 

Kemudian  pada  tahun  2010,  Bocci  melakukan  penelitian 
tentang 
dengan 
menggunakan  Geoadditive  Small  Area  Model,  yang  hasilnya 
menunjukkan  bahwa  model 
ini  dapat  diterapkan  untuk 
memperkirakan  rata-rata  tingkat  distrik  dari  pengeluaran 
konsumsi  per  kapita  log  rumah  tangga  untuk  Republik 
Albania [6]. 

Di  Indonesia,  terdapat  beberapa  permasalahan  yang  dapat 
diukur  dengan  model  Geoadditive  Small  Area  Model,  seperti 
Ardiansyah,  et.  al.  (2018)  pernah  menggunakan  model  ini 
untuk mengestimasi produktivitas padi di Kabupaten Seruyan 
[7], dan Pusponegoro, et. al. (2019) menggunakan metode ini 
untuk  menghitung  kemiskinan  di  Provinsi  Bangka  Belitung 
[2]. 

4 

5 

The Best 
Predictions for 
Empirical 
Logarithm 
Transformation 
Models in Small 
Area Estimation 
with Application 
on SUSENAS 
Data 

Kurnia, A. 
(2009), 
Doctoral 
dissertation-Ph. 
D dissertation, 
Bogor 
Agricultural 
University 

In the context of 
small area 
estimation, direct 
estimators lead to 
unacceptably 
large standard 
errors for areas 
with unduly small 
sample sizes; in 
fact, no sample 
units may be 
selected from 
some small 
domains, 

Therefore, it is 
often necessary to 
use indirect 
estimates that 
“borrow strength” 
by using values of 
the variables of 
interest from 
related areas, thus 
increasing the 
“effective” sample 
size,  

Nonparametric 
regression is 
widely used as a 
method of 
characterizing a 
non-linear 
relationship 
between a variable 
of interest and a 
set of covariates. 

Since, the Fay-
Herriot model and 
the spatial Fay-
Herriot model 
require the 
fulfillment of 
covariate linearity 
assumption as well 
as the normality of 
the response 
distribution that is 
sometimes 
violated and the 
Geoadditive model 
offers those 
handling using the 
smoothing 
function,  

The lognormal 
empirical best 
prediction (EBP) 
model which are 
suggested in this 
research give 
better results than 
standard SAE 
models in terms of 
the smallest 
relative root mean 
square error 
(RRMSE). 

Melakukan estimasi 
langsung dengan 
keterbatasan sampel 
akan menyebabkan 
besarnya nilai 
standar error 

Metode pendugaan 
tidak langsung yang 
“meminjam 
kekuatan” dari 
informasi wilayah 
yang bersesuaian 
atau informasi 
variabel penyerta 
yang mempunyai 
hubungan kuat 
dengan variabel 
amatan 

Pendekatan 
nonparametrik 
dapat digunakan 
dapat melakukan 
pendugaan. 

Banyak kasus yang 
melanggar asumsi 
pada model Fay-
Herriot dan model 
spasial Fay-Herriot, 
maka Geoadditive 
model menawarkan 
penanganan 
tersebut dengan 
menggunakan 
fungsi smoothing 

Penemuan metode 
baru yaitu dengan 
transformasi 
logaritma apabila 
asumsi normalitas 
tidak terpenuhi 
pada estimasi mixed 
linear model 

Geoadditive 
Models for Data 
with Spatial 
Information 

Bocci, C. 
(2010), 
Doctoral 
dissertation-Ph. 
D dissertation, 

As both the SAE 
models and the 
geoadditive 
models are 
formulated  as 

Model SAE dan 
model geoaditive 
diformulasikan 
sebagai model 
campuran linier, 

2 / 8 

 
 
 
 
 
Italy: University 
of Florence 

tampaknya 
merupakan pilihan 
yang jelas untuk 
menggabungkan 
kedua model 

linear mixed 
models, it seems 
an obvious choice 
to merge the two 
models in a 
geoadditive SAE 
model to exploit 
the spatial 
information and 
produce estimates 
at small area 
level. 

IV. METODE PENELITIAN  

4.1  Metode Pengumpulan Data 

Data yang digunakan dalam penelitian terdiri dari data 
simulasi  berupa  data  bangkitan,  dan  data  studi  kasus  yang 
bersumber dari referensi penelitian  Ardiansyah, et. al. (2018) 
[7] untuk menghitung produktivitas padi. 
simulasi 

digunakan  merupakan 
yang 
pembangkitan data yang mengacu kepada penelitian Benavent 
dan  Morales  (2016)  [12],  Ardiansyah,  et.  al.  (2021)  [13]. 
Berikut adalah tahapan analisis data simulasi yang digunakan 
pada penelitian ini. 

Data 

1.  Menentukan jumlah unit dan area kecil 
2.  Membangkitkan      yang  merupakan  variabel  penyerta 
dengan  mengikuti  distribusi  binomial  dan  uniform, 
dengan rincian: 

●               (      ) 
●              (    ) 
●              (    ) 

3.  Membangkitkan      dengan  distribusi  normal  dengan 

rata-rata 0, dan varians random effect (  

 )       

4.   Membangkitkan      dengan  distribusi  normal  dengan 

rata-rata 0, dan varians sampling error (  

 )        

5.  Menentukan  nilai  koefisien                       

6.  Menghitung nilai y sebagai model based yaitu         

     .  
7.  Menentukan 

jumlah  knot  yang  digunakan  dalam 

beberapa skenario 

8.  Menghitung  matriks  Z  dari  spline-2  berdasarkan  fungsi 

basis radial, dengan formula seperti berikut: 

     , (       )-            

dengan  ( )                

9.  Merata-ratakan nilai       dan    yang diperoleh 
10. Menyatakan  bentuk  area  kecil  dengan  mengikuti  suatu 
fungsi  Geoadditive  Small  Area  Model  (             
      ),  kemudian  lakukan  pendugaan  nilai  parameter 

11. Menghitung  nilai  RSE  untuk 

setiap  area  ke-i 
menggunakan  metode  bootstrap  parametrik,  dengan 
langkah seperti berikut: 

●  Bangkitkan efek acak   
●  Bangkitkan efek acak   
●  Bangkitkan efek acak    

  dari  (    ̂ 
  dari  (    ̂ 
   dari  (    ̂ 

 ) 
 ) 
 ) 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

●  Bandingkan antara target parameter  bootstrap (  

 ) 
dengan contoh bootstrap yang dihitung berdasarkan 
Geoadditive Small Area Model( ̂  

  ) 

●  Hitung  nilai  RMSE  bootstrap  dengan  formula 

seperti berikut: 

     ( ̂ 

 )     √

∑ 

, ̂  

  ( )       

 ( )- 

12. Bandingkan  nilai  estimasi  dan  RSE  yang  diperoleh 
dengan  metode  Geoadditive  Small  Area  Model,  dan 
estimasi langsung. 
Dari  proses  tahapan  tersebut,  maka  dapat  dibuat  suatu 
diagram flowchart yang menggambarkan alur pengolahan data 
dengan 
diatas  menggunakan  metode 
data 
Geoadditive Small Area Model sebagai berikut. 

simulasi 

Gambar 1. Flowchart tahapan simulasi data 

4.2  Metode Analisis 
4.2.1  Geoadditive Small Area Model 

Geo-additive 

Small  Area  Model  merupakan 
penggabungan  antara  model  Geo-additive  dan  model  SAE, 
yaitu  dengan  menambahkan  informasi  geospasial  ke  dalam 
model SAE. Berikut persamaan gabungan antara Geo-additive 
Model dan SAE (Geo-additive Small Area Model) dengan dua 
komponen efek acak [7]. 

3 / 8 

 
 
               
              
   
 
          
           
 
 
 
  ]
dengan       [      

  ]
,       [      

( 1 ) 
,      ,     -, 

  [

]   [

], dan     [

]   [

] 

Matriks peubah penyerta menjadi     [   

 ]

,   

adalah vektor koefisien efek tetap berukuran (p+1)x1,  adalah 
matriks  berukuran  nxK  dari  spline-2  berdasarkan  fungsi 
    , (       )-             dan   ( )               .  Efek 
acak     adalah  vektor  koefisien  spline-2  (efek  acak  kedua) 
berukuran Kx1. Efek acak   adalah vektor efek acak area kecil 
berukuran  mx1,     adalah  galat  acak  level  unit,  dan  matriks 
    ,   -dengan           jika  pengamatan     berada  dalam 
area kecil ke   dan         untuk lainnya. 

4.2.2  Empirical Best Linear Unbiased Prediction (EBLUP) 

Metode  Empirical  Best  Linear  Unbiased  Prediction 
(EBLUP)  merupakan  salah  satu  pendekatan  yang  sering 
digunakan  pada  pendugaan  small  area  estimation.  EBLUP 
dilatarbelakangi  oleh  ketidakmampuan  BLUP  (Best  Linear 
Unbiased  Predictor) dalam  melakukan  pendugaan  komponen 
varians yang tidak diketahui. Pada EBLUP, komponen varians 
  tidak diketahui, oleh karena itu diperlukan estimasi varians 
dengan  menggunakan  metode  maximum  likelihood  (ML), 
metode  restricted  maximum  likelihood  (REML),  atau  metode 
Prasad Rao. Persamaan EBLUP adalah sebagai berikut. 

            ̂      (      ̂ )  

  ̂ 

( 2 ) 

dimana: 
 ̂ 
   ̂  
 ̂ 

 ̂    

 komponen penyeimbang dari estimasi EBLUP; 

 ̂ = estimasi koefisien regresi; 
  = estimasi langsung area ke-i yang dihasilkan dari survei; 
  = vektor variabel penyerta area ke-i 

4.2.3  MSE Bootstrap Parametrik 

Metode  bootstrap  parametrik  mengasumsikan  bahwa 
ketiga  efek  acak  menyebar  normal  dengan  rataan  dan 
 )dan 
adalah 
ragamnya 
       (     
 )  dengan                   dan                  . 
        (     
bootstrap 
Prosedur 
dengan 
data 
membangkitkan 
*(        )   
   ̂    ̅   

             dan                  +  sebagai     

parametrik 
contoh 

 ),         (     

dilakukan 

bootstrap 

       ̅  

  .  Efek  acak    

 dibangkitkan  dari   (     

 ),  dan     
  ̂    ̅   

 ),    
    dibangkitkan  dari 
   menjadi  versi 
      ̅         
   dengan 
  dihitung dengan cara yang sama dengan  ̂   ̂ , 

target  parameter          ̅  
  ̂      ̅  ̂ 
       ̅ 

dibangkitkan  dari   (     
 ).  Misalkan    
 (     
bootstrap  dari 
kemudian  versi  bootstrap     
 ̂    ̂ 
dan  ̂ 

 , dan  ̂ 
 tetapi dengan menggunakan data contoh bootstrap. 
      𝑡 𝑡  𝑝  diperkirakan  dengan  simulasi  Monte 
Carlo  seperti  langkah  yang  telah  disebutkan  sebelumnya 
 ( ) 
sebanyak  B  kali  sehingga  diperoleh    
 ( )    
yang  merupakan  nilai  bootstrap  sebenarnya  dengan    

 ( )        

 ( )    

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

 ( )  dan 

juga  diperoleh     

 ( )         
  ( ) 
  ( )  dari  dugaan  Geo-Additive  Small  Area 
  ̂ ( )    ̅  ̂ 
  ( )        ̅  
 ( )  
 ( ) adalah ulangan ke b dari 
 )         ( ̂  

  ̂        ̅   
  ̅  
  ( )         
Model  EBLUP   ̂  
  ̂ 
 ̂    ̂ 
. 
RMSE bootstrap untuk setiap area ke-   diperoleh berdasarkan 
rumus sebagai berikut: 

    dengan   ̂  
 ( )       ̂ 
   .  Secara  teori,      ( ̂ 

 ( ) dan  ̂ ( )    ̂ 

         ̂ 

       )

     ( ̂ 

 )     √

∑ , ̂  

  ( )       

 ( )- 

( 3 ) 

 ). 

    ̂ 

    ̂ 

dengan  ̂ 

       ( ̂ 

 )      ( ̂ 

 )      ( ̂ 

    ̂ 
Nilai  MSE  ini  digunakan  sebagai  ukuran  akurasi  dari 
hasil yang dihasilkan. Selain MSE, RSE atau Relatif Standard 
Error juga dapat digunakan sebagai ukuran yang menentukan 
baik  tidaknya  dari  estimasi.  Dengan  RSE,  peneliti  dapat 
menggunakan  sebagai  perbandingan  standard  error  hasil 
estimasi  antar  karakteristik  karena  sifatnya  yang  lebih  stabil 
yang  disebabkan  akibat  standard  error  dibagi  dengan 
estimatornya.  Santoso  dan  Amanah  (2018)  menyebutkan 
bahwa  standar  kelayakan  data  estimasi  untuk  publikasi  yang 
digunakan  oleh  BPS  menggunakan  nilai  RSE  dengan  nilai 
kurang dari 25%. 

4.3 

Tahapan Penelitian  
Tahapan  pengembangan  metode  Geoadditive  Small 
Area  Model  pada  penelitian  ini  menggunakan  metode  design 
science  research.  Metode  ini  terdiri  dari  lima  tahapan,  yaitu 
sebagai berikut. 

1.  Awareness of problem 

Tahap 
permasalahan terkait Geoadditive Small Area Model. 

untuk  mengidentifikasi 

dilakukan 

ini 

2.  Suggestion  

Tahap ini mendefinisikan solusi dari permasalahan yang 
telah  diidentifikasi  pada 
tahap  sebelumnya.  Dalam 
pencarian  solusi  terhadap  masalah  yang  dihadapi  pada 
penelitian  ini,  dilakukan  dengan  menggunakan  studi 
literatur.  
3.  Development  
Tahapan 
ini  merupakan  proses  perancangan  dan 
pembangunan  program  untuk  menjawab  permasalahan 
berdasarkan  solusi  dan  saran  yang  telah  diusulkan  di 
tahapan sebelumnya. 

4.  Evaluation  

Dalam  tahap  evaluasi  ini  dilakukan  pengujian  terkait 
dengan  ketepatan  dari  program  yang  telah  dibangun. 
Evaluasi  yang  diterapkan  berupa  uji  validitas  yang 
digunakan  untuk  menilai  kebenaran  dari  performa 
algoritma dan program yang telah dibuat. 

5.  Conclusion  

Pada  tahap  ini  dilakukan  penarikan  kesimpulan  serta 
telah 
saran  dari  seluruh 
dilaksanakan. 

tahapan  penelitian  yang 

4 / 8 

 
 
                     
     
     
 
 
 
 
 
 
 
 
 
  
    
 
 
 
  
   
 
 
 
  
   
       
     
 
  
 
  
 
  
 
       
   
  
       
  
      
      
   
 
 
 
   
 
 
 
 
 
 
V.  KERANGKA PIKIR 

Gambar 2. Kerangka pikir penelitian 

Telah  banyak  peneliti  yang  menggunakan  analisis  SAE 
untuk  berbagai  studi  kasus  dalam  estimasi  parameter  pada 
area  kecil  seiring  dengan  perkembangan  teknologi.  Software 
yang sering digunakan peneliti untuk analisis SAE adalah R.  

Secara  implementasinya,  pendekatan  parametrik  sudah 
diimplementasikan  dalam  bentuk  SAS  maupun  package  R, 
sedangkan  pendekatan  semi  ataupun  nonparametrik  belum 
dikembangkan secara maksimal. Ketika pengguna melakukan 
pengolahan  SAE  pada  R  dengan  Geoadditive  Small  Area 
Model,  pengguna  harus  menuliskan  kode  script  secara 
manual.  Sehingga  hal  ini  menyebabkan  ketidakefisienan  dari 
segi  waktu  dan 
ingin 
tenaga.  Maka  dari 
pendekatan 
mengembangkan  metode 
semiparametrik, yaitu Geoadditive Small Area Model menjadi 
dalam  bentuk  package  R,  yang  nantinya  akan  diberi  nama 
„geoSAE‟ 

itu,  peneliti 

dengan 

SAE 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

harus  menulis  kode  script  dengan  manual.  Hal 
ini 
menyebabkan  ketidakefisienan  dari  segi  waktu  dan  tenaga 
karena  harus  menulis  script  secara  manual  pada  software  R 
dalam  melakukan  pengolahan.  Peneliti 
juga  mengalami 
kesulitan dalam penyebarluasan hasil script. 

6.2  Suggestion 

Solusi  yang  ditawarkan  peneliti  untuk  permasalahan 
tersebut  pada  penelitian  ini  adalah  dengan  membangun  suatu 
package (paket) pada software R dengan metode Geoadditive 
Small  Area  Model  untuk  memfasilitasi  serta  memudahkan 
pengguna dalam melakukan analisis SAE dengan metode ini. 
Pada  dasarnya,  pengembangan  metode  Geoadditive  Small 
Area  Model  dengan  software  R  pada  penelitian  ini  merujuk 
kepada  package  “sae”  dari  Molina  untuk  menghitung  nilai 
EBLUP dan nilai MSE. 

6.3  Development 
6.3.1 Pemodelan 

Pada  pengembangan  metode  Geoadditive  Small  Area 
Model,  pemodelan  yang  dibangun  dengan  membuat  diagram 
use  case  yang  mana  hanya 
terdapat  satu  aktor  yang 
berinteraksi  dengan  sistem.  Gambar  4  dibawah  menjelaskan 
bagaimana  interaksi  antara  aktor  sebagai  pengguna  dengan 
sistem, dan program apa saja yang dapat dilakukan sistem.  

VI. HASIL DAN PEMBAHASAN 

6.1  Awareness of problems 

Dalam  penelitian  ini,  proses  mengidentifikasi  masalah 
dilakukan dengan studi literatur dan wawancara dengan dosen 
yang terkait dengan SAE. Hasil yang diperoleh adalah model 
Geoadditive 
belum 
dikembangkan  secara  maksimal,  perlu  dilakukan  suatu 
pengembangan  agar  mendapatkan  hasil  estimasi  dengan 
akurasi yang tinggi pada area kecil. 

Small  Area  Model 

diketahui 

Gambar 3. Diagram fishbone kebutuhan paket R dengan Geoadditive Small 
Area Model 

Saat  ini,  dalam  melakukan  analisis  dengan  Geoadditive 
tools  yang  mampu 
Small  Area  Model  belum  adanya 
memudahkan  pengolahan  data.  Salah  satu  perangkat  lunak 
atau  software  yang  sering  digunakan  dalam  analisis  SAE 
adalah R. Untuk melakukan pengolahan melalui R, pengguna 

Gambar 4. Diagram use case pengembangan Geoadditive Small Area Model 

6.3.2 Perancangan dan Implementasi 

Algoritma  dan  pemodelan  yang  telah  disusun  kemudian 
diimplementasikan  dalam  bentuk 
„geoSAE‟. 
Penjabaran mengenai komponen yang terdapat dalam package 
geoSAE adalah sebagai berikut.  

package 

5 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

TABEL II 
RATA-RATA RSE(%) HASIL SIMULASI DATA BANGKITAN 

Unit 

Area 

Knot 

Mean RSE (%) 

Direct Estimate 

geoSAE 

300 

25 

600 

900 

1200 

1500 

25 

50 

25 

50 

75 

25 

50 

75 

100 

25 

50 

75 

100 

125 

10 

10 

15 

20 

10 

10 

15 

20 

25 

30 

35 

10 

15 

10 

10 

15 

20 

25 

30 

35 

10 

15 

20 

10 

15 

10 

10 

15 

20 

25 

30 

35 

10 

15 

20 

25 

10 

15 

10 

10 

7,502849 

1,609813 

7,961569 

1,164057 

8,182034 

1,206794 

7,777237 

1,253490 

7,784023 

1,650245 

7,988857 

0,979371 

8,208081 

0,961374 

7,918791 

0,965207 

8,196916 

0,988756 

7,903386 

0,952520 

7,869620 

0,945899 

7,999528 

1,288753 

7,865969 

1,305281 

7,808762 

1,594652 

7,898577 

0,857674 

7,993157 

0,863562 

7,994682 

0,850408 

7,857139 

0,843406 

8,119159 

0,842816 

8,231651 

0,850419 

8,057111 

1,221510 

7,835967 

1,170801 

7,966072 

1,199354 

7,598859 

1,450714 

7,777063 

1,457837 

7,750185 

1,694684 

7,831821 

0,732824 

8,044614 

0,764948 

8,106639 

0,765445 

8,122636 

0,767368 

7,968192 

0,745775 

8,038391 

0,754399 

7,860669 

1,067440 

8,088350 

1,086302 

8,047208 

1,075494 

7,661133 

1,050261 

7,854994 

1,322685 

7,878814 

1,321720 

7,923388 

1,518002 

7,889565 

1,653933 

Dari  skenario  di  atas, 

terlihat  perbedaan  yang 
dihasilkan dimana rata rata nilai RSE Geoadditive Small Area 

6 / 8 

Gambar 5. Struktur package geoSAE 

1.  File DESCRIPTION 

Merupakan  informasi  umum  tentang  package  yang 
dibangun.  Informasi  yang  tercakup  di  dalamnya  antara 
lain:  judul  package,  nama  penulis  dan  pengembang, 
versi,  deskripsi, 
lisensi,  dependensi  package  yang 
digunakan, dan beberapa informasi lainnya. 

2.  File NAMESPACE  

File  ini  dibuat  secara  otomatis  menggunakan  package 
roxygen2.  NAMESPACE  menggambarkan 
interaksi 
package yang dibuat dengan package lain dan pengguna.  
File  ini  memastikan  bahwa  package  yang  dibangun  dan 
package lain tidak saling mengganggu. Interaksi dengan 
package 
„import‟, 
sedangkan interaksi dengan pengguna digambarkan pada 
item  „export'  yang  berisi  beberapa  function  yang  dapat 
diakses oleh pengguna.  

lain  digambarkan  pada 

item 

3.  Folder R  

Berisi  dokumentasi  script  R  untuk  menyusun  package, 
dapat berupa function, ataupun dataset.  

4.  Folder man  

File  ini  di  generate  secara  otomatis  dari  komentar 
ini  berisi 
berformat  khusus  pada  script  R.  File 
dokumentasi objek dalam package dalam format.Rd. 

5.  Folder data  

Merupakan  file  yang  berisikan  data  yang  tersedia  pada 
package.  Data  yang 
ini 
merupakan data yang berhubungan dengan metode yang 
digunakan. 

tersedia  dalam  package 

6.4  Evaluation 
6.4.1 Data Simulasi 

Pada  penelitian  ini,  simulasi  data  dilakukan  dengan 
membangkitkan  variabel  penyerta,  random  effect,  dan 
sampling  error  dari  distribusi  tertentu.  Tabel  2  dibawah  ini 
menunjukkan  perbandingan 
rata-rata  nilai  RSE  yang 
dihasilkan dari data bangkitan dengan Geoadditive Small Area 
Model.  

 
 
 
 
 
 
 
 
Model  selalu  memiliki  nilai  yang  lebih  kecil  dibandingkan 
dengan pendugaan langsung.  

Rata-rata nilai RSE yang dihasilkan dengan pendugaan 
langsung  bersifat  fluktuatif,  karena  tidak  dipengaruhi  oleh 
knot maupun jumlah area.  

Lain halnya dengan pendugaan langsung, rata-rata nilai 
RSE dengan Geoadditive Small Area Model dipengaruhi oleh 
knot  dan  jumlah  area.  Pada  area  dan  unit  yang  sama, 
penambahan  jumlah  knot  maka  akan  menurunkan  nilai  RSE. 
Sedangkan penambahan jumlah area kecil dengan jumlah unit 
dan  jumlah  knot  yang  sama,  akan  menghasilkan  RSE  yang 
semakin besar. 

6.4.2 Data Studi Kasus 

Data  studi  kasus  yang  digunakan  pada  penelitian  ini 
bersumber  dari  Ardiansyah,  et.  al.  (2018)  [7]  dengan 
menambahkan 
terdekat  dengan 
Kabupaten  Seruyan,  yaitu  Kotawaringin  Barat  dan 
Kotawaringin Timur. Berikut rincian data yang digunakan: 

lokasi  penelitian  yang 

1.  Data  produktivitas  tanaman  padi  level  unit  sebagai 
peubah  respon  dan  digunakan  sebagai  direct  estimate 
diperoleh  dari  Survei  Ubinan  Tahun  2016  di  15 
kecamatan  pada  Kabupaten  Seruyan,  Kotawaringin 
Barat,    dan  Kotawaringin  Timur  yang  bersumber  dari 
BPS. 

2.  Data  peubah  penyerta  tingkat  kecamatan  diperoleh  dari 
Potensi Desa (PODES) Tahun 2014, dan Dinas Pertanian 
Kabupaten  Seruyan,  Kotawaringin  Barat, 
  dan 
Kotawaringin Timur, dengan rincian sebagai berikut: 

●  X1 = Proporsi luas panen padi sawah terhadap total 
luas  panen  (Podes  untuk  data  level  unit;  Dinas 
Pertanian untuk data populasi tingkat kecamatan) 

●  X2 = Garis lintang atau latitude (Podes) 
●  X3 = Garis bujur atau longitude (Podes) 

1)  Penentuan Titik Knot Optimum  

Gambar 6. Potongan syntax penentuan titik knot optimum pada X2 

Gambar 7. Potongan syntax penentuan titik knot optimum pada X3 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

TABEL III 
JUMLAH TITIK KNOT DAN GCV 

knot  GCV Latitude  GCV Longitude 

1 

2 

3 

4 

5 

6 

7 

8 

9 

75,86093 

63,43841 

75,86093 

60,37617 

75,86093 

58,17688 

75,86093 

57,58701 

71,41754 

57,25100 

57,69196 

56,46000 

66,04990 

56,50157 

67,02682 

56,25743 

59,50842 

55,97979 

10 

56,97506 

55,93138 

knot 

jumlah 

adalah  menentukan 

Sebelum  melakukan  estimasi  area  kecil,  maka  langkah 
optimum 
pertama 
berdasarkan  nilai  GCV  minimum.  Kemulusan  kurva 
permukaan pendugaan spasial dipengaruhi oleh jumlah simpul 
yang digunakan pada fungsi basis radialnya [7]. Dari tabel III, 
output  syntax  yang  dijalankan  menunjukkan  bahwa  model 
aditif  spasial  dengan  jumlah  knot  10  memberikan  GCV 
terendah,  yaitu  sebesar  56,975  untuk  latitude  dan  55,931 
untuk longitude 

2)  Estimasi Produktivitas Padi 

Gambar 8. Potongan syntax function eblupgeo  

TABEL IV 
ESTIMASI PRODUKTIVITAS PADI  

Kecamatan 

Hasil Estimasi Produktivitas Padi 

Direct Estimate 

geoSAE 

Arus Selatan 

Seruyan Hilir 

Seruyan Hilir Timur 

Danau Sembuluh 

Seruyan Tengah 

Batu Ampar 

Seruyan Hulu 

Suling Tambun 

Mentaya Hilir Selatan 

Teluh Sampit 

Pulau Hanaut 

Seranau 

Mentaya Hulu 

Bukit Santuai 

18,512 

30,020 

34,420 

32,820 

26,640 

31,400 

19,860 

20,570 

37,896 

42,636 

29,686 

21,740 

25,385 

24,711 

29,103 

34,811 

35,715 

34,006 

23,523 

22,929 

21,023 

20,409 

34,460 

39,796 

35,127 

29,401 

32,318 

24,321 

7 / 8 

 
 
 
 
 
 
 
 
 
 
 
Telaga Antang 

34,593 

31,213 

Hasil estimasi ketiga pendekatan memberikan nilai yang 
hampir  sama  untuk  15    kecamatan  pada  Kabupaten  Seruyan, 
Kotawaringin Barat,  dan Kotawaringin Timur, hal ini berarti 
kecamatan  yang  memiliki  nilai  estimasi  tinggi  pada  satu 
pendekatan, maka nilai estimasi pada pendekatan lainnya akan 
tinggi pula.  

3)  Perbandingan RSE 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

1.  Script  R  dengan  metode  Geoadditive  Small  Area 
Model telah dijalankan pada data simulasi.  Hasil yang 
diperoleh  dengan  data  simulasi  menunjukkan  bahwa 
metode  Geoadditive  Small  Area  Model  memiliki  rata-
rata  nilai  RSE  yang  lebih  kecil  daripada  pendugaan 
langsung. 

3.  Package 

2.  Metode  Geoadditive  Small  Area  Model  telah  berhasil 
dikembangkan  dengan  pembangunan  package  R 
dengan  nama  „geoSAE‟,  yang  dapat  diakses  pada 
https://cran.r-
project.org/web/packages/geoSAE/index.html 
dibangun 

dapat 
diimplementasikan dengan data simulasi dan data studi 
kasus.  Hasil  yang  diperoleh  dengan  data  studi  kasus 
menunjukkan  bahwa  metode  Geoadditive  Small  Area 
Model  mampu menghasilkan hasil estimasi yang lebih 
presisi dibandingkan dengan pendugaan langsung. 

telah 

yang 

R 

Gambar 9. Potongan syntax function pbmsegeo 

TABEL V 
PERBANDINGAN NILAI RSE  

Kecamatan 

RSE (%) 

Direct Estimate 

geoSAE 

Arus Selatan 

Seruyan Hilir 

Seruyan Hilir Timur 

Danau Sembuluh 

Seruyan Tengah 

Batu Ampar 

Seruyan Hulu 

Suling Tambun 

Mentaya Hilir Selatan 

Teluh Sampit 

Pulau Hanaut 

Seranau 

Mentaya Hulu 

Bukit Santuai 

Telaga Antang 

Rata-Rata 

2,362 

5,397 

7,217 

23,777 

4,942 

6,236 

13,242 

5,611 

1,180 

0,890 

3,019 

3,205 

33,540 

8,814 

12,228 

8,777 

1,979 

2,386 

1,934 

9,414 

1,569 

3,656 

3,102 

1,907 

1,510 

0,618 

2,354 

15,936 

17,073 

2,737 

5,695 

4,791 

DAFTAR PUSTAKA 
[1]  Rao, J., & Molina, I. (2015). Small Area Estimation 2nd Edition. New Jersey: John 

Wiley and Sons Inc. 

[2]  Pusponegoro,  N.  H.,  Djuraidah,  A.,  Fitrianto,  A.,  &  Sumertajaya,  I.  M.  (2019). 
Geoadditive  Models in Small  Area  Estimation of  Poverty. Journal of Data Science 
and Its Applications, 2(1), 11-18. 

[3]  Fay,  R.  E.,  &  Herriot,  R.  A.  (1979).  Estimates  of  Income  for  Small  Places:  An 
Application  of  James-Sten  Procedures  to  Census  Data.  Journal  of  the  American 
Statistical Association, 268-277. 

[4]  Kurnia,  A.  (2009).  The  Best  Predictions  for  Empirical  Logarithm  Transformation 
Models  in  Small  Area  Estimation  with  Application  on  SUSENAS  Data  (Doctoral 
dissertation,  Ph.  D  dissertation,  Department  of  Statistics.  Bogor  Agricultural 
University, Bogor, Indonesia). 

[5]  E.  E.  Kammann  and  M.  P.  Wand,  2003.  Geoadditive  models,  Applied  Statistics, 

52(1), pp.1–18 

[6]  Bocci,  C.  (2010).  Geoadditive  Small  Area  Model  For  The  Estimation  of 

Consumption Expenditure In Albania. Italy: University of Florence. 

[7]  Ardiansyah, M., Djuraidah,  A., &  Kurnia, A.  (2018).  Pendugaan Area Kecil Data 
Produktivitas  Tanaman  Padi  Dengan  Geoadditive  Small  Area  Model.  Jurnal 
Penelitian Pertanian Tanaman Pangan, 2(2), 101-110. 

[8]  Kandala, N. B., Fahrmeir, L., Klasen, S., & Priebe, J. (2009). Geo‐Additive Models 
Of Childhood Undernutrition In Three Sub‐Saharan African Countries. Population, 
Space and Place, 15(5), 461-473. 

[9]  Nkurunziza, H., Gebhardt, A., & Pilz, J. (2011). Geoadditive Modelling of Malaria 

In Burundi. Malaria Journal, 10(1), 1-7. 

[10] Djuraidah,  Anik,  &  Aunuddin.  (2006).  Kriging  pada  Thin-Plate  Spline  dengan 

Pendekatan Model Campuran. Jurnal Matematika Integratif, 5(2), 1-12 

[11] Salvati,  N.,  Chandra,  H.,  Ranalli,  M.  G.  and  Chambers,  R.  (2008),  Small  Area 
Estimation  Using  a  Nonparametric  Model  Based  Direct  Estimator.  Submitted  for 
publication. 

[12] Benavent, Roberto & Morales, Domingo. (2015). Multivariate Fay-Herriot models 
for small area estimation. Computational Statistics and Data Analysis 94 2016 372-
390. 

[13] Ardiansyah,  M.,  Kurnia,  A.,  Sadik,  K.,  Djuraidah,  A.,  &  Wijayanto,  H.  (2021, 
March).  Numerical  Prediction  of  paddy  weight  of  Crop  Cutting  Survey  using 
Generalized  Geoadditive  Linear  Mixed  Model.  In Journal  of  Physics:  Conference 
Series (Vol. 1863, No. 1, p. 012024). IOP Publishing. 

RSE  merupakan  ukuran  presisi  suatu  estimasi  relatif 
terhadap  estimasinya.  Nilai  RSE  bersifat  stabil,  sehingga 
dapat  digunakan  untuk  membandingkan  standard  error  hasil 
estimasi setiap karakteristik. Pada tabel V, terlihat bahwa rata-
rata  nilai  RSE  dengan  pendekatan  Geoadditive  Small  Area 
lebih  kecil  yaitu  sebesar  4,79% 
Model  memiliki  nilai 
dibandingkan  dengan  Direct  Estimate.  Hal  ini  menunjukkan 
bahwa  Geoadditive  Small  Area  Model  mampu  menghasilkan 
hasil estimasi yang lebih presisi. 

VII.  PENUTUP 

Dari  hasil  dan  pembahasan  yang  telah  dijelaskan,  maka 

dapat ditarik kesimpulan sebagai berikut. 

8 / 8 

 
 
 
 
 
 
 
 
 
"
221709773,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Kajian Penentuan Nilai Epsilon Optimal Pada 
Algoritma DMDBSCAN dan Pemetaan Daerah 
Rawan Gempa Bumi Di Indonesia Tahun 2014-2020 

Kamilia Wafa Pakuani (221709773, 4SD2) 
Dosen Pembimbing: Robert Kurniawan 

Ringkasan— Pemantauan wilayah terjadinya titik gempa bumi 
dapat  dilakukan  dengan  mengetahui  penyebaran  titik  gempa 
bumi yang  tersebar  acak  ataupun  mengelompok.  Density-Based 
Spatial  Clustering  of  Applications  with  Noise  (DBSCAN) 
merupakan  salah  satu  algoritma  clustering  berbasis  kepadatan 
dari  sejumlah  data  besar  yang  mengandung  noise  atau  outlier. 
Penelitian  sebelumnya  telah  memodifikasi  Algoritma  DBSCAN 
untuk  mendapatkan  nilai  Eps  optimal  secara  otomatis  dengan 
mengadopsi 
DBSCAN 
(DMDBSCAN)  pada  single  level  density  sesuai  k-dist  plot. 
Pemilihan  parameter  Eps  didapatkan  dari  perhitungan 
perubahan  kemiringan  garis  maksimal  pada  3  jarak  tetangga 
terdekat dalam persebaran data. Namun, cara ini rentan terhadap 
perubahan  kemiringan  garis  yang  sangat  jauh.  Maka  dari  itu, 
penelitian  ini  melakukan  modifikasi  Algoritma  DMDBSCAN 
sehingga  diharapkan  dapat  menjadi  acuan  pencarian  nilai  Eps 
pada  Algoritma  DBSCAN  yang  lebih  tepat  dan  mengetahui 
penyebaran titik gempa bumi di Indonesia. 

Dynamic  Method 

Algoritma 

Kata Kunci— DBSCAN, DMDBSCAN, k-dist plot, gempa bumi 

I.  LATAR BELAKANG 

Berdasarkan  data  yang  diperoleh  dari  Badan  Meteorologi, 
Klimatologi,  dan  Geofisika  (BMKG),  gempa  bumi  selalu 
terjadi  di  Indonesia  setiap  hari,  namun  tidak  semuanya  dapat 
dirasakan oleh penduduk karena magnitudenya kecil dan pusat 
gempa jauh dari pemukiman penduduk. Keaktifan gempa bumi 
di Indonesia sangat tinggi, rata-rata setiap bulannya tercatat 400 
kali. Dalam periode 1991 sampai dengan 2007, tercatat 24 kali 
gempa bumi besar, di antaranya kejadian gempa bumi Aceh 26 
Desember  2004  dengan  kekuatan  9.3  SR.  Gempa  bumi  ini 
diikuti oleh tsunami besar yang menimbulkan korban ratusan 
ribu  jiwa  dan  menimbulkan  kerugian  harta  benda  triliunan 
rupiah  serta  gempa  bumi  Yogyakarta  26  Mei  2006  yang 
menimbulkan  kerusakan  infrastruktur  yang  sangat  parah 
(http://repogempa.bmkg.go.id/).  Gempa  bumi  Padang  30 
(SR) 
September  2009  berkekuatan  7,9  Skala  Richter 
kerugiannya  mencapai  Rp  4,8  triliun  dengan  korban  tewas 
1.195 orang, total rumah rusak sebanyak 271.540 unit. Gempa 
bumi  disertai  tsunami  di  Aceh  2004  menelan  korban  hampir 
300.000 jiwa di Indonesia, Thailand, India, Srilanka, Maldive, 
dan Afrika [1].  

Peristiwa gempa bumi merupakan bencana alam yang tidak 
dapat dihindari maupun dicegah, tetapi akibat yang ditimbulkan 
oleh gempa bumi bisa diminimalisir. Data-data yang diperoleh 
dari  peristiwa  gempa  bumi  dapat  dikelompokkan  untuk 
melakukan  pemetaan  pada  wilayah  terjadinya  gempa  bumi 
serta  dilakukan  analisis  untuk  mengetahui  karakteristik  hasil 

pengelompokan  wilayah  gempa  bumi.  Pengelompokan  ini 
dilakukan menggunakan teknik clustering. 

Clustering  adalah  proses  pengelompokan  kumpulan  data 
menjadi  beberapa  kelompok  sehingga  objek  di  dalam  satu 
kelompok  memiliki  banyak  kesamaan  dan  memiliki  banyak 
perbedaan  dengan  objek  dikelompok  lain.  Perbedaan  dan 
persamaannya  biasanya  berdasarkan  nilai  atribut  dari  objek 
tersebut dan dapat juga berupa perhitungan jarak  [2]. Density 
based spatial clustering of applications with noise (DBSCAN) 
merupakan salah satu algoritma clustering berbasis kepadatan 
untuk data besar yang mengandung noise atau outlier. 

Algoritma  DBSCAN  memiliki  dua  parameter  masukan, 
yaitu  Eps  dan  MinPts.  Parameter  Eps  digunakan  untuk 
menentukan radius (jarak maksimal) titik-titik anggota cluster 
dari  pusat  cluster.  Parameter  MinPts  digunakan  untuk 
memberikan  batasan  jumlah  titik-titik  yang  menjadi  anggota 
cluster  dalam  radius  Eps  tersebut.  Setiap  nilai  Eps  pada 
algoritma  DBSCAN  diadopsi  untuk  menemukan  semua 
kelompok  sehubungan  dengan  tingkat  kepadatan  yang  sesuai 
[3]. Pada penelitian yang telah dilakukan oleh Usman (2014), 
nilai  Eps  optimal  yang  didapatkan  masih  menggunakan  cara 
yang manual dengan memperhitungkan satu per satu nilai Eps 
yang sesuai. Hal ini menyebabkan pencarian nilai Eps menjadi 
kurang efektif dan memerlukan waktu yang lama. 

Pada penelitian ini, Algoritma DBSCAN dimodifikasi untuk 
mendapatkan  nilai  Eps  optimal  secara  otomatis  dengan 
mengadopsi  metode  Dynamic  Method  DBSCAN 
(DMDBSCAN).  DMDBSCAN  merupakan  salah  satu  metode 
modifikasi Algoritma DBSCAN pada nilai Eps yang bervariasi. 
DMDBSCAN menggunakan ide k-dist plot untuk mencari nilai 
Epsilon  (Eps)  untuk  semua  kepadatan  di  dataset.  Algoritma 
DMDBSCAN  menghasilkan  nilai  parameter  Eps  secara 
otomatis.  Pada  penelitian  yang  dilakukan  oleh  Nadia  (2015), 
pemilihan  parameter  Eps  didapatkan  dari  perhitungan 
perubahan  kemiringan  garis  maksimal  pada  3  jarak  tetangga 
terdekat  dalam  persebaran  data.  Namun,  cara  tersebut  rentan 
terhadap  perubahan  kemiringan  garis  yang  sangat  jauh. 
Kamudian, cara ini juga tidak dapat memperhitungkan nilai k 
dalam  perhitungan  k-dist  plot  yang  berperan  sebagai  nilai 
parameter  MinPts  dalam  pembentukan  cluster.  Hal  tersebut 
membuat  fungsi  algoritma  yang  dibagun  menjadi  tidak 
fleksibel untuk nilai k dalam perhitungan k-dist plot dan kurang 
efektif  dalam  menentukan  nilai  parameter  Eps  yang  optimal. 
Maka  dari  itu,  penelitian  ini  melakukan  modifikasi  terhadap 
fungsi  pencarian  nilai  Eps  optimal  secara  otomatis  pada 

 1 / 7 

 
 
 
 
Algoritma  DMDBSCAN  menggunakan  bahasa  pemrograman 
R.  Kemudian,  nilai  parameter  yang  sudah  didapatkan 
digunakan  dalam  pengelompokan  data  titik  gempa  bumi  di 
Indonesia. Data yang digunakan ialah data titik gempa bumi di 
Indonesia selama tujuh tahun, yaitu tahun 2014 sampai dengan 
tahun  2020.  Algoritma  DMDBSCAN  pada  penentuan  nilai 
parameter  Eps  yang  telah  dimodifikasi  diharapkan  dapat 
menjadi acuan pencarian nilai  Eps pada Algoritma DBSCAN 
yang  lebih  baik  dan  dapat  diimplementasikan  pada  data 
penyebaran  titik  gempa  bumi  sehingga  dapat  diketahui 
kelompok wilayah yang berpotensi terjadinya gempa bumi. 

II.  TUJUAN PENELITIAN 
Adapun tujuan dari penelitian ini adalah sebagai berikut: 
1.  Mengetahui pola dan karakteristik wilayah gempa bumi di 
Indonesia pada tahun 2014 sampai dengan tahun 2020. 

2.  Memodifikasi 

Algoritma 

untuk 
mendapatkan  nilai  parameter  Eps  yang  optimal  dan 
mengaplikasikannya  pada  data  gempa  bumi  di  Indonesia 
tahun 2014 sampai dengan tahun 2020. 

DMDBSCAN 

III. PENELITIAN TERKAIT 

Dalam  penyusunan  skripsi  ini,  penulis  mereferensi  dari 
penelitian-penelitian  sebelumnya  yang  berkaitan  dengan  latar 
belakang masalah yang disajikan pada Gambar 1. 

Gambar 1. Peta literatur (literature map) 

IV. METODE PENELITIAN  
Penelitian  ini  menggunakan  data  yang  diperoleh  melalui 
pengunduhan  dari  website  resmi  BMKG  berupa  data  titik 
gempa bumi di Indonesia periode Januari 2014 sampai dengan 
Desember  2020.  Algoritma  DMDBSCAN  digunakan  untuk 
mengelompokkan  data  titik  gempa  bumi  di  Indonesia  pada 
tahun 2014 hingga tahun 2020 ke dalam cluster. 

Di  antara  berbagai 

jenis  algoritma  pengelompokan, 
pengelompokan  berbasis  kepadatan 
lebih  efisien  untuk 
menentukan cluster dalam data dengan kepadatan yang berbeda 
[6].  DBSCAN  merupakan  salah  satu  contoh  pengembangan 
clustering berbasis kepadatan atau yang biasa disebut dengan 
density-based 
ini  berguna  untuk 
clustering.  Metode 
mengelompokkan data non-konveks. 

DBSCAN  tidak  memerlukan  beberapa  cluster  sebagai 
parameter, 
tetapi  membutuhkan  dua  parameter  untuk 
memutuskan  apakah  dua  titik  terdekat  harus  dikumpulkan 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

menjadi  satu  cluster  atau  tidak.  Kedua  parameter  tersebut 
adalah ε (epsilon) sebagai ambang jarak, dan  MinPts (jumlah 
minimum point).  

utama 

Konsep 

dari 
data 

algoritma  DBSCAN 
tetangganya 
dengan 

adalah 
mengelompokkan 
dalam 
satu/beberapa cluster, dimana dalam satu cluster harus terdapat 
jumlah  anggota  yang  memenuhi  nilai  dari  minimum  point 
(MinPts).  Selain  itu,  jarak  antara  titik  utama  (core  point) 
dengan anggota dalam satu cluster tidak boleh lebih dari nilai 
epsilonnya.  Titik utama (core  point)  akan  dipilih  secara  acak 
pada  iterasi  pertama  [7].  Akan  tetapi,  fungsi  DBSCAN 
membutuhkan  pencarian  nilai  Eps  optimal  yang  didapatkan 
masih  menggunakan 
dengan 
memperhitungkan satu per satu nilai Eps yang sesuai.  

yang  manual 

cara 

DMDBSCAN  merupakan  salah  satu  metode  modifikasi 
Algoritme  DBSCAN  pada  nilai  Eps  yang  bervariasi. 
DMDBSCAN menggunakan ide k-dist plot untuk mencari nilai 
Epsilon  (Eps)  untuk  semua  kepadatan  di  dataset.  Algoritma 
DMDBSCAN  menghasilkan  nilai  parameter  Eps  secara 
otomatis [5]. 
3.1.  Data Penelitian 

Data  yang  diolah  dalam  penelitian  ini  adalah  data  gempa 
bumi  di  Indonesia  yang  diperoleh  melalui  website  resmi 
http://repogempa.bmkg.go.id,  dengan  jumlah  dataset  47.908 
dari tahun 2014 sampai dengan tahun 2020.  

Data yang diperoleh berupa: tanggal (Date), waktu (Time), 
latitude  (Lat),  longitude  (Lon),  magnitudo  (M),  kedalaman 
(Dep) dan wilayah (Region). Penulis mencoba memetakan data 
gempa dan mengidentifikasi zona gempa potensial berdasarkan 
sebaran cluster. 
3.2.  Tahapan Penelitian 
3.2.1. Identifikasi Masalah 

Tahap  identifikasi  masalah  akan  dilakukan  identifikasi 
terhadap kasus gempa bumi di Indonesia mengenai penyebab, 
penanggulangan  serta  dampak  yang  ditimbulkan.  Kemudian 
dilakukan  pengidentifikasian  terhadap  jenis  data  dan  metode 
yang digunakan untuk dilakukan pengelompokan. 
3.2.2. Penentuan Nilai Eps Optimal 

Tahapan ini dilakukan modifikasi Algoritma DBSCAN yang 
tersedia  pada  package  dbscan  dalam  pemograman  R 
menggunakan  ide  DMDBSCAN  untuk  mendapatkan  nilai 
parameter Eps yang optimal. DMDBSCAN menggunakan ide 
k-dist plot untuk menemukan nilai parameter Eps yang sesuai 
dengan 
tingkat  kepadataan  data.  Langkah-langkah  yang 
digunakan  untuk  menemukan  Eps  yang  sesuai  pada  setiap 
tingat kepadatan adalah [3]: 

1.  Menghitung k-dist untuk setiap titik dan mempartisi k-

dist plot; 

2.  Jumlah  kepadatan  diberikan  secara  intuitif  oleh  k-dist 

plot; 

3.  Memilih  parameter  Eps  secara  otomatis  untuk  setiap 

kepadatan. 

Fungsi DBSCAN pada package dbscan membutuhkan nilai 
masukan Eps, kemudian dimodifikasi untuk mendapatkan nilai 
Eps  secara  otomatis  dan  optimal.  Alur  algoritma  penentuan 
nilai Eps dapat dilihat pada Gambar 2. 

 2 / 7 

 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

tersebut.  SI  bernilai  0 

cluster 
(atau  mendekati  nol) 
menunjukkan  data  tersebut  posisinya  berada  di  perbatasan  di 
antara  dua  cluster  [9].  Berikut  persamaan  untuk  menghitung 
nilai SI dari sebuah cluster: 

𝑆𝐼 =  

1
𝑘

𝑘
∑ 𝑆𝐼𝑗
𝑗=1

b. 

Indeks Dunn 

Indeks Dunn dirumuskan pada persamaan di bawah, dimana 
c  adalah  banyak  cluster, 𝑑(𝑐𝑖, 𝑐𝑗) adalah  jarak  antara  objek 
cluster ke-i dan objek pada cluster ke-j, dan 𝑑(𝑥𝑘) adalah jarak 
antara  objek-objek  pada  cluster  ke-k.  Nilai  indeks  dunn 
berkisar antara 0 sampai tak hingga. Semakin besar nilai indeks 
dunn, maka semakin baik clustering tersebut [10]. 

𝐷𝑢𝑛𝑛 =   𝑚𝑖𝑛1≤𝑖≤𝑐 {min {

𝑑(𝑐𝑖, 𝑐𝑗)
𝑚𝑎𝑥1≤𝑘≤𝑐(𝑑(𝑥𝑘))

}} 

c. 

Indeks Davies-Bouldin (DB) 

Indeks  DB  bertujuan  untuk  mengidentifikasi  hasil  cluster 
yang memiliki compactness dan separability yang baik. Indeks 
DB  dirumuskan  pada  persamaan  9;  dimana  c  adalah  jumlah 
cluster,  i  dan  j  adalah  label  cluster,  𝑑(𝑥𝑖)  dan 𝑑(𝑥𝑗) adalah 
jarak antara objek pada cluster ke-i dan j terhadap pusat cluster, 
dan 𝑑(𝑐𝑖, 𝑐𝑗) adalah  jarak  antara  pusat  cluster  ke-i  dan  ke-j. 
Semakin kecil nilai indeks DB, maka semakin baik clustering 
tersebut [10]. 

𝐷𝐵 =  

1
𝐶

𝑐

∑ 𝑚𝑎𝑥𝑖≠𝑗 {

𝑖=1

𝑑(𝑥𝑖) + 𝑑(𝑥𝑗)
}
𝑑(𝑐𝑖, 𝑐𝑗)

d.  BetaCV 

BetaCV mengukur baik tidaknya suatu cluster berdasarkan 
rasio  antara  rerata  jarak  dalam  cluster  terhadap  rerata  jarak 
antara cluster. BetaCV dirumuskan pada persamaan 10; dimana 
Win adalah jumlah semua jarak antara titik dalam cluster, Nin 
adalah  jumlah  titik  berpasangan  dalam  cluster,  Wout  adalah 
jumlah semua jarak antara titik pada cluster berbeda, dan Nout 
adalah jumlah titik berpasangan pada cluster berbeda. Semakin 
kecil nilai BetaCV, maka semakin baik clustering tersebut [11]. 

𝐵𝑒𝑡𝑎𝐶𝑉 =  

𝑊𝑖𝑛
⁄
𝑁𝑖𝑛

𝑊𝑜𝑢𝑡
⁄
𝑁𝑜𝑢𝑡

Indeks Calinski-Harabasz 

e. 

Indeks  validitas  Calinski-Harabasz  (CH)  menghitung 
perbandingan antara nilai Sum of Square between cluster (SSB) 
sebagai  separation  dan  nilai  Sum  of  Square  within-cluster 
(SSW)  sebagai  compactness  yang  dikalikan  dengan  faktor 
normalisasi,  yaitu  selisih  jumlah  data  dengan  jumlah  cluster 
dibagi  dengan  jumlah  cluster  dikurang  satu.  Jumlah  cluster 
terbaik ditunjukkan dengan semakin besar nilai  CH [9]. Nilai 
CH dapat dihitung dengan persamaan: 

𝐶𝐻 =  

𝑡𝑟𝑎𝑐𝑒(𝑆𝑆𝐵)
𝑡𝑟𝑎𝑐𝑒(𝑆𝑆𝑊)

𝑥

𝑁 − 𝑘
𝑘 − 1

Apabila  disimpulkan  dari  kelima  indeks  validitas  untuk 
mengevaluasi hasil cluster yang terbentuk, maka hasil cluster 
yang lebih baik ditunjukkan dengan nilai  Silhoutte Index (SI) 
yang  lebih  besar,  indeks  Dunn  yang  lebih  besar,  indeks  DB 
yang lebih kecil, BetaCV yang lebih kecil, dan Indeks Calinski-
Harabasz yang lebih besar. 

 3 / 7 

Gambar 2. Diagram alir algoritma penelitian 

3.2.3. Pengelompokan Data Titik Gempa Menggunakan Nilai 

Eps Optimal pada Algoritma DBSCAN 

Modifikasi  Algoritma  DBSCAN  menghasilkan  nilai 
parameter  Eps  secara  optimal  dan  otomatis.  Nilai  parameter 
Eps  dan  MinPts digunakan untuk mengelompokkan data  titik 
gempa  di  Indonesia  tahun  2014  sampai  dengan  tahun  2020. 
Parameter  Eps  digunakan  untuk  menentukan  radius  (jarak 
maksimal)  titik-titik  anggota  cluster  dari  pusat  cluster  secara 
otomatis.  Parameter  MinPts  dalam  Algoritma  DMDBSCAN 
yang  digunakan  untuk  memberikan  batasan  jumlah  titik-titik 
yang menjadi anggota cluster dalam radius Eps [8]. 

Setelah  diperoleh  nilai  Eps  dan  MinPts,  selanjutnya 
dilakukan  prosedur-prosedur  yang  meliputi:  1)  melakukan 
clustering pada titik yang tersisa dengan cara menghubungkan 
semua titik inti (core) dengan jarak yang kurang dari Eps satu 
sama  lain,  2)  membuat  setiap  kelompok  dari  titik  inti  yang 
terhubung menjadi clustering yang terpisah, dan 3) menetapkan 
setiap  titik  perbatasan  ke  salah  satu  clustering  terdekat  pada 
kelompoknya  [8].  Proses  clustering  dalam  penelitian  ini 
menyisipkan  perhitungan  pencarian  nilai  Eps  ke  dalam 
Algoritma  DBSCAN  yang  ada  di  package  dbscan  pada 
perangkat lunak statistika R 4.1.0 sehingga akan menghasilkan 
anggota cluster dengan parameter yang digunakan. 
3.2.4. Analisis Cluster 

perbandingan  nilai  Eps 

Pada  proses  analisis  dilakukan  analisis  clustering  dan 
implemantasi  Algoritma  DMDBSCAN  pada  pengelompokan 
data  titik  gempa  bumi  menggunakan  R.  Hasil  analisis  ini 
mencakup 
yang  dihasilkan 
menggunakan  Algoritma  DMDBSCAN  sebelum  modifikasi 
dan  setelah  modifikasi.  Evaluasi  clustering  dilakukan  pada 
tahapan ini dengan menggunakan 5 indeks validasi, yaitu nilai 
Silhoutte  Index  (SI),  indeks  Dunn,  indeks  DB,  BetaCV,  dan 
Indeks Calinski-Harabasz. 

a.  Silhoutte Index (SI) 

Nilai SI yang mendekati 1 menunjukkan bahwa data tersebut 
semakin  tepat  berada  dalam  cluster  tersebut.  Nilai  SI  negatif 
menunjukkan  bahwa  data  tersebut  tidak  tepat  berada  dalam 

 
 
 
 
 
 
 
 
 
V.  KERANGKA PIKIR 

Penelitian  ini  menggunakan  data  gempa  bumi  untuk 
diklasterisasi  menggunakan  metode  DMDBSCAN.  Metode 
DMDBSCAN  modifikasi  diimplementasikan  ke  dalam  R-
Package.  Cluster  yang 
terbentuk  selanjutnya  dievaluasi 
menggunakan lima indeks validasi, yaitu Silhoutte Index (SI), 
indeks  Dunn,  indeks  DB,  BetaCV,  dan  Indeks  Calinski-
Harabasz.  Kerangka  pikir  pada  penelitian  ini  disajikan  pada 
Gambar 3. 

Gambar 3. Kerangka pikir 

VI. HASIL DAN PEMBAHASAN 

Data titik gempa bumi yang disediakan oleh BMKG berupa 
data titik gempa bumi di seluruh Indonesia. Sebelum dilakukan 
pengelompokkan,  dilakukan  proses 
sebelumnya,  yaitu 
pemilihan data titik gempa bumi di wilayah Indonesia dengan 
batas latitude dan longitudenya adalah 6°08 N latitude sampai 
11°15 S latitude, dan dari 94°45 E sampai 141°05 E longitude 
[12].  Kemudian,  di  lakukan  pemilihan  data  pada  setiap 
tahunnya dan magnitudo diatas 5.5. 

5.1.  Melihat pola dan karakteristik data titik gempa bumi. 

Gempa bumi adalah getaran atau getar-getar yang terjadi di 
permukaan  bumi  akibat  pelepasan  energi  dari  dalam  secara 
tiba-tiba  yang  menciptakan  gelombang  seismik  yang  diukur 
dengan  menggunakan  alat  Seismometer.  Moment  magnitudo 
adalah skala yang paling umum di mana gempa Bumi terjadi 
untuk seluruh dunia. Gempa 3 magnitude atau lebih sebagian 
besar hampir tidak terlihat dan jika besarnya 7 lebih berpotensi 
menyebabkan kerusakan serius di daerah yang luas, tergantung 
pada kedalaman gempa.  

Berdasarkan kedalaman, gempa bumi yang hiposentrumnya 
berada lebih dari 300 km di bawah permukaan bumi (di dalam 
kerak  bumi)  termasuk  kedalam  kategori  gempa  bumi  dalam. 
Pada umumnya kategori ini tidak terlalu berbahaya. Kemudian, 
gempa bumi yang hiposentrumnya berada antara 60 km sampai 
300 km di bawah permukaan bumi termasuk kedalam kategori 
ini 
gempa  bumi  menengah.  Pada  umumnya  kategori 
menimbulkan  kerusakan  ringan  dan  getarannya  lebih  terasa. 
Terakhir, gempa bumi yang hiposentrumnya berada kurang dari 
60 km dari permukaan bumi termasuk kedalam kategori gempa 
bumi  dangkal.  Gempa  bumi  ini  biasanya  menimbulkan 
kerusakan yang besar. 

Pada tahap ini, dilakukan analisis deskriptif mengenai pola 
dan karakteristik terhadap data gempa bumi di Indonesia tahun 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

2014-2020.  Jumlah  titik  gempa  bumi  yang  tersedia  pada 
website bmkg setiap tahunnya mulai dari 2014-2020 disajikan 
dalam  Tabel  I.  Kemudian,  dilakukan  juga  analisis  deskriptif 
terhadap 2 parameter gempa bumi, yaitu kedalaman dan juga 
magnitudo. Analisis kedua parameter tersebut disajikan dalam 
Tabel II dan Tabel III. 

Tahun 

2014 
2015 

TABEL I 
JUMLAH TITIK DAN DAERAH YANG MEMILIKI TITIK GEMPA TERBANYAK  
PADA DATA GEMPA BUMI TAHUN 2014-2020 
Daerah titik gempa 
Jumlah 
terbanyak 
Observasi 
Java, Indonesia 
2427 
Banda Sea 
4210 
Minahassa Peninsula, 
Sulawesi 
Sulawesi, Indonesia 
Sumbawa Region 
Sulawesi, Indonesia 
Sulawesi, Indonesia 

Jumlah 
titik 
277 
447 

5302 
12018 
11787 
8310 

576 
2749 
1449 
837 

2017 
2018 
2019 
2020 

2016 

3853 

426 

TABEL II 
ANALISIS DESKRIPTIF VARIABEL MAGNITUDO 
PADA DATA GEMPA BUMI TAHUN 2014-2020 
Tahun  Min  Mean  Max  Daerah Magnitudo Tinggi 
2014 
2015 
2016 
2017 
2018 

6.3  Northern Molucca Sea 
6.9 
7.7 
7.2 
7.5  Minahassa 

Irian Jaya, Indonesia 
Southwest of Sumatra 
Celebes Sea 

3.798 
3.971 
3.918 
3.876 
3.414 

1.5 
3 
3 
3 
1.1 

Peninsula, 

2019 
2020 

1.5 
1.1 

3.477 
3.487 

7.3 
7.1 

Sulawesi 
Banda Sea 
Southwest of Sumatra 

TABEL III 
ANALISIS DESKRIPTIF VARIABEL DEPTH/KEDALAMAN 
PADA DATA GEMPA BUMI TAHUN 2014-2020 

Tahun  Min  Mean  Max  Daerah Depth Rendah 
2014 
2015 
2016 
2017 
2018 

732  Northern Sumatra 
692 
750  Halmahera, Indonesia 
750 
750  Minahassa 

54.98 
60.8 
63.14 
58.53 
41.03 

Sulawesi, Indonesia 

Southern Sumatra 

5 
2 
10 
3 
5 

Peninsula, 

2019 
2020 

3 
5 

38.76 
48.43 

750 
750 

Sulawesi 
Seram, Indonesia 
Southwest of Sumatra 

5.2.  Penentuan Nilai Eps Optimal  

Tahap 

ini  dilakukan  dengan  memodifikasi  Algoritma 
DMDBSCAN.  Algoritma  sebelumnya 
telah  menyisipkan 
algoritma  pencarian  nilai  Eps  dengan  k  bernilai  3  pada 
Algoritma DBSCAN yang terdapat pada package dbscan dan 
pemilihan nilai Eps dilakukan dengan mencari perubahan kritis 
yang maksimal dalam kurva.  

DMDBSCAN modifikasi juga menyisipkan pencarian nilai 
Eps dengan nilai k menjadi nilai inputan. Kemudian, pemilihan 
nilai  Eps  optimal  pada  Algoritma  DMDBSCAN  setelah 
modifikasi  dilakukan  dengan  mencari  perubahan  kemiringan 
garis (slope) yang berada pada rentang antara 10% hingga 20% 
dan memilih titik paling rendah jika terdapat lebih dari 1 nilai 

 4 / 7 

 
 
 
 
 
 
 
pada  rentang  tersebut.  Pseudocode  Algoritma  DMDBSCAN 
modifikasi dapat dilihat pada Gambar 4 poin ke-8.  

Untuk menemukan nilai Eps yang sesuai 
Kumpulan data berukuran n 
Eps untuk setiap kepadatan bervariasi 

Algoritma 1 Pseudocode dari teknik yang diusulkan DMDBSCAN untuk 
menemukan  nilai  Eps  yang  sesuai  untuk  setiap  tingkat  kepadatan  dalam 
kumpulan data. 
Purpose  
Input  
Output  
Procedure   1 
2 
3 
4 
5 
6 
7 
8 
9 

for i 
for j = 1 to n 
   d(i,j) ← temukan jarak (xi, xj) 
temukan nilai minimum jarak ke 3 titik terdekat 
   end for 
end for 
urutkan jarak naik dan plot untuk menemukan setiap nilai 
   p(i,j) ← temukan nilai slope antara 10% hingga 20% 
Eps sesuai dengan perubahan kritis dalam kurva 

Gambar 4. Pseudocode Algoritma DMDBSCAN modifikasi 

Algoritma dimulai dengan perhitungan jarak euclidean pada 
setiap pasangan data latitude dan longitude dengan fungsi dist. 
Selanjutnya  mengubah  data  dist  tersebut  menjadi  matriks 
menggunakan  fungsi  as.matrix.  Fungsi  as.matrix  digunakan 
karena  hasil  dist  berupa  matriks  segitiga  atas  sehingga  perlu 
dinormalisasi  menjadi  matriks  utuh.  Normalisasi  digunakan 
untuk memudahkan pencarian k tetangga terdekat pada setiap 
baris perhitungan jarak. Ilustrasi matriks jarak berukuran 𝑛 𝑥 𝑛 
dapat dilihat pada Gambar 5 dengan menggunakan Persamaan 
1. 

0
𝑑(2,1)
𝑑(3,1)
⋮
𝑑(𝑛, 1)
[

0
𝑑(3,2)
⋮
𝑑(𝑛, 2)

0
⋮
⋯

⋯

0

]

Gambar 5. Matriks jarak berukuran n x n 

Pada  Gambar  5, 𝑑(𝑖, 𝑗) adalah  jarak  antara  objek  i  dan  j. 
Pada  umumnya, 𝑑(𝑖, 𝑗) adalah  nilai  postif  yang  mendekati  0 
ketika objek i dan j sangat mirip atau dekat satu sama lain dan 
menjadi lebih besar ketika kedua objek tersebut berbeda. Pada 
Gambar  5  terbentuk  matriks  segitiga  atas  karena  𝑑(𝑖, 𝑗)   =
 𝑑(𝑗, 𝑖)  dan  𝑑(𝑖, 𝑖)   =  0 .  Jarak  euclidean  merupakan 
perhitungan  jarak  yang  banyak  digunakan  karena  sensitif 
terhadap outlier yang ditunjukkan pada Persamaan 1. Variabel 
i dan j pada Persamaan 1 merupakan objek dua dimensi [2]. 

𝑑(𝑖, 𝑗) =   √(𝑥𝑙𝑜𝑛𝑔𝑖 − 𝑥𝑙𝑜𝑛𝑔𝑗)2 + (𝑥𝑙𝑎𝑡𝑖 − 𝑥𝑙𝑎𝑡𝑗)2 …………..(1) 

Setelah perhitungan jarak dan matriks, dilakukan inisialisasi 
jumlah  baris  pada  data  dengan 
untuk  menghitung 
menggunakan  fungsi  nrow.  Selanjutnya  dilakukan  pencarian 
tiga  tetangga  terdekat  dari  setiap  baris  matriks  jarak  untuk 
dilakukan  pengurutan  secara  menaik  untuk  setiap  hasil  jarak 
terdekat  pada  tetangga.  Hasil  pengurutan  dari  setiap  baris 
tetangga  terdekat  dibuat  sebuah  plot  dengan  sumbu  x  dan  y 
ialah  objek  dan  jarak  k  tetangga  terdekat.  Plot  yang  telah 
terbentuk  secara  ascending  dihitung  selisih  kemiringan  garis 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

untuk  mendapatkan  nilai  Eps  [13].  Titik  yang  mempunyai 
perubahan  slope  atau  perubahan  kemiringan  pada  plot  yang 
signifikan  akan  menjadi  nilai  Eps  yang  optimal  [3].  Pada 
Algoritma DMDBSCAN dalam menentukan nilai Eps optimal 
dilakukan  pada  setiap 
level  kepadatan  data  sehingga 
menghasilkan nilai Eps yang bervariasi sesuai dengan tingkat 
kepadatannya  [3].  Pencarian  nilai  Eps  yang  bervariasi  sesuai 
dengan tingkat kepadatannya menggunakan perhitungan selisih 
slope dengan menggunakan threshold antara 10% hingga 20%. 
Selisih slope yang mempunyai selisih antara 10% hingga 20% 
merupakan nilai Eps yang optimal [13]. 

Dalam penelitian ini, algoritma DMDBSCAN yang diadopsi 
ialah menggunakan single level density untuk menentukan nilai 
Eps optimal sehingga hanya menghasilkan satu nilai Eps saja. 
Penentuan  nilai  Eps  untuk  single  level  density  dengan 
menghitung slope antar titik dengan persamaan (y2-y1)/(x2-x1) 
dengan menggunakan nilai threshold antara 10% hingga 20%. 
Jika ada beberapa nilai Eps yang dihasilkan, maka akan dipilih 
nilai  Eps  optimal  yang  terkecil.  Angka  inilah  yang  akan 
digunakan sebagai nilai parameter Eps. 

.  Selanjutnya,  dilakukan  modifikasi  dengan  menambahkan 
pencarian k titik terdekat dari setiap baris matriks. Kemudian, 
dilakukan  pengurutan  secara  ascending  dari  hasil  pencarian 
tetangga terdekat dari matriks tersebut dan dibuat kurva. Lalu, 
pencarian  Eps  dilakukan  dengan  perhitungan  yang  telah 
disebutkan  sebelumnya.  Nilai  Eps  yang  sudah  didapatkan 
dilakukan  pembentukan  cluster  sesuai  dengan  prosedur  pada 
Algoritma DBSCAN. 
5.3.  Perbandingan hasil Algoritma DMDBSCAN sebelum dan 

setelah modifikasi  

Pada  tahapan  ini,  proses  analisis  dilakukan  berdasarkan 
analisis clustering dan implemantasi Algoritma DMDBSCAN 
pada pengelompokan data titik gempa bumi tahun 2020. Hasil 
analisis ini mencakup perbandingan nilai Eps yang dihasilkan 
menggunakan  Algoritma  DMDBSCAN  sebelum  modifikasi 
dan  setelah  modifikasi.  Evaluasi  clustering  dilakukan  pada 
tahapan ini dengan menggunakan 5 indeks validasi, yaitu nilai 
Silhoutte  Index  (SI),  indeks  Dunn,  indeks  DB,  BetaCV,  dan 
Indeks Calinski-Harabasz. 

Analisis  clustering  dilakukan  dengan  melihat  penyebaran 
titik gempa bumi di Indonesia tahun 2014 sampai dengan tahun 
2020. Sebagai contoh untuk perbandingan, digunakan data titik 
gempa  tahun  2020.  Perbandingan  nilai  Eps  yang  dihasilkan 
menggunakan  Algoritma  DMDBSCAN  sebelum  modifikasi 
dan setelah modifikasi dapat dilihat pada Tabel IV. 

TABEL IV 
PERBANDINGAN NILAI EPS YANG DIHASILKAN MENGGUNAKAN ALGORITMA 
DMDBSCAN SEBELUM DAN SETELAH MODIFIKASI 
Nilai Eps saat K = 3,4,5,6,7 

K 
3 
4 
5 
6 
7 

Eps (sebelum modifikasi) 
4.753211 
8.121499 
7.517217 
8.955864 
8.379982 

Eps (setelah modifikasi) 
0.571849 
0.717057 
0.983589 
8.955864 
6.174628 

 5 / 7 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
  
 
 
 
 
 
 
 
 
 
 
 
Berdasarkan hasil dari simulasi parameter yang ditunjukan 
pada  Tabel  IV,  dapat  dilihat  bahwa  nilai  Eps  optimal  yang 
dihasilkan  Algoritma  sebelum  dan  setelah  modifikasi 
menunjukkan perbedaan yang cukup jauh kecuali pada saat k = 
6.  Eps  optimal  yang  dihasilkan  pada  saat  k  =  6  sama-sama 
menghasilkan  nilai  Eps  optimal  sebesar  8.955864.  Untuk 
melihat  seberapa  baik  jumlah  cluster  yang  dihasilkan  dari 
beberapa  pasangan  parameter  ini  dilakukan  perhitungan  5 
indeks validasi, yaitu nilai Silhoutte Index (SI), indeks  Dunn, 
indeks  DB,  BetaCV,  dan  Indeks  Calinski-Harabasz  seperti 
yang disajikan pada Tabel V. 

TABEL V 
PERBANDINGAN NLAI INDEKS VALIDASI ALGORITMA DMDBSCAN SEBELUM 
DAN SETELAH MODIFIKASI TAHUN 2020 

Nilai Indeks Validasi 

k 

Indeks Validasi 

3 

4 

5 

6 

7 

Silhoutte 
Dunn 
Davies-Bouldin 
BetaCV 
Calinski-Harabasz 
Silhoutte 
Dunn 
Davies-Bouldin 
BetaCV 
Calinski-Harabasz 
Silhoutte 
Dunn 
Davies-Bouldin 
BetaCV 
Calinski-Harabasz 
Silhoutte 
Dunn 
Davies-Bouldin 
BetaCV 
Calinski-Harabasz 
Silhoutte 
Dunn 
Davies-Bouldin 
BetaCV 
Calinski-Harabasz 

sebelum 
modifikasi 
0.32 
0.2233 
0.4752 
0.2958 
151 
- 
Inf 
NaN 
Inf 
NaN 
0.53 
Inf 
Nan 
33.14 
Inf 
0.14 
Inf 
Nan 
97.55 
Inf 
0.14 
Inf 
NaN 
97.55 
Inf 

setelah 
modifikasi 
0.65 
1.4287 
0.1705 
0.0061 
120498 
0.49 
5.3611 
0.1025 
0.0035 
386173 
0.56 
4.0551 
0.1197 
0.0086 
61115 
0.14 
Inf 
NaN 
97.55 
Inf 
0.35 
0.2233 
0.4940 
0.3134 
148 

Berdasarkan  Tabel  V,  hasil  evaluasi  cluster  menggunakan 
indeks  validasi  untuk  penggunaan  Algoritma 
kelima 
DMDBSCAN sebelum modifikasi memperlihatkan angka yang 
kurang  baik.  Jika  masing-masing 
indeks  diperhatikan, 
Algoritma  setelah  modifikasi  menghasilkan  nilai  indeks 
Silhoutte, Dunn, dan Calinski-Harabasz yang lebih tinggi yang 
mana ketiga indeks tersebut memiliki arti bahwa semakin tinggi 
nilai  indeks  tersebut  makan  semakin  baik  cluster  yang 
dihasilkan. Sedangkan untuk nilai indeks Davies-Bouldin dan 
BetaCV, Algoritma setelah modifikasi menghasilkan nilai yang 
lebih  rendah  yang  mana  kedua  indeks  tersebut  memiliki  arti 
bahwa  semakin  rendah  nilai  indeks  tersebut  makan  semakin 
baik cluster yang dihasilkan. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

5.4.  Implementasi  modifikasi  Algoritma  DMDBSCAN  pada 
Data Titik Gempa Bumi di Indonesia Tahun 2014 – 2020 
Pada  tahap  ini  akan  dilakukan  pemilihan  data  titik  gempa 
bumi  di  wilayah  Indonesia.  Kemudian,  di  lakukan  pemilihan 
data  pada  setiap  tahunnya dan  magnitudo  diatas  5.5.  Sebagai 
contoh, digunakan data titik gempa tahun 2020 dan persebaran 
data dapat dilihat pada Gambar 6. 

Gambar 6. Titik gempa bumi tahun 2020 dengan magnitudo >= 5.5 

Setelah  dilakukan  pemilihan  terhadap  waktu  terjadinya 
gempa bumi (1 Januari 2020 – 31 Desember 2020), pemilihan 
data titik gempa bumi dilakukan terhadap besarnya magnitudo 
dan  akan  diambil  data  titik  gempa  bumi  yang  memiliki 
magnitudo diatas 5.5. Data titik gempa bumi tahun 2020 setelah 
dilakukan  proses  pemilihan  terhadap  variabel  waktu  dan 
magnitudo  menghasilkan  titik  gempa  bumi  sebanyak  76 
observasi.  Persebaran  data  dari  76  observasi  tersebut  dapat 
dilihat pada Gambar 6. 

TABEL VI 
PERBANDINGAN NLAI INDEKS VALIDASI ALGORITMA DMDBSCAN SETELAH 
MODIFIKASI SAAT KBERNILAI 3-7 DATA GEMPA BUMI TAHUN 2020 

k 

3 
4 
5 
6 
7 

Eps 

0.5718 
0.7170 
0.9835 
9.0118 
6.1746 

Indeks Validasi 

SI 
0.65 
0.49 
0.56 
0.14 
0.35 

DB 

Dunn 
1.4287  0.1705 
5.3611  0.1025 
4.0551  0.1197 

Inf 

NA 

0.2271  0.4975 

BetaCV 
0.0061 
0.0035 
0.0086 
97.52 
0.3131 

CH 
120703 
386830 
61219 
Inf 
148 

Tabel VI merupakan perbandingan hasil indeks validasi dari 
kelima nilai k dan pasangan Eps optimalnya pada data gempa 
bumi  indoensia  tahun  2020  yang  mana  nantinya  nilai  k  dan 
pasangan  Eps  optimalnya  dengan  indeks  terbaik  akan  dipilih 
untuk dijadikan sebagai nilai parameter untuk proses clustering. 
Dapat dilihat bahwa untuk tahun 2020, pasangan dengan indeks 
terbaik ditunjukkan oleh k bernilai 4 dan Eps optimal sebesar 
0.7170.  Selanjutnya  akan  dilakukan  proses  clustering  dengan 
nilai MinPts = 4 dan Eps = 0.7170. Perbandingan nilai indeks 
validasi  pada  Tabel  VI  adalah  contoh  perbandingan 
menggunakan  data  titik  gempa  bumi  Indonesia  tahun  2020. 
Simulasi ini dilakukan juga pada data gempa bumi pada tahun 
2014-2020. 

Proses  clustering  berhasil  dijalankan  dari  hasil  modifikasi 
DBSCAN  pada  pencarian  nilai  Eps  optimal  secara  otomatis 
dengan ide k-dist plot dengan menghasilkan parameter MinPts 
=  4 dan Eps =  0.7170. Parameter ini digunakan untuk proses 
clustering yang menghasilkan 2 cluster dengan noise sebanyak 
66 titik. 

 6 / 7 

 
 
 
 
 
 
 
 
Gambar 7. Wilayah cluster titik gempa bumi 

Visualisasi  wilayah  cluster  titik  gempa  bumi  di  Indonesia 
tahun 2020 dengan manitudo >= 5.5 ditunjukan pada Gambar 
7.  Icon  berwarna  hijau  menunjukan  tiitk  gempa  bumi  yang 
berada pada wilayah cluster 1. Icon berwarna biru menunjukan 
titik  gempa  bumi  yang  berada  pada  wilayah  cluster  2.  Icon 
berwarna merah menunjukan titik gempa bumi yang terdeteksi 
sebagai  noise  atau  outlier.  Hal  ini  dilakukan  juga  terhadap 
tahun 2014 sampai dengan tahun 2020. Hasil daerah kerawanan 
gempa bumi disajikan pada Tabel VII. 

TABEL VII 
DAERAH RAWAN GEMPA BUMI TAHUN 2020 

Tahun 

Jumlah 
Cluster 

Jumlah 
Noise 

Daerah Rawan 
Gempa Bumi 

2014 

2015 

2016 

2017 

2018 

2019 

2020 

2 

2 

2 

2 

2 

2 

2 

6 

37 

37 

20 

73 

72 

66 

Minahassa 
Peninsula, Sulawesi 
Banda Sea 
Sumbawa Region, 
Indonesia 
Halmahera, 
Indonesia 
Sumba Region, 
Indonesia 
Northern Molucca 
Sea 
Banda Sea 

Jumlah 
Titik 
Gempa 

2 Titik 

4 Titik 

2 Titik 

2 Titik 

4 Titik 

4 Titik 

4 Titik 

Berdasarkan Tabel VII, daerah kerawanan gempa bumi dari 
tahun  2014-2020  dapat  diketahui.  pada  tahun  2014,  daerah 
rawan gempa bumi menempati wilayah Minahassa Peninsula, 
Sulawesi.  pada  tahun  2015,  daerah  rawan  genpa  bumi 
menempati wilayah Banda Sea. pada tahun 2016, daerah rawan 
gempa bumi menempati wilayah Sumbawa Region. Pada tahun 
2017,  daerah  rawan  gempa  bumi  menempati  wilayah 
Halmahera.  Pada  tahun  2018,  daerah  rawan  gempa  bumi 
menempati  wilayah  Sumba  Region.  Pada  tahun  2019,  daerah 
rawan gempa bumi menempati wilayah Northern Molucca Sea. 
Pada  tahun  2020,  daerah  rawan  gempa  bumi  menempati 
wilayah Banda Sea. Menariknya, terlihat bahwa wilayah Banda 
Sea  muncul  beberapa  kali  pada  daerah  rawan  gempa  bumi, 
yaitu  pada  tahun  2015  dan  2020.  Hal  ini  dapat  menjadi 
perhatian untuk masyarakat setempat. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

VII. 

PENUTUP 

Analisis deskriptif data gempa bumi di Indonesia pada tahun 
2014-2020  dapat  diketahui  dengan  melihat  daerah  dengan 
jumlah titik gempa bumi terbanyak dan daerah dengan kejadian 
gempa  yang  memiliki  magnitudo  tertinggi  serta  kedalaman 
terdangkal.  Selain  itu,  modifikasi  Algoritma  DMDBSCAN 
untuk mendapatkan nilai parameter Eps yang optimal berhasil 
diterapkan dan menghasilkan parameter yang lebih baik serta 
dapat memperhitungkan nilai k dalam perhitungan k-dist plot. 
Implementasi  modifikasi  Algoritma  DMDBSCAN 
juga 
berhasil  diterapkan  untuk  mendapatkan  daerah  kerawanan 
gempa bumi. 

Berdasarkan hasil yang diperoleh, masih terdapat beberapa 
hal  yang  dapat  dilakukan  untuk  meningkatkan  performa  dari 
pemilihan  nilai  Eps  optimal  seperti  melakukan  studi  lanjutan 
untuk pencarian nilai Eps optimal karena penelitian ini hanya 
memodifikasi nilai slope pada rentang antara 10% hingga 20%, 
dilanjutkan  dengan  pemilihan  nilai  terkecil  dari  rentang 
tersebut.  Penerapan  DMDBSCAN  dalam  pencarian  nilai  Eps 
juga dapat dilakukan untuk kepadatan yang bervariasi. Selain 
itu, hasil analisis deskriptif pada penelitian ini dapat dikaitkan 
dengan  analisis  lainnya  seperti  yang  berkaitan  dengan  ilmu 
geologi dan lempengan bumi. 

DAFTAR PUSTAKA 
[1]  Sunarjo, et al. “Gempa Bumi Indonesia Edisi Populer.” Badan Meteorologi 

Klimatologi dan Geofisika, Jakarta. 2012 

[2]  Han,  Jiawei,  et  al.  “Data  Mining  Concepts  and  Techniques.”  Edisi  3. 

Morgan Kaufmann, USA. 2012 

[3]  Elbatta  MNT.  2012.  An  improvement  for  DBSCAN  algorithm  for  best 
results  in  varied  densities  [disertasi].  Gaza  (PS):  Islamic  University  of 
Gaza 

[4]  Usman  M.  2014.  Spatial  clustering  berbasis  densitas  untuk  penyebaran 
titik  panas  sebagai  indikator  kebakaran  hutan  dan  lahan  gambut  di 
Sumatera [tesis]. Bogor (ID): Institut Pertanian Bogor. 

[5]  Rahmah,  N.  (1970,  January  1).  Penentuan  Nilai  Epsilon  (Eps)  Optimal 
Pada Algoritme Dbscan Untuk Mengelompokkan Data Titik Panas Pada 
Lahan 
Repository. 
https://repository.ipb.ac.id/handle/123456789/80737. 

Sumate. 

Gambut 

IPB 

Di 

[6]  Ankerst M, Breunig MM, Kriegel HP, Sander J. OPTICS: Ordering Points 

to Identify the Clustering Structure. ACM Press; 1999. p. 49–60 

[7]  Masrofin,  F.  (2021,  January  28).  Algoritma  optics  (ordering  points  to 
identify  the  clustering  Structure).  Retrieved  March  31,  2021,  from 
https://algotech.netlify.app/blog/algoritma-optics/ 

[8]  Ester M, Kriegel HP, Sander J, Xu X. 1996. A Density-based Algorithm 
for  Discovering  Clusters  a  Densitybased  Algorithm  for  Discovering 
Clusters  in  Large  Spatial  Databases  with  Noise.  In:  Proceedings  of  the 
Second  International  Conference  on  Knowledge  Discovery  and  Data 
Mining. KDD’96. AAAI Press; 1996. p. 226–231. 

[9]  Baarsch,  Jonathan  &  Celebi,  M.  Emre.  (2012).  Investigation  of  Internal 
Validity Measures for K-Means Clustering. Lecture Notes in Engineering 
and Computer Science. 2195. 471-476. 

[10] Rendón,  Eréndira  &  Abundez,  Itzel  &  Arizmendi,  A.  &  Quiroz,  E.M.. 
(2011).  Internal  versus  external  cluster  validation  indexes.  International 
Journal of Computers and Communications. 5. 27-34. 

[11] University  of  Illinois.  (2020,  January  10).  6.1  Methods  for  Clustering 
Coursera: 

Validation. 
https://www.coursera.org/lecture/cluster-analysis/6-1-methods-for-
clustering-validationk59pn 

Retrieved 

from 

[12] Facts  & 

figures. 

(n.d.).  Retrieved  March  30,  2021, 

from 

https://www.embassyofindonesia.org/index.php/basic-facts/ 

[13] Gaonkar MN, Sawant K. 2013. AutoEps DBSCAN: DBSCAN with Eps 
automatic for large dataset. International Journal on Advanced Computer 
Theory and Engineering. 2(2): 11-16.  

 7 / 7 

 
 
 
 
 
 
 
 
 
 
 
 
"
221709765,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pemanfaatan Data AIS dalam Pemodelan 
Nowcasting Statistik Ekspor Impor Indonesia 

Jimmy Nickelson (221709765, 4SD2) 
Dosen Pembimbing: Rani Nooraeni, SST., M.Stat. 

Ringkasan— AIS merupakan salah satu sumber big data yang 
berpotensi untuk dimanfaatkan sebagai sumber data baru, yaitu 
untuk mengembangkan indikator terkait statistik ekspor impor. 
Dalam  penggunaannya,  diperlukan  tahap  preprocessing  yang 
tepat  untuk  dapat  menggambarkan  data  ekspor  impor  dengan 
baik.  Publikasi  data  ekspor  impor  Indonesia  sendiri  memiliki 
timeliness  yang  kurang,  sehingga  data  AIS  dapat  menjadi 
alternatif  solusi  untuk  meramalkan  nilai  dan  volume  ekspor 
impor  secara  real-time.  Dari  data  AIS  dapat  dibentuk  indikator 
ekspor  impor  dan  menjadi  variabel  prediktor  dalam  model 
peramalan.  Metode  Artificial  Neural  Network  (ANN)  dipilih 
juga  akan 
sebagai  model  peramalan.  Model 
dibandingkan  dengan  model  ARIMA.  Hasil  penelitian 
menunjukkan  bahwa  data  AIS  dapat  digunakan  untuk 
membentuk  indikator  terkait  statistik  ekspor  impor  dengan 
proses  cleaning  yang  tepat.  Nilai  error  yang  dihasilkan  setiap 
model menunjukkan bahwa ANN lebih unggul  daripada ARIMA. 
Sedangkan  model  ARIMA  dapat  menghasilkan  pola  peramalan 
yang lebih baik daripada model ANN pada data nilai impor. 

tersebut 

Kata  Kunci—  Ekspor  impor,  AIS,  metode  peramalan,  ANN, 

ARIMA 

I.  LATAR BELAKANG 

Big data merupakan sebuah temuan baru di bidang teknologi 
informasi  yang  seringkali  dicirikan  oleh  3V,  yaitu  Volume, 
Velocity,  dan  Variety.  Namun  dalam  pemanfaatan  big  data 
diperlukan tahapan  preprocessing  yang baik dan benar untuk 
mendapatkan  insight baru yang akurat. Salah satu contoh big 
data adalah sinyal Sistem Identifikasi Otomatis atau Automatic 
Identification System (AIS). Data AIS dipancarkan oleh kapal 
secara  periodik  dan  diterima  melalui  stasiun  pangkalan  AIS 
yang terletak di tepi daratan atau melalui penerima satelit. Data 
AIS  merekam  kapal-kapal  yang  melakukan  pelayaran 
internasional  secara  kontinu,  sehingga  data  tersebut  dapat 
digunakan untuk menggambarkan pola perdagangan lintas laut 
secara real-time.  

lainnya 

Terdapat  penelitian  yang  menggunakan  data  AIS  dengan 
tujuan  untuk  memprediksi  ekspor  minyak  mentah  [1]. 
Penelitian 
juga  menggunakan  data  AIS  untuk 
mengembangkan indikator perdagangan dan aktivitas maritim 
di  Malta  [2]  ataupun  indikator  perdagangan  lintas  laut  dunia 
menggunakan data mentah dari data AIS [3]. Tim AIS pada UN 
Global Platform juga menganalisis data AIS untuk digunakan 
dalam  berbagai  bidang  penelitian,  seperti  bidang  migrasi, 
bidang lingkungan, bidang maritim dan perikanan, juga bidang 
ekonomi  dan  perdagangan  [4].  Dari  penelitian-penelitian 
tersebut,  dapat  digarisbawahi  bahwa  AIS  memiliki  potensi 
besar dan dapat dimanfaatkan sebagai sumber data baru, seperti 
untuk mengembangkan indikator baru terkait perdagangan luar 
negeri, yang meliputi aktivitas ekspor dan impor. 

Dalam  penggunaan  data  AIS,  terdapat  tahap  yang  penting 
untuk bisa  menghasilkan  indikator  terkait perdagangan,  yaitu 
tahap  preprocessing  data.  Hal  ini  disebabkan  karena  tidak 
semua  data  AIS  yang dikirimkan kapal merupakan data yang 
terkait aktivitas perdagangan luar negeri, sehingga diperlukan 
pemilihan fitur yang dapat mewakili aktivitas tersebut. Dengan 
melakukan  hal  itu  juga  dapat  mengurangi  ukuran  dan 
kompleksitas  data,  serta  dapat  mengurangi  noise  atau  outlier 
pada data. Penelitian ini akan menjelaskan tahap preprocessing 
yang  tepat  untuk  menghasilkan  indikator  perdagangan  luar 
negeri menggunakan data AIS. 

Di  Indonesia,  pengumpulan  data  perdagangan  barang  luar 
negeri dilakukan oleh Badan Pusat Statistik yang diperoleh dari 
hasil  kompilasi  dan  survei.  Data  utama  dikumpulkan 
berdasarkan dokumen-dokumen keterangan ekspor impor yang 
dihasilkan oleh Direktorat Jenderal Bea dan Cukai setiap bulan. 
Data ekspor juga berasal dari PT Pos Indonesia, catatan lain di 
perbatasan, dan hasil survei perdagangan lintas batas laut [5]. 
Namun, pada proses publikasi statistik resmi tersebut terdapat 
jeda waktu sejak data dikumpulkan hingga data dipublikasikan, 
sehingga terjadi keterlambatan dalam publikasi statistik resmi. 
Hal  ini  tentunya  dapat  mempengaruhi  proses  pengambilan 
kebijakan,  karena  pengambilan  kebijakan  membutuhkan  data 
sebagai  landasan  penentuan  kebijakan,  dimana  data  tersebut 
harus  sesuai  dengan  kondisi 
terkini.  Untuk  membantu 
mengurangi  masalah  tersebut,  diperlukan  peramalan  atau 
nowcasting  untuk  menggambarkan  situasi  terkini  dan  untuk 
menjembatani keterlambatan publikasi statistik resmi tersebut. 
Sebagai  alternatif  solusinya,  AIS  dapat  menjadi  indikator 
baru  dan  menjadi  variabel  proxy  untuk  meramalkan  statistik 
ekspor impor. Data AIS dipancarkan oleh kapal setiap menit, 
sehingga  dapat 
tersebut 
menjadikan  data  AIS  berpotensi  untuk  digunakan  dalam 
nowcasting.  Selain  itu,  mayoritas  komoditas  ekspor  impor 
Indonesia  menggunakan  transportasi  laut  [5],  sehingga  data 
AIS  menjadi  relevan  untuk  digunakan  dalam  peramalan 
statistik ekspor impor di Indonesia. 

tersedia  secara  real-time.  Hal 

Peramalan  data 

time  series  pada  umumnya  dapat 
menggunakan  metode  statistik 
tradisional  atau  metode 
artificial  intelligence.  Model  ARIMA  merupakan  salah  satu 
metode  statistik  tradisional  yang  populer  dan  menghasilkan 
prediksi yang baik jika digunakan untuk data berpola linear [6]. 
Sedangkan  metode  artificial  intelligence,  seperti  Artificial 
Neural Network (ANN), dapat mengenali pola dari data masa 
lalu, meskipun terdapat pola nonlinear atau noise pada data [7]. 
Selain itu, metode ini juga lebih fleksibel digunakan daripada 
metode tradisional [8]. 

Berdasarkan 

akan 
tersebut, 
mengeksplorasi penggunaan data AIS dengan melakukan tahap 

penelitian 

uraian 

ini 

 1 / 8 

 
 
 
preprocessing  untuk  memonitor  statistik  ekspor  dan  impor. 
Setelah  itu,  indikator  terkait  statistik  ekspor  impor  akan 
dibangun berdasarkan data tersebut. Adapun metode peramalan 
yang akan digunakan adalah metode ANN dan ARIMA, yang 
nantinya akan dipilih model terbaik untuk  menghasilkan nilai 
indikator  statistik  ekspor  impor.  Sehingga  rumusan  masalah 
pada penelitian ini yaitu, 
1.  Bagaimana  tahap  preprocessing  data  AIS  yang  tepat 
untuk  menghasilkan  data  atau  fitur  yang  sesuai  dengan 
aktivitas ekspor impor?  

2.  Bagaimana  membentuk  indikator  statistik  ekspor  impor 

dari data AIS?  

3.  Bagaimana  hasil  peramalan  statistik  ekspor 

impor 
Indonesia  menggunakan  data  AIS  dengan  metode  ANN 
dan ARIMA? 

II.  TUJUAN PENELITIAN 

Berdasarkan  latar  belakang  tersebut,  maka  penelitian  ini 

bertujuan untuk: 
1.  Melakukan preprocessing  data AIS  untuk menghasilkan 
data atau fitur yang sesuai dengan aktivitas ekspor impor. 
indikator  yang  dapat  digunakan  untuk 

2.  Membentuk 

memprediksi statistik ekspor impor dari data AIS. 

3.  Menerapkan metode peramalan ANN dan ARIMA untuk 
memprediksi statistik ekspor impor dengan data AIS. 

III. KAJIAN PUSTAKA 

A.  Automatic Identification System (AIS) 

AIS  merupakan  sistem  komunikasi  maritim  internasional 
yang  dipancarkan  setiap  kapal  dan  digunakan  untuk  melacak 
pergerakan kapal. AIS dikembangkan oleh IMO (International 
Maritime Organization) dengan tujuan untuk membantu kapal 
menghindari  tabrakan  dan  membantu  otoritas  pelabuhan 
mengontrol  lalu  lintas  laut  secara  efisien.  IMO  mewajibkan 
AIS dipasang di atas kapal pelayaran internasional dengan berat 
300  gross  tonnage  (GT)  atau  lebih,  dan  semua  kapal 
penumpang  [9].  Pesan  AIS  secara  otomatis  dikirim  melalui 
gelombang  radio  berfrekuensi  sangat  tinggi  /  Very  High 
Frequency  (VHF)  dilengkapi  dengan  sistem  GPS  (Global 
Positioning System) setiap dua sampai sepuluh detik jika kapal 
bergerak atau setiap enam menit jika kapal diam [2]. 

B.  Korelasi Pearson 

Korelasi  merupakan  sebuah  nilai  yang  menunjukkan  kuat 
tidaknya  hubungan  antara  dua  variabel  atau  lebih.  Koefisien 
korelasi  pada  variabel  X  dan  Y  pada  n  data  dapat  dihitung 
dengan menggunakan rumus seperti persamaan (1).  

𝑟𝑋𝑌 =

𝑛 ∑

𝑛
𝑖=1

𝑋𝑖𝑌𝑖

𝑛
𝑖=1

𝑛
−∑
𝑖=1 ∑
𝑋𝑖
2√𝑛 ∑
𝑛
𝑛
2
𝑖=1 −(∑
𝑌𝑖
𝑖=1

𝑌𝑖

)

√𝑛 ∑

𝑛
𝑛
2
𝑖=1 −(∑
𝑋𝑖
𝑖=1

𝑋𝑖

2
)

𝑌𝑖

(1) 

dimana n adalah banyaknya data, X adalah variabel independen 
atau  variabel  pertama,  dan  Y  adalah  variabel  dependen  atau 
variabel  kedua.  Nilai  korelasi  akan  memiliki  rentang  nilai 
antara -1 sampai +1. Tanda negatif (-) menunjukkan hubungan 
antarvariabel berbanding terbalik, sedangkan tanda positif (+) 
menunjukkan  arah  sebaliknya.  Semakin  dekat  nilai  korelasi 
dengan  angka  satu  menunjukkan  semakin  kuat  hubungan 
antarvariabel tersebut. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

C.  Metode Seleksi Variabel 

Seleksi  variabel  merupakan  salah  satu  tahap  yang  penting 
dalam membangun model prediksi. Fokus ini yaitu menyeleksi 
beberapa  variabel 
input  yang  dapat  mendeskripsikan 
keseluruhan  data  input  secara  efisien  dan  tetap  memberikan 
hasil  prediksi  data  output  yang  baik,  sehingga  dapat 
membangun  model  yang  sederhana  dan  lebih  komprehensif 
[10].  Pada  penelitian  ini,  metode  seleksi  variabel  yang 
digunakan  yaitu  menggunakan  permutation 
importance, 
stepwise regression, nilai p-value, dan nilai korelasi.  

Permutation  importance  mengukur  tingkat  ‘kepentingan’ 
suatu  variabel  dengan  mengacak  nilai  variabel  tersebut. 
Apabila error suatu model bertambah setelah variabel tersebut 
diacak,  maka  variabel  tersebut  tergolong  ‘penting’  dalam 
model,  karena  model  memerlukan  variabel  tersebut  untuk 
melakukan prediksi [11]. Selanjutnya, variabel tersebut dipilih 
sebagai  variabel  prediktor  dalam  model  peramalan.  Terdapat 
juga  pertimbangan  untuk  mengeluarkan  variabel  apabila 
pengacakan tersebut justru menghasilkan error yang lebih kecil. 
dengan 
mempertimbangkan peningkatan suatu kriteria setiap variabel. 
Metode  ini  diawali  dengan  model  tanpa  variabel.  Kemudian 
untuk 
setiap  perulangan,  variabel  yang  memberikan 
peningkatan kriteria terbesar pada model akan dimasukkan ke 
model.  Namun 
untuk 
mengeluarkan  variabel  jika  tidak  memberikan  peningkatan 
pada model [10].  

regression  memilih 

pertimbangan 

Stepwise 

variabel 

terdapat 

juga 

Metode seleksi variabel juga dilakukan berdasarkan variabel 
yang  signifikan  pada  suatu  model,  terutama  model  statistik 
tradisional. Variabel yang memiliki p-value kurang dari tingkat 
signifikansi (α=0.05) pada model akan dipilih dan dimasukkan 
ke dalam model. Seleksi variabel juga dilakukan berdasarkan 
nilai korelasi antara variabel prediktor dengan variabel respons. 
Variabel  yang  memiliki  korelasi 
tinggi  akan  memiliki 
hubungan linear dan memiliki efek yang hampir sama dengan 
variabel  respons.  Variabel  yang  berkorelasi  melebihi  suatu 
batas nilai akan dipilih sebagai variabel prediktor. 

D.  Metode Peramalan 

AutoRegressive Integrated Moving Average (ARIMA) 
Model ARIMA merupakan salah satu model peramalan data 
runtun waktu. Model ini merupakan kombinasi dari penurunan 
(differencing) dengan autoregresi dan model rata-rata bergerak, 
sehingga  model  ini  menggunakan  informasi  dari  observasi 
masa lalu untuk membuat prediksi. Persamaan model ini dapat 
dituliskan seperti persamaan (2). 

𝑦′

𝑡 = 𝑐 + 𝜙1𝑦′

𝑡−1 + ⋯ + 𝜙𝑝𝑦′

𝑡−𝑝 + 𝜃1𝜀𝑡−1 + ⋯ + 𝜃𝑞𝜀𝑡−𝑞 + 𝜀𝑡  

(2) 

dimana y’t adalah data yt yang diturunkan (differenced series). 
Sedangkan variabel di sebelah kanan meliputi nilai lag dari yt 
dan  nilai  lag  error.  Model  ini  disebut  model  ARIMA  (p,d,q) 
dimana  p  adalah  orde  nilai  autoregressive,  d  adalah  derajat 
turunan yang digunakan, dan q adalah  orde rata-rata bergerak 
(moving  average).  Model  ARIMA  dapat  memasukkan 
informasi  lain  yang  relevan  untuk  meningkatkan  akurasi 
prediksi,  yaitu  dengan  menambahkan  variabel  eksogen  pada 
model, sehingga disebut model ARIMAX [12]. 

 2 / 8 

 
 
  
Artificial Neural Network (ANN) 
ANN  merupakan  sebuah  algoritma  pemodelan  machine 
learning  yang  digunakan  untuk  mengolah  informasi  dan 
menganalisis  data.  ANN  dibangun  mengikuti  analogi  sistem 
syaraf,  yaitu  dari  sekumpulan  unit  atau  neuron  yang  saling 
terhubung.  Neuron-neuron  tersebut  terdiri  dari  beberapa 
lapisan,  yaitu  input  layer  sebagai  lapisan  untuk  data  input, 
hidden layer sebagai lapisan untuk memproses data, dan output 
layer 
hasil 
prediksi/estimasi,  seperti  yang  terlihat  pada  Gambar  1.  Jika 
arsitektur  ANN  memiliki  lebih  dari  satu  hidden  layer,  maka 
arsitektur  tersebut  disebut  multilayer  feed-forward  neural 
network. 

untuk  mengeluarkan 

sebagai 

lapisan 

Gambar 1. Arsitektur ANN 

Salah  satu  algoritma  pembelajaran  ANN  adalah  algoritma 
backpropagation.  Algoritma  ini  mempelajari  data  dengan 
memproses data latih secara berulang. Untuk setiap perulangan, 
bobot  akan  disesuaikan  untuk  mendapatkan  error  sekecil 
mungkin antara hasil prediksi dengan data  target sebenarnya, 
yang  dapat  berupa  data  label  kelas  atau  data  numerik. 
Modifikasi  bobot  ini  dilakukan  dengan  arah  ke  belakang 
(backward) [13]. 

Metode  ANN  memerlukan  parameter  dalam  membentuk 

modelnya. Adapun beberapa parameter tersebut seperti: 
1.  Jumlah hidden layer dan neuronnya.  
2.  Fungsi aktivasi, yaitu fungsi yang mengubah input yang 
sudah  ditambah  bobot  dan  bias  menjadi  nilai  output. 
Fungsi  ini  digunakan  untuk  mengontrol  keluaran  dari 
neuron,  sehingga  hasil  yang  keluar  bisa  dibandingkan 
dengan batasan yang ditentukan. Beberapa contoh fungsi 
aktivasi seperti terlihat pada TABEL I. 

TABEL I 
FUNGSI AKTIVASI ANN 

Nama Fungsi 

Linear 
Sigmoid  

Tanh  
ReLU 
Leaky ReLU 

Fungsi aktivasi f(x) 
cx 
1
1+𝑒−𝑥  
tanh(x) 
max (0, x) 
max (αx, x) 

3.  Learning  rate,  parameter  yang  mengontrol  seberapa 
banyak perubahan jumlah bobot yang diperbarui selama 
pelatihan.  Parameter  ini  juga  mengontrol  kecepatan 
pembelajaran model.  

4.  Momentum,  parameter  yang  dapat  memperlancar 
pembelajaran algoritma sehingga juga dapat mempercepat 
proses pelatihan.  

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Tuning Parameter ANN 
Proses penentuan nilai parameter ANN merupakan hal yang 
penting,  karena  nilai  tersebut  dapat  mempengaruhi  hasil 
prediksi  dan  performa  model.  Maka,  diperlukan  tuning 
parameter untuk menentukan nilai parameter ANN yang sesuai. 
Salah satu metode tuning parameter adalah metode pencarian 
random  search.  Random  search  merupakan  salah  satu 
algoritma  pencarian  yang  melibatkan  pembangkitan  dan 
evaluasi nilai acak ke suatu fungsi objektif, dalam hal ini model 
fungsi  ANN.  Nilai  parameter  model  yang  digunakan  akan 
dibangkitkan  dengan  bilangan  acak  dan  dihitung  nilai  error-
nya. Proses tersebut diulang dengan jumlah perulangan tertentu 
dan menyimpan model yang memiliki nilai error terkecil pada 
setiap  perulangan.  Pada  akhir  perulangan  akan  terdapat  satu 
model yang memiliki nilai error terkecil dan memberikan hasil 
parameter yang terbaik [14].  

E.  Metode Evaluasi Peramalan 

Untuk  mengetahui  keakuratan  hasil  peramalan  dari  model, 
maka perlu dilakukan evaluasi terhadap hasil tersebut. Metode 
evaluasi  peramalan  juga  dapat  digunakan  untuk  membantu 
mencari  sebuah  metode  yang  optimal  dan  membandingkan 
ketepatan  dua  atau  lebih  metode  algoritma  pemodelan  yang 
berbeda.  Terdapat  beberapa  teknik  untuk  mengevaluasi  hasil 
peramalan,  salah  satunya  menggunakan  nilai  RMSE  (Root 
Mean Squared Error). 

Metode RMSE menghitung akar kuadrat dari rata-rata selisih 
kuadrat  antara  nilai  prediksi  dan  nilai  aktual.  Suatu  model 
dikatakan memiliki performa yang bagus apabila RMSE yang 
dihasilkan  minimum.  Fungsi  RMSE  dapat  dituliskan  seperti 
persamaan (3). 

𝑛
𝑅𝑀𝑆𝐸 = √∑ (
𝑡=1
dimana n adalah banyaknya data, ŷt adalah nilai prediksi, dan yt 
adalah nilai sebenarnya. 

(3) 

)2

𝑦̂𝑡−𝑦𝑡
𝑛

IV. METODE PENELITIAN  

A.  Metode Pengumpulan Data  

Pada  penelitian  ini,  data  yang  akan  digunakan  adalah  data 
AIS dan data perdagangan luar negeri Indonesia, yang meliputi 
nilai  dan  volume  ekspor  impor.  Data  AIS  yang  digunakan 
bersumber  dari  penyedia  data  AIS  exactEarth  via  Platform 
Global UN. Cakupan data yang digunakan yaitu data AIS yang 
dikirim kapal di wilayah bounding box Indonesia, terutama di 
dalam wilayah pelabuhan-pelabuhan tertentu yang terdaftar di 
situs web Maritime Safety Information (MSI)1, yaitu sebanyak 
123  pelabuhan.  Rentang  data  yang  digunakan  yaitu  antara 
Desember  2018  hingga  Desember  2020,  dengan  jumlah  fitur 
sebanyak 35 dan jumlah record sebanyak 661.8 juta. Adapun 
fitur yang terdapat pada data AIS di antaranya seperti terlihat 
pada TABEL II. 

Data  statistik  perdagangan  luar  negeri  Indonesia  yang 
digunakan adalah data ekspor dan impor yang bersumber dari 

1 National Geospatial – Maritime Safety Office, “World Port Index: Query 
Results  of  Indonesian  Port”,  https://msi.nga.mil/queryResults?publications/ 

world-port-index?countryName= Indonesia&output=html (diakses 8 Februari 
2021). 

 3 / 8 

 
 
 
 
 
  
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Status navigasi pada data AIS menunjukkan status kapal 
saat pesan tersebut dikirim. Beberapa status navigasi yang 
ada  seperti  ‘under  way  using  engine’,  ‘at  anchor’, 
‘moored’,  ‘sailing’,  ‘engaged  in  fishing’  ,  ‘not  under 
command’, ‘restricted manoeuvrability’ , ‘aground’, dan 
‘not defined’. Filter dilakukan pada pesan AIS yang status 
navigasi  kapal  yang  dilaporkan  terkait  dengan  aktivitas 
ekspor  impor.  Status  navigasi  yang  dimaksud  yaitu  ‘at 
anchor’,  ‘moored’,  dan  ‘restricted  manoeuvrability’. 
Kapal  dengan  status  navigasi  tersebut  dapat  melakukan 
bongkar  muat  atau  perpindahan  muatan  dari/ke  kapal 
sehingga terkait dengan aktivitas ekspor impor [17]. 

•  Filter 4: non-zero draught 

Draught (sarat air kapal) merupakan jarak permukaan air 
dengan  bagian  bawah  kapal.  Sarat  air  kapal  dapat 
digunakan untuk mengestimasi potensi berat kapal yang 
diangkut [2]. Sarat air kapal pada pesan AIS bernilai 0.1 
hingga 25.5 meter. Nilai 25.5 menunjukkan nilai sarat air 
25.5  m  ke  atas.  Sedangkan  nilai  nol  merupakan  nilai 
default atau jika tidak tersedia [15], sehingga pesan yang 
bernilai draught nol akan dieliminasi dari data. 

•  Filter 5: tipe kapal yang relevan 

Tipe  kapal  yang  ada  pada  data  AIS  seperti  ‘fishing’, 
‘passenger’, ‘sailing’, ‘military’, ‘pilot’, ‘cargo’, ‘tanker’, 
dll.  Data  AIS  difilter  berdasarkan  tipe  kapalnya  yang 
terkait dengan aktivitas ekspor impor, yaitu kapal ‘cargo’ 
dan kapal ‘tanker’. Tipe kapal ini dapat mengindikasikan 
komoditas kapal yang diangkut [2].  
•  Filter 6: kapal yang berada di pelabuhan 

Dari data AIS yang dikirim suatu kapal akan diidentifikasi 
apakah  kapal  tersebut  berada  di  suatu  pelabuhan  atau 
tidak. Filter ini digunakan untuk mendapatkan data kapal 
yang  melakukan  aktivitas  ekspor  impor  di  pelabuhan. 
Kapal  ada  di  suatu  pelabuhan  dan  dimasukkan  sebagai 
data valid apabila pesan AIS dikirim di dalam bounding 
box  pelabuhan  tersebut.  Terdapat  pula  pertimbangan 
untuk  mengeliminasi  data  apabila  kapal  tersebut  tidak 
berada di pelabuhan mana pun.  

Membentuk Indikator dari Data AIS 
Indikator terkait aktivitas ekspor impor akan dikembangkan 
dan  diagregasi  dari  data  AIS.  Beberapa  fitur  pada  data  AIS 
dapat  terlihat  pada  TABEL  II.  Dari  beberapa  fitur  tersebut, 
tidak semua fitur dapat digunakan untuk membentuk indikator 
ekspor  impor,  sehingga  akan  dipilih  fitur  yang  berkaitan  dan 
dapat mewakilinya indikator ekspor impor. Fitur data AIS yang 
terpilih  yaitu,  ‘MMSI’,  ‘DTG’,  ‘draught’,  ‘longitude’,  dan 
‘latitude’. Adapun metode penghitungan setiap indikator dapat 
dijelaskan seperti TABEL III. Indikator tersebut akan menjadi 
variabel prediktor untuk meramalkan statistik ekspor impor. 

situs  web  BPS 2.  Data  tersebut  terdiri  dari  nilai  dan  volume 
ekspor impor Indonesia dalam satuan dolar US dan kilogram. 
Data yang digunakan merupakan data bulanan dengan rentang 
waktu  yang  sama,  yaitu  antara  Desember  2018  hingga 
Desember 2020. 

Nama Fitur 

MMSI 

DTG 
Vessel_type 

Nav_status 

SOG 

COG 

Draught 

Longitude 
Latitude 

TABEL II 
FITUR PADA DATA AIS 

Keterangan 
Nomor  unik  kapal  Maritime  Mobile  Service  Identity 
(MMSI) 
Tanggal observasi (yyyy-MM-dd'T'HH:mm:ssZ) 
Tipe kapal, seperti ‘cargo’, ‘tanker’, ‘tug’, ‘passenger’, 
dll 
Status  navigasi  kapal,  seperti  ‘at  anchor’,  ‘moored’, 
‘aground’, dll 
Speed  Over  Ground,  kecepatan  kapal  terhadap  tanah 
atau benda tetap lainnya (knot) 
Course Over Ground, arah sebenarnya dari kemajuan 
kapal terhadap permukaan bumi (derajat) 
Jarak vertikal antara garis air dan dasar lambung kapal 
(meter) 
Koordinat garis bujur dalam WGS 84 (derajat desimal) 
Koordinat  garis  lintang  dalam  WGS  84  (derajat 
desimal) 

B.  Metode Pengolahan Data 

Metode Preprocessing Data 
Pada  data  AIS  bisa  dimungkinkan  terdapat  pesan  yang 
memiliki  noise  atau  outlier.  Misalnya,  terdapat  pesan  yang 
memiliki MMSI tidak sesuai dengan ketentuan. Terdapat juga 
pesan yang tidak terkait dengan aktivitas ekspor impor, seperti 
pesan yang dikirim oleh kapal penumpang, kapal militer, dll. 
Untuk  itu,  diperlukan  tahap  preprocessing  yang  tepat  untuk 
mengurangi noise pada data AIS dan mendapatkan data yang 
terkait  ekspor  impor,    juga  sekaligus  menghapus  fitur  yang 
terdapat missing data. Tahap preprocessing tersebut dilakukan 
dengan  melakukan  beberapa  langkah/kaidah  untuk  memfilter 
data AIS. 
•  Filter 1: MMSI kapal yang valid 

MMSI  kapal  yang  valid  mempunyai  jumlah  karakter 
angka sebanyak sembilan digit [15], sehingga pesan AIS 
yang memiliki MMSI kurang dari atau lebih dari sembilan 
digit, dengan kata lain MMSI < 100000000 atau MMSI > 
999999999 akan dieliminasi dari data. 
•  Filter 2: kapal yang melakukan pelayaran 

Kapal  yang  melaporkan  posisi  dalam  area  kecil  selama 
periode  dua  tahun  dieliminasi  dari  data.  Filter  ini 
digunakan  untuk  menghapus  kapal-kapal  yang  tidak 
meninggalkan pelabuhan atau hanya berlayar dalam area 
kecil  selama  periode  tersebut  tidak  melakukan  aktivitas 
perdagangan,  sehingga  dikeluarkan  dari  unit  yang 
berkontribusi  terhadap  ekspor  impor  [16].  Ukuran  area 
yang digunakan adalah 0.1 derajat desimal (11.1 km).  

•  Filter 3: status kapal berlabuh 

2 Badan Pusat Statistik, “Ekspor dan Impor”, https://www.bps.go.id/exim/ 

(diakses 8 Februari 2021) 

 4 / 8 

 
 
 
 
 
 
 
No 

Nama 
Indikator 
1  Waktu kapal 

di pelabuhan / 
timeInPort 
(X1) 

2 

3 

4 

Jumlah unik 
kapal di 
pelabuhan / 
numVessel 
(X2) 

Jumlah 
kunjungan 
kapal / 
numVisit (X3) 

Jumlah 
perubahan 
sarat air kapal 
/numDraught
Diff  positif 
(X4) dan 
negatif (X5) 

5  Besaran 

perubahan 
sarat air kapal 
/sumDraught
Diff positif 
(X6) dan 
negatif (X7) 

TABEL III 
INDIKATOR AIS YANG DIBENTUK 

Keterangan 

ini  menghitung 

tersebut  dihitung 

Indikator 
rata-rata  waktu  yang 
dihabiskan  kapal  di  suatu  pelabuhan  dalam  satuan 
detik. Metode yang digunakan yaitu menghitung selisih 
waktu pesan AIS yang dikirimkan kapal saat berada di 
dalam  wilayah  pelabuhan.  Kemudian  selisih  waktu 
tersebut  dijumlahkan  setiap  kapal  per  pelabuhan. 
Apabila  waktu  yang  dikirim  kapal  melebihi  tiga hari, 
maka pesan tersebut akan dieliminasi dari data karena 
diasumsikan  pesan  tersebut  dikirim  oleh  kapal  yang 
sedang diperbaiki atau hal lain. 
Fitur yang digunakan: 
MMSI, DTG, Longitude, Latitude 
Indikator  ini  menghitung  jumlah  unik  kapal  yang 
masuk atau  berada  di  wilayah  pelabuhan berdasarkan 
nomor  MMSI-nya  (nomor  identitas  kapal),  sehingga 
indikator ini dihitung dengan satuan jumlah kapal. 
Fitur yang digunakan: 
MMSI, DTG, Longitude, Latitude 
Jumlah  kunjungan  kapal  dapat  dihitung  dengan 
menggunakan  koordinat  pesan  AIS  yang  dikirimkan 
kapal.  Apabila  pesan  sebelumnya  dikirim  di  luar 
wilayah  pelabuhan  dan  pesan  selanjutnya  dikirim  di 
dalamnya,  maka  pesan 
satu 
kunjungan pada pelabuhan tersebut. 
Fitur yang digunakan: 
MMSI, DTG, Longitude, Latitude 
Indikator 
jumlah  kapal  yang 
mengalami perubahan sarat air kapal saat di pelabuhan. 
Nilai sarat air yang semakin besar menunjukkan bahwa 
semakin  dalam  dan  semakin  berat  kapal  tersebut. 
Perubahan sarat kapal dihitung dengan menyelisihkan 
sarat air kapal pada pesan AIS selanjutnya dengan sarat 
air  kapal  yang  dikirimkan  pada  pesan  sebelumnya. 
Perubahan tersebut akan dibedakan menjadi kapal yang 
mengalami  kenaikan  dan  kapal  yang  mengalami 
penurunan  sarat  air  kapal.  Kapal  yang  mengalami 
kenaikan sarat air kapal dapat ditunjukkan dari selisih 
bernilai  positif  dan  mengindikasikan  penambahan 
beban/berat  pada  kapal,  indikator  ini  akan  digunakan 
untuk  melihat  nilai/volume  ekspor.  Begitu  pula 
sebaliknya. 
Fitur yang digunakan: 
MMSI, DTG, Draught, Longitude, Latitude 
Indikator  ini  menghitung  besaran  perubahan  sarat  air 
kapal  saat  di  pelabuhan  dalam  satuan  meter.  Metode 
penghitungan  perubahan  sarat  air  kapal  sama  seperti 
metode 
ini 
menghitung  jumlah  besaran  perubahan  yang  terjadi 
pada setiap kapal di pelabuhan. 
Fitur yang digunakan: 
MMSI, DTG, Draught, Longitude, Latitude 

sebelumnya.  Bedanya, 

ini  menghitung 

indikator 

Transformasi Data 
Data ekspor impor dan indikator dari data AIS akan dihitung 
pertumbuhannya  (growth  rate)  setiap  bulan  dengan  rumus 
pertumbuhan seperti persamaan (4). 

∆𝑦𝑡 =

𝑦𝑡−𝑦𝑡−1
𝑦𝑡−1

× 100  

(4) 

dimana Δyt adalah pertumbuhan yt terhadap yt-1; yt-1 adalah data 
pada periode sebelumnya; dan yt adalah data periode sekarang. 
Data  pertumbuhan  digunakan  dalam  penelitian  ini  karena 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

memberikan nilai korelasi yang lebih baik antara indikator AIS 
dengan  data  ekspor  impor.  Selain  itu,  transformasi  ini  juga 
digunakan untuk melihat dan membandingkan pola pergerakan 
kedua data, serta menjadikan data lebih stasioner. Selanjutnya,  
data tersebut distandardisasi menggunakan persamaan (5). 

𝑍𝑖 =

𝑋𝑖−𝑋̅
𝑆𝑋
dimana   ̅X  dan  SX  adalah  rata-rata  dan  standar  deviasi  dari 
atribut X. Standardisasi berguna untuk memberikan bobot yang 
sama  pada  semua  atribut,  menormalkan  nilai  input,  dan 
mempercepat  proses  pembelajaran  model,  terutama  pada 
algoritma Neural Network [13]. 

(5) 

Penyeleksian Variabel Prediktor 
Beberapa  simulasi  variabel  prediktor  dilakukan  untuk 
menentukan variabel input model peramalan dan mendapatkan 
hasil yang optimal. Adapun beberapa simulasi variabel tersebut 
yaitu, 
1.  Seluruh indikator AIS yang telah dihasilkan. 
2. 

Indikator  AIS  yang  diseleksi  berdasarkan  permutation 
importance dari model ANN.  
Indikator  AIS  yang  diseleksi  menggunakan  stepwise 
regression.  Pada  penelitian  ini,  BIC  digunakan  sebagai 
kriteria untuk menyeleksi variabel dalam model. 
Indikator AIS yang signifikan dalam model ARIMA. 
Indikator AIS yang memiliki korelasi dengan data ekspor 
impor lebih dari batas yang ditentukan, yaitu 0.25.  

3. 

4. 
5. 

C.  Metode Analisis 

Pembagian Data Train dan Data Test 
Data  yang digunakan, yaitu indikator AIS dan data  ekspor 
impor, merupakan data bulanan selama periode dua tahun. Data 
tersebut  berjumlah  24  series  dan  akan  dibagi  menjadi  80% 
sebagai data latih dan 20% sebagai data uji. Data latih (training 
set) akan menggunakan data periode Januari 2019 hingga Juli 
2020, yaitu sebanyak 19 data. Sedangkan data uji (test set) akan 
menggunakan  data  periode  Agustus  hingga  Desember  2020, 
atau sebanyak 5 data. 

Model Peramalan 
Pada penelitian ini, statistik ekspor impor akan diramalkan 
dengan indikator dari data AIS menggunakan metode ANN dan 
ARIMA.  Model  ARIMA  akan  menggunakan  indikator  dari 
data  AIS  sebagai  variabel  eksogen  untuk  meramalkan  data 
ekspor impor. Parameter dalam model ARIMA, yaitu d, akan 
menggunakan  nilai  0,  karena  tidak  melakukan  differencing 
pada  series  data.  Sedangkan  untuk  parameter  p  dan  q  akan 
ditentukan  menggunakan  nilai  AIC  dan  BIC  terkecil  yang 
dihasilkan setiap model [6].  

TABEL IV 
NILAI PARAMETER ANN 

Nama Parameter 
Jumlah hidden layer 
Jumlah neuron 
Fungsi aktivasi 

Learning rate  
Momentum 

Batasan Nilai Parameter 
[1, 4] 
[1, 100] 
{‘identity’, ‘tanh’, ‘sigmoid’, 
‘relu’, ‘leaky_relu’} 
[0.0, 1.0] 
[0.0, 1.0] 

 5 / 8 

 
 
  
Untuk  model  ANN,  arsitektur  yang  digunakan  dalam 
penelitian ini yaitu Multilayer Feed-forward Neural Network, 
yang  memiliki  lebih  dari  satu  hidden  layer.  Algoritma 
pembelajaran yang digunakan yaitu algoritma backpropagation, 
dimana  bobot  setiap 
lapisan  akan  disesuaikan  untuk 
mendapatkan  error  sekecil  mungkin  antara  hasil  prediksi 
dengan  data  target  sebenarnya.  Untuk  mendapatkan  hasil 
prediksi dengan model ANN, terdapat beberapa parameter yang 
perlu ditentukan, seperti terlihat pada TABEL IV. Karena nilai 
parameter akan mempengaruhi hasil dan performa model, maka 
penentuan parameter dan topologi jaringan model ANN  akan 
dilakukan  dengan  tuning  parameter  menggunakan  random 
search.  Nilai  acak  akan  dibangkitkan  pada  setiap  parameter 
ANN  dengan  rentang  nilai  batasan  sesuai  pada  TABEL  IV. 
Proses  tersebut  akan  diulang  dengan  jumlah  iterasi  sebanyak 
100.  

V.  HASIL DAN PEMBAHASAN 

A.  Hasil Preprocessing Data AIS 

Jumlah  data  AIS  yang  digunakan,  yaitu  pada  periode 
Desember 2018 hingga Desember 2020, berjumlah 661.8 juta 
record.  Beberapa  langkah  filter  dilakukan  terhadap  data 
tersebut untuk mengurangi  noise dan mendapatkan data yang 
terkait ekspor impor, seperti yang sudah dijelaskan pada bab IV. 
TABEL V menunjukkan jumlah pesan AIS  pada setiap tahap 
filter,  mulai  dari  sebelum  difilter  hingga  jumlah  data  setelah 
difilter.  

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

pengeruk,  dan  tipe  kapal  lainnya.  Terdapat  juga  pesan  yang 
tidak  teridentifikasi  atau  digolongkan  tipe  kapal  ‘unknown’ 
yang mencakup 3.35%  dari data. Dari semua enam tahap filter 
tersebut,  didapatkan  jumlah  data  AIS  yang  bersih,  yaitu 
sebanyak  73.6  juta  dari  661.8  juta  record.  Selanjutnya,  data 
yang bersih ini akan digunakan untuk dilakukan analisis lebih 
lanjut. 

Aground
Engaged In Fishing
Not Under Command
Restricted Manoeuvrability
Not Defined
Unknown
Moored
Underway Sailing
At Anchor
Under Way Using Engine

0% 10% 20% 30% 40% 50% 60%

Gambar 2. Persentase data AIS setelah filter kedua berdasarkan status navigasi 
kapal 

34.50% 33.41%

8.27%

15.51%

3.12%

1.86%

3.35%

Tanker

Cargo

Tug

Passenger Dredging Other Unknown

Gambar 3. Persentase data AIS setelah filter keempat berdasarkan tipe kapal 

TABEL V.  
JUMLAH DATA AIS PADA SETIAP TAHAP FILTER 

B.  Indikator yang Terbentuk dari Data AIS 

Keterangan 

Seluruh pesan AIS 
Filter 1: MMSI kapal yang valid 
Filter 2: kapal yang melakukan pelayaran 
Filter 3: status kapal berlabuh 
Filter 4: non-zero draught 
Filter 5: tipe kapal yang relevan 
Filter 6: kapal yang berada di pelabuhan  

Jumlah Pesan AIS 
661,847,517 
655,794,385 
640,138,932 
154,253,335 
145,645,162 
98,837,324 
73,576,052 

Perbandingan Pola Indikator AIS dengan Ekspor Impor 
Dari  data  AIS  yang  telah  bersih,  dibentuk  indikator  yang 
terkait  statistik  ekspor  impor.  Indikator  yang  terbentuk  yaitu 
seperti  pada  TABEL  III.  Apabila  series  indikator  tersebut 
dibandingkan  polanya  dengan  series  data  ekspor  impor  pada 
periode  Desember  2018  hingga  Desember  2020,  maka  akan 
terlihat seperti Gambar 4.  

Pada  TABEL  V,  dapat  terlihat  bahwa  tahap  filter  ketiga, 
yaitu  filter  berdasarkan  status  kapal  berlabuh,  mengurangi 
banyak  data  atau  sebesar  75%  dari  data  sebelumnya.  Filter 
ketiga  menggunakan  status  navigasi  kapal  untuk  menentukan 
status  kapal  berlabuh.  Jika  dilihat  berdasarkan  status 
navigasinya,  maka  distribusi data  AIS  berdasarkan  data  pada 
filter  sebelumnya  ditunjukkan  pada  Gambar  2.  Status  ‘under 
way  using  engine’  mendominasi  data  sebesar  53.57%, 
sedangkan status navigasi kapal yang terkait dengan aktivitas 
ekspor impor, yaitu ‘restricted manoeuvrability’, ‘moored’, dan 
‘at anchor’ tercakup dalam data sebesar 25.09% dari data. Hal 
tersebut  menjadikan  banyak  data  yang  dieliminasi.  Terdapat 
pula status ‘unknown’ dan ‘not defined’ yang mencakup sekitar 
12.08% dari data. 

Tahap  filter  kelima  menggunakan 

tipe  kapal  untuk 
menentukan  tipe  kapal  yang  terkait  dengan  ekspor  impor. 
Gambar  3  menunjukkan  distribusi  data  AIS  pada  tahap  filter 
keempat    jika  dilihat  berdasarkan  tipe  kapal.  Terlihat  bahwa 
tipe  kapal  yang  terkait  dengan  aktivitas  ekspor  impor,  yaitu 
kapal  kargo  dan  kapal  tangki,  mencakup  67.9%  dari  data. 
Sisanya merupakan kapal tunda (tug), kapal penumpang, kapal 

Gambar 4. Plot series indikator AIS dan data ekspor impor dari Desember 2018 
hingga Desember 2020 

 6 / 8 

 
 
 
 
 
indikator 

Terlihat  bahwa  indikator  ‘timeInPort’,  ‘numVessel’,  dan 
‘numVisit’  memiliki pola pergerakan yang sama dengan data 
dan 
ekspor.  Sedangkan 
‘sumDraughtDiff’ memiliki pola yang hampir sama pada data 
impor, terutama pada awal periode. Namun terdapat kenaikan 
yang  tinggi  untuk  indikator  ‘timeInPort’,  ‘numDraughtDiff’, 
dan ‘sumDraughtDiff’ pada bulan Desember 2019, sedangkan 
pada data ekspor impor tidak memiliki pola tersebut. 

‘numDraughtDiff’ 

Indikator terkait aktivitas ekspor impor dikembangkan dan 
diagregasi dari data AIS seperti yang sudah dijelaskan pada bab 
IV. Untuk mengetahui hubungan antara indikator AIS dengan 
data ekspor impor, maka dilakukan penghitungan nilai korelasi 
antara  kedua  variabel  tersebut.  Gambar  5  menunjukkan  nilai 
korelasi Pearson antara data ekspor impor dengan indikator AIS 
pada 
tingkat  pertumbuhannya.  Terlihat  bahwa  dengan 
memfilter data AIS yang relevan dengan aktivitas ekspor impor, 
seperti  pada  TABEL  V,  mampu  memberikan  korelasi  yang 
lebih baik antara indikator AIS dan data ekspor impor. 

(a)   

         (b) 

Gambar  5.  Matriks  korelasi  indikator  AIS  dengan  data  ekspor  impor  (a) 
sebelum memfilter data AIS dan (b) setelah memfilter data AIS 

Nilai  korelasi  yang  positif  menunjukkan  bahwa  tingkat 
pertumbuhan indikator AIS memiliki memiliki hubungan yang 
positif dengan data ekspor impor. Indikator AIS ‘numVisit’ dan 
‘numVessel’  memiliki  tingkat  korelasi  cukup  kuat  (≥0.50) 
dengan nilai dan volume ekspor, sedangkan terhadap nilai dan 
volume  impor  memiliki  tingkat  korelasi  sedang  (≥0.25).  Dua 
indikator  tersebut  memiliki  tingkat  korelasi  tertinggi  jika 
dibandingkan indikator AIS lainnya. 

Indikator AIS sebagai Variabel Prediktor 

TABEL VI.  
SIMULASI SELEKSI VARIABEL PREDIKTOR 

Nama 
Parameter 
Nilai 
Ekspor 

Tanpa 
Seleksi 
X1, X2, X3, 
X4, X6 

Permutation 
Importance 
X2, X6 

Stepwise 
Regression 
X3 

Volume 
Ekspor 
Nilai 
Impor 
Volume 
Impor 

X1, X2, X3, 
X4, X6 
X1, X2, X3, 
X5, X7 
X1, X2, X3, 
X5, X7 

X2, X3, X4, 
X6 
X1, X2, X7 

X1 

X2 

X2 

X3 

Var. 
Sig 
X1, 
X2, 
X3 
X3 

X1, 
X3 
X3 

Korelasi 

X2, X3, 
X4, X6 

X1, X2, 
X3, X4 
X2, X3, 
X5 
X2, X3 

Semua  indikator  yang  terbentuk  dari  data  AIS  digunakan 
sebagai  variabel  prediktor  untuk  memprediksi  data  ekspor 
impor.  Beberapa  simulasi  seleksi  variabel  juga  dilakukan 
tersebut  untuk  menentukan  variabel 
terhadap 

indikator 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

prediktor yang tepat bagi model peramalan, seperti permutation 
importance  model  ANN,  stepwise  regression,  variabel  yang 
signifikan  pada  model  ARIMA  (α=0.05),  dan  nilai  korelasi 
yang  lebih  dari  0.25.  Variabel  prediktor  yang  terpilih  untuk 
setiap simulasi tersebut dapat terlihat pada TABEL VI. 

C.  Hasil Peramalan Ekspor Impor dengan ANN dan ARIMA 
Indikator  yang  telah  dihasilkan  akan  dijadikan  variabel 
prediktor  untuk  meramalkan  nilai  dan  volume  ekspor  impor 
menggunakan model ANN dan ARIMA. Adapun nilai RMSE 
hasil  prediksi  model  peramalan  seperti  terlihat  pada  TABEL 
VII.  Berdasarkan  TABEL  VII,  dapat  terlihat  bahwa  model 
ANN  memiliki nilai  error  yang lebih kecil daripada ARIMA 
jika  digunakan  untuk  meramalkan  nilai  dan  volume  ekspor 
serta  volume  impor.  Sedangkan  untuk  nilai  impor,  model 
ARIMA  lebih  cocok  digunakan,  karena  menghasilkan  nilai 
error yang lebih kecil. 

Pada  TABEL  VII  juga  dapat  terlihat  bahwa  terdapat 
beberapa  variabel  AIS  yang  tidak  signifikan  jika  digunakan 
sebagai  variabel  prediktor  pada  model  ARIMA,  bahkan 
terdapat pula variabel yang tidak signifikan sama sekali. Pada 
model  ANN,  terdapat  beberapa  model  yang  ada  indikasi 
overfitting pada model tersebut. Hal tersebut dapat dilihat dari 
nilai error pada data train yang kecil, namun memiliki selisih 
nilai error yang besar pada data test, terutama pada model ANN 
terhadap data impor.  

TABEL VII.  
NILAI RMSE HASIL PREDIKSI MODEL ANN DAN ARIMA 

ANN 

ARIMA 

Variabel 
Respons 
Nilai 
Ekspor 

Variabel Prediktor 

Volume 
Ekspor 

Test 
1.2118 
0.9310 
0.7852b 
1.3935 
0.7950 
1.4106 
1.3347 
1.2974 
1.2938 
1.2305b 
1.0299b 
1.2387 
1.4943 
1.2611 
1.5844 
1.4515 
0.7581b 
1.2394 
1.1988 
1.4102 
a. Variabel prediktor yang signifikan dalam model ARIMA 
b. Nilai terkecil untuk setiap variabel respons 

X1a, X2a, X3a, X4, X6 
X2a, X6a 
X3a 
X1a, X2a, X3a 
X2, X3, X4, X6 
X1, X2, X3a, X4, X6 
X2, X3a, X4, X6 
X2a 
X3a 
X1, X2, X3a, X4 
X1a, X2, X3a, X5, X7 
X1, X2, X7 
X2 
X1a, X3a 
X2, X3a, X5 
X1, X2, X3a, X5, X7 
X1 
X3a 
X3a 
X2, X3a 

Train 
0.1068b 
0.2934 
0.7223 
0.2020 
0.4870 
1.1913 
1.1937 
1.3341 
1.3618 
1.0408b 
0.0055b 
0.0685 
0.6839 
0.3946 
0.3344 
0.0323b 
0.8067 
0.7839 
0.7697 
0.6436 

Volume 
Impor 

Nilai 
Impor 

Train 
0.3769 
0.4905 
0.4569 
0.4334 
0.3494b 
0.7685b 
0.9694 
1.1668 
0.9768 
0.7886 
0.4112b 
0.7100 
0.7173 
0.5232 
0.5655 
0.6563 
0.5663b 
0.7072 
0.7072 
0.6822 

Test 
1.6388 
1.4856 
1.2496b 
1.4063 
1.8891 
1.8301 
1.9395 
1.2391b 
1.7833 
1.8389 
1.0004 
0.9077 
0.9944 
2.2097 
0.9061b 
1.7721 
1.6655 
1.4441b 
1.4441b 
1.6684 

Nilai RMSE pada data train sebesar 0.0055, sedangkan pada 
data  test  sebesar  1.0299.  Hal  tersebut  menjadikan  model 
ARIMA  dipilih  sebagai  model  peramalan  pada  nilai  impor, 
selain juga karena memiliki nilai error yang lebih kecil. Namun 
untuk model yang lain, model ANN memiliki nilai error yang 
lebih kecil sehingga lebih unggul daripada ARIMA. 

Model terbaik dipilih berdasarkan nilai  error terkecil yang 
dihasilkan  pada  data  test.  Variabel  prediktor  yang  digunakan 
pada  setiap  model  dan  parameter  model  terbaik  ditunjukkan 
pada TABEL VIII. Dapat terlihat bahwa seleksi variabel yang 

 7 / 8 

 
 
 
 
 
 
Parameter Model Terbaik 

dengan cakupan data statistik ekspor impor. 

terbaik yaitu menggunakan stepwise regression, nilai korelasi, 
dan  permutation  importance.  Model  ANN  dipilih  untuk 
meramalkan  nilai  ekspor, volume  ekspor, dan  volume  impor. 
Sedangkan untuk nilai impor menggunakan model ARIMA. 

TABEL VIII. 
PARAMETER MODEL TERBAIK DAN VARIABEL PREDIKTOR YANG DIGUNAKAN 

Variabel 
Respons 
Nilai 
ekspor 

Seleksi 
Variabel 

Stepwise 
regression 

Variabel 
Prediktor 
numVisit (X3) 

Volume 
ekspor 

Nilai 
korelasi 

Nilai 
impor 

Nilai 
korelasi 

Volume 
impor 

Permutation 
importance 

timeInPort (X1), 
numVessel (X2), 
numVisit (X3), 
numDraughtDiff 
positif(X4) 
numVessel (X2), 
numVisit (X3), 
numDraughtDiff  
negatif (X5) 
timeInPort (X1) 

hidden_layer': [75, 50, 100], 
'activation': 'leaky_relu', 
'learning_rate': 0.7348, 
'momentum': 0.5662 
hidden_layer': [60, 9, 69], 
'activation': 'leaky_relu', 
'learning_rate': 0.1513, 
'momentum': 0.4370 

ARIMA (2,0,0) 

hidden_layer': [62, 88, 91], 
'activation': 'leaky_relu', 
'learning_rate': 0.9829, 
'momentum': 0.5484 

Model  yang  terbaik  yang  telah  diperoleh  digunakan  untuk 
melakukan  peramalan  setiap  variabel  respons.  Adapun  nilai 
hasil  peramalan  untuk  lima  bulan  jika  dibandingkan  dengan 
nilai aktual seperti terlihat pada Gambar 6. Terlihat bahwa pada 
bulan Desember 2020, semua hasil prediksi memiliki arah yang 
berbeda.  Pada  Gambar  6  menunjukkan  hasil  peramalan  yang 
memiliki  pola  yang  hampir  sama  dan  error  yang  kecil,  yaitu 
peramalan  terhadap  nilai  impor.  Sedangkan  terhadap  data 
ekspor,  hasil  peramalan  tidak  terlalu  sama.  Karena  model 
peramalan  nilai  impor  menggunakan  ARIMA,  maka  dapat 
dikatakan  bahwa  model  ARIMA  dapat  menghasilkan  pola 
peramalan yang lebih baik daripada ANN, terutama pada nilai 
impor dengan indikator AIS sebagai variabel prediktor. 

Gambar 6. Plot prediksi yang polanya hampir sama dan error kecil 

VI.  KESIMPULAN DAN SARAN 
Berdasarkan hasil penelitian, dapat disimpulkan bahwa data 
AIS  dapat  digunakan  untuk  membentuk  indikator  terkait 
statistik  ekspor  impor.  Dengan  melakukan  proses  cleaning 
yang  tepat,  indikator  yang  terbentuk  dari  data  AIS  dapat 
memiliki pola hubungan yang lebih baik dengan statistik ekspor 
impor. Adapun indikator yang terbentuk yaitu waktu kapal di 
pelabuhan,  jumlah  unik  kapal,  jumlah  kunjungan  kapal,  dan 
jumlah serta besaran perubahan sarat air kapal.  Indikator AIS 
juga  dapat  digunakan  sebagai  variabel  prediktor  untuk 
meramalkan data ekspor impor. Model ANN merupakan model 
terbaik  untuk  meramalkan  nilai  ekspor,  volume  ekspor,  dan 
volume  impor.  Sedangkan  untuk  nilai  impor  menggunakan 
metode peramalan ARIMA. Model ANN lebih unggul karena 
memberikan  nilai  error  yang  kecil  untuk  tiga  dari  empat 
statistik ekspor impor, sedangkan model ARIMA memberikan 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

pola  peramalan  yang  lebih  baik  daripada  model  ANN  pada 
statistik nilai impor.  

Adapun saran untuk penelitian selanjutnya yaitu, 

1.  Menggunakan  rentang  data  AIS  dan  data  ekspor  impor 
yang lebih panjang, supaya dapat mengenali pola ekspor 
impor periode sebelum-sebelumnya. 

2.  Menggunakan  data  pelabuhan  Indonesia  yang  sama 

3.  Pengolahan  atau  cleaning  data  AIS  perlu  dikaji  atau 

dikembangkan lebih lanjut. 

4.  Membuat indikator AIS baru dan indikator dengan waktu 
yang lebih spesifik, seperti mingguan atau bahkan harian. 

DAFTAR PUSTAKA 

[1]   R. Adland, H. Jia dan S. P. Strandenes, “Are AIS-based trade volume 
estimates  reliable?  The  case of crude  oil exports,”  Maritime Policy & 
Management, 2017.  

[2]   S. Arslanalp, M. Marini dan P. Tumbarello, “Big Data on Vessel Traffic: 
Nowcasting Trade Flows in Real Time,” IMF Working Paper, December 
2019.  

[3]   D. A. Cerdeiro, A. Komaromi, Y. Liu dan M. Saeed, “World Seaborne 
Trade  in  Real  Time:  A  Proof  of  Concept  for  Building  AIS-based 
Nowcasts from Scratch,” IMF Working Paper, May 2020.  

[4]   UN Global Working Group, “United Nations Global Platform: Data for 

the World,” UN Global Working Group, 2019. 

[5]   Badan  Pusat  Statistik,  “Perdagangan  Luar  Negeri,”  November  2020. 

[Online]. Available: https://www.bps.go.id. 

[6]   Y.  Rahkmawati,  I.  M.  Sumertajaya  dan  M.  N.  Aidi,  “Evaluation  of 
Accuracy in Identification of ARIMA Models Based on Model Selection 
Criteria  for  Inflation  Forecasting  with  the  TSClust  Approach,” 
International Journal of Scientific and Research Publications, vol. 9, no. 
9, pp. 439-443, 2019.  

[7]   J.  Neves  dan  P.  Cortez,  “Combining  Genetic  Algorithms,  Neural 
Networks  and  Data  Filtering  for  Time  Series  Forecasting,”  dalam 
IMACS International Conference on Circuits, Systems and Computers 
(IMACS-CSC'98), Piraeus, Greece, 1998.  

[8]   D.  Zissis,  E.  K.  Xidias  dan  D.  Lekkas,  “Real-time  vessel  behavior 

prediction,” Evolving Systems, no. 7, pp. 29-40, 2016.  

[9]   International Maritime Organization, “Regulations for carriage of AIS,” 

26 Maret 2021. [Online]. Available: https://www.imo.org. 

[10]  I. K. Ahani, M. Salari dan A. Shadman, “Statistical models for multi-
step-ahead  forecasting  of  fine  particular  matter  in  urban  areas,” 
Atmospheric Pollution Research, vol. 10, no. 3, pp. 689-700, 2019.  

[11]  F.  Ahmed,  Y.  Cui,  Y.  Fu  dan  W.  Chen,  “A  Graph  Neural  Network 

Approach for Product Relationship Prediction,” ASME IDETC, 2021.  

[12]  R. Nooraeni, P. N. Sari dan N. P. Yudho, “Using Google trend data as 
an  initial  signal  Indonesia  unemployment  rate,”  dalam  ISI  Worid 
Statistics Congress, Kuala Lumpur, 2019.  

[13]  J. Han, M. Kamber dan J. Pei, Data mining: concepts and technique, San 

Fransisco: Morgan Kaufman Publisher, 2006.  

[14]  J.  Torres,  D.  Gutiérrez-Avilés,  A.  Lora  dan  F.  Martínez-Álvarez, 
“Random  Hyper-parameter  Search-Based  Deep  Neural  Network  for 
Paper Consumption Forecasting,” IWANN, 2019.  

[15]  E.  S.  Raymond,  “AIVDM/AIVDO  protocol  decoding,”  Januari  2021. 
[Online].  Available:  https://gpsd.gitlab.io/gpsd/AIVDM.html.  [Diakses 
Juni 2021]. 

[16]  A.  Noyvirt,  “Faster  Indicators  of  UK  Economic  Activity:  Shipping,” 

Data Science Campus, 2019.  

[17]  USCG,  “Definition  -  Vessel  Restricted  in  Her  Ability  to  Maneuver,” 
[Online].  Available: 

USCG  Navigation  Center,  Januari  2021. 
https://www.navcen.uscg.gov. [Diakses Juni 2021]. 

 8 / 8 

 
 
 
 
"
221709757,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pengembangan Sistem Penyusunan Lampiran Perka 
BPS tentang Wilkerstat dan Laporan Histori Wilayah 

Irza Faikar (221709757, 4SI1) 
Dosen Pembimbing: Firdaus M.B.A 

Ringkasan— Perencanaan  dan  pelaksanaan  kegiatan  statistik 
yang  dilakukan  BPS  erat kaitannya  dengan identifikasi  wilayah 
dan perubahannya. Atas dasar itu, BPS daerah selalu melakukan 
pemutakhiran data wilayah setiap 6 bulan sekali untuk mencatat 
informasi  wilayah  terbaru.  Data  yang  dikumpulkan  akan 
digunakan  dalam  penyusunan  lampiran  Perka  Wilkerstat  dan 
laporan  histori  wilayah  oleh  Subdit  PPS  BPS  RI.  Dalam 
pelaksanaan  penyusunan  tersebut,  ada  beberapa  bagian  yang 
dinilai kurang efisien dan berpeluang untuk dilakukan perbaikan 
seperti penggunaan API service untuk data wilayah, pengecekan 
hierarki  wilayah,  penyusunan  lampiran  Perka  Wilkerstat,  dan 
penyusunan  laporan  histori  wilayah.  Penelitian  ini  bertujuan 
yang  dapat 
untuk  melakukan  pengembangan 
memudahkan,  meningkatkan 
serta  mengurangi 
kesalahan  yang  mungkin  terjadi  pada  proses  penyusunan 
lampiran  Perka  Wilkerstat  dan  penyusunan  laporan  histori 
wilayah.  Pembangunan  sistem  berikut  diharapkan  dapat 
membantu  Subdit  PPS  BPS  RI  untuk  meningkatkan  kualitas 
pekerjaannya. 

efisiensi, 

sistem 

Kata Kunci— Wilkerstat, Perka BPS, histori wilayah. 

I.  LATAR BELAKANG 

Kegiatan  sensus  dan  survei  merupakan  kegiatan  yang  tak 
pernah  lepas  dari  Badan  Pusat  Statistik  (BPS).  Hal  tersebut 
karena  BPS  merupakan  badan  yang  memiliki  kewenangan 
untuk melakukan kegiatan statistik dalam rangka menghasilkan 
data dan statistik terpercaya bagi pemerintah, swasta, maupun 
masyarakat  pada  umumnya.  Sebagaimana  yang  telah  tercatat 
pada Undang-Undang Nomor 16 Tahun 1997 tentang Statistika 
dan  Peraturan  Pemerintah  Nomor  51  Tahun  1999  tentang 
Penyelenggaraan Statistik. 

Perencanaan  dan  pelaksanaan  kegiatan  statistik  yang 
dilakukan  BPS  tidak pernah terlepas  dari  identifikasi  wilayah 
dan  perubahannya.  Wilayah  dan  perubahan  wilayah  tersebut 
penting  untuk  memastikan  suatu  sampel  terpilih  berada  di 
wilayah  yang  tepat  pula  di  kenyataannya  [1].  Oleh  karena 
itulah,  setiap  BPS  daerah  selalu  melakukan  pencatatan 
perubahan  wilayah  yang  terjadi  di  daerahnya  setiap  6  bulan 
sekali  melalui  sistem  MFD  online  [1].  Hasil  dari  pencatatan 
perubahan  wilayah  yang  dilakukan  itu  akan  digunakan  untuk 
menyusun  Peraturan  Kepala  BPS  tentang  Wilayah  Kerja 
Statistik dan histori wilayah. 

Peraturan Kepala BPS tentang Wilayah Kerja Statistik yang 
selanjutnya akan disebut sebagai Perka Wilkerstat merupakan 
Peraturan Kepala BPS yang dikeluarkan setiap 6 bulan sekali 
sebagai  bentuk  pengesahan  atas  perubahan  wilayah  yang 
terjadi.  Sedangkan  histori  wilayah  merupakan  laporan  yang 
berisi tentang perubahan wilayah yang terjadi antar periodenya. 
Laporan tersebut tidak termasuk ke dalam Perka Wilkerstat. 

Perka Wilkerstat terdiri dari lembar pengesahan dan lembar 
lampiran. Lembar pengesahan berisi judul, tanda tangan Kepala 
BPS,  dan  informasi  lain  terkait  Perka  Wilkerstat  tersebut. 
Sedangkan  lembar  lampiran  berisi  rekapitulasi  wilayah  dan 
kode  wilayah  di  seluruh  Indonesia  yang  disusun  berdasarkan 
hierarkinya mulai dari provinsi hingga desa/kelurahan. Untuk 
selanjutnya fokus penelitian ini ada pada bagian lampiran Perka 
Wilkerstat dan laporan histori wilayah. 

Penyusunan  lampiran  Perka  Wilkerstat  dan  laporan  histori 
wilayah  ditugaskan  kepada  Subdirektorat  Pengembangan 
Pemetaan Statistik (Subdit PPS) BPS RI. Data yang digunakan 
tersebut  adalah  hasil  pemutakhiran 
untuk  penyusunan 
perubahan wilayah oleh seluruh BPS daerah sebagaimana yang 
telah  dijelaskan  sebelumnya.  Data  tersebut  dikirim  oleh 
Subdirektorat Pengembangan Kerangka Sampel (Subdit PKS) 
BPS  RI  ke  Subdit  PPS  dengan  sebelumnya  melakukan 
penyusunan laporan  histori  wilayah  dengan  satu  periode  data 
sebelumnya.  

Setelah  menerima  data  wilayah  dan  perubahannya  dari 
Subdit  PKS,  Subdit  PPS  melakukan  penyusunan  lampiran 
Perka  Wilkerstat  dan  laporan  histori  wilayah.  Penyusunan 
laporan  histori  wilayah  yang  dilakukan  Subdit  PPS  memiliki 
perbedaan dengan yang dilakukan Subdit PKS. Perbedaannya 
adalah  Subdit  PPS  melakukan  penyusunan  laporan  histori 
wilayah dengan seluruh periode sebelumnya. 

Proses  penyusunan  lampiran  Perka  Wilkerstat  dan  histori 
wilayah  dimulai  dengan  melakukan  input  data  wilayah  dan 
perubahannya.  Setelah  data  berhasil  diinputkan,  Subdit  PPS 
akan melakukan pengecekan hierarki wilayah bahwa data yang 
masuk  tersebut  sesuai.  Jika  sudah  sesuai,  maka  Subdit  PPS 
dapat  melakukan  penyusunan  lampiran  Perka  Wilkerstat 
berdasarkan 
telah  ditentukan 
sebelumnya.  Selain  itu,  Subdit  PPS  juga  dapat  melakukan 
penyusunan laporan histori wilayah dengan menggunakan data 
yang sama. Hasil akhir dari penyusunan lembaran Perka adalah 
lembaran  lampiran  Perka  Wilkerstat  dalam  format  pdf, 
sedangkan  untuk  penyusunan  laporan  histori  wilayah  adalah 
laporan berformat csv atau xlsx. 

format  penulisan  yang 

Selama ini, proses input data hanya dapat dilakukan dengan 
menggunakan  file.  Subdit  PPS  sendiri  sudah  memiliki  API 
service yang dapat digunakan untuk mendapatkan data wilayah, 
meskipun untuk data perubahannya belum bisa didapatkan dari 
API  service  tersebut.  Sehingga  muncul keinginan  dari  Subdit 
PPS  untuk  dapat  memanfaatkan  API  service  itu  setidaknya 
untuk memudahkan input data wilayah. 

Pada proses pengecekan hierarki wilayah, data wilayah yang 
terdiri  dari  pengecekan  hierarki  atas  ke  bawah, 
dicek 
pengecekan  hierarki  bawah  ke  atas,  dan  pengecekan  wilayah 
terhadap historinya. Seperti pengecekan kecamatan yang tidak 

 1 / 8 

 
 
 
memiliki desa, blok sensus yang tidak terkait dengan desa, atau 
pengecekan  kabupaten  yang  tidak  ada  histori  wilayahnya. 
Seluruh pengecekan tersebut saat ini masih dilakukan dengan 
melihat langsung data dari basis data. 

Pada  proses  penyusunan 

lampiran  Perka  Wilkerstat, 
penyusunan  dilakukan  dengan  menggunakan 
aplikasi 
Microsoft Visual FoxPro. Dalam penggunaan aplikasi tersebut, 
beberapa operasi harus dilakukan secara manual dan scripting. 
Tak  jarang  penyusun  mengalami  kesulitan  untuk  melakukan 
pengodean  format  penulisan  yang  dimaksud  pada  sistem 
berjalan.  Sehingga  tak 
itu 
menghasilkan  keluaran  yang  tidak  sesuai  dengan  format 
penulisan  yang  ditentukan.  Jika  hal  tersebut  terjadi,  maka 
penyusun akan melakukan pengeditan atau mengulangi proses 
penyusunan  hingga  format  penulisan  yang  dimaksud  telah 
sesuai. 

jarang  pula  sistem  berjalan 

Pada  proses  penyusunan 

laporan  histori  wilayah, 
penyusunan  dilakukan  dengan  melakukan  join  tabel  histori 
wilayah antar periodenya. Proses join tabel mengharuskan data 
histori  wilayah  disimpan  secara  keseluruhan  per  periode 
meskipun  wilayah  yang  bersangkutan 
tidak  mengalami 
perubahan.  Hal  ini  berkemungkinan  memberatkan  basis  data 
karena harus menyimpan data wilayah yang sama berkali-kali. 
Selain  itu,  proses  join  tabel  itu  sendiri  juga  berkemungkinan 
memberatkan  basis  data  saat  rentang  periode  histori  wilayah 
yang ingin dilihat terlalu jauh.   

Dari  proses  penyusunan  lampiran  Perka  Wilkerstat  dan 
laporan  histori  wilayah  di  atas,  didapatkan  beberapa 
permasalahan  dan  peluang perbaikan  dalam  proses  bisnisnya.  
Di antaranya sebagai berikut. 
1) 

Input data 

Pada  proses  input  data  wilayah,  terdapat  peluang 
untuk  memudahkan  proses  bisnis  dengan  memanfaatkan 
API  service.  Penggunaan  API  service  diharapkan  dapat 
membantu  memudahkan  Subdit  PPS  dalam  melakukan 
input data wilayah. 
2)  Pengecekan hierarki 

Pengecekan  hierarki  masih  dilakukan  secara  manual 
dengan  melihat  langsung  data  dari  basis  data.  Proses 
pengecekan dengan melihat langsung isi dari basis data ini 
rentan menyebabkan human error dan insiden yang tidak 
disengaja. 

3)  Penyusunan lampiran Perka Wilkerstat 

Hasil  penyusunan  lampiran  Perka  Wilkerstat  yang 
tidak sesuai dengan format penulisan yang ditentukan akan 
dilakukan  pengeditan  atau  penyusunan  ulang  hingga 
lampiran Perka Wilkerstat sesuai dengan format penulisan 
yang  dimaksud.  Hal 
tersebut  menjadikan  kegiatan 
penyusunan menjadi tidak efisien karena kegiatan tersebut 
melibatkan proses yang berulang dan menghabiskan waktu 
[2]. 

4)  Penyusunan laporan histori wilayah 

Proses join tabel berkemungkinan memberatkan basis 
data yaitu saat rentang periode data histori wilayah terlalu 
jauh. Selain itu, data histori wilayah yang disimpan secara 
keseluruhan  di  basis  data 
juga  berkemungkinan 
memberatkan basis data dari sisi penyimpanan. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

II.  TUJUAN PENELITIAN 

Secara  umum,  penelitian  ini  bertujuan  untuk  melakukan 
pengembangan sistem yang dapat memudahkan, meningkatkan 
efisiensi,  serta  mengurangi  kesalahan  yang  mungkin  terjadi 
pada proses penyusunan lampiran Perka Wilkerstat dan laporan 
histori  wilayah.  Secara khusus,  penelitian ini  bertujuan  untuk 
melakukan pengembangan sistem penyusunan lampiran Perka 
BPS  tentang  Wilkerstat  dan  laporan  histori  wilayah  yang 
mampu: 
1.  memanfaatkan penggunaan API service untuk input data 

wilayah, 

2.  melakukan pengecekan hierarki wilayah secara otomatis, 
3.  mengurangi kesalahan format penulisan pada penyusunan 

lampiran Perka Wilkerstat, dan 

4.  mengurangi  beban  basis  data  pada  penyusunan  laporan 

histori wilayah. 

III. PENELITIAN TERKAIT 

Berikut  beberapa  penelitian 

terdahulu  yang  memiliki 
keterkaitan dengan penyusunan lampiran Perka Wilkerstat dan 
histori wilayah. 
1.  Pembangunan  Sistem  Penyusunan  Dokumen  Berbasis 
Template  Dinamis  (Studi  Kasus:  Dokumen  Pengolahan 
Barang  dan Jasa)  oleh  Mutia  Elyani  dari  Sekolah  Tinggi 
Ilmu Statistik (STIS) tahun 2018. Penelitian ini membahas 
tentang  sistem  penyusunan  dokumen  berbasis  template 
guna membantu meningkatkan efisiensi serta mengulangi 
kesalahan  yang  terjadi  [3].  Penelitian  ini  memiliki 
keterkaitan pada bagian penyusunan dokumen yang lebih 
efisien.  Hasil  akhir  dari  penelitian  ini  adalah  sistem 
penyusunan dokumen berbasis template dinamis berbasis 
web. 

2.  Perancangan  Aplikasi  Pencarian  Kode  Wilayah  untuk 
Survei  Migrasi  BPS  dengan  Menerapkan  Algoritma 
Levenshtein  Distance  oleh  Khoirunnisa  dari  Sekolah 
Tinggi  Ilmu  Statistik  (STIS)  tahun  2016.  Penelitian  ini 
membahas  tentang  aplikasi  pencarian  kode  wilayah  yang 
lebih optimal dan efisien dengan menggunakan algoritma  
Levenshtein  Distance  [4].  Penelitian  ini  hanya  memiliki 
kesamaan  pada  objek  yang  diteliti.  Hasil  akhir  dari 
penelitian  ini  adalah  rancangan  aplikasi  pencarian  kode 
wilayah berbasis desktop. 

IV. METODE PENELITIAN  

A.  Metode Pengembangan Sistem 

Metode  pengembangan  sistem  yang  digunakan  pada 
penelitian  ini  adalah  System  Development  Life  Cycle 
(SDLC)  dengan  model  Rapid  Application  Development 
(RAD).  Alasan  penggunaan  RAD  adalah  keterlibatan 
user  yang  cukup  besar  pada  proses  desain,  sehingga 
kebutuhan  pengguna  dapat  dipenuhi  dengan  baik  dan 
lebih meningkatkan kepuasan pengguna [5]. 

RAD  berdasarkan  [6]  memiliki  beberapa  tahapan. 

Berikut penjelasan dari tahapan RAD. 
1.  Perencanaan Kebutuhan (Requirements Planning) 

Tahap perencanaan kebutuhan dilakukan dengan 
melakukan wawancara dengan pihak Subdit PPS dan 
melakukan  studi  literatur  terhadap  dokumen  yang 

 2 / 8 

 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

diberikan dari hasil wawancara tersebut. Tahapan ini 
dilakukan  untuk  mengidentifikasi 
tujuan  dan 
pemenuhan kebutuhan informasi. 

2.  RAD Design Workshop 

pengguna 

RAD  Design  Workshop  terdiri  dari  dua  tahap, 
pertama  melakukan  desain  sistem  bekerja  sama 
kedua  melakukan 
dan 
dengan 
pengembangan  sistem.  Tahapan 
ini  dilakukan 
dengan  membuat  prototype  secara  berulang-ulang 
dengan memperhatikan feedback pengguna di setiap 
perulangannya,  sehingga  nantinya  sistem  akan 
dibangun berdasarkan feedback pengguna itu. 
Implementasi (Implementation) 

3. 

Melakukan 

implementasi  sistem  yang 

telah 
dibangun.  Berupa  melakukan  pengenalan  terhadap 
sistem  yang  telah  dibangun,  menerapkannya,  serta 
melakukan evaluasi akhir dari sistem tersebut. 

Tahapan  RAD  dapat  dilihat  pada  Ilustrasi  yang 

terdapat pada Gambar 1. 

Gambar 1. Tahapan Rapid Application Development (RAD) 
Sumber: Diambil dari [6] 

B.  Cakupan Penelitian 

Penelitian ini memiliki cakupan sebagai berikut. 
●  Sistem  yang  dibangun  adalah  sistem  penyusunan 
laporan  histori 

lampiran  Perka  Wilkerstat  dan 
wilayah. 

●  Sistem hanya mencakup penyusunan Perka Wilkerstat 

pada bagian lampiran saja. 

V.  KERANGKA PIKIR 

Untuk  mendapatkan  gambaran  umum  alur  penelitian, 
keterkaitan  masalah  dengan  tujuan  penelitian,  serta  metode 
yang  digunakan  untuk  mencapai  target  maka  diperlukan 
kerangka pikir penelitian yang digambarkan pada Gambar 2. 

Gambar 2. Kerangka pikir penelitian 

VI. HASIL DAN PEMBAHASAN 

A.  Analisis Sistem Berjalan 

Berikut  adalah  diagram  proses  bisnis  penyusunan 
lampiran  Perka  Wilkerstat  dan  laporan  histori  wilayah 
yang didapatkan dari hasil wawancara dengan Subdit PPS 
BPS RI. 

Gambar 3. Proses bisnis sistem berjalan 

 3 / 8 

 
 
 
 
 
 
 
 
 
   
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

dan 
laporan  histori  wilayah,  mengurangi 
kesalahan  penyusunan,  dan  menghasilkan  file 
penyusunan berukuran seminimal mungkin. 
Information:  Sistem  menggunakan  sumber  data 
dari API service dan file. 

b) 

c)  Economy:  Sistem  memberikan  keuntungan  
kepada Subdit PPS BPS RI dengan berkurangnya 
resource waste dari sisi waktu dan tenaga. 
d)  Control  (and  security):  Sistem  menggunakan 

login untuk dapat diakses. 

e)  Efficiency:  Sistem  meringkas  waktu  yang 

digunakan untuk penyusunan. 

f)  Service: Sistem memberikan layanan yang mudah 

diakses dari berbagai platform. 

C.  Proses Bisnis Sistem Usulan 

Berdasarkan hasil analisis sistem berjalan dan analisis 
kebutuhan, didapatkan diagram proses bisnis sistem usulan 
yang  diharapkan  dapat  memudahkan,  meningkatkan 
efisiensi,  serta  mengurangi  kesalahan  yang  mungkin 
terjadi pada proses penyusunan lampiran Perka Wilkerstat 
dan  penyusunan  laporan  histori  wilayah.  Berikut  hasil 
diagram proses bisnis sistem usulan. 

Gambar 4. Proses bisnis sistem usulan 

 4 / 8 

Gambar 3. Proses bisnis sistem berjalan (lanjutan) 

B.  Analisis Kebutuhan 

Analisis  kebutuhan  berdasarkan  hasil  analisis  sistem 
berjalan dikategorikan menjadi kebutuhan fungsional dan 
kebutuhan nonfungsional. 
1.  Kebutuhan fungsional 

a)  Sistem  mampu  menampilkan 

dashboard 

rekapitulasi wilayah. 

b)  Sistem  mampu  melakukan  penarikan  data 

wilayah dan data histori wilayah. 

c)  Sistem  mampu  melakukan  pengecekan  hierarki 

wilayah secara otomatis. 

d)  Sistem  mampu  menerapkan  format  penulisan 

pada lampiran Perka Wilkerstat.   

e)  Sistem  mampu  menyusun  dan  mengelola  hasil 

f) 

penyusunan lampiran Perka Wilkerstat. 
Sistem  mampu  mengunggah  dan  mengelola 
Perka Wilkerstat. 

g)  Sistem  mampu  menyusun 

laporan  histori 

wilayah. 

h)  Sistem  mampu  memberikan  fasilitas  unduh 
untuk  hasil  penyusunan 
lampiran  Perka 
Wilkerstat, Perka Wilkerstat, dan laporan histori 
wilayah. 

2.  Kebutuhan nonfungsional 

Hasil  analisis  kebutuhan 

sisi 
nonfungsionalnya  dilakukan  dengan  menggunakan 
PIECES framework. 
a)  Performance:  Sistem  mampu  meningkatkan 
kinerja  penyusunan  lampiran  Perka  Wilkerstat 

sistem  dari 

 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

D.  Diagram Use Case 

Diagram  use  case  menampilkan  hubungan  aktor 
dengan daftar skenario yang akan terjadi pada sistem. Pada 
sistem  yang  dibangun  terdapat  2  aktor  yang  terdiri  dari 
admin, dan pengguna biasa. 

Gambar 5. Diagram use case 

E.  Implementasi 

Berikut  beberapa  hasil  implementasi  yang  telah 

dilakukan. 
1.  Penarikan data wilayah dan data histori wilayah  

Penarikan  data  wilayah  dan  data  histori  wilayah 
pada  dasarnya  sama,  hanya  saja  penarikan  data 
wilayah  memiliki  opsi  untuk  melakukan  penarikan 
melalui  API  service  sedangkan  penarikan  histori 
hanya dapat dilakukan melalui file. 

Gambar 6. Activity diagram penarikan data wilayah 

Gambar 7. Activity diagram penarikan data histori wilayah 

Berdasarkan  diagram  aktivitas,  proses  penarikan 
data  wilayah  dan  histori  wilayah  dimulai  dengan 
membuka  halaman  tarik  master  wilayah  atau  tarik 
histori  wilayah.  Selanjutnya,  pengguna  memilih 
metode  untuk  melakukan  penarikan  dan  opsi 
penarikan.  Jika  penarikan  gagal  maka  akan  muncul 
pesan kesalahan. 

Gambar 8. Tampilan halaman awal tarik master wilayah 

Gambar 9. Tampilan loading penarikan wilayah dengan API 

2.  Melakukan pengecekan hierarki 

Pengecekan  hierarki dilakukan  untuk mengetahui 
kesalahan  yang  terjadi  pada  data  wilayah  dan  histori 
wilayah yang ditarik. Jika terjadi kesalahan maka data 
harus diperbaiki dahulu sebelum dapat disimpan. 

 5 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 13. Tampilan data kesalahan saat terjadi kesalahan hierarki 

3.  Melihat dan mengunduh laporan histori lengkap 

Melihat laporan histori wilayah lengkap dilakukan 
untuk  mendapatkan  data  perubahan  wilayah  antar 
periode.  Di  sana  akan  terlihat  jika  ada  perubahan 
wilayah  pada  periode  tertentu.  Pengguna  juga  dapat 
mengunduh laporan histori wilayah tersebut. 

Gambar 10. Activity diagram pengecekan hierarki 

Berdasarkan  diagram 

aktivitas,  pengecekan 
diawali dengan membuka halaman awal cek  hierarki 
wilayah.  Selanjutnya  pengguna  menekan  tombol 
untuk memulai pengecekan. Jika ditemukan kesalahan 
hierarki,  maka  data  kesalahan 
tersebut  akan 
ditampilkan.  Jika  tidak  ditemukan  kesalahan  maka 
pengguna dapat melakukan penyimpanan data. 

Gambar 11. Tampilan halaman awal pengecekan hierarki 

Gambar 15. Activity diagram melihat dan mengunduh laporan histori 
lengkap 

Gambar 12. Tampilan loading pengecekan hierarki 

Gambar 16. Tampilan halaman awal histori lengkap 

Gambar 17. Tampilan hasil laporan histori lengkap 

 6 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
4.  Mengedit template 

Mengedit template dilakukan untuk menyesuaikan 
format  penulisan  lampiran  Perka  Wilkerstat  yang 
memungkinkan terjadi perubahan dari satu periode ke 
periode lainnya. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

5.  Melihat dan mengunduh penyusunan 

Melihat penyusunan dilakukan untuk melihat hasil 
penyusunan  lampiran  Perka  Wilkerstat  yang  telah 
dilakukan.  Hasil 
dilihat 
berdasarkan  kriteria  wilayah  kabupaten,  provinsi, 
keseluruhan,  maupun  hanya 
rekapitulasi 
Wilkerstat saja. Hasil penyusunan tersebut juga dapat 
diunduh. 

penyusunan 

dapat 

tabel 

Gambar 18.  Activity diagram mengedit template 

Gambar 21.  Activity diagram melihat dan mengunduh hasil 
 penyusunan 

diagram 

aktivitas, 

Berdasarkan 

pengeditan 
template dimulai dengan membuka halaman template 
lalu  membuka  halaman  edit  template.  Selanjutnya 
pengguna melakukan pengubahan pada template yang 
setelahnya dapat dilanjutkan untuk dapat menyimpan 
perubahan  tersebut  atau  melihat  preview  terlebih 
dahulu. 

Gambar 19.  Tampilan halaman awal template 

Gambar 20.  Tampilan desain atau edit template 

Gambar 22.  Tampilan halaman awal preview penyusunan 

Gambar 23.  Tampilan preview penyusunan setelah memilih penyusunan 

6.  Fungsi lain 

Fungsi  lain  yang  sudah  diimplementasikan  pada 
sistem  yang  dibangun  meliputi  melihat  dashboard, 
melihat  data  wilayah  dan  data  histori  wilayah, 
tambah,  dan  hapus), 
mengelola  template  (lihat, 
mengelola  penyusunan  (tambah,  edit,  cetak,  dan 
hapus),  dan  mengelola  Perka  (lihat,  unggah,  edit, 
unduh, cetak, dan hapus). 

 7 / 8 

 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

F.  Evaluasi 

1)  Black box testing 

Black box testing dilakukan untuk melihat kesesuaian 
fungsi  pada  sistem  yang  dikembangkan.  Pengujian 
dilakukan  tanpa  mengetahui  struktur  kode  internal 
dari  sistem.  Pada  penelitian  ini,  kesimpulan  yang 
didapatkan  dari  hasil  pengujian  black  box  testing 
adalah setiap fungsi telah berjalan sebagaimana yang 
diharapkan 

2)  System Usability Scale 

System Usability Scale dilakukan untuk melihat daya 
guna  sistem  yang  dikembangkan  dari  sisi  pengguna. 
Kuesioner  SUS  terdiri  dari  10  pertanyaan  dengan 
jawaban dari skala 1 sampai 5. Responden SUS pada 
penelitian ini adalah pegawai BPS yang terkait dengan 
proses  bisnis  penyusunan  lampiran  Perka  Wilkerstat 
dan  laporan  histori  wilayah.  Skor  yang  didapatkan 
adalah 85. Skor tersebut berada di kisaran angka 68-
100  yang  menandakan  sistem  dapat  diterima  oleh 
pengguna. 

VII. 

PENUTUP 

Kesimpulan yang didapatkan dari hasil penelitian mengenai 
sistem  penyusunan  lampiran  Perka  Wilkerstat  dan  laporan 
histori wilayah adalah sebagai berikut. 

2. 

1.  Telah 

informasi 
yang 
lampiran  Perka 

sistem 
dikembangkan 
membantu  proses  penyusunan 
Wilkerstat dan laporan histori wilayah.  
Implementasi  sistem  yang  dikembangkan  meliputi 
fungsi    melihat  dashboard,  penarikan  data  wilayah 
dan  histori  wilayah,  melihat  data  wilayah  dan  data 
histori  wilayah,  melihat  dan  mengunduh  laporan 
histori  lengkap,  mengelola  template  (lihat,  tambah, 
edit,  dan  hapus),  mengelola  penyusunan  (lihat, 
tambah, edit, unduh, cetak, dan hapus), dan mengelola 
Perka (lihat, unggah, edit, unduh, cetak, dan hapus). 
3.  Berdasarkan hasil evaluasi  black box testing, fungsi-
fungsi  yang  ada  pada  sistem  dapat  dikatakan  sudah 
sesuai dengan kondisi yang seharusnya. Berdasarkan 
hasil evaluasi SUS, sistem yang dikembangkan dapat 
diterima oleh pengguna. 

DAFTAR PUSTAKA 
[1]  Badan Pusat Statistik, “Master File Desa Provinsi Sumatera Utara 2019”, 

2019. 

[2]  V.  Kapur.  (2020,  3).  Inefficient  Progress.  [Online].  Available 
https://medium.com/@vishal.kapur/inefficient-progress-f6b12606d893. 
[3]  Khoirunnisa,  “Perancangan  Aplikasi  Pencarian  Kode  Wilayah  untuk 
Survei  Migrasi  BPS  dengan  Menerapkan  Algoritma  Levenshtein 
Distance”, thesis, 2016. 

: 

[4]  M.  Elyani,  “Pembangunan  Sistem  Penyusunan  Dokumen  Berbasis 
Template Dinamis (Studi Kasus: Dokumen Pengolahan Barang dan Jasa)”, 
thesis, 2018. 

[5]  A. Noertjahyana. “Studi Analisis Rapid Application Development sebagai 
Salah Satu Alternatif Metode Pengembangan Perangkat Lunak” J. Inform., 
vol. 3, no. 2, pp. 64–68, 2002, doi: 10.9744/informatika.3.2.pp.64-68. 
[6]  K.  E.  Kendall  and  J.  E.  Kendall,  Systems  Analysis  and  Design,  8th  ed. 

USA: Prentice Hall Press, 2010. 

 8 / 8 

 
 
 
"
221709749,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pembangunan Aplikasi Berbasis Web untuk 
Visualisasi dan Klastering Data Updating Potensi 
Desa Kota Malang Tahun 2019 

Indra Dwi Wicaksono (221709749, 4SD2) 
Dosen Pembimbing: Sukim, SST, M.Si 

Ringkasan—  Visualisasi  data  merupakan  hal  yang  penting 
dalam  menyampaikan  pemahaman  tentang  isi  data  kepada 
masyarakat  umum/publik.  Akses  publik 
terhadap  hasil 
pengolahan  data  yang  dihasilkan  oleh  suatu  instansi  masih 
menggunakan  media  cetak  dan  dilakukan  secara  manual,  salah 
satunya  yaitu  data  Potensi  Desa  (PODES).  Dengan  hadirnya 
internet,  diharapkan  akses  data  PODES  untuk  masyarakat 
semakin dimudahkan. Penelitian ini menghasilkan suatu aplikasi 
berbasis web untuk  mengotomasi visualisasi dan  klastering  data 
PODES  dan  mempublikasikannya.  Data  yang  digunakan  dalam 
penelitian ini yaitu data updating PODES Kota Malang Provinsi 
Jawa  Timur  Tahun  2019.  Aplikasi  ini  dapat  diakses  oleh  admin 
dan guest/masyarakat publik. Hak akses untuk  guest hanya bisa 
melihat dan mengunduh data hasil visualisasi, baik berupa grafik, 
tabel ataupun output klastering. Selain bisa melakukan yang bisa 
dilakukan  oleh  guest,  admin  memiliki  privilege  tambahan,  yaitu 
mengunggah  raw  data  updating  Podes  2019,  registrasi  dan 
mengelola staf. Adapun banyaknya  kelurahan  yang diolah pada 
penelitian ini adalah sebanyak 57 kelurahan dengan 8 (delapan) 
kategori. 

Kata Kunci— Visualisasi, aplikasi, klastering, website. 

I.  LATAR BELAKANG 

Memasuki  era  Revolusi  Industri  4.0  yang  ditandai  dengan 
tren  automasi  dan  pesatnya  pertukaran  data  dan  informasi 
terhadap  segala  aspek  kehidupan, 
memberikan  dampak 
termasuk  dalam  pemenuhan  kebutuhan  hidup  manusia.  Salah 
satu kebutuhan  manusia di era perkembangan teknologi  yang 
makin  pesat  ini  adalah  kebutuhan  akan  informasi  dan 
komunikasi.  Pergeseran  pola  komunikasi  menyebabkan 
pemenuhan kebutuhan akan informasi di sektor industri, jasa, 
maupun  pemerintahan  bergeser  ke  arah  digitalisasi  secara 
online,  atau  dengan  kata  lain  menggunakan  koneksi  internet 
sebagai aksesnya [1]. 

Berdasarkan  Undang-Undang  (UU)  Republik  Indonesia 
Nomor 25 Tahun 2009 tentang Pelayanan Publik, pemerintah 
harus  memberikan  pelayanan  publik  kepada  masyarakat  baik 
pusat maupun daerah. Oleh karena itu, baik pemerintah pusat 
maupun  pemerintah  daerah  harus  menjadi  penyedia  layanan 
informasi  yang 
lengkap  dan  akurat  bagi  masyarakat. 
Pemerintah  daerah  merupakan  salah  satu  penyedia  informasi 
mengenai wilayahnya. Pengambilan kebijakan oleh pemerintah 
daerah  guna  membangun  wilayahnya  agar  tidak  tertinggal 
dengan wilayah lain sangat bergantung pada informasi tersebut. 
Salah satu informasi yang tersedia pada tingkat desa/kelurahan 
saat ini adalah data Potensi Desa (PODES) yang bersumber dari 

Badan Pusat Statistik (BPS). Data PODES dapat memberikan 
rekomendasi kepada masyarakat secara umum dan pemerintah 
daerah secara khusus untuk menentukan kebijakan yang tepat 
sebagai upaya pengembangan daerahnya. 

Namun, penggunaan dan eksplorasi data tidak maksimal di 
level  kecamatan  dan/atau  desa/kelurahan.  Dari  wawancara 
yang dilakukan peneliti kepada Staf Sosial BPS Kota Malang, 
tak  sedikit  kecamatan  dan  kelurahan/desa  yang  menganggap 
bahwa  data  yang  diajukan  merupakan  bahan  dokumentasi 
untuk laporan di akhir tahun. Padahal, data menjadi “senjata” 
untuk  menganalisis  sebuah  fenomena/permasalahan  yang 
selanjutnya  digunakan  sebagai  dasar  pengambilan  keputusan. 
Masalah lain yaitu data terkait desa setempat yang tersimpan di 
basis data kantor kelurahan/desa kurang tersusun rapi dan tidak 
lengkap, sehingga menyulitkan bagi para pencari data (seperti 
pemerintah  kota/kabupaten,  mahasiswa,  peneliti)  untuk 
mendapatkan data yang dibutuhkan. 

Kemajuan  teknologi  yang  semakin  pesat  dan  peningkatan 
kebutuhan  informasi  PODES  seharusnya  dapat  memotivasi 
pemerintah  untuk  melakukan  inovasi  layanan  data  PODES 
kepada  masyarakat.  Inovasi  tersebut  dapat  dilakukan  dalam 
segi  penyediaan,  pengelolaan,  pelayanan,  hingga  penyajian 
informasi  dan/atau  data  melalui  perangkat  elektronik  yang 
marak  tersedia  saat  ini.  Adanya  pemanfaatan  TIK  seperti 
internet,  dapat mewujudkan keefektifan layanan data  PODES 
yang  terintegrasi.  Salah  satu  perwujudan  layanan  yang  dapat 
dilakukan  yaitu  melalui  sebuah  aplikasi  berbasis  web  untuk 
visualisasi  dan  klastering  data  PODES.  Dengan  demikian, 
masyarakat  setempat  semakin  dimudahkan  dalam  mengakses 
data  PODES  pada  tahun  berjalan.  Pemerintah  juga  semakin 
mudah  dalam  memantau  perkembangan  daerahnya  yang 
selanjutnya digunakan dalam analisis kebijakan. 

3.88

4.34

4.99

5.07

5.32

IP-TIK

2015

2016

2017

2018

2019

Gambar 1. IP-TIK Indonesia Tahun 2015-2019 

 1 / 7 

 
 
 
 
 
 
 
Salah  satu  ukuran  yang  digunakan  sebagai  dasar  untuk 
peningkatan akses dalam digitalisasi data PODES yaitu dengan 
melihat  Indeks  Pembangunan  Teknologi  Informasi  dan 
Komunikasi  (IP-TIK)  di  daerah  tersebut.  IP-TIK  merupakan 
suatu  ukuran  standar  yang  dapat  menggambarkan  tingkat 
pembangunan  TIK  suatu  wilayah,  kesenjangan  digital,  serta 
potensi  pengembangan  TIK  [2].  Skala  IP-TIK  berada  pada 
rentang  0–10,  semakin  tinggi  nilai  indeks  menunjukkan 
pembangunan TIK suatu wilayah semakin pesat, demikian pula 
sebaliknya,  semakin 
indeks  menunjukkan 
rendah  nilai 
pembangunan  TIK  di  suatu  wilayah  relatif  masih  lambat. 
Dalam lima tahun terakhir, IP-TIK Indonesia selalu mengalami 
peningkatan. Sedangkan secara khusus, IP-TIK untuk provinsi 
Jawa Timur pada tahun 2019 meningkat secara signifikan, yaitu 
sebesar  5,58  persen  dari  tahun  sebelumnya.  Angka  ini 
menempatkan  provinsi  Jawa  Timur  di  posisi  kedua  setelah 
provinsi  Jawa  Tengah  yang  meningkat  sebesar  6,38  persen 
untuk pergerakan tahun yang sama. Untuk level kabupaten/kota, 
indikator  pembangunan  TIK  dilihat  dari  penetrasi  rumah 
tangga (ruta) pengguna internet dan keberadaan menara telepon 
seluler/Base  Transceiver  Station  (BTS).  Di  Kota  Malang, 
penetrasi ruta yang mengakses internet pada tahun 2018 sebesar 
89,17  persen.  Angka  ini  melampaui  ibukota  provinsi  yang 
hanya  menyentuh  di  angka  86,24  persen.  Tentunya  hal  ini 
didukung oleh jumlah BTS di Kota Malang, yaitu sebanyak 56 
dari 57 kelurahan yang ada di kota Malang [3]. Dengan angka 
ini juga menempatkannya di posisi teratas untuk Provinsi Jawa 
Timur. 

Saat  ini  pendataan  dan  pengolahan  data  PODES  yang 
dilakukan oleh BPS Kota Malang masih menggunakan sistem 
konvensional (manual) dan penyajiannya masih dalam bentuk 
tabel-tabel  atau  grafik  statis  sehingga  kurang  menarik  dan 
belum  bersifat  dinamis.  Hal  ini  dikarenakan  belum  adanya 
sistem  informasi  desa  yang  khusus  menangani  data  PODES. 
Atas  dasar  tersebut,  peneliti  memilih  Kota  Malang  sebagai 
lokus penelitian dalam mengembangkan aplikasi berbasis web 
yang  bermanfaat  dalam  pengolahan  dan  publikasi  updating 
data PODES Tahun 2019. 

II.  TUJUAN PENELITIAN 

Adapun  tujuan  yang  ingin  dicapai  dalam  penelitian  ini 

diberikan sebagai berikut. 

1.  Membangun aplikasi berbasis web untuk visualisasi dan 
klastering data updating Podes 2019 Kota Malang, dan 
2.  melakukan survei kepuasan pengguna untuk mengukur 

seberapa baik aplikasi yang dibangun.  

III. PENELITIAN TERKAIT 

Terdapat  beberapa  penelitian  terdahulu  yang  membahas 
tentang  sistem  untuk  mendigitalisasi  data  PODES.  Sebagian 
penelitian tersebut  memiliki keterkaitan dengan penelitian ini 
karena  membahas  visualisasi  dan  tabulasi  data  dari  raw  data 
PODES yang diberikan. Sebagian yang lain membahas tentang 
algoritme  yang  dipakai  untuk  mengelompokkan  data  bertipe 
campuran dan berdimensi besar. Penelitian oleh Ririn Yustika 
Putri  (2020)  dengan  judul  “Pembangunan  Sistem  Informasi 
Visualisasi Data PODES Berbasis Web” menjadi acuan dalam 
penelitian ini karena menggunakan konsep MVC (Model-View-

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Controller) dalam pembangunan sistemnya, menyediakan fitur 
untuk  mengunduh  hasil  visualisasi  berupa  gambar  dalam 
format  .png  dan  melakukan  pengujian  dalam  bentuk  survei 
kepuasan pengguna [4]. Sedangkan untuk kerangka kerja yang 
digunakan,  peneliti  menggunakan  Codeigniter,  sementara  di 
sini penulis menggunakan Django, framework Python khusus 
untuk pembuatan web. 

Keamanan data PODES merupakan hal penting yang harus 
diperhatikan  oleh  BPS.  Oleh  karena  itu,  penelitian  yang 
dilakukan oleh PKL Politeknik Statistika STIS 59 dengan judul 
“Digitalisasi Data Potensi Desa Melalui Portal Satu Data Desa” 
menjadi  rujukan  dalam  penelitian  ini.  Penelitian  tersebut 
menyematkan  fitur  login  yang  mengizinkan  hanya  admin 
(dalam  hal  ini  pihak  BPS)  untuk  melihat  data  PODES  yang 
tidak boleh dipublikasikan secara umum atau bersifat rahasia. 
Lebih jauh, penelitian ini menyediakan fitur tabel dinamis [5] 
yang  memberikan  akses  kepada  pengguna  untuk  menyusun 
dan/atau  memilih  variabel  dari  satu  atau  beberapa  rincian 
kuesioner  PODES  yang  dibutuhkan  untuk  ditampilkan  dan 
diunduh untuk perangkat lokal. 

Data  updating  PODES  Tahun  2019  terdiri  dari  ratusan 
atribut  yang  merupakan  struktur  data  kompleks,  yakni  terdiri 
dari 312 atribut bertipe data campuran (numerik dan kategorik) 
dengan banyak baris yang merujuk pada jumlah desa/kelurahan 
yang ada di kota/kabupaten tersebut. Berdasarkan karakteristik 
di atas,  maka  diperlukan  suatu  metode  yang dapat digunakan 
untuk  menganalisis  data  bertipe  campuran,  yaitu  metode  K-
Prototype  yang  diusulkan  oleh  Zhexue  Huang  (1998)  dalam 
penelitiannya  yang  berjudul  “Clustering  Large  Datasets  with 
Mixed  Numeric  and  Categorical  Values”  [6].  Peneliti 
menggunakan algoritme K-Prototype untuk mengelompokkan 
data  updating  Podes  Kota  Malang  Tahun  2019  menjadi 
beberapa  klaster/gerombol  per  kategori  yang  terdapat  dalam 
rincian kuesionernya. 

IV. METODE PENELITIAN  
Penelitian  yang dilakukan penulis berpedoman pada solusi 
yang diajukan terhadap permasalahan yang terjadi di BPS Kota 
Malang.  Permasalahan  yang  dialami  yaitu  terkait  pengolahan 
dan  publikasi  data  PODES  yang  masih  dilakukan  secara 
manual. Selain itu, data PODES setiap tahun kurang tersusun 
rapi  sehingga  menyulitkan  pengguna  data  dan  pemerintah 
setempat dalam mengaksesnya. Oleh karena itu, penelitian ini 
mencakup  pengembangan  aplikasi  visualisasi  dan  diseminasi 
data PODES berbasis web untuk memudahkan pihak BPS Kota 
Malang  dan  masyarakat  setempat  dalam  mengakses  dan 
mentabulasi  data  updating  PODES  2019.  Sistem  yang  akan 
dikembangkan merupakan penyempurnaan dari sistem  serupa 
yang  sudah  ada.  Selain  bisa  melihat  dan  mengunduh  hasil 
visualisasi dan tabulasi data PODES 2019, sistem ini juga bisa 
melakukan  klastering  data  PODES  per  kategori  atau  semua 
kategori  dan  mengunduh  hasilnya  dalam  format  excel  dan 
grafik  berekstensi  .png.  Guna  memperbarui  data,  pihak  BPS 
dapat mengunggah raw data updating PODES Tahun 2019 dan 
secara otomatis menampilkan hasilnya di halaman utama tanpa 
perlu  mengolah  datanya  secara  manual.  Dalam  pengelolaan 
sistem dibutuhkan aktor, yaitu pengguna (berupa manusia atau 

 2 / 7 

 
 
alat)  di  luar  sistem  yang  menggunakan  dan  berhubungan 
dengan sistem serta mengikuti aturan yang sudah ditetapkan. 

Untuk  memulai  mengembangkan  aplikasi  berbasis  web 
untuk visualisasi data updating PODES 2019, terlebih dahulu 
penulis  mengumpulkan  data  terkait  kebutuhan  sistem  dengan 
metode sebagai berikut. 
1.  Kajian Pustaka 

Kajian  pustaka  dilakukan  dengan  mengumpulkan 
informasi dari penelitian sebelumnya, yaitu penelitian oleh 
mahasiswa Politeknik Statistika STIS Angkatan 59 melalui 
kegiatan Praktik Kerja Lapangan yang dilakukan di Provinsi 
Daerah Istimewa Yogyakarta pada Maret 2020.  Penelitian 
ini  menghasilkan  sebuah buku  yang berjudul  “Digitalisasi 
Data Potensi Desa Melalui Portal Satu Data Desa (PSDD)” 
yang  menjadi  dasar  bagi  penulis  untuk  menampilkan  data 
apa  saja  yang  harus  ditampilkan  di  halaman  dashboard 
berupa grafik dan tabel berikut fitur-fitur yang terdapat di 
dalamnya.  Selain  itu,  rujukan  lain  bagi  penulis  adalah 
informasi  dari  Badan  Pusat  Statistik  (BPS)  Kota  Malang 
tentang raw data updating PODES 2019 beserta kuesioner 
dan contoh tabulasi data yang diperlukan. 
2.  Mengamati Sistem yang Sejenis 

Dengan maraknya perkembangan aplikasi berbasis web 
atau cloud computing maka hal ini memudahkan pengguna 
dalam mengakses data dan beberapa perhitungan yang ada 
di dalamnya secara praktis. Hal ini dikarenakan pengguna 
tidak  perlu  meng-install  suatu  aplikasi  di  komputer  lokal 
mereka. Pengguna hanya bermodalkan kuota internet untuk 
mengaksesnya, kapanpun dan dimanapun. Sehingga banyak 
model, contoh dan framework atau kerangka kerja yang bisa 
dijadikan  acuan  dan  dipelajari  penulis  dari  sistem  atau 
aplikasi berbasis  web, khususnya yang menangani tentang 
visualisasi data. 
3.  Sumber Data 

Penelitian ini hanya menggunakan data sekunder yang 
diperoleh dari wawancara dengan subject matter, yaitu Staf 
Sosial BPS Kota Malang. Data yang digunakan  yaitu  raw 
data  updating  PODES  Kota  Malang  Tahun  2019  yang 
diperoleh  pada 
ini 
berformat .xlsx yang terdiri dari 57 baris dan 312 kolom. 

tanggal  6  Januari  2021.  Data 

Pengembangan  aplikasi  ini  menggunakan  metode  siklus 
hidup  pengembangan  sistem  atau  yang  bisa  disebut  SDLC 
(System Development Life Cycle). Adapun tahapan dari metode 
ini dijelaskan sebagai berikut. 
1.  Perencanaan Sistem 

Pada 

Pada  tahapan  ini  dilakukan  pengumpulan  data  dan 
informasi.  Pengumpulan  data  berupa  raw  data  updating 
PODES Kota Malang tahun 2019, sedangkan pengumpulan 
informasi berupa studi literatur dari sistem yang sudah ada. 
2.  Analisis Sistem 
tahapan 

terhadap 
ini  dilakukan 
permasalahan  yang  ada.  Perangkat  yang  penulis  gunakan 
dalam analisis sistem yang akan dibangun yaitu flowchart, 
analisis  PIECES  dan  fishbone  diagram.  Flowchart  atau 
bagan  alir  digunakan  untuk  menggambarkan  bagaimana 
proses  yang  berjalan  dalam  sistem,  analisis  PIECES 
sistem, 
digunakan 

untuk  menganalisis 

kebutuhan 

analisis 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

fishbone 

sedangkan 
untuk 
mengidentifikasi,  mengeksplorasi,  dan  menggambarkan 
masalah serta sebab dan akibat dari dibangunnya sistem [7]. 
Berikut analisis PIECES dalam penelitian ini. 

digunakan 

diagram 

TABEL I 
ANALISIS PIECES DALAM PENGOLAHAN DAN PUBLIKASI DATA 
PODES KOTA MALANG 

Kategori 

Identifikasi Masalah 

(1) 

(2) 

Performance 

Data  PODES  ditampilkan 

dalam buku (cetakan kertas) 

dan 

dilakukan 

secara 

manual 

Information 

Kurangnya  sosialisasi  data 

PODES  sehingga  sedikit 

yang 

menggunakannya 

dalam 

analisis 

untuk 

membantu 

mengambil 

kebijakan,  informasi  yang 

ditampilkan  masih  berupa 

tabel  sehingga 

informasi 

yang 

terkandung  kurang 

tersampaikan dengan baik 

Economy 

Tabel  yang  memuat  data 

PODES  masih 

dicetak 

dalam 

bentuk 

kertas, 

sehingga 

membutuhkan 

banyak 

biaya 

dan 

membutuhkan 

tenaga 

khusus  dalam  menganalisis 

data yang kompleks 

Efficiency 

Pengolahan  dan  publikasi 

data  PODES secara manual 

membutuhkan  waktu  yang 

tidak 

sedikit 

dan 

membutuhkan tenaga ahli 

Services 

Akses terhadap data PODES 

secara  manual 

kurang 

tersusun 

rapi 

sehingga 

informasinya kurang akurat 

 3 / 7 

 
 
 
 
 
3.  Perancangan Sistem 

Setelah dilakukan analisis sistem selanjutnya dilakukan 
rancangan  sistem  usulan  untuk  sistem  berbasis  web. 
Rancangan yang dimaksud yaitu dengan membuat business 
process flow dan flowchart untuk sistem usulan, pemodelan 
sistem dengan menggunakan usecase dan activity diagram, 
perancangan basis data dan perancangan user interface atau 
antarmuka pengguna dari sistem usulan. 

Gambar 2. Business Process Flow 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

5.  Evaluasi Sistem 

Tahap  ini  merupakan  tahap  akhir  dari  pengembangan 
sistem.  Setelah  sistem  diimplementasikan  dalam  suatu 
aplikasi  berbasis  web,  selanjutnya  dilakukan  pengujian 
untuk  melihat  apakah  sistem  yang  sudah  dibangun  sudah 
memenuhi kebutuhan yang diperlukan oleh user atau belum. 
Pengujian  yang  dilakukan  juga  digunakan  untuk  melihat 
performa  yang  diinginkan  oleh  user  sudah  terpenuhi  atau 
belum.  Metode  evaluasi  yang  digunakan  yaitu  black-box 
testing dan survei kepuasan sistem kepada pengguna (terdiri 
dari  admin  dan  user  biasa/guest).  Black-box 
testing 
digunakan untuk melihat apakah keluaran yang dihasilkan 
aplikasi  sesuai  dengan  yang  diharapkan  oleh  user  atau 
belum. Sedangkan survei kepuasan sistem kepada pengguna 
digunakan untuk melihat seberapa efektif sistem visualisasi 
yang telah dibangun. 

Aplikasi ini dilengkapi dengan fitur klastering data PODES, 
maka algoritme K-Prototype dipakai untuk mengklaster dataset 
berukuran  besar  dan  bertipe  data  campuran  (numerik  dan 
kategorik).  Adapun  rumus  ukuran  kesamaan  untuk  data  yang 
memiliki atribut numerik dan kategorik adalah sebagai berikut. 

4. 

dan 

function 

antarmuka/tampilan 

Implementasi Sistem 
Tahapan ini adalah penerapan yang diwujudkan dalam 
pembangunan  aplikasi  berdasarkan  perancangan  sistem 
yang 
telah  dilakukan  sebelumnya.  Penerapan  atau 
implementasi  berupa  penulisan  kode  program  untuk 
membuat 
yang 
dibutuhkan.  Adapun penulisan kode  menggunakan bahasa 
Python versi 3.9.1 dengan menggunakan Django versi 3.1.6 
sebagai kerangka kerjanya. Selain berfungsi sebagai proses 
pembuatan  web  dengan  menggunakan  bahasa  Python, 
Django  juga  menyediakan  server  lokal  dan  basis  data 
sebelum aplikasi dimigrasi atau di-hosting ke server online. 
Adapun  untuk  pembuatan  grafik  menggunakan  library 
Chart.js versi 2.9 dan modul matplotlib versi 3.3.4. Semua 
kode  program  di  atas  dituliskan  menggunakan  perangkat 
lunak Visual Studio Code versi 1.57. 

𝑑(𝑋𝑖, 𝑍𝑙) = √∑ (𝑥𝑖𝑗

𝑚𝑟
𝑗=1

2
𝑟 )
𝑟 − 𝑧𝑙𝑗

+ 𝛾𝑙 ∑

𝑚𝑐
𝑗=𝑙+1

𝛿(𝑥𝑖𝑗

𝑐 )
𝑐 , 𝑧𝑙𝑗

      (1) 

𝑟  adalah 
𝑟  adalah  nilai  pada  atribut  numerik  ke-j, 𝑧𝑙𝑗
dimana 𝑥𝑖𝑗
rata-rata atau prototype atribut numerik ke-j klaster l. Adapun 
𝑚𝑟  adalah  jumlah  atribut  numerik  yang  ada  dalam  dataset. 
𝑐 )  adalah  simple  matching  dissimilarity  untuk  data 
𝛿(𝑥𝑖𝑗
bertipe kategorik, dengan 

𝑐 , 𝑧𝑙𝑗

𝛿(𝑥𝑖𝑗

𝑐 , 𝑧𝑙𝑗

𝑐 ) = {

1 − 𝜔(𝑥𝑖𝑗

𝑐 , 𝑙),   jika 𝑥𝑖𝑗
𝑐
𝑐 = 𝑧𝑙𝑗
𝑐
𝑐 ≠ 𝑧𝑙𝑗

1,   jika 𝑥𝑖𝑗

(2) 

dimana  𝜔(𝑥𝑖𝑗

𝑐 , 𝑙)  adalah  nilai  penimbang  untuk  𝑥𝑖𝑗

𝑐  yang 

didefinisikan sebagai berikut 

𝜔(𝑥𝑖𝑗

𝑐 , 𝑙) =

𝑓(𝑥𝑖𝑗
|𝐶𝑙|𝑓(𝑥𝑖𝑗
𝑐 |𝐶𝑙) adalah frekuensi nilai 𝑥𝑖𝑗

𝑐 |𝐶𝑙)
𝑐 |𝐷)

𝑓(𝑥𝑖𝑗

(3) 

jumlah  objek  dalam  klaster  l,  dan 𝑓(𝑥𝑖𝑗

𝑐  dalam klaster l, |𝐶𝑙| adalah 
𝑐 |𝐷) adalah  frekuensi 

Gambar 3. Penulisan Kode Program di Visual Studio Code untuk 

Klastering Bertipe Data Campuran 

nilai 𝑥𝑖𝑗

𝑐  pada satu dataset. 

V.  KERANGKA PIKIR 

Untuk  memandu  penulis  dalam  melakukan  penelitian  ini, 
diperlukan  sebuah  kerangka  pikir  yang  dapat  menjelaskan 
gambaran  permasalahan  serta  solusi  yang  akan  ditempuh. 
Sebagaimana  yang  telah  dijelaskan  di  atas,  penulis  memulai 
penelitiannya dengan mengidentifikasi masalah yang terjadi di 
BPS  Kota  Malang  terkait  pengolahan  dan  publikasi  updating 
data  PODES  2019  melalui  analisis  PIECES.  Berikutnya  dari 

 4 / 7 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
   
 
 
masalah  yang 
mengembangkan aplikasi berbasis web untuk hal tersebut. 

teridentifikasi  akan  diselesaikan  dengan 

Dalam mengembangkan aplikasinya, penulis menggunakan 
metode  SDLC  yang  kemudian  diimplementasikan  dengan 
bahasa  pemrograman  Python  versi  3.9.1.  Hasil  aplikasi  akan 
diuji menggunakan survei kepuasan pengguna (yang diberikan 
kepada  admin  dan  guest)  untuk  visualisasi  menurut  Stephen 
Few.  Adapun  bagan  kerangka  pikir  diberikan  dalam  gambar 
berikut. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 5. Tampilan Header/navbar untuk administrator 

Gambar 6. Tampilan Header/navbar untuk staf 

Gambar 4. Bagan Kerangka Pikir 

VI. HASIL DAN PEMBAHASAN 

Arsitektur Sistem 

Gambar 7. Tampilan Header/navbar untuk guest 

Usecase Diagram 

Usecase Diagram menjelaskan fungsi dari setiap fitur sistem. 

Penjelasannya dapat dilihat pada pada Gambar 8 dan Tabel II. 

Sistem  dibangun  untuk  sisi  client  dalam  tiga  level,  yaitu 
untuk  administrator,  staf  dan  guest.  Administrator  dan  staf 
mengakses  data  yang  sama.  Sedangkan  untuk  guest  terdapat 
batasan  variabel  yang  ditampilkan  di  halaman  visualisasi 
karena  terdapat  beberapa  variabel  sensitif  yang  tidak  boleh 
dapat 
umum. 
dipublikasikan 
mendaftarkan, menghapus dan mengangkat staf menjadi admin, 
sedangkan  staf  tidak.  Baik  administrator  dan  staf  dapat 
mengubah  visualisasi  berdasarkan  raw  data  PODES  yang 
diunggah  melalui menu Upload Data, sedangkan untuk guest 
tidak  bisa  melakukan  hal  tersebut.  Tampilan  header  untuk 
administrator, staf, dan guest diberikan dalam gambar berikut. 

Administrator 

secara 

Gambar 8. Usecase diagram 

 5 / 7 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
TABEL II 
DESKRIPSI USECASE SYSTEM 

2.  Menampilkan klastering data PODES 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Nama Usecase 
(1) 

Login 

Kelola Data 

Melihat  visualisasi 
berdasarkan kategori 

Lihat 
visualisasi 
berdasarkan wilayah 

Deskripsi 
(2) 
Administrator dan staf dapat masuk 
ke  dalam  sistem  untuk  melihat 
semua  visualisasi  dalam  variabel 
PODES 2019 dan mengakses menu 
Upload Data 
Administrator  memiliki  privilege 
tambahan 
untuk  mendaftarkan, 
menghapus,  dan  mengangkat  staf 
menjadi administrator 
Pengguna  (baik  memerlukan  login 
atau tidak) dapat melihat visualisasi, 
tabulasi  dan  klastering  dengan 
memilih kategori yang diinginkan 
Pengguna dapat mempersempit hasil 
visualisasinya 
dengan  memilih 
kecamatan setelah memilih kategori 
yang diinginkan 

Activity Diagram 

Activity  diagram  adalah  diagram  yang  menggambarkan 
aktivitas  aplikasi  saat  aplikasi  digunakan.  Berikut  adalah 
activity diagram yang menjelaskan masing-masing usecase. 

1.  Menampilkan visualisasi data PODES 

Gambar 9. Activity diagram untuk menampilkan visualisasi data PODES 

Gambar  9  menujukkan  aktivitas  yang  dilakukan  pengguna 
saat  mengakses  fitur  visualisasi  data  Podes  berupa  grafik 
dan/atau  tabel  dengan  library  chart.js  dan  vue-datatables. 
Pertama,  pengguna  membuka  aplikasi  web,  baik  yang 
memerlukan  login  atau  tidak.  Kemudian  di  halaman  awal 
memilih salah satu kategori yang diinginkan. Adapun kategori 
yang  tersedia  antara  lain  Perumahan  dan  Lingkungan  Hidup, 
Pendidikan  dan  Kesehatan,  Olahraga  dan  Hiburan,  Angkutan 
dan Komunikasi, Ekonomi, Keterangan Aparatur Pemerintahan 
Desa/Kelurahan, Bencana  Alam dan Mitigasi Bencana  Alam, 
dan Keamanan. Saat kategori dipilih/diklik maka akan tampil 
visualisasi level kota berupa  pie chart dan stacked bar chart. 
juga  dapat  mempersempit  pilihannya  dengan 
Pengguna 
memilih  kolom  kecamatan yang tersedia. Pengguna juga bisa 
memilih  hasil  visualisasi  berupa  tabel  dengan  mengklik  tab 
Tabel. Baik grafik maupun tabel dapat diunduh dalam format 
.png dan .xlsx. 

Gambar 10. Activity diagram untuk mengakses klastering data PODES 

Untuk melakukan klastering data PODES, setelah membuka 
aplikasi web, pengguna memilih menu Klastering yang ada di 
navbar.  Kemudian pengguna  memilih kategori  yang tersedia, 
atau  pengguna  juga  bisa  memilih  klastering  untuk  semua 
kategori.  Output  klastering  berupa  bar  chart  yang 
menunjukkan  banyak  anggota  tiap  klaster  dan  tabel  yang 
merinci  kelurahan  mana  saja  yang  masuk  ke  klaster  tertentu. 
Bar chart dan tabel juga dapat diunduh dalam format .png dan 
.xlsx. 

3.  Mengakses tabel dinamis 

Gambar 11. Activity diagram untuk mengakses tabel dinamis 

Dengan fitur tabel dinamis pengguna dapat memilih variabel 
PODES apapun yang ingin ditampilkan, tidak dibatasi kategori 
dan  bisa  menyaring  hasilnya  dengan  mengetikkan  kata  kunci 
pada  kolom  pencarian.  Secara  otomatis  hasil  tabel  akan 
tersebut.  Untuk 
menyesuaikan  berdasarkan  pencarian 
mengakses fitur ini, setelah pengguna masuk ke aplikasi web, 
pengguna  bisa  memilih  menu  Tabel  Dinamis.  Kemudian 
pengguna  memilih  variabel  yang  disajikan  dalam  dropdown 
menu. Banyak kolom akan menyesuaikan banyaknya variabel 
yang  dipilih  pengguna,  di  sini  pengguna 
juga  bisa 
menghilangkan kolom yang diinginkan dengan mengklik ikon 
X yang muncul di atas dropdown menu. Selain dapat diunduh 
dalam format .xlsx, pengguna juga bisa mengunduh hasil tabel 
dinamis  dalam  format  .pdf  dan  memilih  orientasinya,  baik 
portrait atau lanskap. Pengguna juga dapat langsung mencetak 
hasilnya  ke  printer  yang  tersambung  dengan  komputer  lokal 
dan  mengatur  ukuran  kertasnya  melalui  jendela  baru  yang 
muncul. 

4.  Mengunggah raw data updating PODES 2019 

VII. 

PENUTUP 

Gambar 12. Activity diagram untuk mengunggah raw data updating 

PODES 2019 

 6 / 7 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

VIII. 

PENUTUP 

Kesimpulan 

Kesimpulan  sementara  yang  dapat  diambil  berdasarkan 
skripsi Pembangunan Aplikasi Berbasis Web untuk Visualisasi 
dan  Klastering  Data  Updating  Potensi  Desa  Kota  Malang 
Tahun 2019 adalah sebagai berikut. 

1.  Aplikasi  berbasis  web  untuk  visualisasi  dan  klastering 
data  updating  PODES  2019  digunakan  untuk 
menyajikan data updating PODES 2019 secara digital, 
sehingga  mudah 
lokal 
(PC/laptop/smartphone) masing-masing. 

perangkat 

diakses 

di 

2.  Telah  dibangun  Aplikasi  Visualisasi  Data  Updating 
PODES 2019 menggunakan library chart.js, Vue.js dan 
modul matplotlib. 

3.  Aplikasi visualisasi berbasis web untuk visualisasi data 
updating  PODES  2019  menyajikan  visualisasi  yang 
cukup baik bagi pengguna. 

Saran 

1.  Pengembangan  di  server  side  web  perlu  dilakukan 
peningkatan, misalnya fitur remember me saat login. 
2.  Diperlukan fitur peta tematik dan fitur data series untuk 
data bertipe numerik agar penyajian data lebih menarik 
dan lebih mudah dimonitor setiap tahunnya. 

DAFTAR PUSTAKA 
[1]  Laporan  Hasil  Penelitian  Riset  1  Praktik  Kerja  Lapangan  Politeknik 
Statistika  STIS  Tahun  Akademik  2019/2020,  hal  1.  2020.  Jakarta: 
Politeknik Statistika STIS 

[2]  Berita Resmi Statistik No. 95/12/Th.XXIII, dirilis 15 Desember 2020 
[3]  Statistik Potensi Desa Provinsi Jawa Timur Tahun 2018 
[4]  Putri,  Ririn  Yustika.  2020.  Pembangunan  Sistem  Informasi  Visualisasi 

Data Podes Berbasis Web. Jakarta: Politeknik Statistika STIS 

[5]  Laporan  Hasil  Penelitian  Riset  1  Praktik  Kerja  Lapangan  Politeknik 
Statistika  STIS  Tahun  Akademik  2019/2020,  hal  17.  2020.  Jakarta: 
Politeknik Statistika STIS 

[6]  Huang,  Zhexue.  “Extensions  to  the  k-Means  Algorithm  for  Clustering 
Large Data Sets with Categorical Values”. Data Mining and Knowledge 
Discovery 2, 283–304 (1998). 1998 

[7]  Bentley,  Lonnie D, and Jeffrey  L Whitten. (2007), System Analysis and 

Design Methods – 7th Edition. New York: McGraw-Hill 

[8]  Gunawan, Jonathan & Ivan Gunawan. (2018, 12). Teknologi Single Page 
Available: 

Application 
https://socs.binus.ac.id/2018/12/06/teknologi-single-page-application-
spa/ 

[Online]. 

(SPA). 

Untuk mengubah hasil visualisasi dan tabulasi berdasarkan 
dataset yang dimasukkan, pengguna harus login terlebih dahulu. 
Kemudian pengguna memilih menu Upload Data. Setelah itu 
pengguna diberikan ketentuan yang harus dipenuhi ketika akan 
mengunggah  data.  Pastikan  data  yang  diunggah  pengguna 
adalah  raw  data  updating  PODES  2019  berisikan  semua 
variabel  PODES  2019  (memenuhi  template  yang  diberikan) 
dan  berekstensi  .xlsx.  Setelah  mengklik  menu  Choose  File, 
akan terbuka jendela kecil yang mengharuskan pengguna untuk 
memilih file excel yang sesuai. Apabila pengguna memasukkan 
file  yang  salah,  maka  sistem  akan  menampilkan  pesan  error. 
Saat  proses  unggah  data  selesai,  aplikasi  akan  langsung 
menampilkan halaman awal visualisasi. Klik tombol Perbarui 
yang ada di pojok kiri bawah layar untuk melihat perubahannya. 
Menu  Visualisasi,  Klastering,  dan  Tabel  Dinamis  yang 
terdapat  pada  bagian  atas  aplikasi  ini  menggunakan  konsep 
SPA  (Single  Page  Application)  yang  dalam  implementasinya 
menggunakan  library  Vue.js.  SPA  adalah  salah  satu  jenis 
aplikasi  website dimana hanya  ada satu  halaman  yang  meng-
handle semua aktivitas yang terjadi dalam aplikasi tersebut [8]. 
Sehingga  saat  berpindah  halaman,  sistem 
tidak  perlu 
melakukan  request  kepada  server  setiap  kali  terjadi  interaksi 
pada  aplikasi.  Dengan  demikian,  website  yang  menggunakan 
SPA  memiliki  performa  yang  lebih  cepat  tanpa  harus  load 
halaman secara terus menerus. 

Pengujian Aplikasi 

Untuk  menguji  aplikasi 

ini  secara  subjektif,  peneliti 
melakukan survei kepada pengguna yang diberikan kepada 16 
responden guna mengukur seberapa baik visualisasi yang telah 
dibangun. Untuk mengukur hal tersebut, peneliti memberikan 
pertanyaan terkait tujuh kriteria visualisasi yang baik menurut 
Stephen  Few.  Ukuran  untuk  masing-masing  kriteria  berupa 
rentang nilai dari 1 hingga 10. Angka 1 menunjukan nilai yang 
buruk untuk kriteria tersebut dan nilai 10 menunjukkan tingkat 
kepuasan yang baik untuk kriteria tersebut. Misal untuk kriteria 
tampilan  visualisasi  (aesthetics),  angka  1  menunjukka  bahwa 
visualisasi  yang  ditampilkan  jelek,  sedangkan  angka  10 
menjunjukkan  bahwa  visualisasi  yang  ditampilkan  terlihat 
indah dalam persepsi audiens. Kriteria terkait visualisasi yang 
baik  dan  hasil  survei  kepuasan  pengguna  dapat  dilihat  pada 
Tabel III. 

TABEL III 
HASIL SURVEI KEPUASAN PENGGUNA 

No. 
(1) 
1. 
2. 
3. 
4. 
5. 
6. 
7. 

Kategori 
(2) 

Usefulness 
Completeness 
Perceptibility 
Truthfulness 
Intuitiveness 
Aesthetics 
Engagement 

Nilai Rataan 
(3) 
8,625 
8,1875 
8,625 
8,5625 
8,625 
8,0625 
8,5625 

 7 / 7 

 
 
 
 
 
 
 
 
 
 
"
221709728,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pengembangan Mesin Pencari Data BPS 
Menggunakan Linked Open Data, Klasifikasi Kueri, 
dan Ekspansi Kueri 

Handy Geraldy (221709728, 4SD1) 

Dosen Pembimbing: Lutfi Rahmatuti Maghfiroh SST, MT 

Ringkasan—  BPS  membangun  portal  Indonesia  Data  Hub 
(INDAH)  untuk  mendukung  Satu  Data  Indonesia  (SDI).  Salah 
satu  layanan  portal  INDAH  adalah  mesin  pencari.  Layanan 
tersebut  sulit  diberikan  jika  format  data  yang  digunakan 
bervariasi. Penerapan Linked Open Data (LOD) dapat mengatasi 
kesulitan  tersebut,  tetapi  pembangunan  LOD  dengan  aplikasi 
berbasis  GUI  tidak  efisien.  Oleh  karena 
itu,  LOD  akan 
digunakan  sebagai  basis  pengetahuan  mesin  pencari  portal 
INDAH.  Sementara  itu,  website  BPS  memiliki  fitur  pencarian 
serupa  dengan  mesin  pencari  di  portal  INDAH.  Namun, 
efektivitas pencariannya  masih perlu  ditingkatkan. Oleh  karena 
itu,  penelitian  ini  dilakukan  untuk  mengembangkan  metode 
transformasi  data  yang  efisien  dan  membangun  mesin  pencari 
fungsi  pencarian  yang 
dengan  menerapkan  LOD  dan 
meningkatkan  efektivitas  pencarian.  Fungsi  tersebut  meliputi 
klasifikasi  kueri  dan  ekspansi  kueri.  Hasilnya,  metode 
transformasi  data  menjadi  LOD  yang  dibangun  dapat  berjalan 
secara efisien (rata-rata 1,86 detik/tabel) dan mesin pencari yang 
dibangun  dapat  berjalan  dengan  baik.  Dengan  adanya  fungsi 
pencarian,  hasil  yang  diperoleh  lebih  baik,  dilihat  dari  nilai 
precision,  recall,  dan  relevansi 
infobox  pada  eksperimen 
pencarian. 

Kata  Kunci—  Search  Engine,  Linked  Open  Data,  Text 

classification, Query Expansion 

I.  LATAR BELAKANG 
Berdasarkan  Perpres  Nomor  39  tahun  2019  tentang  Satu 
Data  Indonesia  (SDI)  [1],  SDI  adalah  kebijakan  tata  kelola 
data  pemerintah  untuk  menghasilkan  data  yang  akurat, 
mutakhir,  terpadu,  dan  dapat  dipertanggungjawabkan,  serta 
mudah  diakses  dan  dibagipakaikan  antar  instansi  pusat  dan 
instansi  daerah  sebagai  dasar  perencanaan,  pelaksanaan, 
evaluasi,  dan  pengendalian  pembangunan.  Berdasarkan 
perpres  tersebut  pula,  Badan  Pusat  Statistik  (BPS)  memiliki 
peran  dan  tanggung  jawab  mewujudkan  tercapainya  SDI, 
yaitu sebagai pembina data. Oleh karena itu, BPS membangun 
suatu  sistem  guna  mendukung  tercapainya  SDI.  Sistem 
tersebut  bernama  portal  Indonesia  Data  Hub  (INDAH).  Hal 
tersebut diketahui berdasarkan hasil  wawancara dengan salah 
satu  pegawai  BPS  RI  yang  merupakan  anggota  pengembang 
portal INDAH pada tim ontologi. 

[2],  portal 

INDAH  merupakan  one 

Dilansir  dari  laman  Youtube “INDAH  Berbagi  DATA  dan 
INSIGHT” 
stop 
collaboration  platform  yang  bertujuan  untuk  meningkatkan 
literasi  data  dan  value  of  statistics  serta  mendukung 
interoperabilitas data dan kolaborasi eksplorasi terhadap data. 
Format  penyimpanan  data  yang  digunakan  saat  ini,  yaitu 
tercapainya  SDI. 
Spreadsheet  dan  CSV  menghambat 

Penggunaan format tersebut  menyebabkan integrasi data sulit 
dilakukan  karena  belum  memiliki  format  yang  standar  dan 
terpisah  antara  data  dan  metadata.  Permasalahan  tersebut 
dapat  diatasi  dengan  menerapkan  teknologi  web  semantik 
guna membangun linked open data (LOD) [3].  

LOD  merupakan  data  pada  web  dengan  format  data  dan 
metadata  yang  standar  serta  saling  terhubung.  LOD  dapat 
mendukung  tercapainya  SDI,  terutama  dalam  pemenuhan 
prinsip  standar  metadata  dan  interoperabilitas.  Pada  mesin 
pencari,  penggunaan  LOD  dapat  memudahkan  pencarian 
karena pencarian tidak terbatas pada dokumen yang berisi data, 
melainkan pada data itu sendiri [4]. 

Dalam  pembangunan  LOD  untuk  data  statistik,  W3 
merekomendasikan  suatu  standar,  yaitu  RDF  Data  Cube 
Vocabulary [5]. Terdapat beberapa aplikasi berbasis GUI yang 
dapat digunakan untuk membangun LOD, seperti Protégé  [6] 
telah 
dan  OpenRefine 
menggunakan  OpenRefine  untuk  mentransformasikan  data 
dari  format  spreadsheet  atau  CSV  menjadi  LOD.  Aplikasi 
tersebut mampu menghasilkan sintaks LOD, tetapi pengaturan 
representasi data masih dilakukan secara manual untuk setiap 
tabel. 

[7].  Penelitian  Situmorang 

[8] 

Saat  ini,  BPS  telah  memiliki  fitur  pencarian  yang  serupa 
dengan  mesin  pencari  pada  portal  INDAH,  yaitu  fitur 
pencarian  pada  website  BPS.  Namun,  berdasarkan  publikasi 
Analisis Hasil Survei Kebutuhan Data (SKD) BPS tahun 2020, 
terdapat  saran  dan  rekomendasi  yang  menyebutkan  bahwa 
fitur  pencarian  data  pada  website  BPS  merupakan  prioritas 
perbaikan utama pada Pelayanan Statistik Terpadu (PST) BPS 
Pusat,  sehingga  fitur  pencarian  di  website  BPS  tidak  dapat 
langsung  diadopsi  ke  portal  INDAH.  Saran  dan  rekomendasi 
tersebut  diberikan  karena  atribut  pencarian  data  termasuk 
dalam  empat  atribut  dengan  persentase  kepuasan  terendah. 
Selain  itu,  berdasarkan  analisis  gap  kepuasan  konsumen 
terhadap  pelayanan  BPS  pada  publikasi 
tersebut  pula, 
pencarian data di website BPS memiliki nilai gap paling jauh, 
yaitu sebesar -0,31. Hal ini menunjukkan bahwa kinerja pada 
atribut  pencarian  data  masih  belum  memenuhi  harapan 
konsumen di PST BPS Pusat. 

Salah  satu  peluang  peningkatan  layanan  pencarian  data  di 
website BPS adalah menyediakan layanan pencarian tabel data 
dalam  publikasi.  Publikasi  merupakan  bentuk  penyajian  data 
yang paling sering digunakan BPS. Berdasarkan analisis hasil 
SKD  2020,  50.93%  konsumen  penyedia  data  BPS  Pusat 
memeroleh  data  melalui  sumber  data  publikasi.  Publikasi 
dapat  memuat  tabel  data  yang  bermanfaat  bagi  konsumen, 

 1 / 8 

 
 
 
tetapi  akses  terhadap  tabel  data  tersebut  sulit  karena  fitur 
pencarian  publikasi  di  website  BPS  hanya  dapat  digunakan 
untuk  mencari  file  PDF  publikasi.  Informasi  yang  tersedia 
pada  hasil  pencarian  tersebut  adalah  judul  dan  abstraksi  saja, 
sedangkan 
tabel  data  dalam 
publikasi hanya dapat dilakukan setelah mengunduh  file PDF 
publikasi. 

informasi  mengenai  daftar 

layanan  pencarian 

Selain  menambahkan 

tabel  dalam 
publikasi,  pengembangan  mesin  pencari  dapat  dilakukan 
untuk  meningkatkan  efektivitas  pencarian.  Mesin  pencari 
pada  portal  INDAH  harus  efektif  dalam  menemukan  data 
yang  dicari  karena  dibutuhkan  untuk  memberikan  layanan 
lainnya,  seperti  analisis  data  dan  berbagi  data.  Peningkatan 
efektivitas  dapat  dilakukan  dengan  menyediakan  fungsi 
pencarian  agar  hasil  pencarian  yang  diperoleh  lebih  relevan 
dengan  kata  kunci  atau  kueri  pengguna.  Fungsi  pencarian  ini 
adalah  klasifikasi  kueri  dan  ekspansi  kueri.  Penggunaan 
klasifikasi  kueri  secara  otomatis  pada  mesin  pencari  dapat 
mempersempit  hasil  pencarian  sehingga  tidak  menampilkan 
hasil  yang  tidak  relevan  [9],  [10].    Fungsi  klasifikasi  kueri 
dapat dibangun dengan melakukan manual matching dan/atau 
pemodelan  machine  learning  [10].  Ekspansi  kueri  sangat 
efektif  untuk  mendapatkan  dokumen  yang  sesuai  karena 
memiliki  nilai  keakuratan  yang  tinggi  [11].  Dengan  ekspansi 
kueri,  kata  kunci  tidak  terbatas  pada  kata-kata  yang  diisikan 
pengguna,  tetapi  ditambahkan  dengan  istilah-istilah  yang 
relevan ke dalam kata kunci tersebut [12], [13]. 

Berdasarkan kepentingan untuk mengubah format data BPS 
menjadi  LOD  dan  membangun  mesin  pencari  di  portal 
INDAH, maka penelitian ini dilakukan untuk mengidentifikasi 
dan  menerapkan  metode  pembangunan  LOD  dari  data  BPS, 
kemudian  mengembangkan  mesin 
dengan 
menggunakan    LOD  sebagai  salah  satu  basis  pengetahuan. 
Mesin  pencari  yang  akan  dibangun  diharapkan  dapat 
mengatasi  masalah  pada  fitur  pencarian  di  website  BPS 
sehingga  meningkatkan  efektivitas  pencarian  data  bagi 
pengguna. 

pencari 

II.  TUJUAN PENELITIAN 

Terdapat tiga tujuan utama dari penelitian ini, yaitu: 

1.  Mengembangkan  metode  transformasi  data  menjadi  LOD 

yang efisien untuk tabel dinamis BPS. 

2.  Mengidentifikasi 

peluang 

peningkatan 

efektivitas 

pencarian data di portal INDAH. 

3.  Mengembangkan  mesin  pencari  pada  portal  INDAH 
dengan  menerapkan  LOD  sebagai  basis  pengetahuan  dan 
fungsi-fungsi  yang  meningkatkan  efektivitas  pencarian 
data. 

III. PENELITIAN TERKAIT 

A.  Linked Open Data (LOD) 

Penelitian  Situmorang  [8]  menjelaskan  implementasi  LOD 
pada  data  BPS  dan  membangun  antarmuka  berbasis  GUI 
untuk  menampilkan,  menyaring,  dan  melihat  data  terkait 
dalam  format  RDF.  Pembangunan  LOD  dilakukan  dengan 
transformasi  data  dari  format  excel  atau  CSV  menjadi  RDF 
dengan  aplikasi  OpenRefine.  Aplikasi  tersebut  berbasis  GUI 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

sehingga  memerlukan  banyak  interaksi  pengguna.  Penelitian 
tersebut  memberikan 
pembangunan  LOD 
pedoman 
menggunakan standar RDF data cube vocabulary. 

Penelitian Fakhriyanto [14] mengusulkan struktur data BPS 
dengan  membuat  hirarki  struktur  publikasi  di  BPS.  Struktur 
yang  dibangun  terdiri  dari  judul  dan  halaman  publikasi 
sebagai  root  dan  sejumlah  relasi  dengan  entitas  lain  yang 
dibagi  kedalam  beberapa  level,  yaitu  indikator,  penjelas 
indikator,  wilayah,  dan  waktu.  Struktur  tersebut  kemudian 
dijadikan  dasar  dalam  membangun  basis  pengetahuan 
publikasi.  Kemudian,  dibangun  mesin  pencari  semantik  pada 
layanan  permintaan  data  di  BPS  menggunakan  basis 
pengetahuan  yang  telah  dibangun.  Mesin  pencari  tersebut 
mampu  memahami konteks pengguna dalam permintaan data 
publikasi.  Kemudian,  dikembangkan  pula  sistem  penjawab 
otomatis  melalui  twitter.  Sistem  penjawab  otomatis  tersebut 
dapat memberikan informasi mengenai ketersediaan data yang 
dicari dengan merekomendasikan judul publikasi dan halaman 
data  yang  sesuai.  Sistem  penjawab  otomatis 
tersebut 
mengurangi  beban  kerja  pegawai  BPS  dalam  menjawab 
pertanyaan 
tentang  keberadaan  data.  Kelemahan  dari 
penelitian  tersebut  adalah  penyusunan  basis  pengetahuan 
dilakukan  secara  manual  mengikuti  semantik  dari  kumpulan 
publikasi  data  yang  ada.  Saat  ini  terdapat  lebih  dari  4000 
publikasi  di  website  BPS  sehingga  penyusunan  basis 
pengetahuan  secara  manual  menjadi  tidak  efisien.  Penelitian 
tersebut  memberikan  acuan  pada  penelitian 
ini  untuk 
menyusun basis pengetahuan. 

B.  Pencarian Isi Publikasi BPS 

Penelitian  Mortin  [15]  mengembangkan  mesin  pencari 
publikasi  BPS.  Mesin  pencari  tersebut  menambahkan  daftar 
isi  sebagai  indeks  pencarian,  sehingga  pencarian  dapat 
dilakukan  hingga  ke  daftar 
isi  publikasi.  Pada  sistem 
sebelumnya,  mesin  pencari  hanya  dapat  menampilkan  hasil 
untuk  pencarian  judul  dan  abstraksi  saja.  Daftar  isi  diperoleh 
dengan  cara  parsing dari dokumen  PDF  publikasi.  Penelitian 
tersebut  menghasilkan  mesin  pencari  yang  mampu 
memberikan  hasil  yang  lebih  baik.  Namun,  mesin  pencari 
tersebut  masih  terbatas  pada  pencarian  publikasi  sehingga 
tidak  membagi  hasil  pencarian  berdasarkan  tabel-tabel  yang 
tersebut 
ada  dalam  publikasi 
diperoleh informasi yang bermanfaat bagi penelitian ini, yaitu 
metode  pengumpulan  data  publikasi  BPS,  metode  parsing 
PDF  publikasi,  dan  penyajian  hasil  pencarian  dalam  kategori 
subjek. 

tersebut.  Pada  penelitian 

IV. METODE PENELITIAN 

Alur kerja pada penelitian ini dapat dilihat pada Gambar 1. 
Penelitian  diawali  dengan  melakukan  pengumpulan  dan 
penyiapan  data  yang  diperlukan.  Data  yang  dikumpulkan 
adalah  tabel  dinamis  dan  publikasi  melalui  akses  terhadap 
Web  API  BPS  [16].  Tabel  dinamis  dikumpulkan  dengan 
mengakses API data. Tabel dinamis yang dikumpulkan adalah 
seluruh  tabel  dinamis  yang  memuat  data  beserta  komponen 
penyusunnya,  yaitu  pada  API  subject  category  (subcat), 
subject,  vertical  variable  (vervar),  turunan  variabel  (turvar), 
dan turunan tahun (turth). Vervar dan turvar memuat jenis- 

 2 / 8 

 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

sehingga  dipilih  pada  penelitian 

  triplestore  yang  dipilih  adalah  Apache  Jena  Fuseki  [18]. 
Apache  Jena  Fuseki  digunakan  dalam  pengembangan  portal 
ini  untuk 
INDAH 
memudahkan  integrasi  sistem  yang  dikembangkan  dengan 
project  yang  sedang  dikerjakan  BPS.  Kemudian,  dilakukan 
evaluasi 
yang 
dikembangkan.  Evaluasi  dilakukan  dengan  membandingkan 
waktu  yang  diperlukan  untuk  melakukan  transformasi  data 
menggunakan  metode  ini  dengan  menggunakan  aplikasi 
OpenRefine. 

efisiensi  metode 

transformasi 

data 

Tahap  selanjutnya  adalah  pembangunan  mesin  pencari. 
Pertama, dilakukan penyusunan index dari dokumen pencarian. 
Penyusunan  index  merupakan  proses  identifikasi  kata-kata 
yang  terdapat  pada  dokumen  pencarian  sehingga  tersimpan 
pada  basis  data  mesin  pencari.  Penysusunan  index  dilakukan 
dengan  menggunakan  fitur  indexing  yang  tersedia  pada 
[19].  Kemudian,  dilakukan  perancangan 
Apache  Solr 
mekanisme 
peluang 
data 
pencarian 
pengembangan  yang  ditemukan  dan  masukan  dari  subject 
matter.  Hasilnya,  diperoleh  rancangan  mekanisme  pencarian 
data  yang  memuat  fungsi-fungsi  yang  dinilai  mampu 
meningkatkan  efektivitas  pencarian  atau  diperlukan  dalam 
mesin  pencari  secara  umum.  Selanjutnya,  dibangun  mesin 
pencari  berdasarkan  rancangan 
tersebut.  Mesin  pencari 
diimplementasikan  pada  framework  Flask  [20]  pada  bahasa 
pemrograman  Python,  yang  diintegrasikan  dengan  Apache 
Solr sebagai platform pencarian, Apache Jena Fuseki sebagai 
triplestore, dan Vue.js [21] sebagai framework front-end. 

berdasarkan 

Terakhir,  dilakukan  evaluasi  terhadap  mesin  pencari  yang 
telah  dibangun.  Evaluasi  dilakukan  untuk  mengukur  apakah 
mesin pencari yang dibangun dapat berjalan dengan baik dan 
meningkatkan  efektivitas  pencarian.  Evaluasi  dilakukan 
dengan  black  box  testing  [22]  dan  membandingkan  nilai 
precision dan  recall pada mesin pencari  yang  memuat  fungsi 
pencarian  terhadap  mesin  pencari  tanpa  fungsi  pencarian. 
Penelitian  [23]  dan  [24]  menunjukkan  percobaan  untuk 
menghitung  precision  dan  recall  pada  hasil  pencarian.  Pada 
dengan 
eksperimen 
referensi 
menggunakan kata kunci yang terbagi menjadi tiga jenis, yaitu 
kata (simple), frasa (compound), dan kalimat (complex). 

dilakukan 

tersebut, 

V.  KERANGKA PIKIR 

Kerangka  pikir  pada  penelitian  ini  dapat  dilihat  pada 
Gambar 2. Penelitian ini didasari oleh kepentingan BPS dalam 
mendukung  SDI,  yang  diwujudkan  dalam  pembangunan 
portal  INDAH.  Format  penyimpanan  data  yang  digunakan 
saat ini,  yaitu spreadsheet dan CSV menghambat tercapainya 
SDI. Penggunaan format tersebut menyebabkan integrasi data 
sulit  dilakukan  karena  belum  memiliki  format  yang  standar 
dan terpisah antara data dan metadata. Permasalahan tersebut 
dapat  diatasi  dengan  menerapkan  teknologi  web  semantik 
guna membangun linked open data (LOD). Penggunaan LOD 
memiliki beberapa kelebihan, seperti format yang standar dan 
disepakati  bersama,  mendukung 
interoperabilitas,  dan 
memudahkan  pencarian  karena  pencarian  tidak  terbatas  pada 
dokumen yang berisi data, melainkan pada data itu sendiri.  

 3 / 8 

Gambar 1. Alur Penelitian 

jenis  klasifikasi  data.  Klasifikasi  tersebut  terdiri  dari  grup 
klasifikasi  dan  item  klasifikasi.  “Jenis  kelamin”  merupakan 
contoh  grup  klasifikasi  sedangkan  “perempuan”  atau  “laki-
laki” merupakan contoh item klasifikasi. Dalam tabel dinamis, 
vervar  digunakan  untuk  penamaan  baris  sedangkan  turvar 
digunakan  untuk  penamaan  kolom.  Tabel  dinamis  yang 
dikumpulkan  sejumlah  1131  tabel.  Tabel  dinamis  yang  telah 
dikumpulkan  kemudian  disiapkan  agar  dapat  digunakan  pada 
tahap selanjutnya, meliputi cleaning dan integrating data. 

Publikasi  yang  dikumpulkan  sejumlah  200  publikasi 
terbaru yang dirilis sampai dengan Desember 2020. Dari 200 
publikasi tersebut, publikasi di tingkat nasional yang memuat 
tabel data sejumlah 108 publikasi. Selanjutnya, dicatat nomor 
halaman  awal  dan  akhir  daftar  tabel  dalam  publikasi,  serta 
jumlah halaman romawinya. Pencatatan nomor halaman awal 
dan  akhir  daftar  tabel  dilakukan  untuk  menandai  halaman 
yang  akan  di-parsing.  Jumlah  halaman  romawi  dicatat  untuk 
mengetahui  halaman  sebenarnya  dari  suatu  tabel  di  dalam 
PDF.  Kemudian,  dilakukan  parsing  daftar  tabel  untuk  setiap 
publikasi. Kemudian teks hasil parsing diproses menggunakan 
regular  expression  sehingga  diperoleh  daftar  tabel  yang 
terstruktur.  Dari  108  publikasi  yang  di-parsing,  hanya  71 
publikasi  yang  menghasilkan  daftar  tabel  terstruktur.  Sisanya 
gagal  di-parsing  karena  menghasilkan  teks  yang  berantakan 
atau  tidak  dikenali.  Jumlah  tabel  data  yang  terdapat  pada  71 
publikasi tersebut adalah 2175 tabel. Dari 2175 tabel tersebut, 
terdapat  1946  tabel  yang  unik.  Sisanya  duplikat  karena 
terdapat  tabel  yang  sama  termuat  dalam  lebih  dari  satu 
publikasi. 

Tahap  berikutnya  adalah  transformasi  data  menjadi  LOD 
dengan  bantuan  bahasa  pemrograman  Python  dan  library 
RDFLib  [17].  Peneliti  membangun  fungsi  transformasi  data 
berdasarkan padanan struktur data cube dengan tabel dinamis 
transformasi  data  dilakukan  dengan 
BPS.  Implementasi 
memanggil 
tahap 
transformasi 
fungsi 
implementasi,  diukur  berapa  lama  waktu  yang  diperlukan 
untuk menyelesaikan transformasi data. Setelah ditransformasi, 
data LOD disimpan dalam triplestore. Pada penelitian ini,  

tersebut.  Pada 

 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

efektivitas 

transformasi  terhadap  penggunaan  OpenRefine.  Selanjutnya, 
evaluasi 
dengan 
pencarian 
membandingkan  nilai  precision  dan  recall  pada  hasil 
pencarian sebelum dan sesudah menerapkan fungsi pencarian 
yang disusun. Selain itu, dibandingkan pula relevansi  infobox 
yang  ditampilkan  sebelum  dan  sesudah  menerapkan  fungsi 
pencarian. 

dilakukan 

Gambar 2. Kerangka Pikir 

Namun,  pembangunan  LOD  pada  penelitian  Situmorang  [8] 
tidak efisien karena menggunakan aplikasi berbasis GUI.  

Salah  satu  layanan  pada  portal  INDAH  adalah  pencarian 
data melalui mesin pencari. BPS telah memiliki mesin pencari 
di website BPS. Namun berdasarkan Analisis hasil SKD 2020 
[25],  mesin  pencari  tersebut  masih  perlu  diperbaiki.  Mesin 
pencari  di  portal 
INDAH  diharapkan  efektif  dalam 
menampilkan data yang dicari pengguna.  

Berdasarkan permasalahan tersebut, terdapat peluang untuk 
menerapkan LOD dalam mendukung SDI dan pencarian data. 
Pertama,  terdapat  teknologi  web  semantik  yang  menjadi 
kerangka  dalam  pembangunan  seperti  Resource  Description 
Framework (RDF) sebagai model data dan Data Cube sebagai 
kosakata  RDF  untuk  data  statistik  multidimensi.  Kedua, 
pembangunan  LOD  dapat  lebih  efisien  dengan  menggunakan 
library  yang  tersedia  pada  bahasa  pemrograman  karena  lebih 
fleksibel  dibandingkan  aplikasi  berbasis  GUI.  Kemudian, 
tersedianya  Web  API  BPS  menjadi  peluang  untuk 
mengumpulkan data secara efisien. 

Berdasarkan  peluang  yang  ada,  dilakukan  transformasi 
tabel  dinamis  dari  JSON  menjadi  LOD  sesuai  dengan  RDF 
data cube vocabulary,  mengembangkan fitur pencarian yang 
meningkatkan  efektivitas  pencarian  data,  mengukur  efisiensi 
transformasi data dan efektivitas mesin pencari yang dibangun. 
Selanjutnya,  dilakukan  perancangan  dan  implementasi. 
Metode  transformasi  data  dibangun  menggunakan  bahasa 
pemrograman  Python  dan 
library  RDFLib.  Kemudian, 
dilakukan  perancangan  sistem  yang  melibatkan  beberapa 
teknologi,  yaitu  klasifikasi  kueri  menggunakan  machine 
learning,  ekspansi  kueri  menggunakan  fuzzy  search/look  up 
dictionary,  Word  Correction  dengan  algoritma  Peter  Norvig, 
dan menampilkan infobox dengan SPARQL query. Sistem  ini 
kemudian  diimplementasikan  pada  framework  Flask  dan 
diintegrasikan  dengan  dengan  Apache  Solr  sebagai  platform 
pencarian  dan  Apache  Jena  Fuseki  sebagai  triplestore  untuk 
menyimpan data RDF. 

Hasil  yang  diharapkan  adalah  metode  transformasi  data 
menjadi LOD yang efisien, mesin pencari dengan menerapkan 
LOD  sebagai  basis  pengetahuan  dan  memiliki  fungsi-fungsi 
yang  dapat  meningkatkan  efektivitas  pencarian.  Efisiensi 
metode  transformasi  data  yang  dibangun  diukur  dengan 
membandingkan  waktu  yang  diperlukan  untuk  melakukan 

VI.  HASIL DAN PEMBAHASAN 

A.  Padanan Struktur RDF Data Cube Vocabulary dengan 

tersedia  pada 

Tabel Dinamis BPS 
Pemadanan  struktur  RDF  Data  Cube  Vocabulary  dengan 
tabel  dinamis  dilakukan  untuk  mengetahui  apakah  entitas 
yang 
tabel  dinamis  cukup  untuk  dapat 
ditransformasikan menjadi LOD dan bagaimana setiap entitas 
ditransformasikan  menjadi  LOD  sesuai  dengan  data  cube 
vocabulary.  Data  Cube  Vocabulary  dapat  dibagi  menjadi 
empat  bagian  utama,  yaitu  dataset,  data  structure  definition 
(dsd), slicekey dan slice, dan observation. 

Bagian dataset  memuat keterangan umum  mengenai tabel, 
seperti  judul,  deskripsi,  subjek  data,  penerbit,  dan  tanggal 
terbit,  serta  berisi  relasi  ke  bagian  lain,  yaitu  dsd  dan  slice. 
Informasi  yang  dibutuhkan  untuk  menyusun  bagian  dataset 
terdapat  pada  API  data,  kecuali  tanggal  terbit  atau  tanggal 
update data. 

Bagian  dsd  berisi  kerangka  dari  suatu  tabel,  yang  terdiri 
dari  baris  dan  kolom  (dimension),  nama  data  yang  dimuat 
(measure), dan satuan data (attribute). Selanjutnya, nilai-nilai 
pada  dimension  (sebagai  grup  klasifikasi)  didefinisikan 
sebagai  slice  (sebagai  item  klasifikasi).  Informasi  yang 
dibutuhkan  pada  bagian  dsd  terdapat  pada  API  data,  tetapi 
harus  dilengkapi  dengan  melakukan  matching  pada  API 
vervar dan turvar, Matching dilakukan untuk memeroleh nama 
dan ID dimensi turvar, serta ID dimensi vervar. 

Slice merupakan suatu entitas yang berisi semua data yang 
merujuk  pada  nilai  dimensi  tertentu.  Sedangkan  slicekey 
merupakan  struktur  dari  slice,  yaitu  nilai  dimensi  yang 
menjadi  nama  baris  pada  tabel.  Informasi  yang  dibutuhkan 
terdapat API data tetapi harus dilengkapi dengan ID unik item 
dimensi vervar pada API vervar. 

Bagian  observation  berisi  nilai-nilai  data  pada  tabel.  Nilai 
data tersebut terdapat pada API data. Setiap nilai memilik ID 
unik  yang  merupakan  kombinasi  dari  ID  vervar,  variabel, 
turvar,  tahun,  dan  turunan  tahun,  sehingga  dapat  dipetakan 
untuk setiap slice dan slicekey. 

B.  Transformasi Data Menjadi LOD 

Data  BPS  yang  dapat  langsung  ditransformasi  menjadi 
LOD  adalah data  tabel dinamis  yang diperoleh dari  web API 
BPS.  Data  tersebut  tersimpan  dalam  format  JSON  sehingga 
lebih fleksibel untuk ditransformasikan ke format lain. Untuk 
melakukan 
lain  seperti 
spreadsheet  atau  CSV,  data  harus  diproses  terlebih  dahulu, 
terutama dengan unpivot columns. Unpivot columns dilakukan 
untuk  mengubah  data  dalam  bentuk  kolom  menjadi  data 
dalam bentuk baris. Data dengan nama kolom bertingkat tidak 
dapat dibaca dengan baik oleh bahasa pemrograman sehingga  

transformasi  data  dari 

format 

 4 / 8 

 
 
 
mempersulit transformasi data. 

Struktur  data  tabel  dinamis  dalam  JSON  beserta  relasi 
dengan  komponen  penyusunnya  dipadankan  dengan  struktur 
Data  Cube  untuk  merancang  mekanisme  transformasi  data 
menjadi  LOD.  Kemudian,  fungsi  transformasi  data  menjadi 
LOD  dibangun.  Transformasi  dilakukan  dengan  menyusun 
bagian-bagian  tabel  dinamis  ke  dalam  bagian  LOD  yang 
sesuai.  Selanjutnya,  fungsi  transformasi  diimplementasikan 
pada seluruh tabel dinamis yang ada. Waktu yang diperlukan 
untuk  melakukan  transformasi  1131  tabel  dinamis  menjadi 
LOD adalah 35 menit 13 detik atau sekitar 1,86 detik per tabel. 
Transformasi  menggunakan  metode  ini  memerlukan  waktu 
yang singkat dan memberikan hasil yang sesuai, setiap bagian 
LOD dapat ditulis seperti yang dimaksud. 

durasi 

transformasi 

Peneliti  membandingkan 

data 
menggunakan  fungsi  transformasi  yang  dibangun  dengan 
penggunaan  aplikasi  OpenRefine  secara  manual.  Tabel  I 
menunjukkan 
untuk 
mentransformasikan  lima  tabel  dinamis  ke  format  RDF, 
masing-masing dilakukan lima kali percobaan. 

diperlukan 

waktu 

yang 

Berdasarkan  tabel  I,  durasi  transformasi  data  cenderung 
meningkat  seiring  bertambahnya 
jumlah  observasi  dan 
menurun  dibanding  percobaan  pertama.  Semakin  banyak 
jumlah  observasi,  semakin  banyak  pula  data  yang  harus 
ditransformasi  sehingga  durasi  transformasi  data  menjadi 
lebih  lama.  Setelah  percobaan  pertama,  durasi  transformasi 
menurun karena peneliti menggunakan hasil transformasi pada 
percobaan pertama sebagai acuan. 

Penggunaan  aplikasi  OpenRefine  mempunyai  kelemahan, 
yaitu  sangat  tergantung  pada  kemampuan  dan  ketelitian 
pengguna,  serta  tidak  dapat  menuliskan  node  objek  pada 
prefix lain. Pengguna OpenRefine harus memiliki pemahaman 
yang  baik  mengenai  konsep  RDF  dan  penggunaan  data  cube 
letak  masing-masing 
vocabulary 
komponen  LOD.  Pengguna 
selama 
teliti 
transformasi  untuk  menghindari  terjadinya  salah  ketik,  salah 
pemilihan  opsi,  dan  penyusunan  triple  yang  terlewat  atau 
duplikat. Kemudian, konfigurasi node objek pada OpenRefine 
terbatas. 

sehingga  mengetahui 

juga  harus 

C.  Fungsi Pencarian 

Terdapat  empat  fungsi  pencarian  yang  dibangun,  yaitu 
klasifikasi    kueri,  ekspansi  kueri,  perbaikan  kueri  (word 
correction), dan penampilan penggalan data spesifik. 

Klasifikasi kueri: 
Klasifikasi  kueri  dilakukan  untuk  mengelompokkan  kueri 
pengguna  ke  dalam 
tiga  kategori,  yaitu  sosial  dan 
kependudukan,  ekonomi  dan  perdagangan,  serta  pertanian 
pertambangan.  Fungsi  klasifikasi  kueri  disusun  dengan 
terlebih  dahulu. 
membangun  model  machine 
Pemodelan  dilakukan  menggunakan  data 
training  yang 
berasal  dari  daftar  tabel  dinamis  yang  telah  dikumpulkan.  
Variabel predictor yang digunakan adalah judul tabel dinamis 
item  klasifikasi  yang  sesuai. 
beserta  nama  grup  dan 
Sedangkan variabel target yang digunakan adalah subcat yang 
tercantum pada setiap tabel dinamis. Data test yang digunakan  

learning 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

TABEL I 
EVALUASI DURASI TRANSFORMASI DATA MENGGUNAKAN 
OPENREFINE (MENIT) 

Nomor tabel 
(jumlah 
observasi) 

Percobaan 

1 

2 

3 

4 

5 

1756 (1) 

18:28 

13:49 

12:44 

12:09 

11:47 

1828 (2) 

16:36 

14:03 

13:28 

12:59 

13:10 

1234 (6) 

23:31 

19:02 

18:28 

18:09 

18:57 

1408 (12) 

30:41 

27:20 

26:33 

26:59 

26:31 

1479 (12) 

28:32 

26:57 

25:02 

26:00 

25:48 

adalah daftar judul tabel dalam publikasi yang telah di-parsing. 
Data tersebut diberi label subcat terlebih dahulu secara manual. 
Sebelum dilakukan pemodelan, data diproses menggunakan 
NLP. NLP terdiri dari case folding, membuang tanda baca dan  
angka, 
tokenisasi,  membuang  stopword,  dan  stemming. 
Pemrosesan  tersebut  dilakukan  untuk  menyamakan  bentuk 
penulisan  dan  membuang  elemen  yang  tidak  perlu  atau 
mengganggu  dalam  pembentukan  model.  Selanjutnya, 
dilakukan  feature  extraction  untuk  merepresentasikan  teks 
dalam  bentuk  vektor  menggunakan  ukuran  term  frequency–
inverse document frequency (TF-IDF). 

Peneliti membandingkan performa dari beberapa algoritma 
machine  learning, yaitu Naive Bayes, SVM,  K-NN,  Random 
forest,  dan  Gradient  Boosting.  Pada  setiap  algoritma, 
dilakukan tuning agar memeroleh hasil yang optimal. Ukuran 
performa  yang  diguakan  adalah  akurasi  dan  F1-score. 
Perbandingan  performa  antar  algoritma  machine  learning 
dapat  dilihat  pada  tabel  II.  Berdasarkan  tabel  tersebut 
diketahui bahwa algoritma yang memberikan performa terbaik  
adalah SVM. 

Ekspansi kueri: 
Ekspansi  kueri  dilakukan  untuk  menambahkan  kueri 
pengguna apabila kueri tersebut mengandung item klasifikasi. 
Item  klasifikasi  tidak  tercantum  pada  judul  tabel  sehingga 
tidak  terindeks.  Penambahan  kueri  pengguna  dengan  nama 
grup  klasifikasi  yang 
terindeks  meningkatkan  performa 
pencarian  karena nama  grup  klasifikasi relevan dengan  nama 
item  klasifikasi.  Daftar  grup  dan  item  klasifikasi  berasal  dari 
komponen  penyusun  tabel  dinamis,  yaitu  vervar  dan  turvar. 
Tidak  semua  klasifikasi  digunakan  untuk  ekspansi  kueri, 
peneliti  memilih  item  klasifikasi  yang  spesifik  merujuk  pada 
grup klasifikasi tertentu agar hasil ekspansi kueri tidak terlalu 
luas. 

Perbaikan Kueri / Word Correction: 
Word  correction  merupakan  fungsi  umum  pada  mesin 
ini  digunakan  algoritma  word 
pencari.  Pada  penelitian 
correction  yang  dibangun  oleh  Peter  Norvig  [26].  Algoritma 
ini  merupakan  algoritma  yang  sederhana  dan  ringan,  tetapi 
memiliki  akurasi  yang  cukup  baik,  yaitu  sekitar  80%  sampai 
dengan 90% [26]. 

Infobox: 
Infobox  diperoleh  dari  fungsi  yang  mengakses  LOD  untuk 

menemukan informasi spesifik yang relevan dengan kueri  

 5 / 8 

 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

TABEL II 
PERBANDINGAN PERFORMA ALGORITMA MACHINE LEARNING 
F1_score 3 
(%) 
85,96 

F1_score   
1 (%) 
90,24 

F1_score  
2 (%) 
94,65 

Akurasi 
(%) 
92,41 

Algoritma 

SVM 

Random 
Forest 

Gradient 
Boosting 

91,17 

87,48 

94,52 

83,30 

88,87 

85,15 

93,76 

74,71 

K-NN 

86,48 

81,51 

Naive-Bayes 

82,94 

77,36 

90,24 

87,14 

79,83 

75,11 

Gambar 4. Antarmuka Halaman Beranda 

Gambar 5. Antarmuka Halaman Hasil Pencarian 

saltik  pada  kueri  pengguna,  sistem  akan  memberikan  saran 
perbaikan  kueri.  Jika  saran  kueri  diterima,  pencarian  akan 
dilakukan kembali terhadap kueri tersebut. 

Gambar 3. Flowchart Mekanisme Pencarian Data 

E.  Pembangunan Mesin Pencari 

pengguna [27]. Misalnya, pencarian “kurs” pada Google dapat 
memberikan  informasi  spesifik  berupa  kurs  dollar  terbaru. 
Pada  mesin  pencari  yang  dibangun,  infobox  menampilkan 
penggalan data tabel dinamis. 

D.  Mekanisme Pencarian Data 

Mekanisme  pencarian  data  dapat  dilihat  pada  Gambar  3. 
Pertama, kueri diproses terlebih dahulu, terdiri dari klasifikasi 
kueri,  ekspansi  kueri,  perbaikan  kueri,  dan  penyusunan 
endpoint Solr. Output dari tahap ini adalah endpoint Solr.  

Selanjutnya,  endpoint  tersebut  di-request  ke  Solr  untuk 
mendapatkan  seluruh  hasil  pencarian.  Jika  pengguna  berada 
pada  halaman  pertama,  maka  akan  dilakukan  request  ke  Solr 
untuk  mendapatkan  satu  judul  tabel  dinamis  yang  paling 
relevan. Judul tabel dinamis dan kueri pengguna akan diproses 
untuk  menyusun  endpoint  Fuseki.  Selanjutnya,  dilakukan 
request  ke  Fuseki  untuk  mendapatkan  penggalan  data  dari 
tabel  dinamis  yang  paling  relevan  dengan  kueri  pengguna. 
Penggalan  data  tersebut  akan  ditampilkan  kepada  pengguna 
sebagai info box. 

Hasil pencarian  yang  terdiri dari daftar tabel data  dan  info 

box akan ditampilkan kepada pengguna. Apabila terdapat  

Mesin pencari telah dibangun sesuai rancangan mekanisme 
pencarian data. Bagian antarmuka mesin pencari dapat dilihat 
pada  Gambar  4,  5,  dan  6.  Antarmuka  tersebut  mengikuti 
pembangunan yang telah dilakukan di BPS. 

Gambar  4  menunjukkan  tampilan  beranda  Portal  INDAH. 
Halaman  tersebut  merupakan  hasil  pengembangan  portal 
INDAH di BPS. Halaman pencarian data yang memuat mesin 
pencari  yang  dibangun  dapat  diakses  dengan  mengisi  kata 
kunci atau kueri pada form pencarian di halaman beranda. 

Setelah  pengguna  mengisi  kata  kunci  pada  halaman 
beranda,  pengguna  akan  dialihkan  ke  halaman  pencarian 
seperti pada Gambar 5. Pada halaman tersebut,  terdapat  form 
pencarian  yang  telah  terisi  dengan  kata  kunci  pengguna.  Di 
bawah form tersebut terdapat hasil pencarian. Setiap halaman 
pencarian maksimal memuat 10 hasil pencarian beserta 1 (satu) 
infobox. Pada bagian kiri browser terdapat sidebar yang berisi 
kategori  subjek  yang  terdiri  dari  sosial  dan  kependudukan; 
ekonomi  dan  perdagangan;  dan  pertanian  dan  pertambangan. 
Kategori  subjek  tersebut  terpilih  otomatis  sesuai  hasil  fungsi 
klasifikasi  kueri.  Pengguna  dapat  mengubah  kategori  subjek 
yang  diinginkan  dengan  menekan  pilihan  pada  sidebar 
tersebut.  Apabila  terdapat  salah  ketik  yang  terdeteksi,  akan 
tersedia  link  di  bawah  form  kata  kunci  untuk  melakukan 
pencarian dengan kata kunci yang direkomendasikan. 

 6 / 8 

 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

TABEL III 
PERBANDINGAN NILAI PRECISION, RECALL, DAN RELEVANSI 
INFOBOX PADA MESIN PENCARI DENGAN DAN TANPA FUNGSI 
PENCARIAN 
Recall 

Infobox Relevan 

Precision 

Gambar 6. Antarmuka Halaman Tabel Dinamis 

Gambar  6  menunjukkan  tampilan  halaman  tabel  data 
setelah  pengguna  memilih  hasil  pencarian  berupa  tabel 
dinamis pada Gambar 5. Pada halaman tersebut terdapat judul 
yang  merupakan  gabungan  dari  judul  tabel  dinamis,  satuan 
(jika ada), dan tahun data. Pengguna dapat memilih tahun data 
yang  ditampilkan  pada  sidebar  di  sebelah  kiri.  Sedangkan 
untuk  hasil  pencarian  berupa  publikasi,  akan  ditampilkan 
publikasi pada halaman tabel data yang sesuai. 

F.  Evaluasi Mesin Pencari 

Evaluasi  mesin  pencari  yang  dilakukan  adalah  black  box 
testing  dan  membandingkan  nilai  precision,  recall,  dan 
relevansi infobox pada percobaan pencarian data. 

Black Box Testing:  
Peneliti  melakukan  black  box  testing  untuk  memastikan 
mesin pencari yang dibangun memberikan output yang sesuai 
untuk  setiap  input  yang  diberikan.  Berikut  ini  merupakan 
daftar skenario input yang dilakukan: 

1.  Mengisi kata kunci pencarian di beranda 
2.  Mengganti kata kunci pada halaman pencarian 
3.  Mengisi kata kunci yang memuat salah ketik 
4.  Mengganti subjek kategori pada sidebar 
5.  Mengganti halaman hasil pencarian 
6.  Menekan tombol “Selengkapnya...” pada infobox 
7.  Memilih hasil pencarian berupa tabel dinamis 
8.  Memilih hasil pencarian berupa tabel dalam publikasi 
9.  Mengganti tahun data pada tampilan tabel dinamis 
Setiap skenario  yang diuji  memberikan output  yang sesuai 
dengan  hasil  yang  diharapkan,  sehingga  disimpulkan  bahwa 
skenario tersebut valid. 

Precision, Recall, dan Relevansi Infobox: 
Nilai  precision dan  recall diukur pada  mesin pencari  yang 
menerapkan  fungsi  pencari  dengan  yang  tidak.  Penghitungan 
nilai  precision  dan  recall  dilakukan  dengan  eksperimen 
menggunakan kueri yang terbagi menjadi tiga jenis, yaitu kata 
(simple),  frasa  (compound),  dan  kalimat  (complex),  masing-
masing dua kueri. Kueri yang digunakan antara lain: 

1.  Alpukat 
2.  Inflasi 
3.  IPM perempuan 
4.  Upah tenaga kerja 
5.  Produksi teh di sumatera tahun 2018 
6.  Persentase  Rumah  Tangga  yang  Menguasai  Telepon 

Seluler memiliki rumah sendiri 

No 

1 

2 

3 

4 

5 

6 

Dengan 
Fungsi 
100 

Tanpa 
Fungsi 
0 

Dengan 
Fungsi 
100 

Tanpa 
Fungsi 
0 

Dengan 
Fungsi 
Ya 

Tanpa 
Fungsi 
Tidak 

100 

100 

100 

12 

56 

56 

6 

26,15 

100 

39,51 

100 

39,51 

100 

1,14 

100 

100 

100 

100 

100 

100 

Ya 

Ya 

Ya 

Ya 

Ya 

Ya 

Tidak 

Ya 

Tidak 

Tidak 

Tabel III menunjukkan perbandingan nilai precision, recall, 
dan  relevansi  infobox  untuk  mesin  pencari  yang  memuat 
fungsi  pencarian  (klasifikasi  dan  ekspansi  kueri)  terhadap 
mesin  pencari  yang  tidak  memuat  fungsi  tersebut.  Relevansi 
infobox juga  dibandingkan karena  merupakan  komponen dari 
precision,  yaitu  mengukur  kemampuan  mesin  pencari 
menampilkan hasil yang relevan. Setiap nomor pada tabel III 
merujuk pada kata kunci yang telah disebutkan sebelumnya. 

Berdasarkan  tabel  III,  nilai  precision  pada  mesin  pencari 
dengan  fungsi  pencarian  lebih  tinggi  daripada  mesin  pencari 
tanpa  fungsi  pencarian  pada  4  kueri,  yaitu  “alpukat”,  “upah 
tenaga  kerja”,  “produksi  teh  di  sumatera  tahun  2018”,  dan 
“Persentase  Rumah  Tangga  yang  Menguasai  Telepon  Seluler 
memiliki  rumah  sendiri”.  Sedangkan,  pada  kueri  “inflasi”, 
nilai  keduanya  sebesar  100%  karena  seluruh  tabel  yang 
relevan  dengan  kata  inflasi  berada  pada  kategori  yang  sama, 
yaitu ekonomi dan perdagangan. Kemudian, kata inflasi bukan 
merupakan  item  klasifikasi.  Sedangkan  pada  kueri  “IPM 
perempuan”,  nilai  precision  lebih  tinggi  pada  mesin  pencari 
tanpa  fungsi  pencarian  karena  kata  “perempuan”  merupakan 
item  klasifikasi  yang  diekspan  ke  grup  klasifikasi  “jenis 
kelamin”.  Sehingga,  tabel-tabel  yang  memuat  kata  “jenis 
kelamin”  ikut  terambil  pada  hasil  pencarian  meskipun  tabel 
tersebut tidak relevan dengan IPM. 

Selanjutnya,  nilai  recall  pada  kedua  mesin  pencari  hampir 
sama.  Lima  kueri  menghasilkan  nilai  recall  100%  karena 
penamaan  judul  tabel  dinamis  BPS  konsisten.  Perbedaan 
hanya terlihat pada kueri “alpukat”. Pada kueri tersebut, recall 
pada  mesin  pencari  dengan  fungsi  pencarian  sebesar  100% 
sedangkan pada mesin pencari tanpa fungsi pencari 0%. Nilai 
recall  sebesar  0%  terjadi  karena  tabel  yang  relevan  dengan 
kata  “alpukat”  tidak  memuat  kata  “alpukat”  pada  judulnya. 
Pada  mesin  pencari  dengan  fungsi  pencarian,    kata  “alpukat” 
terdeteksi sebagai item klasifikasi yang kemudian diekspan ke 
grup  klasifikasi  “tanaman  buah”.  Sehingga, 
tabel  yang 
memuat kata “tanaman buah” terambil pada hasil pencarian. 

Infobox dikatakan relevan jika variabel, grup klasifikasi dan 
atau  item  klasifikasi  yang  ditampilkan  pada  infobox  sesuai 
dengan  kueri  pengguna.  Berdasarkan  tabel  III,  mesin  pencari 
dengan fungsi pencarian menghasilkan infobox yang sesuai  

 7 / 8 

 
 
 
 
TABEL IV 
PERBANDINGAN NILAI PRECISION 10 HASIL TERATAS PADA 
MESIN PENCARI DENGAN DAN TANPA FUNGSI PENCARIAN 
No 
1 
2 
3 
4 
5 
6 

Dengan Fungsi Pencarian 
100 
100 
30 
20 
70 
70 

Tanpa Fungsi Pencarian 
0 
100 
30 
20 
50 
30 

untuk  enam  kueri  yang  diberikan.  Sedangkan  tanpa  fungsi 
pencarian, mesin pencari hanya mampu menghasilkan infobox 
yang  sesuai  pada  dua  kueri  yang  diberikan.  Tanpa    fungsi 
klasifikasi  kueri,  tabel  dinamis  dari  kategori  lain  memiliki 
peluang untuk ditampilkan sebagai  infobox. Kemudian, tanpa 
fungsi  ekspansi  kueri,  kueri  pencarian  tidak  ditambahkan 
untuk dapat menemukan judul tabel dinamis yang sesuai. 

Peneliti  mengevaluasi  nilai  precision  untuk  10  hasil 
pencarian  pertama.  Nilai  ini  diukur  karena  pengguna  hanya 
melihat hasil pada halaman pertama atau 10 hasil teratas saja 
[28],  bahkan  beberapa  hasil  teratas  saja  [29].  Tabel  IV 
menunjukkan  perbandingan  nilai  precision  10  hasil  teratas 
untuk mesin pencari dengan dan tanpa fungsi pencarian. 

Berdasarkan  tabel  tersebut,  nilai  precision  pada  mesin 
pencari  dengan  fungsi  pencarian  lebih  tinggi  daripada  mesin 
pencari  tanpa  fungsi  pencarian  pada  3  kueri,  yaitu  “alpukat”, 
“produksi  teh  di  sumatera  tahun  2018”,  dan  “Persentase 
Rumah  Tangga  yang  Menguasai  Telepon  Seluler  memiliki 
rumah sendiri”. Sedangkan pada kueri lainnya, nilai keduanya 
sama,  termasuk  kueri  “IPM  perempuan”  yang  sebelumnya 
memiliki nilai precision lebih tinggi pada mesin pencari tanpa 
fungsi  pencarian.  Nilai  precision  yang  lebih  tinggi  terjadi 
karena klasifikasi kueri dapat membatasi hasil pencarian agar 
tidak  menampilkan  hasil  dari  kategori  lain.  Kemudian, 
ekspansi  kueri  menambahkan  kueri  dengan  kata-kata  relevan 
yang  terdapat  pada  judul  tabel.  Sehingga,  bobot  pencarian 
pada  tabel  tersebut  meningkat  dan  tabel  tersebut  dapat 
ditampilkan pada urutan atas. 

VII. 

PENUTUP  

Untuk  mengatasi  masalah  transformasi  data  menjadi  LOD 
yang  tidak  efisien  dan  pencarian  data  yang  tidak  efektif, 
peneliti 
telah  mengembangkan  mesin  pencari  yang 
menerapkan  LOD  sebagai  basis  pengetahuan  dan  fungsi 
pencarian  yang  meningkatkan  efektivitas  pencarian,  yaitu 
klasifikasi kueri dan ekspansi kueri. Basis pengetahuan berupa 
LOD  diperoleh  dari  hasil  transformasi  data  yang  efisien  dari 
segi  waktu  dan  interaksi  dengan  komputer.  Fungsi  pencarian 
telah  dievaluasi  dan  diketahui  bahwa  dengan  adanya  fungsi 
pencarian,  hasil  pencarian  yang  diperoleh  lebih  baik,  dilihat 
dari  nilai  precision,  recall,  dan  relevansi  infobox  pada 
eksperimen  pencarian  untuk  enam  kueri  yang  diberikan. 
Secara  umum,  mesin  pencari  yang  dibangun  dapat  berjalan 
dengan baik, dilihat dari hasil black box testing. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

DAFTAR PUSTAKA 
[1]  Peraturan Presiden Nomor 39 tahun 2019 tentang Satu Data Indonesia 
[2]  Youtube.  (2020,  9).  Tentang  Indonesia  Data  Hub  (INDAH).  [Online]. 

Available: https://youtu.be/gYTeY6MIXZY 

[3]  M,  Hausenblas.  (2012,  1).  5  Star  Open  Data.  [Online].  Available: 

https://5stardata.info/en/ 

[4]  J.  Lehmann,  et  al,  “Dbpedia–a  large-scale,  multilingual  knowledge  base 
extracted from wikipedia,” Semantic web, vol. 6(2), pp 167-195, 2015  
[5]  W3C  Recommendation.  (2014,  1).  The  RDF  Data  Cube  Vocabulary. 

[Online]. Available: http://www.w3.org/TR/vocab-data-cube/ 

[6]  M. A. Musen, “The protégé project: a look back and a look forward,” AI 

matters, vol. 1(4), pp 4-12, 2015 

[7]  R. Verborgh, and M. D. Wilde, Using OpenRefine. Packt Publishing Ltd,  

2013. 

[8]  A.  Y.  Situmorang.  (2016).  Perancangan  Linked  Statistical  Data  Untuk 
Meningkatkan  Level  Open  Data  Pada  Publikasi  Data  BPS  [skripsi]. 
Jakarta : Politeknik Statistika STIS 

[9]  S. M. Beitzel, et al, “Improving automatic query classification via semi-
supervised  learning,”  in  5th  IEEE  International  Conference  on  Data 
Mining, IEEE, Nov 2005, pp. 8-pp. 

[10] V.  Raval,  and  P.  Kumar,  “SEReleC  (Search  Engine  Result  Refinement 
and  Classification”  In  IEEE-International  Conference  On  Advances  In 
Engineering, Science And Management, IEEE, Mar 2012, pp. 627-631. 
[11] A.  Wibowo,  “Peningkatan  performansi  sistem  temu  balik  informasi 
dengan  metode  phrasal  translation  dan  query  expansion”.  Batam: 
Politeknik Negeri Batam, 2012, pp 37-43. 

[12] H.  V.  Sala,  P.  R.  Leyva,  J.  P.  Febles,  and  V.  E.  Sentí,  ”Information 

retrieval with semantic annotation,” EasyChair, no. 1123, 2019. 

[13] B.  Yu,  ”Research  on  information  retrieval  model  based  on  ontology,” 
EURASIP  Journal  on  Wireless  Communications  and  Networking,  vol. 
2019(1), pp 1-8, 2019. 

[14] Fakhriyanto. (2015). Perancangan Semantic Search Engine dan Penjawab 
Otomatis Berbasis Twitter untuk Permintaan Data Publikasi BPS [skripsi]. 
Jakarta : Politeknik Statistika STIS 

[15] Mortin,  H.J.  (2019).  Pengembangan  Mesin  Pencari  Untuk  Pencarian  Isi 
Publikasi di Website BPS [skripsi]. Jakarta : Politeknik Statistika STIS 
[16] Badan  Pusat  Statistik  (BPS).  API  Documentation.  [Online].  Available  : 

https://webapi.bps.go.id/documentation/ 

[17] RDFLib  Team.  (2020).  RDFLib  5.0.0  Documentation.  [Online]. 

Available: https://rdflib.readthedocs.io/en/stable/ 

[18] Apache  Software  Foundation.  (2021).  Apache  Jena  Fuseki.  [Online]. 

Available: https://jena.apache.org/documentation/fuseki2/ 

[19] Apache  Software  Foundation.  (2021).  Apache  Solr. [Online].  Available: 

https://solr.apache.org/ 

[20] Pallets.  (2010).  Flask  Web  Development,  one  drop  at  a  time  [Online]. 

Available: https://flask.palletsprojects.com/en/2.0.x/ 

[21] Evan  You.  (2021).  Vue.js  The  Progressive  JavaScript  Framework. 

[Online]. Available: https://vuejs.org/ 

[22] S. Nidhra and J. Dondeti, “Black box and white box testing techniques-a 
literature  review,”  International  Journal  of  Embedded  Systems  and 
Applications, vol. 2(2), pp 29-50, 2012. 

[23] S. M. Shafi and R. Rather, Precision and recall of five search engines for 
retrieval of scholarly information in the field of biotechnology, 2005. 
[24] B.  S.  Kumar  and  J.  N.  Prakash,  “Precision  and  relative  recall  of  search 
engines: a comparative study of Google and Yahoo,” Singapore Journal 
of Library & Information Management, vol. 38(1), pp 124-137, 2009. 

[25] BPS RI. 2020. Analisis Hasil Survei Kebutuhan Data 2020. 
[26] Norvig,  P.  (2016,  8).  How  to  Write  a  Spelling  Corrector.  [Online]. 

Available: https://norvig.com/spell-correct.html 

[27] A. Singhal. (2012, 5). Introducing the Knowledge Graph: things, not 

strings. [Online]. Available: 
https://blog.google/products/search/introducing-knowledge-graph-things-
not/ 

[28] B.J. Jansen and A. Spink, “How are we searching the World Wide Web? 
A  comparison  of  nine  search  engine  transaction  logs,”  Information 
Processing & Management vol. 42(1), pp 248–263, 2006. 

[29] M.T. Keane, M. O’Brien and B. Smyth, “Are people biased in their use 
of search engines?,” Communications of the ACM, vol. 51(2), pp 49–52, 
2008. 

 8 / 8 

 
 
 
 
 
"
221709726,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Penerapan Metode Penalized Spline untuk  
Small Area Estimation Pendekatan Nonparametrik  
Studi Kasus Pengeluaran Rata-rata Perkapita  
Daerah Istimewa Yogyakarta 2019  

Hairia Muharimma (221709726, 4SD1) 
Dosen Pembimbing: Dr. Azka Ubaidillah, SST, M.Si 

Ringkasan—  BPS  lebih  mengandalkan  survei  daripada 
sensus  dalam  pengumpulan  data.  Pada  beberapa  kondisi, 
survei  memiliki  kelemahan  dimana  saat  ukuran  sampel 
kecil, maka pendugaan yang baik tidak bisa diperoleh. SAE   
menjadi solusi  untuk mendapatkan pendugaan yang baik 
dengan  menambah  informasi  berupa  variabel  penyerta 
dari  data  domain  besar  tanpa  harus  menambah  ukuran 
sampel.  SAE  umumnya  menggunakan  pemodelan 
parametrik,  namun  pemodelan  menjadi  kurang  fleksibel 
dalam  menyesuaikan  dengan  pola  data  hasil  survei  yang 
mungkin  saja  tidak  mirip  sama  sekali  dengan  distribusi 
formal  yang  ada.  Sehingga  pendekatan  nonparametrik 
menjadi  pilihan  alternatif.  Salah  satu  SAE  pendekatan 
nonparametrik  yang  sering  digunakan  adalah  dengan 
metode  Penalized  Spline.  Untuk  memudahkan  analisis, 
penelitian ini akan melakukan pembuatan function dengan 
R untuk mendapatkan nilai penduga dan mse dari metode 
Penalized  Spline  pendekatan  SAE  nonparametrik.  Hasil 
yang  didapat  adalah  hasil  estimasi  langsung  dan  estimasi 
tidak  langsung  menggunakan  EBLUP  Fay  Herriot  dan 
Regresi  Penalized Spline pada data pengeluaran rata-rata 
perkapita  Daerah  Istimewa  Yogyakarta  2019  yang 
modelnya sudah bekerja cukup baik. 

Kata Kunci— SAE Nonparametrik, Penalized Spline 

I.  LATAR BELAKANG 

Pusat 

(BPS) 

Badan 

adalah 

Statistik 

lembaga 
nonkementrian yang mempunyai tugas menyediakan informasi 
statistik yang berkualitas bagi pengguna data. Kegiatan utama 
BPS  adalah  mengumpulkan,  memproses,  menganalisa,  dan 
menyebarluaskan  data  statistik.  Pengumpulan  data 
ini 
dilakukan  dengan  mengadakan  kegiatan  sensus  dan  survei. 
Selama  ini,  BPS  lebih  mengandalkan  survei  daripada  sensus 
karena keunggulannya yaitu lebih efisien dari segi biaya, waktu, 
dan  sumber  daya  serta  memuat  informasi  yang  lebih  detail. 
Walaupun  memiliki  beberapa  keunggulan,  survei  pun  juga 
memiliki  kekurangan  apabila  ukuran  sampel  yang  digunakan 
tidak  cukup.  Ketika  ukuran  sampel  tidak  cukup  maka 
diperlukan  metode  untuk  meningkatkan  efektivitas  ukuran 
sampel. Hal ini yang menjadikan peran Small Area Estimation 
penting dalam mengatasi kekurangan survei ini. 

Salah  satu  data  yang  sering  mengaplikasikan  peran  SAE 
adalah  data  pengeluaran  perkapita.  Di  Indonesia,  data 
pengeluaran  perkapita  diperoleh  dari  Survei  Sosial  Ekonomi 

Nasional  (Susenas).  Susenas  dirancang  oleh  Badan  Pusat 
Statistik  (BPS)  untuk  menduga  parameter  populasi  pada 
wilayah  nasional  dan  regional  (provinsi,  kabupaten/kota)  dan 
tidak dirancang untuk menduga parameter populasi pada area 
kecil  seperti  kecamatan  atau  desa.  Selain  itu,  pendugaan 
parameternya  didasarkan  pada  rancangan  atau  dikatakan 
sebagai pendugaan langsung. Untuk pendugaan parameter pada 
wilayah  kecil,  umumnya  jumlah  sampel  kurang  mencukupi 
langsung  sehingga  akan 
untuk  melakukan  pendugaan 
menghasilkan presisi (sampling error) yang besar. Salah satu 
upaya  yang  dapat  dilakukan  untuk  memperoleh  pendugaan 
langsung  terhadap  pengeluaran  perkapita  rumah  tangga  pada 
area  kecil  dengan  presisi  yang  memadai  adalah  dengan 
menambah  jumlah  sampel  Susenas.  Akan  tetapi,  hal  ini  sulit 
dilakukan  karena  memerlukan  biaya  yang  besar.  Untuk 
menangani  permasalahan  ini,  data  Susenas  yang  ada  dapat 
dioptimalkan dengan melakukan Pendugaan Area Kecil (Small 
Area Estimation). 

Small  Area  Estimation  (SAE)  merupakan  suatu  teknik 
statistika  untuk  menduga  parameter-parameter  subpopulasi 
ini 
yang  ukuran  sampelnya  kecil.  Teknik  pendugaan 
memanfaatkan data dari domain besar (seperti data sensus, data 
susenas) untuk menduga variabel yang menjadi perhatian pada 
domain yang lebih kecil. Pendugaan sederhana area kecil yang 
didasarkan  pada  penerapan  model  desain  penarikan  contoh 
(design-based)  disebut  sebagai  pendugaan  langsung  (direct-
estimation).  Pendugaan  langsung  tidak  mampu  memberikan 
ketelitian yang cukup bila ukuran sampel dalam area kecil yang 
menjadi  perhatian  sedikit/berukuran  kecil,  sehingga  statistik 
yang dihasilkan akan memiliki varian yang besar atau bahkan 
pendugaan tidak dapat dilakukan karena tidak terwakili dalam 
survei [1].  

SAE  umumnya  menggunakan  pemodelan  parametrik 
untuk  menghubungkan  statistik  area  kecil  dengan  variabel-
variabel pendukungnya. Pemodelan ini kurang fleksibel dalam 
menyesuaikan dengan pola data hasil survei yang mungkin saja 
tidak  mirip  sama  sekali  dengan  distribusi  formal  yang  ada 
sehingga pendekatan nonparametrik menjadi pilihan alternatif 
[2].  Peneliti  akan  mencoba  menggunakan  pemodelan 
nonparametrik  untuk  mengatasi  masalah  ini  yaitu  dengan 
menggunakan metode Penalized Spline.  

Alasan dari pemilihan metode Penalized Spline ini karena 
pemodelannya  umum  digunakan  terutama  untuk  menerapkan  
pola-pola  data  tersembunyi  yang    mungkin  saja  tidak  terlihat 

 1 / 8 

 
 
 
 
yang 

umum 

adalah 

digunakan 

jika  menggunakan metode regresi yang lain. Lalu pada SAE, 
pendekatan 
dengan 
mengungkapkan  hubungan  antara  variabel  yang  diamati  dan 
variabel penyerta sebagai model linear yang dilengkapi dengan 
efek  acak  untuk  area  kecil.  Baik  model  estimasi  Penalized 
Spline  dan  SAE  dapat  dipandang  sebagai  random  effects 
models, maka wajar untuk mencoba menggabungkan keduanya 
ke  dalam  kerangka  SAE  nonparametrik  berdasarkan  regresi 
model campuran linier. 

II.  TUJUAN PENELITIAN 

Berdasarkan  rumusan  masalah, 

tujuan  dilakukannya 

penelitian ini adalah: 
1.  Memperoleh estimator parameter dari model SAE berbasis 
area menggunakan pendekatan parametrik EBLUP FH dan 
nonparametrik Penalized Spline. 

2.  Menduga nilai pengeluaran rata-rata perkapita kecamatan di 
Provinsi  DIY  berdasarkan  model  SAE  berbasis  area 
menggunakan  pendekatan  parametrik  EBLUP  FH  dan 
pendekatan nonparametrik Penalized Spline.  

3.  Memperoleh  nilai  estimasi  mean  squared  error  hasil 
pendugaan pengeluaran rata-rata perkapita di Provinsi DIY. 

III. PENELITIAN TERKAIT 

Berikut  merupakan  penelitian  terkait  yang  digunakan 

sebagai studi literatur pada penelitian ini. 

TABEL I 
TABEL LITERATUR 

No 

Judul 
1  Pemodelan 

Kemiskinan di 
Provinsi 
Bengkulu 
Menggunakan 
Small Area 
Estimation 
dengan 
Pendekatan 
Semiparametrik 
Penalized 
Spline 

Penulis, Publikasi  Tertulis 
I Sriliana, E 
Sunandi, U 
Rafflesia. Jurnal 
MIPA 40 (2) 
(2017): 134-140 

SAE umumnya 
menggunakan pemodelan 
parametrik untuk 
menghubungkan statistik 
area kecil dengan variabel-
variabel pendukungnya. 
Pemodelan ini kurang 
fleksibel dalam 
menyesuaikan dengan pola 
data hasil survei yang 
mungkin saja tidak mirip 
sama sekali dengan 
distribusi formal yang ada 
sehingga pendekatan 
nonparametrik menjadi 
alternatif pilihan[135] 

2  Theory for 
Penalized 
Spline 
regression 

3  Nonparametric 
Small Area 
Estimation 
Using 
Penalized 
Spline 
Regression 

Peter Hall, 
J.D.Opsomer. 
Biometrika 
(2005), 92, 1, pp. 
105-118 
J.D., 
Opsomer, 
G., 
Claeskens. 
Rannali,  M.G., 
Kauermann.  G., 
dan  Breidt.  F.J. 
Journal 
the 
Royal  Statistical 
Society,  Series  B 
70(2008).:265-286 

of 

Penalized Spline merupakan 
satu pendekatan smoothing 
yang popular karena 
kesederhanaannya dan 
fleksibilitasnya[105] 
meskipun regresi Penalized 
Spline paling sering dirujuk 
sebagai metode 
nonparametrik, regresi itu 
benar-benar mewakili kelas 
metode parametrik yang 
fleksibel berdasarkan model 
lnear. [266] 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

4  Prediksi 
Terbaik 
Empirik untuk 
Model 
Transformasi 
Logaritma di 
Dalam 
Pendugaan 
Area Kecil 
dengan 
Penerapan Pada 
Data Susenas 
5.  An Alternative 

Procedure to 
Produce a P-
Spline Small 
Area 
Estimation 
Model Based 
on Partial 
Residual Plot  
and 
Significance 
Test of Spline 
Term 

Anang Kurnia. 
Disertasi(2009):1-
127 

Taly Purwa, 
Journal of 
Physics: 
Conference 
Series(2021):1-10 

Pendugaan area kecil adalah 
suatu metode untuk 
menduga parameter pada 
suatu area yang relatif kecil 
dalam percontohan survei 
dengan memanfaatkan 
informasi dari luar area, dari 
dalam area itu sendiri, dan 
dari luar survei. [23] 

Dalam beberapa kasus, 
asumsi lnearitas tidak 
tidak selalu berlaku atau 
bentuk fungsional antara 
variabel penyerta dan 
variabel langsung tidak 
dapat ditentukan 
dan mengarah ke model 
SAE yang salah sehingga 
estimator yang dihasilkan 
akan bias. [1] 

Penelitian terkait tersebut menjadi dasar dalam penggunaan 
Penalized  Spline  sebagai  metode  untuk  mengestimasi  nilai 
dugaan dengan pendekatan SAE nonparametrik. 

A.  Landasan Teori  

IV. METODE PENELITIAN  

1.  Small Area Estimation  
Small  Area  Estimation  atau  Pendugaan  area  kecil  adalah 
suatu metode untuk menduga parameter pada suatu area yang 
relatif  kecil  dalam  percontohan  survei  dengan  memanfaatkan 
informasi  dari  luar  area,  dari dalam  area  itu  sendiri,  dan  dari 
luar survei [1]. 

 Terdapat  dua 

ide  utama  yang  digunakan  dalam 
mengembangkan metode SAE, yaitu: (1) model pengaruh tetap 
dengan asumsi bahwa keragaman di dalam area kecil peubah 
yang menjadi perhatian dapat diterangkan seluruhnya dengan 
hubungan  keragaman  yang  bersesuaian  pada 
informasi 
tambahan; (2) model pengaruh acak area kecil dengan asumsi 
keragaman  spesifik  area  kecil  tidak  dapat  diterangkan  oleh 
informasi  tambahan.  Pengembangan  metode  untuk  model 
pengaruh  acak  dapat  dimanfaatkan  untuk  mencapai  akurasi 
dalam  area  kecil.  Gabungan  dari  dua  asumsi 
tersebut 
membentuk  model  lnear  campuran.  Salah  satu  sifat  menarik 
dari  model  lnear  campuran  adalah  kemampuannya  dalam 
menduga  kombinasi  lnear  dari  pengaruh  tetap  dan  pengaruh 
acak. Persamaan model liner campuran adalah: 

𝐲 = 𝐗𝛃 + 𝐃𝐮 + 𝛆 

(1) 

Adapun model berbasis area ialah model yang didasarkan 
pada  ketersediaan  data  pendukung  terdapat  untuk  level  area 
tertentu.  Model  untuk  mendapatkan  penduga  langsung  ( 𝑦𝑖̂) 
dapat ditulis sebagai berikut 

𝐲̂𝐄𝐁𝐋𝐔𝐏 = 𝐗𝛃̂ + 𝐃𝐮̂ 

 2 / 8 

 
 
 
 
 
 
 
 
 
 
  
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

𝐲̂𝐄𝐁𝐋𝐔𝐏 = 𝐗𝛃̂ + 𝐃𝐆̂𝐮𝐃′𝐕̂ −𝟏 (𝐲 − 𝐗𝛃̂) 

dimana  𝜆𝛾  adalah  parameter  penalti  dan  diperlakukan 
sebagai  parameter  yang 
tidak  diketahui.  Sedangkan  𝛾 
𝑖 = 1,2, … . 𝑡                                                             
diperlakukan  sebagai  vektor  efek  random  dalam  model  lnear 
campuran.  Dalam notasi matriks, persamaan (8) dapat ditulis 
sebagai berikut 

(2) 

dimana 

𝛃̂ = (𝐗′𝐕̂−𝟏𝐗)−𝟏𝐗′𝐕̂ −𝟏𝐲 
𝐕̂ = 𝐃𝐆̂𝐮𝐃′ + 𝐆̂𝛆 

(3) 
(4) 

𝒚𝒊  adalah  penduga  EBLUP  Fay  Herriot,  𝑥𝑖  adalah 
variabel  penyerta,  𝛽  adalah  koefisien  regresi,  𝑑𝑖  adalah 
konstanta  efek  random, 𝑢𝑖 adalah  pengaruh  acak  area,  dan 𝜀𝑖 
adalah sampling error. 

2.  Regresi Penalized Spline 

Dalam regresi  spline  penting untuk memperhatikan titik-
titik knot data  yang nanti akan digunakan untuk memperoleh 
model matematis yang optimal. Namun hal ini membutuhkan 
waktu  lama  dan  jika  menggunakan  software  maka  akan 
memerlukan memori yang besar. Penggunaan Penalized Spline 
bisa mengatasi masalah ini karena knot terletak pada titik-titik 
kuantil  dari  nilai  variabel  prediktor.  Model  dari  Penalized 
Spline  menggunakan  fungsi  basis  polinomial 
tersegmen 
(truncated polynomial) yang dapat dilihat pada persamaan (5).  
Persamaan model sederhana menurut [3] 

𝑦𝑖 = 𝑚0(𝑥𝑖) + 𝜀𝑖 ; 𝑖 = 1, . , 𝑡 

(5) 

Model regresi pada area level dapat dideskripsikan sebagai 
berikut. Misalkan 𝑦 adalah variabel yang diamati  pada area ke-
2 )  dan   𝑚0(𝑥𝑖) berupa  fungsi  yang 
i  dimana 𝜀𝑖 iid ∼  N(0, 𝜎𝜀
belum  diketahui,  tapi  fungsi  ini  dapat  kita  estimasi  dengan 
menggunakan Penalized Spline dengan pendekatan cukup baik 
oleh persamaan (2), 

𝑚0(𝑥; 𝛽, 𝛾) = 𝛽0 + 𝛽1𝑥 + ⋯ + 𝛽𝑝𝑥𝑝 + ∑ 𝛾𝑙(

𝑝 , 
𝑥 − 𝑘𝑙)+

dengan fungsi truncated: 

𝑙=1

𝐿

(𝑥 − 𝑘𝑙)+

𝑝 = {

(𝑥 − 𝑘𝑙)𝑝   ;  𝑥 − 𝑘𝑙  ≥ 0 
0                       ; 𝑥 − 𝑘𝑙 < 0

(6) 

(7) 

 adalah 

p adalah derajat dari Spline, 𝑥 adalah nilai yang diketahui, 
{ 𝑘1, … . . , 𝑘𝐿}
knot, 𝛽 =
kumpulan 
{𝛽0, 𝛽1 , … , 𝛽𝑝}′  dan   𝛾 = (𝛾1, … , 𝛾𝑝)′  masing-masing  adalah 
koefisien  regresi  dari  parameter  dan  bagian  Penalized  Spline 
2 ), 
dari model. L adalah jumlah knot Spline dan  𝛾𝑙 𝑖𝑖𝑑 ∼ (0, 𝜎𝛾
(l=1,…,L)  dikenal  dengan  menyebarkan  knot  cukup  pada 
rentang 𝑥 dan L yang cukup luas.  

fixed 

Model Penalized Spline diperoleh dengan meminimumkan 
fungsi  penalized  least  square,  yaitu  fungsi  pendugaan  yang 
menggabungkan  antara  fungsi  least  square  dan  kemulusan 
kurva (smooth), berikut persamaannya  : 

𝑚

𝑃𝐿𝑆 = ∑{𝑦𝑖 − 𝑚(𝑥𝑖; 𝛽, 𝛾)}

2

+ 𝜆𝛾𝛾′𝛾

𝑖=1

𝑖 = 1,2, … . 𝑡   

(8) 

𝐏𝐋𝐒 = (𝐲 − 𝐗𝛃 − 𝐙𝛄)𝟐 + λγ𝛄′𝛄                       (9) 

Kemudian  dimisalkan  C  =  [X,  Z]  dan  𝛉 = [

persamaan (9) dapat ditulis sebagai berikut: 
𝐏𝐋𝐒 = (𝐲 − 𝐂𝛉)𝟐   +   λγ𝛉′𝐃𝛉    

𝛃
𝛄

] ,  maka 

(10) 

dengan D merupakan matriks penalti. Suku pertama pada 
persamaan (10) adalah jumlah kuadrat galat dan suku keduanya 
adalah  penalti.  Pada  persamaan  (10)  kita  juga  dapat  mencari 
nilai penduga Penalized Spline yaitu: 

𝛉̂ = (𝐂′𝐂 + λγ𝐃)

−𝟏

𝐂′𝐲                             (11) 

Adapun  untuk  bentuk  area  kecil  pada  model  linier 

campuran menggunakan Penalized Spline 

𝐘 = 𝐗𝛃 + 𝐙𝛄 + 𝐃𝐮 + 𝛆                                   (12) 

Didefinisikan 𝐘 = (𝐲𝟏, … , 𝐲𝐧)′, 𝐃 = (𝐝𝟏, … , 𝐝𝐧)′, 

𝐗 =

1 𝑥1
⋮
⋮
1 𝑥𝑛
[

𝑝
… 𝑥1
⋮
⋮
𝑝]
… 𝑥𝑛

  , 

(𝑥1 − 𝑘𝑙)+
⋮
⋮
(𝑥𝑛 − 𝑘𝑙)+
[

𝑝 …

𝑝 …

𝑝
… (𝑥1 − 𝑘𝐾)+

⋮
⋮

𝑝 ]
… (𝑥1 − 𝑘𝐾)+

 𝐙 =

dimana  

𝛄~(𝟎, 𝚺𝛄)     dengan 𝚺𝛄 = 𝛔𝛄
𝐮~(𝟎, 𝚺𝐮)     dengan 𝚺𝐮 = 𝛔𝐮
𝛆~(𝟎, 𝚺𝛆)     dengan 𝚺𝛆 = 𝛔𝛆

𝟐𝐈𝐊 
𝟐𝐈𝐓 
𝟐𝐈𝐧 
Oleh  karenanya,  jika  komponen  ragam  𝜎𝛾

2 
2 ,  dan  𝜎𝜀
tidak diketahui maka nilai penduga EBLUP dengan Penalized 
Spline menggunakan persamaan berikut, 

2 , 𝜎𝑢

𝐲̂𝐢 = 𝐱̅𝐢𝛃̂ + 𝐳̅𝐢𝛄̂ + 𝐞𝐢𝐮̂ 
Dimana  nilai  dari  tiap-tiap  estimasinya  didapat  dengan 

(13) 

persamaan berikut 

𝛃̂ = (𝐗`𝐕−𝟏𝐗)−𝟏𝐗`𝐕−𝟏𝐘 

𝛄̂ = 𝚺𝛄𝐙`𝐕−𝟏(𝐘 − 𝐗𝛃̂) 

𝐮̂ = 𝚺𝐮𝐃′𝐕−𝟏(𝐘 − 𝐗𝛃̂) 

(14) 

(15) 

(16) 

dan  V  adalah  varians  dari  y  dimana  𝐕 = 𝐙𝚺𝛄𝐙` +
𝐃𝚺𝐮𝐃′ + 𝚺𝛆  dimana 𝐙𝛄   dianggap  sebagai  random  efek  dan 
𝑫𝒖  dianggap sebagai random efek  area kecil. 

3.  Pemilihan Parameter Smoothing Optimal 

 3 / 8 

 
 
 
   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Nilai 𝛉̂ bergantung pada parameter smoothing 𝜆. Jika nilai 
terlalu  besar  akan  menghasilkan  bentuk  kurva  regresi  yang 
terlalu  kecil  akan 
sangat  halus.  Sebaliknya, 
menghasilkan bentuk kurva regresi yang sangat kasar sehingga 
perlu  dioptimalkan.  Untuk  mengoptimalkannya  dapat 
menggunakan  Generalized  Cross  Validation  (GCV)  yang 
didefinisikan sebagai berikut : 

jika  nilai 

𝐺𝐶𝑉 =

𝑀𝑆𝐸(𝜆)

(1−

1
𝑡𝑟(𝑺𝝀))
𝑛

2                

dengan 

𝑀𝑆𝐸(𝜆) =

1

𝑛

∑ (𝑦𝑖 − 𝑦̂𝑖

𝑛
𝑖=1

2)

(17) 

(18) 

𝐒𝛌 = 𝐂(𝐂′𝐂 + λ𝐃)−1C′                                          

(19) 

4.  Pemilihan Jumlah Knot 

Jumlah  knot  merupakan  banyaknya 

titik  knot  atau 
banyaknya titik dimana terjadi perubahan perilaku fungsi pada 
interval yang berlainan. Dalam Penalized Spline, knot terletak 
pada sampel kuantil dari nilai unik (tunggal) variabel penyerta. 
Penentuan  jumlah  knot  tidak  menjadi  terlalu  krusial  karena 
telah  dikontrol  oleh  parameter  𝜆  sehingga 
pemulusan 
penentuannya  dapat  mengikuti  sibjektivitas  peneliti  atau 
menggunakan metode fixed selection yaitu 

𝐾 = min (

1
4

× 𝑏𝑎𝑛𝑦𝑎𝑘𝑛𝑦𝑎 𝑣𝑎𝑟𝑖𝑎𝑏𝑒𝑙 𝑥 𝑦𝑎𝑛𝑔 𝑢𝑛𝑖𝑘, 35) 

(20) 

Penelitian ini adalah sebagai berikut:  

1.  Variabel respon  

Variabel  respon  yang  digunakan  dalam  penelitian  ini 
adalah  pengeluaran  perkapita.  Pengeluaran  perkapita 
diperoleh  dengan  cara  pengeluaran  rumah  tangga  dibagi 
dengan jumlah anggota rumah tangganya.  

2.  Variabel penyerta  

Variabel  penyerta  yang  digunakan  pada  penelitian  ini 
adalah kepadatan penduduk karena berdasarkan penelitian 
terdahulu[2]  diketahui    bahwa  variabel  yang  paling 
berpengaruh pada pengeluaran perkapita adalah kepadatan 
tidak 
penduduk.  Variabel 
didesiminasikan  pada  data  Podes  2018  sehingga  peneliti 
menggunakan publikasi dari BPS untuk mendapatkan nilai 
dari variabel Kepadatan penduduk dengan cara membagi 
jumlah penduduk perkecamatan dengan luas wilayah (km2) 
dari tiap-tiap kecamatan yang ada di Provinsi DIY.  

kepadatan 

penduduk 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

3.  Melakukan eksplorasi data dengan cara membuat statistik 
variabel 
variabel 

respon 

dan 

deskriptif 
untuk 
penyerta/prediktor. 

4.  Melihat korelasi antarvariabel X dan Y. 
5.  Membuat plot untuk variabel X dan Y untuk mengetahui 

persebaran data membentuk pola linier atau tidak. 
Selanjutnya  metode  dan  tahapan  penelitian  yang  akan 
dilakukan  untuk  mencapai  tujuan  penelitian  adalah  sebagai 
berikut: 
1.  Menduga  pengeluaran  perkapita  untuk  masing-masing 
kecamatan di Provinsi DIY menggunakan estimasi Regresi 
Penalized Spline dengan variabel penyerta yang digunakan 
adalah  kepadatan  penduduk  dengan  langkah-langkah 
diantaranya: 
i.  Menentukan orde (linier, kuadratik, atau kubik) dan 
titik knot yang menghasilkan GCV minimum.  
ii.  Menyatakan bentuk area kecil mengikuti suatu fungsi 
P-Spline  berbasis  polinomial  terputus.  𝐘 = 𝐗𝛃 +
𝐙𝛄 + 𝐃𝐮 + 𝛆                                   
iii.  Menganggap  parameter  𝐗𝛃  sebagai 

sebagai 
pengaruh 
tetap  (fixed  effect)  dan  𝐙𝛄  sebagai 
pengaruh  acak  dan  u  sebagai  pengaruh  acak  area 
kecil.  

iv.  Mengestimasi  parameter  regresi  dalam  kerangka 

kerja model linier campuran.  

v.  Mengestimasi  𝛽,  𝛾,  dan  𝑢  menggunakan  metode 

REML.  

vi.  Menduga pengeluaran perkapita rumah tangga untuk 

masing-masing kecamatan di Provinsi DIY. 
2.  Menghitung  nilai  mean  square  error  (MSE)  hasil 
pendugaan  pengeluaran  rata-rata  perkapita  menggunakan 
pendekatan  parametrik  bootstrap  untuk  masing-masing 
kecamatan  dengan  metode  EBLUP  FH  dan  Regresi 
Penalized Spline.  

3.  Menghitung  nilai  Relative  Standar  Error  (RSE)  untuk 
masing masing kecamatan dengan metode EBLUP FH dan 
Regresi Penalized Spline. 

4.  Membandingkan nilai estimasi, MSE, dan RSE dari ketiga 
metode yaitu estimasi langsung, EBLUP FH, dan Regresi 
Penalized Spline. 

C.  Diagram Alir Penelitian 

Diagram  alir  dalam  penelitian  ini  diperlihatkan  dalam 

gambar diagram alir berikut: 

B.  Metode Analisis Data 

Langkah 

analisis  model 

pendugaan 

area 

kecil 

menggunakan Regresi Penalized Spline 

Pre-Processing data yang akan diolah sebagai berikut: 
1.  Menyiapkan data pengeluaran rata-rata perkapita Provinsi 
Daerah  Istimewa  Yogyakarta  2019  dari  Susenas  (Y) dan  
data  kepadatan  penduduk  dari  Podes  2018  publikasi 
Yogyakarta dalam Angka (X). 

2.  Menggabungkan kedua data menjadi satu set data. 

Gambar 1. Diagram alir penelitian  

V.  KERANGKA PIKIR  

Penelitian  tentang  SAE  telah  banyak  dilakukan  untuk 
berbagai  macam  kasus.  Fokus  pada  penelitian  ini  adalah 

 4 / 8 

 
 
 
 
                                  
 
 
 
 
 
 
 
 
 
perbandingan nilai yang dihasilkan oleh penduga EBLUP Fay 
Herriot  dan  EBLUP  Penalized  Spline.  Awalnya,  data  yang 
digunakan  adalah  data  sampel  dimana  dilakukan  estimasi 
langsung  sehingga  diperoleh  hasil  estimasi,  MSE,  dan  RSE. 
Namun  estimasi  wilayah  yang 
lebih  kecil  berpotensi 
mengalami kekurangan data dan menyebabkan error yang besar. 
Data dari variabel penyerta menjadi pilihan untuk melakukan 
estimasi  tidak  langsung  melalui  SAE  yang  menjadi  solusi 
keterbatasan  data  dengan  meminjam  informasi  dari  wilayah 
yang  bersesuaian.  Estimasi  tidak  langsung  ini  bisa  didekati 
dengan  SAE  pendekatan  parametrik  dan  SAE  pendekatan 
nonparametrik.  Salah  satu  metode  yang  digunakan  SAE 
pendekatan  parametrik  untuk  menduga  nilai  estimasi  ialah 
metode  EBLUP  Fay  Herriot.  Adapun  untuk  pendekatan 
nonparametrik, peneliti menggunakan metode Penalized Spline. 
Penalized Spline merupakan satu pendekatan smoothing yang 
populer  karena  kesederhanaannya  dan  fleksibilitasnya[4]. 
Penalized  Spline  juga  mempunyai  kelebihan  dibandingkan 
dengan  metode  spline  lainnya  yaitu  dapat  mengatasi  model 
yang  overfitting  jika  jumlah  knot  yang  digunakan  terlampau 
banyak dengan menambahkan penalti/kendala pada parameter 
spline  dengan  tujuan  untuk  menghindari  kelebihan  knot  [4]. 
Dari kedua estimasi tidak langsung ini akan dibandingkan hasil 
estimasi  mana  yang  lebih  baik  pada  data.  Berikut  kerangka 
pikir penelitian yang disajikan pada Gambar 2. 

Gambar 2. Kerangka Pikir Penelitian 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

VI. HASIL DAN PEMBAHASAN 

Berdasarkan  hasil  yang  didapat  dari  data  Susenas  tahun 
2019,  data  estimasi  langsung  dari  rata-rata  pengeluaran 
perkapita di Provinsi DIY meliputi data yang tersedia pada 77 
dari  78  kecamatan.  Hal  ini  dikarenakan  ada  satu  kecamatan 
yang  tidak  ada  yaitu  kecamatan  Pajang.  Oleh  karenanya 
estimasi  tidak  langsung  yang  dilakukan  menggunakan  SAE 
EBLUP Fay Herriot dan Regresi Penalized Spline dengan 77 
kecamatan  

1.  Eksplorasi Data 

Ringkasan hasil pendugaan pengeluaran per kapita disajikan 

pada Tabel II. 

TABEL II 
TABEL RINGKASAN RATA-RATA PENGELUARAN PERKAPITA DAN KEPADATAN 
PENDUDUK 
1st Qu.  Median 
1018452 
792652 

Min. 
533162 

3rd Qu. 
1565310 

Max 
2686485 

Mean 
11843
91 

Rata-rata 
Pengeluaran 
Perkapita 

Kepadatan 
Penduduk 

278,2 

701,5 

1389,1 

3887,9 

3125,2 

22703,7 

Rata-rata  pengeluaran  perkapita  tertinggi  tahun  2019  di 
Provinsi DIY terdapat pada kecamatan Mlati yaitu sebesar Rp 
2.686.485 dan untuk rata-rata pengeluaran per kapita terendah 
terdapat pada kecamatan  Purwosari yaitu sebesar Rp533.162. 
Adapun kepadatan penduduk tertinggi terletak pada kecamatan 
Girisubo  dan  kepadatan  penduduk  terendah  pada  kecamatan 
Ngampilan. 

Kemudian  akan  disajikan  scatterplot  antara  rata-rata 
pengeluaran perkapita dengan kepadatan penduduk di Provinsi 
DIY tahun 2019 pada Gambar 3. 

Gambar 3. Scatterplot Rata-rata Pengeluaran Perkapita dan 
 Kepadatan Penduduk 

Ditunjukkan 

scatterplot  antara 

rata-rata  pengeluaran 
perkapita  (y)  dengan  kepadatan  penduduk  (𝑥).  Pada  gambar, 
pola hubungan yang terbentuk tidak mengikuti pola kelinieran 
atau mempunyai pola data tertentu dimana perilaku pola data 
ini dapat berubah-ubah. 

 5 / 8 

 
 
 
 
 
 
 
 
 
 
  
 
 
 
2.  Statistik Deskriptif 

Pada tabel dibawah ditunjukkan ringkasan statistik deskriptif 
perbandingan hasil pendugaan menggunakan estimasi langsung, 
EBLUP  Fay  Herriot,  dan  Regresi  Penalized  Spline  yang 
melibatkan 77 kecamatan. 

TABEL III 
TABEL RINGKASAN HASIL ESTIMASI  

Min. 

533.2 

1st 
Qu. 
792.7 

Median  Mean 

3rd Qu.  Max 

1018.5 

1184.4 

1565.3 

2686.5 

534.6 

801.4 

1016.6 

1133.8 

1433.9 

2671.5 

593.5 

858.7 

989.0 

1184.4 

1615.4 

2386.0 

Estimasi 
Langsung 

EBLUP Fay 
Herriot 

Regresi 
Penalized 
Spline 

Lalu  berikutnya  ditampilkan  pula  ringkasan  statistik 
deskriptif perbandingan MSE dan RSE menggunakan estimasi 
langsung, EBLUP Fay Herriot, dan Regresi Penalized Spline. 

TABEL IV 
TABEL RINGKASAN HASIL MSE 
Median  Mean 

1st 
Qu. 

4550 

11557 

20253 

3rd 
Qu. 
28465 

Max 

316365 

4411 

10700 

20127 

23742 

95721 

Min. 

0 

0 

18204 

23035 

24801 

25028 

27142 

32678 

TABEL V 
TABEL RINGKASAN HASIL RSE 
Median  Mean 

Min. 

3rd Qu.  Max 

10.413 

11.698 

16.611 

35.554 

1st 
Qu. 
6.754 

0 

0 

2.127 

3.261 

3.503 

4.505 

8.886 

3.054 

3.981 

4.832 

5.287 

1615.4 

7.018 

Estimasi 
Langsung 

EBLUP Fay 
Herriot 

Regresi 
Penalized 
Spline 

Estimasi 
Langsung 

EBLUP Fay 
Herriot 

Regresi 
Penalized 
Spline 

3.  Orde dan Knot berdasarkan Nilai GCV Optimum 

Penentuan dari orde dan knot terbaik dilihat dari nilai GCV 
optimum  (menghasilkan  nilai  yang  paling  kecil).  Berikut 
adalah  sepuluh  kombinasi  orde  dan  knot  yang  menghasilkan 
nilai GCV terkecil. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

TABEL VI 
TABEL ORDE, KNOT, DAN GCV OPTIMUM 

Orde 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 

Jumlah Knot 
12 
16 
8 
19 
17 
13 
18 
15 
14 
9 
15 

GCV 
104807.5 
105725.5 
106386.0 
108260.9 
108713.4 
109615.7 
115930.4 
121450.4 
122219.7 
123550.5 
121450.4 

Dari  tabel  diatas  didapat  kombinasi  nilai  orde  dan  knot 
terbaik yaitu dengan orde 1 (spline linier) dan jumlah knot 12. 
Adapun  nilai-nilai  pada  lokasi  knotnya  yaitu  412.8844,   
523.7268,  638.1271,  811.2231,  978.6651,  1274.5399, 
1588.6258,  1978.6218,  2415.8043,  3237.9740,  11029.6698, 
dan 15728.4478. 

4.  Nilai Estimasi Parameter dan Model yang Regresi P-Spline 
Setelah Jumlah dan lokasi knot diperoleh selanjutnya adalah 
mengestimasi  parameter  β  dengan  memaksimumkan  fungsi 
likelihood atau log likelihood-nya dan mencari prediktor 𝛄̂ dan 
𝒖̂ yang  merupakan  EBLUP  (Empirical  Best  Linier  Unbiased 
Predictors)  dari  𝛄  dan  u.  sebagai  pengaruh  acak.  berikut 
disajikan  nilai  estimasi  β  untuk  model  P-Spline  terbaik (orde 
satu) 

TABEL VII 
TABEL  ESTIMASI EFEK TETAP PADA REGRESI P-SPLINE 
Estimasi 
622.7532 
0.3419 

Parameter 
𝛽0 
𝛽1 

Sehingga diperoleh estimasi model sebagai berikut: 

𝐘 = 622.7532 + 0.3419𝐗 + 𝛄𝟏(𝐗 − 412.8844) +

𝛄𝟐(𝐗 − 523.7268) + 𝛄𝟑(𝐗 − 638.1271) + 𝛄𝟒(𝐗 −
811.2231) + 𝛄𝟓(𝐗 − 978.6651) + 𝛄𝟔(𝐗 −
1274.5399) + 𝛄𝟕(𝐗 − 1588.6258) + 𝛄𝟖(𝐗 −
1978.6218) + 𝛄𝟗(𝐗 − 2415.8043) + 𝛄𝟏𝟎(𝐗 −
3237.9740) + 𝛄𝟏𝟏(𝐗 − 11029.6698) + 𝛄𝟏𝟐(𝐗 −
15728.4478) + u   

Berdasarkan model P-Spline terbaik diatas, maka: 

a.  Setiap kenaikan satu satuan x untuk x < 412.8844 akan 

merubah nilai y sebesar 0.3419 satuan. 

b.  Untuk  nilai  x ≤ 412.8844,  setiap  kenaikan  satu  satuan 
akan berpengaruh sebesar (0.3419 + 𝛄𝟏) satuan terhadap 
y. 

c.  Untuk nilai 412.8844 < x ≤ 523.7268, setiap kenaikan 
satu  satuan  x  akan  berpengaruh  sebesar (0.3419 + 𝛄𝟏 ) 
satuan terhadap y. 

d.  Untuk nilai 523.7268 < x ≤ 638.1271, setiap kenaikan 
satu satuan x akan  berpengaruh sebesar (0.3419 + 𝛄𝟏 +
𝛄𝟐) satuan terhadap y. 

 6 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

e.  Untuk nilai 638.1271 < x ≤ 811.2231, setiap kenaikan 
satu satuan x akan  berpengaruh sebesar (0.3419 + 𝛄𝟏 +
𝛄𝟐 + 𝛄𝟑) satuan terhadap y. 

f.  Untuk nilai 811.2231 < x ≤ 978.6651, setiap kenaikan 
satu satuan x akan  berpengaruh sebesar (0.3419 + 𝛄𝟏 +
𝛄𝟐 + 𝛄𝟑 + 𝛄𝟒) satuan terhadap y. 

g.  Untuk nilai 978.6651 < x ≤ 1274.539, setiap kenaikan 
satu satuan x akan  berpengaruh sebesar (0.3419 + 𝛄𝟏 +
𝛄𝟐 + 𝛄𝟑 + 𝛄𝟒 + 𝛄𝟓) satuan terhadap y. 

h.  Untuk  nilai  1274.5399 < x ≤ 1588.625 , 

setiap 
kenaikan satu satuan x akan berpengaruh sebesar (0.3419 
+ 𝛄𝟏 + 𝛄𝟐 + 𝛄𝟑 + 𝛄𝟒 + 𝛄𝟓 + 𝛄𝟔) satuan terhadap y. 

i.  Untuk  nilai  1588.6258 < x ≤ 1978.6218 , 

j.  Untuk  nilai  1978.6218 < x ≤ 2415.8043 , 

setiap 
kenaikan satu satuan x akan berpengaruh sebesar (0.3419 
+ 𝛄𝟏 + 𝛄𝟐 + 𝛄𝟑 + 𝛄𝟒 + 𝛄𝟓 + 𝛄𝟔 + 𝛄𝟕) satuan terhadap y. 
setiap 
kenaikan satu satuan x akan berpengaruh sebesar (0.3419 
+  𝛄𝟏 + 𝛄𝟐 + 𝛄𝟑 + 𝛄𝟒 + 𝛄𝟓 + 𝛄𝟔 + 𝛄𝟕 + 𝛄𝟖 ) 
satuan 
terhadap y. 

k.  Untuk  nilai  2415.8043 < x ≤ 3237.9740 , 

setiap 
kenaikan satu satuan x akan berpengaruh sebesar (0.3419 
+ 𝛄𝟏 + 𝛄𝟐 + 𝛄𝟑 + 𝛄𝟒 + 𝛄𝟓 + 𝛄𝟔 + 𝛄𝟕 + 𝛄𝟖 + 𝛄𝟗)  satuan 
terhadap y. 

l.  Untuk  nilai  3237.9740 < x ≤ 11029.6698 ,  setiap 
kenaikan satu satuan x akan berpengaruh sebesar (0.3419 
+  𝛄𝟏 + 𝛄𝟐 + 𝛄𝟑 + 𝛄𝟒 + 𝛄𝟓 + 𝛄𝟔 + 𝛄𝟕 + 𝛄𝟖 + 𝛄𝟗 + 𝛄𝟏𝟎 ) 
satuan terhadap y. 

m.  Untuk  nilai  11029.6698 < x ≤ 15728.4478 ,  setiap 
kenaikan satu satuan x akan berpengaruh sebesar (0.3419 
+ 
𝛄𝟏 + 𝛄𝟐 + 𝛄𝟑 + 𝛄𝟒 + 𝛄𝟓 + 𝛄𝟔 + 𝛄𝟕 + 𝛄𝟖 + 𝛄𝟗 +
𝛄𝟏𝟎 + 𝛄𝟏𝟏) satuan terhadap y. 

n.  Untuk nilai x > 15728.4478, setiap kenaikan satu satuan 
akan  x  berpengaruh  sebesar (0.3419 + 𝛄𝟏 + 𝛄𝟐 + 𝛄𝟑 +
𝛄𝟒 + 𝛄𝟓 + 𝛄𝟔 + 𝛄𝟕 + 𝛄𝟖 + 𝛄𝟗 + 𝛄𝟏𝟎 + 𝛄𝟏𝟏+𝛄𝟏𝟐)  satuan 
terhadap y. 

5.  Perbandingan  Hasil  Estimasi  Langsung,  EBLUP  Fay 

Herriot, dan Regresi P-Spline 
Hasil  estimasi  dari  rata-rata  pengeluaran  perkapita  di 
Provinsi  DIY  disajikan  pada  gambar  (4).  Hasil  estimasi 
langsung didapatkan dari data Susenas. Hasil estimasi dengan 
menggunakan  EBLUP  FH  diperoleh  dengan  menggunakan 
function ‘msefh’ dari package ‘sae’. Lalu untuk estimasi dari 
Regresi Penalized Spline didapat dari pembuatan function di R  

Gambar 4. Perbandingan Hasil Estimasi Langsung dan  
EBLUP Fay Herriot 

Pada  Gambar  4.  terlihat  bahwa  hasil  estimasi  langsung, 
EBLUP  Fay  Herriot,  dan  Regresi  Penalized  Spline  memiliki 
pola yang hampir sama dimana tidak menunjukkan perbedaan 
yang jauh dalam estimasi rata-rata pengeluaran perkapita pada 
ketiga metode yang digunakan. 

6.  Perbandingan  MSE  dan  RSE  Estimasi  Langsung,  EBLUP 

Fay Herriot, dan Regresi P-Spline 
Seberapa baik hasil estimasi dapat dilihat dari perbandingan 
nilai  MSE  dan  RSE.  Berikut  merupakan  MSE  dari  rata-rata 
pengeluaran perkapita di Provinsi DIY 

Gambar 5. Perbandingan MSE Estimasi Langsung dan EBLUP Fay Herriot 

Pada Gambar 5. terlihat bahwa MSE EBLUP Fay Herriot 
dan  MSE  Penalized  Spline  lebih  rendah  daripada  MSE  hasil 
estimasi  langsung  yang  menunjukkan  bahwa  nilai  kedua 
penduga  tidak  langsung  tersebut  lebih  presisi  daripada  MSE 
estimasi 
langsung.  Adapun  pada  nilai  MSE  dengan 
menggunakan  Regresi  Penalized  Spline  menunjukkan  kurva 
yang  lebih  mulus  daripada  MSE  dari  estimasi  langsung  dan 
EBLUP FH sehingga nilai yang dihasilkan pun lebih konsisten. 

 7 / 8 

 
 
 
 
 
 
 
 
 
 
Adapun  RSE  dari  rata-rata  pengeluaran  perkapita  di 

3.  Untuk mendapatkan nilai MSE dapat dengan pendekatan 

Provinsi DIY dapat dilihat pada Gambar 6. 

lain seperti MSE dengan metode Jacknife. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

DAFTAR PUSTAKA 
[1]  A. Kurnia. Prediksi Terbaik Empirik untuk Model Transformasi Logaritma 
di Dalam Pendugaan Area Kecil Dengan Penerapan Pada Data Susenas. 
Bogor: IPB, 2009. 

[2]  F. Apriani. Pemodelan Pengeluaran Per Kapita Menggunakan Small Area 
Estimation  dengan  Pendekatan  Semiparametrik  Penalized  Spline. 
Surabaya: Institut Teknologi Sepuluh Nopember, 2017. 

[3]  D. Ruppert, M.  P.  Wand, and  R.  J. Carroll.  Semiparametric Regression. 

Cambridge: Cambridge University Press, 2003 

[4]  J.D. Opsomer, G. Claeskens, M.G. Rannali, G. Kauermann, dan F.J Breidt. 
Nonparametric Small Area Estimation Using Penalized Spline Regression. 
Journal of the Royal Statistical Society, Series B 70, 2008, pp. 265-286 
[5]  Z.W. Baskara. Pendugaan Area Kecil Menggunakan Pendekatan Penalized 

Spline. Surabaya: Institut Teknologi Sepuluh Nopember, 201. 

[6]  Darsyah,  Moh.Yamin,  and  Rochdi  Wasono.  Pendugaan  Tingkat 
Kemiskinan  di  Kabupaten  Sumenep  Dengan  Pendekatan  SAE.  Jurnal 
Statistik, 2013, pp. 28-36 

[7]  Eubank,  L.  Randall,  Nonparametric  Regression  and  Spline  Smoothing. 

New York: Marcel Dekker, Inc. 1999. 

[8]  C. Giusti, M. Pratesi, dan N. Salvati. A Semiparametric Fay-Herriot model 
using  Penalized  Spline.  Journal  of  The  Indian  Society  Of  Agricultural 
Statistics, 2012. 

[9]  P.  Hall,  and  J.D.  Opsomer.  Theory  for  Penalized  Spline  regression. 

Biomedika, 2005 

[10] A.M.  Kahar.  Pendugaan  Pengeluaran  Per  Kapita  Rumah  Tangga  di 
Kabupaten  Halmahera  Selatan  Menggunakan  Unit  Level  Small  Area 
Estimation dengan Metode Empirical Best Linier Unbiased Predictor Dan 
Penalized  Spline  Regression.  Surabaya:  Institut  Teknologi  Sepuluh 
Nopember, 2019. 

[11] J.  N.  K.  Rao  and  I.  Molina,  Small  Area  Estimation  2nd  Edition.  New 

Jersey:John Wiley and Sons Inc, 2015. 

[12] D.  Ruppert.  Selecting  the  number  of  knots  for  Penalized  Splines.  J. 

Computnl Graph. Statist, pp. 735-757, 2002 

[13] P.  Taly.  An  Alternative  Procedure  to  Produce  a  P-Spline  Small  Area 
Estimation Model on Based Partial Residual Plot and Significance Tet of 
Spline Term. Journal of Physics Conference Series, March 2021. 

[14] D.  Ruppert,  M.P.  Wand,  dan  R.J  Carrol.  Semiparametric  Regression, 
Cambridge  University  Press,  New  York:  Cambridge  University  Press, 
2003.  

[15] I. Sriliana, D. Agustina & E. Sunandi. Pemetaan Kemiskinan di Kabupaten 
Mukomuko  Menggunakan  Small  Area  Estimation  dengan  Pendekatan 
Regresi  Penalized  Spline.  Jurnal  Matematika  Integratif  12(2):  125-133, 
2016.  

[16] F  Yao,  dan  T.C.M.  Lee.  On  Knot  Placement  for  Penalized  Spline 
Regression. Journal of the Korean Statistical Society. No. 37, pp. 259-267, 
2008. 

Gambar 6. Perbandingan RSE Estimasi Langsung dan EBLUP Fay Herriot 

Pada Gambar 6. nilai RSE EBLUP Fay Herriot dan RSE 
Penalized Spline lebih rendah daripada RSE estimasi langsung 
yang  menunjukkan  RSE  kedua  penduga  tidak  langsung  lebih 
akurat  daripada  RSE  estimasi  langsung.  Dilihat  pula  pada 
grafik bahwa nilai RSE sudah dinilai cukup baik karena kedua 
kurva baik RSE EBLUP maupun Penalized Spline berada pada 
rentang nilai dibawah 25%. 

VII. 

PENUTUP 

Kesimpulan  yang  didapat  berdasarkan  penelitian  ini 

diantaranya : 
1.  Hasil dari penelitian ini adalah sudah didapatkan dugaan 
dengan  estimasi  langsung  dan  estimasi  tidak  langsung 
menggunakan  SAE  parametrik  metode  EBLUP  Fay 
Herriot dan SAE nonparametrik Penalized Spline.  

2.  Kedua  penduga  tidak  langsung  memberikan  hasil  yang 
lebih  baik  pada  pendugaan  rata-rata  perkapita  tiap 
kecamatan  di  Provinsi  DIY  daripada  penduga  langsung. 
Oleh  karena  itu,  kedepannya  Regresi  Penalized  Spline 
dapat  digunakan 
sebagai  pilihan  alternatif  untuk 
mengestimasi  nilai  pada  area  kecil  terutama  untuk  data 
yang berpola non-linier 

3.  MSE  yang  didapatkan  dari  penerapan  Regresi  Penalized 
Spline menghasilkan nilai yang lebih smooth dibandingkan 
MSE  yang  dihasilkan  oleh  penduga  langsung  ataupun 
EBLUP FH 
Adapun  saran  yang  dapat  diberikan  untuk  penelitian  ini 

yaitu: 
1.  Penelitian  selanjutnya  dapat  menggunakan  metode 
Penalized  Spline  dengan  variabel  bantu  lebih  dari  satu 
(multivariabel). 

2.  Dapat  menggunakan  menggunakan  basis  fungsi  spline 
yang lebih konsisten untuk data numerik   seperti B-Spline 
ataupun  menggunakan  basis  radial  untuk  menduga  nilai 
area kecil pada data spasial 

 8 / 8 

 
 
 
 
 
"
221709724,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Implementasi dan Evaluasi Visualisasi Data 
Interaktif Pada Publikasi Laporan Bulanan Data 
Sosial Ekonomi Indonesia 

Hafidz Isa Nasruddin Lizana (221709724, 4SD1) 
Dosen Pembimbing: Farid Ridho, S.S.T., M.T. 

Ringkasan—  Setiap  bulan  (Badan  Pusat  Statistik)  BPS 
mempublikasikan  buku  LBDSE  Indonesia,  dimana  di  dalam 
publikasi tersebut berisi perkembangan dari 18 topik data beserta 
visualisasinya.  Namun,  visualisasi  data  pada  publikasi  LBDSE 
tidak lengkap dan hanya bersifat statis, sedangkan data yang ada 
memiliki  dimensi/variabel  yang  cukup  besar.  Sehingga  peneliti 
berfikir  untuk  mengimplementasikan  visualisasi  data  interaktif 
publikasi LBDSE pada SIVIS LBDSE agar memudahkan dalam 
memahami  data-data  nya.  Untuk  menilai  keberhasilan  tujuan 
pengimplementasian  tersebut,  peneliti  akan  melakukan  evaluasi 
terhadap  visualisasi  data  pada  SIVIS  LBDSE  berdasarkan 
penilaian  pengguna  menggunakan  8  indikator  penilaian  yang 
diadopsi  dari  penelitian  terdahulu.  Tujuan  penelitian  ini  yaitu 
untuk mengimplementasikan visualisasi data interaktif pada data 
LBDSE, mengevaluasi visualisasi data pada SIVIS LBDSE, serta 
mengetahui  visualisasi  data  terbaik  terhadap  data  kategorik 
publikasi LBDSE. Dengan bantuan 50 responden, mereka menilai 
bahwa visualisasi data pada SIVIS LBDSE memiliki keunggulan 
dan  kekurangannya  masing-masing  berdasarkan  8  indikator 
penilaian yang digunakan. Namun secara keseluruhan responden 
mengurutkan visualisasi data terbaik untuk data kategorik secara 
berturut-turut yaitu, Bar Chart, Tree Map, Bubble Map, dan Pie 
Chart. 

Kata  Kunci—  Implementasi,  Evaluasi,  Visualisasi  Data 

Interaktif, SIVIS LBDSE, Indikator Penilaian. 

I.  LATAR BELAKANG 

Setiap  bulan  Badan  Pusat  Statistik  (BPS)  menerbitkan 
Publikasi  Laporan  Bulanan  Data  Sosial  Ekonomi  (LBDSE) 
Indonesia.  Publikasi  ini  memuat  perkembangan  data  terbaru 
yang dihimpun dan dirilis oleh BPS sekaligus visualisasi dari 
data-data  tersebut.  Ada 18 topik yang dibahas pada publikasi 
tersebut, dan setiap topik memiliki lebih dari satu data berupa 
tabel. Sayangnya tidak semua data yang ada pada publikasi ini 
memiliki visualisasi, masih banyak data yang tidak dilakukan 
visualisasi data. Selain itu, data-data yang ada pada publikasi 
satu 
ini  memiliki  variabel  atau  dimensi 
(multidimensi).  Sehingga  visualisasi  data  yang  ditampilkan 
juga  terbatas  secara  visual.  Visualisasi  data  yang  ada  pada 
publikasi  berupa  buku  maupun  buku  elektronik,  seperti  pada 
publikasi  ini,  hanya  dapat  ditampilkan  secara  statis.  Artinya 
pengguna tidak dapat melakukan interaksi terhadap visualisasi 
data yang ada.  

lebih  dari 

Dari beberapa permasalahan tersebut maka dibutuhkan suatu 
teknik  analisis  data  berupa  visualisasi  data  interaktif  pada 
publikasi  LBDSE  yang  dapat  menampilkan  berbagai  macam 
data dalam bentuk visualisasi. Seperti yang diungkapkan oleh 
Vitaly  Friedman  [13]  dalam  bukunya  bahwa  ""Tujuan  utama 
dari 
untuk 
adalah 
memvisualisasikan data, mengkomunikasikan informasi secara 
jelas  dan  efektif.”  Melalui  penelitian  ini  juga  diharapkan 

kemampuannya 

visualisasi 

data 

visualisasi data interaktif yang dihasilkan dapat memudahkan 
pengguna dalam memahami data-data LBDSE.  

Banyak  pendapat  yang  mengatakan  bahwa  melalui 
visualisasi data, pengguna dapat memahami suatu data dengan 
mudah.  Namun  tidak  semua  bentuk  visualisasi  data  berhasil 
membantu  pengguna  dalam  memahami  suatu  data.  Edward 
Tufte  [1]  menjelaskan  bahwa,  ""Keberhasilan  dalam  grafik 
statistik  terdiri  dari  mengkomunikasikan  ide  yang  kompleks 
dengan  kejelasan,  ketepatan  dan  efisiensi.”  Oleh  karena  itu, 
dibutuhkan  sebuah  evaluasi  untuk  menilai  apakah  suatu 
visualisasi  berhasil  menyampaikan  maksud  dari  data  tersebut 
atau tidak. 

Berdasarkan  uraian  di  atas,  maka  pada  skripsi  ini  peneliti 
akan mengimplementasikan visualisasi data interaktif publikasi 
LBDSE  pada  SIVIS  LDBSE  (Sistem  Visualisasi  Laporan 
Bulanan Data Sosial Ekonomi). Tidak hanya itu, pada skripsi 
ini peneliti juga akan melakukan evaluasi terhadap visualisasi 
data tersebut untuk menilai apakah tujuan pengimplementasian 
nya dapat tercapai atau tidak.  

II.  TUJUAN PENELITIAN 

Adapun tujuan dilakukannya penelitian ini yaitu: 

1.  Untuk  mengimplementasikan  visualisasi  data  interaktif 
pada  publikasi  Laporan  Bulanan  Data  Sosial  Ekonomi 
(LBDSE). 

2.  Untuk mengetahui penilaian pengguna terhadap visualisasi 
data interaktif pada SIVIS LBDSE berdasarkan beberapa 
kelompok penilaian. 
3.  Untuk  mengetahui 

penilaian 
perbedaan 
visualisasi data interaktif pada SIVIS LBDSE antara kedua 
kelompok responden. 

persepsi 

4.  Untuk mengetahui jenis visualisasi data interaktif terbaik 
terhadap  data  kategorik  pada  publikasi  Laporan  Bulanan 
Data Sosial Ekonomi (LBDSE). 

III. PENELITIAN TERKAIT 

Evaluation of Opinion Visualization Techniques 

Azra Shamim, Vimala Balakrishnan, dan Muhammad Tahir 
pada tahun 2014 juga melakukan penelitian mengenai evaluasi 
sistem  visualisasi  data  opini.  Pada  penelitian  ini,  mereka 
menggunakan beberapa matriks penilaian untuk mengevaluasi 
sistem visualisasi data  opini.  Matrik penilaian tersebut antara 
lain:  eye  pleasing,  easy 
to  understand,  user-friendly, 
usefulness, 
intuitive 
informative 
comprehensiveness,  comparison  ability,  representation  style, 
dan  pre-knowledge  required.  Tujuan  dari  penelitian  tersebut 
yaitu  menentukan  peringkat 
sistem  visualisasi  opini, 
menyelidiki  perbedaan  persepsi  antara  kedua  grup  responden 
seminar  dan  kuesioner  online),  dan  untuk 
(peserta 

design, 

design, 

 1 / 8 

 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

mengidentifikasi  matriks  visualisasi  yang  penting.  Evaluasi 
dilakukan  oleh  146  responden  yang  terdiri  dari  110  peserta 
seminar  dan  36  responden  yang  mengisi  kuesioner  online. 
Seluruh  responden  tersebut  diminta  mengisi  kuesioner  untuk 
menilai 11 visualisasi data  terpilih. Hasil yang diperoleh dari 
kuesioner  tersebut  kemudian  akan  dianalisis  menggunakan 
independent  sample  t-test  untuk 
statistik  deskriptif  dan 
menentukan  peringkat  dan  menyelidiki  perbedaan  persepsi 
antara kedua grup responden. 

IV. METODE PENELITIAN  

A. Ruang Lingkup Penelitian 

Pada  evaluasi  SIVIS  LBDSE  dalam  penelitian 

ini, 
digunakan  beberapa  indikator  penilaian  yang  diadopsi  dari 
penelitian  oleh  Azra  Shamim,  Vimala  Balakrishnan,  dan 
Muhammad Tahir pada tahun 2014. Berikut beberapa indikator 
penilaian yang digunakan dalam penelitian ini. 

dengan rentang skor 1 untuk Sangat Tidak Setuju sampai skor 
5 untuk Sangat Setuju.  

Untuk  menguji  keabsahan  kuesioner  yang  digunakan, 
dilakukan  uji  validitas  dan  reliabilitas  terhadap  8  indikator 
penilaian pada kuesioner Blok Evaluasi dengan menggunakan 
hasil  pengisian  kuesioner  oleh  15  responden  awal  dari  50 
responden  keseluruhan.  Hasilnya  menjelaskan  bahwa 
instrumen  penelitian  berupa  8  indikator  penilaian  yang 
digunakan  pada  evaluasi  SIVIS  LBDSE  valid  dan  sangat 
reliabel.  

C. Metode Analisis 

Pada  penelitian  ini,  dilakukan  analisis  deskriptif  untuk 
menganalisis  penilaian  responden  terhadap  SIVIS  LBDSE 
berdasarkan  kuesioner  yang  telah  dibagikan.  Hasil  skor 
penilaian SIVIS LBDSE berdasarkan 8 indikator dan 6 grafik 
visualisasi data akan dianalisis dan ditampilkan dalam bentuk 
tabel. 

TABEL I 
INDIKATOR PENILAIAN 

Kelompok Penilaian 

Indikator 

Dampak Visual 
Performa Secara Keseluruhan  Kemudahan Pemahaman, Kemudahan 

Daya Tarik Visual 

Desain Secara Keseluruhan 
Kualitas Informasi 
Model Representasi Visual 

Penggunaan, Interaktivitas 
Desain Informatif 
Kegunaan, Kelengkapan 
Kemampuan Perbandingan 

Penelitian  dilakukan  sejak  bulan  November  2020  sampai 
Januari  2021  dengan  mengimplementasikan  visualisasi  data 
interaktif  pada  SIVIS  LBDSE,  kemudian  dilanjutkan  bulan 
Februari, April, dan Mei 2021 dengan mengevaluasi visualisasi 
data tersebut secara online. 

B. Metode Pengumpulan Data 

Data  yang  digunakan  dalam  penelitian  merupakan  data 
primer dan data sekunder. Data primer yang digunakan dalam 
evaluasi SIVIS LBDSE diperoleh melalui instrumen penelitian 
berupa kuesioner yang dibagikan kepada responden penelitian. 
Sedangkan 
dalam 
mengimplementasikan  visualisasi  data  interaktif  pada  SIVIS 
LBDSE diperoleh dari Publikasi Laporan Bulanan Data Sosial 
(LBDSE)  Edisi  Bulan  Oktober,  November, 
Ekonomi 
Desember 2020. 

digunakan 

sekunder 

yang 

data 

Responden  pada  penelitian  ini  terdiri  dari  kelompok 
responden  pemula  yaitu  siswa/i  SMA/MA  di  Bontang  dan 
responden  berpengalaman  yaitu  mahasiswa 
kelompok 
Politeknik Statistika STIS tingkat 4 jurusan Sains Data. 

Sementara itu, instrumen penelitian yang digunakan dalam 
mengevaluasi  SIVIS  LBDSE  pada  penelitian 
ini  yaitu 
kuesioner  dengan  menggunakan  Google  Form.  Dimana 
kuesioner  tersebut  terdiri  dari  8  blok,  antara  lain:  Blok 
Penjelasan  Survei;  Blok  Identitas  Responden;  dan  Blok 
Evaluasi  Bar Chart, Bubble Map, Tree Map, Line  Chart, Pie 
Chart, dan Choropleth Map yang berisi pernyataan-pernyataan 
persetujuan  mengenai  penilaian  visualisasi  data  pada  SIVIS 
LBDSE  berdasarkan  8  indikator  yang  telah  ditentukan. 
Pernyataan  persetujuan  tersebut  menggunakan  skala  Likert 

itu, 

Selain 

juga  dilakukan  analisis 

inferensia  untuk 
mengetahui  perbedaan  persepsi  penilaian  visualisasi  data 
interaktif  publikasi  LBDSE  antara  kelompok  pemula  dengan 
kelompok  berpengalaman.  Analisis  dilakukan  menggunakan 
uji t sampel independen dengan hipotesis awal yaitu tidak ada 
perbedaan  persepsi  penilaian  visualisasi  data 
interaktif 
publikasi LBDSE antara kedua kelompok responden. 

V.  KERANGKA PIKIR 

Berdasarkan  latar  belakang  yang  telah  dijelaskan  bahwa 
publikasi  Laporan  Bulanan  Data  Sosial  Ekonomi  (LBDSE) 
yang  setiap  bulan  diterbitkan  oleh  BPS  memiliki  visualisasi 
data  yang  kurang  lengkap.  Selain  itu  visualisasi  data  yang 
ditampilkan  pada  publikasi  LBDSE  hanya  bersifat  statis, 
sehingga  pembaca  tidak  dapat  melakukan  interaksi  untuk 
mengubah  tampilan  visualisasi  data  yang  ada.  Padahal  data-
data  yang  ada  pada  publikasi  LBDSE  memiliki  dimensi  atau 
variabel  yang  multidimensi.  Maka  dari  itu,  penelitian  ini 
menawarkan  sebuah  solusi  dengan  mengimplementasikan 
visualisasi  data  interaktif  pada  publikasi  LBDSE  melalui 
sebuah  sistem  bernama  SIVIS  LBDSE  (Sistem  Visualisasi 
LBDSE) agar pengguna dapat melihat visualisasi data LBDSE 
dengan  lengkap  dan  dapat  digunakan  secara  interaktif.  Tidak 
hanya  itu,  implementasi  visualisasi  data  interaktif  pada 
publikasi LBDSE  juga telah dievaluasi berdasarkan penilaian 
pengguna. 

Adapun kerangka pikir pada penelitian ini dapat dilihat pada 

Gambar 1. 

 2 / 8 

 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

mengindikasikan  bahwa  visualisasi  data 
interaktif  dapat 
mendukung  pengguna  untuk  menggali  informasi  lebih  dalam 
mengenai data.  

Referensi  [10]  menjelaskan  visualisasi  data  yang  dinamis 
dan  interaktif  dapat  dimanfaatkan  oleh  pengguna  dalam 
mengeksplorasi  data  untuk  dirinya  sendiri.  Hal  tersebut 
dikarenakan  visualisasi  data  interaktif  yang  memungkinkan 
pengguna untuk menampilkan berbagai macam tampilan data 
secara bergantian tergantung keinginan pengguna. 

B. Implementasi Visualisasi Data Interaktif pada SIVIS LBDSE 
SIVIS  LBDSE  (Sistem  Visualisasi  Laporan  Bulanan  Data 
Sosial  Ekonomi)  merupakan  sebuah  sistem  visualisasi  data 
interaktif yang menampilkan visualisasi data publikasi LBDSE 
secara  interaktif  melalui  berbagai  macam  grafik.  Data  yang 
digunakan mencakup seluruh topik  yang ada dalam publikasi 
LBDSE,  antara  lain  :  inflasi,  pertumbuhan  ekonomi,  ekspor-
impor, upah buruh, nilai tukar petani dan harga pangan, harga 
produsen dan harga perdagangan besar, indeks tendensi bisnis 
industri,  pariwisata, 
dan  konsumen, 
transportasi,  ketenagakerjaan,  kemiskinan  dan 
tingkat 
ketimpangan  pengeluaran  penduduk,  indeks  pembangunan 
manusia, indeks perilaku anti korupsi Indonesia (IPAK), indeks 
demokrasi Indonesia, serta luas panen dan produksi padi. 

indeks  produksi 

Diagram alur penggunaan SIVIS LBDSE dijelaskan melalui 

gambar berikut.

Gambar 1. Kerangka Pikir Penelitian 

VI. HASIL DAN PEMBAHASAN 

A. Visualisasi Data Interaktif 

Dalam  sebuah  artikel  [9],  sistem  visualisasi  data  interaktif 
didefinisikan  sebagai  sebuah  “mantra  visualisasi  informasi”, 
yang mana terdiri dari overview, kemudian zoom dan filtering, 
tersebut 
terakhir  details  on  demand.  Mantra-mantra 

Gambar 2. Diagram Alur SIVIS LBDSE

Pada  halaman  beranda,  terdapat  judul  sistem  beserta 
penjelasan  singkat  mengenai  SIVIS  LBDSE.  Selain  itu  juga 
terdapat  beberapa  menu  pada  halaman  beranda  antara  lain, 
Visualisasi  Data,  Publikasi,  dan  Kondef  Grafik.  Pada  menu 
Visualisasi Data terdapat dropdown berupa 18 topik yang ada 
pada publikasi LBDSE, dimana ketika salah satu topik dibuka, 
pengguna  akan  diarahkan  ke  halaman  visualisasi  data  dari 
masing-masing topik tersebut. Di halaman tersebut pengguna 
diminta  untuk  memilih  data  pada  menu  Pilih  Data  dan 
Visualisasi agar SIVIS LBDSE dapat menampilkan visualisasi 
data yang diminta. Selain visualisasi data interaktif, di halaman 

tersebut juga terdapat penjelasan mengenai konsep dan definisi 
dari data yang digunakan. 

Selain itu, visualisasi data pada SIVIS LBDSE memiliki fitur 
interaktif  yang  berbeda-beda  kegunaannya.  Namun  hampir 
setiap jenis visualisasi data menggunakan fitur interaktif yang 
sama. Fitur-fitur interaktif tersebut antara lain, selection, filter, 
detail on demand, zoom, dan search.  

Fitur  selection  digunakan  untuk  memilih  satu  atau  lebih 
variabel dengan cara menekan variabel yang diinginkan. Warna 
variabel yang tidak terpilih akan berubah menjadi sangat pudar, 
sedangkan warna variabel yang terpilih tidak berubah. Hal ini 

 3 / 8 

 
 
 
 
bertujuan  untuk  memudahkan  pengguna  dalam  membedakan 
variabel yang terpilih dan yang tidak terpilih. 

Sementara  itu,  fitur  filter  pada  visualisasi  data  SIVIS 
LBDSE  digunakan  untuk  memilih  data,  visualisasi  data, 
maupun  waktu  data  yang  ingin  ditampilkan.  Pada  beberapa 
jenis visualisasi data, fitur filter juga digunakan untuk memilih 
satu  atau  lebih  variabel  yang  ingin  ditampilkan  dalam 
visualisasi data. Tujuannya agar pengguna dapat fokus dalam 
membandingkan beberapa variabel yang diinginkan. 

Fitur lainnya yang ada pada SIVIS LBDSE yaitu fitur detail 
on demand.  Fitur ini digunakan untuk menampilkan detail data 
dari  salah  satu  variabel  berdasarkan  keinginan  pengguna 
dengan cara menyorot variabel tersebut menggunakan kursor. 
Ketika  salah  satu  variabel  disorot  oleh  pengguna,  visualisasi 
data  akan menampilkan detail variabel tersebut,  seperti nama 
variabel, waktu, nilai, dan lain-lain. 

Selain  itu,  pada  SIVIS  LBDSE  juga  terdapat  fitur  zoom. 
Namun fitur ini hanya terdapat pada visualisasi data Choropleth 
Map.  Pada  visualisasi  data  ini  fitur  zoom  digunakan  untuk 
Indonesia  yang 
memperbesar  atau  memperkecil  peta 
digunakan.  Fitur  ini  dapat  digunakan  dengan  menekan  menu 
zoom  pada  sebelah  kiri  atas  visualisasi  data  atau  dengan 
melakukan scroll menggunakan mouse maupun mousepad. 

Sama hal nya dengan zoom, fitur search pada SIVIS LBDSE 
juga  hanya  terdapat  pada  visualisasi  data  Choropleth  Map. 
Melalui fitur ini, pengguna dapat mencari lokasi suatu daerah 
yang  ada  di  peta  Indonesia  pada  visualisasi  data  Choropleth 
Map.  Fitur  search  ini  dapat  digunakan  melalui  menu  search 
bergambar  kaca  pembesar  pada  sebelah  kiri  atas  visualisasi 
data. 

SIVIS LBDSE menggunakan 6 jenis visualisasi data dalam 
melakukan  visualisasi  data  LBDSE  antara  lain,  Bar  Chart, 
Bubble Map, Tree Map, Line Chart, Pie Chart, dan Choropleth 
Map. Penggunaan masing-masing grafik pada SIVIS LBDSE 
tergantung dari bentuk data yang digunakan. 

1. Bar Chart 

Pada data kategorik, salah satu grafik yang dapat digunakan 
yaitu Bar Chart. Yang membedakan penggunaan nya dengan 
grafik  lainnya  dalam  menampilkan  data  kategorik  yaitu  data 
yang  memiliki  nilai  negatif  hanya  dapat  ditampilkan 
menggunakan  Bar  Chart.  Seperti  yang  dicontohkan  pada 
Gambar  4,  dimana  menampilkan  visualisasi  data  Bar  Chart 
pada  data  Harga  Eceran  Beberapa  Komoditas  Bahan  Pokok 
September-November  2020  (rupiah).  Visualisasi  data  Bar 
Chart  tidak  hanya  menampilkan  data  nya  dalam  bentuk 
visualisasi  batang  yang  statis,  tetapi  juga  menyediakan  fitur-
fitur  interaktif  di  dalamnya,  antara  lain  selection,  filter,  dan 
detail on demand. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

2. Bubble Map 

Visualisasi  data  selanjutnya  pada  SIVIS  LBDSE  yaitu 
visualisasi data Bubble Map. Sama hal nya dengan Bar Chart, 
visualisasi  data  Bubble  Map  digunakan  pada  data  kategorik, 
namun  yang  tidak  memiliki  nilai  negatif.  Hal  ini  disebabkan 
oleh  bangun  datar  yang  digunakan  berupa  lingkaran,  dimana 
semakin besar ukuran lingkaran menunjukkan semakin tinggi 
nilai  variabel  tersebut.  Begitu  pula  sebaliknya,  semakin  kecil 
ukuran  lingkaran  menunjukkan  nilai  variabel  yang  semakin 
rendah.  Selain 
juga 
menggunakan  warna-warna  yang  berbeda  antar  lingkaran 
dengan tujuan menunjukkan kategori data yang berbeda-beda. 
Seperti halnya pada visualisasi data Bar Chart, pada visualisasi 
data  Bubble  Map  juga  terdapat  beberapa  fitur-fitur  interaktif 
yang dapat digunakan pengguna. Fitur-fitur tersebut antara lain, 
selection, filter, dan detail on demand. 

itu  visualisasi  data  Bubble  Map 

Gambar 5. Visualisasi Data Bubble Map 

3. Tree Map 

Sama hal nya dengan visualisasi data Bubble Map, data yang 
digunakan pada visualisasi data Tree Map juga merupakan data 
kategorik yang tidak memiliki nilai negatif. Hal ini disebabkan 
oleh  komponen  Tree  Map  yang  terdiri  dari  segi  empat  yang 
dikumpulkan  berdasarkan  variabel-variabel  pada  data  yang 
digunakan  sampai  membentuk  segi  empat  berukuran  besar. 
Semakin tinggi nilai variabel maka semakin besar ukuran dan 
semakin gelap warna segi empat pada variabel tersebut. Begitu 
pula  sebaliknya,  semakin  kecil  nilai  suatu  variabel  maka 
semakin  kecil  ukuran  dan  semakin  cerah  warna  segi  empat 
tersebut.  Seperti  contoh  yang  ditampilkan  pada  Gambar  6, 
dimana  gambar  tersebut  menampilkan  visualisasi  data  PDB 
Atas  Dasar  Harga  Berlaku  Menurut  Lapangan  Usaha,  Tahun 
2017-2020 (triliun rupiah). 

Selain itu, visualisasi data Tree Map juga memiliki beberapa 

fitur interaktif berupa selection, filter, dan detail on demand.  

Gambar 4. Visualisasi Data Bar Chart 

Gambar 6. Visualisasi Data Tree Map 

 4 / 8 

 
 
 
 
 
4. Line Chart 

6. Choropleth Map 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Sementara  itu,  Line  Chart  digunakan  untuk  data  yang 
memiliki  trend,  baik  bulanan,  triwulanan,  maupun  tahunan. 
Pada visualisasi data Line Chart dalam SIVIS LBDSE, peneliti 
menambahkan  visualisasi  animasi  untuk  melihat  nilai  pada 
periode  waktu  tertentu.  Dengan  animasi  ini,  poin-poin  data 
pada  Line  Chart  dapat  bergerak  secara  otomatis  mengikuti 
periode  waktu  data  tersebut.  Sebagai  contoh  dapat  dilihat 
Gambar 7 berikut. 

Sementara itu,  Choropleth Map digunakan pada data  yang 
memiliki  variabel  geografis.  Choropleth  Map  menampilkan 
data  dalam  bentuk  peta,  dengan  menggunakan  warna-warna 
yang  berbeda  antar  daerah.  Hal 
ini  bertujuan  untuk 
memudahkan pengguna dalam membedakan data antar daerah 
tersebut. Semakin gelap warna pada suatu daerah menunjukkan 
nilai  data  yang  semakin  tinggi.  Begitu  pula  sebaliknya,  jika 
warna pada suatu daerah semakin cerah menunjukkan data pada 
daerah tersebut semakin rendah nilainya.  

Selain  itu,  visualisasi  data  Choropleth  Map  pada  SIVIS 
LBDSE  ini  juga  menggunakan  beberapa  fungsi  interaktif, 
antara lain zoom, filter, search, selection, dan detail on demand. 
Sebagai  contoh  dapat  dilihat  pada  Gambar  9,  yang 
menampilkan  visualisasi  data  Indeks  Demokrasi  Indonesia 
(IDI) Berdasarkan Aspek dan Provinsi Tahun 2019. 

Gambar 7. Visualisasi Data Line Chart 

Gambar  tersebut  menampilkan  visualisasi  data  Line  Chart 
pada  data  Rata-rata  Harga  Beras  di  Penggilingan  Menurut 
Kelompok  Kualitas  (rupiah/kg),  Bulan  September  2019–
November 2020. Ketika tombol play pada menu sebelah kanan 
ditekan,  poin-poin  data  harga  beras  setiap  kelompok  kualitas 
akan  bergerak  secara  otomatis  mulai  bulan  September  2019 
sampai  November  2020.  Selain  itu,  pada  visualisasi  data  ini 
juga memiliki beberapa fitur interaktif, antara lain selection dan 
detail on demand. 

5. Pie Chart 

Selain itu, pada SIVIS LBDSE juga terdapat visualisasi data 
Pie  Chart  untuk  menampilkan  data  kategorik.  Namun  yang 
membedakan dengan Bar Chart, Bubble Map, dan Tree Map, 
data yang digunakan pada Pie Chart berupa persentase dimana 
total  persentase  harus  100%.  Hal 
ini  bertujuan  untuk 
membentuk  Pie  Chart  menjadi  bangun  datar  lingkaran  yang 
sempurna.  Salah  satu  contoh  penggunaan  Pie  Chart  dapat 
dilihat pada Gambar 8. 

Gambar 8. Visualisasi Data Pie Chart 

Gambar 8 menampilkan visualisasi data Pie Chart pada data 
Peranan  Wilayah/Pulau  dalam  Pembentukan  PDB  Nasional 
(persen).  Visualisasi  data 
tersebut  menyediakan  fungsi 
interaktif berupa filter, selection, dan detail on demand. 

Gambar 9. Visualisasi Data Choropleth Map 

C. Evaluasi Visualisasi Data Interaktif pada SIVIS LBDSE 

Selain  mengimplementasikan  visualisasi  data  interaktif, 
pada  penelitian  ini  juga  dilakukan  evaluasi  terhadap  masing-
masing  jenis  visualisasi  data  yang  digunakan  dalam  SIVIS 
LBDSE. Evaluasi dilakukan berdasarkan kelompok-kelompok 
penilaian  beserta  indikatornya  yang  diadopsi  dari  penelitian 
sebelumnya. 

Dengan  mengacu  pada  penghitungan  skor  uji  SUS,  hasil 
penilaian setiap responden pada penelitian ini akan dikali 2.5 
dan  dihitung  rata-rata  untuk  setiap  indikator  penilaian  dan 
visualisasi data.  

Selanjutnya,  rata-rata  skor  penilaian  untuk  setiap  indikator 
penilaian  dan  visualisasi  data  tersebut  dijumlahkan  untuk 
mendapatkan skor akhir masing-masing visualisasi data. Skor 
masing-masing visualisasi data dapat dilihat pada Tabel 2. 

Dari hasil penilaian visualisasi data berdasarkan 8 indikator 
yang ditampilkan pada Tabel 2, memperlihatkan bahwa kedua 
kelompok responden lebih tertarik dengan tampilan visualisasi 
data berupa Choropleth Map. 

Sementara  itu,  visualisasi  data  yang  mudah  dipahami  oleh 
kedua kelompok responden yaitu visualisasi data Bar Chart dan 
Line  Chart.  Sedangkan  visualisasi  data  yang  paling  mudah 
digunakan  oleh  kelompok  pemula  pada  SIVIS  LBDSE  yaitu 
visualisasi  data  Line  Chart.  Demikian  pula  bagi  kelompok 
berpengalaman,  mereka  menilai  bahwa  visualisasi  data  yang 
paling mudah digunakan pada SIVIS LBDSE yaitu visualisasi 
data Line Chart dan Bar Chart.

 5 / 8 

 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

TABEL II 
HASIL PENILAIAN VISUALISASI DATA INTERAKTIF

Indikator 

Daya tarik visual 

Kemudahan Pemahaman 

Kemudahan Penggunaan 

Desain Informatif 

Kegunaan 

Kelengkapan 

Kemampuan Perbandingan 

Interaktivitas 

SKOR 

Responden 
Kelompok pemula 
Kelompok berpengalaman 
Kelompok pemula 
Kelompok berpengalaman 
Kelompok pemula 
Kelompok berpengalaman 
Kelompok pemula 
Kelompok berpengalaman 
Kelompok pemula 
Kelompok berpengalaman 
Kelompok pemula 
Kelompok berpengalaman 
Kelompok pemula 
Kelompok berpengalaman 
Kelompok pemula 
Kelompok berpengalaman 

Bar Chart 
10.7 
10 
10.7 
11.2 
10.3 
11.3 
11 
10.6 
10.8 
10.4 
10.6 
10.4 
11 
10.7 
10.1 
11 
85.4 

Bubble Map 
11.2 
10.2 
10.4 
10 
10.5 
10.7 
10.7 
10.2 
9.7 
10.6 
10 
10 
10.9 
10.5 
10 
10.8 
83.2 

Tree Map 
10.5 
10.4 
10.4 
10.3 
10.6 
10.8 
11 
10.4 
10.3 
10.2 
10.8 
10.6 
10.7 
10.7 
9.9 
11.2 
84.4 

Line Chart 
10.8 
10.3 
10.7 
11.2 
11.1 
11.3 
11.2 
11.5 
10.3 
11.1 
10.8 
11.1 
10.5 
11.2 
10.2 
11.5 
87.4 

Pie Chart 
10.9 
10 
10.3 
10.1 
10.5 
10.7 
10.6 
10.2 
9.7 
10.4 
10.1 
10.3 
10.7 
10.4 
9.6 
11 
82.75 

Choropleth Map 
11.2 
11.2 
10.4 
10.9 
9.9 
11.2 
10.6 
10.5 
10.4 
11.1 
10.4 
10.8 
10.1 
10.7 
10 
11.4 
85.4 

Indikator  lainnya  yaitu  indikator  “Interaktivitas”,  dimana 
kedua  kelompok  responden  sepakat  bahwa  visualisasi  data 
yang paling interaktif yaitu visualisasi data Line Chart.  

Selain itu, berdasarkan indikator “Desain Informatif” dapat 
disimpulkan  bahwa  menurut  kelompok  responden  pemula 
maupun  kelompok  responden  berpengalaman  visualisasi  data 
yang memiliki desain yang paling informatif yaitu visualisasi 
data Line Chart. 

Pada  indikator  “Kegunaan”  kelompok  responden  pemula 
menilai bahwa visualisasi data yang memiliki informasi paling 
berguna dalam meningkatkan pemahaman terhadap data yaitu 
visualisasi  data  Bar  Chart.  Berbeda  dengan  kelompok 
responden berpengalaman yang menilai visualisasi data paling 
berguna yaitu Choropleth Map dan Line Chart. 

Sementara  itu,  berdasarkan  hasil  penilaian  pada  indikator 
“Kelengkapan” visualisasi data yang memiliki informasi paling 
lengkap menurut kelompok responden pemula yaitu Tree Map 
dan  Line  Chart,  sedangkan  menurut  kelompok  responden 
berpengalaman yaitu visualisasi data Line Chart. 

Indikator  penilaian 

terakhir  yang  digunakan  yaitu 
“Kemampuan  Perbandingan”,  dimana  kelompok  responden 
pemula  menilai  visualisasi  data  Bar  Chart  memiliki 
kemampuan perbandingan yang paling tinggi. Berbeda dengan 
kelompok  responden  berpengalaman  yang  menilai  visualisasi 
data  Line  Chart  yang  memiliki  kemampuan  perbandingan 
paling tinggi. 

Dari  penghitungan  skor  untuk  masing-masing  indikator 
telah  dijelaskan 
penilaian  dan  visualisasi  data  yang 
sebelumnya,  dapat  disimpulkan  mengenai  kategori  kualitas 
masing-masing  visualisasi  data.  Pengkategorian  kualitas 
visualisasi data  mengacu pada kelompok peringkat skor SUS 
dari penelitian yang dilakukan oleh A. Bangor, P.T. Kortum, 
dan  J.T.  Miller  dengan  judul  “Determining  What  Individual 
SUS Scores Mean: Adding an Adjective Rating Scale” seperti 
yang ditampilkan pada Gambar 10. 

Gambar 10. Kelompok Peringkat Skor SUS 

Berdasarkan Gambar 10 dapat dikelompokkan skor dengan 
nilai  kurang  dari  25  tergolong  Worst  Imaginable,  25  –  39 
tergolong Poor, 39 – 52 tergolong Ok, 52 – 73 tergolong Good, 
73 – 85 tergolong Excellent, dan lebih dari 85 tergolong Best 
Imaginable. 

Berdasarkan  total  skor  yang  diperoleh  masing-masing 
visualisasi  data  pada  Tabel  2,  dapat  disimpulkan  bahwa  Line 
Chart tergolong kategori Best Imaginable Visualization. Begitu 
pula visualisasi data Bar Chart dan Choropleth Map yang juga 
tergolong kategori Best Imaginable Visualization. Hal tersebut 
disebabkan  oleh  skor  yang  diperoleh  ketiga  visualisasi  data 
tersebut bernilai lebih dari 85. Sementara itu, 3 visualisasi data 
lainnya yaitu Bubble Map, Tree Map, dan Pie Chart tergolong 
kategori  Excellent  Visualization  karena  memperoleh  skor 
antara 73 – 85. 

D.  Perbedaan  Persepsi  Penilaian  Kelompok  Pemula  dan 
Berpengalaman 

Untuk mengetahui perbedaan persepsi penilaian visualisasi 
data interaktif pada SIVIS LBDSE antara kelompok responden 
pemula  dan  kelompok  responden  berpengalaman,  peneliti 
melakukan uji Mann Whitney dengan hipotesis awal yaitu tidak 
ada  perbedaan  persepsi  penilaian  antara  kedua  kelompok 
responden. Data yang digunakan dalam uji ini yaitu skor total 
seluruh  indikator  penilaian  untuk  seluruh  visualisasi  data 
interaktif pada SIVIS LBDSE yang dinilai oleh masing-masing 
responden. 

Dengan  menggunakan  tingkat  signifikansi  sebesar  5%, 
peneliti  melakukan  uji  Mann  Whitney  dengan  perhitungan 
manual  maupun  menggunakan  aplikasi  IBM  SPSS  Statistics 

 6 / 8 

 
 
 
versi 26. Sehingga diperoleh nilai p-value sebesar 0,763 dan Z 
hitung sebesar -0,301. Oleh sebab nilai p-value yang lebih besar 
dari 0,05 maka diputuskan bahwa hipotesis awal diterima. 
Sehingga  dapat  disimpulkan  bahwa  dengan 

tingkat 
signifikansi sebesar 5% tidak ada perbedaan persepsi penilaian 
antara kelompok responden pemula dan kelompok responden 
berpengalaman  dalam  menilai  dan  mengevaluasi  visualisasi 
data  interaktif  pada  SIVIS  LBDSE.  Artinya,  visualisasi  data 
interaktif  pada  SIVIS  LBDSE  dapat  digunakan  oleh  seluruh 
pengguna, baik  pengguna  yang  telah  berpengalaman  maupun 
pengguna yang masih awam terhadap visualisasi data. 

E. Visualisasi Data Kategorik Terbaik 

Dalam  membandingkan  data  kategorik  pada  publikasi 
LBDSE, SIVIS LBDSE menggunakan 4 jenis visualisasi data 
yakni Bar Chart, Bubble Map, Tree Map, dan Pie Chart. Pada 
visualisasi  data  Bar  Chart,  digunakan  tinggi  batang  untuk 
menunjukkan  tinggi  rendahnya  nilai  masing-masing  kategori 
dan warna yang berbeda-beda untuk menunjukkan perbedaan 
setiap kategori data. Begitu pula pada visualisasi data  Bubble 
Map dan Pie Chart dimana kedua visualisasi data tersebut juga 
menggunakan luas area untuk menunjukkan tinggi rendahnya 
nilai  masing-masing  kategori  serta  warna  pada  visualisasi 
untuk  membedakan  setiap  kategori  pada  data.  Sementara  itu, 
visualisasi  data  Tree  Map  menggunakan  luas  area dan  warna 
untuk  menunjukkan 
suatu 
kategori/variabel. 

rendahnya 

tinggi 

nilai 

Berdasarkan  analisis  penilaian  terhadap  masing-masing 
visualisasi  data  dan  indikator  penilaian  yang  telah  dijelaskan 
sebelumnya, peneliti dapat menyimpulkan peringkat visualisasi 
data terbaik untuk data kategorik menurut rata-rata skor yang 
diberikan kedua kelompok responden. 

Berdasarkan Tabel 2, penilaian visualisasi data pada SIVIS 
LBDSE  memperlihatkan  bahwa  peringkat  visualisasi  data 
terbaik  terhadap  data  kategorik  publikasi  LBDSE  secara 
berturut-turut  yaitu  Bar  Chart,  Tree  Map,  Bubble  Map,  dan 
terakhir  Pie  Chart.  Sehingga  dapat  disimpulkan  bahwa 
visualisasi  data  terbaik  untuk  membandingkan  data  kategorik 
menurut penilaian gabungan kedua kelompok responden yaitu 
visualisasi  data  Bar  Chart.  Sedangkan  visualisasi  data  yang 
paling tidak baik yaitu visualisasi data Pie Chart. 

Penilaian  ini  sejalan  dengan  teori  yang  dikemukakan  oleh 
Cleveland and McGill[11] dimana mereka melakukan evaluasi 
keakuratan  berbagai  properti,  elemen  grafis,  dan  substrat 
spasial  yang  dapat  digunakan  dalam  menilai  data  kuantitatif. 
Hasilnya  menjelaskan  bahwa  elemen  grafik  dan  spasial  yang 
paling  akurat  dalam  melakukan  visualisasi  data  kuantitatif 
secara berturut-turut yaitu Position, Length, Orientation, Area, 
Volume, Color and texture. 

Ricardo Mazza[12] menjelaskan elemen grafis adalah segala 
sesuatu yang muncul dan terlihat pada ruang visualisasi. Ada 
empat  jenis  elemen  grafis  yang  biasa  digunakan  pada 
visualisasi  data  yakni  titik,  garis,  permukaan,  dan  volume. 
Sementara  itu,  properti  grafis  merupakan  sifat  dari  elemen 
grafis  yang  membuat  retina  mata  manusia  sangat  sensitif 
terhadapnya.  Hal  itulah  yang  membuat  properti  grafis  juga 
disebut  variabel  retinal.  Properti  grafis  yang  paling  umum 
digunakan adalah ukuran, orientasi, warna, tekstur, dan bentuk. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Seperti  yang  telah  dijelaskan  sebelumnya  bahwa  urutan 
visualisasi  data  terbaik  menurut  penilaian  gabungan  seluruh 
responden  secara  berturut-turut  yaitu  Line  Chart,  Choropleth 
Map,  Bar  Chart,  Tree  Map,  Bubble  Map,  dan  terakhir  Pie 
Chart.  Hal  ini  sejalan  dengan  tingkat  keakuratan  elemen  dan 
properti grafis yang digunakan dalam menilai data kuantitatif 
menurut  Cleveland  dan  McGill,  yang  mana  Line  Chart 
menggunakan 
titik,  Choropleth  Map 
menggunakan elemen warna, Bar Chart menggunakan elemen 
panjang/tinggi, Tree Map menggunakan elemen luas area dan 
warna,  Bubble  Map  menggunakan  elemen  luas  area,  dan  Pie 
luas  area.  Sehingga  dapat 
Chart  menggunakan  elemen 
disimpulkan visualisasi data  terbaik yang dinilai oleh seluruh 
responden juga mengindikasikan visualisasi data yang semakin 
akurat. 

elemen  posisi 

VII. 

PENUTUP 

juga 

itu,  peneliti 

Pada penelitian ini, peneliti berhasil mengimplementasikan 
visualisasi  data  interaktif  pada  data  LBDSE  melalui  6  jenis 
visualisasi data interaktif pada SIVIS LBDSE antara lain, Bar 
Chart,  Bubble  Map,  Tree  Map,  Line  Chart,  Pie  Chart,  dan 
Choropleth  Map.  Tidak  hanya 
telah 
melakukan  evaluasi  terhadap  implementasi  visualisasi  data 
interaktif tersebut. Dengan menggunakan 8 indikator penilaian, 
disimpulkan bahwa visualisasi data Bar Chart, Line Chart, dan 
Choropleth Map tergolong ke dalam kategori Best Imaginable 
Visualization.  Sedangkan  3  visualisasi  data  lainnya  yaitu 
Bubble  Map,  Tree  Map,  dan  Pie  Chart  tergolong  kategori 
Excellent Visualization. Selanjutnya, hasil evaluasi visualisasi 
data  interaktif  menjelaskan  bahwa  seluruh  responden  menilai 
visualisasi  data  terbaik  terhadap  data  kategorik  publikasi 
LBDSE  secara  berturut-turut  yaitu  Bar  Chart,  Tree  Map, 
terakhir  Pie  Chart.  Sehingga  dapat 
Bubble  Map,  dan 
disimpulkan  bahwa  visualisasi  data  kategorik  terbaik  pada 
SIVIS  LBDSE  menurut  penilaian  pengguna  yaitu  visualisasi 
data Bar Chart. 

DAFTAR PUSTAKA 
[1]  Tufte, Edward. The Visual Display of Quantitative Information. Cheshire, 

Connecticut: Graphics Press, 1983. ISBN 0961392142. 

[2]  Subdirektorat  Publikasi  dan Kompilasi  Statistik.  Laporan  Bulanan  Data 
Sosial Ekonomi Oktober 2020. Jakarta: Badan Pusat Statistik, 2020. 
[3]  Subdirektorat  Publikasi  dan Kompilasi  Statistik.  Laporan  Bulanan  Data 
Sosial Ekonomi November 2020. Jakarta: Badan Pusat Statistik, 2020. 
[4]  Subdirektorat  Publikasi  dan Kompilasi  Statistik.  Laporan  Bulanan  Data 
Sosial Ekonomi Desember 2020. Jakarta: Badan Pusat Statistik, 2020. 
[5]  A.  Shamim,  V.  Balakrishnan,  M.  Tahir,  “Evaluation  of  opinion 
visualization techniques”, Information Visualization, vol.14,no.4,pp.1–20, 
Sept 2014. 

[6]  Q.V.  Nguyen,  N.  Miller,  D.  Arness  et  al.,  “Evaluation  on  interactive 
visualization  data  with  scatterplots”,  Visual  Informatics  (2020),  doi: 
https://doi.org/10.1016/j.visinf.2020.09.004. 

[7]  T.  Downer,  M.  Gray,  &  P.  Andersen,  “Three-dimensional  technology: 
Evaluating  the  use  of  visualisation  in  midwifery  education”,  Clinical 
Simulation 
in  Nursing,  vol.  39(C),  pp.  27-32,  Feb  2020, 
https://doi.org/10.1016/j.ecns.2019.10.008. 

[8]  X. Bai, D. White, D. Sundaram, “Purposeful Visualization”, in 2011 44th 
Hawaii  International  Conference  on  System  Sciences,  Kauai,  HI,  USA: 
IEEE, Feb 2011, pp. 1-10. 

[9]  B. Shneiderman, “The Eyes Have It: A Task by Data Type Taxonomy for 
Information  Visualizations”,  In  Proceedings of  the  IEEE Symposium on 

 7 / 8 

 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Visual  Languages,  Washington:  IEEE  Computer  Society  Press,  1996, 
pp.336-343. http://citeseer.ist.psu.edu/409647.html 

[10] S.  Murray,  Interactive  Data  Visualization  for  the  Web,  USA:  O’Reilly 

Media, 2013. 

[11] W.  S.  Cleveland  and  R.  McGill,  “Graphical  perception:  Theory, 
experimentation,  and  application  to  the  development  of  graphical 
methods,”  Journal  of  the  American  Statistical  Association,  vol.  79,  no. 
387, pp. 531–554, Sep. 1984, doi: 10.1080/01621459.1984.10478080. 
[12] R.  Mazza,  Introduction  to  Information  Visualization.  London:  Springer-

Verlag, 2009. doi: 10.1007/978-1-84800-219-7. 

[13] V. Friedman, “Data Visualization and Infographics.” Smashing Magazine, 
January 14, 2008. https://www.smashingmagazine.com/2008/01/monday-
inspiration-data-visualization-and-infographics 

 8 / 8 

 
 
"
221709721,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pengembangan Sistem Informasi Kontrol 
Pemeliharaan Barang Milik Negara  

(SIKOP BMN) 

Studi Kasus Pusdiklat Badan Pusat Statistik RI 

Grujs Farhan Nafis (221709721, 4SI1) 

Dosen Pembimbing: Takdir SST., M.T. 

Ringkasan—  Upaya  pemeliharaan  Barang  Milik  Negara 
(BMN)  yang  baik  merupakan  salah  satu  faktor  penting  guna 
mewujudkan  optimalisasi  pemanfaatan  Anggaran  Pendapatan 
dan  Belanja  Negara  (APBN)  oleh  instansi  pemerintah.  Namun, 
masih  terdapat  beberapa  permasalahan  yang  harus  dihadapi, 
diantaranya:  kuantitas  BMN  yang  cukup  banyak  menyebabkan 
kegiatan pemantauan kondisi BMN mengalami hambatan, upaya 
pemeliharaan  BMN  oleh  penyedia  jasa  yang  masih  belum 
terjamin kualitasnya dan prosedur permintaan perbaikan BMN 
oleh  pegawai  dirasa  masih  kurang  efektif  karena  masih  secara 
manual. Untuk mengatasi permasalahan diatas, telah dilakukan 
penelitian  mengenai  Pengembangan  Sistem  Informasi  Kontrol 
Pemeliharaan  Barang  Milik  Negara  (SIKOP  BMN)  dengan 
tujuan  untuk  mengintegrasikan  pihak-pihak  yang  ikut  serta 
dalam upaya pemeliharaan BMN, diantaranya Pegawai, KSBRT, 
Operator  BMN,  PPK,  PP,  Teknisi,  dan  Penyedia.  Penelitian  ini 
juga dilakukan agar prosedur pemeliharaan BMN dapat menjadi 
lebih    mudah  dan  efektif.  Penelitian  ini  dilakukan  dengan 
mengkaji  studi  kasus  yang  ada  pada  Pusdiklat  BPS  RI.  Hasil 
evaluasi sistem menunjukkan bahwa sistem usulan dapat diterima 
dengan cukup baik oleh pengguna. 

Kata Kunci—  sistem informasi, kontrol pemeliharan, BMN. 

I. LATAR BELAKANG 

Barang  Milik  Negara  (BMN)  adalah  semua  barang  yang 
dibeli  atau  diperoleh  atas  beban  Anggaran  Pendapatan  dan 
Belanja  Negara  (APBN)  atau  berasal  dari  perolehan  lainnya 
yang sah [1]. Karena barang tersebut salah satunya dibeli atau 
diperoleh atas beban APBN [2], barang tersebut perlu dirawat 
dan dipelihara dengan baik sebagai bentuk tanggung jawab dari 
instansi terkait terhadap negara agar dapat dimanfaatkan secara 
optimal sesuai dengan fungsionalitas barang tersebut. 

Upaya pemeliharaan aset BMN secara umum dibagi menjadi 
dua, yaitu perbaikan dan perawatan rutin aset. Perbaikan adalah 
aktivitas yang dilakukan oleh pihak instansi untuk memulihkan 
kondisi  fisik  atau  fungsional aset  yang  mengalami  kerusakan 
agar dapat digunakan dengan normal untuk kebutuhan harian 
instansi. Sedangkan perawatan rutin merupakan aktivitas yang 
dilakukan secara periodik oleh instansi untuk mengoptimalkan 
performa  aset  dalam  fungsi  kinerja-nya,  serta  memantau 
kondisi aset yang mana jika terdapat kerusakan baik secara fisik 

maupun secara fungsional dapat terdeteksi lebih awal sehingga 
akan segera dilakukan penanganan berupa perbaikan agar dapat 
meminimalisir terjadinya kerusakan besar yang menyebabkan 
status aset menjadi “rusak berat” dan pada akhirnya tidak dapat 
digunakan lagi oleh pihak instansi. 

Realitas  yang  terjadi  di  lapangan,  jumlah  operator  BMN 
yang  bertugas  untuk  mengawasi  dan  mengelola  BMN  tidak 
seimbang dengan banyaknya jumlah BMN yang ada. Sehingga 
mengakibatkan  kurangnya  pengawasan  terhadap  status  BMN 
meliputi  lokasi,  kondisi  fisik  dan  perawatan  yang  telah 
dilakukan.  Ditambah  lagi  dengan  prosedur  perawatan  dan 
pemeliharaan  BMN  yang  masih  belum  terintegrasi  dengan 
suatu  sistem  sehingga 
tidak  dapat  menjamin  kualitas 
pemeliharaan  BMN  serta  memungkinkan  terjadinya  markup 
pemeliharaan  berulang  yang  nantinya  akan  menyebabkan 
pemborosan  biaya  APBN  dan  pada  akhirnya  akan 
mengakibatkan kerugian negara yang cukup besar  

Saat  ini,  upaya  pengelolaan  dan  pencatatan  BMN  telah 
dilakukan  melalui  aplikasi  yang  dikembangkan  oleh 
Kementrian  Keuangan  Republik  Indonesia  yaitu  Sistem 
Informasi  Manajemen  dan  Akuntansi  Barang  Milik  Negara 
(SIMAK BMN) [3][4]. Namun, pada aplikasi ini masih belum 
tersedia fitur untuk mengontrol kegiatan pemeliharaan BMN.  

Dengan  dikembangkannya  sistem  yang  terintegrasi  untuk 
mengontrol  kegiatan  pemeliharaan  BMN,  maka  diharapkan 
prosedur  pemeliharaan  BMN  akan  menjadi  lebih  terstruktur 
dan  nantinya  dapat  dipertanggung 
jawabkan  kualitas 
pengerjaannya.  Karena  pada  proses  pengerjaannya,  penyedia 
jasa  yang  ditugaskan  untuk  melakukan  pemeliharaan  dan 
perawatan  BMN  diharuskan  melampirkan  bukti  hasil 
pekerjaannya kedalam  sistem.  Selain  itu,  status  pemeliharaan 
BMN  akan  tercatat  dalam  sistem  sehingga  dapat  dipantau 
riwayat 
secara  berkala  oleh  operator  BMN  mengenai 
pemeliharaan aset-aset BMN. 

II. TUJUAN PENELITIAN 

Tujuan umum penelitian ini adalah untuk mengembangkan 
aplikasi SIKOP BMN sebagai sistem informasi yang dirancang 

 1 / 9 

 
 
 
dapat membantu pemeliharaan aset BMN mulai dari  pelaporan 
keluhan pegawai mengenai permasalahan seputar aset  sampai 
dokumentasi kegiatan perawatan dan perbaikan yang dilakukan 
penyedia,  sehingga  kegiatan  pemeliharaan  di  Pusdiklat  BPS 
dapat  lebih  terstruktur  dan  tercatat  dengan  baik  agar  bisa 
memudahkan  pegawai  dan  menjadi  bahan  evaluasi  untuk 
kegiatan  pemeliharaan  selanjutnya.  Sedangkan  tujuan  khusus 
dari penelitian ini adalah sebagai berikut. 

1.  Mengembangkan  modul 
pegawai 

untuk 
berkepentingan 
mempermudah 
mengakses informasi mengenai aset BMN tanpa harus 
lewat aplikasi SIMAK BMN. 

daftar  BMN 
yang 

2.  Mengembangkan  modul  laporan  keluhan  aset  BMN 
oleh pegawai untuk memudahkan pegawai dan pihak 
yang berkepentingan dalam membuat dan mengelola 
laporan permasalahan terkait aset BMN yang dipakai. 

3.  Mengembangkan  modul  pemeliharaan  (perawatan 
dan  perbaikan)  aset  BMN  untuk  mencatat  dan 
mendokumentasikan  kegiatan  pemeliharaan  yang 
dilakukan oleh penyedia. 

4.  Mengembangkan  modul  kelola  pengguna,  kelola 
bagian,  kelola  penyedia,  dan  notifikasi  untuk 
mempermudah koordinasi para pihak dalam kegiatan 
pemeliharaan. 

III. PENELITIAN TERKAIT 

TABEL I 
TABEL LITERATUR 

No 

Penulis 

Bahasan 

1 

2 

3 

4 

Amelia Safitri [5] 

Sistem Informasi pengelolaan BMN di BPS 

Muhammad Naufal 
Afaf [6] 

sistem 

Pengembangan 
informasi 
pengelolaan  BMN  di  Politeknik  Statistika 
STIS  dan  penggunaan  QR  Code  sebagai 
identitas BMN tidak habis pakai 

Najwan Yusnianda 
[7] 

Pengembangan 
Informasi 
Pengelolaan  Pengadaan  Langsung  di  BPS 
RI 

Sistem 

Rozan Fikri [8] 

Pengembangan  Aplikasi  Sistem  Informasi 
Pengadaan Langsung (SIPELANG) di BPS 
RI 

Pada  penelitian  [5],  peneliti  mengembangkan  sistem 
informasi pengelolaan BMN dengan mengambil studi kasus di 
Badan  Pusat  Statistik  (BPS).  Dalam  penelitian  ini  peneliti 
befokus  pada  pemantauan  daftar  aset  yang  ada  dikantor  BPS 
yang  tersebar  diseluruh  Indonesia  oleh  kantor  pusat  dengan 
sebagai  media  untuk 
internet 
memanfaatkan 
mengirimkan data aset BMN pada tiap kantor daerah.  

jaringan 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pada  penelitian  [6],  peneliti  mengembangkan  sistem 
informasi pengelolaan BMN dengan mengambil studi kasus di 
Politeknik  Statistika  STIS  dan  memanfaatkan  teknologi  QR 
Code sebagai alat bantu identifikasi barang BMN tidak habis 
pakai.  

Sedangkan pada penelitian [7], peneliti membangun sistem 
informasi berupa aplikasi berbasis web untuk mengakomodasi 
pengelolaan  kegiatan  pengadaan  langsung  di  BPS.  Aplikasi 
tersebut  dapat  mengakomodasi  kegiatan  pengadaan  langsung 
pada kelas C dan juga mencakup tahapan kegiatan pengadaan 
langsung  sampai  dengan  serah  terima  hasil  pekerjaan.  Lalu 
aplikasi  ini  dilanjutkan  dan  disempurnakan  oleh  peneliti  [8] 
sehingga  sistem  mampu mengakomodasi  kegiatan pengadaan 
tahap 
langsung  pada 
penagihan/pembayaran. 

sampai  dengan 

semua  kelas 

Penelitian  ini  akan  menggunakan  referensi  pada  keempat 
penelitian  diatas  dalam  mengembangkan  sistem  informasi 
pengelolaan BMN yang memfokuskan pengembangan namun 
tidak  terbatas  pada  fitur  kontrol  pemeliharaan    BMN  dengan 
mengambil studi kasus di Pusdiklat BPS RI. 

IV. METODE PENELITIAN 

A. Ruang Lingkup Penelitian  

Ruang 

lingkup  dari  penelitian 

ini  adalah  kegiatan 
pemeliharaan  aset  BMN  yang  mencakup  pengambilan  data 
mengenai aset yang telah terdaftar dari aplikasi SIMAK BMN, 
lalu  kegiatan  pelaporan  keluhan  mengenai  aset  BMN  oleh 
pegawai  untuk  kemudian  ditindaklanjuti  oleh  staff  yang 
berwenang dan kegiatan pemeliharaan oleh penyedia meliputi 
perawatan  dan  perbaikan.  Namun  dalam  penelitian  ini  tidak 
mencakup  pada  pengelolan  dokumen  pengadaan  yang 
dibutuhkan untuk kegiatan pemeliharaan yang dilakukan. 

B. Sumber Data dan Metadata 

Data  yang  dibutuhkan  dalam  pengembangan  sistem  ini 
adalah data aset BMN milik Pusdiklat BPS. Sumber data aset 
BMN  yang  diperlukan  didalam  sistem  informasi  ini  diambil 
dari basis data aplikasi SIMAK BMN yang selama ini dipakai 
untuk  melakukan  pencatatan  aset  BMN  oleh  Pusdiklat. 
Sedangkan  metadatanya  dicari  dari  aplikasi  dan  ditambah 
berbagai sumber dokumentasi yang berkaitan.  

C. Metode Pengumpulan Data 

Pengumpulan  data  untuk  analisis  kebutuhan  sistem 
dilakukan dengan tiga cara, yaitu studi literatur, wawancara dan 
kuesioner. Pada metode studi literatur dilakukan pencarian dan 
pencatatan  sumber-sumber  yang  berhubungan  dengan  topik 
penelitian  ini  berupa  penelitian  terkait  maupun  dokumen 
dokumentasi  yang 
relevan.  Pada  metode  wawancara, 
wawancara dilakukan secara jarak jauh menggunakan aplikasi 
Whatsapp dengan subject matter dari Pusdiklat BPS RI. Dalam 
wawancara  diajukan  berbagai  pertanyaan  kepada  narasumber 
mengenaai sistem yang telah berjalan, kebutuhan pada sistem 

 2 / 9 

 
 
 
informasi  yang  akan  dikembangkan  dan  evaluasi  sistem 
menggunakan  metode  Black-Box  Testing.  Pada  metode 
kuesioner, akan dilakukan survei User Acceptance Test (UAT) 
menggunakan Likert Scale dan  System Usability Scale (SUS) 
kepada  responden  untuk  mengevaluasi  sistem  yang  telah 
dikembangkan. Responden yang dimaksud adalah pihak-pihak 
dengan peran tertentu yang berkaitan dan akan memanfaatkan 
sistem ini dikemudian hari. 

D. Pengembangan Sistem 

Pengembangan 

sistem  mengadopsi  metode  System 
Development  Life  Cycle  (SDLC)  dengan  pendekatan  model 
Waterfall  [9].    Penggunaan  model  ini  dalam  penelitian  ini 
ini  cocok  digunakan  untuk 
disebabkan  karena  model 
jelas 
pengembangan  software  yang  dari  awal  sudah 
kebutuhannya dan tidak banyak perubahan dengan tiap tahapan 
pengembangan dilakukan berturut-turut secara bertahap. Pada 
penelitian  ini  tahapan  model  Waterfall  yang  digunakan 
memiliki  5  tahapan  yaitu  Perencanaan,  Analisis,  Desain, 
Implementasi dan Evaluasi.  

a. Perencanaan 
Pada  tahap  perencanaan  dilakukan  pengumpulan  data  dan 
informasi mengenai sistem yang sedang berjalan, ruang lingkup 
pemeliharaan 
permasalahan 
tujuan 
kegiatan 
dikembangkannya  sistem 
informasi.  Pengumpulan  data 
dilakukan  dengan  melakukan  studi  literatur  dan  wawancara 
kepada subject matter. 

dan 

b. Analisis 
Setelah  melalui  tahap  perencanaan,  didapatkan  informasi 
mengenai sistem yang sedang berjalan dan permasalahan yang 
timbul, kemudian dilakukan tahap analisis. Pada tahap analisis 
dilakukan  analisis  terhadap  sistem  berjalan,  analisis  masalah, 
analisis kebutuhan dan analisis solusi. 

c. Desain 
Pada  tahap  desain  berdasarkan  hasil  analisis  yang  telah 
diperoleh, dilakukan rancangan bisnis proses, arsitektur sistem, 
penggunaan  dn  aktivitas,  basis  data  dan  interface  sistem 
informasi usulan.  

d. Implementasi 
Pada  tahap  implementasi  dilakukan  implementasi  dari 
tahapan desain yang telah dilakukan. Pada tahapan ini, untuk 
mempermudah dan mempercepat proses pengembangan sistem 
ini,  penulis  menggunakan  kerangka  sistem  yang  telah  dibuat 
sebelumnya pada penelitan [8] yang mana memiliki alur kerja 
dan kebutuhan yang cukup mirip dengan penelitian ini.  

e. Evaluasi 
Setelah 

tahap 

implementasi  dilakukan, 

selanjutnya 
dilakukan tahap uji dan evaluasi dengan menggunakan metode 
Black-Box  Testing,  System  Usability  Scale  (SUS)  dan  Likert 
Scale. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

D. Metode Uji dan Evaluasi 

Fungsionalitas  sistem  informasi  akan  diuji  dengan  metode 
Black-box  Testing  [10]  oleh  pengembang  dan  responden. 
Sedangkan  User  Acceptance  Testing  untuk  menguji 
penerimaan pengguna terhadap sistem akan dilakukan dengan 
kuesioner buatan sendiri dan  kuesioner System Usability Scale 
(SUS) [11] dan Likert Scale. 

V. KERANGKA PIKIR 

Gambar 1. Diagram kerangka pikir 

Berdasarkan  masalah  pada  Gambar  1,  dalam 

tujuan 
penelitian 
ini  penulis  menitikberatkan  pada  solusi  atas 
permasalahan  diatas.  Dengan  metode  yang  telah  dipaparkan 
sebelumnya, penulis mencoba membangun sebuah sistem yang 
diharapkan  dapat  mengatasi  permasalahan  yang  berkaitan 
dengan kegiatan pemeliharan aset BMN. 

VI. HASIL DAN PEMBAHASAN 

A. Analisis  

a. Analisis Sistem Berjalan 
Analisis  sistem  berjalan  dilakukan  untuk  mengetahui 
bagaimana tahapan dan peran didalam kegiatan pemeliharaan 
di  Pusdiklat  BPS.  Terdapat  7  aktor  utama  yang  memerankan 
peran dalam kegiatan pemeliharaan aset BMN, yait: Pegawai, 
Staff, Subbagian Rumah Tangga, Teknisi, Operator BMN, PPK, 
PP  dan  Penyedia.  Setiap  aktor  memiliki  peran  yang  berbeda 
dalam 
aset  BMN.  Kegiatan 
pemeliharaan aset BMN dalam analisis ini akan dibagi menjadi 
tiga  kegiatan  utama,  yaitu  pelaporan  keluhan  pegawai  terkait 
aset BMN, perawatan rutin aset BMN dan perbaikan aset BMN 
yang berstatus kondisi rusak ringan.  

kegiatan  pemeliharaan 

Untuk  kegiatan  pertama  yaitu  pelaporan  keluhan  saat  ini 
dilakukan dengan cara pegawai menyampaikan langsung pada 
staff  subbagian  rumah  tangga  mengenai  permasalahan  atau 
kondisi  aset  yang  bermasalah,  lalu  staff  akan  berkoordinasi 
dengan  teknisi  untuk  melakukan  pengecekan  aset  yang 

 3 / 9 

 
 
 
 
 
 
 
 
 
 
 
 
dilaporkan bermasalah tadi. Kemudian jika teknisi menemukan 
bahwa aset sudah tidak dapat diperbaiki lagi, maka teknisi akan 
menyampaikan pada operator BMN bahwa kondisi aset sudah 
tidak dapat dipakai sehingga operator BMN akan memperbarui 
status kondisi aset BMN pada aplikasi SIMAK BMN menjadi 
rusak  berat,  namun  jika  aset  masih  memungkinkan  untuk 
diperbaiki maka teknisi akan mencoba memperbaikinya dahulu 
sampai  kondisi  aset  BMN  pulih  dan  menyelesaikan  laporan 
yang masuk, namun jika teknisi tidak mampu memperbaikinya 
sendiri,  maka  teknisi  akan  membuat  rekomendasi  untuk 
melakukan  pengadaan  perbaikan  aset  BMN  kepada  PPK  dan 
memberitahukan  kepada  operator  BMN  yang  kemudian  akan 
memperbarui  status  kondisi  aset  BMN  pada  aplikasi  menjadi 
rusak ringan.  

Pada  kegiatan  perawatan  rutin  aset,  PPK  akan  membuat 
pengadaan  perawatan  aset  BMN  dan  kemudian  meminta  PP 
untuk  mencarikan  penyedia  yang  berkapasitas  untuk 
melakukan  perawatan  rutin  pada  aset  BMN  yang  telah 
ditentukan,  lalu  penyedia  yang  terpilih  melakukan  perawatan 
sesuai  kesepakatan  yang  dibuat  pada  periode  yang  telah 
ditentukan. Jika dalam masa perawatan oleh penyedia, ternyata 
aset  yang  dirawat  mengalami  kerusakan  dan  memerlukan 
perbaikan  maka  penyedia  akan  mengajukan  penawaran 
perbaikan  pada  PPK,  jika  PPK  menyetujui  maka  akan 
dilakukan  perbaikan  oleh  penyedia.  Setelah  habis  masa 
perawatan  aset  maka  PPK  akan  memperpanjang  kontrak 
dengan penyedia yang sama atau mencari penyedia lain untuk 
melanjutkan kegiatan pemeliharaan aset BMN. Pada kegiatan 
perbaikan  aset,  hampir  mirip  dengan  kegiatan  perawatan 
dengan  sedikit  perbedaan  yaitu  pada  kesepakatan  masa 
perbaikan  yang  tidak  berkala  seperti  yang  terjadi  pada 
perawatan rutin namun hanya sesuai kebutuhan perbaikan saja.  

b. Analisis Masalah 
Dari  hasil  analisis  sistem  berjalan  tersebut  kemudian 
dilakukan  analisis  masalah  yang  timbul  atau  yang  berpotensi 
terjadi menggunakan Ishikawa Diagram. Berikut permasalahan 
dalam  kegiatan  pemeliharaan  aset  BMN  yang  digambarkan 
dalam Ishikawa Diagram pada Gambar 2. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 2. Ishikawa (Fishbone) diagram 

c. Analisis Kebutuhan 
Dari analisis masalah tersebut, kemudian dilakukan analisis 
kebutuhan  terhadap  sistem  informasi  yang  diuraikan  sebagai 
berikut. 

a.  Modul  untuk  memantau  informasi  mengenai  aset 
BMN yang terdaftar dalam aplikasi SIMAK BMN. 

b.  Modul  untuk  mempermudah  koordinasi  pelaporan 
keluhan aset BMN dan kegiatan perawatan rutin dan 
perbaikan aset BMN. 

c.  Modul  untuk  memantau  aktifitas  pemeliharaan  yang 

dilakukan oleh penyedia. 

d. 

Implementasi,  penggunaan,  dan  pengembangan 
aplikasi  mudah  dan  fleksibel.  Akses  aplikasi  dapat 
dilakukan melalui berbagai jenis gawai.. 

d. Analisis Solusi 
Berdasarkan  analisis  kebutuhan  yang  telah  dibuat  diatas, 
dilakukan  analisis  solusi  untuk  mendapatkan  solusi  dari 
masalah  yang  dapat  dilakukan  sesuai  dengan  kebutuhan 
pengguna. Pertama, mengenai platform, solusi yang disarankan 
adalah  mengembangkan  sistem  informasi  berbasiskan  web. 
Selanjutnya mengenai modul yang dibutuhkan oleh pengguna, 
sistem yang dikembangkan akan memuat modul-modul sebagai 
berikut : 

a.  Modul kelola laporan 

b.  Modul kelola pemeliharaan aset BMN   

c.  Modul administrasi sistem 

 4 / 9 

 
 
 
 
 
 
 
 
 
 
B. Rancangan Sistem Informasi Usulan 

Gambar 3. Digram konteks sistem usulan 

a. Rancangan Bisnis Proses 
Perbedaan  utama  bisnis  proses  sistem  usulan  dan  sistem 
berjalan  yaitu  pada  kegiatan  pelaporan  keluhan  pegawai, 
laporan  tidak  disampaikan  kepada  staff  subbagian  rumah 
tangga namun disampaikan langsung kepada kepala subbagian 
rumah  tangga  yang  akan  mengkoordinasikan  teknisi  dan 
operator BMN yang akan menangani laporan yang masuk. Lalu 
pada  kegiatan  pemeliharaan  (perawatan  rutin  dan  perbaikan) 
ditambahkan prosedur  yang mana penyedia diharuskan untuk 
menglampirkan  bukti  pemeliharaan  berupa  foto  dokumentasi 
kegiatan dan hasil pemeliharaan untuk keperluan pemantauan 
kinerja  penyedia  dan  penilaian  kinerja  penyedia  oleh  PPK 
sebagai bahan evaluasi kinerja penyedia.  

b. Rancangan arsitektur  
Desain  rancangan  arsitektur  sistem 

informasi  usulan 
menggunakan  pendekatan  client-server.  dengan  gambaran 
sebagai berikut. 

Gambar 4. Rancangan arsitektur sistem 

Sistem  ini  akan    diintegrasikan  dengan  basis  data  aplikasi 
SIMAK  BMN  untuk  mendapatkan  data  aset  yang  terdaftar, 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

namun  saat  ini  basis  data  tersebut  hanya  tersedia  lokal  di 
komputer operator BMN. Agar basis data SIMAK BMN dapat 
tersedia  didalam  jaringan  Pusdiklat,  maka  penulis  akan 
mencoba  memindahkan  basis  data  dari  komputer  pegawai ke 
server  Pusdiklat  kemudian  melakukan  konfigurasi  pada 
aplikasi SIMAK BMN agar menghubungkan aplikasi basis data 
yang telah dipindahkan ke server Pusdiklat.   

c. Rancangan penggunaan dan aktivitas 
Use  case  diagram  sistem  informasi  usulan  dirancang 
mempunyai  tujuh  aktor  yang  memiliki  peran  masing-masing 
dalam  menjalankan  fitur-fitur  pada  modul  yang  terdapat  di 
dalam  sistem  informasi  yang  dikembangkan.  Terdapat  tujuh 
aktor  sebagai  pengguna,  yaitu  Pegawai,  Kepala  Subbagian 
Rumah  Tangga (KasubagRT),  Teknisi,  Operator  BMN,  PPK, 
PP dan Penyedia.  

Gambar 5. Use case diagram 

Seluruh aktor kecuali Pegawai dan Penyedia memiliki status 
sebagai admin dalam sistem yang mengizinkan pengguna untuk 
mengakses modul kelola pengguna, kelola bagian, daftar BMN 
dan  hasil  kerja.  Aktor  pegawai  memiliki  akses  modul  buat 
laporan  dengan  fitur  utama  untuk  membuat,  menghapus  dan 
mengedit laporan keluhan. Seluruh pengguna kecuali penyedia 
memiliki  hak  akses  didalam  sistem  sebagai  pegawai. 
KasubagRT memiliki akses modul kelola laporan dengan fitur 
utama untuk menerima dan memilih teknisi dan operator yang 
menangani  atau  menolak  laporan  pegawai.  Teknisi  memiliki 
akses modul kelola laporan dengan fitur utama untuk mengecek 
laporan  dan  mengirim  peremintaan  pengadaan  kepada  PPK.  
Operator  BMN  memiliki  akses  modul  kelola  ubah  kondisi 
dengan fitur utama untuk mengupdate status perubahan kondisi 

 5 / 9 

 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

aset  BMN  di  aplikasi  SIMAK  BMN.      PPK  memiliki  akses 
modul kelola pemeliharaan dengan fitur utama untuk membuat, 
menghapus  dan  mengedit  kegiatan  perawatan  dan  perbaikan 
aset juga memantau dan menilai kinerja penyedia.  PP memiliki 
akses  modul  kelola  penyedia dan  modul  kelola pemeliharaan 
dengan fitur utama untuk menambahkan atau memilih penyedia 
yang akan menangani kegiatan pemeliharaan yang dibuat oleh 
PPK.  

d. Rancangan basis data 
Ada  tiga  tahapan  desain  rancangan  yang  dilakukan  dalam 
membangun basis data, yaitu rancangan konseptual, rancangan 
logikal,  dan 
fisik.  Berikut  diagram  Entity 
Relationship Diagram (ERD) rancangan fisik basis data SIKOP 
BMN. 

rancangan 

Gambar 7. Rancangan UI 

C. Implementasi Sistem Informasi Usulan 

Gambar 6. Entity relationship diagram  

Sistem juga memanfaatkan basis data dari aplikasi SIMAK 
BMN  untuk  mendapatkan  informasi  mengenai  data  aset 
terdaftar.  Dalam  implementasinya  untuk  mendapatkan  query 
yang paling optimum dalam menampilkan data dari basis data 
SIMAK BMN, maka ada sedikit modifikasi penambahan tabel 
trigger  untuk 
pembantu 
mengupdate tabel pembantu tersebut setiap ada transaksi data 
yang terjadi pada tabel utama (t_masteru) pada struktur  basis 
data  bawaan  SIMAK  BMN  tetap  tidak  mempengaruhi  bisnis 
proses  dari aplikasi asal sehingga tidak mengganggu jalannya 
aplikasi SIMAK BMN. 

(t_flag_kor)  dan  penggunaan 

e. Rancangan antarmuka 
Dalam penelitian, rancangan antarmuka dibuat mirip seperti 
sistem yang dibuat pada penelitian [8] karena desain antarmuka 
yang digunakan dinilai sudah cukup baik untuk menjadi acuan 
desain  antarmuka  penelitian  ini.  Secara  umum,  rancangan 
antarmuka dalam sistem ini ialah pada Gambar 7. 

yang 

Sistem 

informasi 

dikembangkan 
usulan 
diimplementasikan dalam aplikasi berbasis web. Implementasi 
laptop  dengan 
dilakukan  pada  perangkat  keras  berupa 
spesifikasi  processor  Intel  Core  i7-4510U,  RAM  16GB,  dan 
Dual  GPU  Intel  Integrated  Graphics  |  Radeon  HD  8550M. 
Perangkat lunak yang digunakan untuk pengembangan sistem 
informasi  antara  lain  XAMPP  7.3.27  sebagai  penyedia  web 
server  Apache  HTTP  Server  dan  basis  data  MySQL  lokal 
dengan bahasa pemrograman PHP versi 7.2, Codeigniter 3.1.11 
sebagai  framework  aplikasi  berbasis  web  dengan  arsitektur 
MVC, Bootstrap sebagai framework front-end aplikasi berbasis 
web  untuk  mempercepat  pengembangan  antarmuka  web, dan 
perangkat  lunak  lainnya  seperti  library  kode  program  yang 
mendukung  pengembangan  sistem  informasi.  Berikut  ini 
telah 
beberapa  contoh 
diselesaikan. 

implementasi  antarmuka  yang 

 6 / 9 

 
 
 
 
 
 
Gambar 8. Antarmuka halaman daftar BMN 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

D. Pengujian dan Evaluasi Sistem Informasi Usulan 

Dari  17  responden  yang  terpilih  dari  7  peran  yang  ada 
didalam sistem,  sebanyak 12 orang dari 5 peran mengirimkan 
respon pengujian dengan rincian sebagai berikut. 

a. Pegawai  

: 5 dari 7 orang terpilih 

b. KSBRT  

: 0 dari 1 orang terpilih 

c. PPK  

d. PP   

: 1 dari 1 orang terpilih 

: 1 dari 1 orang terpilih 

Gambar 9. Antarmuka halaman hasil kerja 

e. Teknisi  

: 0 dari 2 orang terpilih 

f. Operator BMN  

: 2 dari 2 orang terpilih 

g. Penyedia  

: 3 dari 3 orang terpilih 

a. Black-box Testing 
Hasil  pengujian  black-box  testing  dengan  48  (empat puluh 
delapan)  skenario  kejadian  berdasarkan 
tahapan  dalam 
menjalankan  fungsi/  fitur  utama.  Dari  hasil  akhir  pengujian 
tersebut didapat bahwa fungsi/ fitur utama yang terdapat dalam 
aplikasi sesuai dengan yang diharapkan. 

Gambar 10. Antarmuka halaman buat laporan pegawai 

Gambar 11. Antarmuka halaman kelola laporan PPK 

b. User Acceptance Test (UAT) 

1. Likert Scale 

 Komponen evaluasi:  

- Bisnis Proses Usulan  

- Sistem Usulan  

 Indikator evaluasi: 

- Kemudahan 

- Kecepatan 

- Kenyamanan 

- Fungsionalitas 

 Hasil evaluasi: 

     Evaluasi Bisnis Proses Usulan (BPU) 

- Skor Kemudahan  

: 0,89 

- Skor Kecepatan 

: 0,87 

- Skor Kenyamanan 

: 0,88 

Gambar 12. Antarmuka halaman kelola pemeliharaan PPK 

- Skor Fungsionalitas 

: 0,925 

Skor Rata-Rata BPU: 0,891 

 7 / 9 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
     Evaluasi Sistem Informasi Usulan (SIU) 

- Skor Kemudahan  

: 0,706 

- Skor Kecepatan 

: 0,722 

- Skor Kenyamanan 

: 0,702 

- Skor Fungsionalitas 

: 0,833 

Skor Rata-Rata SIU: 0,741 

Skor Agregat Rata-Rata: 0.816 

     Interpretasi Skor : 

0,0  -   0,2  :  Sangat Buruk 

0,2  -   0,4  :  Buruk 

0,4  -   0,6  :  Cukup 

0,6  -   0,8  :  Baik 

0,8  -   1,0  :  Sangat Baik 

2. System Usability Scale (SUS) 

Berikut  merupakan 

tabel  skor  yang  didapatkan  dari 

pengujian sistem dengan menggunakan kuesioner SUS. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Re
sp
on
de
n 

(1) 

Nilai Skala Pertanyaan 

1 

2 

3 

4 

5 

6 

7 

8 

9 

10 

(2) 

(3) 

(4) 

(5) 

(6) 

(7) 

(8) 

(9) 

(10
) 

(1
1) 

Rata-rata 

Skor* 
(2,5) 

(12) 

71,6 

Hasil  dari  pengujian  UAT  menggunakan  kuesioner  buatan 
sendiri  dan  SUS.  Pada  kuesioner  buatan  sendiri  ada  dua 
komponen yang dievaluasi, yaitu Bisnis Proses Usulan (BPU) 
yang  mendapatkan  skor  rata-rata  sebesar  0,891  dengan 
interpretasi  sangat  baik  dan  Sistem  Informasi  Usulan  (SIU) 
mendapatkan  skor rata-rata  sebesar  0,741  dengan  interpretasi 
baik,  serta  skor  agregat  sebesar  0,816  dengan  interpretasi 
sangat  baik.  Pada  pengujian  menggunakan  kuesioner  SUS 
diperoleh  skor  rata-rata  sebesar  71,5.  Dari  perolehan  skor 
tersebut,  berdasarkan  penelitian  Bangor  [12],  skor  yang 
didapatkan  sistem  ini  berada  diatas  rata-rata  skor  SUS  hasil 
yang didapatkan sistem web dan aplikasi (68). Sehingga dapat 
disimpulkan bahwa sistem informasi yang telah dikembangkan 
masuk kedalam rentang penerimaan yang cukup oleh pengguna.  

TABEL II 
HASIL SKOR SUS 
Nilai Skala Pertanyaan 

1 

2 

3 

4 

5 

6 

7 

8 

9 

10 

(2) 

(3) 

(4) 

(5) 

(6) 

(7) 

(8) 

(9) 

(10
) 

(1
1) 

3 

4 

3 

3 

2 

4 

3 

4 

4 

1 

2 

4 

4 

4 

3 

3 

3 

4 

2 

3 

3 

3 

2 

1 

4 

4 

3 

3 

3 

3 

2 

4 

3 

3 

2 

3 

4 

4 

4 

3 

3 

4 

2 

1 

3 

3 

1 

0 

3 

4 

4 

3 

3 

3 

3 

3 

3 

3 

2 

3 

3 

4 

3 

3 

3 

4 

3 

2 

3 

3 

2 

1 

4 

4 

3 

3 

3 

3 

2 

3 

4 

2 

2 

3 

4 

4 

2 

3 

3 

3 

2 

4 

3 

3 

2 

0 

4 

4 

3 

3 

3 

4 

2 

3 

4 

3 

3 

3 

3 

4 

3 

3 

4 

3 

1 

1 

0 

1 

1 

1 

Re
sp
on
de
n 

(1) 

1 

2 

3 

4 

5 

6 

7 

8 

9 

10 

11 

12 

VII. PENUTUP 

Skor* 
(2,5) 

(12) 

90 

100 

77,5 

75 

75 

85 

55 

70 

72 

62,5 

47,5 

47,5 

Kesimpulan  dari  penelitian  ini  adalah  telah  dilakukan 
pengembangan Sistem Informasi Kontrol Pemeliharaan Barang 
Milik Negara (SIKOP-BMN) untuk membantu proses kontrol 
  Tahap 
kegiatan  pemeliharaan  aset  BMN  di  Pusdiklat. 
pengembangan  sistem  telah  dilakukan  sampai  tahap  evaluasi 
sistem  dan  dihasilkan  skor  agregat  sebesar  0,816  dengan 
interpretasi  bahwa  bisnis  proses  dan  sistem  informasi  usulan 
yang telah di buat sangat baik diterima oleh pengguna dan skor 
rata-rata SUS sebesar 71,5 yang mengindikasikan bahwa sistem 
berada dalam rentang penerimaan yang cukup oleh pengguna. 
Saran  untuk  penelitian  selanjutnya  adalah  menambahkan 
modul tambahan lain selain pemeliharaan untuk meningkatkan 
fungsionalitas  sistem  seperti  permintaan  pengadaan  langsung 
oleh pegawai 

DAFTAR PUSTAKA 

[1]Republik  Indonesia  (2014).  Undang-Undang  No.  4  Tahun  2004  tentang 
Pengelolaan Barang Milik Negara/ Daerah. Jakarta: Sekretriat Negara. 

[2]________________  (1945).  Undang-Undang  Dasar  1945  Amendemen  IV 
Bab VIII Pasal 23 tentang Anggaran Pendapatan dan Belanja Negara.  

[3]DJPB  Kemenkeu  RI.  (2020,  3)  Update  Terbaru  Aplkasi  SIMAK  BMN 
[Online]. Available: https://djpb.kemenkeu.go.id/portal/id/unduh/aplikasi/162-
aplikasi-simak.html 

[4]BMN  BMKG. 
https://bmn.bmkg.go.id/simak-bmn/ 

(2017,  3)  SIMAK  BMN 

[Online].  Available:  

[5]Safitri, Amelia (2016). Sistem Informasi Pengelolaan BMN di BPS [Skipsi]. 
Jakarta: Politeknik Statistika STIS. 

 8 / 9 

 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

[6]Afaf,  M.N.  (2019).  Pengembangan  Sistem  Informasi  Pengelolaan  BMN 
Politeknik Statistika STIS [Skipsi]. Jakarta: Politeknik Statistika STIS. 

[7]Yusnianda,  Najwan  (2018).  Pembangunan  Sistem  Informasi  Pengelolaan 
Pengadaan Langsung di BPS RI [Skripsi]. Jakarta: Politeknik Statistika STIS. 

[8]Fikri, Rozan (2020). Pengembangan Aplikasi Sistem Informasi Pengadaan 
Langsung (SIPELANG) di Badan Pusat Statistik. Jakarta: Politeknik Statistika 
STIS. 

[9]Y.  Bassil, “A  simulation model for the  waterfall  software  development  
life cycle,”  The  International  Journal  of Engineering & Technology, vol. 2, 
no. 5, pp. 742-749, 2012. 

[10]Nidhra,  S.  (2012).  Black  Box  and  White  Box  Testing  Techniques  -  A 
Literature  Review.  International  Journal  of  Embedded  Systems  and 
Applications. 2. 29-50. 10.5121/ijesa.2012.2204.  

[11]Brooke,  J.  (1996).  SUS  -  A  quick  and  dirty  usability  scale.  Usability 
evaluation in industry, 189 (194), 4-7. 

[12]Bangor, A., Kortum, P. T., & Miller, J. T. (2008). An Empirical Evaluation 
of  the  System  Usability  Scale.  International  Journal  of  Human–Computer 
Interaction, 24(6), 574–594.  

 9 / 9 

 
 
 
"
221709719,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pengembangan Sistem Informasi Manajemen Arsip 
Berbasis Web Pada Pusdiklat BPS 

Grandis Zahron Rusydi (221709719, 4SI1) 

Dosen Pembimbing: Ibnu Santoso, S.S.T., M.T. 

Ringkasan—  Arsip  merupakan  rekaman  kegiatan  atau 
peristiwa dalam berbagai bentuk yang dikelola dengan baik dan 
teliti.  Pengelolaan  arsip  Pusdiklat  BPS  masih  dikelola  dengan 
menggunakan aplikasi Microsoft Excel dan file scan arsip hanya 
disimpan  dalam  folder  di  satu  komputer  saja  serta  data  arsip 
yang  digunakan  belum  terintegrasi  antar  pegawai  arsip. 
Pengelolaan  seperti  ini  menyebabkan  lamanya  dalam  proses 
pengelolaan  arsip  baik  dari  pengumpulan,  pelaporan,  atau 
pencarian arsip dan dapat menyebabkan terjadi kehilangan data 
atau  file  arsip.  Maka  peneliti  akan  mengembangkan  sistem 
informasi  manajemen  berbasis  web  dengan  menggunakan 
metode  System  Development  Life  Cycle  (SDLC)  model  prototipe 
dengan menggunakan bahasa pemrograman PHP. Pada evaluasi 
sistem  pada  penelitian ini  menggunakan  black  box testing,  SUS, 
dan pencocokan dokumen SRS dengan mendapatkan hasil yang 
sesuai  dengan  harapan  pengguna  dan  pada uji  SUS  didapatkan 
skor sebesar 84.5. Selain ketiga evaluasi tersebut, terdapat surat 
pernyataan dari pihak pusdiklat yang menyatakan bahwa sistem 
yang  dibuat  sudah  dapat  mengefisienkan  pekerjaan  dalam 
pengelolaan  arsip  dan  memenuhi  persyaratan  yang  diminta  di 
Pusdiklat BPS. 

Kata Kunci— Arsip, web, pengelolaan, SDLC. 

I.  LATAR BELAKANG 

Kebutuhan akan informasi semakin berkembang pesat pada 
era  globalisasi  seperti  saat  ini,  sehingga  banyak  perusahaan 
yang  terus-menerus  melakukan  perubahan  misalnya  sistem 
yang  terkomputerisasi  di  dalam  perusahaannya.  Hal  tersebut 
dapat  meningkatkan  persaingan  serta  kualitas  kerja  antar 
perusahaan. Teknologi menjadi pokok utama dalam melakukan 
pekerjaan,  seperti  adanya  komputer  dan  jaringan  internet. 
Namun ada beberapa perusahaan dalam pemanfaatan teknologi 
kurang  bisa maksimal  seperti dalam  pengelolaan arsip  secara 
web  atau  di  berbagai  bidang  yang  berhubungan  dengan 
kearsipan  masih  sedikit  yang  mengaplikasikan  teknologi  ini. 
Maka  dari  itu  sistem  informasi  sangat  diperlukan  guna 
mengelola informasi yang teliti, cepat, dan tepat. 

Menurut Undang-Undang Nomor 7 tahun 1971, arsip adalah 
naskah-naskah yang dibuat dan diterima oleh lembaga-lembaga 
dan badan-badan pemerintah dalam bentuk corak apapun, baik 
dalam  keadaan  tunggal  maupun  berkelompok  dalam  rangka 
pelaksanaan kegiatan pemerataan. 

Dalam instansi pemerintah khususnya Badan Pusat Statistik 
(BPS),  mempunyai  kegiatan  salah  satunya  di  bidang 
pendidikan dan latihan yang dilakukan oleh Pusat Pendidikan 
dan  Pelatihan  (Pusdiklat)  BPS.  Fungsi  Pusdiklat  BPS  dalam 
pelaksanaan  urusan  tata  usaha  tidak  terlepas  dari  kegiatan 
penatausahaan arsip. Pada pelaksanaan kegiatan pencatatan dan 
pengolahan  arsip, 
ini  masih  dilakukan  dengan 
menggunakan  aplikasi  Microsoft  Excel.  Hal  ini  menjadi 
masalah ketika komputer yang digunakan untuk mencatat arsip 

saat 

sedang  rusak,  sehingga  data  dari  pencatatan  bisa  saja  hilang. 
Masalah lain yang dihadapi adalah file arsip yang telah di scan 
hanya  disimpan  dalam  folder  komputer  saja  setiap  unit  kerja 
sehingga sering terjadi lambatnya proses dalam pencarian  file 
scan arsip tersebut karena file yang terlalu banyak. Selain hal 
tersebut, data atau file arsip yang digunakan belum terintegrasi 
antar  pegawai  arsip  oleh  sebab  itu  menyebabkan  lambatnya 
pelaporan arsip atau pengumpulan atau pencarian arsip. 

Berdasarkan Peraturan Kepala Badan Pusat Statistik Nomor 
99  Tahun  2018  tentang  Pedoman  Klasifikasi  Arsip  di 
Lingkungan  Badan  Pusat  Statistik  dan  Peraturan  Kepala 
Badan Pusat Statistik Nomor 95 Tahun 2018 tentang Pedoman 
Arsip  Aktif  di  Lingkungan  Badan  Pusat  Statistik,  maka perlu 
dilakukan  penataan  kearsipan  sesuai  dengan  perkembangan 
dan  teknologi  serta  perlunya  dikembagakan  sistem  informasi 
manajemen  arsip  dilingkup  Pusdiklat  BPS.  Sistem  informasi 
ini  akan  dibentuk  dengan  sistem  berbasis  web  yang  dapat 
dijalankan melalui komputer personal (PC). 

Sistem  informasi  ini  dikembangkan  menggunakan  bahasa 
pemrograman  PHP  dengan  database  MySQL.  Adapun  sistem 
informasi  manajemen  arsip  berbasis  web  diharapkan  dapat 
membantu  Pusdiklat  BPS  dalam  mengefisienkan  pengelolaan 
arsip dan pendokumentasian file scan dengan penyimpanan file 
digital pada sistem. 

II.  TUJUAN PENELITIAN 

umum 

Tujuan  penelitian  ini  meliputi  tujuan  umum  dan  khusus. 
adalah 
penelitian 
Tujuan 
mengembangkan sistem manajemen arsip Pusdiklat BPS yang 
telah ada dengan cara membuat sistem yang baru dengan web 
untuk mengefisienkan pekerjaan dalam pengelolaan arsip. 

skripsi 

dari 

ini 

Adapun tujuan khusus dari penelitian ini, yaitu 
1.  Melakukan 
analisis 

dengan 
dan 
menggambarkan proses bisnis sistem berjalan saat ini 
sebagai dasar pembuatan alternatif solusi yaitu sistem 
usulan. 

identifikasi 

2.  Merancang  sistem  usulan  yang  meliputi  rancangan 
jaringan, 

alur  dan  prosedur  usulan,  arsitektur 
rancangan basis data, dan rancangan antar muka. 

3.  Membuat  sistem  usulan  yang 

terintegrasi  antar 
pegawai arsip yang dapat login di komputer manapun. 

4.  Membuat penyimpanan file arsip yang telah di  scan 

secara digital. 

5.  Melakukan  uji  coba  hasil  perancangan  sistem  serta 
evaluasi  atau  perbaikan  terhadap  sistem  usulan  yang 
telah dikembangkan. 

1 / 9 

 
 
 
 
 
III. PENELITIAN TERKAIT 

IV. METODE PENELITIAN 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Metode  yang  dipergunakan  dalam  penelitian  ini  adalah 
system  development  life  cycle  (SDLC)  model  prototipe. 
Prototipe  adalah  satu  versi  dari  sebuah  sistem  potensial  yang 
memberikan  ide  bagi  para  pengembang  dan  calon  pengguna, 
bagaimana  sistem  akan  berfungsi  dalam  bentuk  yang  telah 
selesai (Raymond, 2007). 

Komentar 

Peneliti 
mengambil 
bahasa 
pemrograman 
PHP dan 
database 
MySQL. 

No 

Judul 

1  Sistem 

informasi 
pengarsipan 
dokumen 
berbasis web 

TABEL I 
PENELITIAN TERKAIT 
Penulis, 
Publikasi 
Agustina 
Simangunsong 
, Jurnal 
Mantik Penusa 
Volume 2 
No.1, Juni 
2018 

Tertulis 

Menggunakan 
bahasa 
pemrograman 
PHP dan 
database 
MysQL 
dengan 
metode 
pengumpulan 
data dengan 
studi 
kepustakaan, 
wawancara, 
observasi, dan 
dokumen 

2  Sistem 

informasi 
penatausahaan 
arsip online (Si- 
Pentol) pada 
Dinas 
Perhubungan 

3.  Analisis dan 
perancangan 
sistem 
pengelolaan 
arsip berbasis 
web (Studi 
Kasus: KPU 
Kabupaten 
Tebo) 
4.  Perancangan 
sistem 
informasi 
pengelolaan 
arsip berbasis 
web (studi 
kasus: Badan 
Kearsipan 
Daerah Provinsi 
Jawa Barat) 

5  Rancang 

bangun sistem 
penerimaan 
mahasiswa baru 
online dengan 
model sdlc 
model prototipe 
di universitas 
islam syekh- 
yusuf 

Gunawan dan 
Muhammad 
Multazam, 
Jurnal Speed 
(Sentra 
Penelitian 
Engineering 
dan Edukasi) 
Volume 10 No 
1, 2018 
Basri dan Jovi 
Devitra, Jurnal 
Manajemen 
Sistem 
Informasi 
Volume 2 No 
1, Maret 2017 

Devie 
Firmansyah, 
Jurnal LPKIA 
Volume 4 No 
1, Juni 2014 

Taufik 
Hidayat dan 
Sukisno, 
Jurnal Pelita 
( Penelitian 
dan Karya 
Ilmiah) Edisi 
XVIII Vol 2, 
Juli-Desember 
2018 

Merancang 
aplikasi 
menggunakan 
web dan 
android 

Peneliti 
mengambil 
metode 
dengan web 
saja. 

Peneliti 
mengambil 
teknik 
pemodelan 
berbasis objek. 

Menggunakan 
bahasa 
pemrograman 
PHP dengan 
teknik 
pemodelan 
berbasis objek. 

Menggunakan 
sistem 
berbasis web 
dengan 
menggunakan 
macromedia 
dreamweaver 
dan database 
server 

Menggunakan 
salah satu 
metode dari 
sdlc yaitu 
model 
prototipe 

Peneliti 
mengambil 
metode 
dengan web 
beserta 
database 
server 

Peneliti 
mengambil 
metode sdlc 
model 
prototipe 

Gambar 1. SDLC Model Prototipe 

Berikut  merupakan  tahapan-tahapan  dalam  mekanisme 
pengembangan  sistem  yang  dibuat,  tahapan  tahapan  tersebut 
adalah: 

1.  Mengidentifikasikan kebutuhan pemakai 

Pada  tahap  ini  pengembang  dan  pelanggan  bersama 
sama  mendefinisikan  garis  sistem  yang  akan  dibuat 
format  seluruh  perangkat 
lunak  ataupun  semua 
kebutuhan untuk untuk mendapatkan gagasan dan yang 
sesuai diinginkan pemakai terhadap sistem. 

2.  Membangun prototipe 

Pada  tahap  ini  berfokus  untuk  menganalisis  sistem 
yang  dimungkinkan  bekerja  sama  dengan  spesialis 
informasi lainnya, dan membuat perancangan sementara 
yang  berfokus  pada  penyajian  pada  pengguna  atau 
client. 

3.  Evaluasi prototipe 

Pada  tahap  evaluasi,  akan  dilakukan  oleh  pengguna 
atau  client.  pengguna  akan  mempertanyakan  apakah 
prototipe  yang  sudah  dibangun  sudah  sesuai  dengan 
keinginan pengguna. Jika sudah sesuai maka langkah 4 
akan  diambil.  Jika  tidak  prototipe  tidak  sesuai,  maka 
dilakukan revisi dengan mengulang langkah 1, 2, dan 3. 

4.  Pengodean 

Dalam  tahap  ini  prototipe  yang  sudah  disepakati 
diterjemahkan  ke  dalam  bahasa  pemrograman  yang 
sesuai. 

5.  Menguji sistem 

Pada tahap ini sistem sudah menjadi suatu perangkat 
lunak  yang  siap  pakai,  harus  di  tes  dahulu  sebelum 
digunakan. Pengujian ini dilakukan dengan black box. 

6.  Evaluasi sistem 

Dalam tahap evaluasi sistem, pengguna mengevaluasi 
apakah  sistem  yang  sudah  jadi  sudah  sesuai  dengan 
yang  diharapkan.  Jika  ya,  langkah  7  dilakukan;  jika 
tidak, ulangi langkah 4 dan 5. 

7.  Menggunakan sistem perangkat lunak 

Perangkat lunak yang telah diuji dan diterima client 

atau pengguna siap untuk digunakan. 

2 / 9 

 
 
 
  
 
 
 
V.  KERANGKA PIKIR 

Untuk  membantu  dalam  penyusunan  penelitian  ini,  maka 
perlu  adanya  susunan  kerangka  pikir  yang  jelas  tahapan- 
tahapannya.  Kerangka  pikir  ini  merupakan  langkah-langkah 
yang  akan  dilakukan  dalam  penyelesaian  masalah  yang  di 
Pusdiklat  BPS  bagian  arsip,  berikut  adalah  gambar  dari 
kerangka pikir. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

sedangkan  bendahara/pegawai  selain  bagian  arsip 
bertugas  menyerahkan 
telah 
ditandatangani  oleh  pihak  terkait  kepada  pegawai 
bagian  arsip.  Berikut  ini  adalah  gambar  diagram  alir 
dari proses bisnis pengelolaan arsip pusdiklat BPS. 

dokumen 

yang 

Gambar 2. Kerangka Pikir 

Pada  Gambar  2  menjelaskan  kerangka  pikir  untuk 
menghasilkan  suatu  aplikasi  yang  membutuhkan  beberapa 
komponen,  yaitu  analisis  kebutuhan,  perancangan  sistem, 
metode  penelitian,  dan  metode  pengumpulan  data.  Aplikasi 
yang  diciptakan  diharapkan  sudah  terintegrasi  dan  dapat 
mengefisienkan pengelolaan arsip secara cepat. 

VI. HASIL DAN PEMBAHASAN 

1.  Analisis Sistem Berjalan 

Analisis  sistem  berjalan  dilakukan  dengan  metode 
wawancara  terhadap  subject  matter.  Pada  wawancara 
diketahui sistem yang berjalan masih dilakukan dengan 
menggunakan  Microsoft  Excel  sebagai  pencatatan 
dokumen arsip dan penyimpanan file scan masih dalam 
satu folder di komputer personal (PC). Selain itu, dalam 
pengelolaanya belum terintegrasi antar pegawai arsip. 

Sistem  yang  sedang  berjalan  saat  ini,  memiliki  dua 
aktor dalam pengelolaan arsip, yaitu pegawai arsip, dan 
bendahara/pegawai  lain.  Pegawai  arsip  bertugas  untuk 
mengentri data atau dokumen arsip dan menyimpannya, 

Gambar 3. Proses Bisnis Sistem Berjalan 

Setelah  dilakukan  analisis  sistem  berjalan,  terdapat 
beberapa  masalah  atau  kendala  yang  terjadi.  Kendala 
atau masalah tersebut dianalisis menggunakan fishbone 
diagram. Berikut merupakan gambar fishbone diagram. 

Gambar 4. Fishbone diagram 

3 / 9 

 
 
 
 
 
 
 
 
 
 
2.  Analisis Kebutuhan Sistem 

Pada  analisis  sistem  berjalan  didapatkan  gambar 
umum mengenai permasalahan pada sistem yang telah 
ada.  Oleh  karena  itu,  pada  sistem  yang  dibuat  ini 
diharapkan  dapat  memenuhi  kebutuhan.  Kebutuhan 
tersebut  terbagi  menjadi  dua,  meliputi  kebutuhan 
fungsional dan nonfungsional. 
a.  Kebutuhan fungsional 

input 

Kebutuhan  fungsional  adalah  pernyataan 
layanan sistem yang harus disediakan, bagaimana 
sistem  bereaksi  pada 
tertentu  dan 
bagaimana  perilaku  sistem  pada  situasi  tertentu. 
Pada kebutuhan fungsional berisi tentang proses- 
proses  apa  saja  yang  nantinya  dilakukan  oleh 
sistem.  Kebutuhan 
sistem 
informasi  manajemen  arsip  Pusdiklat  BPS  yang 
saat ini berjalan adalah sebagai berikut. 

fungsional  dari 

i 

ii 

iii 

Sistem yang mampu mengakomodir proses 
kegiatan  arsip  Pusdiklat  BPS  yang  efektif 
dan  efisien  dengan  penggunaan  informasi 
yang  terintegrasi  untuk  semua  pengguna 
yang mengurus arsip Pusdiklat BPS. 
Sistem  mampu  melakukan  pembuatan 
laporan  arsip  yang  cepat  dari  segi  waktu 
maupun kebutuhan sehingga laporan dapat 
dibuat  kapan  saja  pada  waktu  yang 
dibutuhkan. Pembuatan laporan juga dapat 
disesuaikan  dengan  kebutuhan  pengguna 
arsip Pusdiklat BPS. 
Sistem  dapat  menyimpan  data 
histori 
pengguna 
aktivitas apapun terkait arsip. 
Sistem  dapat  memfasilitasi  pencarian 
informasi arsip secara cepat sesuai kriteria- 
kriteria  yang  diinginkan  oleh  pengguna 
arsip Pusdiklat BPS. 
Sistem dapat menyediakan pengiriman file 
digital  dengan  format  hanya  pdf  sesuai 
yang dibutuhkan. 
Sistem  dapat  melakukan  perekaman  data 
secara rinci dan lengkap dengan penerapan 
validasi yang optimal. 
Sistem  yang dapat  memantau  peminjaman 
arsip secara up to date dan direkam secara 
cepat. 
b.  Kebutuhan non fungsional 

tentang 
dalam  melakukan 

vii 

vi 

iv 

v 

Kebutuhan non fungsional adalah kebutuhan 
yang menitikberatkan pada properti perilaku yang 
dimiliki  oleh  sistem.  Kebutuhan  non  fungsional 
sering  disebut  sebagai  layanan  atau  fungsi  yang 
ditawarkan  sistem  dan  tidak  secara  langsung 
tertentu.  Kebutuhan  non 
terkait  pada 
terhadap 
fungsional  memberikan 
ini 
kebutuhan 
untuk 
analisis 
menggunakan 
mengidentifikasi kebutuhan non fungsional dari 

fungsional.  Pada  penelitian 

PIECES 

batasan 

fitur 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

sistem yang berjalan dan menganalisis sistem yang 
dibuat. 

No 

Aspek 

1. 

Performance 

2. 

Information 

3. 

Economy 

4. 

Control 

5. 

Efficiency 

6. 

Service 

TABEL II 
ANALISIS PIECES 

Sistem Berjalan 

Lamanya waktu 
pengelolaan arsip 
karena masih 
dilakukan 
menggunakan 
Microsoft Excel 

Data tidak aman 
dari ancaman 
kerusakan data 

Tenaga yang 
dikeluarkan untuk 
melaksanakan 
pekerjaan lebih 
besar 

Tidak ada validasi 
data 

Tidak ada 
autentikasi, 
sehingga data bisa 
diakses orang yang 
tak memiliki hak, 
dan dapat 
melanggar kode 
etik penggunaan 
data. 

Penggunaan satu 
komputer setiap 
unit kerja, 
menyebabkan 
dibutuhkan waktu 
yang lama dalam 
kegiatan 
pengelolaan arsip. 

Monitoring 
aktivitas pegawai 
belum ada 

Sistem tidak 
terintegrasi dan 
terkoordinasi 
dengan pegawai 
arsip. 

Data bisa jadi tidak 
konsisten 

Sistem yang akan 
dibangun 

Waktu pengelolaan 
arsip dilakukan secara 
komputerisasi dengan 
web sehingga lebih 
efektif dan efisien. 

Data akan aman dan 
terhindar dari kerusakan 
karena disimpan di 
database server 

Meminimalisir  tenaga 
yang dikeluarkan oleh 
pegawai 

Adanya validasi dari 
data yang dipakai, 
sehingga data bisa 
sinkron 

Keamanan data aman, 
karena sudah ada hak 
akses nya dan data 
tersimpan dalam 
database server 

Dapat digunakan oleh 
beberapa komputer 
dengan pegawai yang 
berbeda-beda 

Monitoring pegawai 
sudah ada. 

Sistem sudah 
terintegrasi dan 
terkoordinasi antar 
pegawai arsip. 

Data sudah konsisten 

4 / 9 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
3.  Rancangan Sistem Usulan 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pada Gambar 6 merupakan gambar use case diagram yang 
menjelaskan 
interaksi  antara  sistem  eksternal  dengan 
pengguna.  Pada  gambar  tersebut  terdapat  dua  faktor  yaitu 
admin  dan  pegawai,  perbedaan  antara  admin  dan  pegawai 
tidaklah  berbeda  jauh.  Perbedaan  di  antara  kedua  faktor 
tersebut  adalah  menghapus  data,  menghapus 
  akun 
pengguna,  membuat  akun  pengguna  dan  mengubah  data 
akun pengguna yang hanya dimiliki oleh admin saja. 

5.  Rancangan Basis Data 

Rancangan  basis  data  pada  sistem 

ini  digambarkan 
menggunakan  Entity  Relationship  Diagram  ERD  (Entity 
Relationship  Diagram)  yang  dapat  dilihat  pada  Gambar  7. 
Pada  gambar  tersebut  terdapat  13  entitas  yang  digunakan 
pada sistem ini. Namun, pada gambar tersebut entitas utama 
ada tiga, yaitu arsip, user, dan arsip_pinjam. 

Gambar 5. Rancangan Sistem Usulan 

Pada  Gambar  5  terdapat  rancangan  sistem  usulan, 
dalam  rancangan  tersebut  terdapat  dua  faktor  yaitu 
pegawai  arsip  dan  bendahara  atau  pegawai  lain.  Pada 
gambar tersebut, sistem dimulai ketika dokumen sudah 
diberikan  ke  pegawai  arsip  dan  sudah  dilakukan 
pengecekan.  Setelah  itu,  dokumen  akan  dientrikan  ke 
dalam  sistem  yang  tersimpan  di  database  server 
pusdiklat.  Lalu,  dokumen  di  scan  dan  file  scan  akan 
tersimpan  di  database  server  pusdiklat  BPS  juga. 
Sementara  dokumen  hardfile-nya  tersimpan  di  suatu 
ruangan. 

4.  Use Case Diagram 

Gambar 7. Rancangan Basis Data 

6.  Tampilan Interface 

a.  Halaman Login 

login,  halaman 

Gambar 8. Halaman Login 
Pada  Gambar  8  merupakan  tampilan  interface 
halaman 
tersebut  merupakan 
halaman  yang  pertama  kali  dilihat  pengguna 
menggunakan  sistem  ini.  Pada  halaman  tersebut 
terdapat  form  login  yang  mengharuskan  pengguna 
untuk  mengisi  isian  alamat  email  dan  kata  sandi 
dengan  benar  jika  ingin  menggunakan  sistem  ini. 
telah 
Oleh  karena 

itu,  hanya  pengguna  yang 

Gambar 6. Use Case Diagram 

5 / 9 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
terdaftar  dan  memiliki  akun  yang  dapat 
menggunakan  sistem 
ini.  Pengguna  yang 
belum  memiliki  akun,  dapat  meminta  pada 
administrator  untuk  dibuatkan  akun  agar  bisa 
mengakses sistem ini. 
b.  Halaman Dashboard 

Setelah  proses  login  berhasil,  pengguna 
akan  diarahkan  menuju  halaman  dashboard 
untuk  halaman  admin  seperti  Gambar  9, 
sedangkan  untuk  halaman  pegawai  seperti 
Gambar 10. Perbedaan kedua gambar tersebut 
ialah adanya aktivitas seluruh pengguna sistem 
yang hanya dimiliki oleh admin saja. 

i.  Admin 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 12. Halaman Master Data 

Pada Gambar 12, merupakan halaman master data 
bagian  fungsi arsip. Pada master data  yang terbagi 
menjadi  3  bagian  yaitu  klasifikasi  arsip,  kelola 
jabatan, dan kelola unit pengolah. Untuk isian setiap 
tabel pada halamannya hampir sama antara 3 bagian 
itu.  Pada  bagian  klasifikasi  arsip terbagi menjadi  5 
bagian untuk mengatur kode klasifikasi yaitu fungsi 
arsip,  bidang  arsip,  masalah  arsip,  sub  masalah 
arsip,  dan  sub  sub  masalah  arsip.  Pada  kelola 
jabatan  maupun  kelola  unit  pengolah  digunakan 
untuk mengatur data pribadi pengguna arsip supaya 
lebih dinamis dengan perkembangan yang ada. 

e.  Halaman Peminjaman Arsip 

Gambar 9. Halaman Dashboard Admin 

ii.  Pegawai 

Gambar 13. Halaman Peminjaman Arsip 

Gambar 10. Halaman Dashboard Pegawai 

c.  Halaman Arsip 

Pada  Gambar 

13  merupakan 

halaman 
peminjaman  arsip  yang  berfungsi  untuk  memantau 
peminjaman  arsip  di  Pusdiklat  BPS.  Fitur  yang 
digunakan  pada  halaman  ini  seperti,  penambahan 
peminjaman  arsip,  pengubahan  peminjaman  arsip, 
pencarian  peminjaman  arsip  dan  penghapusan 
peminjaman  arsip.  Pada  penghapusan  arsip  hanya 
dimiliki oleh pengguna berlevel admin saja. 

f.  Halaman Kelola Pengguna 

Gambar 11. Halaman Arsip 
Pada  Gambar  11  merupakan  contoh  salah 
satu dari halaman arsip, yang berfungsi untuk 
melihat  arsip  yang  telah  masuk  di  database. 
Halaman  arsip  ini  terbagi  menjadi  dua  yaitu 
arsip  keuangan  dan  arsip  non  keuangan.  Di 
setiap arsip  tersebut  juga terbagi menjadi  dua 
yaitu  daftar  berkas  arsip  dan  daftar  isi  berkas 
arsip.  Daftar  berkas  arsip  berisi  arsip  secara 
umum  dari  suatu  dokumen,  sedangkan  daftar 
isi berkas arsip merupakan penjelas dari daftar 
berkas arsip. 

d.  Halaman Master Data 

Gambar 14. Halaman Kelola Pengguna 

Pada  Gambar  14,  merupakan  halaman  kelola 
pengguna yang berfungsi untuk mengatur pengguna 
yang  ada  di  sistem.  Fitur-fitur  yang  terlihat  pada 
halaman  ini  dapat  berupa  penambahan  pengguna, 
pengubahan  data  pengguna,  detail  data  pengguna, 
dan  penghapusan  data  pengguna,  penonaktifan 
pengguna.  Namun,  pengguna  berlevel  pegawai 
hanya mempunyai fitur detail pengguna saja. 

6 / 9 

 
 
 
 
 
 
 
 
 
 
 
g.  Halaman Profil 

Gambar 15. Halaman Profil 
Pada  Gambar  15  merupakan  halaman  profil 
yang  berisi 
informasi-informasi 
pengguna  seperti  foto,  nama,  jabatan,  unit 
pengolah, alamat, serta sunting profil. 

tentang 

7.  Evaluasi sistem 

Evaluasi  sistem  dilakukan  agar  peneliti  dapat 
mengetahui  apakah  sistem  berjalan  sesuai  dengan 
yang  diharapkan  dan  apakah  sistem  layak  untuk 
digunakan  atau  tidak.  Dalam  penelitian  ini,  peneliti 
menggunakan beberapa evaluasi, yaitu dengan metode 
black  box  testing,  kuesioner  system  usability  testing, 
pencocokan  dengan  dokumen  software  requirement 
spesification, 
tentang 
surat 
kesesuaian  sistem  yang  dibangun  dengan  persyaratan 
yang diminta. 

pernyataan 

serta 

a.  Black box testing 
TABEL III 
BLACK BOX TESTING 

Kasus 
Memasukkan  alamat 
email  dan  password 
yang 
atau 
salah 
status akun  sudah 
tidak aktif 
Memasukkan 
username 
password 
benar 
akun aktif 
Menekan 
dashboard 

dan 
dengan 
status 

menu 

dan 

Menekan menu arsip 
keuangan 

Menekan menu arsip 
non-keuangan 

Menekan 
menu 
daftar isi  berkas arsip 

Menekan 
daftar berkas arsip 

menu 

Menekan menu 
master data 

Menekan sub menu 
klasifikasi arsip 

No 

1. 

2. 

3. 

4. 

5. 

6. 

7. 

8. 

9. 

10. 

11. 

Menekan 
peminjaman arsip 

daftar 

Admin  mengelola 
data pengguna 

Hasil yang diharapkan 

Hasil 

Sistem 
tidak 
mengizinkan  akses  ke 
aplikasi  dan  kembali  ke 
halaman login 

Sistem 
mengijinkan 
akses  ke  aplikasi  dan 
menampilkan 
halaman 
sesuai hak akses 

Sistem menampilkan 
halaman dashboard 

Sistem menampilkan sub 
menu  daftar  berkas arsip 
dan daftar isi berkas arsip 
Sistem  menampilkan sub 
menu daftar berkas arsip 
dan daftar isi berkas arsip 
Sistem menampilkan 
halaman daftar isi 
berkas 
Sistem menampilkan 
halaman daftar 
berkas 
Sistem  menampilkan  sub 
menu  klasifikasi  arsip, 
sub  menu  kelola  jabatan, 
sub menu kelola unit 
pengolah 
Sistem  menampilkan  sub 
sub  menu  fungsi  arsip, 
bidang  arsip,  masalah 
arsip, sub- masalah arsip, 
sub-sub masalah arsip 
Sistem 
halaman 
peminjaman arsip 
Sistem mengelola data 
semua pengguna 

menampilkan 
daftar 

Sesuai 

Sesuai 

Sesuai 

Sesuai 

Sesuai 

Sesuai 

Sesuai 

Sesuai 

Sesuai 

Sesuai 

Sesuai 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pada tabel tiga, terdapat beberapa hasil uji black 
box testing yang menyatakan bahwa sistem  sudah 
sesuai dan dapat berjalan dengan baik. 

b. 

System Usability Scale (SUS) 
TABEL IV 
SYSTEM USABILITY SCALE 

1 

5 

4 

4 

4 

4 

4 

4 

4 

4 

4 

41 

82 

2 

5 

5 

5 

1 

5 

4 

5 

5 

5 

4 

44 

88 

Pertanyaan 

P1 

P2 

P3 

P4 

P5 

P6 

P7 

P8 

P9 

P10 

Jumlah 

Skor 

Total 

Rata-rata 

Responden 

3 

5 

4 

5 

3 

5 

4 

3 

4 

4 

4 

41 

82 

338 

84.5 

4 

5 

4 

4 

3 

5 

4 

4 

5 

5 

4 

43 

86 

Pada  tabel  empat  merupakan  hasil  SUS  yang  berfungsi 
untuk  menentukan  bahwa  sistem  layak  digunakan  atau 
tidak  layak  digunakan.  Pada  hasil  tersebut  dari  empat 
responden  yang  merupakan  pegawai  arsip  Pusdiklat  BPS 
didapatkan  skor  sebesar  84.5  yang  sudah  dapat  dikatakan 
bahwa sistem layak digunakan. 

c. 

Software Requirements Specification (SRS) 

TABEL V 
SOFTWARE REQUIREMENTS SPECIFICATION 

NO 

Fungsi Produk 

Hasil Pengujian 

1. 

2. 

3. 

4. 

5. 

6. 

7. 

8. 

9. 

Kelola pengguna 

Laporan arsip 

Daftar berkas arsip 

Daftar isi berkas arsip 

Hak akses 

Klasifikasi arsip 

Kelola Jabatan 

Kelola Unit Pengolah 

Dashboard 

10.  Daftar pinjaman arsip 

Sesuai 

Sesuai 

Sesuai 

Sesuai 

Sesuai 

Sesuai 

Sesuai 

Sesuai 

Sesuai 

Sesuai 

Pada  tabel  lima  terdapat  hasil  SRS  yang  merupakan  
hasil kesesuaian antara fungsi produk dengan sistem. Pada 
uji  tersebut  didapatkan  hasil  yang  sesuai  dengan  harapan 
pengguna sistem. 

7 / 9 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

6. 

7. 

Jumlah 

Keterangan 

Menunjukkan 
dan menjelaskan 
Jumlah 
- 
fisik serta 
jumlah 
logis 
-  Media dari 
unit 
deskripsi 
Menunjukkan 
informasi yang 
penting yang 
tidak terdapat 
pada salah satu 
elemen di atas 

Jumlah fisik 
dalam angka 

Char 

40 

Text 

- 

Suatu informasi 
tertentu selain 
elemen yang 
sudah 
ditetapkan 

Berdasarkan 

PENUTUP 
hingga 
analisis 
pengimplementasian  pada penelitian  ini,  maka  penelitian  ini  sudah 
mencakup sebagai berikut. 

VII. 
hasil 

perancangan 

1.  Sistem  telah  berhasil  dibangun  sesuai  SRS  yang 
telah  diimplementasikan  di  server 
https://pusdiklat- 

dengan 

alamat 

disepakati  dan 
pusdiklat 
bps.online/simas/. 

2.  Hasil evaluasi pada SUS didapatkan skor 84.5, sehingga 
sudah  dapat  dikatakan  bahwa  kepuasan  pengguna 
terhadap  sistem  ini  dikategorikan  baik  dan  dapat 
diterima oleh pengguna. Selain itu, pada hasil uji black 
box testing mendapatkan setiap fungsi yang dibuat pada 
sistem ini dapat berjalan sesuai dengan harapan, hal ini 
dikuatkan  dengan  pencocokan  dokumen  SRS,  dengan 
mendapatkan  hasil  yang  sesuai  dengan  harapan 
pengguna.  Sementara  itu,  hasil  evaluasi  ini  juga 
diperkuat  dengan  surat  pernyataan  yang  disetujui  oleh 
subject matter berisi tentang persetujuan bahwa sistem 
sudah relevan  dan  memiliki  kontribusi  pada  pekerjaan 
serta  memenuhi  persyaratan  yang diminta di  Pusdiklat 
BPS. 

3.  Sistem pada autentikasi untuk saat ini, yang dibutuhkan 
hanya dua level yaitu: admin dan pegawai, selanjutnya 
jika  dibuat  pengembangan  lebih  lanjut  dan  subject 
matter membutuhkan, akan dibuat level lanjutan sesuai 
dengan unit kerja masing-masing. 

4.  Sistem  yang  dikembangkan  untuk  saat 

ini,  pada 
pelaporan  arsip  hanya  sesuai  filter  yang  diinginkan, 
belum  dapat  memilih  secara  acak  arsip  yang 
dibutuhkan,  selanjutnya  jika  dibuat  pengembangan 
lebih  lanjut  dan  subject  matter  membutuhkan,  akan 
dibuat pelaporan arsip dengan memilih arsip secara acak 
sesuai yang dibutuhkan. 

d. 

Surat Pernyataan 

Gambar 16. Surat Pernyataan 

tentang 

Pada  gambar  16  berisi 

surat 
pernyataan  dari  Pusdiklat  BPS  yang  menyatakan 
bahwa  sistem  yang  dibuat  oleh  peneliti  dapat 
mengefisienkan  pekerjaan  dalam  pengelolaan 
arsip  dan  memenuhi  persyaratan  yang  diminta 
Pusdiklat BPS. 

8.  Metadata arsip Pusdiklat BPS 

Metadata  merupakan  data  yang  menjelaskan  isi, 
konteks, dan struktur arsip serta manajemen sepanjang 
masa. Metadata arsip elektronik dapat berupa informasi 
yang  dibutuhkan  untuk  dimungkinkan  arsip  supaya 
jelas  dan  bisa  digunakan.  Berikut  merupakan  elemen 
data  yang  harus  ada  di  dalam  metadata  pengelolaan 
arsip Pusdiklat BPS. 

Tipe 
Data 
Char 

Panjang 
Karakter 
45 

Char 

40 

No 

1. 

2. 

3. 

4. 

5. 

Nama 
Elemen 

Nomor 
arsip 

Kode 
klasifikasi 

Pencipta 
arsip 

Uraian 
Informasi 

Kurun 
waktu 

Tabel VI 
METADATA ARSIP PUSDIKLAT BPS 

Tujuan 

Penjelasan 

Memperlihatkan 
keunikan suatu 
item arsip 
Menunjukkan 
kode yang 
memperlihatkan 
pengelompokkan 
fungsi informasi 
yang terdapat 
dalam suatu 
arsip 
Memperlihatkan 
pembuat dari 
suatu arsip 

Memperjelas 
isian dari suatu 
arsip 
Berisi waktu dari 
transaksi  yang 
tercatat  dalam 
arsip 

Nomor pada 
item arsip 
maupun kode 
Kode yang 
disesuaikan 
dengan kode 
saat 
pengelolaan 
arsip 

Memperjelaskan 
seseorang atau 
kelompok atau 
organisasi yang 
membuat suatu 
arsip 
Ringkasan dari 
suatu arsip 

Text 

Tanggal suatu 
dokumen dibuat 

Date 

Char 

120 

DAFTAR PUSTAKA 

- 

8 

[1]H.  Kristanto,  Konsep  Dan  Perancangan  Database,  Yogyakarta:  Andi, 

2004. 

[2]Badan Pusat Statistik, Peraturan Kepala Badan Pusat Statistik Nomor 99 
Tahun  2018  tentang  Pedoman  Klasifikasi  Arsip  di  Lingkungan  Badan 
Pusat Statistik, Jakarta: Badan Pusat Statistik, 2018 

[3]A. Simangunsong, Sistem Informasi Pengarsipan Dokumen Berbasis Web, 

Mantik Penusa, vol. 2 no.1, pp. 11-19, Jun 2018 

[4]L. Gie, Administrasi Perkantoran Modern, Yogyakarta: Liberty, 2000. 
[5]K. Gunawan dan Muhammad Multazam, Sistem Informasi Penatausahaan 
Arsip Online (Si-Pentol) pada Dinas, Speed, vol. 10 No. 1, no. 2354-6654, 
pp. 54-59, 2018. 

[6]  Jogiyanto, Analisis dan Desain, Yogyakarta: Andi, 2005. 
[7]  F. Devie, Perancangan Sistem Informasi Pengelolaan Arsip Visual 

Berbasis Web (Studi Kasus Badan Kearsipan Daerah Provinsi Jawa 

8 / 9 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Barat, LPKIA, vol.4 no.1, pp. 23-26, Jun 2014. 

[8]  Basri  dan  Joni  Devitra,  Analisis  dan  Perancangan  Sistem 
Pengelolaan  Arsip  Berbasis  Web  (Studi  Kasus:  KPU  Kabupaten 
Tebo), Manajemen Sistem Informasi, vol.2 no.1, pp. 227-243, Mar 
2017. 

[9]  Taufik  Hidayat  dan  Sukisno,  Rancang  bangun  sistem  penerimaan 
mahasiswa  baru  online  dengan  model  sdlc  model  prototipe  di 
universitas  islam  syekh-yusuf,  Jurnal  Pelita  (Penelitian  dan  Karya 
Ilmiah) Edisi XVIII Vol 2, Juli-Desember 2018. 

[10] Basuki,  Awan  Pribadi,  Membangun  Web  Berbasis  PHP  dengan 

Framework Codeigniter, Yogyakarta: Lokomedia, 2010. 

[11] Davis,  B,  Gordon.,  Sistem  informasi  manajemen,  Jakarta:  PT 

Pustaka Binaman Pressindo, 1991. 

[12] Ramakrishnan,  R.  dan  Gehrke,  J.,  Sistem  Manajemen  Database, 

Yogyakarta: Penerbit Andi, 2003. 

[13] Sukamto, R. A. Dan Shalahuddin, M., Rekayasa Perangkat Lunak, 

Bandung: Informatika, 2013. 

9 / 9 

 
 
"
221709711,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Deteksi Indikasi Depresi pada Tweet Berbahasa 
Indonesia Menggunakan Lexicon 

Ganif Susilo Aji (221709711, 4SD2) 
Dosen Pembimbing: Nori Wilantika S.ST, M.T.I. 

Ringkasan—  Semakin  berkembangnya  zaman,  kesehatan 
mental mulai diperhatikan terutama di kalangan anak muda. Hal 
tersebut  juga  diikuti  oleh  semakin  meningkatnya  gangguan 
kesehatan  mental  terutama  depresi.  Untuk  menanggulangi 
masalah  tersebut  perlu  dilakukan  upaya  dalam  mencegah  dan 
mengatasinya. Langkah awal yang diperlukan yaitu dengan cara 
mengindentifikasi  masyarakat  yang  sedang  mengalami  gejala 
depresi tersebut. Berkembangnya penggunaan media sosial, salah 
satunya  Twitter,  memiliki  potensi  untuk  membantu  mengatasi 
masalah meningkatnya kasus depresi di Indonesia. Penelitian ini 
mencoba  mendeteksi  indikasi  depresi  pada  tweet  berbahasa 
Indonesia.  Lexicon  LabMT  dan  Sentitsrength  digunakan  untuk 
mengklasifikasikan  tweet  terindikasi  depresi  dan  tweet  tidak 
terindikasi  depresi.  Hasil  dari  penelitian  ini  yaitu  proporsi 
pengguna  yang  terindikasi  depresi  berdasarkan  lexicon  LabMT 
sebesar 7,04% lebih mendekati prevalensi depresi penduduk usia 
15  keatas  sebesar  5,9%  dibanding  dengan  lexicon  Sentistrength 
sebesar  48,4%.  Jika  dilihat  dari  kata-kata  yang  sering  muncul 
pada  tweet  yang  terindikasi  depresi  berdasarkan  LabMT  lebih 
menggambarkan indikasi depresi dibanding yang dihasilkan oleh 
tersebut,  dapat 
lexicon  Sentistrength.  Berdasarkan  hasil 
disimpulkan bahwa lexicon LabMT lebih baik dalam mendeteksi 
indikasi depresi pada tweet berbahasa Indonesia. 

Kata  Kunci—  Depresi,  Twitter,  Kesehatan  Mental,  Deteksi, 

Lexicon. 

I.  LATAR BELAKANG 

Berdasarkan data Hasil Riskesdas 2018, terdapat 7,1 per mil 
rumah  tangga  yang  memiliki  anggota  rumah  tangga  dengan 
gangguan  jiwa.  Artinya  dari  1000  rumah  tangga  terdapat  7 
diantaranya  yang  memiliki  penderita  gangguan  jiwa.  Dari 
rumah tangga yang memiliki penderita gangguan jiwa, terdapat 
14,05% yang pernah melakukan pasung dan 31,5% diantaranya 
melakukan pasung pada 3 bulan terakhir. Pada 2018, penduduk 
Indonesia  umur  15  tahun  keatas  memiliki  prevalensi  depresi 
sebesar  6%.  Dari  seluruh  penderita  depresi,  hanya  9%  yang 
menjalani  pengobatan  medis[1].  Pada  publikasi  Profil 
Kesehatan Indonesia 2019 menyebutkan berdasarkan aplikasi 
keluarga  sehat  per  Februari  2020,  persentase  keluarga  yang 
memiliki  penderita  gangguan  jiwa  berat  diobati  dan  tidak 
ditelantarkan sebesar 36,75%. Artinya masih terdapat lebih dari 
60% yang masih belum diobati atau terurus[2].  

Salah  satu  gangguan  kesehatan  mental  yaitu  depresi. 
Menurut  Sulistyorini  dan  Sabarisman,  depresi  merupakan 
gangguan emosional yang ditandai dengan perasaan tertekan, 
perasaan  bersalah, kesedihan,  kehilangan  minat,  dan  menarik 
diri  dari  hubungan  dengan  orang  lain.  Depresi  merupakan 
reaksi yang normal dan wajar jika dalam waktu yang singkat. 
Namun  jika  tidak  segera  mendapatkan  pertolongan  secara 

psikologis  dan  medis,  depresi  bisa  menjadi  gangguan  mental 
parah  dan  bisa  berujung  bunuh  diri[3].  Maka  dari  itu 
penanganan  terhadap  masalah  kesehatan  mental  menjadi  hal 
yang  penting.  Hal  tersebut  dikarenakan  masalah  kesehatan 
mental  sangat  mempengaruhi  produktivitas  dari  penderitanya 
[4]. Mengenali lebih awal tanda-tanda depresi dapat membantu 
dalam pencegahan dan penanggulangan kasus depresi. Adapun 
tanda-tanda  depresi  yang  bisa  dilihat  yaitu merasa  sedih  atau 
sedang depresi, kehilangan kesenangan, kesulitan atau terlalu 
banyak  tidur,  kehilangan  energi,  merasa  tidak  berharga  atau 
bersalah, dan berpikir tentang kematian atau bunuh diri [5].  

teknologi 

Berkembangnya 

informasi  meningkatkan 
pertukaran  informasi  menjadi  sangat  cepat.  Terutama  pada 
penggunaan  teknologi  media  sosial  yang  sekarang  masif 
penggunaannya. Banyak berbagai media sosial yang digunakan 
seperti  Instagram,  Twitter,  Facebook,  Youtube,  dan  masih 
banyak media sosial yang lainnya. Menurut hasil laporan survei 
dari  We  Are  Social  pada  awal  tahun  2020,  pengguna  media 
sosial di Indonesia mencapai 160 juta yang mana itu mencapai 
59% dari populasi penduduk Indonesia. Twitter menjadi salah 
satu media sosial yang paling sering digunakan oleh masyaraat 
Indonesia. Twitter berada di urutan kelima dari sepuluh besar 
media  sosial  yang  sering  digunakan  di  Indonesia  dengan 
persentase pengguna yang mengakses yaitu  sebesar 56% dari 
seluruh pengguna internet dari Indonesia [6]. 

Terdapat  beberapa  tujuan  penggunaan  media  sosial  yaitu 
untuk  mendapatkan  informasi  dan  hiburan,  mengekspresikan 
diri,  berkomunikasi  dengan  teman  dan  keluarga,  dan  yang 
terakhir  yaitu  untuk  kepentingan  profesional  dan  mengisi 
waktu  luang[7].  Mengekspresikan  diri  dan  mengungkapkan 
perasaan  di  media  sosial  menjadi  salah  satu  yang  dilakukan. 
Dari  fakta  tersebut,  terdapat peluang  untuk dilakukan  deteksi 
depresi  dari  media  sosial.  Sudah  terdapat  banyak  penelitian 
terdahulu  yang  dilakukan  untuk  mendeteksi  emosi  dari  data 
media sosial. Seperti penelitian tentang mendeteksi stress dan 
relaksasi dari tweet [8], memprediksi mental illness khususnya 
depresi  dan  Post  Trauma  Stress  Disorder  (PTSD)  dari  data 
Twitter  [9],  dan  membuat  korpus  Bahasa  Indonesia  untuk 
mendeteksi emosi dari teks [10].  

Dari  fakta  mengenai  banyaknya  pengguna  media  sosial 
khususnya  Twitter  di  Indonesia,  terdapat  potensi  untuk 
melakukan  pendekatan  media 
sosial  Twitter  dalam 
mengidentifikasi  depresi  pada  masyarakat  Indonesia.  Hal 
tersebut  didukung oleh penelitian  tentang  mendeteksi  depresi 
dan gangguan kesehatan mental lain di media sosial [11] yang 
menyatakan  bahwa  depresi  dan  gangguan  kesehatan  mental 
lain bisa dideteksi melalui media online.  

 1 / 9 

 
 
 
 
 
Pada  penelitian  terdahulu  terdapat  beberapa  metode  untuk 
mendeteksi  depresi  dari  tweet,  yaitu  menggunakan  lexicon-
based  dan  machine  learning-based.  Pada  metode  machine 
learning-based terdapat kendala yaitu pada saat memberi label 
pada  data  training  yang  dilakukan  oleh  bukan  orang  yang 
profesional  dalam  bidangnya,  maka  hasilnya  akan  kurang 
akurat.  Seperti  pada  penelitian  De  Choudhury  et  al[12]  yang 
memberikan  label  terhadap  pengguna  depresi  yang  telah 
didiagnosa secara klinis. Penelitian lainnya yaitu dari [9] yang 
menggunakan kuesioner CES-D dan PHQ-9 untuk memberikan 
label  depresi  pada  pengguna  yang  sebelumnya  dikumpulkan 
dan  diidentifikasi  menjadi  depresi  dan  bukan  depresi  
berdasarkan  self-declared.  Metode  yang  kedua  yaitu  dengan 
menggunakan  lexicon  yang  sudah  dibuat  oleh  para  peneliti 
terdahulu.  Namun  jumlah  lexicon  yang  mendukung  untuk 
diterapkan pada teks berbahasa Indonesia masih terbatas.  

II.  TUJUAN PENELITIAN 

Berdasarkan  latar  belakang  di  atas,  pada  penelitian  ada 
beberapa  tujuan  yang  ingin  dicapai.  Penelitian  ini  bertujuan 
untuk  mendeteksi  indikasi  depresi  pada  tweet  berbahasa 
itu 
Indonesia  menggunakan 
juga  untuk 
lexicon.  Selain 
mengetahui  perbandingan  hasil  deteksi 
indikasi  depresi 
menggunakan beberapa lexicon.  

III. PENELITIAN TERKAIT 

Pada  penelitian  ini  terdapat  penelitian  yang  terkait,  yaitu 
penelitian  dari  Asita  Sekar  dan  Siti  Mariyah  [13].  Penelitian 
tersebut  bukan  mengenai  depresi,  namun  penelitian  tersebut 
berisi  pembuatan  Indeks  Kebahagiaan  Subjektif  (IKS)  yang 
menggunakan data tweet untuk menghasilkan nilai indeksnya. 
Data yang dikumpulkan dengan cara  streaming ini dilakukan 
pada bulan April hingga Juli 2018. Terdapat 2,4 juta tweet yang 
dianalisis dan 100 ribu tweet untuk evaluasi. Penelitian tersebut 
menggunakan  lexicon  Language  Assessment  by  Mechanical 
Turk  (LabMT)  untuk  memberikan  nilai  dari  setiap  kata  pada 
tweet.  Kemudian  nilai  dari  setiap  kata  diolah  untuk 
menghasilkan  indeks  kebahagiaan  subjektif  dari  setiap  tweet. 
Selanjutnya,  indeks  kebahagiaan  subjektif  tersebut  dievaluasi 
dengan  cara  dibandingkan  dengan  nilai  hasil  dari  manual 
labeling. Manual labeling dilakukan dengan cara membagikan 
web  link  ke  Twitter  sehingga  setiap  orang  bisa  mengakses 
kemudian memberikan nilai dari suatu tweet dengan rang 1 – 9 
dengan  keterangan  semakin  besar  nilainya  semakin  bahagia 
dan 0 untuk tweet yang tidak diketahui. Hasil dari perbandingan 
kedua metode LabMT dan manual labeling tidak menunjukkan 
perbedaan yang signifikan yang diukur menggunakan MAPE, 
MSE,  RMSE,  dan  MAD.  Sehingga  disimpulkan  bahwa 
penggunaan  LabMT  relevan  untuk  menghasilkan  IKS.  Sama 
seperti  depresi,  kebahagiaan  juga  merupakan  kondisi  mental. 
Oleh  karena  itu,  berdasarkan  penelitian  ini,  penulis  mencoba 
menggunakan LabMT untuk mendeteksi depresi. 

Penelitian lain dari [14], penelitian ini tentang memprediksi 
kemunculan depresi dan Pasca Trauma Stress Disorder (PTSD) 
pada pengguna Twitter. Data depresi yang dikumpulkan yaitu 
279.951 tweet dari 204 individu dengan rincian 105 depresi dan 
99 sehat. Sedangkan untuk data PTSD yaitu 243.775 dari 174 
pengguna dengan rincian 63 memiliki PTSD dan sisanya sehat. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pengumpulan  data  tweet  tersebut  dilakukan  menggunakan 
akses dari API Twitter dengan batasan 3200 tweet per pengguna 
dan dilakukan antara 1 Februari 2016 dan 10 Juni 2016. Pada 
penelitian tersebut menggunakan 3 metode untuk memprediksi 
kemunculan  depresi.  Metode  yang  digunakan  yaitu  LabMT, 
LIWC  2007,  dan  ANEW.  Dari  ketiga  metode  tersebut,  skor 
kebahagiaan  LabMT  menjadi  prediktor  terkuat  baik  untuk 
depresi  maupun  Pasca  Trauma  Stress  Disorder  (PTSD). 
Penelitian dari [15] juga mendeteksi bahasa yang menunjukkan 
depresi 
penelitiannya  mencoba 
menggunakan  beberapa  lexicon  seperti  LabMT,  Emolex, 
AFINN,  LIWC,  VADER,  HSL,  dan  CBET.  Dari  beberapa 
lexicon  tersebut  didapat  dua  lexicon  yang  menghasilkan 
performa  yang  terbaik  yaitu  LIWC  dan  HSL.  Namun,  kedua 
tersedia  secara  publik.  Sehingga 
lexicon 
dalam 
biaya 
diperlukan 
menggunakan lexicon tersebut.  

dari  Twitter.  Pada 

untuk  mendapatkan 

tersebut 

akses 

tidak 

Selain  beberapa  lexicon  pada  penelitian  diatas,  terdapat 
lexicon  lain  yang  dapat  digunakan  yaitu  TensiStrength  dan 
Sentistrength. Seperti pada penelitian [16] yang menggunakan 
TensiStrength  untuk  mendeteksi  tingkat  stress  dan  relaksasi 
pada media sosial teks. Pada penelitian tersebut juga dijelaskan 
bahwa lexicon yang digunakan untuk mendeteksi emosi yaitu 
TensiStrength diadaptasi dari lexicon Sentistrength yang dibuat 
untuk  mendeteksi  sentimen.  Sedangkan  Sentistrength  sendiri 
dibuat  dengan  kata-kata  yang  berasal  dari  program  LIWC. 
Dengan  demikian,  Sentistrength  selanjutnya  dapat  digunakan 
untuk mendeteksi emosi, tingkat stress, dan relaksasi. 

Penelitian  yang  menggunakan  lexicon  Sentistrength  yaitu 
pada  penelitian  [17].  Pada  penelitian  tersebut  menggunakan 
NRC  Affect  Intensity  Lexicon  dan  Sentistrength  untuk 
membantu mendeteksi emosi pada tweet yang memiliki pesan 
bunuh diri. Hasil dari penelitian tersebut menunjukkan bahwa 
tweet  yang  berisi  konten  bunuh  diri  memiliki  hubungan  kuat 
dengan ketakutan, kesedihan dan sentimen negatif. 

Dari  penelitian-penelitian  terdahulu  yang  telah  dijabarkan, 
peneliti memutuskan menggunakan  LabMT dan  Sentistrength 
sebagai  metode  untuk  mendeteksi  indikasi  depresi  dari  tweet 
pada  penelitian  ini.  Terbatasnya  lexicon  yang  tersedia  secara 
publik  menjadi  salah  satu  keterbatasan  pada  penelitian  ini. 
Mudahnya  mengakses  lexicon  LabMT  dan  Sentistrength 
dibanding beberapa lexicon lainnya menjadi alasan dipilihnya 
lexicon  tersebut.    Untuk  objek  yang  dilakukan  identifikasi 
indikasi  depresi  yaitu  tweet,  bukan  pengguna.  Hal  tersebut 
karena  keterbatasan  untuk  mendapatkan  data  pengguna  yang 
depresi yang teruji secara klinis. 

IV. METODE PENELITIAN  
penelitian-penelitian 

Berdasarkan 

yang 
mendeteksi  depresi  dan  sejenisnya,  didapatkan  metode 
penelitian seperti yang ditunjukkan pada Gambar 1. 

sebelumnya 

 2 / 9 

 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

3.  Pengubahan  huruf  besar  pada  teks  menjadi  huruf  kecil 

(case folding). 

4.  Pemisahan  kalimat  pada 

teks  menjadi  kata-kata 

(tokenization). 

5.  Penghapusan  stopword  (stopword  removal),  menghapus 
kata-kata  yang  tidak  digunakan  seperti  kata:  yang,  ke, 
tetapi, atau, di, dengan, dan sebagainya. 

6.  Pengubahan kata pada teks menjadi kata dasar (stemming). 

Setelah  dilakukan  proses  preprocessing, 
selanjutnya 
dilakukan data cleaning untuk membersihkan data dari tweet 
yang  kosong  (akibat  setelah  dilakukan  preprocessing), 
duplikasi pada tweet, tweet yang bukan berbahasa Indonesia, 
serta tweet yang bukan berasal dari akun personal. 

tweet 

Deteksi Indikasi Depresi 
Setelah  data  selesai  dari  preprocessing,  selanjutnya 
dilakukan  klasifikasi  data 
secara  keseluruhan 
menggunakan  lexicon  LabMT  dan  Sentistrength.  LabMT  dan 
Senstistrength yang digunakan pada penelitian ini yaitu lexicon 
yang sudah dalam bentuk Bahasa Indonesia.  Lexicon  LabMT 
dalam  Bahasa  Indonesia  yang  digunakan  pada  penelitian  ini 
didapat dari library python LabMT-simple yang dikembangkan 
oleh Andy Reagan[19]. Sedangkan untuk lexicon Sentistrength 
didapat  dari  program  python  Sentistrength  dalam  Bahasa 
Indonesia  yang  mana  merupakan  pengembangan  pada 
penelitian  mengenai  pendeteksian  sentimen  menggunakan 
Sentistrength [20]. 

Pemberian  skor  masing-masing kata  dengan  menggunakan 
lexicon LabMT dan Sentistrength dan dilakukan penghitungan 
skor  akhir  pada  tweet.  Berikutnya  dilakukan  klasifikasi 
berdasarkan  nilai  skor  akhir  tweet  menjadi  “1”  untuk  tweet 
yang  terindikasi  depresi  dan  “0”  untuk  tweet  yang  tidak 
terindikasi  depresi  dengan  penentuan  cut-off  masing-masing 
lexicon.  Pada  LabMT,  tweet  mendapatkan  label  “1”  jika  skor 
kurang dari 4 dan mendapat label “0” jika skor ≥ 4. Nilai cut-
off tersebut didasarkan pada  lexicon LabMT, kata yang netral 
yaitu kata dengan skor diantara 4 dan 6 dalam skala 1 hingga 9 
[14].  Sedangkan  untuk  skor <  4  menunjukkan kesedihan dan 
skor  >  6  menunjukkan  kebahagiaan.  Sedangkan  untuk 
Sentistrength,  tweet  mendapatkan  label  “1”  jika  nilai  skor 
kurang dari (-1) dan mendapat label “0” jika skor ≥ (-1).  Nilai 
cut-off    tersebut  didasarkan  pada  nilai  skor  pada  lexicon 
Sentistrength  diantara  ‘+1’  (not  positive)  hingga  ‘+5’ 
(extremely  positive)  dan  ‘-1’  (not  negative)  hingga  ‘-5’ 
(extremely negative) [17]. 

Rumus  menghitung  skor  akhir  dari  tweet  didapat  dari 

penelitian [13]: 

ℎ𝑎𝑣𝑔(𝑇) =   ∑ ℎ𝑎𝑣𝑔(𝑤𝑖)𝑓𝑖
𝑁
∑ 𝑓𝑖
𝑖−1

𝑁
𝑖−1

(1) 

Dimana  𝑓𝑖  adalah  frekuensi  kata  unik  ke-i  dalam 
sebuah T (tweet) dan ℎ𝑎𝑣𝑔(𝑤𝑖) adalah data nilai dari kata-kata 
yang tersedia di LabMT dan Sentistrength. 

 3 / 9 

Gambar 1. Tahapan Penelitian 

Pengumpulan Data dari Twitter 
Pengumpulan  data  penelitian  ini  menggunakan  package 
twint  untuk  melakukan  scraping  data  Twitter  pada  situsnya 
www.twitter.com [18]. Twint tidak menggunakan API untuk 
melakukan  scraping.  Twint  hampir  dapat  mengambil  semua 
tweet yang ada dan tanpa memiliki batasan jumlah. Hal tersebut 
menjadi  keunggulan  bagi  Twint  dibanding  dengan  tools  lain. 
Scraping  dilakukan  dengan  menggunakan  batasan  wilayah 
dengan  titik  pusat  Jakarta  dalam  radius  80  Kilometer  untuk 
mendapatkan  data  tweet.  Data  yang  diambil  yaitu  data  tweet 
pada  rentang  waktu  selama  1  November  2020  hingga  31 
Desember 2020.  Data hasil scraping kemudian disimpan pada 
file dengan format .csv. 

Pre-Processing Data 
Sebelum  data  tweet  dianalisis,  dilakukan  terlebih  dahulu 
proses  preprocessing  untuk  mendapatkan  data  tweet  yang 
bebas dari noise agar mendapat kualitas hasil yang baik. Data 
hasil  dari  preprocessing  diharapkan  dapat  digunakan  untuk 
mendapatkan hasil yang baik saat digunakan pada tahap deteksi 
indikasi depresi.  

Peneliti  membandingkan  beberapa  preprocessing  yang 
sebelumnya  dan 
tahapan  preprocessing 

dilakukan  pada  penelitian-penelitian 
memutuskan  untuk  menggunakan 
sebagai berikut: 

1.  Penghapusan  bagian  pada  dokumen  tweet  yang  tidak 
digunakan  seperti  emoji,  url,  username,  symbol,  dan 
hashtags. 

2.  Penyeragaman  bentuk  huruf  serta  penghapusan  tanda 

baca dan angka (remove punctuation). 

 
 
 
 
 
 
 
 
 
     
 
 
 
 
 
 
Metode Analisis 
Analisis  yang  sudah  dilakukan  pada  penelitian  ini  yaitu 
analisis  deskriptif  dengan  membandingkan  hasil  klasifikasi 
tweet yang terindikasi depresi dari kedua  lexicon LabMT dan 
Sentistrength.  Perbandingan  dilakukan  untuk  melihat 
karakteristik  dari  kedua  lexicon  dalam  melakukan  klasifikasi 
terindikasi  depresi.  Perbandingan  dilakukan 
tweet  yang 
berdasarkan  kata-kata  yang  sering  muncul  dari  tweet  yang 
terindikasi  depresi,  jumlah  proporsi  tweet  yang  terindikasi 
depresi  berdasarkan  waktu,  dan  membandingkan  jumlah 
proporsi  pengguna  yang  memposting  tweet  yang  terindikasi 
depresi. 

V.  KERANGKA PIKIR 

Gambar 2. Kerangka Pikir 

 Pada  Gambar  2  terlihat  bahwa  data  yang  digunakan  pada 
penelitian  yaitu  teks  pada  tweet  yang  mana  merupakan  data 
yang  diambil  dari  media  sosial  Twitter.  Pengumpulan  data 
dilakukan dengan scraping memanfaatkan package twint yang 
dibuat dengan bahasa pemograman python. Data preprocessing 
dilakukan  untuk  membersihkan  dan  merapikan  data  sebelum 
dilakukan  deteksi  indikasi  depresi.  Deteksi  indikasi  depresi 
dilakukan  menggunakan 
dan 
Sentistrength. Dilakukan juga metode analisis secara deskriptif 
untuk  melihat  performa  dari  hasil  deteksi  indikasi  depresi 
menggunakan kedua lexicon. 

yaitu  LabMT 

lexicon 

VI. HASIL DAN PEMBAHASAN 

Pengumpulan Data 
Pengumpulan data dilakukan menggunakan package Twint. 
Data  hasil  scraping  disimpan  pada  file  csv.  Gambar  3 
merupakan  fungsi  yang  dibuat  untuk  mendapatkan  tweet 
berdasarkan batasan-batasan yang ditentukan. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 3. Fungsi scraping 

Pada  Gambar 3 “c.Geo” menunjukkan wilayah mana yang 
sudah  dilakukan  pengambilan  data  dengan  memasukkan  titik 
geocode  dan  radiusnya.  Pada  penelitian 
ini  berisi  (-
6.20906,106.83539, 80km) yang artinya scraping data  Twitter 
dilakukan  pada  titik  pusat  di  Jakarta  dengan  radius  80 
Kilometer.  Untuk  “c.Since”  menunjukkan  mulai  kapan  data 
yang akan diambil. Pada penelitian ini diisi “2020-11-01” yang 
artinya  data  dikumpulkan  mulai  tanggal  1  November  2020. 
Sedangkan  “c.Until”  menunjukkan  sampai  kapan  data  yang 
akan  diambil.  Pada  penelitian  ini  diisi  “2020-12-31”  yang 
artinya data yang dikumpulkan hingga 31 Desember 2020. 

Berikutnya  yaitu  untuk  mengatur  format  data  yang  akan 
disimpan,  “c.Store_csv”  menunjukkan  untuk  penyimpanan 
data  dalam  bentuk  csv.  Untuk  “c.Custom”  menunjukkan 
pengorganisasian format data pada file csv saat disimpan. Pada 
penelitian  ini  data  disimpan  berdasarkan  format  kolom 
“id”,”date”,”time”,  “username”,    dan  “tweet”.  Sedangkan 
“c.Lang” menunjukkan batasan bahasa apa yang akan disimpan 
yang mana pada penelitian ini menggunakan bahasa Indonesia 
sehingga  diisi  kode  “id”.  Namun  pada  kenyataannya  masih 
terdapat  bahasa  lain  yang  ikut  dikumpulkan.  Yang  terakhir 
“c.Output”  menunjukkan  dalam  menentukan  nama  file  untuk 
data  yang  akan  disimpan.  Pada  penelitian  ini  file  disimpan 
denga nama “Jabodetabek.csv”. 
Adapun proses scraping ditunjukkan pada Gambar 4. 

Gambar 4. Contoh proses scraping 

Data  tweet  yang  berhasil  dikumpulkan  yaitu  sebanyak 
1.449.256 
tweet  dengan  rincian  pada  bulan  November 
sebanyak 745.962 tweet dan bulan Desember sebanyak 703.294 
tweet. 

Preprocessing 
Tahap  selanjutnya  setelah  pengumpulan  data  yaitu 
preprocessing. Pada preperocessing dilakukan beberapa tahap 
yang alurnya ditunjukkan pada Gambar 5. 

 4 / 9 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

TABEL I 

HASIL DARI TAHAPAN PREPROCESSING 

No 

tweet  

tweet_clean 

1 

2 

3 

4 

@seyiyiyi Udh biasa kan sel 

udh biasa kan sel 

aduh malu, udah ngotot eh tapi 
gw yg salah. Rasanya udh 
masukin alamat rmh yg bener, 
taunya alamat kantor. wtf ... 
sianying duhh! 
Alhamdulillah ..fankit 
@happybox__08 udah 
nyampek ðŸ˜Š,semoga nambah 
lagi koleksinya.Trimakasih 
sudah ikutan â˜ºï¸â˜ºï¸ðŸ’™ 
Surge Upon a Surge' of Virus 
Cases Now Threatens to 
Decimate The US For One 
Reason  AFP  29 NOVEMBER 
2020 

aduh malu udah ngotot eh gw 
yg salah rasa udh masukin 
alamat rmh yg bener tau 
alamat kantor wtf sianying 
duh 
alhamdulilah fankit udah 
nyampek moga nambah 
koleksinyatrimakasih ikut  

surge upon a surge of virus 
cases now threatens to 
decimate the us for one reason 
afp november 

Setelah  dilakukan  preprocessing,  dilakukan  data  cleaning 
yang  berupa  penghapusan  data  kosong 
(hasil  dari 
preprocessing),  duplikasi  tweet,  tweet  yang  bukan  berbahasa 
Indonesia,  dan  pemilahan  akun  pengguna  yang  bersifat 
personal.  

 Tahapan  data  cleaning  yang  pertama  yaitu  dilakukan 
penghapusan tweet yang kosong. Didapatkan data yang kosong 
sebanyak 86.414 tweet, sehingga masih tersisa 1.362.842 tweet 
atau  94%  dari  jumlah  tweet  awal  sebanyak  1.449.256  tweet. 
Contoh tweet yang dihapus bisa dilihat pada Tabel II. 

TABEL II 

TWEET KOSONG HASIL PREPROCESSING 

tweet 

tweet_clean 

â™¥â™¥â™¥  https://t.co/B52KjVGjVo 

Ø£ÙŽØ¹ÙÙˆØ°Ù 
Ø¨ÙØ§Ù„Ù„ÙŽÙ‘Ù‡Ù 
Ù…ÙÙ†ÙŽ 
Ø§Ù„Ø´ÙŽÙ‘ÙŠÙ’Ø·ÙŽØ§Ù†Ù(cid:144) 
Ø§Ù„Ø±ÙŽÙ‘Ø¬ÙÙŠÙ…Ù  
https://t.co/p3MYsYmYdA 
Ø³ÙˆØ§Ù‚ Ø³ÙŠØ§ØÙ‡ 
Ø£Ù†Ø¯ÙˆÙ†ÙŠØ³ÙŠØ§ 
â˜Žï¸(cid:143)+6287733192079  
#Ù…Ù…Ø´ÙŠ_Ø§Ù„Ù†ÙÙ„_Ø-
ÙŠ_Ø§Ù„Ø¹Ø²ÙŠØ²ÙŠÙ‡  
https://t.co/a5eEbcxeUF 
ðŸ¤£ðŸ¤£ðŸ¤£ðŸ¤£ 

A 

Tahapan  data  cleaning 

selanjutnya  yaitu  dilakukan 
penghapusan  duplikasi  pada  tweet  hasil  preprocessing.  Pada 
tahap penghapusan duplikasi ini didapatkan 241.920 tweet yang 
dihapus karena terdeteksi duplikasi tweet. Sehingga tweet yang 
tersisa  setelah tahap ini yaitu sebanyak 1.120.489  tweet yang 
mana  jumlah  tersebut  yaitu  sebanyak  82,2%  dari  tahapan 
sebelumnya  yaitu  1.362.842  tweet  atau  sebanyak  77,3%  dari 
jumlah tweet awal yaitu 1.449.256 tweet. 

 5 / 9 

Gambar 5. Tahapan Preprocessing 

yang 

pertama 

Tahapan 

preprocessing 

dilakukan 
penghapusan  url  dan  username  pada  teks.  Pada  teks  yang 
mengandung kata berawalan ‘@’ dan “https” dihapus. Setelah 
itu,  dilakukan  penghapusan  emoji  pada  teks.  Penghapusan 
emoji meliputi emotikon, simbol, bendera dan lain-lain. Namun 
dikarenakan  banyaknya  emoji  sehingga  masih  terdapat  emoji 
yang tidak terhapus. Selanjutnya dilakukan penghapusan tanda 
baca seperti karakter yang berada dalam tanda kurung berikut 
(!""#$%&'()*+,-./:;<=>?@[\\]^_`{|}~),  selain  itu  tahap  ini  juga 
menghapus angka 1- 9. Selain menghapus tanda baca dan angka, 
peneliti juga menambahkan penghapusan repetisi karakter dan 
spasi kosong pada fungsi diatas. 

tokenization.  Setelah 

Tahapan  berikutnya  dilakukan  pemotongan  kalimat 
menjadi kata per kata dalam bentuk list (tokenization). Namun 
sebelum  dilakukan  tokenization, dilakukan  pengubahan  huruf 
besar menjadi huruf kecil terlebih dahulu yang dilakukan saat 
tokenization, 
pemanggilan 
fungsi 
dilakukan  penghapusan  kata-kata 
stopword.  Stopword 
merupakan  kata-kata  umum  yang  banyak  ditemui  hampir 
disetiap kalimat dan tidak memiliki makna berarti. Contoh kata 
stopword yaitu “dan”, “di”, “ke”, “dari” dan sebagainya. Pada 
penelitian 
sastrawi  untuk 
menyiapkan daftar kata-kata stopword untuk Bahasa Indonesia. 
Terakhir,  dilakukan  pengubahan  kata-kata  pada  teks  menjadi 
kata dasar seperti “semoga” menjadi “moga” atau dengan kata 
lain dilakukan penghapusan imbuhan pada kata yang ada pada 
teks. Pada tahap ini juga menggunakan package sastrawi untuk 
mendapatkan list kata-kata dalam Bahasa Indonesia yang akan 
diubah menjadi kata dasarnya.  

ini  menggunakan  package 

Tabel  1  menunjukkan  contoh  data  sebelum  dan  setelah 

melewati setiap tahapan preprocessing. 

 
 
 
 
 
 
 
 
 
 
 
 
  
                
    
  
  
 
 
Tahapan data cleaning yang terakhir yaitu penghapusan tweet 
yang  bukan  berbahasa  Indonesia.  Pada  tahap  ini,  dilakukan 
deteksi  bahasa  pada  tweet  menggunakan  package  langdetect. 
Hasil  dari  deteksi  bahasa  menggunakan  langdetetct  kepada 
tweet  hasil  tahapan  sebelumnya  yaitu  terdapat  560.885  tweet 
yang tidak menggunakan Bahasa Indonesia dan sisanya yaitu 
64,05% dari tahapan sebelumnya atau sebanyak 780.604 tweet 
yang berbahasa Indonesia. Tabel III menunjukkan contoh tweet 
yang tidak terdeteksi berbahasa Indonesia. 

TABEL III 
TWEET YANG TERDETEKSI BUKAN BERBAHASA INDONESIA 

Tweet 

bahasa 

ayo sini odey 

Tagalog 

ngakak bgt mas mas jawa 

Tagalog 

Tagalog 

Somalia 

English 

Pagi 

wadon ora kaya ramane 
lah 
hahaha that was year ago 
in finland i try to fel how 
you fel right now in blody 
cold winter hapy 
Christmas 

Dari  hasil  deteksi  bahasa  masih  terdapat  tweet  berbahasa 
Indonesia namun tidak terdeteksi sebagai bahasa Indonesia. Hal 
ini dikarenakan penggunaan kata  yang tidak baku pada  tweet 
sehingga tidak terdeteksi sebagai bahasa Indonesia. Meskipun 
begitu,  peneliti 
tweet 
berbahasa Indonesia berdasarkan package langdetect. 

tetap  menggunakan  hasil  deteksi 

Selanjutnya  dilakukan  pemilahan  akun  Twitter  yang 
bersifat  personal  dan  non-personal.  Pemilahan  dilakukan 
dengan  cara  membuat  kamus  dengan  mendaftar  akun  non-
personal  yang  diberi  label  secara  manual dari 10.000  sampel 
tweet.  Pemberian  label  akun  non-personal  dilakukan  dengan 
dasar  melihat  konten  akun  Twitter  secara  manual  apakah 
merupakan  akun  untuk  bisinis,  pemerintah,  organisasi  non 
pemerintah  atau  media  massa.  Kamus  yang  berisi  136  daftar 
akun 
untuk 
kemudian 
mengkategorikan seluruh tweet dari hasil tahapan sebelumnya 
yaitu  filter  bahasa.  Hasilnya  didapatkan  sebanyak  712.894 
tweet yang berasal dari akun personal dan sisanya 5.023 tweet 
yang  berasal  dari  akun  non-personal.  Tweet  yang  sudah 
dilakukan deteksi indikasi depresi yaitu tweet yang berasal dari 
akun  personal.  Contoh  akun  non  personal  antara 
lain 
“husada_menik”, 
“polsektirtajaya”, 
“cilegonpolsek”, “dpmptspbogorkab”, dll. 

“salestokoislam”, 

non-personal 

digunakan 

Deteksi Indikasi Depresi 
Tweet yang sudah dilakukan deteksi indikasi depresi yaitu 
tweet  dari  hasil  tahapan  preprocessing  hingga  filter  akun. 
Jumlah tweet yang tersisa dan sudah dilakukan deteksi indikasi 
depresi yaitu sebanyak 712.894 tweet atau sebanyak 49.2% dari 
jumlah  tweet  awal  hasil  scraping  yang  sebanyak  1.449.256 
tweet. Jumlah tweet dari setiap tahapan preprocessing dan data 
cleaning dapat dilihat pada Tabel IV. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

TABEL IV 
JUMLAH TWEET DARI TAHAPAN PREPROCESSING DAN DATA 
CLEANING 

Tahapan 

Dihapus 

Tersisa 

Jumlah 

Preprocessing 

Hapus duplikasi 

Filter Bahasa 

Filter akun 

86.414 

1.362.842 

1.449.256 

241.920 

1.120.922 

1.362.842 

403.005 

717.917 

1.120.922 

5.023 

712.894 

717.917 

Hasil 

tweet  dari 

tahapan  preprocessing  selanjutnya 
diklasifikasikan  menjadi  “1”  untuk  tweet  yang  terindikasi 
depresi  dan  “0”  untuk  tweet  yang  tidak  terindikasi  depresi. 
Langkah  pertama  dalam  tahapan  ini  yaitu  memberikan  skor 
tweet  berdasarkan  lexicon  yang 
pada  setiap  kata  pada 
digunakan  yaitu  LabMT  dan  Sentistrength.  Contoh  dari  hasil 
skoring  terlihat  dari  Tabel  V  dimana  skor  Sentistrength 
merupakan  hasil  skoring  menggunakan  lexicon  Sentistrength 
dan  skor  LabMT  merupakan  hasil  skoring  menggunakan 
lexicon LabMT. 

TABEL V 
SKORING MENGGUNAKAN LEXICON 

No 

tweet 

1 

2 

3 

4 

5 

bulan akhir masyaalah lihat kota 
jakarta banyak orang juang 
khilafah kamu gimana 
demokrasimatikhilafahdinanti 
mtdi 
gak bangun mimpi 

hamlud dakwah kota buru selatan 
jangan neng thok ae ayo naikin 
tagar hashtag gtgt 
demokrasimatikhilafahdinanti 
demokrasisengsarakanperempuan 
demokrasisengsarakanperempuan 
bek 
hore nama sebut terima kasih kak 
kak pak 

Skor 
Sentistrength 
[] 

[3] 

[] 

[4] 

juang khilafah kota padang 
panjang mana suara salam juang 
khilafah jakarta jangan lupa suport 
demokrasimatikhilafahdinanti 
demokrasisengsarakanperempuan 

[1, -4] 

Skor 
LabMT 
[6.06, 4.56, 
5.72, 5.76, 
6.04, 6.42, 
5.58, 5.72, 
5.22] 
[4.2, 5.8, 
6.34] 
[5.76, 4.58, 
5.22, 4.04, 
5.76, 4.84, 
6.4, 4.92] 

[7.4, 5.58, 
5.3, 6.14, 
7, 5.82, 
5.82, 5.6] 
[5.76, 5.9, 
5.5, 4.9, 
5.72, 6.38, 
6.04, 4.04, 
3.88] 

Selanjutnya dilakukan rerata skor pada setiap  tweet untuk 
mendapatkan  skor  akhir  dari  suatu  tweet.  Berdasarkan  skor 
akhir  tersebut,  masing-masing  tweet  diberikan  label  “1”  jika 
terindikasi depresi, dan label “0” jika tidak terindikasi depresi. 
Pemberian label mengacu kepada nilai cut-off masing-masing 
lexicon seperti yang telah dijelaskan pada bagian metodologi. 
Berikut contoh dari hasil skor akhir dan hasil labeling masing-
masing tweet yang terdapat pada Tabel VI. 

 6 / 9 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
TABEL VI 
SKOR AKHIR DAN HASIL KLASIFIKASI INDIKASI DEPRESI PADA 
TWEET 

No 

Skor Akhir 
Sentistrength 

1 

2 

3 

4 

5 

3 

4 

Skor 
Akhir 
LabMT 
5.675556 

5.446667 

5.19 

6.0825 

Indikasi 
Depresi 
Sentistrength 
0 

Indikasi 
Depresi 
LabMT 
0 

0 

0 

0 

1 

0 

0 

0 

0 

-1.5 

5.346667 

total 

tweet 

secara 

keeluruhan. 

Tabel  VII  menunjukkan  jumlah  tweet  hasil  klasifikasi 
menggunakan  lexicon  LabMT  dan  Sentistrength.  LabMT 
mengklasifikasikan  tweet  yang  terindikasi  depresi  sebanyak 
5.898 tweet atau sebanyak 0,83% dari total tweet keseluruhan. 
Sedangkan  untuk  Sentistrength,  mengklasifikasikan  tweet 
terindikasi  depresi  sebanyak  147.336  atau  sebanyak  20,67% 
Sentistrength 
dari 
mengklasifikasikan  tweet  yang  terindikasi  depresi  jauh  lebih 
banyak  dibanding  tweet  yang  diklasifikasikan  terindikasi 
depresi menurut  LabMT. Keduanya  mengklasifikasikan  tweet 
yang sama sebanyak 563.443 untuk tweet yang tidak terindikasi 
depresi  dan  3.783  untuk  tweet  yang  terindikasi  depresi.  Jadi 
kedua  lexicon  menghasilkan  klasifikasi  tweet  yang  sama 
sebanyak 567.226 tweet atau sebanyak 79,57% dari total tweet 
keseluruhan dan menghasilkan klasifikasi tweet yang berbeda 
sebanyak  145.668 
tweet 
keseluruhan.  
Jumlah 

terindikasi  depresi  berdasarkan 
Sentistrength  dan  LabMT  sangat  jauh  berbeda.  Hal  ini  dapat 
dikarenakan  oleh  jumlah  kata  pada  masing-masing  lexicon. 
Pada  Sentistrength  yang  dalam  versi  Bahasa  Indonesia  ini 
hanya  memiliki  jumlah  kata  sebanyak  1729  kata.  Sedangkan 
pada  LabMT  versi  Bahasa  Indonesia  yang  digunakan  pada 
penelitian  ini  memiliki  jumlah  kata  sebanyak  9980  kata. 
Banyaknya  jumlah  kata  pada  lexicon  mempengaruhi  proses 
labeling.  Banyaknya  tweet  yang  diklasifikasikan  ke  dalam 
tweet yang terindikasi depresi merupakan salah satu akibat dari 
pengaruh banyaknya kata yang ada pada lexicon. Banyak yang 
dalam satu tweet hanya terdapat satu kata yang diberi skor oleh 
Lexicon Sentistrength. Artinya jika suatu tweet mendapat satu 
skor  negatif  kurang  dari  (-1),  mengakibatkan  tweet  tersebut 
mendapat label “1” atau tweet terindikasi depresi. 

tweet  atau  20,43%  dari 

tweet  yang 

total 

TABEL VII 
SKOR AKHIR DAN HASIL KLASIFIKASI INDIKASI DEPRESI PADA 
TWEET 

LabMT 

h
t
g
n
e
r
t
s
i
t
n
e
S

0 

1 

Jumlah 

0 

1 

563.443 

2.115 

565.558 

143.553 

3.783 

147.336 

Jumlah 

706.996 

5.898 

712.894 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Jumlah pengguna Twitter dari 712.894 tweet yang dianalisis 
berasal dari 53.853 pengguna. Dari sejumlah pengguna tersebut, 
yang  memposting  tweet  terindikasi  depresi  menurut  LabMT 
yaitu  sebanyak  3795  pengguna.  Sedangkan  untuk  jumlah 
pengguna yang memposting tweet terindikasi depresi menurut 
Sentistrength yaitu sebanyak 26.081 pengguna. Sehingga dari 
angka pengguna tersebut didapatkan persentase pengguna yang 
memposting tweet terindikasi depresi berdasarkan LabMT yaitu 
sebesar  7,04%.  Sedangkan  persentase  pengguna  yang 
memposting 
berdasarkan 
terindikasi 
Sentistrength  yaitu  sebesar  48,4%.  Berdasarkan  data  yang 
terakhir  dari  Hasil  Riset  Kesehatan  Dasar  2018  menyatakan 
bahwa tingkat prevalensi penduduk usia 15 tahun ke atas untuk 
DKI Jakarta yaitu sebesar 5,9%[21]. Berdasarkan angka-angka 
tersebut,  deteksi  indikasi  depresi  berdasarkan  lexicon  LabMT 
jauh lebih mendekati. 

depresi 

tweet 

Contoh tweet yang terindikasi depresi menurtu LabMT dan 

Sentistrength dapat dilihat pada Tabel VIII dan Tabel IX. 

TABEL VIII 
TWEET YANG TERINDIKASI DEPRESI BERDASARKAN LABMT 

No 

tweet 

1  bukan pusiing penyakit..tp pusing 

pikiran ðŸ˜”ðŸ˜” #pusing #gakkerja2  
#garagaracorona  
#efeknyasampesekarang 

2  Depresi terus bunuh dirinya kaga 

3  Ancur di koyak koyak ekspetasi. 

4  Tombol EXITnya dimana sih ini 
anjir ?!!! CAPEK BANGSAT! 

5  @Zoelvichar_ Masih ada pening 

pening dan susah tidur 

TABEL IX 
TWEET YANG TERINDIKASI DEPRESI BERDASARKAN 
SENTISTRENGTH 

No 

1 

2 

3 

tweet 
Aku smakin sadar kalo banyak waktuku yg 
terbuang sia sia krn hal yg ga seharusnya aku 
terlalu khawatirkan di hidupku 

gua ga bisa tidur grÂ² baca au anjr mana tadi 
minum kopi ðŸ’†ðŸ¿â€â™€ï¸ 

Pusing , trauma , depresi , capek . Udah 
semua . Tinggal apa ??? 

4  @chocoocheese Disemangatin malah sedih 

gua:( 

5  @teobokiboy Gatau nih Lg stress bgt akhir2 

ini Lg banyak yg di pikirin 

 7 / 9 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
  
  
  
  
 
Berdasarkan  wordcloud  Gambar  6  dapat  dilihat  kata-kata 
yang  sering  muncul  pada  tweet  yang  terindikasi  depresi 
berdasarkan  LabMT  yaitu  kata  yang  memiliki  makna  negatif 
seperti “parah”, “takut”, “mati”,”bangsat” dan “sedih”. 

Gambar 6. Wordcloud tweet terindikasi depresi menurut LabMT 

Berbeda dengan LabMT, kata-kata yang sering muncul pada 
tweet 
terindikasi  depresi  oleh  Sentistrength  banyak 
mengandung  kata-kata  yang  memiliki  makna  netral  seperti 
“aku”, “mau”, “orang”, dan kata-kata lainnya yang terlihat dari 
wordcloud  pada  Gambar  7.  Kata-kata  yang  sering  muncul 
tersebut  masih  banyak  yang  termasuk  kata-kata  yang  tidak 
memiliki makna seperti atau stopwords “yg”,”ga”, “aja”, “gak” 
dan lainnya. Hal tersebut dikarenakan pada penelitian ini tidak 
terdapat tahapan normalisasi pada preprocessing. Normalisasi 
pada penelitian ini tidak dilakukan karena  pada penelitian ini 
ingin mempertahankan kata-kata asli yang ada tweet sehingga 
tidak menghilangkan sifat asli dari kata-kata pada tweet yang 
ada. Selain itu, Hal ini menunjukkan bahwa walaupun banyak 
tweet  yang  diklasifikasikan  terindikasi  depresi  oleh  lexicon 
Sentistrength, itu bukan berarti menunjukkan hasil yang lebih 
akurat. 

Gambar 7. Wordcloud tweet terindikasi depresi menurut Sentistrength 

Jika  dilihat  dari  persentase  jumlah  tweet  yang  terindikasi 
depresi  terhadap  jumlah  tweet  secara  keseluruhan  per  hari, 
kedua  lexicon  memiliki  pola  yang  berbeda.  Berdasarkan 
lexicon  LabMT  persentase  jumlah  tweet  yang  terindikasi 
depresi  memiliki  rata-rata  sebesar  0,84%,  sedangkan  untuk 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Sentistrength  sebesar  20,66%.  Persentase  jumlah  tweet  yang 
terindikasi  depresi  tertinggi  berdasarkan  LabMT  berada  pada 
awal  bulan  Desember  yaitu  tanggal  1  dan  3  sebesar  1,25%. 
Sedangkan  persentase  jumlah  tweet  yang  terindikasi  depresi 
tertinggi  berdasarkan  Sentistrength  berada  pada  awal  bulan 
November  sebesar  22,53%.  Namun  jika  dilihat  secara  lebih 
rinci,  persentase  jumlah  tweet  yang  terindikasi  depresi  untuk 
awal bulan Desember berdasarkan Sentistrength juga tergolong 
diatas  rata-rata  yaitu  sekitar  21%.  Untuk  persentase  jumlah 
tweet  yang  terindikasi  depresi  terendah  berdasarkan  LabMT 
yaitu  pada  16  Desember  sebesar  0,59%.  Sedangkan  untuk 
persentase  jumlah  tweet  yang  terindikasi  depresi  berdasarkan 
Sentistrength  berada  pada  akhir  tahun  yaitu  31  Desember 
sebesar  18,28%.  Lebih  jelasnya  mengenai  perbandingan 
persentase  jumlah  tweet  terindikasi  depresi  (per  hari)  oleh 
kedua lexicon dapat dilihat pada Gambar 8 dan Gambar 9. 

Gambar  8.  Persentase  jumlah  tweet  (per  hari)  yang  terindikasi  depresi 

menurut LabMT 

Gambar  9.  Persentase  jumlah  tweet  (per  hari)  yang  terindikasi  depresi 

menurut Sentistrength 

Jika dibandingkan dengan jumlah kasus baru positif covid-
19  (per  hari)  pada  Gambar  10  yang  diambil  dari  website 
Pemerintah Daerah DKI Jakarta[22], terlihat persentase jumlah 
tweet  yang  terindikasi  depresi  berdasarkan  lexicon  LabMT 
lebih  mendekati  karena  memiliki  tren  yang  sama  yaitu  naik. 
Sedangkan  untuk  persentase  jumlah  tweet  yang  terindikasi 
depresi  berdasarkan  Sentistrength  terlihat  memiliki  tren  yang 
stabil atau datar. 

 8 / 9 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Kesehatan Mental di Indonesia,” Insa. J. Psikol. dan Kesehat. 
Ment., vol. 5, no. 2, p. 162, 2020, doi: 
10.20473/jpkm.v5i22020.162-171. 
American Psychiatric Association, “What Is Depression?,” 2020. 
https://www.psychiatry.org/patients-families/depression/what-is-
depression (accessed Jan. 14, 2021). 
We Are Social & Hootsuite, “Indonesia Digital report 2020,” Glob. 
Digit. Insights, p. 247, 2020, [Online]. Available: 
https://datareportal.com/reports/digital-2020-global-digital-
overview.(accesed Dec. 19,2020) 
Z. Papacharissi, “The Self Online: The Utility of Personal Home 
Pages,” J. Broadcast. Electron. Media, vol. 46, no. 3, pp. 346–368, 
2002, doi: 10.1207/s15506878jobem4603_3. 
R. Gopalakrishna Pillai, M. Thelwall, and C. Orasan, “Detection of 
Stress and Relaxation Magnitudes for Tweets,” Web Conf. 2018 - 
Companion World Wide Web Conf. WWW 2018, no. April, pp. 
1677–1684, 2018, doi: 10.1145/3184558.3191627. 
S. Almouzini, M. Khemakhem, and A. Alageel, “Detecting Arabic 
Depressed Users from Twitter Data,” in Procedia Computer 
Science, Jan. 2019, vol. 163, pp. 257–265, doi: 
10.1016/j.procs.2019.12.107. 
J. Bata, “# AkuGalau : Korpus Bahasa Indonesia untuk Deteksi 
Emosi dari Teks,” vol. 12, pp. 103–110, 2019. 
S. C. Guntuku, D. B. Yaden, M. L. Kern, L. H. Ungar, and J. C. 
Eichstaedt, “Detecting depression and mental illness on social 
media: an integrative review,” Curr. Opin. Behav. Sci., vol. 18, pp. 
43–49, 2017, doi: 10.1016/j.cobeha.2017.07.005. 
M. De Choudhury, M. Gamon, S. Counts, and E. Horvitz, 
“Predicting depression via social media,” Proc. 7th Int. Conf. 
Weblogs Soc. Media, ICWSM 2013, vol. 2, pp. 128–137, 2013. 
A. Sekar Asri and S. Mariyah, “Asia – Pacific Economic Statistics 
Week 2019,” in Asia – Pacific Economic Statistics Week 2019, 
2019, no. June. 
A. G. Reece, A. J. Reagan, K. L. M. Lix, P. S. Dodds, C. M. 
Danforth, and E. J. Langer, “Forecasting the onset and course of 
mental illness with Twitter data,” Sci. Rep., vol. 7, no. 1, pp. 1–11, 
2017, doi: 10.1038/s41598-017-12961-9. 
N. Farruque, O. Zaiane, and R. Goebel, “Augmenting Semantic 
Representation of Depressive Language: From Forums to 
Microblogs,” Lect. Notes Comput. Sci. (including Subser. Lect. 
Notes Artif. Intell. Lect. Notes Bioinformatics), vol. 11908 LNAI, 
pp. 359–375, 2020, doi: 10.1007/978-3-030-46133-1_22. 
M. Thelwall, “The Heart and Soul of the Web? Sentiment Strength 
Detection in the Social Web with SentiStrength,” vol. 5, pp. 119–
134, 2017, doi: 10.1007/978-3-319-43639-5_7. 
S. M. Sarsam, H. Al-Samarraie, A. I. Alzahrani, W. Alnumay, and 
A. P. Smith, “A lexicon-based approach to detecting suicide-related 
messages on Twitter,” Biomed. Signal Process. Control, vol. 65, no. 
November 2020, p. 102355, 2021, doi: 
10.1016/j.bspc.2020.102355. 
“twint/README.md at master · twintproject/twint,” 2021. 
https://github.com/twintproject/twint/blob/master/README.md 
(accessed Jan. 14, 2021). 
“andyreagan/labMT-simple: a simple labMT usage script,” 2020. 
https://github.com/andyreagan/labMT-simple (accessed Jan. 14, 
2021). 
R. Sianipar and E. B. Setiawan, “Pendeteksian Kekuatan Sentimen 
Pada Teks Tweet Berbahasa Indonesia Menggunakan 
Sentistrength,” e-Proceeding Eng.  Vol.2, No.3 Desember 2015, vol. 
2, no. 3, pp. 7922–7928, 2015, [Online]. Available: 
https://openlibrary.telkomuniversity.ac.id/pustaka/files/104179/jurn
al_eproc/jurnal_eproc.pdf. 
Pusat Data dan Informasi Kementrian Kesehatan RI, “Situasi 
Kesehatan Jiwa DI Indonesia,” InfoDATIN. p. 12, 2019. 
“Covid-19.” https://corona.jakarta.go.id/id/data-pemantauan 
(accessed Jul. 01, 2021). 

[5] 

[6] 

[7] 

[8] 

[9] 

[10] 

[11] 

[12] 

[13] 

[14] 

[15] 

[16] 

[17] 

[18] 

[19] 

[20] 

[21] 

[22] 

 9 / 9 

Sumber : https://corona.jakarta.go.id/id/data-pemantauan 
Gambar 10. Jumlah kasus baru positif covid19 (harian) 

VII. 

PENUTUP 

jauh 

lebih  sedikit  dibanding 

Penelitian ini menunjukkan bahwa  deteksi indikasi depresi 
pada tweet berbahasa Indonesia dapat dilakukan menggunakan 
lexicon LabMT dan Sentistrength. Lexicon LabMT mendeteksi 
indikasi  depresi 
lexicon 
Sentistrength  yaitu  sebanyak  5.898  tweet  untuk  LabMT  dan 
147.336 untuk  Sentistrength dari keseluruhan  tweet sebanyak 
712.894 tweet. Namun demikian, kata-kata yang sering muncul 
pada  tweet  yang  terindikasi  depresi  berdasarkan  LabMT 
memiliki  makna  yang  negatif  dan  lebih  mengindikasikan 
depresi dibanding dengan kata-kata pada tweet yang terindikasi 
depresi  berdasarkan  Sentistrength.  Persentase  jumlah  tweet 
yang terindikasi depresi per hari memiliki pola yang  berbeda 
dari kedua lexicon. LabMT lebih cenderung menghasilkan pola 
dengan tren naik. Sedangkan Sentistrength menghasilkan pola 
dengan tren yang stabil atau konsisten. 

Persentase pengguna  Twitter  yang memposting  tweet yang 
terindikasi  depresi  berdasarkan  LabMT  yaitu  7,04%.  Angka 
tersebut  lebih  mendekati  tingkat  prevalensi  depresi  pada 
penduduk  usia  15  tahun  keatas  di  DKI  Jakarta  yaitu  sebesar 
5,9%. Persentase pengguna yang memposting tweet terindikasi 
depresi  berdasarkan  Sentistrength  jauh  melampaui  angka 
tersebut,  yaitu  sebesar  48,4%.  Dengan  demikian,  pada 
penelitian  ini  lebih  merekomendsaikan  lexicon  LabMT  untuk 
digunakan  dalam  mendeteksi  indikasi  depresi  pada  teks 
berbahasa  Indonesia  di  media  sosial.  Pengembangan  lebih 
lanjut  dalam  penggunaan  LabMT  dalam  mendeteksi  indikasi 
depresi pada tweet berbahasa Indonesia bisa menjadi alat untuk 
mendapatkan  data  pendukung  mengenai  tingkat  depresi  di 
Indonesia.  Sehingga  monitoring  mengenai  tingkat  depresi  di 
Indonesia bisa lebih kerap dilakukan. 

DAFTAR PUSTAKA 

[1] 

[2] 

[3] 

[4] 

Kementerian Kesehatan RI Badan Penelitian dan Pengembangan, 
“Hasil Utama Riset Kesehatan Dasar,” Kementrian Kesehat. 
Republik Indones., pp. 1–100, 2018, [Online]. Available: 
http://www.depkes.go.id/resources/download/info-terkini/hasil-
riskesdas-2018.pdf. 
Kemenkes RI, Profil Kesehatan Indonesia Tahun 2019, vol. 42, no. 
4. 2019. 
W. Sulistyorini and M. Sabarisman, “Depresi : Suatu Tinjauan 
Psikologis,” Sosio Inf., vol. 3, no. 2, pp. 153–164, 2017, doi: 
10.33007/inf.v3i2.939. 
I. A. Ridlo, “Pandemi COVID-19 dan Tantangan Kebijakan 

 
 
 
 
 
 
 
"
221709708,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pengklasifikasian Kemiskinan di Indonesia dengan 
Menggunakan Metode Algoritma Decision Tree 
C5.0 

Frenaldy Wahyudi Wenas (221709708, 4SD1) 
Dosen Pembimbing: Robert Kurniawan, SST, M.Si. 

Ringkasan—  Kemiskinan  merupakan  masalah  yang  komplek 
bagi  banyak  negara.  Di  Indonesia,  persentase  kemiskinan  di 
beberapa  provinsi  masih  memerlukan  perhatian  serius  dari 
ini  bertujuan  untuk 
pemerintah.  Sehingga,  penelitian 
mengklasifikasikan  provinsi-provinsi  di  Indonesia  ke  dalam 
kategori  kemiskinan  berdasarkan  target  persentase  kemiskinan 
yang ada. Dengan menggunakan data yang diperoleh dari Badan 
Pusat  Statistik  (BPS)  serta  menerapkan  metode  klasifikasi 
Decision Tree dengan algoritma C5.0 dari variabel-variabel yang 
diduga mampu mengelompokkan kemiskinan. Decision Tree yang 
telah  dibentuk  kemudian  dilakukan  proses  boosting  sehingga 
didapatkan tiga model unik terbaik dengan error rate sebesar 2.9 
persen. Variabel-variabel yang termasuk ke dalam model tersebut 
adalah  harapan  hidup,  persentase  Kelompok  Usaha  Bersama 
(KUBE), tingkat pengangguran terbuka, persentase pekerja tani 
di  sektor  informal,  rata-rata  lama  sekolah,  dan  persentase 
penduduk  pengguna  ponsel.  Berdasarkan  hasil  pengolahan 
dengan Variable Importance Measure, diperoleh tiga variabel yang 
berkontribusi  besar  dalam  pemodelan  ini  yaitu  harapan  hidup, 
persentase  sekolah  dasar  di  desa/kelurahan,  dan  persentase 
pekerja tani di sektor informal.  

Kata Kunci— Klasifikasi, kemiskinan, decision tree, algoritma 

C5.0 

I.  LATAR BELAKANG 

Kemiskinan  masih  menjadi  permasalahan  krusial  bagi 
seluruh  negara  di  dunia.  Hal  ini  berujung  pada  penetapan 
kemiskinan sebagai tujuan nomor satu dalam SDGs oleh United 
Nations. Pada tahun 2015, terdapat sekitar 736 juta penduduk 
dunia atau yang setara dengan sepuluh persen penduduk dunia 
yang  menghabiskan  biaya  hidup  kurang  dari  $1.90  sehari; 
banyak  yang  kekurangan  makanan,  air  minum  bersih  dan 
sanitasi. Angka ini memang banyak menurun jika berpatokan 
pada data kemiskinan dunia pada tahun 1990 dimana 36 persen 
penduduk dunia hidup dalam keadaan tersebut. Tetapi, dengan 
secara 
adanya  perkembangan  pengentasan  kemiskinan 
internasional  yang 
laju 
pengurangan  kemiskinan,  memunculkan  kekhawatiran  baru 
itu, 
bagi  penyelesaian  kemiskinan 
pengentasan kemiskinan dunia masih jauh dari kata berakhir. 

tidak  merata  dan  melambatnya 

[34].  Oleh  karena 

Kemiskinan  menurut  World  Bank  [30]  merupakan  suatu 
ketidakcukupan/kekurangan  kesejahteraan.  Secara  absolut, 
World  Bank  mengukur  kemiskinan  berdasarkan  pendapatan 
yang kurang dari $2 per hari. Sedangkan menurut Badan Pusat 
Statistik,  kemiskinan  merupakan  dipandang  berdasarkan 
kemampuan  untuk  memenuhi  kebutuhan  dasar,  sehingga 

kemiskinan dipandang dari ketidakmampuan dari sisi ekonomi 
untuk  memenuhi  kebutuhan  dasar  makanan  dan  bukan 
makanan yang diukur dari sisi pengeluaran. 

Kemiskinan  yang  ada  di  Indonesia  sendiri,  dalam  hampir 
sepuluh  tahun  terakhir  ini  menurut  Badan  Pusat  Statistik 
menunjukkan  tren  yang  menurun.  Pada  Maret  2011,  tercatat 
ada  12,49  persen  penduduk  miskin.  Sedangkan  pada  Maret 
2020 persentase penduduk miskin turun menjadi 9,78 persen. 
Namun,  persentase  kemiskinan  ini  sedikit  berbeda  dari  yang 
ditargetkan  oleh  BAPPENAS  pada  tahun  2020  yakni  8,5-9,0 
persen. 

Jika  ditelaah  lebih  lanjut,  permasalahan  kemiskinan  yang 
ada  di  Indonesia  kurang  lebih  sama  dengan  apa  yang 
didefinisikan oleh World Bank [34]. Pertama, laju pengurangan 
kemiskinan  yang  cenderung  melambat.  Jika  melihat  satu 
dekade  ini  laju  pengurangan  kemiskinan  di  Indonesia  hanya 
mencapai angka 2.38 persen. Angka ini menunjukan penurunan 
dibandingkan  dengan  dekade  sebelumnya  yang  mencapai 
angka  3,42  persen.  Masalah  berikutnya  yaitu  perkembangan 
pengentasan  kemiskinan  yang  tidak  merata  yang  ada  di 
Indonesia. Hal ini bisa dilihat dari besaran persentase penduduk 
miskin  menurut  provinsi  untuk  tahun  2020  yang  ada  di 
Indonesia  yang  masih  tidak  merata.  Karena  belum  bisa 
menjangkau  seluruh  provinsi  yang  ada,  khususnya  provinsi 
yang berada di daerah timur Indonesia. 

Masalah kemiskinan memanglah sebuah masalah mendasar 
yang  kompleks  dan  bersifat  multidimensional  [2].  Sehingga 
ketersediaan  data  kemiskinan  yang  akurat 
sangatlah 
dibutuhkan  untuk  membantu  dalam  pengambilan  keputusan. 
Sifat  multidimensional  dari  kemiskinan  ini  menyebabkan 
kemiskinan bisa dilihat dari berbagai faktor. Sehingga indikator 
yang bisa digunakan dalam pengukurannya sangatlah banyak. 
Dari  kumpulan  indikator  yang  banyak  tersebut,  berpotensi 
terbaik  yang  mampu  menjelaskan 
memiliki 
kemiskinan.  Salah  satu  cara  untuk  menemukan  indikator 
tersebut adalah dengan melakukan klasifikasi. 

indikator 

diantaranya 

Banyak  metode  klasifikasi  yang  telah  dikembangkan  oleh 
banyak  peneliti  terdahulu,  salah  satunya  adalah  metode 
decision  tree.  Metode  ini  dipilih  karena  keunggulannya  yang  
beberapa 
yang  mudah 
adalah 
diinterpretasikan,  menghilangkan  feature  yang  tidak  penting, 
dan lebih efisien dibandingkan model lain yang lebih kompleks 
[13].  Dengan  adanya  penghilangan  ini,  penentuan  indikator-
indikator apa saja yang mampu menjelaskan kemiskinan sebaik 
mungkin bisa dipilih dari banyak variabel yang tersedia. 

hasil 

 1 / 8 

 
 
 
 
 
Kocakoc  [10],  pada  klasifikasi  decision  tree,  data  latih 
dibagi  ke  dalam  subset  berdasarkan  beberapa  karakteristik. 
Proses ini kemudian akan diulangi secara rekursif dan berlanjut 
hingga pengulangan tidak memiliki efek lagi pada model atau 
yang disebut recursive partitioning. Terdapat banyak algoritma 
untuk klasifikasi decision tree seperti Random Forest, Boosted 
Trees,  Rotation  Forest,  ID3,  C4.5,  C5.0,  Classification  and 
Regression  Tree  (CART),  QUEST,  CRUISE,  Chi-Square 
Automatic Interaction Detector (CHAID), dan MARS [10]. 

yang 

diatas, 

Seperti 

terdapat 

disebutkan 

banyak 
implementasi untuk decision tree, tapi salah satu yang paling 
dikenal,  khususnya  dalam  dunia  industri,  adalah  dengan 
menggunakan  algoritma  C5.0  [14].  Algoritma  ini  sendiri 
merupakan  sebuah  versi  hasil  pengembangan  dari  algoritma 
sebelumnya, C4.5, yang mana juga merupakan pengembangan 
dari algoritma ID3. 

Berdasarkan hasil uraian diatas, maka penelitian ini berfokus 
pada pengklasifikasian provinsi-provinsi yang ada di Indonesia 
berdasarkan  status  kemiskinannya  dan  diperoleh  indikator-
indikator yang mampu menjelaskan kemiskinan dengan lebih 
baik.  Model  klasifikasi  yang  digunakan  adalah  klasifikasi 
decision tree dengan algoritma C5.0. 

II.  TUJUAN PENELITIAN 
Adapun tujuan dari penelitian ini adalah sebagai berikut: 
1.  Mengklasifikasikan  dan  memetakan  kemiskinan  di 
Indonesia  menurut  Provinsi  dengan  menggunakan 
metode decision tree algoritma C5.0. 

2.  Memperoleh  indikator-indikator  terbaik  yang  mampu 

menjelaskan kemiskinan. 

3.  Melihat  hubungan  antar  indikator  kemiskinan  yang 

diperoleh.  

III. PENELITIAN TERKAIT 

Sumargo  dan  Simanjuntak  [30]  melakukan  penelitian 
tentang deprivasi utama kemiskinan multidimensi di Indonesia 
dengan menggunakan data SUSENAS dan metode pengukuran 
kemiskinan multidimensi Alkire-Foster, serta 12 indikator yang 
berasal  dari  tiga  dimensi  yaitu  pendidikan,  kesehatan  dan 
standar  hidup.  Ditemukan  bahwa  penduduk  miskin 
multidimensi mengalami deprivasi pada indikator lama sekolah 
dan  imunisasi,  kecuali  di  Maluku  adalah  penolong  kelahiran, 
dan Papua adalah melek huruf. Berdasarkan hasil yang ada juga, 
ternyata  tiap-tiap  daerah  memiliki  prioritas  bantuan  program 
kemiskinan yang berbeda-beda. Oleh karena itu, penghitungan 
kemiskinan multidimensi yang berbasis pada pemenuhan hak-
hak dasar sekiranya dibutuhkan. 

Khaliq  dan  Uspri 

tentang  kemiskinan 
[9]  meniliti 
multidimensi dan kaitannya dengan perlindungan sosial dengan 
menggunakan  data  SUSENAS.  Berdasarkan  estimasi  yang 
dilakukan dengan metode SEM BAGGING, didapatkan bahwa 
pendidikan, kesehatan, dan kualitas hidup memiliki hubungan 
signifikan  negatif  terhadap  kemiskinan  multidimensi.  Hal  ini 
berarti perbaikan akses pangan, kesehatan dan pendidikan yang 
memadai  bagi  masyarakat  miskin  masih  dianggap  penting. 
Selain itu, perlindungan sosial di Indonesia memiliki hubungan 
positif dengan kemiskinan multidimensi. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

bahwa 

rendahnya 

faktor-faktor 

Sutomo  dan  Shalihati  [27]  dalam  penelitian  mengenai 
kemiskinan  di  Kabupaten  Purbalingga  dalam  perspektif 
yang 
geospasial  menemukan 
mempengaruhi  tingkat  keselarasan  antara  kemiskinan  dan 
perkembangan wilayah adalah rendahnya kesadaran mengenai 
pentingnya  pendidikan, 
tingkat  pendapatan, 
kurangnya  lapangan  pekerjaan  bagi  kaum  laki-laki,  serta 
kondisi morfologi wilayah yang condong berbukit dan berada 
pada daerah pegunungan dengan lereng terjal mendorong desa 
pada kesulitan dalam kondisi sosial ekonomi dan aksesibilitas. 
Firdausi  [4]  melakukan  estimasi  dan  proyeksi  tingkat 
kemiskinan  dengan  menggunakan  faktor-faktor  kemiskinan 
yang  antara  lain  adalah  PDRB  perkapita  dan  angka  harapan 
hidup. Berdasarkan hasil analisis yang ada, didapatkan bahwa 
PDRB perkapita dan angka harapan hidup memiliki pengaruh 
negatif yang signifikan terhadap tingkat kemiskinan. Sehingga, 
peningkatan pada dua faktor tadi bisa menjadi salah satu cara 
dalam menekan angka kemiskinan.  

Abu  Bakar  [1]  mencoba  mengklasifikasikan  kemiskinan 
multidimensi  di  Malaysia  dengan  menggunakan  Random 
Forest  dan  Decision  Tree  C4.5.  Berdasarkan  hasil  yang 
diperoleh Random Forest disimpulkan menjadi metode terbaik 
dengan pencapaian akurasi 99 persen. Tetapi, jika diperhatikan 
perbandingan  antara  hasil  yang  diperoleh  kedua  metode 
tersebut, akurasi yang diperoleh Decision Tree hanya memiliki 
selisih  satu  persen.  Selain  itu,  jika  diperhatikan  dari  waktu 
proses kedua metode, decision tree unggul dengan perbedaan 
yang hampir mencapai sepuluh kali lipat. Mengingat algoritma 
yang  digunakan  adalah  C4.5,  maka  algoritma  C5.0,  yang 
merupakan  pengembangan  dari  C4.5,  berpeluang  untuk 
memiliki performa lebih baik. 

Garg [6] dalam penelitiannya mengenai evaluasi penerimaan 
investor untuk emas berdasarkan beberapa  atribut atau faktor 
dengan  menggunakan  klasifikasi  decision 
tree.  Dalam 
penelitiannya digunakan dua macam decision tree, yaitu R Part 
Tree dan C50. Dari kedua jenis tree tersebut, peneliti kemudian 
membentuk 
diantaranya 
menggunakan R Part Tree dan dua lainnya menggunakan C50 
dengan perlakuan yang berbeda-beda. Dari hasil yang diperoleh 
didapatkan  bahwa  model  5  yang  menggunakan  C50  yang 
menggunakan semua variabel menunjukkan performa tertinggi 
dan  menunjukkan  bahwa  seluruh  variabel  berperan  penting 
dalam pengambilan keputusan investor.  

enam  model, 

empat 

yang 

IV. METODE PENELITIAN  

A.  Data 

Data  yang digunakan  dalam penelitian  ini  merupakan data 
sekunder  yang  berasal  dari  BPS  (Badan  Pusat  Statistik). 
Adapun  data  yang  digunakan  terdiri  atas  24  variabel  yang 
berkaitan dengan kemiskinan yang ada di seluruh provinsi di 
Indonesia pada tahun 2018.  

Pemilihan  variabel-variabel  tersebut  berdasarkan  World 
Bank [32] pada publikasinya yang berjudul “Making the new 
Indonesia  work  for  the  poor:  Era  Baru  dalam  Pengentasan 
Kemiskinan di Indonesia”. Disitu disebutkan bahwa terdapat 5 
faktor  penyebab  kemiskinan  utama  di  Indonesia,  yaitu 
pendidikan, tipe pekerjaan, gender, akses pada pelayanan dan 
infrastruktur dasar, dan lokasi geografis. 

 2 / 8 

 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Berdasarkan  uraian  dari  setiap  faktor  yang  ada  pada 
publikasi  tersebut  [32],  maka  beberapa  indikator  dipilih. 
Adapun indikator-indikator tersebut adalah sebagai berikut: 

purity [12]. Entropi dari data sampel mengindikasikan variasi 
dari nilai suatu kelas. Entropi secara umum bisa didefinisikan 
sebagai berikut: 

𝑐

TABEL I.   Faktor dan Indikator Kemiskinan  

𝐸𝑛𝑡𝑟𝑜𝑝𝑦(𝑆1) =   ∑ −𝑝𝑖𝑙𝑜𝑔2(𝑝𝑖)

(1) 

Faktor 
Kemiskinan 

Pendidikan 

Tipe 
Pekerjaan 

Gender 

Indikator 

Rata-rata Lama Sekolah 

Persentase Banyaknya Fasilitas SD di 
Desa/Kelurahan 
Persentase  Pekerja  Pertanian  di 
Sektor Informal 
Tingkat Pengangguran Terbuka 
Indeks Pemberdayaan Gender 

Air Layak 

Listrik  

Sanitasi Layak 

Fasilitas 
Persentase  Banyaknya 
SMP/SMA/SMK di Desa/Kelurahan 

Akses  pada 
Pelayanan 
dan 
Infrastruktur 
Dasar 

Lokasi 
Geografis 

Persentase  Banyaknya 
Lembaga 
Desa/Kelurahan 

Keterampilan 

Fasilitas 
di 

Persentase  Banyaknya 
Fasilitas 
Perkreditan  (KUR,  KUK,  KKP-E, 
KUBE) di Desa/Kelurahan 
Persentase  Penduduk  Usia  5  Tahun 
Ke  Atas  Yang  Memiliki  Telepon 
Seluler 
Rural Access Index 

Jawa/Bali,  Sumatera,  Kalimantan, 
Sulawesi,  Nusa  Tenggara/Maluku, 
Papua 

Berdasarkan  penelitian  terdahulu  yang  telah  dijabarkan, 
peneliti  kemudian  menambahkan  faktor  kesehatan,  dengan 
menggunakan  Harapan  Hidup  sebagai  indikatornya,  standar 
hidup layak, yang akan menggunakan Pendapatan per Kapita 
sebagai indikatornya dan penerimaan daerah yang digambarkan 
dengan  menggunakan  PDRB  perkapita.  Selain  itu,  untuk 
keperluan preprocessing, variabel jumlah desa untuk tiap-tiap 
provinsi juga ditambahkan. 

B.  Algoritma C5.0  

Algoritma  C5.0  merupakan  perkembangan  dari  algoritma 
C4.5 yang juga merupakan perkembangan dari algoritma ID3. 
Dengan  munculnya  algoritma  C5.0  ini,  keakurasian  mampu 
meningkat, dan menggunakan memori yang lebih sedikit dari 
yang pendahulunya [13]. 

Atribut  decision  tree  adalah  atribut  yang  memungkinkan 
untuk  mendapatkan  tree  yang  paling  kecil  ukurannya  atau 
atribut  yang  bisa  membagi  objek  menurut  kelasnya  [15]. 
Atribut yang kemudian dipilih adalah atribut yang paling murni 
(pure). Kemurnian ini kemudian diukur menggunakan ukuran 
purity.  C5.0  menggunakan  konsep  entropi  untuk  menghitung 

𝑖=1

Pada  persamaan  (1),  untuk  sekumpulan  data  tertentu  (S), 
nilai  c  adalah  banyaknya  kelas  yang  berbeda  dan  pi  adalah 
proporsi dari niali yang jatuh pada kelas ke-i.  

Berdasarkan  perhitungan  purity,  algoritma 

ini  harus 
memutuskan pada  feature mana yang akan dibagi.  Algoritma 
ini  menggunakan  entropi  untuk  menghitung  perubahan  pada 
homogenitas  yang  menghasilkan  pembagian  untuk  masing-
masing  feature,  yang  disebut  sebagai  information  gain. 
Information Gain untuk sebuah feature F dihitung berdasarkan 
perbedaan antara entropi pada segmen sebelum pembagian (S1) 
dan setelah pembagian (S2) [12]: 

𝐼𝑛𝑓𝑜𝐺𝑎𝑖𝑛(𝐹) = 𝐸𝑛𝑡𝑟𝑜𝑝𝑦(𝑆1) − 𝐸𝑛𝑡𝑟𝑜𝑝𝑦(𝑆2) 

(2) 

Satu  hal  yang  perlu  diperhatikan  ialah  setelah  pembagian, 
data akan dibagi menjadi lebih dari satu partisi. Oleh karena itu, 
fungsi perhitungan  entropi  perlu  memperhatikan  total  entropi 
untuk  keseluruhan  partisi  (Pi).  Hal  ini  dilakukan  dengan 
menimbang setiap entropi partisi berdasarkan proporsi  record 
masuk  ke  dalam  partisi  (wi).  yang  mana  bisa  didefinisikan 
sebagai berikut [12]: 

𝑛
𝐸𝑛𝑡𝑟𝑜𝑝𝑦(𝑆2) =   ∑ 𝑤𝑖𝐸𝑛𝑡𝑟𝑜𝑝𝑦(𝑃𝑖)
𝑖=1

(3) 

Semakin besar nilai information gain, semakin baik sebuah 
feature  dalam  membuat  kelompok-kelompok  yang  homogen 
setelah pembagian pada feature tersebut [10]. Jika information 
gain  bernilai  nol,  maka  tidak  ada  pengurangan  pada  entropi 
setelah  pembagian.  Di  sisi  lain  nilai  information  gain  sama 
dengan nilai entropi sebelum pembagian. Ini mengindikasikan 
bahwa  entropi  setelah  pembagian  sama  dengan  nol,  yang 
berarti  hasil  pembagian  menghasilkan  kelompok  yang 
homogen. 

Algoritma  C5.0  [36]  menangani  banyak  keputusan  secara 
otomatis  menggunakan  penalaran  yang  semestinya.  Strategi 
utamanya adalah dengan menerapkan post-pruning. Metode ini 
dilakukan dengan cara membentuk sebuah tree yang besar yang 
over  fit  dari  data  latih.  Setelah  itu,  node  dan  cabang  yang 
memiliki efek yang rendah pada klasifikasi akan dihilangkan. 
Karena  hal  inilah,  kemudian  algoritma  ini  digunakan  pada 
penelitian ini demi mendapatkan model terbaik dari banyaknya 
variabel yang digunakan. 

C.  Boosting pada C5.0 

Salah  satu  metode  untuk  mengurangi  kesalahan  pada 
prediksi  yang  dihasilkan  oleh  decision  tree  adalah  dengan 
menerapkan  algoritma  boosting.  Boosting  sendiri  adalah 
sebuah  pendekatan  machine  learning  yang  bertujuan  untuk 
membangun  aturan  prediksi  yang  sangat  akurat  dengan 

 3 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
menggabungkan aturan yang cenderung lemah dan tidak akurat 
[19].  Pada  C5.0,  algoritma  boosting  yang  diterapkan  adalah 
Adaptive  Boosting  atau  AdaBoost,  yang  dikembangkan  oleh 
Freund dan Schapire  [5]. Pokok utama dari AdaBoost adalah 
menimbang  data  point  pada  setiap  iterasi  boosting  secara 
berturut-turut selama pembangunan klasifikasi. 

AdaBoost  pada  dasarnya  menghasilkan  kumpulan  weak 
classifier, dimana pada setiap proses iterasi algoritmanya akan 
menemukan classifier terbaik berdasarkan penimbang sampel 
yang ada. Sampel yang terklasifikasi salah pada iterasi tertentu 
akan diberikan penimbang yang lebih berat, sedangkan sampel 
yang terklasifikasi benar akan menerima penimbang yang lebih 
ringan.  Hal  ini  dilakukan  untuk  memaksa  base  learner 
memberi perhatian khusus pada mereka [20]. 

C5.0 memang pada dasarnya menerapkan AdaBoost sebagai 
metode boosting-nya. Tetapi, algoritma yang digunakan telah 
dimodifikasi  sehingga  terdapat  beberapa  perbedaan  antara 
keduanya.  Adapun  perbedaan  antar  keduanya  adalah  sebagai 
berikut [8]:  
1)  C5.0  mencoba  untuk  mempertahankan  ukuran  tree  sama 
dengan  yang  awalnya  (yang  dibangun  tanpa  proses 
boosting).  

2)  C5.0  menghitung  peluang  kelas  untuk  seluruh  model 
boosted dan di dalam model ini, rata-rata penimbang juga 
dihitung.  Kemudian,  dari  model  tersebut,  C5.0  memilih 
kelas yang memiliki probabilitas maksimum yang ada di 
dalam grup. 

3)  Prosedur boosting berakhir jika model sudah sangat efektif 
(penjumlahan  dari  penimbang  untuk  sampel  yang 
terklasifikasi  salah  kurang  dari  0.1),  atau  sangat  tidak 
efektif (rata-rata penimbang sampel yang salah lebih dari 
50 persen) 

D.  Preprocessing 

Metode preprocessing digunakan untuk mengklasifikasikan 
persentase  kemiskinan  menjadi  dua  kelas,  yaitu  miskin  atau 
tidak.  Klasifikasi 
target  persentase 
ini  didasarkan  oleh 
kemiskinan  yang  ditentukan  oleh  BAPPENAS  yang  sesuai 
dengan Rencana Kerja Pemerintah pada 2018 yakni, 9,5 sampai 
10 persen [16]. Adapun yang dijadikan sebagai Batasan pada 
penelitian ini adalah 10 persen. 

seperti 

Persentase 

Banyaknya 

Persentase  Banyaknya 

Preprocessing juga dilakukan untuk beberapa indikator yang 
Fasilitas 
ada, 
SD/SMP/SMA/SMK, 
Fasilitas 
Lembaga  Kursus  dan  Pelatihan,  dan  Persentase  Banyaknya 
Fasilitas  Perkreditan  (KUR,  KUK,  KKP-E,  KUBE)  di 
Desa/Kelurahan.  Semua  indikator  tersebut  diperoleh  dari 
publikasi Potensi Desa. Pada publikasi tersebut, indikator yang 
akan digunakan belum dalam bentuk persentase jadi dilakukan 
penghitungan  dengan  cara  membaginya  dengan  jumlah  desa 
untuk tiap-tiap provinsi. Penggunaan persentase dilakukan agar 
indikator  yang  ada  memiliki  nilai  yang  setara  dan  bisa 
dibandingkan antara satu sama lain. 

Berdasarkan  definisinya  [33],  Rural  Access  Index  bisa 
dibagi  menjadi  empat  faktor,  yaitu  manfaat,  kualitas  jalan, 
kepadatan  jalan  pedesaan,  dan  kepadatan  penduduk.  Dalam 
penelitian  ini,  karena  keterbatasan  data  yang  ada,  jadi  hanya 
salah satu faktor yang digunakan, yaitu kualitas jalan. Prinsip 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

utama dari kualitas jalan yang dihitung adalah jalan yang bisa 
dilalui sepanjang tahun, tetapi sedikit gangguan dalam waktu 
singkat  karena  cuaca  masih  bisa  diterima  [35].  Karena  tidak 
adanya  data  panjang  jalan  dengan  kualitas  yang  baik,  maka 
dalam  penghitungan  ini  didekati  dengan  salah  satu  indikator 
yang  bisa  diperoleh  dari  data  Potensi  Desa  yakni  Banyaknya 
Desa/Kelurahan  yang  Menggunakan  Prasarana  Transportasi 
Darat  atau  Darat  dan  Air  Menurut  Keberadaan  Jalan  yang 
Dapat Dilalui Kendaraan Roda Empat. Sehingga kualitas jalan 
bisa dihitung sebagai berikut [33]: 

𝑄𝑢𝑎𝑙𝑖𝑡𝑦 =

𝐺𝑜𝑜𝑑𝑅𝑑𝐾𝑚
𝑇𝑜𝑡𝑎𝑙𝑅𝑑𝐾𝑚

(4) 

E.  Korelasi Pearson 

Koefision korelasi merupakan suatu ukuran yang digunakan 
untuk mengetahui derajat  hubungan antar variabel [24]. Nilai 
yang dihasilkan dari koefisien korelasi berada diantara -1 < 0 < 
1.  Jika  korelasi  bernilai  -1,  maka  variabel  X  dan  Y  memiliki 
korelasi negatif sempurna dan jika bernilai 1 maka variabel X 
dan  Y  memiliki  korelasi  positif  sempurna  [25].  Salah  satu 
koefisien  korelasi  yang  sering  digunakan  adalah  korelasi 
pearson. Koefisien korelasi pearson yang dinotasikan dengan r 
bisa dirumuskan sebagai berikut: 

1
𝑛

∑ (𝑋𝑖 − 𝑋̅)(𝑌𝑖 − 𝑌̅)

𝑛
𝑖=1

𝑟 =

((

1
𝑛

∑ (𝑋𝑖 − 𝑋̅)2

𝑛
𝑖=1

) (

1
𝑛

∑ (𝑌𝑖 − 𝑌̅)2

𝑛
𝑖=1

))

1
2

(5) 

V.  KERANGKA PIKIR 

Gambar 1.  

Kerangka Pikir Penelitian 

Pada  Gambar  1  disajikan  secara  visual  tentang  kerangka 
piker penelitian. Penelitian ini mengambil kemiskinan sebagai 
topik utama penelitian, lebih khususnya kemiskinan yang ada 
di  berbagai  provinsi  di  Indonesia.  Dengan  berdasarkan  pada 
lima  faktor  yang  mempengaruhi  kemiskinan  yang  telah 
diuraikan  oleh  World  Bank,  kemiskinan  kemudian  akan 
dikelompok-kelompokkan berdasarkan kriteria tertentu dengan 
menggunakan metode klasifikasi decision tree. Hasil yang ada 
kemudian  akan  diterapkan  pada  simulasi  dengan  data  yang 
tahapan 
tertentu  pada 
sama  untuk  melihat  pola 
pengklasifikasiannya.  Selanjutnya,  model 
telah 
didapatkan  akan  dilakukan  dipetakan  dan  dianalisis  untuk 

tiap 
yang 

 4 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
mendapatkan  indikator  kemiskinan  terbaik  yang  mampu 
menjelaskan kemiskinan di Indonesia. 

VI. HASIL DAN PEMBAHASAN 

A.  Klasifikasi Kemiskinan dengan Decision Tree C5.0 

Untuk  menentukan 

indikator  mana  yang  mampu 
menjelaskan  kemiskinan  dengan  baik,  indikator  yang  telah 
dipilih tersebut kemudian dimodelkan menggunakan algoritma 
C5.0  dengan  menggunakan  pemrograman  R.  Klasifikasi 
dilakukan  dalam  dua  tahapan  yaitu  tahapan  awal  dengan 
membangun  decision  tree  dan  kedua  dilanjutkan  dengan 
melakukan proses boosting sebanyak 100 kali.  

Dari 100 tree yang dihasilkan, terdapat tiga tree unik yang 
terbentuk yang memiliki error paling rendah, yakni 2.9 persen, 
yang  selanjutnya  akan  dianggap  sebagai  model  terbaik  pada 
penelitian ini. Dari ketiga model tersebut, dua model memiliki 
indikator  yang  serupa  hanya  saja  root  dari  kedua  tree  yang 
berbeda. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

adanya hubungan khusus antara ketiga indikator pada model 16. 
Temuan  lain  yang  bisa  dilihat  dari  model  yang  ada  adalah 
hubungan  antara  indikator  KUBE  dan  TPT.  Hal  ini  karena 
semakin  besarnya  KUBE  dan  TPT  selalu  mengindikasikan 
bahwa suatu provinsi tidak tergolong miskin. 

Selanjutnya  dilakukan  simulasi  dengan  cara  menerapkan 
model  yang  ada  dari  node  ke  node  untuk  melihat  bagaimana 
perubahan  klasifikasi  kemiskinan.  Klasifikasi  yang  dilakukan 
kemudian akan dibandingkan dengan target kelas untuk melihat 
akurasinya.  Adapun  hasilnya  untuk  tiap-tiap  model  adalah 
sebagai berikut: 
1)  Model 7 

Untuk  model  ini,  klasifikasi  diawali  dengan  HH  sebagai 
pembagi  awalnya.  Pada  tahap  ini  akurasi  yang  didapatkan 
adalah  73,53  persen.  Pada  tahap  ini,  kesalahan  yang  terjadi 
adalah  false  negative.  Kesalahan  ini  diduga  karena  nilai  HH 
yang  dijadikan  sebagai  pembagi  berada  di  bawah  nilai  HH 
nasional. Sehingga, yang terklasifikasi miskin adalah provinsi 
yang memang memiliki nilai HH jauh dibawah nilai nasional. 
Setelah  itu,  dilanjutkan  dengan  menjelajah  node  berikutnya 
yaitu KUBE. Hal ini kemudian mampu meningkatkan akurasi 
menjadi  79,41  persen.  Berbeda  dengan  sebelumnya,  kali  ini 
kesalahan yang terjadi adalah false positive. Pada node terakhir, 
akurasi mencapai 97,1 atau hanya terdapat satu kesalahan saja. 
Adapun jenis kesalahannya adalah false negative yang terjadi 
pada Provinsi Aceh. 

Gambar 3.  

HH [1] => KUBE [3], (d) HH [1] => KUBE [3] =>TPT [5]  

Simulasi Model 7. (a) target klasifikasi, (b) HH [1], (c) 

2)  Model 16 

Gambar 2.  

Tiga Model Unik Terbaik Decision Tree. (a) Model 7, 

(b) Model 16, dan (c) Model 18 

Dari  ketiga  model  seperti  yang  terlihat  pada  Gambar  2, 
dengan  melihat  root  node  pada  masing-masing  model  maka 
faktor  penentu  kemiskinan  utama  di  Indonesia  adalah  faktor 
kesehatan,  pekerjaan,  dan  akses  terhadap  pelayanan  dan 
infrastruktur dasar. Selain itu, terdapat temuan menarik, dimana 
pada  model  16  daerah  dengan  RLS  yang  tinggi  justru 
diklasifikasikan sebagai daerah yang miskin dan RLS rendah 
yang  diikuti  dengan  pengguna  telepon  seluler  yang  tinggi 
dikategorikan sebagai tidak miskin. Fakta ini membuat muncul 

Gambar 4.  

(c) Tani [1] => RLS [3], (d) Tani [1] => RLS [3] => HP [4] 

Simulasi Model 16. (a) target klasifikasi, (b) Tani [1], 

Model 

ini  diawali  dengan  Tani  sebagai  root  yang 
menghasilkan  akurasi  sebesar  76,47  persen.  Kesalahan  yang 
terjadi  pada 
ini 
mengindikasikan  bahwa  tidak  selamanya  daerah  dengan 
tingkat pekerja tani yang tinggi merupakan daerah yang miskin. 

false  positive.  Hal 

ini  adalah 

tahap 

 5 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Selanjutnya  klasifikasi  dilanjutkan  dengan  menjelajah  RLS, 
akurasi  yang  didapatkan  sebesar  61,76  persen.  Akurasi  yang 
didapatkan  menurun  jika  dibandingkan  dengan  sebelumnya. 
Pada  tahap  ini,  kesalahan  yang  terjadi  berupa  false  negative. 
Setelah  indikator  HP  dimasukkan,  akurasi  yang  didapatkan 
sebesar  97.1  persen,  dimana  satu-satunya  kesalahan  terjadi 
untuk  Provinsi  Maluku  Utara  dengan  kesalahan  bertipe  false 
positive. 
3)  Model 18 

meningkatnya  rata-rata  lama  sekolah  atau  pengguna  telepon 
seluler  di  suatu  daerah  akan  mengurangi  pekerja  pertanian 
sektor informal di daerah tersebut. 

Variabel  HH,  KUBE  dan  TPT  berada  dalam  satu  pohon 
untuk dua model yang diperoleh pada penelitian ini. Walaupun 
demikian terlihat bahwa ketiga variabel tersebut tidak memiliki 
ini  mengindikasikan  bahwa  keberadaan 
hubungan.  Hal 
beberapa variabel dalam satu tree tidak akan menjamin bahwa 
variabel-variabel tersebut akan saling memiliki hubungan satu 
sama lain. 

Gambar 5.  

Simulasi Model 18. (a) KUBE [1], (b) KUBE [1] => 

HH [2], (c) KUBE [1] => TPT [3], (d) KUBE [1] =>HH [2], KUBE [1] 
=> TPT [5] 

Model 18 diawali dengan KUBE sebagai pembagi sehingga 
mendapatkan  akurasi  sebesar  67.64  persen.  Kesalahan  yang 
terjadi pada tahap ini cenderung  false negative. Hal ini karena 
nilai KUBE yang dijadikan patokan pembagi berada di bawah 
nilai  rata-rata  KUBE  nasional  Berbeda  dengan  dua  simulasi 
sebelumnya, simulasi ini dilakukan dengan menjelajahi setiap 
node  satu  per  satu  sebelum  melihat  secara  keseluruhan. 
Klasifikasi kemudian dilanjutkan dengan menjelajahi node HH, 
yang mana menghasilkan akurasi sebesar 79.41 persen. Setelah 
itu,  yang dijelajah  adalah  node  TPT  dengan melupakan  node 
HH, hasilnya adalah akurasi sebesar 85.29 persen. Nilai akurasi 
yang lebih tinggi dibandingkan dengan menggunakan HH  ini 
semakin  memperkuat  dugaan  peneliti  bahwa 
terdapat 
hubungan  antara  KUBE  dengan  TPT.  Terakhir,  seluruh 
indikator digunakan dan dihasilkan akurasi sebesar 97.1 persen 
dengan  provinsi  Aceh  merupakan  satu-satunya  provinsi  yang 
terjadi kesalahan. 

Dari ketiga simulasi tersebut, didapatkan bahwa penggunaan 
model  dengan  variabel  yang  berbeda  akan  menghasilkan 
perbedaan pada hasil yang didapatkan.  

B.  Hubungan Antar Indikator Kemiskinan  

Dengan  adanya  dugaan-dugaan  mengenai  hubungan  antar 
indikator  yang  digunakan,  maka  dilakukanlah  uji  korelasi. 
Setelah dilakukan uji korelasi untuk keseluruhan variabel yang 
digunakan  pada  penelitian  ini,  ditemukan  bahwa  dari  dua 
dugaan yang telah dijabarkan sebelumnya, hanya RLS dan HP 
yang  memiliki  hubungan,  yakni  hubungan  korelasi  positif 
sebesar 0.67. Selain itu, ditemukan juga bahwa  variabel Tani 
dengan  RLS  dan  Tani  dengan  HP  memiliki  korelasi  negatif 
yang masing-masing sebesar -0.52 dan -0.68. Berdasarkan dua 
temuan  tersebut,  diketahui  bahwa  variabel  RLS  dan  HP 
memiliki  hubungan  linier  positif  atau  bisa  dibilang  kedua 
variabel  ini  akan  meningkat  secara  beriringan.  Sebaliknya, 
variabel  Tani  dengan  RLS  dan  Tani  dengan  HP  memiliki 
hubungan  linier  negatif  atau  bisa  diartikan  bahwa  semakin 

Gambar 6.  

Korelasi Antar Indikator Kemiskinan 

C.  Indikator Kemiskinan Terbaik 

Melihat tiga model terbaik yang telah diperoleh sebelumnya, 
bisa diperoleh enam indikator kemiskinan, yaitu Pekerja Tani, 
Rata-rata Lama Sekolah, Pengguna Telepon Seluler, Harapan 
Hidup, Kelompok Usaha Bersama dan Tingkat Pengangguran 
Terbuka.  

indikator 

Untuk  mendapatkan 

terbaik,  Abu  Bakar 
menggunakan  beberapa  metode  yaitu  MDG 
impurity, 
Information Gain, model linier dan korelasi Pearson [1]. Hasil 
dari  keempat  metode  tersebut  kemudian  diberi  ranking  dan 
dihitung  median  ranking-nya.  Pada  penelitian  ini,  indikator 
terbaik  dipilih  dihitung  menggunakan  Splits  dan  Usage 
Importance. Hasil dari kedua metode tersebut kemudian diberi 
ranking untuk melihat indikator terbaik untuk kemiskinan. 

Berdasarkan  split  importance  dan  usage  importance,  bisa 
disimpulkan indikator mana saja yang memiliki pengaruh yang 
sangat besar dalam pemodelan ini. Berbeda dengan model yang 
didapatkan  sebelumnya,  berdasarkan  splits  importance-nya 
lima  indikator  terbaik  adalah  Harapan  Hidup,  Persentase 
Sekolah  Dasar,  Pekerja  Tani  Sektor  Informal,  Tingkat 
Pengangguran  Terbuka,  dan  Pengguna  Telepon  Seluler.  Jika 
dilihat dari faktornya maka faktor yang mempengaruhi adalah 
faktor  kesehatan,  pendidikan,  pekerjaan  dan  akses  terhadap 
pelayanan dan infrastruktur dasar. 

lima 

Berdasarkan  usage 

indikator  yang 
importance, 
memiliki pengaruh paling besar pada keseluruhan pemodelan 
adalah Rata-rata Lama Sekolah, Pekerja Tani Sektor Informal, 
Air  Layak,  Pengguna  Telepon  Seluler,  dan  Harapan  Hidup. 
Faktor  yang  mempengaruhi  kemiskinan  berdasarkan  usage 
importance adalah faktor pendidikan, pekerjaan, akses terhadap 
pelayanan dan infrastruktur dasar dan kesehatan. 

 6 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
TABEL II.   Splits dan Usage Importance 

Variable 

HH 
SD 
Tani 
TPT 
HP 
KUBE 
KKP-E 
Air Layak 
RLS 
LKP 
PDRB 
Listrik 
Pengeluaran 
RAI 
IDG 
Sanitasi 
Wilayah 
KUR 
KUK 
SMP 
SMA 
SMK 

Split 
Importance 
19.5454 
15.4545 
14.5454 
11.8181 
9.5454 
8.1818 
6.3636 
4.0909 
4.0909 
2.2727 
1.8181 
0.9090 
0.4545 
0.4545 
0 
0 
0 
0 
0 
0 
0 
0 

Variable 

RLS 
Tani 
Air Layak 
HP 
HH 
TPT 
Pengeluaran 
PDRB 
SD 
LKP 
KKP-E 
KUBE 
Listrik 
RAI 
IDG 
Sanitasi 
Wilayah 
KUR 
KUK 
SMP 
SMA 
SMK 

Usage 
Importance 
100 
100 
100 
100 
100 
100 
100 
100 
100 
100 
100 
100 
50 
50 
0 
0 
0 
0 
0 
0 
0 
0 

Dengan  melakukan  pe-ranking-an  pada  splits  dan  usage 
importance  yang  diperoleh,  maka  tiga  indikator  dengan 
kontribusi terbesar pada penelitian ini adalah faktor kesehatan, 
pendidikan dan pekerjaan. 

Selain  itu,  terdapat  beberapa  indikator  yang  sebelumnya 
diduga  memiliki  pengaruh  tetapi  tidak  memiliki  kontribusi 
dalam  model  ini.  Adapun  indikator  tersebut  adalah  IDG, 
Sanitasi, Wilayah, KUR, KUK, SMP, SMA dan SMK, terlihat 
pada  Tabel  II  bahwa  nilai  splits  dan  usage  importance-nya 
adalah  nol.  Sehingga,  tidak  adanya  kontribusi  dari  variabel-
variabel  tersebut  mengindikasikan  bahwa  indikator  tersebut 
tidak mempengaruhi kemiskinan. 

D.  Diskusi dan Pembahasan 

Berdasarkan  model  7  dan  18,  provinsi  dengan  persentase 
KUBE  dan  TPT  yang  tinggi  mayoritas  terklasifikasi  sebagai 
provinsi  yang  tidak  miskin.  Ini  mengindikasikan  bahwa 
program  KUBE  akan  lebih  efektif  dijalankan  pada  provinsi 
dengan tingkat pengangguran yang tinggi. 

Sitepu menyatakan bahwa pelaksanaan KUBE di beberapa 
daerah di Indonesia masih kurang efektif. KUBE dinilai tidak 
cukup  efektif  karena  implementasi  kegiatan  yang  dilakukan 
dengan  cara  yang  kurang  sungguh-sungguh  [22].  Pada  tahun 
2017, dalam tulisannya yang lain, Sitepu menyatakan terdapat 
tiga faktor yang mempengaruhi kinerja KUBE yaitu, sistem dan 
manajemen  pelaksanaan  program,  kualitas  SDM  pelaksana 
program dan kualitas SM peserta program [23]. 

Berdasarkan  model  yang  ada,  provinsi  yang  memiliki 
persentase  KUBE  dibawah  5  persen  mayoritas  terklasifikasi 
sebagai tidak miskin. Batasan tersebut berada dibawah rata-rata 
nasional  yang  melebihi  7  persen.  Oleh  karena  itu,  peneliti 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

menganggap  bahwa  pemberian  KUBE  perlu  dibatasi  untuk 
setiap  wilayahnya  sehingga  manajemen  pelaksanaan  bisa 
dijalankan dengan lebih baik.  

Berdasarkan model 16, terdapat indikasi bahwa peningkatan 
Rata-rata Lama Sekolah pada daerah dengan pekerja tani sektor 
informal  yang  tinggi  cenderung  tidak  berpengaruh  untuk 
menekan  angka  kemiskinan.  Hal  ini  terjadi  karena  mayoritas 
petani  memiliki  tingkat  pendidikan  yang  rendah  [28]  dan 
penyerapan tenaga kerja banyak terjadi pada  sektor pertanian 
yang tidak membutuhkan pendidikan tinggi [29]. Berdasarkan 
data BPS, petani yang memiliki ijazah dengan tingkat tertinggi 
sampai  dengan  SMP  berjumlah  82,45  persen  [3].  Sehingga 
provinsi yang mayoritas penduduk bekerja di sektor pertanian 
informal  meskipun  memiliki  tingkat  pendidikan  yang  tinggi 
tetap akan terklasifikasi sebagai provinsi yang miskin. 

Berdasarkan  kontribusinya  pada  model,  terdapat  beberpa 
indikator yang sama sekali tidak berpengaruh pada model ini. 
Berdasarkan  kajian  yang  dilakukan,  terdapat  alasan-alasan 
tertentu  mengapa  suatu  indikator  tidak  memiliki  kontribusi 
pada  pemodelan  ini.  Adapun beberapa  alasan  tersebut  adalah 
sebagai berikut. 

Pertama, kenaikan pendapatan perempuan merupakan reaksi 
dari  adanya  aksi  berupa  program  pemberdayaan  perempuan 
dalam  sektor  ekonomi  yang  mampu  mengangkat  tingkat 
kesejahteraan keluarga dan menekan angka kemiskinan secara 
umum  [17].  Berdasarkan  hasil  tersebut,  bisa  disimpulkan 
bahwa  penurunan  angka  kemiskinan  lewat  faktor  gender 
merupakan  hasil  aksi-reaksi  antara  peningkatan  pendapatan 
perempuan dan pemberdayaan mereka. Hal ini membuat kedua 
hal ini saling beterkaitan.  

Untuk  faktor  geografis,  penelitian 

ini  menggunakan 
klasifikasi  wilayah  yang  dijabarkan  oleh  World  Bank  pada 
tahun  2006  [32].  Adanya  jarak  waktu  yang  cukup  jauh  itu 
membuat pengelompokkan yang  digunakan bisa saja menjadi 
kurang relevan. 

Menurut Hasan [7], Kredit Usaha Rakyat (KUR) memiliki 
efek negatif pada kemiskinan untuk jangka panjang. Selain itu 
penarikan  dana  untuk  kredit  bank  biasanya  digunakan  untuk 
modal  kerja  (93  persen)  dan  investasi  (33  persen)  dengan 
jangka waktu pinjaman 1 sampai dengan 3 tahun [21]. Hal ini 
membuat KUR yang kebanyakan digunakan untuk modal kerja 
dan  investasi  perlu  disalurkan  dulu  secara  efektif  sehingga 
dampak bagi peminjam kredit akan terasa dalam waktu lama. 

Pada  indikator  akses  pendidikan  menengah,  kurangnya 
kontribusinya  pada  pengklasifikasian  ini  diduga  karena  efek 
agregat yang digunakan hingga tingkat provinsi. Efek agregat 
hingga  ke  tingkat  provinsi  ini  diduga  bisa  menghilangkan 
karakteristik-karakteristik 
bisa  menggambarkan 
yang 
perbedaan  yang  terjadi  pada  tingkat  kabupaten  dan  kota. 
Mengingat  indikator  yang  digunakan  untuk  menggambarkan 
karakteristik provinsi adalah akses pada tingkat desa. Selain itu, 
berdasarkan standar sarana dan prasarana yang diaturan dalam 
Peraturan Menteri Pendidikan Nasional Nomor 24 Tahun 2007, 
keberadaan fasilitas SMP dan SMA adalah minimal satu untuk 
setiap  kecamatan  yang  ada.  Sehingga,  penggunaan  indikator 
akses pada tingkat desa diduga membuat indikator-indikator ini 
memiliki kontribusi yang minim pada model ini. 

 7 / 8 

 
 
 
 
Hal yang serupa diduga terjadi untuk indikator sanitasi layak. 
Pada tahun 2018, proporsi rumah tangga dengan sanitasi layak 
menurut  perdesaan  dan  perkotaan  adalah  55,47  persen  dan 
80,48  persen.  Perbedaan  tersebut  akan  sulit  ditangkap  ketika 
indikator  yang  digunakan  adalah  hasil  agregat  pada  tingkat 
provinsi.  Oleh  karena  itu,  walaupun  akses  terhadap  sanitasi 
layak  menunjukkan  adanya  pengaruh  terhadap  pengurangan 
kemiskinan,  penggunaan  indikator  yang  merupakan  hasil 
agregat pada tingkat provinsi menyebabkan kontribusinya pada 
model ini tidak ada sama sekali.  

VII. 

PENUTUP 

1.  Kesimpulan 

Berdasarkan  pemodelan  yang  dilakukan  ditemukan  tiga 
model  unik  terbaik.  Jika  melihat  dari  indikator  yang  menjadi 
root  node  yakni  Tani,  HH  dan  KUBE,  akar  permasalahan 
kemiskinan berdasarkan pemodelan ini adalah ketenagakerjaan, 
kesehatan  dan  akses  infrastruktur.  Sedikit  berbeda  dengan 
model  yang  diperoleh,  berdasarkan  Variable  Importance 
Measure,  faktor  penentu  kemiskinan  adalah  kesehatan, 
pendidikan dan pekerjaan yang digambarkan dengan indikator 
HH, SD dan tani. Selain itu, indikator-indikator yang tergabung 
dalam satu tree belum tentu akan memiliki hubungan yang kuat 
antara satu sama lain. 

2.  Saran 

Untuk  pemerintah,  penerimaan  usulan  Bansos  KUBE  ada 
baiknya  perlu  dikaji  ulang.  Karena  berdasarkan  temuan  yang 
ada,  semakin  banyaknya  persentase  penerima  KUBE  di  tiap 
desa  yang  ada  di  provinsi  tidak  berarti  provinsi  tersebut  bisa 
tergolong daerah yang tidak miskin. 

Untuk  penelitian  kedepannya,  agar  bisa  menentukan 
lebih  bisa  memberi  kontribusi 
indikator-indikator  yang 
terhadap pemodelan untuk klasifikasi kemiskinan. Karena pada 
penelitian ini masih cukup banyak indikator yang ternyata tidak 
terlalu berperan penting dalam pemodelan. 

Penelitian  ini  hanya  menggunakan  satu  metode  klasifikasi 
saja, sehingga tidak ada bahan pembandingnya. Oleh karena itu, 
perbandingan  dengan  metode  lain  masih  diperlukan  untuk 
mendapatkan metode terbaik. 

DAFTAR PUSTAKA 

[1]  Abu  Bakar,  A.,  Hamdan,  R.,  &  Sani,  N.  S.  (2020).  Ensemble 
Learning for Multidimensional Poverty Classification. Sains Malaysiana, 
49(2), 447–459. https://doi.org/10.17576/jsm-2020-4902-24 
[2]  Badan  Pusat  Statistik.  (2020).  Data  dan  Informasi  Kemiskinan 
Kabupaten/Kota. Jakarta: Badan Pusat Statistik. 
[3]  Badan Pusat Statistik. (2019). Hasil Survei Pertanian Antar Sensus 
(SUTAS) 2018 Seri-A2. Jakarta: Badan Pusat Statistik. 
[4] 
Firdausi, N. T. (2010). Proyeksi Tingkat Kemiskinan di Indonesia 
(Studi Kasus : 30 Provinsi (Skripsi). Universitas Diponegoro, Semarang, 
Indonesia. 
[5] 
Freund,  Y.,  &  Schapire,  R.  E.  (1997).  A  Decision-Theoretic 
Generalization  of  On-line  Learning  and  an  Application  to  Boosting. 
Journal of Computer and System Sciences, 55(1), 119-139. 
[6]  Garg, S. (2021). An evaluation of investor acceptability for physical 
gold using classification (decision tree). Materials Today: Proceedings, 37, 
950-954. doi:10.1016/j.matpr.2020.06.177 
[7]  Hasan,  Ibrahim.  (2016).  Analisa  Pertumbuhan  Penyaluran  KUR 
Pada UMKM Dalam Rangka Penurunan Angka Kemiskinan di Indonesia. 
Jurnal Ilmiah Mahasiswa FEB Universitas Brawijaya, 4(2). 
[8] 
Jansson, J. (2016). Decision Tree Classification of Products Using 
C5.0 and Prediction of Workload Using Time Series Analysis (Disertasi). 
Diambil dari http://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-200968. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Journal, 

Sosial.  Business  Management 

[9]  Khaliq,  A.,  dan  Uspri,  B.  (2017).  Kemiskinan  Multidimensi  dan 
Perlindungan 
13(2). 
https://doi.org/10.30813/bmj.v13i2.921 
[10]  Kocakoc, I. D. and Keser, I. (2019). Exploring Decision Rules for 
Election  Results  by  Classification  Trees.  11th  International  Conference 
“The Economic of the Balkan and the Eastern European Countries in the 
changing World”, May 2019. Bucharest. 
[11]  Kementerian  Pendidikan  Nasional.  (2007).  Peraturan  Menteri 
Pendidikan Nasional Republik Indonesia Nomor 24 Tahun 2007 Tentang 
Standar Sarana dan Prasarana untuk Sekolah Dasar/Madrassah Ibtidaiyah 
(SD/MI), Sekolah Menengah Pertama/Madrasah Tsanawiyah (SMP/MTs), 
dan Sekolah Menengah Atas/Madrasah Aliyah (SMA/MA). Jakarta. 
[12]  Kuhn, M. & Johnson, K. (2013) Applied Predictive Modelling. New 
York: Springer. 
[13]  Lantz, B. (2015). Machine learning with R discover how to build 
machine  learning  algorithms,  prepare  data,  and  dig  deep  into  data 
prediction techniques with R. Birmingham: Packt Publishing. 
[14]  Pal, J. C., Hall, M.A., Frank, E., & Witten I.H. (2016) “Data Mining: 
Practical Machine Learning Tools and Techniques”, Fourth Edition. New 
Zealand: Morgan Kaufmann. 
[15]  Putranto, R. A., Wuryandari, T., & Sudarno. (2015). Perbandingan 
Analisis  Klasifikasi  Antara  Decision  Tree  dan  Support  Vector  Machine 
Multiclass Untuk Penentuan Jurusan Pada Siswa SMA. Jurnal Gaussian, 
4(4), 1007-1016. 
[16]  Republik Indonesia. (2018). Peraturan Presiden Republik Indonesia 
Nomor 79 Tahun 2017 tentang Rencana Kerja Pemerintah (RKP) Tahun 
2018. Jakarta: Republik Indonesia. 
[17]  Rustinsyah,  R.  (2018).  Women  Empowerment  for  Poverty 
Reduction in Ring-1 Rural Area of a Cement Company in Tuban, East Java 
Province, Indonesia. Masyarakat, Kebudayaan Dan Politik, 31(1), 107-118. 
https://doi.org/10.20473/mkp.v31i12018.107-118  
[18]  Saleh,  Samsubar. 
(2002).  Faktor-Fakotr  Penentu  Tingkat 
Kemiskinan Regional di Indonesia. Jurnal Ekonomi Pembangunan, 7(2), 
87-102. 
[19]  Schapire, R. E. (2013). Explaining AdaBoost. Empirical Inference, 
37–52. https://doi.org/10.1007/978-3-642-41136-6_5  
[20]  Schapire,  R.  E.,  &  Freund,  Y.  (2012).  Boosting:  foundations  and 
algorithms. MIT Press. 
[21]  Sihaloho, H.D., Darmansyah. Soekro, Shinta R.I & Santoso, Wijoyo. 
2014.  Pemanfaatan  Sekuritisasi  Aset  Dalam  Mendorong  Sektor  Riil  : 
Alternatif Pembiayaan UMKM. Buletin Ekonomi Moneter dan Perbankan. 
Vol. 17 (2). 
[22]  Sitepu,  Anwar.  (2016).  Analisis  Efektivitas  Kelompok  Usaha 
Bersama  (KUBE)  Sebagai  Instrumen  Program  Penanggulangan  Fakir 
Miskin. Sosia Informa 2(1), 53-68. doi: 10.33007/inf.v2il.212  
[23]  Sitepu, Anwar. (2017). Kinerja Kelompok Usaha Bersama (KUBE) 
dalam Penanganganan Fakir Miskin di Wilayah Perkotaan. Sosia Informa 
16(2), 101-118. 
[24]  Siregar,  Syofian.  (2013).  Statistik  Parametrik  untuk  Penelitian 
Kualitatif. Jakarta: Bumi Aksara. 
[25]  Sudjana. (2005). Metode Statistika. Bandung: Tarsito. 
[26]  Sumargo,  B.,  &  Simanjuntak,  N.  M.  (2019).  Deprivasi  Utama 
Kemiskinan  Multidimensi  Antarprovinsi  di  Indonesia.  Jurnal  Ekonomi 
Dan 
160–172. 
Pembangunan 
https://doi.org/10.21002/jepi.v19i2.793 
[27]  Sutomo dan Sakinah F. Shalihati. (2015). Kajian Kemiskinan dan 
Perkembangan  Wilayah  Kabupaten  Purbalingga  dalam  Perspektif 
Geospasial. Jurnal Geoedukasi. 4(1), 7-22. 
[28]  Taopik, O. A., Dayat, Achdiyat, Billah, M. T., & Anwarudin. (2018). 
Profil  Petani  Muda  di  Kabupaten  Cianjur  Jawa  Barat.  Jurnal  Triton 
9(2),71-85. 
[29]  W.,  M.  A.,  &  Masjkur,  S.  U.  (2020).  Pengaruh  Rata-Rata  Lama 
Sekolah  dan  Pertumbuhan  Ekonomi  Serta  Penyerapan  Tenaga  Kerja 
Terhadap  Persentase  Penduduk  Miskin.  Jurnal  Ekonomi  Dan  Bisnis 
Airlangga, 28(1), 22. https://doi.org/10.20473/jeba.v28i12018.22-41  
[30]  World  Bank.  (2000).  World  Development  Report  2000-01: 
Attacking Poverty. Oxford: Oxford University Press for the World Bank. 
[31]  World Bank. (2006). Making the new Indonesia work for the poor 
Bank. 
(English). 
http://documents.worldbank.org/curated/en/880681468267341607/Makin
g-the-new-Indonesia-work-for-the-poor. 
[32]  World Bank. (2006). Making the new Indonesia work for the poor 
(Vol. 2) : Era baru  dalam pengentasan kemiskinan di Indonesia (Bahasa 
(Indonesian)). 
Bank. 
http://documents.worldbank.org/curated/en/501361468259732486/Era-
baru-dalam-pengentasan-kemiskinan-di-Indonesia. 
[33]  World  Bank.  (2016).  Measuring Rural  Access  Index:  Using  New 
Technologies. Washington, DC: World Bank.  
[34]  World Bank (2018). Poverty and Shared Prosperity 2018: Piecing 
Together the Poverty Puzzle. Washington. DC: World Bank. 
[35]  World  Bank.  (2020).  SDG  9.1.1.  Rural  Access  Index  Metadata. 
Washington, DC: World Bank.. 
[36]  Yobero,  Czar.  (2018).  Determining  Crediworthiness  for  Loan 
Applications Using C5.0 DecisionTrees. Retrieved December 8, 2019, dari 
https://rpubs.com/cyobero/C50. 

Washington, 

Washington, 

Indonesia, 

World 

World 

19(2), 

DC: 

DC: 

 8 / 8 

 
 
 
"
221709702,"Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik 

Perbandingan Penggunaan Kamus Normalisasi 
dalam Analisis Sentimen Berbahasa Indonesia 

Firnanda Zuhad (221709702, 4SD1) 
Dosen Pembimbing: Nori Wilantika, S.S.T., M.T.I. 

Ringkasan—  Normalisasi  merupakan  salah  satu  tahapan  text 
preprocessing  dalam  Natural  Language  Processing.  Pengaruh 
normalisasi  dengan  kamus  normalisasi  dalam  analisis  sentimen 
berbahasa  Indonesia  belum  diketahui.  Dengan membandingkan 
data  yang  tidak dinormalisasi dan data yang  dinormalisasi  dari 
beberapa  dataset,  kita  bisa  mengetahui  bagaimana  pengaruh 
normalisasi  dengan  kamus  Colloquial  Indonesian  Lexicon  dan 
modul formalizer pada INANLP dalam analisis sentimen. Untuk 
menganalisis  sentimen  dari  dataset,  metode  klasifikasi  yang 
digunakan  dalam  membuat  model  adalah  Multinomial  Naïve 
Bayes. Dengan metode tersebut, dihasilkan performa dari delapan 
dataset  yang  berbeda  berupa  akurasi,  presisi,  dan  recall.  Dari 
data  performa,  diuji  dengan  menggunakan  uji  wilcoxon  signed 
rank  test  untuk  menentukan  apakah  normalisasi  mampu 
meningkatkan  performa  dari  analisis  sentimen.  Dari  hasil  uji 
hipotesis  diperoleh  bahwa  hasil  uji  keseluruhan  dari  performa 
kamus  menunjukkan  nilai  p-value  kurang  dari  0,05.  Dengan 
begitu,  normalisasi  pada  analisis  sentimen  berbahasa  indonesia 
tidak  menyebabkan  kenaikan  performa  dari  indikator  akurasi, 
presisi, dan recall. 

Kata  Kunci—  Normalisasi,  Analisis  Sentimen,  Multinomial 

Naïve Bayes, Preprocessing, Wilcoxon Signed Ranks Test. 

I.  LATAR BELAKANG 

sehari-hari 

Penggunaan  media  sosial  dalam  berkomunikasi 

telah 
menjadi  kegiatan 
setiap  orang  di  masa 
perkembangan  teknologi  yang  begitu  mutakhir.  Pengguna 
membutuhkan sebuah gawai yang tersambung dengan internet 
agar bisa bersosialisasi dalam media sosial.  Pada tahun 2020, 
Jumlah pengguna gawai di Indonesia sebanyak 81,87 juta orang 
[1].  Dengan  berkomunikasi  melalui  media  sosial,  pengguna 
mampu mengekspresikan perasaan atau mengungkapkan opini 
terhadap  topik  tertentu.  Salah  satu  media  sosial  yang  sering 
dipakai dalam berkomunikasi adalah Twitter. Saat ini terdapat 
13,2 juta pengguna Twitter di Indonesia [2]. 

Data twit yang berisi opini yang diunggah di Twitter dapat 
dimanfaatkan  untuk  analisis  sentimen  seperti  yang  dilakukan 
oleh  [3],  [4],  dan  [5].  Dengan  analisis  sentimen,  kita  dapat 
mencari tahu bagaimana opini individu dari sekumpulan data 
twit  pengguna  Twitter.  Analisis  sentimen  menganalisis  opini 
pengguna media sosial terhadap suatu topik tertentu sehingga 
dapat diketahui dominasi antara positif dan negatif dalam twit 
pengguna [6]. 

Sebelum melakukan analisis sentimen, data teks yang telah 
diperoleh harus dilakukan  preprocessing  terlebih dahulu agar 
data  menjadi  lebih  terstruktur  dan  tertata  sesuai  format 
algoritma  yang  akan  digunakan  untuk  analisis.  Salah  satu 
tahapan  preprocessing  adalah  normalisasi,  yaitu  mengganti 
kata yang tak baku dengan konteks kata yang sesuai [7]. 

Kata tak baku biasa digunakan pada platform media sosial 
[8].  Pada  umumnya,  85%  twit  di  Twitter  mengandung  slang 
atau kata tak baku [9]. Contoh penggunaan kata tak baku dalam 

Bahasa Indonesia adalah gue(saya) dan gaa(tidak) [9]. Dalam 
bahasa Indonesia, penggunaan kata tak baku tersebut umunya 
disebut dengan bahasa “alay”. 

indonesia  yang  dilakukan  oleh 

Kata  tak  baku  dalam  media  sosial  kebanyakan  tidak 
ditemukan  dalam  kamus,  sehingga  kata  tersebut  tidak  bisa 
diturunkan secara morfologis dari kata yang ada di kamus [7]. 
Kata tak baku dalam media sosial sering kali tidak sesuai untuk 
digunakan  sebagai  data  dalam  tugas  Natural  Language 
Processing seperti Machine Translation, Information Retrieval, 
dan  Opinion  Mining  karena  ketidakteraturan  dari  corak 
bahasanya  [10].  Penelitian  oleh  [10],  menunjukkan  bahwa 
normalisasi  dapat  meningkatkan  performa  pada  pemrosesan 
data berbahasa inggris. Sedangkan pada penelitian normalisasi 
[11], 
dalam  bahasa 
menunjukkan bahwa  normalisasi tidak memberikan pengaruh 
terhadap  performa.  Namun  sayangnya 
yang  signifikan 
penelitian  yang  diambil  oleh  penelitian 
tersebut  hanya 
berdasarkan ujicoba pada satu dataset saja. Penelitian tersebut 
belum  menguji  coba  pengaruh  normalisasi  dalam  beberapa 
dataset sehingga penelitian ini bermaskud untuk menguji lebih 
lanjut  apakah  normalisasi  memberikan  pengaruh  yang 
signifikan terhadap performa analisis teks berbahasa indonesia. 
tak  baku  dapat  dinormalisasi  dengan 
menggunakan  kamus  Colloquial  atau  kamus  normalisasi. 
Kamus  yang  tersedia  untuk  normalisasi  teks  berbahasa 
Indonesia  antara  lain  Kamus  Colloquial  Indonesian  Lexicion 
yang  dibuat  oleh  [11]  dan  Kamus  Alay  dalam  penelitian 
InaNLP(Indonesian Natural Language Processing) yang dibuat 
oleh  [12].  Walaupun  demikian,  tidak  semua  kamus  memiliki 
efektivitas  dan  performa yang  sama.  Kamus  bahasa  tak  baku 
yang  telah  dibuat  oleh  penelitian  sebelumnya  memiliki 
kelebihan  dan  kekurangannya  masing-masing  sehingga  perlu 
dilakukan pengujian untuk melihat efektivitas kamus itu sendiri 
maupun  perbandingan  performa  antar  kamus  yang  tersedia. 
Untuk  melihat  performa  kedua  kamus 
tersebut,  kami 
mengujinya  pada    beberapa  dataset  kemudian  diproses  lebih 
lanjut  menggunakan  analisis  sentimen  sehingga  performa 
kedua kamus dapat dibandingkan dan dilihat mana yang lebih 
baik. 

Kata  yang 

II.  TUJUAN PENELITIAN 

Penelitian ini bertujuan untuk menguji pengaruh normalisasi 
pada  analisis  sentimen  dalam  bahasa  indonesia  dengan  cara 
membandingkan  performa  data  yang  tidak  dinormalisasi 
dengan  data  yang  dinormalisasi  menggunakan  kamus 
Colloquial Indonesian Lexicon dan Kamus pada INANLP. Dari 
perbandingan 
tersebut  akan  diambil  kesimpulan  apakah 
terdapat  pengaruh  yang  signifikan  dari  penggunaan  kedua 
kamus  tersebut  dalam  normalisasi  pada  analisis  sentimen. 
Selain  membandingkan  pengaruh  (efektivitas  kamus),  kedua 

 1 / 10 

 
 
 
 
kamus  dapat  dibandingkan  performanya  untuk  mencari  tahu 
mana yang terbaik antara dua kamus tersebut. 

III. PENELITIAN TERKAIT 

Pada  penelitian  sebelumnya  yang  dilakukan  oleh  [11], 
peneliti  membangun  sebuah  kamus  untuk  normalisasi  dalam 
Bahasa  Indonesia.  Peneliti  memberi  nama  kamus  tersebut 
dengan  nama  “Kamus  Alay”.  Kata  pada  Kamus  Alay 
dikumpulkan  dari  komentar  Instagram.  Penelitian  tersebut 
bertujuan menguji pengaruh normalisasi  dengan Kamus Alay 
dengan  cara  membandingkan  F1-score  dari  kombinasi 
algortima sekumpulan fitur teks dengan algoritma Xgboost dan 
SVM. Dari uji evaluasi kamus tersebut, peneliti menunjukkan 
bahwa  tidak  ada  perbedaan  yang  signifikan  antara  nilai  F1-
score  dalam  data  yang  dinormalisasi  maupun  yang  tidak. 
Kesimpulan  penelitian  menyebutkan  bahwa  diperlukan 
penelitian lebih lanjut untuk memanfaatkan kamus dalam kasus 
yang lain seperti analisis sentimen. 

Normalisasi  pada  Bahasa  Indonesia  bisa  juga  dilakukan 
toolkit  bernama 
dengan  menggunakan  kamus  pada 
InaNLP(Indonesian  Natural  Language  Processing)  yang 
dikembangkan  oleh  [12].  Penelitian  tersebut  bertujuan  untuk 
membangun  dan  menguji  sebuah  toolkit  yang  bisa  secara 
langsung digunakan dalam studi natural language processing. 
Toolkit  yang  dibuat  dalam  penelitian  tersebut  memiliki 
beberapa  modul  salah  satunya  adalah  word  normalization. 
Kami  akan  memakai  kamus  yang  digunakan  dalam  modul 
tersebut untuk sebagai kamus kedua normalisasi. 

tersebut, 

Penelitian  mengenai  penggunaan  algoritma  multinomial 
naïve  bayes  telah  dilakukan oleh  [13],  [14],  dan  [15].  Dalam 
penelitian 
algoritma 
multinomial  naïve  bayes  untuk  mengklasifikasikan  sentimen 
positif dan negatif pada analisis sentimen. Metode naïve bayes 
dengan model multinomial inilah yang akan digunakan dalam 
penelitian ini. 

peneliti  menggunakan 

Untuk  membandingkan  akurasi  antara  dua  tipe  data  yang 
dihasilkan  maka  kami  mengacu  kepada  penelitian  oleh  [16], 
tentang perbandingan antara dua classifier yang menyimpulkan 
apakah terdapat perbedaan yang signifikan antara dua classifier 
algoritma.  Hasilnya  menunjukkan  bahwa  Wilcoxon  Signed-
Ranks  Test  memberikan  nilai  p-value  yang  lebih  kecil 
dibandingkan  dengan  uji  lainnya  sehingga  uji  tersebut  akan 
cenderung  menolak  null  hypothesis  dibandingkan  dengan  uji 
yang  lain.  Walaupun  begitu,  uji  Wilcoxon  ini  hanya  bisa 
dipakai  saat  uji  asumsi  Paired  t-test  tidak  terpenuhi.  Pada 
penelitian ini, uji Wilcoxon Signed-Rank Test akan digunakan 
sebagai analisis pembanding antara hasil analisis sentimen dari 
data  yang dinormalisasi dengan yang tidak dinormalisasi dan 
performa antara dua kamus. 

IV. METODE PENELITIAN  

Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 1. Alur tahapan penelitian 

Dari  gambar  1,  tahapan  penelitian  dapat  dijelaskan  secara 

rinci sebagai berikut: 

A.  Pengumpulan Data Twit 

Penelitian ini Tahapan awal penelitian adalah pengumpulun 
data  dari  platform  media  sosial  yaitu  Twitter.  Data  yang 
dikumpulkan  berupa  twit  dari  pengguna  twitter  dengan 
menggunakan  package  Twint  dalam  bahasa  pemograman 
python. Data twit yang dikumpulkan adalah twit dengan kata 
kunci  vaksin  &  indonesia.  Dari  data  twit  tersebut,  diperoleh 
satu  dataset  dengan  pengambilan  data  pada  rentang  waktu  1 
minggu  pada  minggu  pertama  bulan  Januari  2021.  Karena 
dataset yang dibutuhkan lebih dari satu maka dataset yang lain 
menggunakan dataset dari penelitian sebelumnya. Dataset yang 
akan digunakan adalah data yang telah diberikan label positif 
ataupun negatif yang bersumber dari skripsi dan github. 

B.  Text preprocessing 

Tahapan kedua pada penelitian ini adalah text preprocessing. 
Data twit akan di-preprocessing dengan beberapa langkah yaitu 
lowercasing,  cleaning,  menghapus  whitespace,  menghapus 
duplikat,  tokenisasi,  normalisasi,  menghapus  stopword,  dan 
stemming. Keseluruhan proses tersebut berfungsi mengurangi 
noise pada data dan menghasilkan data yang siap pakai untuk 
proses  selanjutnya.  Dengan  berkurangnya  noise  maka 
diharapkan performa dari analisis akan mengalami kenaikan. 

C.  Feature Extraction 

Data yang sudah di-preprocessing akan diproses lebih lanjut 
dengan feature extraction agar data yang berbentuk teks(string) 
diubah menjadi vektor. Data teks perlu diubah menjadi vektor 
agar bisa dianalisis dengan kalkulasi multinomial naïve bayes 
karena  data  teks  tidak  dapat  secara  langsung  diproses.  Pada 
proses  ini  menggunakan  package  sklearn  dengan  fungsi 
TfidfVectorizer. 

D.  Klasifikasi 

Data  yang  sudah  diubah  dari  teks  menjadi  vektor  akan 
dijadikan  sebagai  input  dalam  algoritma  naïve  bayes.  Model 
seleksi  data  menggunakan  metode  kfold  dengan  nilai  k=5. 
Dengan metode ini, setiap data twit yang telah siap dianalisis 
maka data yang telah diberi label dibagi menjadi 2 bagian yaitu 
data  training  dan  testing.  Data  tranning yang  berisi  pasangan 
twit  dan 
label  akan  digunakan  sebagai  acuan  sumber 
pembentukan model. Probabilitas kemunculan kelas positif dan 
negatif dihitung dari setiap fitur yang mempresentasikan twit. 

 2 / 10 

 
 
 
Ketika  mengklasifikasikan  twit  yang  baru,  maka  tinggal 
mengalikan nilai probabilitas setiap fitur untuk masing-masing 
kelas.  Nilai  probabilitas  terbesar  diantara  kelas  positif  dan 
negatif akan dijadikan dasar penentu kelas twit baru tersebut. 

E. Evaluasi dan Hasil 

Tahapan  selanjutnya  adalah  evaluasi  hasil  performa 
algoritma  klasifikasi  yang  digunakan.  Confusion  matrix 
digunakan untuk mengevaluasi performa dari algoritma naïve 
bayes.  Dari  Confusion  matrix  tersebut,  akan  didapatkan  tiga 
indikator performa yaitu akurasi, presisi, dan recall dari masing 
masing  dataset  dan  kategori  dataset.  Setelah  didapatkan 
indikator  performa  tersebut,  kita  dapat  mengetahui  seberapa 
baik  model  yang  digunakan  dalam  penelitian  ini.  Walaupun 
begitu, hal tersebut bukanlah fokus utama dalam penelitian ini 
melainkan perbandingan performa kamus. 

F. Perbandingan Performa 

Tahapan  terakhir  adalah  membandingkan  performa  yang 
dihasilkan  dari  masing-masing  kategori  dataset.  Setelah 
analisis  sentimen  mengeluarkan  hasil  performa  berupa  nilai 
akurasi,  presisi,  dan  recall  dari  setiap  dataset  dengan  kamus 
maupun  tanpa  kamus,  maka  untuk  analisis  seberapa  besar 
efektivitas  kamus  menggunakan  uji  Wilcoxon  Signed-Ranks. 
Untuk  menguji  efektivitas  kamus,  Hipotesis  nol  untuk  uji ini 
adalah bahwa performa analisis data yang tidak dinormalisasi 
sama  dengan  data  yang  dinormalisasi.  Sedangkan  Hipotesis 
alternatifnya  adalah  bahwa  performa  analisis  data  yang  tidak 
dinormalisasi  tidak  sama  dengan  data  yang  dinormalisasi. 
Untuk membandingkan performa kedua kamus, Hipotesis nol 
untuk  uji  ini  adalah  bahwa  performa  analisis  data  yang 
dinormalisasi  dengan  kamus  Colloquial  Indonesian  Lexicon 
InaNLP.  Sedangkan  Hipotesis 
sama  dengan  kamus 
alternatifnya  adalah  bahwa  performa  analisis  data  yang 
dinormalisasi  dengan  kamus  Colloquial  Indonesian  Lexicon 
tidak sama dengan kamus InaNLP. 

V.  KERANGKA PIKIR 

ini  mengacu  pada 

Kerangka  Pikir  penelitian 

teori 
normalisasi  serta  pengaruh  normalisasi  tersebut  terhadap 
performa algoritma klasifikasi teks. Kerangka pikir ini diadopsi 
dari  buku  dalam  [12]  yang  menjelaskan  bahwa  analisis 
sentimen dipengaruhi oleh metode klasifikasi dan metode text 
preprocessing. 

Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 2. Kerangka pikir penelitian 

Dari  Gambar  2,  Metode  klasifikasi  yang  digunakan  pada 
analisis sentimen adalah metode naïve bayes  classifier model 
multivariate  [17].  Dari  metode  text  preprocessing  yang 
digunakan telah disebutkan sebelumnya, penelitian ini berfokus 
pada normalisasi [7] yang mana normalisasi ditetapkan sebagai 
variabel  kontrol  pada  dataset.  Kamus  normalisasi  yang 
digunakan berasal dari penelitian [11] dan [12]. Dataset pada 
penelitian  dibagi  menjadi  tiga  tipe  seperti  pada  gambar  3 
kemudian  dibandingkan  performanya  menggunakan  metode 
wilcoxon signed-rank test [16]. 

VI. HIPOTESIS PENELITIAN 

Hipotesis  dalam  uji  wilcoxon  signed-rank  dari  penelitian 

adalah sebagai berikut: 
A.  Data  Yang  Tak  Dinormalisasi  Dengan  Data  Yang 
Dinormalisasi Dengan Kamus Normalisasi Satu 

Indikator Akurasi 
H0: M0 = M1 
H1: M0 ≠ M1 
M0 = nilai median akurasi dari data yang tidak dinormalisasi 
M1=  nilai  median  akurasi  dari  data  yang  dinormalisasi 

dengan kamus satu 

Indikator Presisi 
H0: M0 = M1 
H1: M0 ≠ M1 
M0 = nilai median presisi dari data yang tidak dinormalisasi 
M1= nilai median presis dari data yang dinormalisasi dengan 

kamus satu 

Indikator Recall 
H0: M0 = M1 
H1: M0 ≠ M1 

 3 / 10 

 
 
 
 
 
 
M0 = nilai median recall dari data yang tidak dinormalisasi 
M1= nilai median recall dari data yang dinormalisasi dengan 

Pengumpulan  data  dibagi  menjadi  dua  bahasan  yaitu 

sebagai berikut: 

Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik 

kamus satu 

B.  Data  Yang  Tak  Dinormalisasi  Dengan  Data  Yang 
Dinormalisasi Dengan Kamus Normalisasi Dua 

Indikator Akurasi 
H0: M0 = M2 
H1: M0 ≠ M2 
M0 = nilai median akurasi dari data yang tidak dinormalisasi 
M2=  nilai  median  akurasi  dari  data  yang  dinormalisasi 

dengan kamus dua 

Indikator Presisi 
H0: M0 = M2 
H1: M0 ≠ M2 
M0 = nilai median presisi dari data yang tidak dinormalisasi 
M2=  nilai  median  presisi  dari  data  yang  dinormalisasi 

dengan kamus dua 

Indikator Recall 
H0: M0 = M2 
H1: M0 ≠ M2 
M0 = nilai median recall dari data yang tidak dinormalisasi 
M2= nilai median recall dari data yang dinormalisasi dengan 

kamus dua 

C. Data Yang Dinormalisasi Dengan Kamus Normalisasi Satu 
Dengan Data Yang Dinormalisasi Dengan Kamus Normalisasi 

Indikator Akurasi 
H0: M1 = M2 
H1: M1 ≠ M2 
M1  =  nilai  median  akurasi  dari  data  yang  dinormalisasi 

dengan kamus satu 

M2=  nilai  median  akurasi  dari  data  yang  dinormalisasi 

dengan kamus dua 

Indikator Presisi 
H0: M1 = M2 
H1: M1 ≠ M2 
M1  =  nilai  median  presisi  dari  data  yang  dinormalisasi 

dengan kamus satu 

M2=  nilai  median  presisi  dari  data  yang  dinormalisasi 

dengan kamus dua 

Indikator Recall 
H0: M1 = M2 
H1: M1 ≠ M2 
M1 = nilai median recall dari data yang dinormalisasi dengan 

kamus satu 

M2= nilai median recall dari data yang dinormalisasi dengan 

kamus dua 

1. Dataset dalam Penelitian 

Pada  tahap paling  awal penelitian  yaitu  pengumpulan  data 
telah  berhasil  mengumpulkan  delapan  dataset  pada  tabel  5 
dengan rincian jumlah data sebelum di-preprocessing sebagai 
berikut: 

TABEL I  
DESKRIPSI DATASET 

Keterangan Dataset 
Data  twit  dengan  kata  kunci 
vaksin  dan  indonesia  dengan 
rentang  waktu  1  Januari  2021 
sampai dengan 8 Januari 2021 
Data  twit  dengan  kata  kunci 
menggunakan 
#RUUPKS 
dengan rentang waktu 1 Januari 
2018  sampai  degan  31  Maret 
2019 
Data  twit  dengan  kata  kunci 
“Ujian Nasional” 

twit 

Data  twit  dengan  kata  kunci 
“harga cabe” dan “harga cabai” 
dengan rentang waktu 1 Januari 
2018  sampai  31  Desember 
2018  
Data 
tentang  ujaran 
kebencian  dari  beberapa  kata 
kunci dengan rentang waktu 20 
Maret 
10 
September 2018 
twit 
Data 
covid-19 di Indonesia 
Data 
tentang  sentimen 
debat  kedua  pilkada  DKI 
Jakarta 
Data  twit  September  sampai 
Desember 2018 

tentang  sentimen 

sampai 

2018 

twit 

No.  Sumber 
1. 

Pengumpulan 
Sendiri 

Nama Penulis 
- 

2. 

Skripsi 

Silvia  Ni’matul 
Maula [19] 

3. 

Skripsi 

4. 

Skripsi 

Bambang  Dwi 
Putra  Nugraha 
[20] 
Muhammad 
Firdaus [21] 

5. 

Github 

Dhika  Rangga 
[22] 

6. 

7. 

Github 

Github 

8. 

Github 

Yahdi Indrawan 
[23] 
Rio 
[24] 

Chandra 

Ridi  Ferdiana, 
Fahim  Jatmiko, 
Desi 
Dwi 
Purwanti, 
Artmita  Sekar 
Ayu, 
Tri 
Fajar 
Wiliam 
Dicka [25] 

Dari Tabel 1, dataset yang dikumpulkan terdiri dari satu data 
yang dikumpulkan sendiri, tiga dataset dari skripsi mahasiswa 
Politeknik  Statistika  STIS,  dan  empat  data  dari  internet yang 
bersumber dari github. 

TABEL II 
DESKRIPSI JUMLAH DATA DALAM DATASET 

No.  Nama Penulis 
1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 

- 
Silvia Ni’matul Maula [19] 
Bambang Dwi Putra Nugraha [20] 
Muhammad Firdaus [21] 
Dhika Rangga [22] 
Yahdi Indrawan [23] 
Rio Chandra [24] 
Ridi Ferdiana, dkk [25] 

Jumlah Data 
3665 
560 
1491 
2290 
14210 
636 
1506 
5164 

VII. 

HASIL DAN PEMBAHASAN 

A.  Pengumpulan Data 

Dari  Tabel 2,  dataset 1 dikumpulkan dengan cara  scraping 
dari twit dengan package twint. Dari package ini dapat dibuat 

 4 / 10 

 
 
 
 
 
 
 
 
 
 
fungsi untuk mengatur kata kunci, bahasa, rentang waktu, jenis 
file output, dan kostumisasi variabel dari target twit yang akan 
indonesia 
di-scraping.  Dengan  kata  kunci  vaksin  dan 
menunjukkan bahwa setiap twit yang mengandung kata vaksin 
serta  indonesia  yang  diunggah  dari  tanggal  1  Januari  2021 
sampai 8 Januari 2021 dalam bahasa Indonesia akan dicetak ke 
dalam data csv dengan variabel output berupa nomor id, tanggal 
dan  waktu,  nama  pengguna,  isi  twit,  dan  hashtag  yang 
digunakan.  Walapun  demikian  yang  digunakan  dalam  proses 
selanjutnya hanya variabel twit. 

Data  yang  berhasil  dikumpulkan  pada  proses  scraping 
berjumlah 3665   data. Contoh data yang di-scraping ada pada 
tabel 3 sebagai berikut: 

TABEL III 
CONTOH DATA TWIT DALAM DATASET YANG DI-SCRAPING 

jarak 

Twit 
KEREN,  pak  @erickthohir  benar  memastikan 
kesiapan  vaksin  covid  utk  rakyat  Indonesia,  kita  jg 
harus  bersatu  dukung  kerja  pemerintah  dlm 
menyelesaikan pandemi ini, Masker, Mencuci tangan, 
#pkpindonesia 
menjaga 
#partaizamanWOW 
#indonesiabisalawancorona  
#coronagoaway 
Yth.  Pak  @jokowi  jika  @MajelisUlamaID  lamban 
&amp;  ogah2an  terbitkn  Sertifikasi„  Vaksin  Corona 
yg  sdh  ada,  mohon  dahulukan  saja  Sdra2  kami  Non 
Muslim yg juga sgt berhak sgra diobati. Kami ikhlas. 
@MajelisUlamaID 
suka  muslim 
agaknya 
Indonesia punah sblm waktunya. 

#pkpi 

lbh 

Hashtags 
['pkpi', 
'pkpindonesia', 
'partaizamanwo
w', 
'indonesiabisala
wancorona', 
'coronagoaway'] 
[] 

Pada Tabel 3, pada kolom hashtags “[]” menunjukkan bahwa 

twit tidak memiliki hashtags sama sekali. 

2. Kamus dalam Penelitian 

Rincian  tentang  jumlah  kata  yang  terdapat  dalam  kamus 

adalah sebagai berikut: 

TABEL IV 
DESKRIPSI TENTANG KAMUS 

Sumber 
Pengambila
n 

https://githu
b.com/nasal
sabila/kamu
s-alay 

Disebut 
sebagai 

Kamus 
normalisasi 
Satu 

https://githu
b.com/pang
gi/pujangga 

Kamus 
normalisasi 
dua 

Nama 
Kamus 

Nama 
Penulis 

Judul 
Penelitian 

Colloquial 
Indonesian 
Lexicon 

Kamus 
Alay 

Salsabila, 
Winatmo
ko, 
Septiandr
i, 
dan 
Jamal 
[11] 

Purwaria
nti, 
Andhika, 
Wicakso
no,  Afif, 
dan 
Ferdian 
[12] 

Colloquial 
Indonesian 
Lexicon 

InaNLP: 
Indonesia 
Natural 
Language 
Processing 
Toolkit 
Case 
Complaint 
Tweet 
Classification 

study: 

Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik 

Dari Tabel 4, kedua kamus diperoleh dari internet yaitu dari 
github dalam bentuk format csv. Kamus Colloquial Indonesian 
Lexicon  akan  disebut  sebagai  kamus  normalisasi  satu. 
Sedangkan  kamus  INANLP  akan  disebut  sebagai  kamus 
normalisasi dua. 

TABEL V 
DESKRIPSI JUMLAH KATA DALAM KAMUS 

No. 
1. 
2. 

Nama Kamus 
Colloquial Indonesian Lexicon 
Kamus Alay 

Jumlah Kata 
15006 
1147 

Dari Tabel 5, jumlah kata yang terdapat pada kamus 1, pada 
kamus yang diunduh dari github memiliki duplikat kata. Oleh 
karena itu duplikat kata yang ada di kamus dihapus sehingga 
jumlah  kata  yang  unique  dalam  kamus  sejumlah  13380  kata. 
Sedangkan  pada  kamus  2,  semua  kata  dalam  kamus  telah 
unique. Dari sisi jumlah kata, kamus 1 lebih banyak daripada 
kamus 2 menunjukkan bahwa semakin banyak kata yang akan 
dinormalisasi dengan kamus 1. 

B. Text preprocessing 

Alur tahapan preprocessing adalah sebagai berikut: 

Gambar 3. Flowchart tahapan pada proses preprocessing 

Dari Gambar 3, alur preprocessing dijelaskan secara detail 

sebagai berikut: 
1. Lowercasing 

Lowercasing dilakukan dengan menggunakan fungsi lower() 
dari  tipe  dataframe  dimana  setiap  data  bertipe  string  pada 
dataset akan diubah menjadi huruf kecil. 

2. Cleaning 

Pada  proses  cleaning,  ada  beberapa  elemen  yang  dihapus 
yaitu emoji, emoticon, mentions, hashtag, retweet, angka, dan 
hyperlink.  Emoji  adalah  karakter  khusus  untuk menghasilkan 
Mentions adalah nama pengguna yang ditandai dengan karakter 
“@” pada awal kata dan diikuti dengan huruf, angka maupun 
karakter  khusus  seperti  “_”.  Hashtag  adalah  penanda  yang 
diawali  dengan  karakter  “#”  yang  biasanya  berkaitan  dengan 
topik  twit.  Retweet  adalah  elemen  dalam  twit  yang  ditandai 
dengan kata rt yang maksudnya adalah memposting ulang twit 
dari akun pengguna lain. Angka pada data teks adalah elemen 
penganggu  pada  proses  text  processing.  Hyperlink  adalah 

 5 / 10 

 
 
 
 
 
alamat sebuah web yang ditandai dengan elemen “http:/” pada 
awal kata. Package yang digunakan dalam proses cleaning teks 
adalah  package  regular  expression.  Dengan  memanfaatkan 
fungsi sub(), maka elemen string yang akan dihapus pada data 
dapat  disubstitusi  dengan  elemen  lain  yang  biasanya  adalah 
spasi(“ “). 

3. Menghapus Punctuation 

Pada proses ini, semua bentuk tanda baca seperti “.”, “,”, “!”, 
dan sebaganya akan dihapus dari data. Karena elemen ini tidak 
dibutuhkan  pada  analisis.  Dengan  menggunakan  fungsi 
replace() untuk mensubtitusi elemen pada data string. Apabila 
terdapat elemen tanda baca maka akan diganti dengan spasi(“ “). 

4. Menghapus Whitespace 

Pada proses menghapus whitespace, whitespace(spasi) yang 
dihasilkan  pada  tahapan  cleaning  sebelumnya  akan  dihapus 
karena hanya akan menambah elemen yang tak diperlukan pada 
data. Proses ini dilakukan dengan menggunakan fungsi strip() 
dalam package string. 

5. Menghapus Duplikat 

Data  yang  telah  dieksekusi  pada  tahap-tahap  sebelumnya 
akan dicek apakah terdapat data twit yang terduplikasi. Jika ada 
data twit yang terduplikasi maka seluruh data yang terduplikasi 
akan  dihapus  dengan  menyisakan  satu  data  saja.  Proses  ini 
menggunakan fungsi drop_duplicates(). 

6. Tokenisasi 

Pada  tahap  ini,  setiap  kata  dalam  kalimat  pada  setiap  data 
twit  akan  diubah  menjadi  token-token(kata).  Indikator  untuk 
memisahkan kata dalam kalimat adalah whitespace(“ “) dalam 
kalimat.  Proses  ini  menggunakan  fungsi  word_tokenize() 
dalam  package  NLTK.  Setiap  baris  data  pada  data  twit  akan 
dieksekusi dengan fungsi word_tokenize(). 

7. Normalisasi 

Tahap yang paling penting dalam penelitian ini adalah tahap 
normalisasi dengan kamus yaitu kamus Colloquial Indonesian 
Lexicon dan kamus pada INANLP yang didapatkan dari github. 
Proses  ini  menginput  kedua  kamus  tersebut  kedalam  coding 
python  sebagai  data  dengan  tipe  dictionary.  Konsep  proses 
normalisasi dengan data tipe dictionary ini adalah apabila pada 
setiap  token  data  twit  terdapat  kata  pada  kamus  maka  kata 
tersebut  akan  diubah  menjadi  kata  pada  bentuk  bakunya. 
Dataset akan dibagi menjadi tiga versi pada tahap ini yaitu data 
yang  tidak  dinormalisasi,  data  yang  dinormalisasi  dengan 
kamus  Colloquial  Indonesian  Lexicon,  dan  data  yang 
dinormalisasi dengan kamus INANLP. 

8. Menghapus Stopword 

Proses 

ini  adalah  salah  satu  proses  penting  dalam 
preprocessing.  Stopword  adalah  kata  yang  tidak  dibutuhkan 
dalam analisis dan kemunculannya hanya akan memperburuk 
performa  sehingga  perlu  dihapus.  Daftar  stopword  yang 
dihapus dalam penelitian ini adalah daftar stopword yang ada 

Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik 

di  package  Sastrawi.  Proses 
StopWordRemoverFactory() dari package Sastrawi. 

ini  menggunakan 

fungsi 

9. Stemming 

Proses  stemming  adalah  proses  mengubah  kata  pada  data 
twit  yang  memiliki  imbuhan  menjadi  kata  dasar.  Proses  ini 
menggunakan fungsi StemmerFactory() pada package Sastrawi.  

Dari  keseluruhan  proses  preprocessing  dapat  diringkas  pada 
tabel 6. 

TABEL VI  
PERBANDINGAN SEBELUM DAN SESUDAH DATA DI-
PREPROCESSING 

Sesudah Preprocessing 
keren pak benar pasti kesiap vaksin 
covid rakyat indonesia satu dukung 
kerja  perintah  selesai  pandemi 
masker cuci tangan jaga jarak 

Sebelum Preprocessing 
KEREN,  pak  @erickthohir  benar 
memastikan  kesiapan  vaksin  covid 
utk  rakyat  Indonesia,  kita  jg  harus 
bersatu  dukung  kerja  pemerintah 
dlm  menyelesaikan  pandemi  ini, 
Masker,  Mencuci  tangan,  menjaga 
#pkpindonesia 
jarak 
#partaizamanWOW 
#indonesiabisalawancorona  
#coronagoaway 

#pkpi 

B. Pembangunan Model Klasifikasi Sentimen 

1. Feature Extraction 

Pada tahap ini, fitur dari data yang telah di-preprocessing 
sebelumnya  akan  diekstrak  sehingga  data  yang  berbentuk 
string tersebut akan dikonversi menjadi sebuah vektor dengan 
metode  TFIDF.  Dalam  penelitian  ini  model  bahasa  n-gram 
yang dipakai adalah model unigram. Proses ini menggunakan 
fungsi TfidfVectorizer. 

2. Model Multinomial Naïve Bayes 

Proses  ini  membuat  sebuah  model  klasifikasi  dengan 
multinomial  naïve  bayes  dengan  menggunakan  data  twit 
trainning dan label trainning. Dengan menggunakan fungsi fit 
dalam package MultinomialNB dalam sklearn. 

C. Performa Kamus Normalisasi Satu 

1.  Persentase  Kata  Tak  Baku  Dalam  Dataset  berdasarkan 
kamus normalisasi satu 

TABEL VII 

PERSENTASE KATA TAK BAKU DALAM DATASET MENURUT 
KAMUS NORMALISASI SATU 
Jumlah Kata 
Total 

Jumlah Kata Tak 
Baku 

Dataset 

Persentase 

No. 

Dataset 1 
Dataset 2 
Dataset 3 
Dataset 4 
Dataset 5 
Dataset 6 
Dataset 7 
Dataset 8 

1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
Rata-rata 

44331 
10076 
16584 
32869 
146286 
10059 
15046 
57281 
41566,5 

3018 
614 
1320 
3882 
380 
860 
1985 
6886 
2368,125 

6,80% 
6,09% 
7,95% 
11,81% 
0,25% 
8,54% 
13,19% 
12,02% 
8,33% 

 6 / 10 

 
 
 
 
 
 
Dari Tabel 7, dari kedelapan dataset memiliki persentase 
kata  tak  baku  terendah  sebesar  0,25%.  Sedangkan  persentase 
kata  tak baku  tertinggi sebesar 13,19%. Nilai persentase kata 
tak baku memiliki rata-rata sebesar 8,33%. Pada dataset 5 yang 
memiliki 146286 kata, hanya 380 kata yang termasuk kata tak 
baku.  Walaupun  jumlah  kata  dalam  dataset  sangat  banyak, 
ternyata  memiliki jumlah kata tak baku yang sedikit menurut 
kamus normalisasi satu. Kecilnya persentase kata tak baku pada 
dataset dapat disebabkan oleh beberapa faktor yaitu jumlah kata 
baku dalam dataset memang sangat banyak dibandingkan kata 
tak baku, jumlah kata tak baku dari dataset sebenarnya banyak 
tapi  tidak  terdapat  pada  kamus  sehingga  suatu  kata  akan 
dikenali  kamus  sebagai  kata  baku,  ataupun  penggunaan  kata 
dalam bahasa lain sehingga kata tidak dikenali oleh kamus. 

2.  Frekuensi  Kata  Tak  Baku  Terbanyak  Berdasarkan  Kamus 
Normalisasi Satu 

Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik 

Dataset 5 
Dataset 6 
Dataset 7 
Dataset 8 

5. 
6. 
7. 
8. 
Rata-rata 

146286 
10059 
15046 
57281 
41566,5 

5973 
845 
1417 
6375 
2801,75 

4,08% 
8,40% 
9,41% 
11,12% 
7,71% 

Dari  Tabel  8,  dari  delapan  dataset  memiliki  persentase 
kata  tak  baku  terendah  sebesar  4,08%.  Sedangkan  persentase 
kata  tak baku  tertinggi sebesar 11,12%.  Nilai persentase kata 
tak  baku  memiliki  rata-rata  sebesar  7,71%.  Jumlah  kata  tak 
baku dalam dataset berada pada rentang 649 sampai 6375 kata. 
Pada tabel, terlihat bahwa dataset 5 tetap memiliki persentase 
kata  tak  baku  terkecil  menurut  kamus  normalisasi  dua.  Bila 
dibandingkan  dengan  kamus  normalisasi  satu,  normalisasi 
dengan kamus satu menghasilkan rata-rata persentase kata tak 
baku  lebih  tinggi  sebesar  0,62%  daripada  normalisasi  kamus 
dua. 

2.  Frekuensi  Kata  Tak  Baku  Terbanyak  Berdasarkan  Kamus 
Normalisasi Dua 

Gambar 4. Grafik 10 kata tak baku dengan frekuensi tertinggi menurut 

kamus normalisasi satu 

Dari gambar 4, terlihat bahwa sepuluh kata tak baku dengan 
frekuensi  terbanyak  adalah  kata  “yg”(yang)  yang  berjumlah 
2086 kata pada keseluruhan dataset. Kata “yg” memiliki nilai 
frekuensi yang cukup berbeda secara signifikan dibandingkan 
dengan  frekuensi  kata  “co”,  ”aja”,  dan  seterusnya.  Dari 
kesepuluh  kata  tak  baku,  tidak  terdapat  kata  yang  memiliki 
makna  sentimen(ungkapan  emosi).  Kesepuluh  kata  tersebut 
merupakan  kata  stopword(kata  yang  memiliki  kemunculan 
tinggi).  Dengan  demikian,  penggunaan  kata  tak  baku  sering 
digunakan  pada  kata  yang  termasuk  stopword  sehingga 
kemungkinan  besar  normalisasi  memiliki  pengaruh  terhadap 
tahapan stopword removal. 

D. Performa Kamus Normalisasi Dua 
1.  Persentase  Kata  Tak  Baku  Dalam  Dataset  berdasarkan 
kamus normalisasi dua 

TABEL VIII 

PERSENTASE KATA TAK BAKU DALAM DATASET MENURUT 
KAMUS NORMALISASI DUA 

No. 

Dataset 

Jumlah Kata 
Total 

Jumlah Kata Tak 
Baku Total 

Persentase 

1. 
2. 
3. 
4. 

Dataset 1 
Dataset 2 
Dataset 3 
Dataset 4 

44331 
10076 
16584 
32869 

2946 
649 
959 
3250 

6,64% 
6,44% 
5,78% 
9,88% 

Gambar 5. Grafik 10 kata tak baku dengan frekuensi tertinggi menurut 

kamus normalisasi dua 

Dari  gambar  5,  terlihat  bahwa  kata  dengan  frekuensi 
terbanyak  dari  kamus  normalisasi  satu  dan  dua  adalah  kata 
“yg”(yang). Dibandingkan dengan frekuensi kata tak baku pada 
kamus normalisasi satu, terlihat bahwa terdapat beberapa kata 
yang  tak  ada  pada kamus normalisasi  satu  seperti  kata  “gue” 
dan ”tau”. Dari kesepuluh kata tersebut, terdapat dua kata yang 
memiliki makna sentimen(ungkapan emosi) yaitu “wkwk” dan 
“haha” yang berarti tertawa(senang). Delapan dari sepuluh kata 
pada  gambar  merupakan  kata  stopword(kata  yang  memiliki 
kemunculan  tinggi).  Dengan  demikian,  penggunaan  kata  tak 
baku  sering  digunakan  dalam  kata  stopword  sehingga 
kemungkinan  besar  normalisasi  memiliki  pengaruh  terhadap 
tahapan stopword removal. 

E. Perbandingan Performa Kamus 
1. Perbandingan Persentase Kata Tak Baku dari kedua kamus 

 7 / 10 

 
 
 
 
 
 
Gambar 6. Grafik Perbandingan Persentase Kata Tak Baku 

Dari gambar 6, terlihat bahwa selisih antara persentase kata 
tak  baku  dari  kedua  kamus  tidak  terlalu  signifikan  pada 
beberapa dataset seperti pada dataset 1, 2, 6 dan 8. Sedangkan 
selisih kata tak baku paling signfikan berbeda ada pada dataset 
5  dan  dataset  7  dengan  selisih  masing-masing  sekitar  3,7%. 
Selisih  yang  sangat  besar  ini  disebabkan  oleh  perbedaan 
pengenalan kata tak baku oleh kedua kamus yang cukup banyak. 
Contohnya kata “minta”(meminta) tidak terdapat dalam kamus 
normalisasi  satu  tetapi  terdapat  pada  kamus  normalisasi  dua 
dan kata “n”(dan) terdapat dalam kamus normalisasi satu tetapi 
tidak terdapat kamus normalisasi dua. 

2. Perbandingan Performa 

PERBANDINGAN AKURASI DARI TIGA TIPE DATA(DALAM %) 
yang 

No.  Dataset 

yang 

yang 

TABEL IX 

Data 
tidak 
dinormalisasi 

Dataset 1 
Dataset 2 
Dataset 3 
Dataset 4 
Dataset 5 
Dataset 6 
Dataset 7 
Dataset 8 

1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
Median 

86,12 
86,49 
75,42 
64,84 
82,72 
72,95 
60,41 
74,61 
75,01 

Data 
dinormalisasi 
dengan  kamus 
satu 
85,67 
86,49 
76,43 
64,18 
82,68 
72,13 
63,45 
73,93 
75,18 

Data 
Dinormalisasi 
dengan  Kamus 
Dua 
85,91 
89,19 
76,77 
65,05 
82,79 
72,95 
62,94 
70,45 
74,86 

Dari Tabel 9, terlihat bahwa nilai median akurasi dari data 
yang 
tidak  dinormalisasi  mengalami  kenaikan  setelah 
dinormalisasi  dengan  kamus  satu.  Sedangkan  nilai  median 
akurasi  data  yang  tidak  dinormalisasi  mengalami  penurunan 
setelah dinormalisasi dengan kamus dua. Setelah dinormalisasi 
dengan kamus satu median nilai akurasi mengalami kenaikan 
sebesar  0,17%  sedangkan  dengan  kamus  normalisasi  dua 
mengalami penurunan 0,15%. 

PERBANDINGAN PRESISI DARI TIGA TIPE DATA(DALAM %) 

TABEL X 

No.  Dataset 

yang 

Data 
tidak 
dinormalisasi 

1. 
2. 
3. 
4. 

Dataset 1 
Dataset 2 
Dataset 3 
Dataset 4 

86,12 
83,64 
74,91 
76,00 

yang 

Data 
dinormalisasi 
dengan  kamus 
satu 
85,63 
84,91 
75,70 
66,67 

yang 

Data 
Dinormalisasi 
dengan  Kamus 
Dua 
85,91 
88,46 
75,97 
75,00 

Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik 

Dataset 5 
Dataset 6 
Dataset 7 
Dataset 8 

5. 
6. 
7. 
8. 
Median 

88,84 
72,50 
59,06 
79,95 
77,97 

88,90 
72,27 
61,79 
79,02 
77,36 

89,08 
72,50 
61,48 
74,22 
75,48 

Dari Tabel 10, terlihat bahwa nilai median presisi dari data 
tidak  dinormalisasi  mengalami  penurunan  setelah 
yang 
dinormalisasi  dengan  kamus  satu  dan  kamus  dua.  Setelah 
dinormalisasi  dengan  kamus  satu  median  nilai  presisi 
mengalami penurunan sebesar 0,61% sedangkan dengan kamus 
normalisasi dua mengalami penurunan 2,49%. 

PERBANDINGAN RECALL DARI TIGA TIPE DATA(DALAM %) 

TABEL XI 

No.  Dataset 

yang 

Data 
tidak 
dinormalisasi 

Dataset 1 
Dataset 2 
Dataset 3 
Dataset 4 
Dataset 5 
Dataset 6 
Dataset 7 
Dataset 8 

1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
Median 

100,00 
88,46 
99,54 
10,98 
72,57 
100,00 
74,26 
64,50 
81,36 

yang 

Data 
dinormalisasi 
dengan  kamus 
satu 
100,00 
86,54 
99,54 
11,56 
72,41 
98,85 
75,25 
63,91 
80,89 

yang 

kamus 

Data 
dinormalisasi 
dengan 
dua 
100,00 
88,46 
99,54 
12,14 
72,27 
100,00 
74,26 
61,22 
81,36 

Dari Tabel 11, terlihat bahwa nilai median recall dari data 
yang  tidak dinormalisasi  setelah  dinormalisasi  dengan kamus 
satu  mengalami  penurunan.  Sedangkan  setelah  dinormalisasi 
dengan  kamus  dua  tetap  sama.  Setelah  dinormalisasi  dengan 
kamus satu median nilai presisi mengalami penurunan sebesar 
0,47%  sedangkan  dengan  kamus  normalisasi  dua 
tidak 
mengalami kenaikan ataupun penurunan. 

F. Uji Hipotesis 
1.  Data  yang 
dinormalisasi dengan kamus satu 

tidak  dinormalisasi  dengan  data  yang 

TABEL XII 

HASIL UJI HIPOTESIS ANTARA DATA YANG TIDAK 
DINORMALISASI DENGAN DATA YANG DINORMALISASI DENGAN 
KAMUS SATU 

No. 
1. 
2. 
3. 

Indikator 
Akurasi 
Presisi 
Recall 

Nilai p-value 
0,8657 
1,0000 
0,3454 

Keputusan 
Gagal Tolak H0 
Gagal Tolak H0 
Gagal Tolak H0 

Dari  Tabel 12, kesimpulan pada pengujian terhadap ketiga 
indikator  performa  menunjukkan  bahwa 
terdapat 
perbedaan  yang  signifikan  antara  ketiga  indikator  dari  data 
yang  tidak  dinormalisasi  dengan  data  yang  dinormalisasi 
dengan kamus normalisasi satu. 

tidak 

Dari hasil pengujian p-value, terlihat bahwa memang tidak 
terdapat  perbedaan  nilai  performa  dari  data  yang  tidak 
dinormalisasi  dengan  data  yang  dinormalisasi  dengan  kamus 
satu atau dengan kata lain perfoma data setelah dinormalisasi 
sama dengan data sebelum dinormalisasi. Hal ini memberikan 
hasil yang sama dengan penelitian sebelumnya yaitu [11] yang 
mana  nilai  F1-score  dari  penelitian  tersebut  memang  tidak 

 8 / 10 

 
 
 
 
berbeda  secara  signifikan.  Nilai  presisi  dan  recall  dari 
penelitian  ini  juga  memberikan  hasil  yang  sama  dengan 
penelitian tersebut. 

2.  Data  yang 
dinormalisasi dengan kamus dua 

tidak  dinormalisasi  dengan  data  yang 

TABEL XIII 

HASIL UJI HIPOTESIS ANTARA DATA YANG TIDAK 
DINORMALISASI DENGAN DATA YANG DINORMALISASI DENGAN 
KAMUS DUA 

No. 
1. 
2. 
3. 

Indikator 
Akurasi 
Presisi 
Recall 

Nilai p-value 
0,7531 
1,0000 
0,5929 

Keputusan 
Gagal Tolak H0 
Gagal Tolak H0 
Gagal Tolak H0 

Dari Tabel 13, kesimpulan pada pengujian terhadap ketiga 
indikator  performa  menunjukkan  bahwa 
terdapat 
perbedaan  yang  signifikan  antara  ketiga  indikator  dari  data 
yang  tidak  dinormalisasi  dengan  data  yang  dinormalisasi 
dengan kamus normalisasi dua. 

tidak 

3. Data yang dinormalisasi dengan kamus normalisasi satu dan 
kamus normalisasi dua 

TABEL XIV 

HASIL UJI HIPOTESIS ANTARA DATA YANG DINORMALISASI 
DENGAN KAMUS NORMALISASI SATU DENGAN DATA YANG 
DINORMALISASI DENGAN KAMUS DUA 

No. 
1. 
2. 
3. 

Indikator 
Akurasi 
Presisi 
Recall 

Nilai p-value 
0,9165 
0,7531 
0,8927 

Keputusan 
Gagal Tolak H0 
Gagal Tolak H0 
Gagal Tolak H0 

Dari  Tabel 14, kesimpulan pada  pengujian terhadap ketiga 
terdapat 
indikator  performa  menunjukkan  bahwa 
perbedaan  yang  signifikan  antara  ketiga  indikator  dari  data 
yang  dinormalisasi  dengan  kamus  normalisasi  satu  dengan 
kamus normalisasi dua. 

tidak 

VIII. 

PENUTUP 

Dari penelitian ini didapatkan hasil uji performa data yang 
tidak  dinormalisasi  dengan  performa  data  yang  dinormalisasi 
menggunakan  kamus  satu(Colloquial  Indonesian  Lexicon) 
dalam analisis sentimen yang menunjukkan bahwa bahwa tidak 
terdapat  perbedaan  performa  yang  signifikan  baik  akurasi, 
presisi,  maupun  recall.  Demikian  pula  dengan  hasil  uji 
performa data yang tidak dinormalisasi dengan performa data 
yang dinormalisasi menggunakan kamus dua(kamus INANLP) 
dalam analisis sentimen menghasilkan kesimpulan yang sama. 
Dalam  analisis,  normalisasi  menggunakan  kamus  pada 
INANLP  tidak  menghasilkan  kenaikan  performa  yang  besar 
sehingga  normalisasi  dengan  kamus  tersebut  dalam  analisis 
sentimen  bisa  dilakukan  ataupun  tidak.  Walaupun  demikian 
pada  pembahasan  sebelumnya,  terlihat  bahwa  kata  yang 
dinormalisasi  didominasi  oleh  kata  stopword  sehingga 
kemunculan kata ini kemungkinan tidak memberikan dampak 
yang  besar  terhadap  performa.  Disisi  lain,  dataset  yang 
digunakan dalam penelitian memiliki keterbatasan karakteristik 
sehingga pengaruh normalisasi ini dapat diuji coba pada dataset 

Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik 

lain yang memiliki  kata stopword yang lebih sedikit dan bentuk 
kata tak baku lebih banyak. 

Dari  hasil  uji  performa  data  yang  dinormalisasi  dengan 
kamus  satu(Colloquial Indonesian  Lexicon)  dengan performa 
data  yang  dinormalisasi  dengan  kamus  dua(INANLP) 
menghasilkan  kesimpulan  bahwa  tidak  terdapat  perbedaan 
yang  signifikan  dari  performa  kedua  kamus  dalam  analisis 
sentimen  baik  akurasi,  presisi,  maupun  recall.  Dengan 
demikian,  normalisasi  pada  analisis  sentimen  menggunakan 
kamus  satu  ataupun  kamus  dua  tetap  menghasilkan  performa 
yang tidak memiliki selisih yang besar. 

DAFTAR PUSTAKA 

[1] 

[2] 

[3] 

[4] 

[5] 

[6] 

[7] 

[8] 

[9] 

[10] 

“Indonesia smartphone users,” Statista. 
https://www.statista.com/statistics/266729/smartphone-users-in-
indonesia/ (diakses Apr 30, 2021). 
“• Twitter: most users by country | Statista.” 
https://www.statista.com/statistics/242606/number-of-active-twitter-
users-in-selected-countries/ (diakses Apr 30, 2021). 
I. F. Rozi, S. H. Pramono, dan E. A. Dahlan, “Implementasi Opinion 
Mining (Analisis Sentimen) untuk Ekstraksi Data Opini Publik pada 
Perguruan Tinggi,” J. EECCIS, vol. 6, no. 1, hlm. 37–43, 2013. 
I. Sunni dan D. H. Widyantoro, “Analisis sentimen dan ekstraksi topik 
penentu sentimen pada opini terhadap tokoh publik,” J. Sarj. ITB Bid. 
Tek. Elektro Dan Inform., vol. 1, no. 2, 2012. 
N. Monarizqa, L. E. Nugroho, dan B. S. Hantono, “Penerapan 
Analisis Sentimen Pada Twitter Berbahasa Indonesia Sebagai Pemberi 
Rating,” J. Penelit. Tek. Elektro Dan Teknol. Inf., vol. 1, no. 3, 2014. 
B. Liu, “Sentiment analysis and opinion mining,” Synth. Lect. Hum. 
Lang. Technol., vol. 5, no. 1, hlm. 1–167, 2012. 
R. Sproat, A. W. Black, S. Chen, S. Kumar, M. Ostendorf, dan C. 
Richards, “Normalization of non-standard words,” Comput. Speech 
Lang., vol. 15, no. 3, hlm. 287–333, 2001. 
D. S. Maylawati dan G. P. Saptawati, “Set of Frequent Word Item sets 
as Feature Representation for Text with Indonesian Slang,” dalam 
Journal of Physics: Conference Series, 2017, vol. 801, no. 1, hlm. 
012066. 
A. F. Hidayatullah, “Language tweet characteristics of Indonesian 
citizens,” dalam 2015 International Conference on Science and 
Technology (TICST), 2015, hlm. 397–401. 
E. Clark dan K. Araki, “Text normalization in social media: progress, 
problems and applications for a pre-processing system of casual 
English,” Procedia-Soc. Behav. Sci., vol. 27, hlm. 2–11, 2011. 

[11]  N. A. Salsabila, Y. A. Winatmoko, A. A. Septiandri, dan A. Jamal, 
“Colloquial indonesian lexicon,” dalam 2018 International 
Conference on Asian Language Processing (IALP), 2018, hlm. 226–
229. 

[12]  A. Purwarianti, A. Andhika, A. F. Wicaksono, I. Afif, dan F. Ferdian, 

“InaNLP: Indonesia natural language processing toolkit, case study: 
Complaint tweet classification,” dalam 2016 International Conference 
On Advanced Informatics: Concepts, Theory And Application 
(ICAICTA), 2016, hlm. 1–5. 

[13]  G. P. Wiratama dan A. Rusli, “Sentiment Analysis of Application 

User Feedback in Bahasa Indonesia Using Multinomial Naive Bayes,” 
dalam 2019 5th International Conference on New Media Studies 
(CONMEDIA), 2019, hlm. 223–227. 

[14]  K. Hulliyah, N. S. A. A. Bakar, A. R. Ismail, dan M. O. Pratama, “A 

Benchmark of Modeling for Sentiment Analysis of The Indonesian 
Presidential Election in 2019,” dalam 2019 7th International 
Conference on Cyber and IT Service Management (CITSM), 2019, 
vol. 7, hlm. 1–4. 

[15]  M. Abbas, K. A. Memon, A. A. Jamali, S. Memon, dan A. Ahmed, 
“Multinomial Naive Bayes classification model for sentiment 
analysis,” IJCSNS, vol. 19, no. 3, hlm. 62, 2019. 
J. Demšar, “Statistical comparisons of classifiers over multiple data 
sets,” J. Mach. Learn. Res., vol. 7, hlm. 1–30, 2006. 

[16] 

 9 / 10 

 
 
 
 
Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik 

[17]  C. C. Aggarwal dan C. Zhai, Mining text data. Springer Science & 

[18] 

[19] 

Business Media, 2012. 
J. Han, M. Kamber, dan J. Pei, “Data mining concepts and techniques 
third edition,” Morgan Kaufmann Ser. Data Manag. Syst., vol. 5, no. 
4, hlm. 83–124, 2011. 
S. N. Maula, “Kajian Perbandingan Algoritma Sentistrength dan 
Naïve bayes Analisis Sentimen Twitter (Studi Kasus: Pengesahan 
RUU Penghapusan Kekerasan Seksual).” Politeknik Statistika 
Sekolah Tinggi Ilmu Statistik, 2019. 

[20]  B. D. P. Nugraha, “Kajian Analisis Sentimen Data Twitter 

Menggunakan Metode Support Vector Machine Dengan Optimasi Pso 
dan Firefly.” Jakarta: Politeknik Statistika Sekolah Tinggi Ilmu 
Statistik, 2017. 

[21]  M. Firdaus, “Kajian Perbandingan Metode Klasifikasi CNN Dan 

LSTM Pada Analisis Sentimen Berbahasa Indonesia (Studi Kasus: 
Data Twitter Tentang Harga Cabai).” Politeknik Statistika Sekolah 
Tinggi Ilmu Statistik, 2019. 

[22]  D. Rangga, “Twitter Sentiment Analysis Final Project,” Sep 13, 2020. 

[Daring]. Tersedia pada: 
https://github.com/devildances/TwitterSentimentAnalysis_Final_Proj
ect 

[23]  Y. Indrawan, “yahdiindrawan/covid19-sentiment-dataset,” Feb 09, 
2021. https://github.com/yahdiindrawan/covid19-sentiment-dataset 
(diakses Apr 30, 2021). 

[24]  R. C. Rajagukguk, “riochr17/Analisis-Sentimen-ID,” Mar 31, 2021. 
https://github.com/riochr17/Analisis-Sentimen-ID (diakses Apr 30, 
2021). 

[25]  R. Ferdiana, F. Jatmiko, D. D. Purwanti, A. S. T. Ayu, dan W. F. 
Dicka, “Dataset Indonesia untuk Analisis Sentimen,” J. Nas. Tek. 
Elektro Dan Teknol. Inf. JNTETI, vol. 8, no. 4, hlm. 334–339, 2019. 

 10 / 10 

 
 
 
"
221709696,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Kajian Pemanfaatan Telemedicine di Indonesia 
Studi Kasus : Laman Alodokter, Dokter.id, dan Honestdocs 

Faza Nur Fuadina (221709696, 4SD2) 
Dosen Pembimbing: Nucke Widowati Kusumo Projo, S.Si, M.Sc, Ph.D 

internet  dan 

Ringkasan—Perkembangan 

teknologi  di 
Indonesia  terjadi  di  berbagai  sisi  kehidupan  termasuk  pada 
bidang  kesehatan  dengan  adanya  e-health.  Pemanfaatan 
telemedicine  sebagai  salah  satu  wujud  e-health  di  Indonesia 
dirasa masih awam di kalangan sebagian masyarakat mengingat 
keberadaannya  tidak  sebanyak  e-commerce  yang  bergerak  di 
sektor  ekonomi.  Adanya  pandemi  COVID-19  menjadi  alasan 
terciptanya aturan pembatasan pergerakan, namun juga dapat 
menjadi  alasan  adanya  pemanfaatan  telemedicine  di  Indonesia. 
Dengan memanfaatkan data pada  telemedicine, peneliti melihat 
sejauh  mana  pemanfaatan  telemedicine  di  Indonesia  serta 
melihat fenomena kesehatan yang tertangkap pada data tersebut. 
Analisis deskriptif dan text mining digunakan untuk mengetahui 
pemanfaatan 
telemedicine  dengan  metode  Named  Entity 
Recognition (NER) dan Latent Dirichlet Allocation (LDA). Selain 
itu,  dilakukan  kajian  literatur  untuk  mengidentifikasi  potensi 
pemanfaatan  data  telemedicine  dalam  pengumpulan  statistik 
kesehatan di Indonesia. Hasil menunjukkan bahwa : telemedicine 
sudah  banyak  dimanfaatkan  di  Indonesia,  data  telekonsultasi 
klinis  dan  judul  artikel  pada  telemedicine  dapat  menghasilkan 
berbagai topik kesehatan, dan data telemedicine berpotensi untuk 
dimanfaatkan sebagai sumber pengumpulan statistik kesehatan. 
Kata Kunci— telemedicine, telekonsultasi klinis, kesehatan, text 

mining 

I.  LATAR BELAKANG 

Jasa 

Masyarakat membutuhkan peningkatan kecanggihan teknologi 
dan  permintaan  fasilitas  internet  yang  semakin  tinggi  saat  ini. 
Asosiasi  Penyelenggara 
(APJII) 
menyelenggarakan  Survei    Penetrasi  dan  Perilaku  Penggunaan 
Internet pada tahun 2019 dan menghasilkan fakta bahwa jumlah 
pengguna  internet  meningkat  23,5  juta  atau  8,9%  dibandingkan 
pada  2018  lalu.  Terjadinya  peningkatan  ini  mengindikasikan 
bahwa  terdapat  perkembangan  teknologi  yang  membutuhkan 
akses internet di berbagai sisi kehidupan. 

Indonesia 

Internet 

Dari  sisi  ekonomi,  Setiawan  menyatakan  pertumbuhan 
teknologi  digital  dan  perkembangan  internet  membawa  pada 
perubahan model bisnis. Hal ini memunculkan revolusi ekonomi 
menuju ekonomi digital pada berbagai macam lini bisnis seperti 
perdagangan  online  (e-commerce),  bisnis  rintisan  berbasis 
teknologi  (startup), dan  layanan  keuangan digital  atau  financial 
technology  (fintech)  [1].  Disisi  lain,  Palinggi  dan  Limbongan 
menyatakan bahwa jumlah pengguna internet  yang ada saat ini 
mampu  dimanfaatkan  oleh  para  pelaku  usaha  khususnya  dalam 
peningkatan  ekonomi  melalui  industri  e-commerce  [2].  Adanya 
dampak perkembangan internet dan teknologi pada sisi ekonomi 
dimungkinkan  dapat  terjadi  pada  sisi  sosial  seperti  pada  bidang 
kesehatan. 

Menurut World Health Organization (WHO), istilah electronic 
health  atau  biasa  dikenal  dengan  e-health  adalah  bentuk 
pemanfaatan Teknologi, Informasi, dan Komunikasi (TIK) yang 
hemat biaya dan aman untuk mendukung berbagai hal di bidang 

untuk  menerapkan  TIK 

kesehatan [3]. Dalam Global Information Society Watch (GISW) 
2007,  e-health  menjadi  salah  satu  sistem  informasi  yang 
disarankan  untuk  dikembangkan  oleh  Perserikatan  Bangsa 
Bangsa  (PBB)  seiring  dengan  permintaan  PBB  terhadap  negara 
anggota 
infrastruktur 
pembangunan  yang  terintegrasi  [3].  Pada  tahun  2005,  WHO 
mencetuskan  adanya  Resolusi  WHA  28.28  (World  Health 
Assembly  Resolution  on  e-health,  Mei  2005)  yang 
turut 
membahas  mengenai  penggunaan  TIK  untuk  mendukung 
permasalahan kesehatan. Beriringan dengan perkembangan TIK 
pada bidang kesehatan di Indonesia, muncul berbagai bentuk  e-
health (e-kesehatan), baik yang dibangun oleh instansi kesehatan 
seperti  situs  mandiri  milik  rumah  sakit  atau  klinik  kesehatan, 
maupun milik swasta.  

dalam 

Berdasarkan Peraturan Menteri Kesehatan Republik Indonesia 
(Permenkes  RI)  Nomor  46  Tahun  2017  Tentang  Strategi  E-
Kesehatan  Nasional,  salah  satu  bentuk  inisiatif  implementasi  e-
kesehatan di Indonesia dalam bentuk dukungan layanan kesehatan 
individu adalah telemedicine [4]. Disisi lain, menurut Permenkes 
RI  Nomor  20  Tahun  2019  tentang  Penyelenggaraan  Pelayanan 
Telemedicine  antar  Fasilitas  Pelayanan  Kesehatan,  telemedicine 
diartikan sebagai pemberian pelayanan kesehatan jarak jauh oleh 
profesional kesehatan dengan menggunakan teknologi informasi 
dan  komunikasi,  yang  mana  salah  satu  bentuk  pelayanannya 
berwujud  telekonsultasi  klinis.  Telekonsultasi  klinis  merupakan 
pelayanan  konsultasi  klinis 
jauh  untuk  membantu 
menegakkan diagnosis, dan/atau memberikan pertimbangan/saran 
tata laksana [5]. Beberapa contoh telemedicine milik swasta yaitu 
Alodokter, Dokter.id, Halodocs, Honestdocs, dan yesdok. 

jarak 

Pentingnya keberadaan e-health di Indonesia sudah diakui oleh 
pemerintah seperti yang disampaikan oleh Daryo Soemitro bahwa 
rencana  kerja  strategis  pemerintah  terkait  penerapan  e-health 
sudah mulai dikembangkan sejak tahun 2015. Beberapa e-health 
yang  saat  ini  mulai  berkembang  adalah  telemedicine  [6].  Oleh 
sebab  itu,  wajar  bila  saat  ini  sudah  banyak  instansi  kesehatan 
pemerintah  maupun  swasta  yang  membangun  e-health  guna 
mendukung pelayanannya. Indonesia memiliki potensi yang besar 
untuk  menerapkan  e-health  karena  dengan  media  elektronik 
sudah  dapat  menjangkau  kepulauan  yang  ada  di  Indonesia 
walaupun masih terdapat daerah yang belum memiliki listrik dan 
internet [7]. Namun, permasalahan tentang e-health di Indonesia 
belum  banyak  dibahas  [8].  Peluang  berkembangnya  telemedika 
[9].  Program 
(telemedicine)  di 
telemedicine sangat penting bagi masyarakat di daerah tertinggal 
dan perbatasan [10]. 

sangat  besar 

Indonesia 

Menurut  Kementerian  Kesehatan  Republik 

Indonesia 
(Kemenkes RI), Coronavirus Disease 2019 (COVID-19) adalah 
penyakit  yang  disebabkan  oleh  virus  Sars-CoV-2.  Persebaran 
COVID-19  yang  terlampau  cepat  dan  meluas  membuat  WHO  
menetapkan  COVID-19  sebagai  pandemi.  Adanya  pandemi  ini 
memunculkan  berbagai  peraturan  baru  yang  membatasi 

 1 / 8 

 
 
 
 
kehidupan  bermasyarakat,  salah  satunya  yaitu  PP  Nomor  21 
Tahun 2020 tentang Pembatasan Sosial Berskala Besar (PP PSBB) 
dalam  rangka  percepatan  penanganan  COVID-19.  Peraturan 
tersebut  menyatakan  adanya  pembatasan  pergerakan  orang  dan 
barang  guna  mencegah  penyebaran  virus  COVID-19.  Segala 
bentuk  kegiatan  dianjurkan  untuk  dikerjakan  di  rumah  saja 
termasuk kegiatan beribadah, belajar, dan bekerja [11]. Namun, 
kesehatan adalah kebutuhan penting dalam menjalani kehidupan 
sehari-hari.  Apabila  terdapat  gangguan  kesehatan,  masyarakat 
membutuhkan fasilitas dan pelayanan kesehatan guna menopang 
kesehatan yang lebih baik. Masyarakat tetap harus mengupayakan 
terpenuhinya  kebutuhan  layanan  tersebut  ditengah  berlakunya 
pembatasan 
e-health  maupun 
telemedicine  menjadi 
solusi  yang  memungkinkan  dari 
permasalahan tersebut.  

pergerakan.  Pemanfaatan 

Statistik  kesehatan  diperlukan  untuk  membantu  pemerintah 
atau instansi kesehatan menetapkan kebijakan. Saat ini, kegiatan 
pengumpulan statistik kesehatan dilakukan melalui pengumpulan 
statistik  dasar  oleh  BPS  dan  secara  sektoral  dikumpulkan  oleh 
Kementrian Kesehatan (Kemenkes). Pengumpulan data kesehatan 
yang  dilakukan  BPS  yaitu  Susenas  dan  SDKI,  sedangkan 
Kemenkes melakukannya melalui Riskesdas, Sirkesnas, Rifaskes 
dan Risnakes. Data yang dikumpulkan BPS bertujuan mengetahui 
kondisi  kesehatan  di  Indonesia.    Data  yang  dikumpulkan  oleh 
Kemenkes  mayoritas  adalah  mengenai  fasilitas,  layanan,  dan 
tenaga  kesehatan.  Pelaksanaan  pengumpulan  data  oleh  BPS 
maupun  Kemenkes  memiliki  rentang  waktu  pelaksanaan  yang 
masih  cukup  lama.  Pengumpulan  data  statistik  kesehatan  dirasa 
kurang mutakhir, kurang menyeluruh, dan kurang lengkap karena 
cakupan  variabelnya  kurang  mencakup  fenomena  kesehatan 
secara luas.  

Menurut Salahudin Big Data meliputi data dan informasi yang 
mencakup  volume,  velocity,  variety,  veracity,  dan    values. 
Manusia    menghasilkan   Big  Data   yang   diproduksi   di   dunia  
maya    melalui    mekanisme    teknologi;  komputer,  kecerdasan 
buatan, dan mobile internet [12]. Adanya e-health dalam bentuk 
suatu  sistem  elektronik  turut  menghasilkan  Big  Data  yang 
menyimpan data dan informasi. Terdapat suatu proses penemuan 
pengetahuan baru melalui  data mining yang disebut  Knowledge 
Discovery  Process  (KDP)  yang  dapat  diartikan  sebagai  suatu 
tahapan  analisis  untuk  mengidentifikasi  pola  yang  baru, 
bermanfaat,  dan dapat dipahami  dari kumpulan data  yang  besar 
dan  kompleks.  Big  Data  menjadi  salah  satu  bentuk  data  yang 
besar  dan  kompleks  sehingga  KDP  dapat  diterapkan  dalam 
melakukan analisis terhadapnya.  

II.  TUJUAN PENELITIAN 

Peneliti  memanfaatkan  data  telemedicine  untuk  melakukan 

penelitian dengan tujuan : 
•  Mengetahui  pemanfaatan  telemedicine  di  Indonesia  dengan 

melakukan analisis fitur. 

•  Memodelkan  topik  kesehatan  dengan  data  telekonsultasi 

klinis dan judul artikel. 

•  Mengidentifikasi  potensi  pemanfaatan  data  telemedicine 

dalam pengumpulan statistik kesehatan di Indonesia. 

III. PENELITIAN TERKAIT 

Proses studi literatur atas permasalahan yang diangkat dalam 
penelitian ini menggunakan beberapa penelitian terkait. Temuan 
penelitian terkait dirangkum dalam peta literatur pada Gambar I. 
Penelitian  pada  bagian  berwarna  hijau  membahas  mengenai 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

wujud  perkembangan  TIK  di  bidang  kesehatan.  Bagian  warna 
biru  merupakan  bagian  utama  dimana  penelitian  yang  tertulis 
memiliki keterkaitan yang paling erat yaitu mengenai analisis data 
kesehatan  dari  Big  Data  baik  dengan  metode  Latent  Dirichlet 
Allocation (LDA), Named Entity Recognition (NER), atau analisis 
lainnya.    Bagian  warna  jingga  menampilkan  penelitian  terkait 
yang  memanfaatkan  metode  LDA.  Bagian  ungu  mencakup 
penelitian terkait NER dan bagian abu-abu mencakup penelitian 
yang menerapkan Knowledge Discovery Process (KDP). 

Gambar 1. Peta literatur (literature map) 

Islam  melakukan  perbandingan  metode  pemodelan  topik 
dengan  LDA,  LSA,  dan  NMF,  dengan  hasil  pemodelan 
menggunakan LDA menghasilkan topik yang lebih koheren dan 
konsisten [13]. Dari penelitian terkait pemodelan topik kesehatan 
dengan  metode  LDA,  metode  tersebut  dapat  memodelkan  topik 
kesehatan  dari  data  media  sosial,  berita,  dan  artikel  kesehatan 
dengan  baik.  Namun  yang  berbeda  dengan  yang  dikerjakan 
peneliti  adalah  belum  adanya  penelitian  menggunakan  data 
telekonsultasi klinis pada telemedicine yang menjadi studi kasus 
penelitian  ini.  Selain  itu,  belum  ada  juga  penelitian  yang 
mengidentifikasi  potensi  data 
tersebut  dalam  melengkapi 
pengumpulan statistik kesehatan yang sudah ada. 

IV. METODE PENELITIAN  

A.  Ruang Lingkup Penelitian 

Penelitian  dilaksanakan  mulai  Bulan  Oktober  2020 
hingga    Bulan  Juni  2021.  Objek  penelitian  ini  adalah  tiga 
laman 
telemedicine  yaitu  Alodokter,  Dokter.id,  dan 
Honestdocs. Data yang digunakan yaitu data dari telemedicine 
tersebut  dalam  lingkup  waktu  Bulan  Mei  2019  hingga 
Oktober  2020,  serta  metadata  pengumpulan  data  kesehatan 
dari BPS dan Kemenkes. 
B.  Metode Pengumpulan Data 

Data  yang  bersumber  dari  telemedicine  dikumpulkan 
dengan 
cara  Web  Scraping  memanfaatkan  bahasa 
pemrogaman Python 3.9 dengan modul request dan selenium. 
Sedangkan  metadata  dikumpulkan  dengan  cara  mengunduh 
dokumen terkait dari website BPS dan Kemenkes. Gambar 2 
menampilkan  diagram  alir  dari  proses  pengumpulan  data 
telemedicine.  Hal  pertama  yang  dilakukan  dalam  proses  ini 
yaitu mengumpulkan Uniform Resource Locator (URL). URL 
akan  membawa  pengguna  ke  laman  telemedicine  yang 
menampilkan  beberapa  informasi  yang  dibutuhkan  dalam 

 2 / 8 

 
 
 
penelitian.  Karena  data  yang  ingin  dikumpulkan  terdapat 
dalam beberapa halaman web yang berbeda, perlu dilakukan 
pengumpulan URL dari seluruh data yang ingin dikumpulkan 
sehingga  proses  pengumpulan  data  dapat  terprogram  secara 
efisien dan efektif. Setelah URL terkumpul barulah data dapat 
dikumpulkan.  Sebelum  masuk  dalam  proses  analisis,  data 
hasil  scraping  akan  dilakukan  decoding  untuk  mengubah 
format  dari  kode  yang  sesuai  dengan  struktur  penulisan 
pemrogaman  web,  menjadi 
format  penulisan  Bahasa 
Indonesia.  Kemudian  dilakukan  data  selection  dan  data 
validation untuk mengeliminasi data yang tidak sesuai dengan 
kriteria data yang dibutuhkan. Hasil akhir pengumpulan data 
ini peneliti sebut dengan “Data Bersih” dan siap untuk masuk 
dalam tahap analisis data. 

Gambar 2. Diagram alir pengumpulan data telemedicine 

C.  Metode Analisis Data 

Metode analisis data yang digunakan diterapkan secara 
terpisah sesuai dengan jenis datanya. Data telemedicine akan 
tujuan  pertama  dan  kedua. 
dianalisis  guna  memenuhi 
Sedangkan  metadata  pengumpulan  statistik  kesehatan 
dianalisis  dengan  metode  kajian  literatur  guna  memenuhi 
tujuan  penelitian  ketiga.  Seluruh  data  akan  dianalisis, 
dilanjutkan  dengan  proses  interpretasi  dan  visualisasi  data. 
Pada proses akhir, ditarik kesimpulan atas hasil penelitian. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

dianalisis.  Dalam  preprocessing,  terlebih  dahulu  dibuat 
kamus normal dari token kata unik data teks yang digunakan. 
Kamus  normal  mencakup  kata  yang  sudah  melalui  tahap 
normalisasi dan stemming menjadi kata dasar. Kamus normal 
kemudian  diaplikasikan  pada  data  diikuti  dengan  proses 
stopword baru kemudian menjadi “Data Siap Olah”. Data ini 
selanjutnya  dapat  dilakukan  analisis  deskriptif  dan  analisis 
teks (text mining). Text Mining adalah penggalian data untuk 
menyelesaikan  masalah  kebutuhan 
informasi  dengan 
menerapkan  teknik  data  mining,  machine  learning,  natural 
language  processing,  pencarian  informasi,  dan  manajemen 
pengetahuan [14]. 

Dua metode text mining yang akan diterapkan dalam 
penelitian ini yaitu LDA dan NER. Latent Dirichlet Allocation 
(LDA)  merupakan  model  Bayesian  hirarki  tiga  tingkat,  di 
mana  setiap  item  koleksi  dimodelkan  sebagai  campuran 
terbatas  atas  serangkaian  set  topik.  Setiap  topik  dimodelkan 
sebagai  campuran  tak  terbatas  melalui  set  yang  mendasari 
probabilitas  topik.  Dalam  konteks  pembuatan  model  teks, 
probabilitas  topik  memberikan  representasi  eksplisit  dari 
sebuah dokumen [15]. Metode ini digunakan dalam penelitian 
ini untuk memodelkan topik kesehatan dari data telekonsultasi 
klinis dan judul artikel kesehatan. Named Entity Recognition 
(NER)  dikenal  sebagai serangkaian  teknik  berbasis  statistik, 
algoritma, dan kamus yang digunakan untuk menelusuri teks 
tidak  terstruktur  dan  mengklasifikasikan  kata  ke  dalam 
kategori  yang  telah ditentukan sebelumnya  [16].  Metode  ini 
tujuan  penggunaannya  untuk  mengidentifikasi  istilah  obat, 
penyakit, dan istilah tentang COVID-19. Metode NER yang 
digunakan  adalah  metode  NER  berbasis  kamus  dengan 
memanfaatkan algoritma FuzzyWuzzy. 

Sebagai  bentuk  validasi  data  telemedicine,  peneliti 
melakukan  matching  data  terhadap  data  dokter  dan  data 
rumah  sakit.  Data  dokter  pada  telemedicine  dilakukan 
matching  data  dengan  data  dokter  dari  Ikatan  Dokter 
Indonesia  (IDI).  Sedangkan  data  rumah  sakit  dilakukan 
matching  data  dengan  data  rumah  sakit  terakreditasi  milik 
Kementerian  Kesehatan.  Proses  matching  data  dilakukan 
dengan pengukuran kesamaan istilah menggunakan algoritma 
FuzzyWuzzy. 

V.  KERANGKA PIKIR 

Gambar 3. Diagram alir analisis data telemedicine 

Alur  analisis  data  yang  dilakukan 

terhadap  data 
telemedicine ditunjukkan pada Gambar 3. “Data Bersih” yang 
dihasilkan  dari  proses  pengumpulan  data  (Gambar  2) 
memerlukan  preprocessing  hingga  berwujud  data  yang  siap 

Gambar 4. Kerangka pikir penelitian 

 3 / 8 

 
 
 
 
 
Gambar 4 menampilkan kerangka pikir dari penelitian ini. 
Dimulai  dari  penggambaran  e-health,  telemedicine,  dan 
telekonsultasi  klinis 
telekonsultasi  klinis,  bahwasannya 
merupakan bagian dari telemedicine dan telemedicine bagian 
dari  e-health.  Kemudian,  data  yang  bersumber  dari 
telemedicine  akan  dikumpulkan  dengan  cara  Web  Scraping 
untuk  kemudian  dapat  dilakukan  analisis  fitur,  pemodelan 
topik,  identifikasi  potensi  pemanfaatan,  dan  matching  data. 
Analisis fitur dilakukan menggunakan analisis deskriptif dan 
NER.  Pemodelan  topik  akan  dilakukan  degan  metode  LDA. 
Data telemedicine dieksplorasi dan dikaitkan dengan statistik 
kesehatan  yang  ada  di  Indonesia  untuk  dapat  diidentifikasi 
potensi pemanfaatannya. Selanjutnya dilakukan matching data 
untuk validasi data telemedicine terhadap statistik kesehatan. 

VI. HASIL DAN PEMBAHASAN 

A.  Analisis Pemanfaatan Telemedicine Berdasarkan Fitur 

Analisis  yang  dilakukan  berupa  perbandingan  antar 
telemedicine  dan  juga  analisis  pada  setiap  fitur  dengan 
menggunakan data gabungan ketiga laman telemedicine yang 
ada.  Fitur-fitur  yang  di  analisis  yaitu  fitur  “tanya  dokter” 
(telekonsultasi klinis), direktori rumah sakit, direktori dokter, 
artikel, juga obat dan penyakit. 
1.  Tanya Dokter (Telekonsultasi Klinis) 

Fitur “tanya dokter” yang merupakan perwujudan adanya 
telekonsultasi  klinis  merupakan  fitur  yang  berwujud  tanya 
jawab  antara  pengguna  dengan  dokter/tenaga  medis  lainnya 
dalam telemedicine. Tabel I menampilkan data perbandingan 
dan  gabungan  ketiga  telemedicine  yang  dapat  menunjukkan 
kondisi fitur dan bukti nyata pemanfaatan fitur tersebut.  
TABEL I 
HASIL ANALISIS FITUR TELEKONSULTASI KLINIS 

Perbandingan 

Banyak Pertanyaan 
Rata-rata lama jawab 
Pengaruh COVID-19 
Tren 

Alodokter 
163.115 
14,19 jam 
Tidak Ada 
Naik 

Laman telemedicine 
Dokter.id 
1.188 
11,13 jam 
Ada 
Turun 

Honestdoc 
22.954 
1,068 jam 
Tidak Ada 
Turun 

Gabungan 

187.257 
3,9 jam 
Tidak Ada 
Turun 

Selama  periode  Mei  2019  -  Oktober  2020,  secara  total 
terdapat 187.257 pertanyaan dari ketiga laman  telemedicine. 
Pertanyaan terbanyak bersumber dari Alodokter. Secara rata-
rata, Honestdocs memiliki kecepatan menjawab yang paling 
cepat.  Secara  gabungan,  tren  pengguna  fitur  ini  turun  dan 
hanya  Alodokter  saja  yang  menampilkan  tren  naik  (seiring 
waktu penggunanya semakin banyak). Hanya Dokter.id yang 
menunjukkan  adanya  pengaruh  COVID-19  dimana  yang 
dimaksud disini adalah adanya peningkatan pengguna secara 
signifikan saat masuknya COVID-19 di Indonesia yaitu pada 
Bulan Maret 2020. Namun dari hasil identifikasi pembahasan 
COVID-19 dengan metode NER, secara keseluruhan terdapat 
2%  pembahasan  dalam  fitur  ini.  Pembahasan  tertinggi  pada 
Bulan  Maret  2020  bertepatan  masuknya  COVID-19  ke 
tren  pembahasan 
Indonesia.  Selama  periode 
COVID-19  dalam 
tren  naik 
(penggunaan  meningkat).  Adanya  tren  ini  membuktikan 
bahwa 
telemedicine  cukup  mutakhir  menggambarkan 
fenomena kesahatan yang banyak dibahas oleh masyarakat. 
2.  Rumah Sakit 

ini  menunjukkan 

tersebut, 

fitur 

Fitur  Rumah  Sakit 

fitur  yang 
mengandung  informasi  direktori/daftar  RS  lengkap  beserta 

(RS)  merupakan 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

informasi  umumnya  (alamat,  kontak).  Tabel  II  menampikan 
data  perbandingan  serta  gabungan  dari  fitur  ini  pada  ketiga 
telemedicine  yang  diteliti.  Total  terdapat  2.122  RS  terdaftar 
dari ketiga laman tersebut dimana sebagian besar merupakan 
data  RS  dari  Alodokter.  Dari  segi  kelengkapan  atributnya, 
Alodokter  memiliki 
tingkat  kelengkapan  atribut  yang 
tertinggi, begitu pula dengan cakupan provinsi yang memiliki 
RS  terdaftar  dalam  telemedicine.  Secara  gabungan,  cakupan 
provinsi sebesar 97,06% atau 33 dari 34 provinsi dimana lima 
provinsi dengan RS terbanyak yaitu DKI Jakarta, Jawa Barat, 
Jawa Timur, Jawa Tengah, Banten. 

TABEL II 
HASIL ANALISIS FITUR RUMAH SAKIT 

Perbandingan 

Banyak RS 
Kelengkapan Atribut 
Cakupan Provinsi 

3.  Dokter 

Alodokter 
1.183 
90,11% 
97,06% 

Laman telemedicine 
Dokter.id 
692 
7,08% 
91,18% 

Honestdoc 
247 
89,88% 
61,76% 

Gabungan 

2.122 
- 
97,06% 

dokter 

dengan 

Serupa  dengan  RS,  fitur  Dokter  juga  menampilkan 
informasi 
direktori/daftar 
dilengkapi 
pribadinya  (spesialisnya,  alamat,  kontak,  tempat  &  jadwal 
praktek).  Fitur  ini  hanya  dimiliki  Alodokter  dan  Dokter.id. 
Hasil analisis fitur ini ditunjukkan pada Tabel III. Secara total 
ada 12.859 dokter terdaftar. Alodokter memiliki dokter lebih 
banyak  dan  kelengkapan  atribut  yang 
lebih  besar 
dibandingkan  dengan  Dokter.id.  Untuk  cakupan  provinsi 
keberadaan  dokter  memiliki  nilai  yang  sama  yaitu  91,18% 
namun bila data digabungkan, cakupan provinsinya mencapai 
97,06%.  Lima  provinsi  dengan  dokter  terbanyak  yaitu  DKI 
Jakarta, Jawa Barat, Banten, Jawa Timur, Jawa Tengah. Jika 
dilihat dari spesialisasinya, lima spesialisasi dokter terbanyak 
yaitu  dokter  kandungan,  dokter  gigi,  dokter  anak,  dokter 
umum, dokter penyakit dalam. Lima provinsi dengan rumah 
sakit dan dokter terbanyak berada di Pulau Jawa. Hal tersebeut 
memiliki kesesuaian proporsionalitas dengan  fakta dari BPS 
bahwa 56,10% penduduk Indonesia berada di Pulau Jawa. 

TABEL III 
HASIL ANALISIS FITUR DOKTER 

Laman telemedicine 

Alodokter 
12.712 
45,08% 
91,18% 

Dokter.id 
147 
6,12% 
91,18% 

Gabungan 

12.859 
- 
97,06% 

Perbandingan 

Banyak Dokter 
Kelengkapan Atribut 
Cakupan Provinsi 

4.  Artikel 

terkait  dengan  kesehatan.  Ketiga 

Fitur  artikel  merupakan  halaman  yang  menampilkan 
telemedicine 
artikel 
memiliki fitur ini dengan total 9.096 artikel yang dirilis selama 
periode  data  penelitian,  dengan  penyumbang  terbanyak  dari 
laman Honestdocs. Tabel IV menampilkan hasil analisis fitur 
artikel. Dilihat dari kelengkapan atributnya, hanya Alodokter 
yang atributnya tidak lengkap. Kaitannya dengan COVID-19, 
hanya Honestdocs yang tidak menunjukkan adanya pengaruh 
dalam  artikel  yang  dirilis.  Namun  bila  dilihat  banyaknya 
pembahasan  COVID-19  dalam  fitur  ini,  hanya  2,21%  saja 
yang  membahas  mengenai  COVID-19  dengan  pembahasan 
terbanyak  pada  Bulan  April  2020  atau  sebulan  setelah 
COVID-19  masuk  ke  Indonesia.  Untuk  tren  banyaknya 

 4 / 8 

 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

tren  naik 

1.  Pemodelan Topik Data Telekonsultasi Klinis 

artikel,  hanya  Dokter,id  yang  menunjukkan 
(meningkat seiring waktu).  

TABEL IV 
HASIL ANALISIS FITUR ARTIKEL 

Perbandingan 

Banyak Artikel 
Kelengkapan Atribut 
Pengaruh COVID-19 
Tren 

Alodokter 
2.351 
95,36% 
Ada 
Turun 

Laman telemedicine 
Dokter.id 
1.120 
100% 
Ada 
Naik 

Honestdoc 
5.625 
100% 
Tidak Ada 
Turun 

Gabungan 

9.096 
- 
Tidak Ada 
Turun 

5.  Obat dan Penyakit 

Fitur  Obat  dan  Penyakit  merupakan  fitur  yang 
menyajikan  daftar  serta  penjelasan  terkait  obat/penyakit. 
Dalam  penelitian  ini,  data  obat  dan  penyakit  bukan 
dimanfaatkan  untuk  dianalisis  fiturnya,  melainkan  untuk 
mengidentifikasi  bahasan  umum  dari  kesehatan  dimana 
ketika  seseorang  melakukan  konsultasi/berobat  ke  dokter/ 
tenaga  medis,  umumnya  menanyakan  mengenai  penyakit 
yang  dideritanya  atau  obat  yang  harus  dikonsumsi  ataupun 
penjelasan  suatu  obat.  Dengan  menggunakan  metode  NER 
berbasis  kamus  obat  dan  penyakit  dari  data  telemedicine, 
tujuan analisis ini untuk melihat bagaimana pembahasan obat 
dan  penyakit  dalam  data 
telekonsultasi  klinis  dari 
telemedicine.  

istilah  obat.  Sedangkan  untuk 

Hasil  yang  didapatkan,  banyaknya  istilah  obat  yang 
disebut  sebanyak  129.043  kali  dengan  kata  unik  sebanyak 
2.230 
istilah  penyakit 
disebutkan  sebanyak  224.934  kali  dengan  istilah  penyakit 
unik  sebanyak  6.634  kata.  Bila  dibandingkan,  istilah 
penyakit  lebih  banya  dibahas  dengan  perbandingan  obat  : 
penyakit  =  1  :  1,74.  Hal  yang  sama  juga  terlihat  pada  tiap 
waktu  pembahasannya  dimana  selama  periode  Mei  2019  – 
Oktober 2020, penyakit selalu lebih banyak dibahas daripada 
obat. 

Dikaitkan  dengan  COVID-19,  istilah  “covid”  masuk 
dalam  25  penyakit  yang  paling  banyak  dibahas.  Pada  saat 
kemuculan COVID-19 di Indonesia yaitu Bulan Maret 2020, 
“covid”  termasuk  dalam  10  penyakit  teratas  yang  paling 
banyak dibahas pada Bulan Maret - April 2020. Untuk istilah 
obat,  kaitannya  dengan  COVID-19  obat  yang  banyak 
dibicarakan  masyarakat  yaitu  “vitamin”,  “suplemen”,  dan 
“vaksin”  masuk  dalam  25  pembahasan  terbanyak.  Pada 
Bulan  Maret-April  2020  ketiga  obat  tersebut  juga  masuk 
dalam 10 bahasan teratas. 

B.  Pemodelan Topik Kesehatan 

Pembahasan yang kedua yaitu membahas hasil analisis 
teks  dengan  tujuan  mengelompokkan  data  teks  yang  ada 
berdasarkan topik-topik yang tersembunyi dari data tersebut. 
Analisis  ini  dilakukan  dengan  metode  Latent  Dirichlet 
Allocation  (LDA).  Data  yang  akan  dianalisis  yaitu  data 
pertanyaan  pada  fitur  “tanya  dokter”  (telekonsultasi  klinis) 
dan  data  judul  artikel.  Sebelum  digunakan  untuk  analisis, 
kedua  data  tersebut  telah  melewati  tahap  preprocessing 
hingga  menjadi  “data  siap  olah”.  Pembahasan  ini  akan 
yang 
memaparkan  mengenai 
tersembunyi  dibalik  data  yang  ada,  juga  tren  banyaknya 
pembahasan tiap topik untuk setiap waktunya. 

topik-topik 

kesehatan 

Untuk  mendapatkan  model  terbaik,  terlebih  dahulu 
dilakukan tuning parameter untuk menentukan banyak topik 
yang paling baik untuk memodelkan data tersebut. Parameter 
yang di-tuning yaitu banyaknya topik, sedangkan nilai yang 
digunakan  sebagai  penanda  baik  buruknya  model  yaitu 
coherence score. Dalam proses tuning parameter, model yang 
menghasilkan nilai coherence score paling tinggi merupakan 
model  yang  terbaik.  Gambar  5  menampilkan  diagram  garis 
hasil tuning 1 hingga 20 topik beserta nilai coherence score 
untuk setiap topik. Didapatkan nilai coherence score tertinggi 
yaitu  0,5425  dimana  banyak  topik  diterapkan  dalam  LDA 
terhadap  data  telekonsultasi  klinis  sebanyak  12  topik.  Nilai 
tersebut berbeda tipis dengan model 15 topik yang memiliki 
coherence  score  0,5408.  Dengan  memanfaatkan  10  kata 
dengan peluang tertinggi yang membangun keduabelas topik, 
dapat diambil beberapa bahasan yang dimungkinkan dibahas 
pada setiap topik seperti ditampikan pada Tabel V. 

Gambar 5. Hasil tuning parameter LDA data telekonsultasi klinis 

TABEL V 
HASIL PEMODELAN TOPIK DATA TELEKONSULTASI KLINIS 

Topik 

Topik 1 

Topik 2 

Topik 3 

Topik 4 

Topik 5 

Topik 6 

Topik 7 

Topik 8 

Topik 9 

Topik 10 

Topik 11 

Topik 12 

Bahasan 

Pencernaan pada bayi-anak mencakup nafsu makan, minum, BAB, 
susu. 
Program hamil; Ukuran besar/kecil dari payudara atau dari penis 
serta bahayanya; Adanya benjolan pada payudara/penis berukuran 
besar/kecil serta bahayanya. 
Mencakup beberapa gejala umum masuk angin yang menyerang 
badan, dimungkinkan terjadi saat malam/bangun tidur atau 
berlangsung terus menerus. 
Perasaan sesak napas yang terjadi pada orang tua, atau yang 
dirasakan ketika akibat sedang takut, atau yang menyebabkan 
seseorang menjadi takut. 
Keluhan sakit/nyeri di bagian kanan/kiri dari kepala/tulang/perut 
yang bersifat kadang-kadang. 
Gatal/bintik pada kulit yang berwarna merah/putih, dimungkinkan 
juga terdapat cairan berwarna merah/putih 
Keluhan sakit sebelah kanan/atas/bawah gigi berlubang; Membahas 
mengenai berat badan (kg). 
Efek samping dari minum/mengonsumsi sesuatu, menyebabkan 
masalah asam lambung; Membahas mengenai telapak tangan yang 
mungkin mengindikasikan masalah asam lambung. 
Mata/kelopak mata/telinga/kaki/telapak kaki bengkak; Cacar air.; 
Operasi mata/kelopak mata/telinga/kaki/telapak kaki. 
Masalah menstruasi karena telat atau tentang darah yang keluar; 
Hubungan intim keluar darah; Membahas hamil secara umum, atau 
indikasi hamil karena menstruasi terlambat atau karena hubungan 
intim; Keluar darah saat berhubungan intim. 
KB dan produknya (pil, suntik); Jerawat di wajah. 
Bekas luka/operasi; Mual/muntah karena operasi atau hal lain; 
Terdapat luka di jari/kaki; Operasi/luka di jantung. 

Topik 10 adalah topik yang paling banyak dibahas yaitu 
mengenai masalah menstruasi; hubungan intim keluar darah; 
dan  hamil.  Topik  ini  termasuk  topik  yang  tabu  bila 
dikonsultasikan  secara  langsung,  namun  dengan  dengan 

 5 / 8 

 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

paling  tinggi  merupakan  model  yang  terbaik.  Gambar  6 
menampilkan  diagram  garis  hasil  tuning  1  hingga  20  topik 
beserta  nilai  coherence  score  untuk  setiap  topik.  Nilai 
tertinggi yaitu 0,4485 yang dicapai ketika topik dimodelkan 
dengan  14  topik.  Nilai  tersebut  berbeda  sangat  tipis  dengan 
model  19  topik  yang  memiliki  coherence  score  0,4482. 
Dengan memanfaatkan 10 kata dengan peluang tertinggi yang 
membangun  kesembilanbelas  topik,  dapat  diambil  beberapa 
bahasan yang dimungkinkan dibahas pada setiap topik seperti 
ditampikan pada VI.  

TABEL VI 
HASIL PEMODELAN TOPIK DATA JUDUL ARTIKEL 

Bahasan 

Cara/ciri/penyebab/obat anemia atau penyakit kelamin. 
Mencegah/dampak merokok; Cara berhenti merokok; Cara menjaga 
daya tahan anak. 
Efek samping patah tulang/merokok/rokok elektrik pada tubuh atau 
pada perempuan secara khusus 
Manfaat/tips/fakta menarik olahraga (umum) atau khusus bagi anak; 
Manfaat/tips/fakta menarik dari alat kontrasepsi/(bagi) kelamin 
Penyebab atau cara hamil/buang air/mengatasi rasa gatal di vagina atau 
di gigi. 
Penyebab/cara mengatasi berdarah ketika hubungan seksual; Cara jaga 
kesehatan anak. 
Penyebab sakit kepala/kulit kering; Bahan alami yang baik untuk 
paru/vagina/kulit kering. 
Fakta/mitos/dampak buruk dari infeksi jamur/kanker usus secara umum 
atau secara khusus untuk orang tua. 
Mengenali penyebab/gejala/pengobatan nyeri menstruasi atau nyeri 
tulang secara umum atau secara khusus bagi anak; Pengobatan vape. 
Penyakit menular akibat hubungan seks; Stroke; Penyakit dan 
Kesehatan anak. 
Infeksi bakteri/vagina/luka bakar; Cara efektif mengatasi 
bau/infeksi/luka bakar. 
Hubungan intim dan cara menghindarinya; KB, minum pil KB, cara 
menghindarinya; Makanan; Turun badan. 
Tanda penyakit jantung/virus corona secara umum atau pada yang 
berumur masih muda.; Vitamin. 
Bahaya/tips/cara sehat minum atau pakai obat secara umum atau 
khusus bagi anak. 

Topik 
Topik 1 

Topik 2 

Topik 3 

Topik 4 

Topik 5 

Topik 6 

Topik 7 

Topik 8 

Topik 9 

Topik 10 

Topik 11 

Topik 12 

Topik 13 

Topik 14 

Dalam  periode  data  penelitian, artikel  dalam  Topik 14 
paling  sering  muncul  yaitu  mengenai  bahaya/tips/cara  sehat 
minum  atau  pakai  obat.  Topik  yang  paling  sedikit  muncul 
adalah topik 12 tentang Hubungan intim, KB, dan Makanan. 

telemedicine  topik  ini  yang  paling  banyak  dikonsultasikan. 
Sedangkan  topik  yang  paling  sedikit  dibahas  yaitu  topik  12 
mengenai bekas luka/operasi; mual/muntah; Terdapat luka di 
jari/kaki; operasi; dan luka. 

Gambar 6. Diagram Garis Pemodelan Topik Data Telekonsultasi Klinis 

Untuk  melihat  perkembangan 

tren  kesehatan 
berdasarkan  topik  untuk  tiap  waktunya,  disajikan  diagram 
garis pada Gambar 3. Terlihat bahwa selain topik 3, 4, dan 10, 
kemunculan  topik  cenderung  stabil  (tidak  menunjukkan 
perubahan  gerakan  yang  jelas  berbeda  dengan  topik  yang 
lain).  Topik  10  muncul  terbanyak  pada  awal  periode  dan 
terlihat  sangat  berbeda  dengan topik  lainnya, namun  seiring 
waktu  semakin  menurun  dan  menjadi  stabil  seperti  topik 
lainnya.  Topik  3 dan 4  memiliki  perubahan yang  jelas  pada 
saat yang bersamaan yaitu pada Bulan Maret 2020. Topik 3 
membahas  mengenai  sesak  napas,  sedangkan 
topik  4 
membahas tentang keluhan sakit/nyeri. Kedua topik ini sedikit 
berkaitan  dengan  gejala  COVID-19  yang  mana  muncul  di 
Indonesia sejak Bulan Maret 2020. Berdasarkan edaran yang 
diterbitkan  oleh  Kementerian  kesehatan  bersama  dengan 
GERMAS  (Gerakan  Masyarakat  Hidup  Sehat),  gejala  klinis 
dari COVID-19 yaitu demam, batuk, pilek, gangguan (sesak) 
pernapasan,  sakit  tenggorokan,  letih,  dan  lesu.  Perubahan 
yang signifikan pada topik 3 dan 4 tersebut mengindikasikan 
telemedicine menunjukkan keadaan yang sebenernya dari tren 
kesehatan di Indonesia.  
2.  Pemodelan Topik Data Judul Artikel 

Gambar 7. Hasil tuning parameter LDA data judul artikel 

Data selanjutnya yang dianalisis yaitu data judul artikel. 
Sebelum memodelkan topik, terlebih dahulu perlu dilakukan 
tuning parameter. Sama dengan data sebelumnya, parameter 
yang di-tuning yaitu banyaknya topik sedangkan penanda baik 
buruknya model yaitu coherence score. Nilai coherence score 

Gambar 4. Diagram Garis Pemodelan Topik Data Artikel 

Untuk melihat tren topik setiap bulannya, dapat dilihat 
pada  Gambar  4.  Terlihat  garisnya  saling  berjajaran 
menunjukkan  bahwa,  tiap  waktunya  semua  topik  cenderung 
muncul  dengan  jumlah yang  hampir  sama.  Namun,  terdapat 
satu garis yang memiliki gerakan berbeda pada pertengahan 
periode  waktu  yaitu  garis  topik  13  tentang  tanda  penyakit 
jantung/virus corona secara umum dan  membahas mengenai 
vitamin. Terdapat kenaikan yang tiba-tiba pada Bulan Maret - 

 6 / 8 

 
 
 
 
 
 
April  2020.  Topik  ini  jelas  berkaitan  dengan  fenomena 
kesehatan di Indonesia pada waktu itu yaitu awal munculnya 
COVID-19.  

C.  Potensi  Pemanfaatan  Big  Data 

telemedicine  dalam 

Pengumpulan Data Kesehatan di Indonesia 
1.  Potensi  Pemanfaatan  Big  Data  telemedicine  untuk 

Melengkapi Official statistics 
Official  Statisctics  adalah  statistik  yang  diproduksi 
berdasarkan  standar  kualitas  tinggi  secara  nasional  maupun 
internasional  dan  diproduksi/disediakan  oleh  lembaga  resmi 
statistik  atau  lembaga  pemerintahan  lain  atau  organisasi 
internasional  untuk  umum.  Menurut  UU  No.17  Tahun 1997 
Tentang  Statistik,  terdapat  tiga  jenis  statistik  yaitu  statistik 
dasar,  sektoral,  dan  khusus  [17].  Berkaitan  dengan  statistik 
Indonesia 
kesehatan,  penghasil  statistik  kesehatan  di 
bersumber dari pengumpulan data statistik dasar oleh BPS dan 
serta 
sektoral  oleh  Kementerian  Kesehatan 
(Kemenkes).  Terdapat  beberapa  survei/riset  yang  digunakan 
untuk mendapatkan statistik kesehatan. Dalam penelitian ini, 
peneliti hanya membahas beberapa survei/riset pengumpulan 
data  kesehatan  yang berkaitan dengan  data yang  didapatkan 
dari telemedicine. 

statistik 

Dari beberapa survei BPS, terdapat dua survei yang erat 
kaitannya  dengan  pengumpulan  data  kesehatan  yaitu  SDKI 
dan  Susenas.  Survei  Demografi  dan  Kesehatan  Indonesia 
(SDKI)  adalah  survei  yang 
tujuan  utamanya  untuk 
menyediakan estimasi terbaru indikator dasar demografi dan 
kesehatan  [18].  SDKI  diselenggarakan  dalam  periode  lima 
tahunan  (terakhir  2017).  Bila  dikaitkan  dengan  hasil 
pemodelan  topik,  terdapat  beberapa  topik  berkaitan  dengan 
bahasan  pada  SDKI  yaitu  KB,  reproduksi,  kontrasepsi,  dan 
kesehatan  anak.  Survei  Sosial  Ekonomi  Nasional  (Susenas) 
merupakan  survei  dengan  tujuan  utama:  tersedianya  data 
tentang  kesejahteraan  rumah  tangga  mencakup  antara  lain 
pendidikan,  kesehatan  dan  kemampuan  daya  beli  [19]. 
Susenas dilaksanakan pada setiap semester yaitu pada bulan 
Maret dan September. Terkait hasil pemodelan topik, bahasan 
pada Susenas ini turut mencakup kehamilan, KB, imunisasi, 
pemeliharaan kesehatan, serta makanan sehat. 

Terdapat  empat  survei/riset  yang diselenggarakan oleh 
Kemenkes terkait dengan data penelitian yaitu Riskesdas dan 
Sirkesnas  yang  berkaitan  dengan  kondisi  kesehatan  di 
masyarakat,  serta  Risnakes  dan  Rifaskes  yang  berkaitan 
dengan fasilitas dan sumber daya kesehatan. Riset Kesehatan 
Dasar (Riskesdas) adalah Riset yang diselenggarakan dengan 
tujuan  menyediakan  informasi  derajat  kesehatan  yang  telah 
dicapai  selama  kurun  waktu  5  tahun  terakhir  dan  informasi 
besaran  masalah  faktor  risiko  terkait  derajat  kesehatan yang 
diukur,  sebagai  bahan  pertimbangan  dalam  merumuskan 
kebijakan pembangunan kesehatan di Indonesia [20]. Survei 
Indikator Kesehatan Nasional (Sirkesnas) adalah survei yang 
bertujuan  untuk  mengetahui  capaian 
indikator  Renstra 
Kemenkes  tahun  2015-2019  yang  tidak  terdapat  dalam 
laporan pusat, pencatatan dan pelaporan rutin. Selain itu, juga 
untuk  mengetahui  capaian 
indikator  RPJMN  bidang 
kesehatan tahun 2015-2019, dimana keduanya dilihat secara 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

lingkup  nasional  [21].  Riset  Fasilitas  Kesehatan  (Rifaskes) 
adalah  riset  kesehatan  nasional  berbasis  fasilitas  yang 
bertujuan  untuk  memperoleh  rekomendasi  untuk  perbaikan 
pelaksanaan  Jaminan  Kesehatan  Nasional 
[22].  Riset 
Ketenagaan di Bidang Kesehatan (Risnakes) adalah riset oleh 
Badan  Litbangkes  yang  bertujuan  memperoleh  gambaran 
ketenagaan  di  bidang  kesehatan  di  fasilitas  pelayanan 
kesehatan [23]. 

Bila  dibandingkan, 

survei/riset  oleh  Kemenkes 
membahas lebih detail dan secara khusus tentang kesehatan. 
Berbeda  dengan  BPS  membahas  bersamaan  dengan 
demografi atau ekonomi. Untuk Riskesdas dan Sirkesnas, arah 
penggunaan  hasil 
sama  dengan 
pengumpulan data BPS. Namun untuk Rifaskes dan Risnakes 
penggunaannya  lebih  mengarah  pada  pendataan  terperinci 
sumber daya pelayanan kesehatan di Indonesia 

survei/riset  hampir 

Survei/riset  yang  diadakan  oleh  BPS/Kemenkes 
hasilnya  sama-sama  dimanfaatkan  oleh  pemerintah  atau 
instansi khusus yang ingin membuat kebijakan dan pengelola 
program kependudukan dan kesehatan termasuk didalamnya 
perencanaan, monitoring, dan evaluasi kebijakan pemerintah. 
Potensi 
dalam  mendukung 
telemedicine 
penerapan 
pemanfaatan tersebut yaitu untuk: 

•  Menggambarkan  kondisi  kesehatan  saat  ini  dan  yang 

lalu. 

•  Merencanakan pembangunan layanan kesehatan  
•  Mengetahui pola kesehatan dalam suatu periode waktu.  
•  Menentukan  kebijakan  atas  pengelolaan  program 
layanan  kesehatan  seperti  ketersediaan  layanan  dan 
ketersediaan obat atau alat/produk penunjang kesehatan 
lainnya. 

•  Memperbaharui  tren  kesehatan  terkini  pada  periode 

waktu yang lebih singkat 

ini 

Hal 

•  Mengetahui  fenomena  kesehatan  secara  luas,  di  dalam 
dan luar ruang lingkup survei/riset yang sudah ada.  
•  Mengidentifikasi kejadian langka maupun kejadian baru 
yang belum ada dalam survei/riset yang sudah ada. 
•  Menjadi sarana publikasi dan direktori yang terpercaya. 
telemedicine 
tidak  hanya  berlaku  untuk 
perusahaan kesehatan saja namun juga  telemedicine dimiliki 
lembaga/instansi  kesehatan  pemerintah.  Namun  yang  perlu 
dikembangkan  disini  adalah  fitur-fitur  yang  dimiliki  oleh 
telemedicine  yang  bila  disesuaikan  dengan  penelitian,  fitur 
utama  yang  berpotensi  besar  melengkapi  statistik  kesehatan 
yang sudah ada adalah fitur telekonsultasi klinis. Oleh karena 
itu,  adanya  upaya  pembangunan  ataupun  pengembangan 
telemedicine yang sudah ada dapat mendukung terwujudnya 
pemanfaatan data dari telemedicine.  

Selain dari fitur telekonsultasi klinis, data sumber daya 
layanan  kesehatan  juga  dapat  dimanfaatkan.  Untuk  fasilitas 
yang sifatnya milik negara pasti sudah tidak perlu diragukan 
lagi kelengkapan datanya, namun untuk fasilitas milik swasta 
terdapat  kemungkinan  bahwa  datanya  belum  tercatat  oleh 
pemerintah  karena  baru  atau  belum  terdaftar  atau  bersifat 
ilegal.  Apabila  fasilitas  tersebut  terdaftar  di  telemedicine, 
kejadian ini dapat ditindak lanjut oleh pihak yang berwenang 
agar tidak menimbulkan permasalahan dikemudian hari. 

 7 / 8 

 
 
 
2.  Validasi Data Telemedicine 

dokter 

Sebagai bentuk validasi data dari telemedicine terhadap 
data yang sudah ada, data yang bersumber dari  telemedicine 
perlu dibandingkan dengan statistik resmi (official statistics) 
kesehatan.  Dalam  penelitian  ini,  bentuk  validasi  yang 
dilakukan  yaitu  melakukan  matching  data  dokter  dari 
telemedicine  terhadap  data  dokter  pada  Ikatan  Dokter 
Indonesia  (IDI)  serta  melakukan  matching  data  rumah sakit 
dari telemedicine dengan data milik Kementerian Kesehatan. 
Pertama untuk data dokter, matching dilakukan dengan 
memanfaatkan  fitur  “Direktori  Anggota”  IDI  berwujud 
IDI 
pencarian 
pada 
(http://www.idionline.org/about/direktori-anggota/). 
Dari 
laman  Alodokter  dan  Dokter.id,  terkumpul  data  dokter 
sebanyak  12.859.  Dari  data  tersebut  hanya  96,85%  atau 
sebanyak  12.454  dokter  saja  yang  memiliki  alamat  atau 
domisili yang valid dan bisa dilakukan matching data dengan 
data keanggotaan IDI. Hasil menunjukkan bahwa 53,43% atau 
sebanyak 6.654 dokter telemedicine yang terdaftar (tervalidasi) 
pada  keanggotaan  IDI.  Hal  ini  cukup  banyak  mengingat 
jumlahnya  yang  lebih  dari  separuh  dari  total  dokter  yang 
terdaftar pada telemedicine. Bila diperinci untuk setiap laman, 
Alodokter memiliki lebih banyak dokter terdaftar (tervalidasi) 
dibandingkan dengan Dokter.id. 

laman  web 

resmi 

Validasi  kedua  yaitu  validasi  data  rumah  sakit  pada 
telemedicine  dengan  data  rumah  sakit  terakreditasi  milik 
Kemenkes 
(http://sirs.kemkes.go.id/fo/home/akreditasi).  
Namun,  data  milik  Kemenkes  ini  hanya  mengandung  data 
rumah  sakit  saja.  Hal 
ini  berbeda  dengan  data  dari 
telemedicine yang mana tidak seluruhnya merupakan rumah 
sakit karena ada beberapa jenis fasilitas kesehatan yang serupa 
dengan rumah sakit namun ikut terdaftar sebagai rumah sakit, 
contohnya  seperti  klinik,  rumah  bersalin,  dan  sebagainya. 
Dari  2.122  data  rumah  sakit,  hanya  66,21%  atau  sebanyak 
1.405  data  saja  yang  benar  merupakan  rumah  sakit.  Data 
kemudian diseleksi berdasarkan valid tidaknya alamat rumah 
sakit.  Didapatkan  hasil  bahwa  banyaknya  rumah  sakit  yang 
memiliki  alamat  valid  sejumlah  1.390  rumah  sakit.  Jumlah 
tersebut cukup banyak karena jika dilihat persentase dari total 
rumah 
sakit  yang  akan  dilakukan  matching  data, 
persentasenya  mencapai  98,93%.  Hasil  validasi  data  rumah 
sakit menubjukkan bahwa, hanya 38,49% atau sebanyak 535 
rumah sakit saja yang terdaftar (tervalidasi) keberaannya pada 
data  Kemenkes.  Bila  dilihat  lebih  detail  lagi  berdasarkan 
laman  telemedicine,  urutan  proporsi  banyaknya  rumah  sakit 
tervalidasi mulai dari yang terbesar yaitu Dokter.id, Alodokter, 
kemudian Honestdocs. 

VII. 

PENUTUP 

Berdasarkan penelitian yang telah dilakukan, peneliti menarik 

beberapa kesimpulan sebagai berikut : 
•  Berdasarkan  analisis  yang  dilakukan  terhadap  fitur-fiturnya, 
masyarakat  sudah  banyak  memanfaatkan  telemedicine  dan 
fenomena  COVID-19  tertangkap  data  telemedicine  melalui 
fitur telekonsultasi klinis, artikel, serta obat dan penyakit. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

•  Topik  kesehatan  dapat  dimodelkan    menjadi  12  topik  untuk 
data telekonsultasi klinis dan 14 topik untuk data artikel, serta 
menampilkan tren dari topik kesehatan tersebut. 

dalam 

•  Data 

berpotensi 

telemedicine 

dimanfaatkan 
mendukung official statistics kesehatan di Indonesia. 
Besar  harapan  peneliti  akan  adanya  pengembangan  atas 
penelitian  ini.  Arah  pengembangan  tersebut  dapat  berupa 
pengembangan  ruang  lingkup  penelitian  seperti  menggunakan 
telemedicine  lainnya,  atau  dapat  juga  fitur  lain.  Selain  itu dapat 
juga  dilakukan  pengembangan  dengan  menambahkan  analisis 
tambahan  baik  untuk  data  atau  fitur  yang  sama  maupun  yang 
berbeda  seperti  analisis  isi  artikel,  analisis  sentimen  data 
pertanyaan,  atau  penggunaan  kecerdasan  buatan  untuk  deteksi 
penyakit. 

DAFTAR PUSTAKA 
[1]  A. B. Setiawan, “Revolusi Bisnis Berbasis Platform Sebagai Penggerak,” 
in Jurnal Masyarakat Telematika dan Informasi, vol. 9, pp. 61-76, 2018. 
[2]  S. Palinggi and E. C. Limbongan, “Pengaruh Internet Terhadap Industri 
E-Commerce  Dan  Regulasi  Perlindungan  Data  Pribadi  Pelanggan  di 
Indonesia,”  in  Seminar  Nasional  Riset  dan  Teknologi  (SEMNAS 
RISTEK) 2020. 27 Januari 2020.. 

[3]  United Nations, Global Information Society Watch. APC and iTem, 2017.  
[4]  Menteri Kesehatan RI, Peraturan Menteri Kesehatan Republik Indonesia 

Nomor 46 Tahun 2017 tentang Strategi E-Kesehatan Nasional  

[5]  Menteri Kesehatan RI, Peraturan Menteri Kesehatan Republik Indonesia 
Nomor 20 Tahun 2019 tentang penyelenggaraan pelayanan telemedicine 
antar fasilitas pelayanan kesehatan, 

[6]  D. Soemitro,  “Tantangan e-kesehatan di Indonesia,” in Buletin Jendela 

Data dan Informasi Kesehatan, 2016, pp. 1-21. 

[7]  E.  Kristianto,  “Telemedicine  Di  Indonesia,”  in  Jurnal  Teknik  dan  Ilmu 
Komputer Universitas Kristen Krida Wacana – Jakarta, vol. 02, no. 06, 
pp. 167-171, April - Juni 2013. 

[8]  I.  Widiyastuti,  “Analisa  Aplikasi  Telemedicine  Berbasis  Website  di 
Instansi Kesehatan Pemerintah dan Swasta serta Potensi Implementasinya 
di  Indonesia,”  in  Jurnal  Penelitian  Ilmu  Pengetahuan  dan  Teknologi 
Komunikasi, vol.10, no.2, Desember 2008. 

[9]  Santoso et al, “Perkembangan dan Masa Depan Telemedika di Indonesia,” 

in CITEE 2015, 15 September 2015. 

[10] S. Ariyanti and Kautsarina, “Kajian Tekno-Ekonomi pada Telehealth di 
Indonesia,” in Buletin Pos dan Telekomunikasi, vol. 15, no.1, pp. 43, 2017. 
[11] Pemerintah  Indonesia,  Peraturan  Pemerintah  Nomor  21  Tahun  2020 
tentang  Pembatasan  Sosial  Berskala  Besar  dalam  Rangka  Percepatan 
Penanganan Corona Virus Disease 2019 (COVID-19). 31 Maret 2020. 
[12] Salahudin,  Mengenal  Big  Data  (BD)  dan  Big  Data  Analysis  (BDA). 
Available  from:  https://www.researchgate.net/publication/332108784_ 
Mengenal_Big_Data_BD_dan_Big_Data_Analysis_BDA. 

[13] T. Islam, “Yoga-Veganism: Correlation Mining of Twitter Health Data,” 
in Conference: 8th KDD Workshop on Issues of Sentiment Discovery and 
Opinion Mining (WISDOM) 2019. Alaska, USA 

[14] R.  Feldman  and  J.  Sanger,  The  Text  Mining  Handbook:  Advanced 
Approaches  in  Analyzing  Unstructured  Data.  New  York:  Cambridge 
University Press, 2007. 

[15] D. M. Blei, “Latent Dirichlet Allocation,” in Journal of Machine Learning 

Research 3 (2003) 993-1022 

[16] E.  Verhoeven,  (2019,  7)  Fuzzy  matching  entities  in  a  custom  entity 
https://medium.com/tailo-ai/fuzzy-matching-

dictionary.  Available: 
entities-in-a-custom-entity-dictionary-310158d2b60e 

[17] Pemerintah Indonesia, UU No.16 Tahun 1997. 19 Mei 1997. 
[18] Badan Pusat Statistik, Laporan SDKI 2017. 2018. 
[19] Badan Pusat Statistik, Ringkasan Metadata Statistik Dasar 2019.  
[20] Kementerian  Kesehatan  RI,  Laporan  Nasional  Riskesdas  2018.  Badan 

Penelitian dan pengembangan kesehatan, 2016. 

[21] Kementerian Kesehatan RI, Laporan Survei Indikator Kesehatan Nasional 
(Sirkesnas) 2016. Badan Penelitian dan pengembangan kesehatan, 2019. 
[22] Kementerian  Kesehatan  RI,  Laporan  Riset  Fasilitas  Kesehatan  2019. 

Badan Penelitian dan pengembangan kesehatan, 2019. 

[23] Kementerian kesehatan RI Laporan Risnakes 2017. Badan Penelitian dan 

pengembangan kesehatan, 2019. 

 8 / 8 

 
 
"
221709694,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Ekstraksi Informasi Gangguan dan Lokasi Pada 
IndiHome Menggunakan Named Entity Recognition 
(NER) 

Fauziah (221709694, 4SD2) 
Dosen Pembimbing: Ibnu Santoso, SST, MT 

Ringkasan— Perkembangan media sosial memudahkan 
pengguna  untuk  berkomunikasi  dan  mendapatkan  informasi. 
Twitter  merupakan  media  sosial  yang  popular  di  Indonesia. 
Tweet  yang  dikirim  pengguna  Twitter  dapat  dijadikan  sebagai 
sumber  informasi.  Suatu  rangkaian  proses  diperlukan  untuk 
mengekstrak  informasi  dari  tweet.  Pada  penelitian  ini  akan 
dilakukan  ekstraksi  informasi  dari  tweet  yang  ditujukan  untuk 
IndiHome  pada  Januari  2020  hingga  Desember  2020  dengan 
mengklasifikasi  entitas  pada  tweet  dalam  kelas-kelas  tertentu 
menggunakan  Named  Entity  Recognition  (NER)  pada  spaCy. 
Informasi  yang  akan  diekstrak  adalah  gangguan  yang  terjadi 
pada  IndiHome  dan 
lokasi  terjadinya  gangguan  tersebut. 
Ekstraksi  tersebut  dilakukan  untuk  memperoleh  informasi 
terkait  jenis  gangguan  apa  saja  yang  terjadi  pada  layanan 
IndiHome  serta  wilayah  mana  saja  yang  mengalami  gangguan 
layanan IndiHome. Model dibangun menggunakan data berlabel 
yang  di-split  menjadi  train  dan  test  data.  Model  tersebut 
menghasilkan  akurasi  sebesar  86,15%.  Gangguan  IndiHome 
paling banyak terjadi di Pulau Jawa, tepatnya Kota Bekasi. 

Kata Kunci— Twitter, Named Entity Recognition (NER), SpaCy, 

Gangguan  IndiHome. 

I.  LATAR BELAKANG 

Indonesia  Digital  Home 

Internet  Service  Provider  (ISP)  merupakan  produsen  atau 
lembaga yang memberikan pelayanan kepada konsumen agar 
bisa  mengakses  internet  dan  berbagai  media  online.  Internet 
Service  Provider  (ISP)  mempunyai  jaringan  baik  secara 
domestik  maupun  internasional  sehingga  pengguna  dapat 
terhubung ke jaringan internet global [1]. Salah satu operator 
penyedia  ISP  yang  banyak  digunakan  oleh  masyarakat 
Indonesia  adalah 
(IndiHome). 
IndiHome  adalah  salah  satu  produk  layanan  komunikasi  dari 
PT  Telekomunikasi 
IndiHome 
Indonesia 
menyediakan  layanan  komunikasi  serta  data  seperti  Internet 
(Internet  on  Fiber  atau  High  Speed  Internet),  telepon  rumah 
(voice),  dan  televisi  interaktif  (IndiHome  TV).  Sebagai  suatu 
perusahaan IndiHome harus terus melakukan evaluasi terhadap 
produk dan jasanya untuk dapat terus meningkatkan performa 
atau  kinerja  dari  layanan  tersebut.  Covid-19  yang  saat  ini 
sedang  melanda  dunia,  mengharuskan  setiap  orang  untuk 
mengurangi aktivitas diluar ruangan sehingga aktivitas banyak 
dilakukan  secara  online  seperti  bekerja,  sekolah,  maupun 
kuliah. Aktivitas online yang meningkat tentunya menjadikan 
kebutuhan internet lebih besar. Apabila terjadi gangguan pada 
jaringan internet maka aktivitas online yang sedang dilakukan 
setiap orang akan ikut terganggu.  

(Telkom). 

Seiring dengan berkembangnya internet, semakin besar pula 
penggunaan  media  sosial  untuk  berkomunikasi  dan  mencari 
informasi. Salah satu media sosial yang saat ini popular adalah 
Twitter.  Twitter  adalah  media  sosial  online  yang  digunakan 
untuk  mencari  informasi  tentang  bisnis,  hiburan,  ekonomi, 
politik,  dan  lainnya  [27].  Twitter  memungkinkan  pengguna 
untuk berbagi ide dan informasi langsung melalui pesan yang 
disebut tweet. Perbedaan Twitter dengan media sosial lainnya 
adalah  Twitter  memungkinkan  tiap  pengguna  melacak  tweet 
pengguna lain tanpa persetujuan dari pengguna  tersebut [12]. 
Berdasarkan hasil publikasi oleh statista.com pada 29 Oktober 
2020,  Indonesia  menempati  peringkat  ke-7  negara  dengan 
pengguna Twitter terbanyak didunia yakni 13,2 juta pengguna 
[13].  Masyarakat  sering  memberikan  kritik,  saran,  kepuasan 
mereka  atau  pertanyaan  terkait  layanan  IndiHome  melalui 
tweet  yang  mereka  publikasi  dapat 
Twitter.  Demikian 
digunakan sebagai sumber data untuk mendapatkan informasi 
atau  pengetahuan.  Sebagai  sumber  informasi,  popularitas 
Twitter  menyebabkan  munculnya  berbagai  penelitian 
contohnya ekstraksi informasi. Hal ini yang mendorong penulis 
agar memanfaatkan data Twitter untuk memperoleh informasi 
terkait  gangguan  yang 
IndiHome  dan 
mengidentifikasi wilayah yang terkena gangguan tersebut. 

terjadi  pada 

Pada  penelitian  ini,  peneliti  akan  menggunakan  salah  satu 
taks  NLP  yaitu  Information  Extraction  (IE).  NLP  (Natural 
Language Processing) merupakan teknik untuk mengekstraksi 
representasi makna yang lebih lengkap dari teks, yang biasanya 
menggunakan konsep linguistik seperti bagian dari ucapan dan 
konsep  struktur  gramatikal  [3].  NLP  dapat  diterapkan  atau 
diaplikasikan  dalam  beberapa  jenis  perkerjaan,  salah  satunya 
yaitu Information Extraction (IE) yang dapat mengambil suatu 
bagian  yang diinginkan dalam  sebuah  teks atau dokumen.  IE 
memiliki  salah  satu  sub-taks  yang  dapat  membantu  proses 
pengidentifikasian dan ekstraksi informasi yang berupa  entity 
atau  biasa  disebut  dengan  Named  Entity  Recognition  (NER). 
Proses  NER  membantu  user  untuk  menghasilkan  corpus  dan 
mengklasifikasikannya  ke  dalam  grup  seperti  nama  orang, 
organisasi,  lokasi  dan  sebagainya  [24].  Pada  penelitian  ini, 
NER  akan  dikembangkan  melalui  pendekatan  machine 
learning  dengan  menggunakan  metode  supervised  learning. 
Dimana  model  akan  belajar mengenali  entitas  dari  data  yang 
sudah  dilabeli.  Pembangunan  model  akan  dibantu  dengan 
library  spaCy.  Model  spaCy  sudah  tersedia  dalam  beberapa 
Bahasa  diantaranya  Bahasa  Inggris,  Bahasa  Jerman,  Bahasa 
Spanyol,  Bahasa  Portugis,  Bahasa  Prancis,  Bahasa  Italia  dan 

 1 / 8 

 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Bahasa  Belanda.  Sejauh  ini,  spaCy  belum  merilis  Bahasa 
Indonesia  secara  resmi.  Namun,  masih  tetap  bisa  dilakukan 
training  model  dengan  data  yang  dimiliki.  Data  berupa  teks 
yang  didapatkan  dari  hasil  scraping  akan  diekstrasi 
menggunakan  Named  Entity  Recognition  (NER)  kedalam 
bentuk entitas lokasi dan gangguan. 

II.  TUJUAN PENELITIAN 

Tujuan dari penelitian ini adalah sebagai berikut: 
1.  Melakukan pembangunan dan penggunaan model NER. 
2.  Melakukan  ekstraksi  entitas  gangguan  dan  lokasi  dari 
tweet  berbahasa  Indonesia  yang  ditujukan  kepada 
IndiHome. 

3.  Mengetahui  jenis  gangguan  dan  lokasi  terjadinya 

gangguan pada IndiHome. 

4.  Menvisualisasikan  hasil  gangguan  IndiHome  serta 

memetakan lokasi terjadinya gangguan tersebut. 

III. PENELITIAN TERKAIT 

Gambar 1. Peta literatur (literature map) 

Sistem pengenalan dan ekstraksi entitas bernama pada NER 
dapat dikategorikan ke dalam tiga kelas [32] yaitu rule-based 
NER,  Machine  Learning-based  NER  dan  hybrid  NER. 
Gambar  1  adalah  peta  literatur  yang  berisi  penelitian  terkait 
dengan NER. 

tahun 

Beberapa 

informasi  banyak 
terakhir  ekstraksi 
dilakukan  pada  penelitian.  Jurnal  penelitian  yang  berkaitan 
dengan  topik  skripsi  ini  dimuat  dalam  tabel  literatur.  Tabel  I 
menunjukkan tabel perbandingan literatur.

TABEL I 
TABEL LITERATUR 

No 
1 

Judul 
Pelacakan  Gangguan  Kereta 
Komuter  Melalui 
Twitter 
Crawling 

Penulis, Publikasi 
Lya Hulliyyatus Suadaa, 2015. 
Jurnal  Aplikasi  Statistika  dan 
Komputasi Statistik, 71- 82. 

2 

3 

4 

5 

6 

Bidirectional 
LSTM-CNNs 
Untuk  Ekstraksi  Entity  Lokasi 
Kebakaran  Pada  Berita  Online 
Berbahasa Indonesia 

Alif  Andika  Putra,  2020. 
Skripsi. 

Information  Extraction 
on 
Tourism  Domain  using  SpaCy 
and BERT 

Analisis Data Geospasial: Studi 
Kasus  Kecelakaan  Lalu  Lintas 
di Indonesia 

Chantana  Chantrapornchai, 
Aphisit Tunsakul, 2021. ECTI 
Transactions  on  Computer 
and  Information  Technology, 
108-122. 

Mayszea  Prawika  Firdausya, 
2019. Skripsi. 

Implementasi  Name  Entity 
Recognition  Untuk  Klasifikasi 
Berita Kriminal 

Muhammad 
Rahmatullah, 2019. Skripsi. 

Rifqi 

Named  Entity  Recognition  for 
Norwegian 

Johansen, 

Bjarte 
Proceedings  of 
Nordic 
Computational 
222-231. 

2019. 
the  22nd 
on 
Linguistics, 

Conference 

persebaran 

Tujuan 
Analisis  statistik  tweet  komuter  kereta, 
analisis dan lacak instrusi kereta komuter 
melalui  Twitter,  serta  mengembangkan 
sistem  prototipe  untuk  melacak  instrusi 
kereta komuter melalui twitter 
Melakukan  pengkajian  pembangunan 
dan  penggunaan  model  NER  dengan 
metode.  Melakukan  ekstraksi  entity 
lokasi  dan  waktu  pada  berita  online 
berbahasa  Indonesia  untuk  melihat  dan 
memetakan 
kejadian 
kebakaran berdasarkan waktu dan lokasi 
di provinsi DKI Jakarta 
Membangun  model  untuk  mengekstrak 
entitas  yang  diinginkan,  yaitu,  nama, 
lokasi,  atau  fasilitas  serta  jenis  relasi, 
mengklasifikasikan 
atau 
penggunaan klasifikasi untuk meringkas 
ulasan. 
Mengekstrak  data  geospasial  tentang 
kecelakaan 
lintas  di  Indonesia, 
menganalisis  data  geospasial  tentang 
lintas  di  Indonesia, 
kecelakaan 
data 
Menvisualisasikan 
geospasial tentang kecelakaan lalu lintas 
di Indonesia menggunakan GIS. 
Mengklasifikasikan  kasus  kriminalitas 
yang  didapatkan  dari  berita  online, 
menganalisis 
lokasi 
mengenai  kasus  kriminalitas  yang 
didapatkan  dari  berita  online  menurut 
wilayah administratif di Indonesia. 
Memodelkan  NER 
untuk  Bahasa 
Norwegia menggunakan korpus Nynorsk 
dan Bokmål. 

persebaran 

persebaran 

ulasan 

lalu 

lalu 

Metode 

Pendekatan rule-based. 

Name  Entity  Recognition  (NER) 
dengan pendekatan deep learning 
model 
network 
hybrid 
Bidirectional 
LSTM-CNNs 
(BLSTM-CNNs). 

Machine  learning-based  Name 
Entity  Recognition  (NER),  dan 
klasifikasi  teks  dengan  Spacy, 
BERT. 

Named 
(NER), Geocoding. 

Entity 

Recognition 

Named Entity Recognition (NER) 
dengan pendekatan rule based. 

Name Entity Recognition (NER). 

 2 / 8 

 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Mengekstrak  informasi  lokasi,  waktu, 
tanggal,  dan  gambar  berkaitan  dengan 
kemacetan lalu lintas di Bandung. 

Pendekatan  rule-based,  Named 
Entity 
(NER), 
Recognition 
Support Vector Machine 

informasi  antara 

Mendapatkan 
lain 
lokasi,  kondisi  lalu  lintas,  penyebab 
kemacetan,  kondisi  cuaca,  dan  waktu 
kejadian 

Pendekatan  rule-based,  Named 
Entity 
(NER), 
Recognition 
Support Vector Machine (SVM). 

Mengidentifikasi entitas lokasi tweet. 

Pendekatan  rule-based,  Named 
Entity Recognition (NER). 

Mengekstrak  entity  lokasi  pada  berita 
kriminal  online  pada 
surat  kabar 
berbahasa Indonesia. 

Untuk  mendapatkan  data  lalu  lintas, 
sehingga  informasi  lalu  lintas  tersebut 
dapat  disajikan  dalam  tampilan  peta 
sebagai aplikasi mobile Android. 

Memberikan  label  presisi tinggi ke  data 
yang tidak berlabel untuk secara otomatis 
mengekstrak data yang memiliki akurasi 
tinggi dan non-redundan, yang mengarah 
ke  pengklasifikasi  yang  jauh  lebih  baik 
pada iterasi berikutnya. 

algorithm 

contextual 

Rule-based 
yang 
digabungkan dengan morphology 
token, 
component 
SVM (Support Vector Machine). 
Pendekatan  rule-based,  Part  of 
Speech (POS) tagging 

Entity 

Recognition 
Named 
(NER),  Conditional  Random 
Fields (CRFs). 

Mengekstrak  dan  mengklasifikasikan 
informasi lalu lintas. 

SVM (Support Vector Machine). 

7 

8 

9 

10 

11 

Twitter  Information  Extraction 
for Smart City  
Case Studi: Traffic Congestion 
of Bandung 

Information  Extraction 
for 
Traffic  Congestion  in  Social 
Network. 

Ekstraksi  Nama  Lokasi  dari 
Tweet Informasi Lalu Lintas 

Ekstraksi Entity Lokasi Tindak 
Kriminal  pada  Surat  Kabar 
Online Berbahasa Indonesia 

Traffic  Condition  Information 
Extraction  &  Visualization 
from  Social  Media  Twitter  for 
Android Mobile Application 

12 

Simple 

A 
Semisupervised 
Algorithm  for  Named  Entity 
Recognition 

13 

Social-based 
Traffic 
Information  Extraction  and 
Classification 

Suhono 
Raidah  Hanifah, 
Harso 
Ayu 
Supangkat, 
Purwarianti,  2014.    ICT  For 
(ICISS), 
Society 
Smart 
International  Conference  on 
Bandung. 
M.  Riza  Alifi,  Suhono  Harso 
Supangkat,  2016.    ICT  For 
(ICISS), 
Society 
Smart 
International  Conference  on 
Surabaya. 
Yuda Munarko, 2015. Seminar 
Teknologi 
dan  Rekayasa 
(SENTRA), 235-239. 
Neno 
Sari 
Sulistiyawan, 
Widya  Sihwi,  Wiranto,  2016. 
Research Gate. 

Sri  Krisna  Endarnoto,  Sonny 
Satriyo 
Anto 
Pradipta, 
James  Purnama, 
Nugroho, 
International 
2011. 
Conference 
on  Electrical 
Engineering  and  Informatics 
on Bandung 
Sriharsha 
Wenhui 
Liao, 
Veeramachaneni, 
2009. 
Proceedings  of  the  NAACL 
HLT  Workshop  on  Semi-
for 
supervised 
Natural Language Processing, 
58–65. 
Wanichayapong, 
Napong 
Pruthipunyaskul, 
Wasawat 
Wasan 
Pattara-Atikom, 
Pimwadee  Chaovalit,  2011. 
11th  International  Conference 
on  ITS  Telecommunications, 
107-112. 

Learning 

IV. METODE PENELITIAN  

4.1 Ruang Lingkup Penelitian 

Data  yang  digunakan  yaitu  tweet  berbahasa  Indonesia 
tentang gangguan yang terjadi pada IndiHome. Ruang lingkup 
penelitian  ini  meliputi  pengambilan  tweet  melalui  scraping, 
pengolahan  data  tweet,  pelabelan  data,  pembangunan  model 
NER,  serta  mengidentifikasi  jenis  gangguan  dan  lokasi 
menggunakan  NER.  Semua  proses  dari  pengambilan  data 
hingga  pemodelan  NER  dilakukan  menggunakan  bahasa 
pemograman python.  

4.2 Metode Pengumpulan Data 

Data yang digunakan dalam penelitian ini merupakan data 
tweet  berbahasa  Indonesia  yang  bersumber  dari  pengambilan 
tweet  yang  dilakukan  dalam  periode  Januari  2020  sampai 
dengan Desember 2020 menggunakan metode  scraping. Data 
yang  diambil  adalah  tweet  yang  ditujukan  kepada  IndiHome. 
Scraping  untuk  pengambilan  data  ini  dilakukan  dengan 
menggunakan user interface berupa jupyter lab dengan bahasa 
pemograman  python.  Scraping  pada  bahasa  pemograman 
python  dapat  dilakukan  dengan  mengunakan  library  yang 
tersedia pada python. Library yang digunakan pada penelitian 
ini adalah twint. Twint adalah scraping tool dengan python yang 

memungkinkan  untuk  mengambil  Tweet  dari  profil  Twitter 
tanpa menggunakan API Twitter.  

4.3 Metode Analisis 

Pada penelitian ini dilakukan langkah-langkah analisis yang 
dimulai  dari  preprocessing,  filtering  data,  pelabelan  data, 
pemodelan NER menggunakan spaCy, pengujian model NER, 
sampai dengan hasil informasi berupa gangguan dan lokasi. 
A.  Preprocessing Data 

Proses  preprocessing  yang  dilakukan  pada  penelitian  ini 
adalah sebagai berikut: 
1.  Cleaning 

Data tweet yang telah diperoleh dilakukan pembersihan 
tanda baca  dan menghapus fitur khusus seperti  URL, 
hashtag, username, mention dan emotikon. 

2.  Case Folding 

Tahap ini adalah mengubah atau mengkonversi semua 
huruf dalam teks menjadi huruf kecil. 

B.  Filtering Data 

Pada  proses  filtering,  data  tweet  yang  telah  melalui  tahap 
filtering 
preprocessing  diseleksi  secara  manual.  Proses 
dilakukan  dengan 
tweet  yang 
diperlukan  oleh  peneliti  yaitu  tweet  yang  berkaitan  dengan 

tujuan  untuk  menyeleksi 

 3 / 8 

 
 
 
 
 
 
gangguan  terhadap  IndiHome.  Tweet  yang  tidak  membahas 
tentang gangguan IndiHome tidak digunakan untuk penelitian 
ini. 

C.  Pelabelan Data 

yang 

diakses 

Data  tweet  yang  telah  diseleksi  dilabeli  menggunakan 
SpaCy  NER  annotation  tool  yang  dipersembahkan  oleh  Tim 
Agate 
pada 
dapat 
agateteam.org/spaCynerannotate.  Label  G1,  G2,  G3,  G4,  dan 
G5 digunakan untuk mengklasifikasikan jenis gangguan yang 
terjadi  pada  IndiHome.  Pelabelan  untuk  lokasi  dilakukan 
dengan  menggunakan  notasi  BIO  (Begin,  Inside,  Other) 
sebagai  skema  pelabelan  yang  menandakan  urutan  yaitu  B-
LOC,  I-LOC.  Klasifikasi  label  gangguan  dapat  dilihat  pada 
tabel berikut. 

TABEL II 
Klasifikasi Gangguan Pada IndiHome 

Keterangan 

PON merah, LOS merah 
Internet atau jaringan lemot/lelet/ lambat 
Internet  atau  jaringan  mati,  lost  connection,  jaringan 
tidak  bisa,  jaringan  terputus,  jaringan  down,  trouble, 
error 
Perbaikan atau pemeliharan jaringan (maintenance) 
Gangguan massal 

Label 
G1 
G2 
G3 

G4 
G5 

Kemudian, data yang telah dilakukan pelabelan akan di split 
menjadi train data dan test data. Data tersebut akan digunakan 
untuk melakukan pemodelan NER. 

D.  Pemodelan dan Pengujian model NER 

Pemodelan  NER  dilakukan  dengan  menggunakan  library 
pada  python  yaitu  spaCy.  SpaCy  adalah 
library  yang 
memungkinkan  melakukan  tugas  terkait  Natural  Language 
tagging,  named  entity 
Processing  seperti  part-of-speech 
recognition dan dependency parsing. SpaCy belum merilis pre-
train model untuk bahasa Indonesia secara resmi. Metode yang 
akan dipakai merupakan metode yang sudah dibuat oleh spaCy 
pada  bahasa  lain.  Model  ini  akan  mengikuti metode  dan  alur 
algoritma yang sudah ada di library spaCy. Model NER yang 
digunakan spaCy adalah  Transition Based Parser. Transition 
Based Parser adalah pendekatan untuk prediksi terstruktur di 
mana  tugas  memprediksi  struktur  dipetakan  ke  serangkaian 
transisi keadaan. Transition Based Parser juga mengasumsikan 
bahwa  informasi  paling  menentukan  tentang  entitas  akan 
mendekati token awal. Jika entitas panjang dan ditandai dengan 
token  di  tengahnya,  komponen  tersebut  kemungkinan  akan 
tugas  dengan  buruk.  Elemen  kunci  dalam 
melakukan 
transition-based  parsing  adalah  gagasan  konfigurasi  yang 
terdiri dari tumpukan (stack), buffer input kata atau token dan 
satu set relasi yang mewakili pohon ketergantungan (depedency 
tree). Dalam kerangka transition-based parsing, fitur tersebut 
perlu  diekstraksi  dari  konfigurasi  yang  membentuk  data 
training.  Hasil  akhir  dari  pembuatan  model  ini  akan  berupa 
model  NER  yang  siap  digunakan  untuk  pemrosesan  bahasa 
alami  dengan  Bahasa  Indonesia.  Pengujian  model  dilakukan 
dengan  menggunakan  perhitungan  akurasi  menggunakan  test 
data. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pada tahap ini akan dilakukan analisis mengenai gangguan 
apa saja yang banyak dikeluhkan pada tweet dan lokasi mana 
saja yang sering terjadi gangguan IndiHome. Kemudian akan 
dilakukan  visualisasi  dari  jenis  gangguan  IndiHome  yang 
terjadi dan pemetaan lokasi terjadinya gangguan IndiHome. 

V.  KERANGKA PIKIR 

Penelitian ini menggunakan data sekunder berupa data tweet 
gangguan  pada  IndiHome  yang  diperoleh  dari  hasil  scraping 
menggunakan  twint.  Data  tweet  yang  diperoleh  kemudian 
dilakukan tahap preprocessing yaitu cleaning dan case folding 
yang digunakan untuk mengolah data sehingga dapat dilakukan 
tahap selanjutnya. Data yang telah melalui tahap preprocessing 
dilakukan  filtering  untuk  memilih  tweet  yang  membahas 
tentang  gangguan  pada  IndiHome  kemudian  akan  dilakukan 
telah  ditentukan  peneliti. 
labelling  dengan 
Selanjutnya data yang telah dilakukan labelling dibagi menjadi 
dua,  yaitu  training  data  dan  testing  data.  Data  tersebut  akan 
digunakan  untuk  melakukan  pembangunan  model  Named 
Entity  Recognition  (NER)  dengan  spaCy.    Model  NER  yang 
dibangun 
akan  digunakan  untuk  mengekstraksi  dan 
mengklasifikasikan  entitas  yang  terdapat  pada  tweet.  Hasil 
klasifikasi  akan  memberikan  informasi  berupa  gangguan  apa 
saja yang terjadi pada IndiHome dan lokasi gangguan. Gambar 
2 menunjukan diagram alur dari kerangka pikir penelitian. 

label  yang 

Gambar 2. Kerangka Pikir Penelitian 

VI. HASIL DAN PEMBAHASAN 

6.1 Hasil Scraping dan Prerocessing Data 

Pengambilan  data  tweet  pada  penelitian  ini  dilakukan 
menggunakan  metode  scraping  dalam  rentang  waktu  Januari 
2020  sampai  dengan  Desember  2020.  Scraping  dilakukan 
menggunakan  library  twint  pada  bahasa  pemograman  python 
menggunakan  kata  kunci  @IndiHome.  Data  hasil  scraping 
dapat dilihat pada gambar 3. 

E.  Ekstraksi Informasi 

Gambar 3. Data hasil scraping twitter 

 4 / 8 

 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Data tweet hasil scraping merupakan data yang masih kotor 
dan tidak dapat digunakan sebagai data training dan test untuk 
pembentukan model NER. Oleh karena itu, sebelum memasuki 
tahapan selanjutnya, data tersebut harus dibersihkan atau biasa 
disebut  dengan  preprocessing  data.  Preprocessing  data  yang 
dilakukan dimulai dari cleaning dan case folding. 

Model yang dibangun dari training data akan disimpan lalu di 
lihat akurasinya menggunakan testing data. 

6.5 Implementasi Model NER  

Setelah  proses  pembangunan  model  NER  dilakukan, 
peneliti melakukan perhitungan akurasi menggunakan test data. 
III 
Hasil  akurasi  dari  model  adalah  86,15%.  Tabel 
menunjukkan nilai F1 untuk tiap entitas. 

Gambar 4. Data tweet hasil prepocessing 

6.2 Filtering Data 

Data  tweet  hasil  preprocessing  masih  mengandung  data 
yang  tidak  sesuai  dengan  ketentuan  tentang  kriteria  yang 
digunakan pada penelitian ini. Terdapat data tweet yang tidak 
membahas  mengenai  gangguan  yang  terjadi  pada  IndiHome. 
Oleh  karena  itu,  data  tweet  yang  telah  didapatkan  dari  hasil 
scraping perlu dilakukan filtering, agar didapatkan data sesuai 
dengan kriteria yang telah ditetapkan. Filtering data dilakukan 
secara  menual  menggunakan  software  Microsoft  Excel  oleh 
peneliti. 

Berdasarkan  hasil  dari  proses  filtering,  data  tweet  terkait 
gangguan  IndiHome  didapatkan  sebanyak  107224  tweet. 
Kemudian  diambil  7500  tweet  untuk  dilabel  entitasnya  yang 
akan digunakan untuk membangun model. 

6.3 Pelabelan Data 

Pelabelan data diklasifikasikan menjadi tujuh kelas yaitu B-
LOC,  I-LOC,  G1,  G2,  G3,  G4,  dan  G5  masing-masing 
menandakan  entitas  lokasi  dan  entitas  gangguan.  Pelabelan 
lokasi dilakukan dengan menggunakan notasi BI (Begin, Inside) 
sebagai skema pelabelan yang menandakan urutan. 

Gambar 5. Data tweet hasil pelabelan 

Data tweet yang telah dilakukan pelabelan kemudian akan 
di split menjadi training dan test data. 7500 data tweet dibagi 
90% sebagai training data dan 10% sebagai testing data.  

6.4 Pembangunan Model NER 

Pembangunan  model  Named  Entity  Recognition  (NER) 
yang  digunakan  pada  penelitian  ini  menggunakan  library 
spaCy. Pembangunan model dimulai dengan membuat training 
model  dengan  data  yang  telah  dilabelkan  sesuai  format  pada 
spaCy,  selanjutnya  akan  di  train  dengan  beberapa  iterasi 
sebelum digunakan. 

Data yang akan digunakan pada model adalah  7500 tweet 
berlabel  yang  disimpan  dalam  format  pickle.  Data  dibagi 
menjadi 90% sebagai training data dan 10% sebagai test data. 

TABEL III 
Nilai F1 masing-masing entitas 

Entitas 
B-LOC 
I-LOC 
G1 
G2 
G3 
G4 
G5 

F1 Score 
86,614 
63,829 
84,905 
97,172 
82,016 
85,185 
92,592 

itu, 

Selain 

tweet 
menggunakan model NER. Hasil klasifikasi tweet dimuat pada 
gambar 6 dibawah ini. 

peneliti  melakukan 

klasifikasi 

Gambar 6. Hasil klasifikasi tweet menggunakan model NER spaCy 

6.6 Hasil dan Pemetaan Entitas Lokasi Gangguan  

A.  Hasil Gangguan IndiHome 

Banyaknya  jenis  gangguan  yang  sering  disebutkan  dalam 

tweet dapat dilihat pada Gambar 7 berikut. 

 Gambar 7. Jenis gangguan yang terjadi pada IndiHome tahun 2020 

 5 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Berdasarkan gambar tersebut, gangguan yang sering terjadi 
pada  IndiHome  adalah  label  G3  yaitu  internet  atau  jaringan 
mati, lost connection, jaringan tidak bisa tersambung, jaringan 
terputus,  jaringan  down,  jaringan  trouble  dan  jaringan  error 
sebesar 41,5%. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pada  penelitian  ini,  data  entitas  lokasi  akan  dilakukan 
visualisasi yang berupa pemetaan lokasi. Pemetaan lokasi titik 
gangguan  dilakukan  untuk  setiap  data  lokasi  berdasarkan 
koordinat longitude dan latitude yang telah didapatkan melalui 
proses  geocoding  sebelumya.  Visualisasi  data  dilakukan 
menggunakan Tableau. Visualisasi ini untuk melihat persebaran 
lokasi terjadinya gangguan IndiHome  serta melihat kabupaten 
atau  kota  mana yang  sering  terjadi  gangguan  IndiHome.  Data 
visualisasi disajikan berupa gambar peta dan grafik. 

Gambar 8. Gangguan IndiHome Periode Januari 2020 - Desember 2020 

Gambar 8 menunjukkan gangguan tiap bulannya pada tahun 
2020. Berdasarkan grafik tersebut gangguan yang paling sering 
terjadi  di  bulan  Januari,  Februari,  Juni,  Juli,  Agustus, 
September  dan  November  adalah  label  G3  sedangkan  bulan 
Maret, April, Mei, Oktober dan Desember adalah label G2 yaitu 
internet atau jaringan lelet/lemot/lambat. 

B.  Pemetaan Lokasi Gangguan 

Data  lokasi  gangguan  merupakan  data  alamat  titik  lokasi 
gangguan yang berupa teks. Untuk dapat melakukan pemetaan 
lokasi  diperlukan  informasi  lain  mengenai  koordinat  longitude 
dan  latitude  lokasi  pada  peta.  Pada  penelitian  ini,  untuk 
memperoleh  data  koordinat  longitude  dan  latitude,  peneliti 
melakukan genarate longitude dan latitude dengan menggunakan 
geocoding  pada  setiap  lokasi  gangguan  IndiHome.  Geocoding 
dilakukan  dengan  menggunakan  library  python  yaitu  geopy. 
Tabel IV memuat beberapa data alamat, koordinat longitude dan 
latitude hasil geocoding. 

TABEL IV 
Data hasil geocoding lokasi 

Alamat 

Latitude 

Longitude 

karawang timur 

-6,3570955 

107,3375794 

pondokgede, bekasi 

-6,28556695 

106,9180452 

cempaka putih 

-6,1812145 

106,8685477 

cipinang melayu jakarta timur 

-6,24947735 

106,911258 

depok 

-6,4071903 

106,8158348 

jatisampurna bekasi 

-6,3718141 

106,9144393 

bekasi selatan 

-6,25316025 

107,0076716 

cikunir 

cileungsi 

bekasi 

-7,3424825 

108,1537932 

-6,4054013 

106,9514418 

-6,2349858 

106,9945444 

Gambar 9. Pemetaan titik lokasi terjadinya gangguan IndiHome tahun 2020 

terjadinya  gangguan 

Pada  gambar  9  dapat  dilihat  bahwa  Pulau  Jawa  memiliki 
IndiHome  paling  banyak 
jumlah 
dibandingkan  pulau  lain.  Pada  gambar  10  dibawah 
ini 
menunjukkan  20  kabupaten  atau  kota  yang  paling  sering 
mengalami gangguan IndiHome.  

Gambar 10. Kabupaten/kota sering mengalami gangguan IndiHome tahun 
2020 

Berdasarkan grafik tersebut kabupaten atau kota yang paling 
banyak  terjadi  gangguan  IndiHome  adalah  Bekasi,  diikuti 
Jakarta Timur, Depok, Tangerang Selatan dan seterusnya. 

 6 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

[5]  A.  Willyawan,  “Named  Entity  Recognition  (NER)  Bahasa  Indonesia 
Menggunakan  Conditional  Random  Field  dan  Pos-Tagging,”  Skripsi, 
2018. 

[6]  B. Johansen, “Named-Entity Recognition for Norwegian,” Proceedings of 
the 22nd Nordic Conference on Computational Linguistics, 2019, 222-231. 
[7]  C.  Chantrapornchai,  A.  Tunsakul,  “Information  Extraction  on  Tourism 
Domain  using  SpaCy  and  BERT,”  Jurnal  ECTI  Transactions  on  CIT, 
vol.15, no.1, pp. 108-122, April 2021, doi: https://doi.org/10.37936/ecti-
cit.2021151.228621. 

[8]  D.  W.  Wulandari,  P.  P.  Adikara,  and  S.  Adinugroho,  “Named  Entity 
Recognition (NER) Pada Dokumen Biologi Menggunakan Rule Based dan 
Naïve Bayes Classifier,” Jurnal Pengembangan Teknologi Informasi dan 
Ilmu Komputer, vol.2, no.12, pp. 4555-4563, November 2018.  

[9]  E.  Utami,  S.  Hartati,  “Pendekatan  Metode  Rule  Based  Dalam 
Mengalihbahasakan  Teks  Bahasa  Inggris  Ke  Teks  Bahasa  Indonesia,” 
Jurnal Informatika, vol.8, no.1, pp. 42-53, Mei 2007. 

[10] H.  Permana,  K.  K.  Purnamasari,  “Named  Entity  Recognition 
Menggunakan  Metode  Bidirectional  LSTM-Crf  Pada  Teks  Bahasa 
Indonesia,” 2019. 

[11] I.  Irfana,  “Pengenalan  Entitas  Bernama  Pada  Artikel  Berita  Bahasa 
Indonesia  Menggunakan  Metode  Berbasis  Aturan,”  UGM,  Yogyakarta, 
2015. 

[12] I.  P.  Windasari,  F.  N.  Uzzi,  and  K.  I.  Satoto,  “Sentiment  Analysis  on 
Twitter Posts: An analysis of Positive or Negative Opinion on GoJek,” Int. 
Conf. Inf. Tech, Comput. Electr. Eng., pp. 266–269, 2017. 

[13] J.Clement.  (2020,  10).  Countries  with  the  most  Twitter  users  2020. 
[Online].  Available:  https://www.statista.com/statistics/242606/number-
of-active-twitter-users-in-selected-countries/.  

[14] K. K. Purnamasari dan I. S. Suwardi, “Rule-based Part of Speech Tagger 
for Indonesian Language,” IOP Conference Series: Materials Science and 
Engineering, no. 1, pp. 1-4, 2018. 

[15] K.  Xu,  Z.  Zhou,  T.  Hao  dan  W.  Liu,  “A  Bidirectional  LSTM  and 
to  Medical  Named  Entity 

Conditional  Random  Fields  Approach 
Recognition,” 2018. 

[16]  L.  H.  Suadaa,  “Pelacakan  Gangguan  Kereta  Komuter  Melalui  Twitter 
Crawling,” Jurnal Aplikasi Statistika dan Komputasi Statistik, 71-82, 2015. 
[17] M.  Fachri,  “Pengenalan  Entitas  Bernama  Pada  Teks  Bahasa  Indonesia 
Menggunakan  Hidden  Markov  Model,”  Universitas  Gadjah  Mada, 
Yogyakarta, 2014. 

[18] M.  I.  Tiarasani,  “Pengenalan  Entitas  Bernama  Pada  Artikel  Berita 
Berbahasa Indonesia Menggunakan Metode Hidden Markov Model Dan 
Rule Based,” UGM, Yogyakarta, 2018. 

[19] M. Maimaiti, A. Wumaier, K. Abiderexiti dan T. Yibulayin, “Bidirectional 
Long  Short-Term  Memory  Network  with  a  Conditional  Random  Field 
Layer for Uyghur Part-Of-Speech Tagging,” 2017. 

[20] M. P. Firdausya, “Analisis Data Geospasial: Studi Kasus Kecelakaan Lalu 

Lintas di Indonesia,” Skripsi, 2019. 

[21] M.  R.  Alifi,  S.  H.  Supangkat,  “Information  Extraction  for  Traffic 
Congestion  in  Social  Network,”  ICT  for  Smart  Society  (ICISS),  2016 
International Conference on, Surabaya, 2016, pp. 53-58. 

[22] M.  R.  Rahmatullah,  “Implementasi  Name  Entity  Recognition  Untuk 

Klasifikasi Berita Kriminal,” Skripsi, 2019. 

[23] N.  Jaariyah,  “Pengenalan  Entitas  Bernama  pada  Teks  Bahasa  Indonesia 
Menggunakan  Conditional  Random  Fields,”  Perpustakaan  UNIKOM, 
Bandung, 2017. 

[24] N.  Sulistiyawan,  S.W.  Sihwi,  Wiranto,  “Ekstraksi  Entity  Lokasi  Tindak 
Kriminal pada Surat Kabar Online Berbahasa Indonesia”, Research Gate, 
2016. 

[25] N.  Wanichayapong,  W.  Pruthipunyaskul,  W.  Pattara-Atikom  and  P. 
Chaovalit, ""Social-based traffic information extraction and classification,"" 
ITS Telecommunications (ITST), 2011 11th International Conference on, 
St. Petersburg, 2011, pp. 107-112. 

[26] R.  Hanifah,  S.  H.  Supangkat  and  A.  Purwarianti,  ""Twitter  Information 
Extraction  for  Smart  City,""  ICT  for  Smart  Society  (ICISS),  2014 
International Conference on, Bandung, 2014, pp. 295-299. 

[27] R.  Tineges,  A.  Triayudi,  I.  D.  Sholihati,  “Analisis  Sentimen  Terhadap 
Layanan  IndiHome  Berdasarkan  Twitter  Dengan  Metode  Klasifikasi 
Support Vector Machine (SVM),” Jurnal Media Informatika Budidarma, 
vol.4, no.3, pp. 650-658, Juli 2020. 

[28] Spacy.  (2021,  6).  Parser  dan  NER  Architectures.  [Online].  Available: 

https://spacy.io/api/architectures#TransitionBasedParser.  

 7 / 8 

Gambar 11. Persebaran titik lokasi terjadinya gangguan IndiHome di Bekasi 

Berdasarkan gambar 11 dapat dilihat  pemetaan titik lokasi 
gangguan  IndiHome  yang  terjadi  di  kota  Bekasi  pada  tahun 
2020. 

VII. 

PENUTUP 

Berdasarkan hasil dan pembahasan diatas dapat disimpulkan 

bahwa: 

1.  Telah  berhasil  dibangun  model  Name  Entity 
Recognition  (NER)  untuk  teks  berbahasa  Indonesia 
menggunakan  spaCy  dengan  hasil  akurasi  86,15%. 
Entitas  berupa  jenis  gangguan  dan  lokasi  pada  tweet 
telah  berhasil  diekstraksi  yang  masing-masing 
diklasifikasikan ke dalam kelas tertentu dengan label B-
LOC, I-LOC, G1, G2, G3, G4 dan G5. 

2.  Jenis gangguan yang sering muncul secara keseluruhan 
pada  tahun  2020  adalah  label  G3.  Berdasarkan  tiap 
bulannya  jenis  gangguan  yang  terjadi  pada  bulan 
Januari,  Februari,  Juni,  Juli,  Agustus,  September  dan 
November adalah label G3 yaitu internet atau jaringan 
mati,  lost  connection,  jaringan  tidak  bisa  tersambung, 
jaringan  terputus,  jaringan  down,  jaringan  trouble  dan 
jaringan  error  sedangkan  bulan  Maret,  April,  Mei, 
Oktober  dan  Desember  adalah  label  G2  yaitu  internet 
atau jaringan lelet/lemot/lambat. 

3.  Pulau  Jawa  adalah  lokasi  paling  banyak  terjadinya 
gangguan  IndiHome.  Kabupaten  atau  kota  di  Pulau 
Jawa  yang  paling  sering  mengalami  gangguan  adalah 
Bekasi. 

DAFTAR PUSTAKA 
[1]  Admin.  (2020,  5).  Apa  itu  ISP?  Internet  Service  Provider.  [Online].      

Available: https://lintas.net.id/id/internet-service-provider-isp/. 

[2]  A. A. Putra, “Bidirectional LSTM-CNNs Untuk Ekstraksi  Entity Lokasi 
Kebakaran Pada Berita Online Berbahasa Indonesia,” Skripsi, 2020. 
[3]  A.  Kao,  S.  R.  Poteet,  NLP  and  Text  Mining.  British  Library:  Springer, 

2007. 

[4]  A.  Setioaji, L. Muflikhah dan M.  A.  Fauzi,  “Named  Entity Recognition 
Menggunakan  Hidden  Markov  Model  dan  Algoritma  Viterbi  pada  Teks 
Tanaman  Obat,”  Jurnal  Pengembangan  Teknologi  Informasi  dan  Ilmu 
Komputer, vol. I, no. 12, pp. 1858-1864, 2017. 

 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

[29] S.  K.  Endarnoto,  S.  Pradipta,  A.  S.  Nugroho,  J.  Purnama,  “Traffic 
Condition  Information  Extraction  &  Visualization  from  Social  Media 
Twitter  for  Android  Mobile  Application”,  International  Conference  on 
Electrical Engineering and Informatics, 2011. 

[30] W.  Liao,  S.  Veeramachaneni,  “A  Simple  Semisupervised  Algorithm  for 
Named Entity Recognition”, Proceedings of the NAACL HLT Workshop 
on Semi-supervised Learning for Natural Language Processing, Colorado, 
2009, 58–65. 

[31] Y. Munarko, “Ekstraksi Nama Lokasi dari Tweets Informasi Lalu Lintas,” 

SENTRA, pp. 235-239, 2015. 

[32] Y. C. Wu, dkk, Extracting Named Entities Using Suport Vektor Machines. 

Verlag Berlin Heidelberg: Springer, 2006. 

[33] Z.  Huang,  W.  Xu  dan  K.  Yu,  “Bidirectional  LSTM-CRF  Models  for 

Sequence Tagging,” 2015. 

 8 / 8 

 
 
"
221709682,"Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik 

Penggunaan Homography Computer Vision dalam 
Image Preprocessing untuk Model Pengenalan 
Tulisan Tangan 
SUSENAS 2020 

Farabi Arsy Edodivano T. (221709682, 4SD2) 
Dosen Pembimbing: Dr. Drs. Waris Marsisno, M.Stat 

Ringkasan—  Dokumentasi  digital  menjadi  suatu  kebutuhan 
dalam  berbagai  keperluan  data  dan  informasi  yang  terkandung 
dalam  suatu  dokumen.  Dokumentasi  digital  dapat  dilakukan 
dengan  pengenalan  karakter  atau  tulisan.  Salah  satu  model 
pengenalan  karakter  atau  tulisan  yang  dapat  digunakan  adalah 
Convolutional Neural Networks (CNN). Model CNN memerlukan 
tahap  image  preprocessing  agar  dihasilkan  akurasi  yang  lebih 
baik. Tahap image preprocessing dapat dilakukan menggunakan 
metode, yang salah satunya adalah homography. Pada penelitian 
ini  digunakan  metode  homography  computer  vision dalam  tahap 
image  preprocessing  dengan  model  CNN.  Tujuan  penelitian  ini 
adalah untuk membandingkan hasil metode homography dengan 
metode  lainnya  pada  model  yang  sama;  dan  untuk  mengetahui 
sejauh  mana  hasil  akurasi  model  yang  pada  tahap  image 
preprocessing-nya menggunakan  metode homography. Penelitian 
dilakukan  dengan  langkah:  input  document  (scanned/images); 
image  preprocessing  (homography);  dan  perbandingan  akurasi, 
dengan menggunakan dataset EMNIST untuk training data. Hasil 
yang diperoleh pada penelitian ini yaitu, metode homography yang 
digunakan mampu mengatasi masalah perspektif gambar.Jumlah 
model  CNN  yang  telah  dibentuk  adalah  2,  akurasi  dari  validasi 
data  actual  model  khusus  pengenalan  huruf  adalah  73%  dan 
akurasi  model  pengenalan  angka  71%  dengan  menggunaka 
metode 
lainnya  atau  gabungan,  sedangkan  untuk  metode 
homographi akurasi jauh lebih rendah yaitu 55% untuk prediksi 
huruf dan 57% untuk prediksi angka. 

Kata  Kunci—  Homography,  Image  Preprocessing,  CNN, 

computer vision,  RANSAC 

I.  LATAR BELAKANG 

Badan  Pusat  Statistik  (BPS)  merupakan  Lembaga  Non 
Pemerintah  penyedia  kebutuhan  data  atau  di  bidang  kegiatan 
statistik  berdasarkan  peraturan  yang  berlaku.  Salah  satu 
kewenangan  BPS  adalah  Penetapan  dan  penyelenggaraan 
statistik  nasional.  Dalam  menjalankan  kewenangan  tersebut, 
BPS  melakukan  kegiatan  rutin  dalam  waktu  tertentu  seperti 
survey atau sensus.  

Pengumpulan  data  dari  kegiatan  survei  atau  sensus  yang 
dilakukan  BPS  Sebagian  besar  masih  menggunakan  PAPI 
(Paper  And  Pencil  Interviewing).  PAPI  adalah  Teknik 
tangan  dalam 
pencacahan  berbasis  kertas  dan 
pendataan.  Hasil  pencacahan  dengan  PAPI  akan  dimasukkan 
secara manual kedalam database. Kegiatan memasukkan data 
secara  manual  membutuhkan  tenaga  kerja,  biaya,  dan  waktu 
yang  besar.  Berdasarkan  Peraturan  Kepala  Badan  Pusat 
Statistik  Nomor  43  tahun  2017  tentang  Harga  Satuan  Pokok 

tulisan 

Kegiatan  Badan  Pusat  Statistik  Tahun  anggaran  2018,  biaya 
yang dibutuhkan untuk  memasukkan data berkisar Rp5.000  – 
Rp19.000 per dokumen. Hal tersebut menyebabkan biaya yang 
dibutuhkan  semakin  besar  seiring  bertambahnya 
jumlah 
dokumen, sehingga kegiatan memasukkan data secara manual 
masih tidak efisien, khususnya dari segi keuangan. 

Kegiatan  memasukkan  data  membutuhkan  kemampuan 
komunikasi  secara 
tertulis  yang  baik  dan  kemampuan 
mengoperasikan  perangkat  lunak  yang  digunakan.  Dalam 
pelaksanaanya  harus  disertai  dengan  konsentrasi  yang  tinggi, 
ketelitian,  dan  kesabaran  yang  besar.  Jika  ketiga  komponen 
tersebut 
akan  menimbulkan 
permasalahan,  seperti  kesalahan  penulisan  (tipografi).  Selain 
itu,  kecepatan  pengetikan  juga  berpengaruh  pada  lamanya 
waktu yang dibutuhkan untuk menyelesaikan proses entri data. 
Kegiatan  memasukkan  data  secara  manual  memiliki  banyak 
kelemahan sehingga tidak efisien. 

terpenuhi,  maka 

tidak 

OCR atau Optical Character Recognition merupakan salah 
satu  metode  pengenalan  teks  atau  objek.  Proses  OCR  yaitu 
mengonversikan  gambar  huruf  menjadi  karakter  ke  dalam 
bahasa  mesin  yang  dikenali  oleh  komputer.  Pengenalan  teks 
pada  OCR  terdiri  dari  pengenalan  karakter  cetak  baku  dan 
pengenalan karakter tulis tangan. 

tulisan 

Pengenalan 

tangan  merupakan  proses  yang 
kompleks.  Tulisan  tangan  memiliki  ketidakteraturan  pada 
setiap  karakter  tulisan  tangan,  tergantung  pada  gaya  tulisan 
tangan  oleh  masing-masing  individu.  Ketidakteraturan  ini 
memunculkan  variasi  yang  tinggi  sehingga  proses  klasifikasi 
karakter  akan  sangat  rumit.  Untungnya,  terdapat  metode 
pembelajaran otomatis telah dikembangkan dengan baik untuk 
mengakomodasi  variabilitas  ini  yaitu  neural  networks  [1]. 
Neural networks, awalnya dirancang menyerupai jaringan saraf 
neuron  otak  manusia,  mengekstraksi  fitur  dari  data  karakter, 
kemudian  menentukan  fitur  mana  yang  relevan  dengan 
klasifikasi sampai tingkat tertentu menggunakan data training. 
Pada  perkembangannya  terdapat  jenis  neural  networks  yang 
khusus  digunakan  untuk  mengolah  data  gambar  yaitu 
Convolutional Neural Networks (CNN). Banyak Sekali model 
pengenalan  yang  dilakukan  dengan  menggunakan  berbagai 
macam algoritmanya, salah satunya adalah CNN.  

Model  pengenalan  akan  dilakukan  untuk  memprediksi 
karakter  atau  isian  tulisan  tangan.  Form  processing  memiliki 
tahapan-tahapan untuk mendapatkan gambar hasil scan tulisan 
tangan. Form processing mempunyai metode tersendiri seperti 
perlakuan normalisasi, binarization, donoise, skew correction, 
dan  resize  ukuran  dokumen.  Hal  tersebut  menunjukkan  form 

 1 / 8 

 
 
 
 
 
 
processing  mempengaruhi  tingkat  akurasi  model  pengenalan 
yang dibangun.  

registration  dapat  meningkatkan  hasil  akhir  seiring  gambar 
yang digunakan bervariasi.  

Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik 

Di  zaman  yang  modern  ini  gambar  dokumen  tidak  hanya 
dapat diambil melalui scanner, telpon gengganm tiap individu 
sekarang  mempunya  kamera  yang  kualitasnya  berbeda-beda. 
Gambar  yang  diambil  pun  bisa  saja  dengan  perspektif  yang 
berbeda,  dari  atas  maupun  samping.  Tentunya  perbedaan 
perspektif gambar tersebut harus dilakukan preprocessing agar 
hasil  gambar  yang  digunakan  ke  dalam  model  pengenaan 
karakter dapat dibaca dengan baik.  

sama 

Salah  satu  metode  yang  dapat  digunakan  dari  masalah 
tersebut  yaitu  homography  matrix.  Pada  computer  vision,  2 
gambar  pada  bidang  yang 
terhubung  melalui 
homography . Matriks dari homography berukuran 3x3 dengan 
8  Degree  of  Freedom  (DoF)  [2].  Metode  ini  membutuhkan 
gambar  reference  sebagai  acuan,  dan  gambar  yang  ingin  di 
trasnformasikan  atau  di  proses.  2  gambar  tersebut  akan 
dicocokan (matches) dengan keypoint dari tiap gambar. Dengan 
begitu tentunya  tahapan normalisasi, skewness correction, dan 
resize sudah pasti dilakukan sehingga metode ini membuatnya 
lebih efektif untuk memproses gambar. 

II.  TUJUAN PENELITIAN 

Berdasarkan masalah yang didapatkan, tujuan penelitian ini 

adalah : 

1.  Menggunakan  homography  untuk  mengatasi  masalah 
perspektif gambar dan efesiensi form preprocessing 
2.  Membandingkan hasil gambar dari metode homography 
dengan  metode  lainnya  terhadap  akurasi  penggunaan 
metode pada model yang sama. 

III. PENELITIAN TERKAIT 

Terdapat  penelitian 

tentang  performa  dari  ORB  [3]. 
Penelitian  tersebut  merupakan  penelitian  yang  membuat 
algoritma ORB untuk pertama kalinya. Algoritma ORB sendiri 
menggunakan BRIEF dan FAST. Hasil dari penelitian tersebut 
adalah  descriptor  baru  yaitu  ORB,  dan  mendemontrasikan 
kinerja serta efisiensi dari metode tersebut. 

Penelitian lainnya yang dilakukan oleh Daniel Barath dkk [4] 
lebih  menekankan  teori  dibandingkan  praktiknya.  Penelitan 
tersebut  menggunakan  metode  SIFT,  bukan  menggunakan 
ORB.  Hasil  penelitian  tersebut  mendapatkan  2  rumus  baru 
estimasi  homograpy.  Rumus  tersebut  didapatkan  dari  batasan 
umum.  

Penelitian  tentang  penggunaan  RANSAC  yang  dilakukan 
oleh Zhang rong[5]. Penelitian tersebut fokus pada komputasi 
otomatis dari sebuah homografi gambar dengan estimator yang 
didapatkan melalui RANSAC. Metode matching keypoint yang 
digunakan  adalah  NCC  matching.  Hasil  penelitian  tersebut 
mengatakan  bahwa  algoritma  RANSAC  secara  efisien 
mengeliminasi korespodensi yang tidak akurat.  

Penelitian  lebih  lanjut  yang  dilakukan  oleh  Dr.  Mehfuza 
Holia dkk [6] tentang automisasi image registration. Penelitian 
tersebut  menyatakan  bahwa  image  registration  merupakan 
tahap  yang  penting  dalam  semua  image  preprocesing.  Image 

Penelitian  selanjutnya  adalah  tentang  klasifikasi  tulisan 
tangan  menggunakan  database  EMNIST  dan  model  neural 
network  [7].  Dari  hasil  penelitian  tersebut  dikatakan  bahwa 
model  CNN 
dalam 
menyelesaikan masalah pengenalan gambar.  

dapat  meningkatkan 

performa 

Dari berbagai penelitian terkait, maka dibutuhkan penelitian 
yang  fokus  pada  metode  homography  dalam  tahap  image 
preprocessing. Hasil gambar transformasi yang diperoleh akan 
digunakan ke dalam model CNN.  

IV. METODE PENELITIAN  
Metode yang digunakan dalam penelitian ini menggunakan 
algoritma homography. Berikut ini merupakan algoritma yang 
digunakan untuk mendapatkan hasil gambar transformasi dari 
homography : 

Gambar 1. Algoritma Homography yang digunakan 

A.  Import 2 images 

Gambar yang digunakan sebagai gambar acuan (reference) 
adalah gambar asli dari form kuesioner yang tidak terdapat 
isian. Gambar acuan didapatkan melalui website resmi BPS 
melalui sebuah file bernama kuesioner keluarga. Sedangkan 
gambar  yang  akan  diproses  adalah  gambar  yang  sudah 
terdapat isian. Gambar isian tersebut merupakan hasil dari 
scan atau foto kamera android.  

 2 / 8 

 
 
 
 
 
Gambar 2. Gambar reference 

Gambar 3. Gambar yang akan diproses 

B.  Convert to grayscale 

Warna pada pixel tidak memberikan informasi yang penting 
sehingga  gambar  yang  awalnya  memiliki  banyak  warna 
diubah  menjadi  warna  abu-abu.  Gambar  berwarna  tidak 
digunakan karena membutuhkan waktu scanning yang lebih 
besar [8]. Warna abu-abu memiliki intensitas warna merah, 
hijau, dan biru (RGB) yang sama [13]. 

C.  Initiate ORB Detector 

ORB  merupakan  perpaduan  dari  detektor  keypoint  FAST 
dan BRIEF yang dimodifikasi. Pada implementasi didalam 
computer  vision,  keypoint  harus  ditemukan  pada  gambar. 
Keypoint  yang  digunakan  harus  stabil.  Ada  beberapa 
detektor  yang  dapat  digunakan  dalam  openCv  (sebuah 
library  Python),  seperti  ORB  ,  SIFT,  dan  SURF.   Berikut 
merupakan algoritma dari ORB [9]:  

Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 4. Flowchart Algorita ORB 

Fungsi yang digunakan untuk membuat ORB pada openCv 
adalah  cv.orb().  Fungsi 
tersebut  memiliki  sebuah  nilai 
parameter berapa banyak objek ORB  yang  ingin  diinisialisasi 
(default  object  500).  Dikarenakan  gambar  yang  digunakan 
adalah gambar dokumentasi atau form kuesioner makan ORB 
yang  digunakan  untuk  meningkatkan  akurasi  hasil  gambar 
sebanyak 5000.  

D.  Find keypoint 

Setelah objek ORB dibuat, keypoint pada gambar akan 
terbentuk. Jumlah keypoint bergantung pada berapa banyak 
objek  ORB  yang  akan  diinisialisasi.  Pada  kasus  ini, 
Keypoint  yang  terbentuk  belum  jelas.  Seluruh  objek  yang 
ada  pada  gambar  bisa  saja  menjadi  keypoint  berdasarkan 
algoritma  ORB  yang  digunakan  tadi  seperti  logo  BPS, 
tulisan, dan kotak.   

Gambar 5. Keypoint dari ORB yang diinisilisasi 

 3 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
E.  Match Keypoint  

Metode yang digunakna untuk pencocokan keypoint adalah 
Brute  Force  Matches 
(BFM).  BFM  menggunakan 
pendekatan brute – force untuk pencocokan keypoint [10]. 
BFM  mengambil  sebua  keypoint  pada  gambar  pertama 
(gambar  acuan)  kemudian  dibandingkan  dengan  semua 
keypoint yang ada pada gambar kedua (gambar yang ingin 
diproses).  Setelah  itu  dicari  yang  mana  yang  paling  dekat 
jarak  antar  keypoint  (dalam  bentuk  matriks).  BFM 
deksriptor  secara  konvensional  dilakukan  menggunakan 
jarak Hamming [11]. Saat jarak hamming dihitung, output 
yang  akan  didapatkan  berupa  nilai  yang  menentukan 
kesamaan antara kedua deksriptor. Jika nilai yang diperoleh 
kurang  dari  trashold,  maka  deskriptor  bisa  dicocokkan 
(match). 

F.  RANSAC 

Random  Sampling  Consensus  (RANSAC)  adalah  metode 
yang  digunakan  untuk  memperkirakan  parameter  model 
yang cocok dengan beberapa data diantaranya ada pencilan 
yang dimana bagian dari data yang rusak [12]. Tujuan dari 
langkah  ini  agar  bad  keypoint  dapat  difilter.  Sehingga 
nantinya  keypoint  yang  tersisa  adalah  keypoint  terbaik. 
Berikut merupakan algoritma dari RANSAC  

Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik 

G.  Find Homography Matrix and Transformed 

Titik 2D (x,y) pada sebuah gambar dapat direpresentasikan 
sebagai vektor 3D x = (𝑥1, 𝑥2, 𝑥3) dimana x = 𝑥1
 dan y = 𝑥2
. 
𝑥3
𝑥2
Secara umum rumus matriks homography yaitu  

𝑠    

𝑥′
𝑦′
1

    = 𝐻    

𝑥
𝑦
1

    =    

ℎ11 ℎ12 ℎ13
ℎ21 ℎ22 ℎ23
ℎ31 ℎ32 ℎ33

𝑥
𝑦
1

               … (1) 

Gambar 7. Proses Matrik Homography 

Dari  gambar  7  setelah  didapatkan  matriks  homography  maka 
dilakukan  transform  pada  gambar  yang  diproses.  Dengan 
homography,  skewness  correction,  voice,  serta  resolusi  dari 
gambar akan sama dengan gambar yang menjadi acuan.  

Gambar 6. Algoritma RANSAC 

Gambar 8. Gambar yang diproses setelah dilakukan transformasi 

H.  Model Pengenalan Karakter  
Setelah  gambar  hasil  dari  proses  homography  dilakukan, 
gambar  tersebut  akan  digunakan  untuk  pengenalan  karakter. 
tersebut  dengan 
Kemudian  dibandingkan  akurasi  model 

 4 / 8 

 
 
 
 
 
 
 
 
 
      
 
 
 
 
 
 
 
 
Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik 

metode  image  preprocessing  lainnya.  Model  yang  akan 
digunakan  pada  penelitian  ini  adalah  model  Convulutional 
Neural Networj (CNN). Dataset yang digunakan untuk training 
model  adalah  EMNIST  berupa  huruf  dan  angka  yang 
dikonversi menjadi bentuk digit.  

V.  KERANGKA PIKIR 

Gambar 10. EMNIST dataset berupa huruf 
Berdasarkan  gambar  10  dapat  diketahui  bahwa  data 
EMNIST    huruf  memiliki  kelas  yang  tidak  balanced,  artinya 
tiap  huruf  memiliki  jumlah  dataset  yang  berbeda.  Huruf  “O” 
merupakan  dataset  terbanyak  dan  huruf  “I”  merupakan  kelas 
dengan  jumlah  dataset  terkecil.  Dataset  EMNIST  merupakan 
bentuk pixel dalam .csv file, sehingga jika ingin melihat gambar 
langsung  diperlukan  plot  array.  Gambar  dataset  dikhususkan 
atau  diperlakukan  hanya  untuk  gambar  dengan  ukuran  28x28 
pixels. 

Gambar 9. Kerangka Pikir Penelitian 

Pada penelitian ini perlu dilakukan training data agar model 
pengenalan tulisan tangan dapat digunakan. Output dari model 
tersebut  adalah  akurasi  dari  model  tersebut.  Penelitian  ini 
bertujuan  membandingkan  metode  homography  yang 
digunakan dalam proses  image preprocessing dengan  metode 
lainnya dalam model yang sama.  

Model  yang  akan  digunakan  adalah  model  CNN.  Model 
CNN  membutuhkan  dataset.  Dataset  yang  digunakan  adalah 
EMNIST  dataset  berupa  huruf  dan  angka  yang  dikonversi 
menjadi bentuk digit.  

VI. HASIL DAN PEMBAHASAN 

A.  Summaries Dataset 

Hasil  summaries  dataset  EMNIST  yang  berupa  huruf 

adalah sebagai berikut. 

Gambar 11. Contoh EMNIST dataset huruf 

 5 / 8 

 
 
 
 
 
 
 
 
 
Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 12. Contoh EMNIST dataset angka 

B.  CNN Model 

Model  yang  dibuat  pada  penelitian  ini  terdiri  dari  dua 
model,  yaitu  model  pengenalan  karakter  berupa  huruf  dan 
model pengenalan karakter berupa angka. Dua model tersebut 
menggunakan 2 dataset berbeda dari EMNIST, yaitu EMNIST 
latter  dan  EMNIST  digit.  Data  yang  akan  dilatih  berjumlah 
80% dan dari validasi atau test dataset berujumlah 20%.  

Model  yang  terbentuk  untuk  pengenalan  tulisan  huruf 
berupa  model  CNN  dengan  3  lapis  Conv2D  dengan  masing 
masing kernel size 3x3, setelah itu dibentuk lapisan flatten dan 
Dense dengan ukuran 64, 128, dan 26.  Selain  itu model  yang 
digunakan  juga  menggunakan  MaxPool2D  dengan  pool  size 
2x2. Untuk model pengenalan tulisan tangan angka atau digit 
yang terbentuk berupa model CNN yang menggunakan lapisan 
Conv2D dengan  masing –  masing kernel  size 3x3, setelah  itu 
menggunaka  MaxPool2D  dengan  ukuran  2x2  dan  model  ini 
menggunakan  lapisan  flatten  dan  2  lapisan  Dense  dengan 
ukuran 100 dan 10.  

Gambar 14. Ploting Loss model pengenalan tulisan tangan huruf 

Gambar 15. Ploting Accuracy model pengenalan tulisan tangan huruf 

Gambar 16. Ploting Akurasi dan Loss model pengenalan tulisan tangan 

angka atau digit 

C.  Penggunaan Metode Homography dan Lainnya  

Homography  untuk  image  preprocessing  sudah  dilakukan 
setiap tahapannya dan mendapatkan hasil gambar yang telah di 
transformasi.  Selanjutnya  hasil  gambar  tersebut  digunakan 
dalam model pengenalan tulisan tangan, yaitu CNN.  

Gambar 13. Summary model pengenalan tulisan tangan huruf 

 6 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
Gambar 17. Keypoint Matching.Gambar proses merupakan gambar hasil 
discanned 

Gambar 18. Keypoint Matching.Gambar yang digunakan hasil pengambilan 
foto dari kamera android. 

Gambar 19. Hasil gambar yang diproses (transformed) 

Berdasarkan  gambar  19  dapat  diketahui  bahwa  awalnya 
gambar memiliki perspektif yang berbeda dapat ditransformasi 
menggunakan  homography.  Sehingga  tidak  diperlukan  lagi 
proses skewness correction dan normalisasi gambar. 

Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 20. Gambar crop huruf dari Homography 

Selain  itu,  metode  atau  kumpulan  teknik  yang  akan 
skewness  correction,  binarization, 

dibandingkan  yaitu 
denoice,reshape,resize, dan contrast. 

D.  Perbandingan Akurasi Model  

Akurasi  merupakan  proporsi  dari  semua  hasil  yang 
diprediksi  benar  dibagi  dengan  keseluruhan  data  observasi 
prediksi. Akurasi menggambarkan seberapa tepat model dapat 
memprediksi  dengan  benar.  Data  actual  yang  digunakan 
merupakan  kuesioner  SUSENAS  2020  pada  halaman  1  saja. 
Hal  ini  karena  hanya  pada  halaman  1  saja  yang  memiliki 
banyak  isian.  Sedangkan  pada  halaman  lain  tidak  memiliki 
banyak isian dan cenderung kosong. Data actual tidak terbatas 
hanya pada  isian kotak saja,  melainkan  jawaban terbuka  juga 
dapat dimasukkan kedalam data actual.  

Gambar 21. Gambar crop jawaban terbuka 

Model CNN 

Pengenalan 
Huruf 
Pengenalan 
Angka 

Metode 
Homography 

Akurasi 
Metode atau Gabungan 
teknik lainnya 

55,5714257 

73,42857 

57,7439623 

71,37735649 

Tabel 1. Perbandingan akurasi metode 

Berdasarkan tabel 1 dapat diketahui  bahwa akurasi  model 
menggunakan  metode  atau  teknik  gabungan  lebih  baik  dari 
pada dengan metode homography. Akurasi ini dapat dikatakan 
cukup rendah dibandingkan dengan akurasi model saat validasi 
atau dilatih. Hal ini dikarenakan banyak temuan berupa tulisan 
sangat rapat yang mempersulit crop gambar dan gambar tidak 
tidak 
berkualitas  baik.  Selain 
membuat  kualitas  menjadi  semakin  baik  yang  dapat  dilihat 
pada  gambar  20.  Metode  tersebut  hanya  dapat  memperbaiki 
kemiringan  dan  ketepatan  titik  atau  koordinasi  pixel  gambar 
serta perspektif gambar.  

itu,  metode  homography 

(A) 

(B) 

Gambar 22. A) gambar tulisan tangan dengan jarak space yang sangat minim 
B) angka yang ditulis diluar kotak 

VII. 

PENUTUP 

Telah berhasil dilakukan metode homography yang mampu 
mengatasi masalah perspektif gambar. Hal tersebut ditunjukkan 
dengan  homography  matriks  berhasil  mentransformasikan 

 7 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
gambar  yang  ingin  di  proses  sedemikian  rupa  dapat  menjadi 
sama seperti gambar acuan (referensi).  

[8] 

Selain  itu  2  model  CNN  juga  sudah  dapat  dibentuk,  untuk 
model pengenalan CNN huruf dibentuk dengan dengan 3 lapis 
Conv2D  dengan  masing  masing  kernel  size  3x3,  setelah  itu 
dibentuk lapisan flatten dan Dense dengan ukuran 64, 128, dan 
26.  Selain  itu,  model  yang  digunakan  juga  menggunakan 
MaxPool2D  dengan  pool  size  2x2.  Untuk  model  CNN 
pengenalan  angka  atau  digit  menggunakan  lapisan  Conv2D 
dengan  masing-masing  kernel 
itu 
menggunakan  MaxPool2D  dengan  ukuran  2x2  dan  model  ini 
menggunakan  lapisan  flatten  dan  2  lapisan  Dense  dengan 
ukuran 100 dan 10. Masing – masing model memiliki akurasi 
pada dataset training dan validasi diatas 98%, tetapi saat model 
dicobakan ke data actual akurasi jauh lebih rendah yaitu untuk 
model  CNN  huruf  73%  dan  model  CNN  angka  71%  dengan 
metode lainnya, sedangkan untuk metode homography akurasi 
model hanya 55% untuk pengenalan huruf dan 57% pengenalan 
angka.  

size  3x3, 

setelah 

Dapat disimpulkan bahwa metode homography hanya untuk 
mengatasi  perspektif  gambar  serta  titik  atau  koordinat  pixel 
gambar  yang  tidak  meningkatkan  kualitas  gambar  secara 
signifikan.  Sedangkan  metode  lainnya  atau  teknik  gabungan 
dapat mengatasi kualitas gambar selain kemiringan gambar.  

[1] 

[2] 

[3] 

[4] 

[5] 

[6] 

DAFTAR PUSTAKA 

V. R. Kulkarni, S. Mujawar, and S. Apte, “H Ash F 
Unction Implementation Using a Rtificial N Eural N 
Etwork,” Soft Comput., vol. 1, no. 1, pp. 1–8, 2010. 

R. Collins, “Lecture 16 : Planar Homographies 
Motivation : Points on Planar Surface,” 2007, 
[Online]. Available: 
http://www.cse.psu.edu/~rcollins/CSE486/lecture16.p
df. 

M. Hasenbusch, A. Pelissetto, and E. Vicari, “The 
critical behavior of 3D Ising spin glass models: 
Universality and scaling corrections,” J. Stat. Mech. 
Theory Exp., vol. 2008, no. 2, pp. 2564–2571, 2008, 
doi: 10.1088/1742-5468/2008/02/L02001. 

D. Barath and Z. Kukelova, “Homography from two 
orientation- and scale-covariant features,” Proc. IEEE 
Int. Conf. Comput. Vis., vol. 2019-Octob, pp. 1091–
1099, 2019, doi: 10.1109/ICCV.2019.00118. 

R. Zhang, “Automatic Computation of a Homography 
by RANSAC Algorithm RANSAC for a Homography 
Direct Linear Transformation ( DLT ),” Comput. Vis. 

H. Mehfuza and S. Zankhana, “Automatic Image 
Registration,” Int. Conf. Re- search Innov. Sci. Eng. 
&Technology, ICRISET2017, vol. 1, pp. 401–411, 
2017. 

[7]  W. Zhu, “Classification of MNIST Handwritten Digit 

Database using Neural Network,” 2000. 

Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik 

S. Shirali-Shahreza, M. T. Manzuri-Shalmani, and M. 
H. Shirali-Shahreza, “Preparing Persian/Arabic 
Scanned Images for OCR,” pp. 1332–1336, 2006, doi: 
10.1109/ictta.2006.1684574. 

[9] 

Y. Lei, Z. Yu, and Y. Gong, “An Improved ORB 
Algorithm of Extracting and Matching Features,” Int. 
J. Signal Process. Image Process. Pattern Recognit., 
vol. 8, no. 5, pp. 117–126, 2015, doi: 
10.14257/ijsip.2015.8.5.12. 

[10]  B. Matchers and A. Jakubovi, “Image Feature 

Matching and Object Detection Using Brute-Force 
Matchers,” no. September, pp. 16–19, 2018. 

[11] 

[12] 

E. Bostanci, “Is Hamming distance only way for 
matching binary image feature descriptors?,” 
Electron. Lett., vol. 50, no. 11, pp. 806–808, 2014, 
doi: 10.1049/el.2014.0773. 

L. Moisan, P. Moulon, and P. Monasse, “Automatic 
Homographic Registration of a Pair of Images, with A 
Contrario Elimination of Outliers,” Image Process. 
Line, vol. 2, pp. 56–73, 2012, doi: 
10.5201/ipol.2012.mmm-oh. 

[13]        Fisher  R,  Perkins  S.,  Walker  A.  and  Wolfat  E.,  2003 

‘image preprocessing learning resource’ 

 8 / 8 

 
 
 
"
221709679,"Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik 

Pengembangan Sistem Informasi Geografis 
Pengelolaan dan Visualisasi Data Sampel Survei 

Fajrian Aidil Pratama (221709679, 4SI1) 
Dosen Pembimbing: Takdir, SST, M.T 

Ringkasan—  Pengelolaan  data  sampel  dari  kegiatan  survei 
non-KSA  yang  dilakukan  masih  belum  optimal.  Salah  satu 
permasalahan  yang  timbul  akibat  hal  ini  adalah  tidak  adanya 
informasi mengenai respondent burdent, sehingga pengguna tidak 
mengetahui sudah berapa kali satu sampel terpilih dan di survei 
mana  saja  sampel  tersebut  terpilih.  Hal  ini  bisa  meningkatkan 
kemungkinan  terjadinya non-respon.  Selain itu, pengguna tidak 
bisa melakukan visualisasi sebaran data sampel secara geospasial 
sehingga  pengguna  tidak  bisa  melihat  bagaimana  pola  sebaran 
sampel.  Untuk  membantu  mengatasi  permasalahan  tersebut, 
melalui  penelitian  ini,  dibangun  sebuah  SIG  pengelolaan  dan 
visualisasi  data  sampel  survei  berbasis  web  yang  diharapkan 
dapat  menangani  permasalahan  pengolahan  data  sampel  survei 
non-KSA  di  BPS.  Sampai  saat  ini,  sistem  yang  dibangun  sudah 
berhasil dikembangkan. 

Kata Kunci— data visualization, gis, sampel, webgis, geospatial 

I.  LATAR BELAKANG 

Dalam setiap kegiatan survei yang dilaksanakan oleh BPS, 
tentunya  akan  selalu  ada  data  sampel  terpilih  yang  akan 
digunakan.  Selama  ini,  data  sampel  yang  dihasilkan  dari 
penarikan  sampel  di  BPS  masih  berupa daftar  tabular.  Selain 
itu,  pengelolaan  data  sampel  dari  kegiatan  survei  yang 
dilakukan, khususnya kegiatan survei berbasis non-KSA masih 
belum optimal sehingga menimbulkan beberapa permasalahan. 
Permasalahan  pertama  yang  mungkin  saja  terjadi  adalah 
operator  tidak  mengetahui  apakah  hasil  penarikan  sampel 
tersebar secara merata terhadap populasi.  

Bentuk data yang tabular juga menyebabkan pengguna tidak 
dapat  melihat  seperti  apa  visualisasi  dari  sampel  tersebut. 
Pengguna  tidak  dapat  membandingkan  sampel,  tidak  dapat 
mengetahui  muatan  sampel  terpusat  di  wilayah  mana,  dan 
bagaimana  proporsi  sampelnya  terhadap  populasi  di  setiap 
wilayah. Dengan mengetahui informasi tersebut, pengguna bisa 
memilih set sampel yang tepat dari alternatif set sampel yang 
ada.  Misalnya  ada  dua  set  sampel  alternatif,    pengguna  bisa 
membandingkan  kedua  set  sampel  tersebut  lalu  bisa  dipilih 
sampel yang memiliki lebih sedikit wilayah remote area yang 
terpilih sehingga akan lebih hemat dari sisi anggaran. 

 Belum tersedia informasi tentang respondent burdent/beban 
responden, sehingga pengguna tidak mengetahui dalam periode 
tertentu, berapa kali satu sampel terpilih dan di survei apa saja. 
Hal ini penting diketahui karena jika responden terlalu sering 
terpilih sebagai sampel di berbagai survei yang berbeda, besar 
kemungkinan akan terjadi non-respon dari responden tersebut 
karena responden burdent dari sampel tersebut cukup besar. 

Proses pengumpulan data pada beberapa survei di BPS saat 
ini  juga  sudah  mulai  didigitalisasi  dengan  adanya  CAPI, 
CAWI, dan beberapa sistem entri data lainnya. Oleh karena itu, 
diperlukan  sistem  yang  dapat  menyediakan  service    yang 

nantinya  dapat  memberikan  data  sampel  dan  preprinted  dari 
kegiatan  survei  agar  dapat  diakses  oleh  sistem  lain  seperti 
CAPI, CAWI, dan sistem entri data lainnya di BPS. 

Selain  itu,  untuk  memudahkan  pengguna  diperlukan  juga 
suatu sistem yang bisa diakses dari mana saja, kapan saja dan 
lintas  platform.  Hal  ini  untuk  mengefisiensikan  biaya  dan 
waktu  yang  deiperlukan  untuk  pengembangan  sistem. 
Teknologi  yang  paling  mendukung  untuk  memenuhi  hal 
tersebut adalah teknologi web. 

Pada  penelitian  kali  ini  telah  dibangun  sebuah  sistem 
informasi  geografis  berbasis  web  dalam  bentuk  website  yang 
dapat  digunakan  untuk  mengelola  data  sampel  sekaligus 
melakukan visualisasi interaktif dari data sampel survei.  

II.  TUJUAN PENELITIAN 

Berdasarkan  latar  belakang,  tujuan  diadakannya  penelitian 

ini adalah : 

2.1. Membangun  sistem  yang  dapat  digunakan  untuk 

mengelola data sampel survei. 

2.2. Menampilkan  bentuk  dan  pola  penyebaran  data 
sampel survei serta analisis otokorelasi spasial melalui 
penghitungan Indeks Moran’s I. 

2.3. Menyediakan  service  yang  memberikan  data  sampel 

survei untuk sistem entri data di BPS. 

III. PENELITIAN TERKAIT 

Terdapat beberapa penelitian terkait, diantaranya penelitian 
[1].  Pada  penelitian  ini  penulis  mencoba  merancang  sebuah 
API  untuk  memudahkan  dan  menyederhanakan  proses 
pembuatan visualisasi geospasial yang interaktif pada website 
menggunakan  API  yang  mereka  buat  yaitu  mapmap.js. 
Penelitian  ini  hanya  menunjukan  seperti  bagaimana  cara 
menggunakan  mapmap.js  pada  website  yang  akan dibuat  dan 
apa  yang  membedakan  mapmap.js  dengan  API  untuk 
visualisasi  data  geospasial  lainnya  seperti  Google Maps  API, 
D3.js, leaflet.js, dsb.  

Sedangkan penelitian [2] merupakan penelitian yang berisi 
proses perancangan  sistem informasi geografis dengan fungsi-
fungsi  yang hampir  serupa  dengan  yang  akan  dibangun  pada 
penelitian ini. Perbedaannya hanya dari data yang akan diolah 
dan juga dari segi permasalahan yang ingin diselesaikan. Pada 
penelitian ini, peneliti mencoba membangun sistem informasi 
geografis 
dan 
memvisualisasikan  data  sarana  yang  ada  di  Kabupaten 
Pasaman Barat. 

untuk  mengelola 

digunakan 

yang 

TABEL I 

TABEL LITERATUR 

 1 / 8 

 
 
 
 
 
 
 
No 

Penelitian 

1  F. Leddermann 
dan G. Gartner 
(2015)[1] 

2  Harison dan A. 
Syarif (2016)[2] 

Deskripsi Penelitian 
Pada penelitian in, peneliti membuat library 
javascript  yang dapat digunakan untuk 
keperluan visualisasi spasial. Library ini dapat 
dijadikan sebagai referensi dalam 
mengembangkan sistem yang sedang 
dikembangkan.  

Pada penelitian ini, peneliti membangu sistem 
informasi geografis untuk menampilkan 
visualisasi spasial dari data lokasi sarana dan 
prasarana di wilayah Kabupaten Pasaman 
Barat. 

IV. METODE PENELITIAN  
Metode  penelitian  yang  digunakan  pada  pengembangan 
sistem  visualisasi  data  ini  adalah  menggunakan  metode 
Software  Development  Life  Cycle  (SDLC)  dengan  model 
prototype.  Model  protoype  digunakan  agar  sistem  dapat 
memenuhi  kebutuhan  pengguna.  Dengan  model  ini,  jika 
pengguna  tidak  puas,  menginginkan  adanya  fitur  baru  atau 
terdapat error pada aplikasi, maka dapat dilakukan perbaikan 
dengan lebih cepat (iterasi).  

Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik 

Peneliti  melakukan  studi  literatur  untuk  mencari 
penelitian  terkait  sebagai  referensi  untuk  mengetahui 
konsep. definisi, serta tools yang bisa digunakan. 

4.1.3 Observasi 

Peneliti melakukan observasi pada beberapa aplikasi 
SIG  dan  tools  yang  sudah  ada.  Observasi  dilakukan 
untuk  mempelajari  kelebihan  dan  kekurangan  dari 
masing-masing sistem dan tools tersebut sehingga dapat 
ditemukan solusi dari kelemahan yang ada. 

4.2.  Prototype Design 

Pada  tahapan  ini  mulai  dirancang  purwarupa  untuk 
sistem  yang  akan  dibuat  berdasarkan  hasil  identifikasi 
kebutuhan pada tahap sebelumnya, . Tahapan ini sendiri 
memiliki beberapa sub-tahapan yang perlu dilalui.yaitu 
design,  prototyping,  user  evaluation,  dan  terakhir 
review & update 

4.2.1. Design 

Peneliti mulai membuat desain dari sistem yang akan 
analisis  kebutuhan 

dibangun  berdasarkan  hasil 
pengguna. 

4.2.2. Prototyping 

Gambar 1. Model Prototype 
Tahapan  dari  model  prototipe  dapat  dilihat  pada  Gambar 

1[3]. Tahapan dari model ini antara lain : 
4.1.  Requirement Analysis 

hal 

dalam 

Pada tahapan ini dilakukan wawancara langsung ke 
subject  matter 
ini  Subdirektorat 
Pengembangan  Kerangka  Sampel  dan  Subdirektorat 
Pengembangan  Pemetaan  Statistik  BPS  RI.  Dari 
tahapan  ini  dihasilkan  daftar  kebutuhan  sistem  yang 
akan digunakan untuk menentukan batasan perancangan 
sistem  yang  akan  dibuat.  Setelah  seluruh  kebutuhan 
sistem  berhasil  didapatkan,  selanjutnya  dilakukan 
pengumpulan  data  yang  dibutuhkan  melalui  beberapa 
metode diantaranya : 

4.1.1  Wawancara  

Peneliti melakukan wanwacara langsung ke subject 
matter  untuk  mengetahui  permasalahan  yang  ada  di 
Subdit  Pengembangan  Kerangka  Sampel 
dan 
Subdirektorat  Pengembangan  Pemetaan  Statistik  BPS 
RI serta rencana ke depan dari masing-masing subdit.  

4.1.2 Studi Literatur  

Setelah  membuat  desain,  dibuat  sebuah  purwarupa 
yang diasumsikan bisa memenuhi kebutuhan-kebutuhan 
pengguna.  Pada 
fungsionalitas  dari 
fitur/aplikasi  yang  dikembangkan  yang  akan  menjadi 
perhatian utama. 

tahapan 

ini, 

4.2.3. User Evaluation 

Purwarupa  yang  sudah  jadi  dibuat  selanjutnya  akan 
dievaluasi oleh pengguna. Evaluasi ini dilakukan untuk 
mencoba  apakah  purwarupa  yang  dibuat  sudah  bisa 
memenuhi kebutuhan pengguna. 

4.2.4. Review & Update 

Jika  dari  hasil  evaluasi  pengguna  purwarupa  dinilai 
belum bisa memenuhi kebutuhan maka purwarupa akan 
di-review  kembali  dan  peneliti  akan  melakukan  terasi 
dari  tahapan  4.2.1  agar  purwarupa  bisa  memenuhi 
kebutuhan pengguna. Purwarupa yang diverifikasi oleh 
pengguna akan dilanjutkan ke tahap selanjutnya. 

4.3.  Development 

Pada tahapan ini,  purwarupa yang sudah diverifikasi 
pengguna akan dilakukan pengembangan lanjutan untuk 
menyempurnakan  fungsi  dan  tampilan  dari  purwarupa 
agar siap untuk diintegrasikan ke aplikasi utama. 

4.4.  Testing 

Pada  tahapan  ini  fitur/aplikasi  yang  sudah  selesai 
dikembangkan  lslu  dilakukan  pengujian  kelayakan. 
Pengujian dilakukan dengan metode Black-box testing.  
Black-box  testing  [4]  dilakukan  untuk  memastikan 
bahwa fitur/aplikasi yang dikembangkan telah berjalan 

 2 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
sebagaimana  mestinya  dan  sesuai  dengan  kebutuhan 
pengguna. Tester akan melakukan pengecekan satu per 
satu  pada  setiap  fitur  aplikasi  yang  dibuat.  Variabel 
kepuasan  pengguna  yang  akan  diukur  diantaranya 
adalah  dari  segi  tampilan  aplikasi  (UI),  kemudahan 
penggunaan aplikasi (UX), dan fitur aplikasi. 

Selain  itu  juga  terdapat  automated  testing  dalam  hal 
ini memanfaatkan PHPUnit [5] dilakukan pada fitur atau 
fungsi untuk mengotomatisasi proses black-box testing 
pada fitur dan fungsi. Hal ini untuk memastikan bahwa 
codebase  aplikasi  scalable  dan  maintainable  agar 
mencegah  terjadinya bug saat melakukan  penambahan 
fitur atau refactoring code.  

4.5.  Maintenance 

Terakhir  adalah  tahapan  maintenance.  Tahapan  ini 
dimaksudkan  untuk  memastikan  bahwa  data  dan  fitur 
yang disajikan oleh aplikasi selalu up to date. Selain itu, 
tahapan  ini  juga diperlukan untuk menjaga sistem jika 
suatu saat terjadi error atau ditemukan bug pada aplikasi 
dan penambahan fitur jika dikemudian hari terdapat fitur 
yang dirasa perlu untuk ditambahkan pada sistem. 

V.  KERANGKA PIKIR 

Rancangan kerangka  pikir dibuat untuk membantu peneliti 
dalam  menentukan  alur  dari  pengerjaan  penelitian  ini.  Pada 
Gambar 2 dapat dilihat dari analisis existing system yaitu belum 
adanya sistem yang dikhususkan dalam mengelola data sampel 
survei  dan  untuk  memvisualisasikan  data  sampel  survei 
tersebut  ke  dalam  peta  tematik  maupun  visualisasi  grafis 
lainnya.  

Hasil  analisis  kebutuhan  selanjutnya  digunakan  untuk 
merumuskan  permasalahan  yang  ada.  Setelah  itu  dilakukan 
studi  literatur terkait  permasalahan yang ada untuk  kemudian 
dibangun solusi agar kebutuhan sistem bisa terpenuhi. 

Feedback  dari  perumusan  masalah  selanjutnya  dianalisis 
kembali untuk kemudian diimplementasikan pada sistem baru. 

Gambar 2. Kerangka Pikir 

Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik 

VI. HASIL DAN PEMBAHASAN 

6.1. Analisis Sistem Berjalan 

Berdasarkan hasil wawancara yang telah dilakukan dengan 
Subdirektorat  Pengembangan  Kerangka  Sampel 
dan 
Subdirektorat  Pengembangan  Pemetaan  Statistik  BPS  RI, 
diketahui bahwa data sampel survei  yang ada di BPS sampai 
saat  ini  masih  dikelola  dengan  aplikasi  FoxPro.  Data  sampel 
terpilih yang dihasilkan kemudian disebarkan ke daerah-daerah 
dalam bentuk data tabular. Karena data sampel yang ada masih 
dalam  bentuk 
tabular,  data  tersebut  masih  belum  bisa 
diintegrasikan langsung  dengan  sistem entri data yang  ada  di 
BPS seperti CAPI dan CAWI. 

Pada halaman WebGIS BPS, pengguna bisa menampilkan 
peta  tematik  interaktif  untuk  memvisualisasikan  data  dengan 
memilih  data  yang  tersedia  untuk  kemudian  divisualisasikan 
pada peta wilayah Indonesia.  

6.2. Identifikasi Masalah 

Berdasarkan  hasil  analisis  sistem  berjalan,  selanjutnya 
dilakukan analisis masalah yang timbul atau berpotensi terjadi 
menggunakan Fishbone Diagram. Berikut permasalahan dalam 
kegiatan  pengelolaan  data  sampel  survei  yang  digambarkan 
pada Fishbone Diagram seperti terlihat pada Gambar 3: 

Gambar 3. Diagram Fishbone 

6.3. Analisis Kebutuhan 

Berdasarkan  identifikasi  masalah  di  atas,  kebutuhan  yang 

harus dipenuhi dalam pembangunan sistem ini adalah: 

1.  Membangun website pengelolaan dan visualisasi data 

sampel survei non-KSA. 

2.  Membangun sistem yang dapat menampilkan riwayat 

sampel terpilih/beban responden 

3.  Membangun  sistem  yang  menyediakan  fitur  untuk 
mengunduh  data  sampel  secara  mandiri  bagi 
pengguna di daerah-daerah 

4.  Membangun  visualisasi  peta  tematik  yang  dapat 
menampilkan  data  sebaran  sampel  dan  agregat 
variabel sampel. 

5.  Membangun  API  Service  untuk  menyediakan  data 

sampel bagi sistem lain yang ada di BPS 

6.4. Rancangan Sistem 

Dalam perancangan sistem, berdasarkan analisis kebutuhan 
dilakukan desain arsitektur sistem dan desain antarmuka sistem. 
Arsitektur  sistem  biasanya  mencakup  informasi  mengenai 
komponen  sistem  dan  interaksi  antar  komponen  tersebut.  
Untuk perancangan antarmuka, terlebih dahulu dibangun dalam 
bentuk mockup untuk dijadikan dasar penentuan GUI sistem.  

 3 / 8 

 
 
 
 
 
 
 
 
Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik 

Visualisasi  ini  dibangun  dengan  menggunakan 
bahasa  pemrograman  JavaScript  dan  framework  Vue.js 
dengan  library  ArcGIS  Javascript  API,  Turf.js,  dan 
Bootstrap-Vue.  

Vue.js  digunakan  sebagai  kerangka  kerja  untuk 
mengintegrasikan  beberapa  library  yang  akan  digunakan. 
Bootstrap-Vue merupakan plugin tambahan dari Vue.js dan 
Bootstrap yang menyediakan beberapa komponen UI yang 
bisa digunakan pada website. Library ini digunakan untuk 
mengatur komponen-komponen UI pada website.  

Library  ArcGIS  Javascript  API  digunakan  untuk 
menampilkan  basemap  pada  website  untuk  keperluan 
visualisasi tematik data sampel. Turf.js merupakan library 
open source yang digunakan untuk melakukan analisis data 
spasial pada  bahasa  pemrograman  javascript.  Pada sistem 
ini, Turf.js digunakan untuk menghitung Indeks Moran’s I 
[7] guna mencari tahu pola penyebaran sampel yang dibuat. 

6.5. Implementasi 

6.5.1. Implementasi Antarmuka 

Sistem 

Informasi  Geografis  Pengelolaan  dan 
Visualisasi  Data  Sampel  Survei  berbasis  web  terdiri  dari 
sembilan  halaman  yaitu  halaman  landing,  halaman  login, 
halaman  registrasi,  halaman  dashboard,  halaman  log 
website,halaman  pengelolaan  data  sampel, 
  halaman 
pengelolaan  data  survei,  halaman  pengelolaan  data 
pengguna, halaman pengelolaan data kolom tabel, halaman 
pengelolaan  data  API  Key  (untuk  akses  ke  sistem  lain), 
halaman dokumentasi API, halaman notifikasi, dan halaman 
profil.  

Sampai  saat  ini,  seluruh  halaman  tersebut  telah 
berhasil  dibuat.  Berikut  ini  adalah  rincian  dari  beberapa 
fitur utama aplikasi yang sudah selesai dibuat. 
6.5.1.1 Halaman Dashboard 

  Halaman ini adalah fitur utama dari aplikasi yang akan 
ditampilkan  setelah  pengguna  masuk  ke  aplikasi.  Pada 
ini  pengguna  bisa  melihat  daftar  sampel, 
halaman 
mengunduh  data  sampel,  dan  melakukan  visualisasi  data 
sampel. 

6.4.1. Arsitektur Sistem 

Sistem dibagi ke dalam beberapa bagian yaitu bagian 
front-end  (antarmuka),  back-end,  dan  API  Service  yang 
digunakan untuk integrasi ke sistem lain. 

Bagian  front-end  (antarmuka)  adalah  bagian  sistem 
yang berinteraksi dengan pengguna. Pada bagian front-end 
dirancang  dengan  menggunakan  kerangka  kerja  Vue.js 
dikombinasikan  dengan  beberapa  library  sesuai  dengan 
fungsi dan tujuannya yaitu Bootstrap-Vue untuk mengatur 
komponen  UI  seperti  tombol,  form,  dsb,  Turf.js  untuk 
keperluan analisis data geospasial, ArcGIS Javascript API 
untuk  menampilkan  peta,  dan  Webpack  sebagai  asset 
management untuk mengelola library di sisi front-end. 

Bagian back-end menggunakan bahasa pemrograman 
PHP  dengan  kerangka  kerja  Laravel  dan  PostgreSQL 
sebagai DBMS untuk mengelola data. Back-end mengatur 
semua  keperluan  data  dan  fungsional  dari  front-end 
sekaligus sebagai REST server untuk API Service. 

Terakhir untuk bagian API Service dirancang dengan 
desain REST API  dengan  otentikasi berbasis  API  Key [6] 
menggunakan 
library  bawaan  Laravel  yaitu  Laravel 
Sanctum  untuk  mengelola  API  Key  dan  otentikasi  API 
Service. Secara visual, arsitektur sistem dapat terlihat pada 
Gambar 4. 

Gambar 4. Arsitektur Sistem 

6.4.2. Rancangan Antarmuka 

Visualisasi yang akan  ditampilkan dalam website ini 
terbagi  menjadi  4  bagian  yaitu  Peta  Indonesia,  pie  chart, 
bar chart, dan tabel. Visualisasi jenis peta digunakan untuk 
memvisualisasikan  sebaran  sampel  dan  agregat  dari 
variabel sampel.  Visualisasi jenis  pie chart  dan  bar chart 
akan digunakan untuk membandingkan variabel sampel dan 
juga  proporsi  sampel  dengan  populasi.  Terakhir  untuk 
visualisasi  tabel  digunakan  untuk  menampilkan  informasi 
lengkap  sampel.  Contoh  rancangan  antarmuka  visualisasi 
dapat dilihat pada Gambar 5. 

Gambar 6. Halaman Dashboard 

Fitur  visualisasi  data  sampel  dan  lihat  daftar  sampel 
survei bisa digunakan oleh seluruh pengguna, dengan syarat 
pengguna  harus  melengkapi  informasi  wilayah  kerja. 
Sementara fitur unduh data sampel hanya bisa dipakai jika 
sampel survei sudah final. 

Dari 

input  pengguna,  sistem  diharapkan  bisa 
menghasilkan  output  berupa  tabel  daftar  sampel  survei, 

Gambar 5. Rancangan Antaremuka Dashboard 

 4 / 8 

 
 
 
 
 
 
 
visualisasi  geospasial  data  sampel,  dan  Indeks  Moran’s  I 
untuk keperluan analisis otokorelasi spasial.  

Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 7. Halaman Dashboard menampilkan daftar 

sampel sesuai input pengguna 

Gambar 10. Kolom beban cacah pada tabel daftar sampel 

Gambar 8. Indeks Moran’s I menggunakan Turf.js pada data 

sampel di Kab. Sintang, Kalimantan Barat 

Gambar 9. Implementasi visualisasi data agregat 

Gambar 11. Halaman detail sampel yang menampilkan 
seluruh atribut sampel dan riwayat cacah sampel 

6.5.1.2. Halaman Buat API Key 

Pada  halaman  ini,  pengguna  bisa  mengelola  API  Key 
yang  nantinya  akan  digunakan  oleh  aplikasi  lain  untuk 
mengakses  data  sampel  dari  aplikasi  ini.  API  Key  ini  
mengamankan  dan  mengautentikasi  aplikasi  yang  ingin 
mengakses  data  untuk  memastikan  bahwa  aplikasi  yang 
ingin  mengakses  data  merupakan  aplikasi  yang  sudah 
terdaftar pada sistem. 

Untuk  membuat  API  Key  baru,  penguna  cukup 
memasukkan nama aplikasi yang akan menggunakan API 
Key tersebut.  

Gambar 10 Implementasi visualisasi data kategorik 

Selain  melihat  daftar  sampel,  pengguna  juga  bisa 
melihat beban cacah sampel dan juga riwayat cacah sampel. 
Informasi  riwayat  cacah  dapat  dilihat  dengan  menekan 
tombol  detail  pada  sampel  yang  ingin  dilihat  riwayat 
cacahnya. 

Gambar 12. Halaman Pengelolaan API Key 

 5 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 13. Contoh respons API saat melakukan request 
tanpa API-KEY 

Gambar 16. Tampilan detail survei 

Gambar 17 & 18. Tampilan sidebar untuk pengguna 
dengan peran viewer (kiri) dan pengguna dengan peran 
admin (kanan) 

6.5.1.4. Halaman Pengelolaan Data Sampel 

Pada  halaman  ini,  admin  bisa  mengelola  data  sampel 
untuk  kegiatan  survei.  Fitur  yang  bisa  dilakukan  oleh 
admin  diantaranya  menambahkan  data  sampel/frame 
sampel,  mengubah  data  sampel  survei,  dan  melakukan 
finalisasi  data  sampel  survei.  Jika  sampel  untuk  survei 
sudah difinalisasi, maka admin tidak dapat mengubah data 
sampelnya lagi. 

Proses  unggah  data  dilakukan  di  latar  belakang, 
sehingga  admin  tetap  bisa  menggunakan  aplikasi  selagi 
menunggu  unggah  selesai.  Untuk  memantau  progres 
unggah,  cukup  dengan  menekan  tombol  refresh.  Sistem 
akan memberikan notifikasi jika data berhasil atau gagal 
diunggah ke database. 

Gambar 14. Contoh response API saat melakukan request 
dengan API-KEY 

6.5.1.3. Halaman Pengelolaan Data Survei 

Setiap  data  sampel  yang  diunggah  ke  sistem  wajib 
menyertakan informasi survei yang menggunakan sampel 
tersebut.  Halaman  ini  dibuat  untuk  mengelola  data 
kegiatan survei yang akan dihubungkan pada data sampel. 

Gambar 15. Halaman Pengelolaan Data Survei 

  Halaman ini hanya bisa diakses oleh pengguna dengan 
peran  admin/super  admin.  Pada  halaman  ini  admin  bisa 
menambahkan,  melihat 
atau 
menghapus data survei. 

detail,  mengubah, 

Gambar 19. Formulir unggah data sampel. Formulir yang 
digunakan sama untuk proses menambahkan maupun 
mengubah data sampel survei 

 6 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
Gambar 20. Notifikasi ketika proses unggah berhasil 

6.6. Pengujian dan Evaluasi Sistem Informasi Usulan 

Dari  10  responden  yang  terpilih,  sebanyak  6  responden 
mengirimkan respon pengujian dengan rincian sebagai berikut: 
  Subdit  Pengembangan  Pemetaan  Statistik  BPS  RI  :  4 

orang dari 5 orang terpilih 

  BPS Kab. Donggala : 2 orang dari 5 orang terpilih 

6.6.1. Black-box Testing 

Hasil  pengujian  black-box  testing  dengan  39  (tiga  puluh 
sembilan)  skenario  kejadian  berdasarkan  tahapan  dalam 
menjalankan fungsi/fitur utama. Dari hasil pengujian tersebut 
dapat disimpulkan bahwa fungsi/fitur utama yang terdapat pada 
aplikasi berfungsi sesuai dengan yang diharapkan. 

6.6.2. User Acceptance Test 

6.6.2.1. Kuesioner Rancangan Sendiri 
  Komponen Evaluasi : 

-  Sistem Informasi Usulan 

 

Indikator Evaluasi 
-  Kemudahan 
-  Efisiensi 
-  Kepuasan 
-  Fungsionalitas 

  Hasil Evaluasi 

Evaluasi Sistem Informasi Usulan 
-  Kemudahan  
: 90 
-  Efisiensi 
: 88,89 
-  Kepuasan 
: 90 
-  Fungsionalitas  : 90 

Skor Total SIU : 89,72 

: Sangat Buruk 

Interpretasi Skor  : 
0 – 19 
20 – 39  : Buruk 
40 – 59  : Cukup 
60 – 79  : Baik 
80 – 100 : Sangat Baik 

6.6.3. System Usability Scale 

Berikut  ini  tabel  skor  hasil  pengujian  sistem  dengan 

menggunakan kuesioner SUS : 

Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik 

Resp. 

1 
2 
3 
4 
5 
6 

1 
3 
2 
4 
1 
3 
4 

2 
4 
2 
4 
1 
4 
3 

3 
4 
2 
4 
2 
3 
3 

TABEL II 
HASIL SKOR SUS 
Nilai Pertanyaan 
4 
3 
3 
4 
0 
3 
3 

6 
4 
3 
4 
2 
4 
4 

5 
4 
3 
4 
1 
2 
3 

7 
3 
2 
4 
2 
3 
3 

8 
4 
3 
4 
2 
3 
4 

Rata-Rata 

9  10 
2 
3 
1 
2 
4 
4 
1 
1 
1 
3 
1 
3 

𝑆𝑘𝑜𝑟 
× 2,5 
85 
57,5 
100 
32,5 
72,5 
77,5 
70,83 

Hasil  pengujian  UAT  menggunakan  kuesioner  buatan 
sendiri menguji satu komponen evaluasi yaitu Sistem Informasi 
Usulan (SIU) didapatkan skor rata-rata sebesar 89,72 (delapan 
puluh  sembilan  koma  tujuh  dua)  dengan  interpretasi  Sangat 
Baik. Sementara untuk pengujian menggunakan kuesioner SUS 
diperoleh  skor  rata-rata  sebesar  70,83  (tujuh  puluh  koma 
delapan tiga). Berdasarkan penelitian [8], skor yang didapatkan 
oleh sistem  ini berada  diatas rata-rata skor  SUS untuk sistem 
web  dan  aplikasi  (68)  sehingga  dapat  disimpulkan  bahwa 
sistem  informasi  yang  telah  dikembangkan  masuk  ke  dalam 
rentang penerimaan yang cukup oleh pengguna. 

VII. 

PENUTUP 

7.1. Kesimpulan 

Berdasarkan analisis dan perancangan yang dilakukan pada 

penelitian ini, maka dapat disimpulkan: 
1.  Sistem  Informasi  Geografis  Pengelolaan  dan  Visualisasi 

Data Sampel Survei telah selesai dibangun. 

2.  Sistem mampu menampilkan visualisasi data sampel pada 
peta  berdasarkan 
juga  bisa 
menghitung  Indeks  Moran’s  I  untuk  keperluan  analisis 
otokorelasi spasial sampel. 

input  pengguna.  Sistem 

3.  Sistem  mampu  menyediakan  API  Services  untuk 

mengintegrasikan data sampel ke sistem lainnya. 

4.  Hasil evaluasi sistem dengan metode UAT telah dilakukan 
dan  dihasilkan  skor  sebesar  89,72  dengan  interpretasi 
bahwa sistem yang dibuat sudah sangat baik. 

5.  Hasil evaluasi sistem dengan metode SUS telah dilakukan 
dan  dihasilkan  skor  rata-rata  SUS  sebesar  70,83  yang 
menandakan  bahwa  sistem  yang  dibuat  berada  dalam 
rentang penerimaan yang cukup oleh pengguna. 

7.2. Saran 

Pengembangan  sistem  ini  masih  terdapat  beberapa 
kekurangan.  Kekurangan  tersebut  diantaranya  adalah  sistem 
hanya memvisualisasikan data agregat dan data kategorik dari 
sampel, belum ada visualisasi proporsi antara data sampel dan 
populasi. Sistem ini tujuannya bisa mengelola data sampel dari 
berbagai kegiatan survei di BPS. Namun, karena saat ini tabel 
dirancang  berdasarkan  data  yang  tersedia    sehingga  perlu 
penyesuaian atribut kolom pada tabel sampel agar bisa memuat 
data sampel dari berbagai survei. Penelitian lanjutan diperlukan 
untuk mengatasi berbagai kekurangan tersebut. 

 7 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik 

DAFTAR PUSTAKA 

[1]  F. Ledermann and G. Gartner, “Mapmap. js: A Data-Driven Web Mapping 
API for Thematic Cartography,” 27th Int. Cartogr. Conf., no. 27, pp. 1043–
1053, 2015. 

[2]   Harison  and  A.  Syarif,  “Sistem  Informasi  Geografis  Sarana  Pada 
Kabupaten Pasaman Barat,” J. TEKNOIF, vol. 4, no. 2, pp. 40–50, 2016. 

[3]  R.  Pressman  and  B.  Maxim,  Software  Engineering  :  A  Practitioner's 

Approach, 8th Ed. 2014 

[4]  Larrea,  Martín.  (2017).  Black-Box  Testing  Technique  for  Information 
Visualization.  Sequencing  Constraints  with  Low-Level  Interactions. 
Journal of Computer Science & Technology. 17.  

[5]  Dusch, Volker. (2013). PHPUnit Best Practices. Fosdem. 

[6]  Xiang-Wen Huang, Chin-Yun Hsieh, Cheng Hao Wu and Yu Chin Cheng, 
""A  Token-Based  User  Authentication  Mechanism  for Data  Exchange in 
RESTful  API,""  2015  18th  International  Conference  on  Network-Based 
Information Systems, 2015, pp. 601-606, doi: 10.1109/NBiS.2015.89. 

[7]   Tillé, Y., Dickson, M. M., Espa, G., & Giuliani, D. (2018). Measuringthe 
spatial  balance  of  a  sample:  A  new  measure  based on  Moran’s  I  index. 
Spatial Statistics. https://doi.org/10.1016/j.spasta.2018.02.001 

[8]  Bangor, A., Kortum, P. T., & Miller, J. T. (2008). An Empirical Evaluation of 
the  System  Usability  Scale.  International  Journal  of  Human–Computer 
Interaction, 24(6), 574–594. 

 8 / 8 

 
 
 
 
 
"
221709674,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Sistem Penerjemah Bahasa Isyarat 
Studi Kasus: Bahasa Isyarat Indonesia 

Faiq Zakki Mubarok (221709674, 4SD2) 
Dosen Pembimbing: Robert Kurniawan, S. ST., M. Si. 

pada 

umumnya 

Ringkasan—Komunikasi 

tunarungu.  Penyandang 

dilakukan 
menggunakan  bahasa  verbal,  namun  terdapat  kelompok  yang 
memiliki keterbatasan dalam berkomunikasi secara verbal yaitu 
penyandang 
tunarungu  biasanya 
menggunakan bahasa isyarat sebagai alat untuk berkomunikasi. 
Permasalahannya adalah tidak semua orang bisa mengerti bahasa 
isyarat, dan tidak semua orang bisa menggunakan bahasa verbal 
sebagai alat komunikasi. Sehingga penelitian ini bertujuan untuk 
mengembangkan  modul  penerjemah  bahasa  isyarat  BISINDO 
sebagai  penghubung  antara  masyarakat  umum  dengan 
penyandang  tunarungu  sehingga  dapat  memperkuat  hubungan 
interpersonal  antara  kedua  belah  pihak.  Modul  penerjemah 
Bahasa  Isyarat  Indonesia  dibangun  dengan  bantuan  modul 
Mediapipe sebagai modul ekstraksi fitur tangan dan pose tubuh, 
dan  pelatihan  modul  penerjemah  menggunakan  arsitektur 
LSTM.  Berdasarkan  hasil  pengujian,  model  LSTM  yang 
dibangun  mendapatkan  akurasi  sebesar  92,857%  dan  memiliki 
efisiensi  yang 
lebih  baik  dibandingkan  model  penelitian 
terdahulu. 

Kata Kunci— Penerjemah Bahasa Isyarat, BISINDO, Ekstraksi 

Fitur, Mediapipe. 

I.  LATAR BELAKANG 

Manusia  sebagai  makhluk  sosial  selalu  ingin  berhubungan 
dengan manusia lainnya untuk mengetahui situasi dan kondisi 
di  sekitarnya.  Keingintahuan  manusia  akan  lingkungan  di 
sekitarnya ini memaksa manusia untuk berkomunikasi, karena 
dengan  berkomunikasi  manusia  bisa  mendapatkan  informasi 
dari sudut pandang lawan komunikasinya.  

alat 

Pada  umumnya,  komunikasi  menggunakan  bahasa  verbal 
sebagai 
lawan 
komunikasinya  dan  hanya  menggunakan  komunikasi  non 
verbal  sebagai  pelengkap  pesan  verbal  untuk  memastikan 
bahwa makna pesan dapat tersampaikan seutuhnya [1]. 

berkomunikasi 

kepada 

untuk 

Namun, komunikasi secara verbal tidak dapat dilakukan oleh 
semua  orang  karena 
terdapat  kelompok  yang  memiliki 
keterbatasan  dalam  komunikasi  verbal  baik  secara  reseptif 
lain)  maupun  ekspresif 
(memahami  pembicaraan  orang 
(berbicara)  [2].  Kelompok  tersebut  adalah  kelompok  yang 
memiliki keterbatasan pada indra pendengar dan biasa disebut 
sebagai tunarungu [3]. 

Penyandang 

tunarungu  biasanya  menggunakan  bahasa 
isyarat  sebagai  alat  komunikasi.  Bahasa  isyarat  merupakan 
bahasa  yang  bersifat  dinamis  karena  bahasa 
isyarat 
menggunakan  gestur  atau  perubahan  gestur  tubuh  untuk 
menggantikan peran bahasa sebagai alat komunikasi verbal [4]. 
Penyandang 
Indonesia 
berkomunikasi dengan menggunakan dua sistem bahasa isyarat 

tunawicara  dan 

tunarungu  di 

yaitu  SIBI  (Sistem  Isyarat  Bahasa  Indonesia)  dan  BISINDO 
(Bahasa Isyarat Indonesia) [5]. 

tunarungu/tunawicara 

tidak  memahami  bahasa 

Permasalahan  yang  masih  sering  muncul  adalah  ketika 
ingin  berkomunikasi 
penyandang 
dengan  orang  yang 
isyarat. 
Permasalahan tersebut tidak berjalan beriringan dengan Pasal 
24  Undang-Undang  Republik  Indonesia  No.  8  Tahun  2016 
Tentang  Penyandang  Disabilitas,  yang  menyebutkan  bahwa 
penyandang  disabilitas  memiliki  hak  untuk  mendapatkan 
informasi  dan  berkomunikasi  melalui  media  yang  mudah 
diakses  [6].  Permasalahan  tersebut  juga  bisa  menimbulkan 
kesalahpahaman  dan  bahkan  dapat  memengaruhi  hubungan 
interpersonal  antara  masyarakat  umum  dan  penyandang 
tunarungu/tunawicara [4].  

Tantangan  yang  masih  dihadapi  oleh  modul  penerjemah 
bahasa  isyarat  adalah  kompleksnya  gerakan  tubuh  untuk 
mengekspresikan sebuah kata, kasusnya sedikit berbeda ketika 
menerjemahkan  alfabet  dalam  bahasa  isyarat  karena  alfabet 
dalam  bahasa  isyarat  umumnya  bersifat  statis.  Bahkan  setiap 
orang mungkin memiliki ciri khas tersendiri (perbedaan bentuk 
tubuh, kecepatan gerakan, dan perbedaan tangan utama) yang 
membuat penerjemahan  bahasa  isyarat  menjadi  semakin  sulit 
untuk dilakukan.  

Modul  penerjemah  bahasa  isyarat  berkembang  semakin 
cepat dengan ditemukannya beberapa alat tambahan yang dapat 
menangkap data gerakan seperti Flex Sensor Gloves. Zakaria et 
al.  [7]  menggunakan  Flex  Sensor  Gloves  untuk  melakukan 
klasifikasi  huruf  dan  angka  dalam  SIBI  dan  mendapatkan 
akurasi  65,38%  dengan  model  k-Nearest  Neighbour  (kNN). 
Selain  Flex  Sensor  Gloves,  terdapat  alat  lain  yang  dapat 
menangkap gestur  tubuh yaitu Leap Motion. Deasy et al. [8] 
menggunakan  Leap  Motion  dan  mendapatkan  akurasi  24% 
dalam melakukan klasifikasi kata “Abang”, “Adik”, “Makan”, 
“Sabar”,  dan  “Saya”  menggunakan  Hidden  Markov  Model. 
Selain itu, juga terdapat sensor Kinect yang dikembangkan oleh 
Microsoft  yang  dapat  digunakan  untuk  menangkap  data 
gerakan.  Liu  et  al.  [9]  menggunakan  Microsoft  Kinect  dan 
mendapatkan akurasi sebesar 86% dalam melakukan klasifikasi 
100 kata Chinese Sign Language (CSL) menggunakan model 
LSTM. Microsoft Kinect juga digunakan oleh Xiao et al. [10], 
penelitian tersebut mendapatkan akurasi sebesar 80,28% dalam 
melakukan  klasifikasi  500  kata  CSL  menggunakan  model 
bidirectional LSTM. Kelemahan dari alat-alat  tersebut adalah 
biaya  produksinya  yang  mahal  dan  kurang praktis  digunakan 
dalam kehidupan sehari-hari. 

Tantangan  lain  dari  penerjemahan  bahasa  isyarat  adalah 
bagaimana  cara  memodelkan  gerakan  sekuensial  yang 
merepresentasikan  sebuah  kata.  Tantangan  tersebut  dapat 

 1 / 7 

 
 
 
 
 
diatasi  dengan  menggunakan  arsitektur  RNN  karena  RNN 
dapat  digunakan  untuk  memodelkan  data  sekuensial  [10]. 
Namun,  RNN  memiliki  masalah  krusial  yaitu  vanishing 
gradient yang dapat mempersulit proses training model RNN 
[11].  Sehingga,  LSTM  dirumuskan  dengan  tujuan  dapat 
mengurangi  masalah  vanishing  gradient  pada  RNN  ketika 
dihadapkan  oleh  data  yang  cukup  banyak  [12].  LSTM  dapat 
diaplikasikan sebagai solusi  untuk berbagai masalah machine 
learning  seperti  activity  recognition  [12],  speech  recognition 
[13],  machine 
character 
recognition[15],  dan  bahkan  sign 
language  recognition 
[9][10][16].  

translation 

optical 

[14], 

Keberhasilan  LSTM  dalam  melakukan  penerjemahan 
bahasa isyarat dibuktikan oleh Liu et al. [9], Xiao et al. [10], 
dan  Cui  et  al.  [16].  Namun,  model  LSTM  yang  digunakan 
sebagai  model  penerjemah  bahasa  isyarat  pada  penelitian 
tersebut  masih  dapat  dioptimasi  [10].  Sehingga  penelitian  ini 
juga  bertujuan  untuk  mengidentifikasi  model  terbaik  dalam 
melakukan penerjemahan bahasa isyarat. 

Pada  penelitian  ini,  modul  penerjemah  bahasa  isyarat  di 
implementasikan  dengan  menggunakan  modul  Mediapipe 
sebagai  modul  ekstraksi  fitur  tangan  dan  pose  tubuh,  dan 
menggunakan  model  LSTM  untuk  melakukan  klasifikasi 
gerakan dalam BISINDO. 

Dari 

permasalahan 

II.  TUJUAN PENELITIAN 
akan 
tersebut, 
mengembangkan  sebuah  modul  penerjemah  kata  dalam 
BISINDO yang diharapkan dapat membantu masyarakat umum 
dalam berkomunikasi dengan penyandang tunarungu.  

penelitian 

ini 

Selain  itu,  penelitian  ini  juga  akan  mencari  model  terbaik 
dalam  melakukan  klasifikasi  gerakan  bahasa  isyarat  yang 
mengacu pada standar BISINDO. 

III. PENELITIAN TERKAIT 

tantangan 

Salah  satu 

terbesar  dalam  menyelesaikan 
permasalahan penerjemahan bahasa isyarat adalah bagaimana 
menangkap  data  gerakan  gestur.  Secara  umum,  terdapat  2 
metode  untuk  menangkap  data  gerakan  gestur,  yaitu  dengan 
menggunakan  sebuah  alat  (device  based),  baik  dengan 
menggunakan sarung tangan yang ditanami sebuah sensor (flex 
sensor  gloves)  [7][17],  maupun  menggunakan  alat  berupa 
sensor  gerakan  seperti  Microsoft  Kinect  [5][9][10][18]  atau 
Leap  Motion  [8][19],  dan  metode  selanjutnya  adalah  dengan 
menggunakan pengolahan citra (vision based) [4][20][21][22]. 
Dewasa  ini,  metode  pengumpulan  data  gerakan  dengan 
pengolahan  citra  lebih  diminati.  Hal  tersebut  dikarenakan 
metode  pengolahan  citra  tidak  membutuhkan  alat  tambahan, 
sehinggabiaya menjadi lebih terjangkau dan mudah digunakan 
Di  Indonesia,  sudah  cukup  banyak  penelitian  mengenai 
sistem  penerjemah  bahasa  isyarat.  Namun,  kebanyakan  dari 
penelitian  sistem  penerjemah  bahasa  isyarat  di  Indonesia 
adalah  menerjemahkan  alfabet  yang  jarang  sekali  digunakan 
dalam  kehidupan  sehari-hari.  Oleh  karena  itu,  penelitian  ini 
bertujuan  untuk  menerjemahkan  kata  yang  digunakan  dalam 
percakapan sehari-hari sesuai standar BISINDO. Selengkapnya, 
jurnal  penelitian  yang  bersinggungan  dengan  penelitian  ini 
dicantumkan pada Tabel 1. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

TABEL I 
CONTOH KETERANGAN TABEL 

Referensi 

[5] 

[7] 

[8] 

[9] 

[10] 

[17] 

Data yang 
digunakan  

Kata “Hari”, 
“Pohon”, “Ibu”, 
“Aku”, dan 
“Kamu” dalam 
BISINDO 
Huruf dan angka 
dalam SIBI 

Kata “Abang”, 
“Adik”, 
“Makan”, 
“Sabar”, dan 
“Saya” dalam 
SIBI 
100 Kata dalam 
Chinese Sign 
Language 
500 Kata dalam 
Chinese Sign 
Language 
Huruf dalam 
BISINDO 

Ekstraksi 
Fitur 
Microsoft 
Kinect 

Model 
klasifikasi 

Random 
Forest 

Akurasi 

70% 

Flex 
Sensor 
Gloves 

Leap 
motion 

kNN 

65.38% 

24% 

Hidden 
Markov 
Model 

Microsoft 
Kinect 

LSTM 

86% 

Microsoft 
Kinect 

Bidirectional 
LSTM 

80,28% 

kNN 

40% 

Flex 
Sensor 
Gloves 

IV. METODE PENELITIAN  

A.  Metode Pengumpulan Data 

Dikarenakan minimnya ketersediaan dataset bahasa isyarat 
khususnya  BISINDO,  sehingga  pengumpulan  data  pada 
penelitian ini dilakukan secara mandiri menggunakan kamera 
ponsel pintar dengan resolusi HD 1280x720. 

Video  yang  dikumpulkan  terdiri  atas  5  kata  umum  dalam 
BISINDO  yang  biasanya  digunakan  oleh  penyandang 
tunarungu pada percakapan sehari-hari, diantaranya adalah kata 
“Dimana”, “Kapan”, “Berapa”, “Ayah”, dan “Ibu”. Setiap kata 
direkam  sebanyak  14  kali  dengan  latar  belakang  video  yang 
berbeda. 

B.  Pre-processing Data 

Sebelum  melakukan  proses  pelatihan  model  machine 
learning,  video  perlu  dipisahkan  menjadi  folder  tertentu 
menurut setiap kata.  

Selain  itu,  tahap  pre-processing  juga  mencakup  proses 
labelling  untuk  setiap  video  sebagai  acuan  bagi  model 
penerjemah untuk melakukan klasifikasi. 

Proses split data latih dan data tes dilakukan secara otomatis 
menggunakan  modul  sklearn  dengan  60%  dari  total  dataset 
sebagai data latih, 20% sebagai data testing, dan sisanya 20% 
sebagai data validation. 

C.  Ekstraksi Fitur 

Penerjemahan  kata  dalam  bahasa 

isyarat  memiliki 
kemiripan dengan proses klasifikasi action recognition, karena 
dalam bahasa isyarat membutuhkan beberapa susunan gerakan 

 2 / 7 

 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 2. Fitur tangan 

Proses ekstraksi fitur mencakup transformasi menjadi jarak 
euclidean  kombinasi  setiap  fitur  seperti  yang  diformulasikan 
pada Persamaan 1.  

𝑑(𝑥, 𝑦) =   √(𝑥2 − 𝑥1)2 + (𝑦2 − 𝑦1)2   

  (1) 

Konversi  menjadi 

jarak  euclidean  bertujuan  untuk 
mengubah  data  multi-dimensi  menjadi  data  satu  dimensi 
sehingga  mempermudah  proses  klasifikasi  [30].  Konversi 
menjadi jarak euclidean juga bertujuan mengurangi overfitting 
karena  perbedaan  tinggi  setiap  individu  dapat  berbeda  beda 
yang menyebabkan variasi koordinat x dan y dari setiap peraga 
[5].  Proses  selengkapnya  dari  ekstraksi  fitur  seperti  pada 
Gambar 3. 

untuk dapat merepresentasikan sebuah kata. Perbedaan paling 
utama  adalah  bahwa  action  recognition  membutuhkan 
keseluruhan  badan  sebagai  data  untuk  melakukan  klasifikasi 
[23], 
hanya 
membutuhkan ekspresi wajah, pose tubuh, dan gerakan tangan 
[24]. 

penerjemah 

sedangkan 

bahasa 

isyarat 

Sehingga,  untuk  merancang  sebuah  modul  penerjemah 
bahasa  isyarat  diperlukan  sebuah  modul  untuk  melakukan 
ekstraksi  ﬁtur  baik  tangan,  wajah,  maupun  pose  tubuh. 
Ekstraksi  ﬁtur  pada  penelitian  ini  menggunakan  modul 
Mediapipe yang dikembangkan oleh Google pada tahun 2019 
lalu [25]. 

Mediapipe  merupakan 

framework  untuk  membangun 
pipeline  machine  learning  yang  dapat  melakukan  inferensi 
terhadap  data  sensori  [25].  Mediapipe  memiliki  beberapa 
keunggulan  dibandingkan  modul  ekstraksi  fitur  yang  lain 
seperti OpenPose [26] dan DensePose [27]. Diantaranya adalah 
Mediapipe  dapat  dituliskan  dengan  beberapa  bahasa 
pemrograman  seperti  Python,  Javascript,  dan  C++,  bahkan 
Mediapipe  juga  menawarkan  fitur  multi-platform  sehingga 
Mediapipe  dapat  digunakan  dalam  berbagai  macam  kondisi. 
Proses inferensi menggunakan Mediapipe juga jauh lebih cepat 
dibandingkan modul ekstraksi fitur yang lain dengan delay 1,1 
milliseconds  untuk  mendeteksi  telapak  tangan  pada  iPhone 
11[28].  Selain  itu,  Mediapipe  tidak  hanya  dapat  digunakan 
sebagai  modul  ekstraksi  fitur,  namun  Mediapipe  juga  dapat 
menyelesaikan permasalahan machine learning lainnya seperti 
Face Detection, Face Mesh, Iris Detection, Object Detection, 
Instant  Motion  Tracking,  3D  Objectron,  dan  permasalahan 
machine learning lainnya [29]. 

Fitur  yang  digunakan  pada  penelitian  ini  mencakup 
koordinat x dan y dari  fitur mata kanan, mata kiri, mulut kanan 
dan mulut kiri. Kode setiap fitur dapat dilihat pada Gambar 1. 

Gambar 1. Fitur pose tubuh 

Gambar 3. Diagram alur ekstraksi fitur 

Selain  fitur  pose  tubuh,  penelitian  ini  juga  menggunakan 
fitur  tangan  yang  terdiri  atas  koordinat  x  dan  y  dari  fitur 
pergelangan tangan, pucuk ibu jari, pucuk jari telunjuk, pucuk 
jari tengah, pucuk jari manis, dan pucuk jari kelingking. Kode 
setiap fitur dapat dilihat pada Gambar 2. 

D.  Long-Short Term Memory 

RNN  merupakan  arsitektur  model  yang  cocok  untuk  data 
sekuensial,  termasuk  gerakan  yang  dapat  merepresentasikan 
kata dalam bahasa isyarat [10]. Namun, RNN memiliki masalah 
krusial yaitu vanishing gradient yang dapat mempersulit proses 
training model RNN [11]. 

 3 / 7 

 
 
 
 
 
 
 
 
 
 
Long-Short  Term  Memory  (LSTM)  merupakan  varian 
khusus  dari  RNN  yang  dikembangkan  untuk  mengatasi 
masalah vanishing gradient pada RNN ketika digunakan dalam 
environment  dalam  waktu  yang  lama  [31].  LSTM  memiliki 
prinsip kerja yang sama dengan RNN, tetapi LSTM mengganti 
hidden unit pada RNN dengan memory block yang terdiri dari 
4  komponen  yaitu  input  gate,  output  gate,  forget  gate,  dan 
memory cells [32]. Secara umum, struktur LSTM seperti pada 
Gambar 4. 

Model  yang  diusulkan  menggunakan  arsitektur  LSTM 
sebagai backbone, model terdiri atas 2 layer LSTM dengan unit 
sebesar  64  dan  128,1  layer  average  pooling,  1  layer  dropout 
regularization  untuk  mengurangi  overfitting,  dan  satu  fully-
connected  layer  dengan  fungsi  aktivasi  softmax.  Model  di 
compile dengan menggunakan Adamax sebagai optimizer, dan 
loss  function  berupa  binary  cross  entropy.  Arsitektur  model 
selengkapnya dapat dilihat pada Gambar 5. 

Gambar 4. Struktur LSTM 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Confusion  matrix  memetakan 

jumlah  data  uji  yang 
diklasiﬁkasikan dengan benar dan yang diklasiﬁkasikan dengan 
salah  dan  dapat  digambarkan  seperti  pada  Tabel  II.  Akurasi 
adalah  persentase  observasi  yang  di  klasiﬁkasikan  dengan 
benar. Akurasi dirumuskan seperti pada Persamaan 2. 

TABEL II 
Confusion Matrix 

Predicted 

True 

True 
Positive 
(TP) 
False 
Positive 
(FP) 

False 

False 
Negative 
(FN) 
True 
Negative 
(TN) 

Actual 

True 

False 

𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦 =  

𝑇𝑃

(𝑇𝑃+𝑇𝑁+𝐹𝑃+𝐹𝑁)

       (2) 

V.  KERANGKA PIKIR 

Studi  ini  berfokus  pada  pemanfaatan  Mediapipe  yang 
dikembangkan  oleh  Google  untuk  melakukan  ekstraksi  ﬁtur 
sehingga  dapat  dilakukan  proses  machine  learning  untuk 
menerjemahkan  input  berupa  video  gerakan  dalam  bahasa 
isyarat BISINDO. Secara umum, proses penerjemahan bahasa 
isyarat dapat dilihat pada Gambar 6. 

Gambar 6. Kerangka pikir penelitian 

VI. HASIL DAN PEMBAHASAN 

Gambar 5. Arsitektur Model LSTM 

E.  Teknik Evaluasi Hasil 

Evaluasi  modul  penerjemah  bahasa  isyarat  diukur  dengan 
menggunakan  akurasi  yang  dihitung  berdasarkan 
tabel 
Confusion  Matrix.  Confusion  matrix  merupakan  salah  satu 
metode  yang  dapat  digunakan  untuk  mengevaluasi  performa 
suatu  model  machine  learning  dalam  melakukan  klasifikasi 
[33].  

A.  Sistem Penerjemah Bahasa Isyarat 

Proses  pelatihan  model  dimulai  dengan  proses  membaca 
video  seperti  pada  Gambar  7.  Setelah  video  terbaca,  modul 
Mediapipe  melakukan  proses  ekstraksi  ﬁtur  tangan  dan  pose 
tubuh  seperti  yang  ditunjukkan  pada  Gambar  8.  Modul 
Mediapipe kemudian menyimpan nilai x dan y dari ﬁtur tangan 
dan pose tubuh dan kemudian melakukan penghitungan jarak 
euclidean  terhadap  setiap  kombinasi  jari  dan  pergelangan 
tangan seperti yang ditunjukkan pada Gambar 9. 

 4 / 7 

 
 
 
 
 
 
 
   
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

akan dilakukan interpolasi data dengan cara menambahkan data 
rata-rata di setiap 2 frame. 

Dataset  yang  terdiri  atas  euclidean  distance  kombinasi 
setiap  fitur,  koordinat  x  dan  y  dari  setiap  fitur,  kemudian 
dimasukkan  ke  dalam  model  LSTM  seperti  pada  Gambar  5 
untuk melakukan proses training data. 

Gambar 7. Proses capture frame dari video 

Gambar 8. Proses ekstraksi fitur tangan dan pose dari frame 

Gambar 10. Grafik akurasi training dan testing model LSTM 

Gambar 9. Proses penghitungan euclidean distance dari setiap fitur 

Untuk mencegah estimasi pose yang kurang baik, penelitian 
ini membatasi conﬁdence modul Mediapipe menjadi 0.5 baik 
dalam  proses  estimasi  pose  maupun  tangan,  proses  estimasi 
tangan  juga  dibatasi  sehingga  modul  Mediapipe  hanya  dapat 
mendeteksi  dua  tangan  dengan  jarak  terdekat  dari  kamera. 
Namun, penetapan confidence minimum berpengaruh terhadap 
dataset yang akan menjadi data latih model, karena tidak semua 
frame  dapat  mendeteksi  fitur  pose  dan  tangan  dengan 
confidence  yang  ditentukan.  Perbedaan  panjang  video  juga 
membuat  tidak  dapat  dilakukannya  proses  pelatihan  model, 
sehingga ditentukan banyaknya frame yang akan menjadi data 
latih model adalah 15.  

Gambar 11. Grafik loss training dan testing model LSTM 

Dari  Gambar  10  dan  Gambar  11,  proses  training  model 
berhenti  pada  epoch  162  dikarenakan  modul  earlystopping 
yang  mencegah  model  menjadi  overfit  terhadap  data  train. 
Modul earlystopping menggunakan parameter patience sebesar 
20 yang artinya apabila tidak ada pengurangan  loss dalam 20 
epoch  ke  depan  maka  proses  training  dihentikan  dan  model 
dianggap sudah menjadi model terbaik. Proses training model 
berhenti dengan train accuracy sebesar 100% dan test accuracy 
sebesar 85.71%. 

Video  dengan  jumlah  frame  di  mana  pose  dan  tangan 
terdeteksi  lebih  dari  15  frame  akan  dilakukan  reduksi  data 
secara  rekursif  setiap  2  frame,  untuk  video  dengan  jumlah 
frame di mana pose dan tangan terdeteksi kurang dari 15 frame 

Proses pengetesan model dilakukan dengan 20% dari total 
dataset  yang  telah  dilakukan  splitting  sebelumnya.  Hasil 
pengetesan model ditunjukkan pada Tabel III dan  didapatkan 
akurasi model sebesar 92.857%. 

 5 / 7 

 
 
 
 
 
 
 
TABEL III 
HASIL TESTING MODEL 

Nomor 

Actual 

Predicted 

Confidence 

1 

2 

3 

4 

5 

6 

7 

8 

9 

10 

11 

12 

13 

14 

Ayah 

Ibu 

Ayah 

Ibu 

0.999859 

0.999763 

Dimana 

Dimana 

0.999799 

Kapan 

Ibu 

Berapa 

Ibu 

Ayah 

Berapa 

Ayah 

Kapan 

Kapan 

Ibu 

Berapa 

Dimana 

Ayah 

Berapa 

Ayah 

Kapan 

0.999900 

0.999940 

0.999830 

0.632657 

0.995243 

0.999881 

0.999700 

0.990236 

Dimana 

Dimana 

0.999850 

Berapa 

Berapa 

0.999103 

Dimana 

Dimana 

0.999838 

B.  Identifikasi Model Terbaik 

Model penerjemah bahasa isyarat dengan arsitektur LSTM 
yang dibangun mendapatkan akurasi testing sebesar 92.857%, 
hal  tersebut  ditunjukkan  pada  Tabel  III  yang  memiliki  1 
kesalahan yaitu kata  “Ibu”  yang diterjemahkan menjadi kata 
“Dimana”.  Selain  itu,  model  penerjemah  tidak  melakukan 
kesalahan dalam klasifikasi pada kata lainnya. 

Namun,  untuk  dapat  mengidentifikasi  model 

terbaik, 
diperlukan  sebuah  pembanding  yaitu  dengan  model  yang 
pernah diteliti sebelumnya.  

Perbandingan model dilakukan dengan model LSTM yang 
dibangun  oleh  Tao  Liu,  et  al.  [9],  dan  model  yang  dibangun 
oleh  Zhang,  et  al.  [15].  Kedua  model  tersebut  dipilih  karena 
kesuksesannya  dalam  mengklasifikasikan  ratusan  kelas  dan 
memiliki arsitektur yang sama yaitu LSTM dan GRU. 

Percobaan perbandingan model dilakukan sebanyak 100 kali 
dengan  modul  earlystopping  yang  memiliki  patience  sebesar 
20. Hasil perbandingan model dapat dilihat pada Tabel IV. 

TABEL IV 
PERBANDINGAN MODEL DENGAN EARLY STOPPING 

Model 

LSTM 

LSTM [9] 

GRU [15] 

Rata-Rata Akurasi 

Rata-Rata Kecepatan 
(dalam detik) 

88.071% 

51.143% 

56.786% 

36.961 

41.371 

42.483 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Model  LSTM  yang  diajukan  pada  penelitian  ini  memiliki 
performa yang cukup baik dengan rata-rata akurasi yang lebih 
tinggi  dan  rata-rata  kecepatan  pelatihan  yang  lebih  cepat. 
Namun,  modul  earlystopping  dapat  membuat  epoch  setiap 
pelatihan  berbeda-beda  sehingga  kecepatan  pelatihan  dapat 
bervariasi. Oleh karena itu, perbandingan model dilakukan lagi 
sebanyak  100  kali dengan  100  epoch  setiap pelatihan  model. 
Hasil perbandingan model dapat dilihat pada Tabel V. 

TABEL V 
PERBANDINGAN MODEL DENGAN 100 EPOCH 

Model 

Rata-Rata Akurasi 

Rata-Rata Kecepatan 
(dalam detik) 

LSTM 

LSTM [9] 

GRU [15] 

88.928 % 

66.714% 

74% 

34.214 

58.586 

48.522 

Dari Tabel V, dapat diketahui bahwa dengan jumlah epoch 
yang  sama  yaitu  100,  model  LSTM  yang  diajukan  pada 
penelitian ini memiliki performa yang lebih baik, baik dari segi 
akurasi  maupun  kecepatan  pelatihan  model.  Sehingga  dapat 
disimpulkan  bahwa  model  LSTM  yang  diajukan  pada 
penelitian  ini  merupakan  model  terbaik  dari  segi  akurasi 
maupun dari segi efisiensi  pelatihan yang dapat diidentifikasi 
dari penelitian terdahulu. 

VII. 

PENUTUP 

Berdasarkan  hasil  dan  pembahasan  yang  telah  dituliskan 
pada bagian sebelumnya, maka dapat ditarik kesimpulan bahwa 
modul  Mediapipe  dapat  digunakan  sebagai  modul  ekstraksi 
fitur,  dan  modul  Mediapipe  memiliki  performa  yang  cukup 
baik, hal tersebut dapat dibuktikan dengan keberhasilan model 
LSTM  dalam  mengklasifikasikan  gerakan  yang  memiliki  arti 
bahwa data yang digunakan sebagai data latih merupakan data 
yang bagus. 

Selain itu, Model LSTM yang diusulkan pada penelitian ini 
juga memiliki performa yang cukup baik dengan akurasi testing 
sebesar  92.857%,  bahkan  model  yang  diusulkan  memiliki 
performa  yang  lebih  baik  dari  model  yang  telah  diteliti 
sebelumnya,  baik  dari  segi  akurasi  model  dalam  melakukan 
klasifikasi gerakan BISINDO, atau kecepatan pelatihan model. 
agar 
mengumpulkan  dataset  yang  lebih  banyak  sehingga  dapat 
memperluas kosakata yang dapat diterjemahkan oleh model. 

Diharapkan 

selanjutnya 

penelitian 

untuk 

DAFTAR PUSTAKA 
A. Atih, “KOMUNIKASI VERBAL DAN NON VERBAL 
DALAM HUBUNGAN INTERPERSONAL,” Universitas Negeri 
Jakarta, 2015. 
T. Hernawati, “Pengembangan Kemampuan Berbahasa Dan 
Berbicara Anak Tunarungu,” JASSI_anakku, vol. 7, no. 1, pp. 101–
110, 2007. 
M. Pradikja Hendra, H. Tolle, and K. Candra Brata, 
“Pengembangan Aplikasi Pembelajaran Bahasa Isyarat Berbasis 
Android Tablet,” J. Pengemb. Teknol. Inf. dan Ilmu Komput. Univ. 
Brawijaya, vol. 2, no. 8, pp. 2548–964, 2018, [Online]. Available: 
http://j-ptiik.ub.ac.id. 
R. Hartanto, A. Susanto, and P. I. Santosa, “Preliminary design of 

[1] 

[2] 

[3] 

[4] 

 6 / 7 

 
 
 
 
 
 
 
 
[22] 

[23] 

[24] 

[25] 

[26] 

[27] 

[28] 

[29] 

[30] 

[31] 

[32] 

[33] 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

M. Y. Andrian, D. Purwanto, and R. Mardiyanto, “Penerjemahan 
Bahasa Isyarat Indonesia Menggunakan Kamera pada Telepon 
Genggam Android,” J. Tek. ITS, vol. 6, no. 1, pp. 221–224, Mar. 
2017, doi: 10.12962/j23373539.v6i1.21981. 
C. Li, Z. Cui, W. Zheng, C. Xu, and J. Yang, “Spatio-Temporal 
Graph Convolution for Skeleton Based Action Recognition,” 32nd 
AAAI Conf. Artif. Intell. AAAI 2018, pp. 3482–3489, Feb. 2018, 
[Online]. Available: http://arxiv.org/abs/1802.09834. 
S. Jiang, B. Sun, L. Wang, Y. Bai, K. Li, and Y. Fu, “Skeleton 
Aware Multi-modal Sign Language Recognition,” Mar. 2021, 
[Online]. Available: http://arxiv.org/abs/2103.08833. 
C. Lugaresi et al., “MediaPipe: A Framework for Building 
Perception Pipelines,” Jun. 2019, [Online]. Available: 
http://arxiv.org/abs/1906.08172. 
Z. Cao, G. Hidalgo, T. Simon, S. E. Wei, and Y. Sheikh, 
“OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part 
Affinity Fields,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 43, 
no. 1, pp. 172–186, 2021, doi: 10.1109/TPAMI.2019.2929257. 
R. A. Güler, N. Neverova, and I. Kokkinos, “DensePose: Dense 
Human Pose Estimation in the Wild,” Proc. IEEE Comput. Soc. 
Conf. Comput. Vis. Pattern Recognit., pp. 7297–7306, 2018, doi: 
10.1109/CVPR.2018.00762. 
F. Zhang et al., “MediaPipe Hands: On-device Real-time Hand 
Tracking,” 2020, [Online]. Available: 
http://arxiv.org/abs/2006.10214. 
V. Bazarevsky, I. Grishchenko, K. Raveendran, T. Zhu, F. Zhang, 
and M. Grundmann, “BlazePose: On-device Real-time Body Pose 
tracking,” Jun. 2020, [Online]. Available: 
http://arxiv.org/abs/2006.10204. 
M. A. LIZAMANIHI, “Penerjemah Bahasa Isyarat Dengan 
Menggunakan Metode Neural Network,” POLITEKNIK 
PERKAPALAN NEGERI SURABAYA, 2019. 
T. A. Le, M. Y. Arkhipov, and M. S. Burtsev, “Application of a 
Hybrid Bi-LSTM-CRF Model to the Task of Russian Named Entity 
Recognition,” in Communications in Computer and Information 
Science, vol. 789, 2018, pp. 91–103. 
G. Lample, M. Ballesteros, S. Subramanian, K. Kawakami, and C. 
Dyer, “Neural Architectures for Named Entity Recognition,” Proc. 
2016 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. 
Lang. Technol., pp. 260–270, Mar. 2016, doi: 10.18653/v1/N16-
1030. 
A. Rohim, Y. A. Sari, and Tibyani, “Convolution neural network 
(cnn) untuk pengklasifikasian citra makanan tradisional,” J. 
Pengemb. Teknol. Inf. dan Ilmu Komput., vol. 3, no. 7, pp. 7038–
7042, 2019, [Online]. Available: http://j-ptiik.ub.ac.id/index.php/j-
ptiik/article/view/5851/2789. 

[5] 

[6] 

[7] 

[8] 

[9] 

[10] 

[11] 

[12] 

[13] 

[14] 

[15] 

[16] 

[17] 

[18] 

[19] 

[20] 

[21] 

static indonesian sign language recognition system,” in 2013 
International Conference on Information Technology and Electrical 
Engineering (ICITEE), Oct. 2013, no. 2, pp. 187–192, doi: 
10.1109/ICITEED.2013.6676236. 
M. R. Amiarrahman and T. Handhika, “Analisis dan Implementasi 
Algoritma Klasifikasi Random Forest Dalam Pengenalan Bahasa 
Isyarat Indonesia (BISINDO),” Semin. Nas. Inov. Teknol., pp. 83–
88, 2018. 
Republik Indonesia, Undang-Undang No. 8. Indonesia: https://pug-
pupr.pu.go.id/_uploads/PP/UU.%20No.%208%20Th.%202016.pdf, 
2016, p. 19. 
Z. Zakaria, R. A. Firmanyah, and Y. A. Prabowo, “Rancang bangun 
Flex Sensor Gloves untuk penerjemah Bahasa Isyarat menggunakan 
K-Nearest Neighbors,” Semin. Nas. Sains dan Teknol. Terap. VII, 
pp. 361–366, 2019, [Online]. Available: 
https://ejurnal.itats.ac.id/sntekpan/article/view/597/400. 
Deasy and N. Ma’muriyah, “Perancangan Pembuatan Aplikasi 
Pengenalan dan Penerjemah Bahasa Isyarat SIBI Menggunakan 
Leap Motion dengan Hidden Markov Models,” TELCOMATICS, 
vol. 5, no. 1, pp. 1–11, 2020, doi: 10.37253/telcomatics.v5i1.838. 
T. Liu, W. Zhou, and H. Li, “Sign language recognition with long 
short-term memory,” in 2016 IEEE International Conference on 
Image Processing (ICIP), Sep. 2016, pp. 2871–2875, doi: 
10.1109/ICIP.2016.7532884. 
Q. Xiao, M. Qin, and Y. Yin, “Skeleton-based Chinese sign 
language recognition and generation for bidirectional 
communication between deaf and hearing people,” Neural 
Networks, vol. 125, pp. 41–55, May 2020, doi: 
10.1016/j.neunet.2020.01.030. 
S. Hochreiter and J. Schmidhuber, “Long Short-Term Memory,” 
Neural Comput., vol. 9, no. 8, pp. 1735–1780, Nov. 1997, doi: 
10.1162/neco.1997.9.8.1735. 
J. Donahue et al., “Long-Term Recurrent Convolutional Networks 
for Visual Recognition and Description.,” IEEE Trans. Pattern 
Anal. Mach. Intell., vol. 39, no. 4, pp. 677–691, Apr. 2017, doi: 
10.1109/TPAMI.2016.2599174. 
A. Graves and N. Jaitly, “Towards end-to-end speech recognition 
with recurrent neural networks,” 31st Int. Conf. Mach. Learn. ICML 
2014, vol. 5, pp. 3771–3779, 2014. 
I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to Sequence 
Learning with Neural Networks,” Adv. Neural Inf. Process. Syst., 
vol. 4, no. January, pp. 3104–3112, Sep. 2014, [Online]. Available: 
http://arxiv.org/abs/1409.3215. 
X.-Y. Zhang, F. Yin, Y.-M. Zhang, C.-L. Liu, and Y. Bengio, 
“Drawing and Recognizing Chinese Characters with Recurrent 
Neural Network,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 40, 
no. 4, pp. 849–862, Apr. 2018, doi: 10.1109/TPAMI.2017.2695539. 
R. Cui, H. Liu, and C. Zhang, “Recurrent Convolutional Neural 
Networks for Continuous Sign Language Recognition by Staged 
Optimization,” in 2017 IEEE Conference on Computer Vision and 
Pattern Recognition (CVPR), Jul. 2017, vol. 2017-Janua, pp. 1610–
1618, doi: 10.1109/CVPR.2017.175. 
G. R. Mauludi, A. Setiyadi, T. Informatika, U. Komputer, and J. D. 
Bandung, “Bahasa Indonesia Bagi Tunarungu Dan Tunawicara 
Menggunakan Arduino.” 
N. P. Permatasari, “SISTEM PENERJEMAH BAHASA ISYARAT 
MENGGUNAKAN METODE CONVOLUTIONAL NEURAL 
NETWORK (CNN) BERBASIS SENSOR 2.5D,” UNIVERSITAS 
PENDIDIKAN INDONESIA, 2016. 
A. R. RAMLI, “RANCANG BANGUN SISTEM PENTERJEMAH 
KATA BAHASA ISYARAT INDONESIA (SIBI) DENGAN 
MENGGUNAKAN ALGORITMA LINEAR DISCRIMINANT 
ANALISYS BERBASIS DESKTOP,” UIN ALAUDDIN 
MAKASSAR, 2019. 
A. D. Putri, “MODEL PENERJEMAH BAHASA ISYARAT 
INDONESIA DARI VIDEO MENGGUNAKAN HYBRID 
METHOD,” UNIVERSITAS PEMBANGUNAN NASIONAL 
“VETERAN” JAKARTA, 2019. 
D. Gustiar, S. H. Sitorus, and D. M. Midyanti, “PENERJEMAHAN 
BAHASA ISYARAT MENGGUNAKAN METODE 
GENERALIZED LEARNING VECTOR QUANTIZATION 
(GLVQ),” Coding J. Komput. dan Apl., vol. 8, no. 3, p. 1, Sep. 
2020, doi: 10.26418/coding.v8i3.42156. 

 7 / 7 

 
 
 
 
"
221709671,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pengembangan Package R Small Area Estimation 
Zero Inflated Poisson 

Fadheel Wisnu Utomo (221709671,4SD1) 
Dosen Pembimbing: Ika Yuni Wulansari, S.S.T., M. Stat. 

Ringkasan— Kebutuhan akan penggunaan metode Small Area 
Estimation (SAE) makin tinggi seiring bertambahnya waktu baik 
itu sektor pemerintah maupun sektor swasta. Dalam penelitian ini 
akan dibahas tentang penggunaan metode Small Area Estimation 
Zero  Inflated  Poisson  (SAE  ZIP).  Metode  ini  dipilih  karena 
dianggap  dapat  menjelaskan  kejadian  dari  keberadaan  isian  0 
atau  kosong  pada  data  secara  berlebih  pada  data  count  (excess 
zero).  Sebagai  hasil  akhir  penelitian,  akan  dibangun  package  R 
untuk dipublikasikan agar manfaatnya lebih luas. Dalam proses 
pembangunannya, 
bangkitan 
akan  menggunakan 
berdasarkan  literasi  dengan  membangkitkan  sebuah  variabel 
bebas, variabel target dan sebuah variabel yang menampung  nilai 
vardir (variance of direct estimation). Hasil penelitian hingga saat 
ini,  metode  SAE  ZIP  memiliki  presisi  yang  lebih  baik  ketika 
dibandinkan  dengan  hasil  estimasi  langsung  dan  EBLUP  FH  . 
Sebuah package R telah berhasil dibangun dan dipublikasi dalam 
situs CRAN dengan nama ‘zipsae’, publikasi ini dilakukan agar 
algoritma yang telah dibangun dapat bermanfaat lebih luas. 
Kata Kunci— Zero-Inflated, Poisson, excess zero, zipsae. 

data 

I.  LATAR BELAKANG 

Data  statistik  sudah  menjadi  kebutuhan  dasar  di  era 
teknologi saat ini. Terutama di Indonesia sejak reformasi pada 
tahun  1998  yang  membawa  perubahan  besar  dalam  sistem 
politik  dan  pemerintah.  Baik  perubahan  sistem  sentralisasi 
menjadi sistem desentralisasi dan sistem politik yang berubah 
dari  mayoritas  tunggal  menjadi  sistem  multi-partai.  Kedua 
perubahan ini menjadi alasan akan kebutuhan data pada tingkat 
kebupaten,  kecamatan  bahkan  desa.  Untuk  merencanakan 
pembangunan dan pengambilan keputusan yang tepat sasaran, 
tentunya  dibutuhkan  data  hingga 
terkecil  untuk 
menunjangnya,  metode  SAE  mampu  menjawab  akan 
kebutuhan data ini [14] 

level 

SAE  sendiri  telah  lama  digunakan,  semenjak  keberadaan 
ilmu  Statistik  pada  abad  ke-11  di  Inggris  dan  abad  ke-17  di 
Kanada berdasarkan data sensus dan administrasinya [16]. Para 
Demografer  telah  menggunakan  berbagai  jenis  dari  estimasi 
tidak  langsung  tentang  SAE  pada  populasi  serta  karakteristik 
menarik  didalamnya.  Untuk  menggunakan  metode  ini  kita 
perlu menerapkan jenis SAE yang tepat berdasarkan jenis data 
yang digunakan dalam proses estimasi. 

Seiring  berjalannya  waktu,  tuntutan  akan  penggunaan 
metode  estimasi  tidak  langsung  seperti  small  area  estimation 
(SAE) terus meningkat. Suatu area dikatakan besar jika jumlah 
sampel  dari  area  tersebut  cukup  banyak  untuk  melakukan 
estimasi  langsung  sehingga  menghasilkan  estimator  yang 
cukup akurat. Sedangkan suatu area dikatakan kecil jika jumlah 
sampel  area  tersebut  tidak  cukup  banyak  untuk  melakukan 

estimasi langsung  [16]. Hasil estimasi tidak cukup presisi jika 
kita  tetap  melakukan  estimasi  langsung  terhadap  area  yang 
jumlah  sampelnya  tidak  cukup.  Referensi  [16]  telah  banyak 
melakukan penelitian terkait SAE dengan meminjam kekuatan 
pada  area  sekitarnya  untuk  menghasilkan  presisi  yang  lebih 
baik  sehingga  metode  ini  dapat  menjawab  permasalahan 
sebelumnya. Peminjaman kekuatan ini sendiri dapat dilakukan 
pada  area  yang  berdekatan  berdasarkan  ruang,  waktu  atau 
melalui  informasi  lainnya  yang  dianggap  berkorelasi  dengan 
variabel  amatan.  SAE  sendiri  merupakan  suatu  metode 
statistika  untuk  menduga  parameter  dengan  meminjam 
kekuatan  variabel  lain  dari  area  terdekat  atau  memiliki 
hubungan yang erat agar mendapatkan akurasi terbaik [1]. 

SAE  telah  lama  digunakan,  semenjak  keberadaan  ilmu 
Statistik pada abad ke-11 di Inggris dan abad ke-17 di Kanada 
berdasarkan  data  sensus  dan  administrasinya 
[16].  Para 
Demografer  telah  menggunakan  berbagai  jenis  dari  estimasi 
tidak  langsung  tentang  SAE  pada  populasi  serta  karakteristik 
menarik  didalamnya.  Untuk  menggunakan  metode  ini  kita 
perlu menerapkan jenis SAE yang tepat berdasarkan jenis dan 
karakteristik data yang digunakan dalam proses estimasi. 

Dalam  prakteknya  suatu  data  pada  data  count  yang  akan 
dianalisis  dapat  memiliki  keberadaan  isian  data  null  atau  0 
secara  berlebih.  Berlebih  yang  dimaksud  adalah  dari  dataset 
yang  dimiliki  terdapat  50%  atau  lebih  variabel  yang  bernilai 
null  atau  0,  hal  ini  disebut  excess  zero.  Namun,  nilai  0  ini 
memiliki arti yang bisa saja menandakan tidak ada atau tidak 
terjadinya suatu kejadian. Besarnya proporsi data yang bernilai 
null  atau  0  dapat  berakibat  pada  ketepatan  dari  inferensia. 
Dengan  demikian,  kita  dapat  mengasumsikan  bahwa  nilai 
peluang dari kejadian tersebut sangatlah kecil sehingga jumlah 
[1] 
kejadiannya  didominasi  oleh  angka  nol.  Referensi 
mengatakan  bahwa,  peubah  jumlah  kejadian  dengan  peluang 
kejadian sangat kecil biasanya diasumsikan menyebar Poisson. 
Model  zero-inflated  Poisson  (ZIP)  merupakan  salah  satu 
metode  yang  dapat  digunakan  untuk  mengatasi  masalah 
overdispersi akibat excess zeros  [3]. Referensi  [17] melakukan 
permodelan  zero-inflated  Negative  Binomial  (ZINB)  untuk 
mengatasi  masalah  overdispersi  pada 
regresi  Poisson. 
Referensi  [1]  mengatakan  bahwa,  beberapa  metode  untuk 
menangani overdispersi adalah dengan model binomial negatif, 
pendekatan quasi-likelihood dan sebaran Tweedie. 

Penelitian  terkait  yang  pernah  dilakukan  [3],  memodelkan 
suatu  persamaan  small  area  estimation  with  zero-inflated 
model  (SAE  ZIP)  yang  berbeda  dari  yang  pernah  diajukan 
sebelumnya  [5],[12]  .  Perbedaannya  terletak  pada  perhitungan 
estimasi  variabel  target,  dimana  model  yang  diajukan  oleh 

 1 / 7 

 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Arisona  dilakukan  pengurangan 
menangkap nilai nol.  

terhadap  model  yang 

Pada penelitian ini akan diterjemahkan model yang di ajukan 
[3]  kedalam  bahasa  R  dengan  menambahkan  perhitungan 
random  effect,  serta  diterapkan  kepada  data  bangkitan  dan 
dipublikasikan menjadi package agar bermanfaat lebih luas.  

II.  TUJUAN PENELITIAN 

Berdasarkan latar belakang yang telah disusun sebelumnya, 
berikut tujuan dari penelitian yang diuraikan menjadi beberapa 
poin: 

1.  Menerjemahkan  rumus  Small  Area  Estimation 
(SAE)  dengan  model  Zero-Inflated  Poisson  (ZIP) 
serta dituliskan ke dalam bahasa R. 

2.  Mengaplikasikan syntax yang telah disusun dalam 

bahasa R pada data simulasi. 

3.  Mengembangkan script menjadi sebuah package R. 

III. PENELITIAN TERKAIT 

4  Zero Inflated 

Binomial Models 
in Small Area 
Estimation with 
Application to 
Unemployment 
Data in Indonesia 

Budi 
Hartono, 
Anang 
Kurnia dan 
Indahwati 
(2017) 

Tertulis 

Komentar 

Menggunakan 
permodelan 
untuk data 
lengkap dan 
data non-zero 
didalamnya. 

5  Estimation 

Techniques for 
Regression Model 
with Zero-inflated 
Poisson Data 

Shakhawat 
Hossain dan 
Hatem A. 
Howlader 
(2015) 

No 

Judul 

1  Kajian Pendugaan 
Area Kecil Pada 
Data Overdispersi 
Menggunakan 
Regresi Zero-
Inflated Poisson 

Penulis, 
Publikasi 
Dian 
Christien 
Arisona 
(2018) 

2  Small-Area 

Estimation with 
Zero-Inflated 
Data – a 
Simulation Study 

Sabine 
Krieg, Harm 
Jan Boonstra 
dan Marc 
Smeets 
(2016) 

Metode Small 
Area 
Estimation 
(SAE) 
dengan 
model Zero-
Inflated 
Poisson (ZIP) 
merupakan 
gabungan 
dari metode 
Small Area 
Estimation 
(SAE) dan 
Zero-Inflated 
Poisson (ZIP) 
itu sendiri. 

An estimator 
is described 
that takes the 
zero inflation 
into account. 
There are two 
approaches to 
estimate the 
models: the 
frequentist 
approach and 
the 
Bayesian . 

Dalam 
makalah 
tersebut, 
dicetuskan 
bahwa 
terdapat 2 
pendekatan 
untuk 
menghitung 
estimasi data 
dengan 
kondisi Zero-
Inflated. 
Yakni 
pendekatan 
Bayesian dan 
Frequentist.  

6  Small Area 

Estimation for 
Estimating The 
Number of Infant 
Mortality Using 
Mixed Effects 
Zero Inflated 
Poisson Model 

Arie 
Anggreyani , 
Indahwati , 
dan Anang 
Kurnia 
(2016) 

Penggunaan 
metode ZIP 
pada data 
infant 
mortality. 
Yang 
dikumpulkan 
melalui 
survey DHSI. 

explore the 
SAE under 
the two-part 
random 
effects model 
for zero-
inflated data 
In recent 
years, the 
needs of 
small area 
statistics have 
greatly 
increased in 
various 
development 
sectors, such 
us the need 
for indicators 
of health, 
education, 
employment, 
and others at 
the level of 
smaller areas, 
halaman 1. 
Mengestimasi 
nilai 
parameter 
dalam 
covarians, 
halaman 2. 

dalam masing-
masing 
modelnya. 

Estimasi 
kerangka kecil 
dapat 
bermanfaat di 
segala bidang 
data. 

Menggunakan 
metode ZIP 
dengan data 
biomedis. 
Dalam tulisan 
ini, 
menggunakan 
data yang 
berasal dari 
tahun 1987 
dan 1988 
tentang 
bagaimana 
gambaran 
orang amerika 
membayar dan 
menggunakan 
layanan 
kesehatan. 

Survey DHSI 
berskala 
nasional dan 
design 
sampling-nya 
sangat 
kompleks 
sehingga 
metode 
estimasi 
langsung tidak 
dapat 
memodelkan 
data dengan 
presisi. 

3  Small Area 

Estimation for 
Zero-Inflated 
Data 

Hukum 
Chandra dan 
U. C. Sud 
(2011) 

In this article, 
unlike 
Pfeffermann 
et al. (2008), 
we adopt 
frequentist 
approach to 

Dalam tiap 
model, baik 
model untuk 
data lengkap 
dan data non-
zero pengaruh 
acak dihitung 

7  Model Regresi 

Poisson Terbaik 
Menggunakan 
Zero-Inflated 
Poisson ( Zip ) 

Ilham 
Kurniawan 
(2017) 

Model Zero-
Inflated 
Poisson (ZIP) 
lebih baik 
daripada 

Pernah 
dilakukan 
penelitian 
sebelumnya 
untuk 
mengukur 

 2 / 7 

 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Dan Zero-Inflated 
Negative 
Binomial ( Zinb ) 

model Zero-
Inflated 
Binomial 
(ZINB) 

yang mana 
lebih baik 
diantara 
metode ZIP 
dan ZINB 

𝐸(𝑦̂𝑖𝑗|𝑣̂) = 𝑃(𝛿𝑖𝑗 = 1)𝐸(𝑦𝑖𝑗|𝛿𝑖𝑗 = 1) = (1 − 𝜔̂𝑖𝑗)𝜇̂𝑖𝑗 (11) 

dimana 𝜇̂𝑖𝑗 = 𝜆̂

𝑖𝑗 sehingga 

𝐸(𝑦̂𝑖𝑗) = (1 − 𝜔̂𝑖𝑗)𝜆̂

𝑖𝑗    

       (12) 

IV. METODE PENELITIAN  

1.  Metode Analisis 

Metode  yang  digunakan  dalam  penelitian  ini  adalah  Small 
Area  Estimation  (SAE)  dengan  model  Zero  Inflated  Poisson 
(ZIP). Metode ini merupakan gabungan dari metode SAE dan 
ZIP itu sendiri [3]. Metode analisis ini diyakini dapat mengatasi 
keberadaan  dari  excess  zero  yang  terjadi  pada  data  amatan 
khususnya data count.  

Didapat  vektor  respon  dengan 𝑌 =   (𝑌1, 𝑌2, … 𝑌𝑚)𝑡  tersebar 
secara ZIP dimana 𝑖 merupakan indeks dari pengamatan serta 
ukuran contoh sebanyak m, dapat kita tulis 

𝑌𝑖 ~ {

0, 𝑑𝑒𝑛𝑔𝑎𝑛 𝑝𝑒𝑙𝑢𝑎𝑛𝑔 𝜔𝑖
 𝑝𝑜𝑖𝑠𝑠𝑜𝑛 (𝜆𝑖), 𝑑𝑒𝑛𝑔𝑎𝑛 𝑝𝑒𝑙𝑢𝑎𝑛𝑔 (1  −  𝜔𝑖)

    (1) 

Peluang munculnya r bagi 𝑌 adalah 

𝑌𝑖 ~ {

0, 𝑑𝑒𝑛𝑔𝑎𝑛 𝑝𝑒𝑙𝑢𝑎𝑛𝑔 𝜔𝑖   +   (1  −  𝜔𝑖) 𝑒𝑥𝑝(−𝜆) 
𝑝𝑜𝑖𝑠𝑠𝑜𝑛 (𝜆𝑖), 𝑑𝑒𝑛𝑔𝑎𝑛 𝑝𝑒𝑙𝑢𝑎𝑛𝑔 (1  −  𝜔𝑖)  𝑒𝑥𝑝(−𝜆)𝜆𝑟
dimana 𝑟 = 1,2, … , 𝑛𝑖, dimana 𝑛𝑖, merupakan jumlah kejadian 
pada  variabel  target  pada  wilayah  ke-𝑖,  serta 𝑖 = 1,2 …  , 𝑚. 
Parameter dari model Zero Inflated Poisson (ZIP) adalah 𝜔 =
(𝜔1, 𝜔2, … , 𝜔𝑁)𝑡  dan  𝜆 = (𝜆1, 𝜆2, … , 𝜆𝑁)𝑡 .  Adapun  𝜔  dan  𝜆 
memiliki model penghubung, yaitu  

(2) 

𝑟!

log(𝜆) = 𝐁𝛽   

(3) 

dan 

𝑙𝑜𝑔𝑖𝑡(𝜔) = 𝐆𝛾     

(4) 
dimana  B  dan  G  merupakan  matriks  peubah  penjelas  [12], 𝛽 
adalah  vektor  parameter  berukuran  (𝑝 + 1) 𝑥 1 ,  dan  𝛾  juga 
merupakan vektor parameter berukuran (𝑞 + 1) 𝑥 1 yang akan 
diduga nilainya. Adapun model untuk sebaran Poisson dengan 
model  penghubung 𝑙𝑜𝑔 adalah  bagian  data  bernilai  tidak  nol 
yang didefinisikan sebagai berikut, 

𝑡𝛽 + 𝑣𝑛𝑧,𝑖  
𝑙𝑜𝑔(𝜆𝑖𝑗) = 𝑥𝑛𝑧,𝑖𝑗
dimana 𝑖 = 1,2, … , 𝑚  , 𝑗 = 1,2, … , 𝑁𝑖 , dan 
𝑣𝑛𝑧,𝑖~𝑁(0, 𝜎2
nilai nol sebagai berikut, 

𝑣,𝑛𝑧). Referensi [12] memodelkan peluang dari 

(5) 

𝑙𝑜𝑔𝑖𝑡(𝜔𝑖𝑗) = log (

𝜔𝑖𝑗
1−𝜔𝑖𝑗
dimana 𝑖 = 1,2, … , 𝑚  , 𝑗 = 1,2, … , 𝑁𝑖 , dan 𝑣𝑧,𝑖~𝑁(0, 𝜎2

(6) 
𝑣,𝑧).  
Dari persamaan (5) dan (6) didapat penduga terbaik empirik 

𝑡𝛽𝑧 + 𝑣𝑧,𝑖 

) = 𝑥𝑧,𝑖𝑗

bagi 𝜔𝑖𝑗 dan 𝜆𝑖𝑗 adalah sebagai berikut 

𝜆̂

𝒊𝒋 = 𝑒𝑥𝑝(𝑥𝑛𝑧,𝑖𝑗

𝑡𝛽̂
𝑛𝑧   +   𝑣̂𝑛𝑧,𝑖) 
𝑡𝛽̂𝑧 + 𝑣̂𝑧,𝑖)
𝑒𝑥𝑝(𝑥𝑧,𝑖𝑗
𝑡𝛽̂𝑧 + 𝑣̂𝑧,𝑖)
1+𝑒𝑥𝑝(𝑥𝑧,𝑖𝑗
dimana 𝛽̂ adalah sebagai berikut 

𝜔̂𝒊𝒋 =

(7) 

(8) 

−1

(𝑥′𝑉̂ −1𝑦) 

̂ 𝑍′ +   𝜎2

𝛽̂ =   (𝑥′𝑉̂ −1𝑥)
𝑉̂ = 𝑍𝜎2

    (9) 
(10) 
𝑣
Perhitungan  nilai  𝛽̂  berlaku  baik  untuk  permodelan  nilai 
tidak  nol  dan  nilai  nol.  Penduga  empirik  diatas  merupakan 
kombinasi dari proporsi SAE pada data biner [5], prediksi nilai 
tengah small area dengan peubah acak Poisson, dan SAE untuk 
data  zero-inflated  [12].  Lalu  didapat  nilai  harapan  untuk  SAE 
ZIP adalah 

𝑒 

Selanjutnya, dilakukan estimasi menggunakan metode SAE 
ZIP. Dilakukan perhitungan untuk mendapat nilai presisi dari 
hasil estimasi yang telah dilakukan. Untuk mendapatkan nilai 
yang dimaksud, diwakilkan dengan nilai Relative Standard 
Error (RSE). Menghitung komponen ini diperlukan hasil dari 
estimasi itu sendiri (𝜃) dan komponen lain yakni 𝑆𝐸(𝜃). 
𝑅𝑆𝐸 =
2.  Pembangkitan Data 

𝑆𝐸(𝜃)

𝜃

(13) 

Data yang digunakan dalam penelitian ini berasal dari data 
bangkitan  untuk  mengukur  performa  dari  metode  yang 
dikembangkan [3],[4],[5],[12]. Ada pun alat ukur performa berupa 
Relative  standard  error  (RSE).  Berikut 
tahapan  dalam 
membangkitkan data: 
1.  Menentukan jumlah area pada populasi yaitu sebanyak 300. 
0,𝑧, 𝛽̂
2.  Menentukan nilai 𝛽̂
3.  Membangkitkan nilai variabel bebas (𝑥) mengikuti sebaran 

𝑣,𝑛𝑧, 𝜎2

0,𝑛𝑧, 𝛽̂

1,𝑛𝑧, 𝛽̂

1,𝑧, 𝜎2

𝑣,𝑧 . 

sebaran normal dengan rataan 30 dan simpangan baku 5. 

4.  Membangkitkan  nilai 

random 
𝑣,𝑧 yang telah ditentukan. 

𝜎2

𝑣,𝑛𝑧, 𝜎2

effect  berdasarkan 

5.  Membangkitkan  variabel  target  berdasarkan  model  pada 

persamaan (7) dan (8). 

6.  Membangkitkan suatu variabel dengan sebaran Bernoulli 
(Ω).  Sebagai  variabel 

dengan  parameter  omega 
kontaminan nilai 0. 

Gambar  1. Diagram alir membangkitkan data simulasi 

3.  Tahapan Penelitian 

Penelitian  akan  dilakukan  menggunakan  metode  Design 

Science Research (DSR) [15].  

 3 / 7 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Poisson 
(ZIP)  serta  dituliskan  ke  dalam  bahasa  R, 
mengaplikasikan  syntax  yang  telah  disusun  dalam  bahasa  R 
terhadap  data  simulasi  serta  mengembangkan  script  menjadi 
sebuah package R. 

3.  Perancangan dan pengembangan solusi 

Untuk mengatasi masalah yang ada, implementasi algoritma 
akan  dilakukan  dalam  bahasa  pemrograman  R  dan 
menggunakan aplikasi R-studio yang kemudian dipublikasikan 
sebagai paket R. Berikut disajikan proses dalam pembentukan 
package R. 

Gambar  3. Arsitektur rancangan package zipsae 

Dalam  pembangunan  package  R,  penulis  menggunakan 
library  pendukung.  Seperti  roxygen  dan  usethis.  Roxygen 
bermanfaat  dalam  membantu  mempercepat  pengerjaan 
pembangunan  package  dengan  men-generate  syntax  yang 
diperlukan  seperti  baris  dokumentasi  tentang  fungsi  yang 
dimaksud  pada  setiap  file  R.  Sedangkan  usethis  digunakan 
dalam  melakukan  dokumentasi  didalam  git.  Selain  itu  ada 
untuk 
tambahan 
mempermudah  dalam  proses  pengembangan  package. 
Singkatnya  devtools  seperti  menjadi  penghubung  antara 
roxygen dengan pengguna melalui aplikasi R-studio.  

extension 

devtools 

dengan 

nama 

4.  Pembuatan simulasi 

Setelah melakukan perancangan dan pengembangan solusi, 
akan  dilakukan  simulasi/demonstrasi  terkait  algoritma  yang 
telah  dibangun  menggunakan  data  bangkitan  yang  telah 
dijelaskan  sebelumnya.  Tujuan  melakukan  tahap  ini  adalah 
untuk  mengetahui  apakah  hasil  dari  perancangan  dan 
pengembangan sudah berfungsi sebagaimana mestinya.  

5.  Pengujian 

Dalam  tahap  ini,  dilakukan  observasi  serta  pengukuran 
terhadap  hasil  simulasi/demonstrasi  yang  telah  dilakukan 
berdasarkan hasil perancangan yang telah dikembangkan. Dari 
hasil  yang  didapat,  akan  diukur  performanya  dengan  suatu 
instrumen pengukuran.  

Sebagai  instrumen  pengukuran,  akan  digunakan  Relative 
Standard Error (RSE) pada metode yang telah dilakukan, yakni 
hasil estimasi pada metode estimasi tidak langsung, Small Area 
Estimation (SAE) dengan model Zero Inflated Poisson (ZIP). 
Relative Standard Error atau RSE sendiri merupakan hasil 
bagi antara  Standard Error  (SE) estimasi dibagi dengan nilai 
estimasi itu sendiri. Untuk mendapatkan nilai yang lebih stabil, 
dilakukan proses Jackknife untuk perhitungan RSE. 

6.  Kesimpulan 

Penarikan  kesimpulan  dari  hasil  penelitian  yang  telah 
dilakukan.  Pada  proses  ini  pengambilan  kesimpulan  akan 
diambil  berdasar  hasil  yang 
telah  didapat  dari  proses 
sebelumnya.  Dapat  diketahui  bahwa  jika  terjadi  ketidak 

 4 / 7 

Gambar  2. Diagram alir design science research (DSR) 

Penjabaran  metode  berdasar  diagram  alir  diatas  adalah 

sebagai berikut: 
1. 

Identifikasi masalah dan motivasi 

Proses identifikasi masalah dilakukan melalui studi literatur 
dan konsultasi dengan dosen ahli yang terkait dalam penelitian 
SAE. Dalam proses ini teridentifikasi bahwa keberadaan nilai 
nol pada data count cukup menimbulkan masalah baik dari segi 
analisis dan tidak tepatnya hasil inferensia yang didapat. 

SAE 

sendiri 

sedang  dikaji  penggunaannya  dalam 
menganalisis  oleh  BPS,  dimana  penggunaannya  sangatlah 
membantu  dalam  penyelesaian  dalam  melakukan  estimasi. 
Khususnya  kasus  pada  Indonesia  sendiri,  untuk  melakukan 
sensus dibutuhkan tenaga, waktu dan dana yang lebih banyak 
dibandingkan  dengan  survei.  Untuk  memproduksi  statistik 
dasar, BPS melakukan berbagai survei untuk menunjang data 
yang dibutuhkan. Harapan kedepannya, penggunaan SAE pada 
tahap  analisis  dalam  memproduksi  statistik  dasar  maupun 
statistik resmi dapat digunakan dan umumnya lebih luas lagi.  
Pada  data  count  dapat  terjadi  kondisi  excess  zero.  Untuk 
mengatasi hal tersebut digunakan metode SAE dengan model 
Zero  Inflated  Poisson  (ZIP)  yang  dinilai  dapat  mengatasi 
kondisi excess zero ini. 

Untuk  membangun  algoritma  SAE  dengan  model  Zero 
Inflated Poisson (ZIP) ini tentunya membutuhkan waktu, selain 
waktu  dibutuhkan  pemahaman  tentang  metode  tersebut  dan 
karena kurangnya atau terbatasnya literasi tentang metode ini 
dapat  mengakibatkan  perbedaan  pemahaman  pada  setiap 
individu sehingga dapat menimbulkan perbedaan diantaranya. 
Sehingga  mempublikasikan  metode  tersebut  dalam  bahasa  R 
yang  dikemas  dalam  paket  R  akan  sangat  berguna  dan 
memecahkan masalah ini. 

2.  Penentuan tujuan dari penelitian 

Tujuan  dari  penelitian  ini  adalah  menerjemahkan  rumus 
Small  Area  Estimation  (SAE)  dengan  model  Zero-Inflated 

 
 
 
 
sesuaian  terhadap  hasil  atau  proses  yang  diinginkan,  maka 
dapat  melakukan  revisi  atau  merubah  rancangan  dari  solusi 
yang diinginkan. 

V.  KERANGKA PIKIR 

Gambar    dibawah  ini  merupakan  alur  pikiran  bagaimana 
penelitian ini terjadi. Awalnya, ditemukan masalah pada hasil 
survey khususnya yang mencacah data count karena ditemukan 
isian dengan nilai 0 banyak. 

Untuk mengolah data dengan isian 0 yang banyak ini cukup 
menjebak  dan  tidak  begitu  memiliki  literasi  yang  banyak. 
Khususnya  saat  di-implementasikan  dengan  Small  Area 
Estimation (SAE). Apabila memutuskan untuk menghapus data 
dengan  nilai  isian  nol  ataupun  melakukan  estimasi  langsung 
terhadap data dengan nilai isian nol yang banyak, maka dapat 
menyebabkan tidak tepatnya inferensia yang akan dilakukan. 

Gambar  4. Diagram alir kereangka pikir penelitian 

Dengan melihat hal tersebut diputuskan untuk mencari suatu 
metode  yang  dapat  mengatasi  permasalahan  tersebut  baik 
melalui  pencarian  literasi  maupun  konsultasi  ke  pihak  yang 
berkompeten.  Ditemukan  ada  beberapa  metode,  diantaranya 
ada Zero-Inflated Negative Binomial (ZINB) dan Zero-Inflated 
Poisson  (ZIP)  [13].  Sebagai  keputusan,  dipilih  metode  Zero-
Inflated Poisson (ZIP) [3] karena dinilai karakteristiknya yang 
sering ditemui dalam data isian yang dimiliki oleh Badan Pusat 
lebih 
Statistik  atau  BPS 
implementatif, sehingga dengan alasan tersebut-lah di putuskan 
untuk  mengembangkan  dan  meng-implementasikan  metode 
Small  Area  Estimation  (SAE)  dengan  model  Zero-Inflated 
Poisson (ZIP). 

itu  sendiri  dengan  harapan 

VI. HASIL DAN PEMBAHASAN 

Sebagai hasil dari proses yang telah dilakukan hingga  saat 
ini  menerapkan  metode  estimasi  tidak  langsung,  Small  Area 
Estimation  Zero-Inflated  Poisson  (SAE  ZIP)  terhadap  data 
bangkitan menggunakan algoritma yang telah dibangun. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar  5. Tangkapan layar dari algoritma dalam bahasa R 

Gambar 5 merupakan hasil dari  screenshoot dari potongan 
algoritma yang telah diterjemahkan kedalam bahasa R. Dalam 
simulasi  yang  akan  dilakukan  berikut  nilai  variabel  yang 
digunakan  [3],[4],[5],[12]  dalam  membangkitkan  data  terlampir 
dalam  tabel  1.  Setelah  data  dibangkitkan  dan  dilakukan 
estimasi  terhadap  data  tersebut,  digunakan  suatu  metode 
pengukuran  Relative  Standard  Error  (RSE)  untuk  mengukur 
presisi dari hasil estimasi yang akan didapat. 

TABEL I 

NILAI VARIABEL DATA BANGKITAN  
Variabel 
𝛽0,𝑛𝑧 

Nilai 
1 

𝛽0,𝑧 

𝛽1,𝑛𝑧 

𝛽1,𝑧 

𝜎2

𝑣,𝑛𝑧 

𝜎2

𝑣,𝑧 

vardir 

Ω 

-1.59 

-0.006 

0.015 

0.08 

0.2 

0.3 

0.1 

Gambar  6. Screenshoot dari hasil data bangkitan 

 5 / 7 

 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

TABEL II 

NILAI RATA-RATA RSE  

METODE 
DIRECT 
ESTIMATE  

Nilai rata-rata RSE 
0.3345368 

EBLUP FH 

0.274458 

SAE ZIP 

0.04016407 

Apabila dilihat secara rata-rata, nilai RSE yang dihasilkan 
dengan  metode  SAE  ZIP  memiliki  nilai  yang  paling  rendah 
dengan  nilai  0.  04016407  atau  4%.  Sedangkan  nilai  rata-rata 
RSE untuk metode estimasi langsung dan EBLUP FH berturut 
turut adalah 0.3345368 dan 0.274458 atau 33% dan 27%. 

Gambar  8. Tangkapan layar situs CRAN package zipsae 

Pada gambar 8 diberikan tangkapan layar yang berasal dari 
situs  CRAN.  Dapat  dilihat  bahwa  package  zipsae  dapat  di-
install  melalui  aplikasi  R-studio.  Selain  tersedia  dalam 
repository  CRAN,  versi  terbaru  dari  package  juga  selalu 
tersedia dalam git dan dapat diakses melalui tautan berikut ini 
https://github.com/dheel/zipsae.  

VII. 

PENUTUP 

Dari  hasil  penelitian  yang  telah  dilakukan  hingga  saat  ini, 

dapat disimpulkan bahwa: 

1.  Metode estimasi tidak langsung Small Area Estimation 
telah  berhasil 

(SAE  ZIP) 

Zero-Inflated  Poisson 
diterjemahkan kedalam bahasa R. 

2.  Mengaplikasikan  script  yang  telah  dibangun  terhadap 

data bangkitan berhasil dilakukan. 

3.  Script  yang  telah  dibangun  sudah  dikembangkan  lebih 
lanjut  menjadi  suatu  package  R  dengan  nama  ‘zipsae’ 
dan  dapat  diunduh  melalui  situs  CRAN  atau  melalui 
tautan berikut https://cran.r-project.org/package=zipsae.  
Dalam  penyusunan  makalah  sidang  ini  terdapat  perubahan 
ketika  dibandingkan  dengan  makalah  seminar.  Adapun 
perubahan  yang  terjadi  adalah  perbedaan  metode  yang 
digunakan dalam melakukan estimasi dan pembangkitan data. 
Hal  ini  terjadi  karena  penggunaan  metode  estimasi  dan 
pembangkitan  data  pada  proses  sebelumnya  dinilai  salah  dan 
kurang  tepat,  sehingga  butuh  perbaikan  berdasarkan  literatur 
yang ada. 

 6 / 7 

Gambar  7. Diagram baris resampling jackknife 

Pada  gambar  6 merupakan  tangkapan  layar  dari  data  yang 
telah  dibangkitkan  menggunakan  variabel  yang 
telah 
ditetapkan  sebelumnya.  Dapat  dilihat  bahwa  variabel  yang 
dibangkitkan berjumlah 3, variabel target, variabel bebas dan 
sebuah  variabel  yang  menyimpan  nilai  vardir  (variance  of 
direct estimation). 

Setelah  melakukan  pembangkitan  data, 

selanjutnya 
dilakukan  estimasi  menggunakan  metode  Small  Area 
Estimation Zero-Inflated Poisson  (SAE ZIP). Adapun seperti 
yang  telah  diketahui  bahwa  penghitungan  error  pada  SAE 
adalah masalah yang luar biasa [5]. Sehingga dalam menghitung 
Relative  Standard  Error 
(RSE)  dilakukan  pendekatan 
Jackknife yang hasilnya dapat dilihat dari gambar 7.  

Pada  gambar  7  terdapat  tiga  warna  didalam  diagram  garis 
yang  merepresentasikan  metode  masing-masing.  Untuk  garis 
berwarna  merah  adalah  nilai  RSE  yang  didapat  dari  hasil 
perhitungan  estimasi 
langsung,  garis  berwarna  hijau 
merupakan nilai RSE hasil dari estimasi menggunakan metode 
Small  Area  Estimation  (SAE)  atau  Empirical  Best  Linier 
Unbiased  Prediction  Fay-Herriot  (EBLUP  FH).  Sedangkan 
garis  dengan  warna  biru  adalah  nilai  RSE  yang  didapat  dari 
hasil  estimasi  menggunakan  metode  Small  Area  Estimation 
Zero-Inflated Poisson (SAE ZIP). 

Diantara  ketiga  metode,  nilai  RSE  dari  metode  estimasi 
langsung  atau  direct  estimation  mendapat  nilai  tertinggi 
diantara  dua  metode  lainnya.  Meski  beberapa  titik  didalam 
diagram  tersebut  nilainya  terdapat  nol.  Selanjutnya,  terdapat 
garis  berwarna  hijau  didalam  diagram.  Jika  kita  lihat,  garis 
hijau terletak diantara garis berwarna merah dan biru, dibawah 
garis  merah  dan  diatas  garis  biru.    Dengan  kata  lain,  hasil 
estimasi yang dihasilkan melalui metode EBLUP FH lebih baik 
daripada  metode  estimasi  langsung  atau  direct  estimation, 
namun tidak lebih baik daripada metode estimasi menggunakan 
metode  Small  Area  Estimation  Zero-Inflated  Poisson  (SAE 
ZIP).  

garis 

adalah 

Warna 

selanjutnya, 

yang 
merepresentasikan hasil RSE dari metode SAE ZIP. Garis biru 
terletak  dibawah  garis  merah  dan  hijau.  Hal  ini  dapat  kita 
simpulkan bahwa metode SAE ZIP lebih baik daripada metode 
yang diwakilkan oleh warna merah dan hijau (direct estimate 
dan EBLUP FH).  

biru 

 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

[22] Widiarti, R. R. Periwi, and A. Sutrisno, “Perbandingan Mean Squared 

Error ( MSE ) Metode Prasad-Rao dan Jiang-Lahiri-Wan Pada Pendugaan 
Area Kecil,” vol. 2, no. 2502, pp. 56–60, 2017. 

DAFTAR PUSTAKA 

[1]  A. Anggreyani, Indahwati, and A. Kurnia, “Small Area Estimation For 
Estimating The Number of Infant Mortality Using Mixed Effects Zero 
Inflated Poisson Model,” pp. 108–115, 2015. 

[2]  L. Amaliana and E. D. Lusiana, “Penerapan Metode Empirical Best 

Linear Unbiased Prediction ( EBLUP ) pada Model Fay-Herriot Small 
Area Estimation ( SAE ),” Pros. Semin. Nas. Integr. Mat. dan Nilai 
Islam., vol. 1, no. 1, pp. 312–319, 2017, [Online]. Available: 
http://conferences.uin-
malang.ac.id/index.php/SIMANIS/article/view/108. 

[3]  D. C. Arisona, “Kajian Pendugaan Area Kecil Pada Data Overdispersi 

Menggunakan Regresi Zero-Inflated Poisson,” Institut Pertanian Bogor, 
2018. 

[4]  R. Benavent and D. Morales, “Multivariate Fay-Herriot models for small 

area estimation,” Comput. Stat. Data Anal., vol. 94, pp. 372–390, 2016, 
doi: 10.1016/j.csda.2015.07.013. 

[5]  H. Chandra and U. C. Sud, “Small area estimation for zero-inflated data,” 
Commun. Stat. Simul. Comput., vol. 41, no. 5, pp. 632–643, 2012, doi: 
10.1080/03610918.2011.598991. 

[6]  G. S. Datta, J. N. K. Rao, and D. D. Smith, “Erratum: On measuring the 
variability of small area estimators under a basic area level model 
(Biometrika (2005) 92 (183-96)),” Biometrika, vol. 99, no. 2, p. 509, 
2012, doi: 10.1093/biomet/ass016. 

[7]  N. P. P. Dewanti, M. Susilawati, and I. G. A. M. Srinadi, “Perbandingan 
Rregresi Zero Inflated Poisson (ZIP) dan Rregresi Zero Inflated Negative 
Binomial (ZINB) Pada Data Overdispersion (Studi Kasus: Angka 
Kematian Ibu di Provinsi Bali),” E-Jurnal Mat., vol. 5, no. 4, p. 133, 
2016, doi: 10.24843/mtk.2016.v05.i04.p132. 

[8]  B. Hartono and A. Kurnia, “Zero Inflated Binomial Models in Small Area 
Estimation with Application to Unemployment Data in Indonesia,” IJCSN 
-International J. Comput. Sci. Netw., vol. 6, no. 65, pp. 746–752, 2017. 
[9]  S. Hossain and H. A. Howlader, “Estimation Techniques for Regression 
Model with Zero-inflated Poisson Data,” Int. J. Stat. Probab., vol. 4, no. 
4, 2015, doi: 10.5539/ijsp.v4n4p64. 

[10] N. Istiana, “Excess Zero pada Regresi Poisson,” 2011. 

https://nofitaistiana.wordpress.com/2011/08/12/excess-zero-pada-regresi-
poisson/  (accessed Nov. 02, 2020). 

[11] N. Istiana, A. Kurnia, and A. Ubaidillah, “Quasi Poisson Model for 

Estimating Under-Five Mortality Rate in Small Area,” no. January, 2020, 
doi: 10.4108/eai.2-8-2019.2290343. 

[12] S. Krieg, H. J. Boonstra, and M. Smeets, “Small-area estimation with 

zero-inflated data – a simulation study,” J. Off. Stat., vol. 32, no. 4, pp. 
963–986, 2016, doi: 10.1515/JOS-2016-0051. 

[13] I. Kurniawan, “Model Regresi Poisson Terbaik Menggunakan Zero-

Inflated Poisson ( Zip ) Dan Zero-Inflated Negative Binomial ( Zinb ),” 
Semarang National University, 2017. 

[14] R. Ningtyas, R. Rahmawati, and Y. Wilandari, “Penerapan Metode 

Empirical Best Linear Unbiased Prediction (Eblup) Pada Model Penduga 
Area Kecil Dalam Pendugaan Pengeluaran Per Kapita Di Kabupaten 
Brebes,” J.  Gaussian, vol. 4, no. 4, pp. 977–986, 2015, [Online]. 
Available: http://ejournal-s1.undip.ac.id/index.php/gaussian. 

[15] K. Peffers, T. Tuunanen, M. A. Rothenberger, and S. Chatterjee, “A 

design science research methodology for information systems research,” 
J. Manag. Inf. Syst., vol. 24, no. 3, pp. 45–77, 2007, doi: 
10.2753/MIS0742-1222240302. 

[16] J. N. K. Rao and I. Molina, Small Area Estimation, Second. New Jersey: 

John Wiley & Sons, Inc., 2015. 

[17] A. . Sharma and V. . Landge, “Zero Inflated Negative Binomial for 

Modeling Heavy Vehicle Crash Rate on Indian Rural Highway,” Int. J. 
Adv. Eng. Technol., vol. 5, no. 2, pp. 292–301, 2013. 

[18] A. Ubaidillah, K. A. Notodiputro, A. Kurnia, and I. W. Mangku, 
“Multivariate Fay-Herriot models for small area estimation with 
application to household consumption per capita expenditure in 
Indonesia,” J. Appl. Stat., vol. 46, no. 15, pp. 2845–2861, 2019, doi: 
10.1080/02664763.2019.1615420. 

[19] B. Wahyu, “Zero Inflated Binomial Models In Small Area,” Bogor 

Agricultural University, 2017. 

[20] J. L. Whitten and L. D. Bentley, System Analsis & Design Methods, 7th 

ed. New York: McGraw-Hill Irwin, 2007. 

[21] J. Jiang, P. Lahiri, and S. M. Wan, “A unified jackknife theory for 

empirical best prediction with M-estimation,” Ann. Stat., vol. 30, no. 6, 
pp. 1782–1810, 2002, doi: 10.1214/aos/1043351257. 

 7 / 7 

 
 
 
 
 
 
"
221709654,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Aplikasi Text Mining terhadap Perilaku 
Cyberbullying berdasarkan Karakteristik Demografi 
Pengguna Twitter di Indonesia  

Eka Putri S (221709654, 4SD1) 
Dosen Pembimbing: Budi Yuniarto, SST., M.Si. 

Ringkasan— Cyberbullying menjadi salah satu isu sosial yang 
perlu diperhatikan dalam penggunaan media sosial yang semakin 
meningkat  dan  tidak  bisa  dihindari.  Media  sosial  sebagai  alat 
komunikasi terhubung secara luas dengan siapa pun memberikan 
semakin  banyak  kesempatan  untuk  berekspresi  dengan  batasan 
penggunaan bahasa yang  minim. Penelitian ini bertujuan untuk 
mengidentifikasi  teks  cyberbullying  pada  studi  kasus  Twitter 
dengan  menggunakan  metode  Lexicon  Based  dan  Multinomial 
Naïve Bayes. Metode Multinomial Naïve Bayes dilakukan dengan 
teknik semi-supervised learning untuk menghasilkan pseudo-label 
pada  data  yang  berjumlah  banyak.  Penelitian 
juga 
memprediksi  karakteristik  demografi  pengguna  mencakup 
gender  dan  kelompok  umur.  Untuk  mengetahui  gambaran  dari 
pola pengelompokkan cyberbullying, dilakukan analisis deskriptif 
terhadap  data 
  Metode 
Multinomial Naïve Bayes dengan teknik  semi-supervised learning 
menunjukkan akurasi yang lebih baik daripada metode Lexicon 
Based  dalam  mengklasifikasi  twit  cyberbullying.  Hasilnya  pada 
kurun  waktu  Januari  2021,  cyberbullying  di  Twitter  didominasi 
oleh gender laki-laki dan kelompok usia di bawah 30 tahun. 

twit  yang  sudah  dikumpulkan. 

ini 

Kata  Kunci—  Cyberbullying,  Text  Mining,  Klasifikasi, 

Multinomial Naïve Bayes, Lexicon Based 

I.  LATAR BELAKANG 

Peran media sosial sebagai alat komunikasi utama semakin 
membuka  celah  bagi  pelaku  cyberbullying.  Semakin  mudah 
akses  yang  diberikan  oleh 
teknologi  digital  bagi  para 
penggunanya  di  seluruh  dunia  semakin  pula  memberikan 
peluang  untuk  terjadinya  konflik  dalam  berkomunikasi  di 
internet.  Salah  satunya  adalah  kebebasan  berekspresi  yang 
memperkecil batasan dalam penggunaan bahasa di media sosial. 
Kebebasan dan anonimitas yang difasilitasi oleh media sosial 
membuat  kaum  muda  rentan  dengan  perilaku  perundungan 
maya dan menjadi salah satu ancaman pada isu sosial [1]. 

Berdasarkan  data  dari  Global  Digital  Reports,  baik 
pengguna internet dan pengguna media sosial di Indonesia terus 
meningkat.  Pada  tahun  2020,  pengguna  internet  meningkat 
sebanyak 17% dan pengguna media sosial meningkat sebanyak 
6,67% dari tahun sebelumnya. Waktu rata-rata yang digunakan 
di dunia maya adalah 7 jam 59 menit per hari melampaui rata-
rata global dengan waktu 6 jam 43 menit di internet per harinya 
[2]. 

Di Indonesia, Twitter menjadi salah satu dari lima platform 
media sosial dengan pengguna terbanyak [3]. Dirilis pada tahun 

2006,  Twitter  memberikan  “tweet”  sebagai  fitur  utamanya. 
Istilah  “tweet”  berarti  kicauan  yang  merujuk  kepada  tulisan 
status  pengguna  dengan  jumlah  karakter  yang  dibatasi.  Salah 
satu  kegiatan  yang  banyak  dilakukan  di  Twitter  adalah 
mendiskusikan berbagai topik dan isu yang sedang berlangsung. 
Dengan berbagai fiturnya, Twitter menjadi media sosial yang 
memudahkan  penggunanya  untuk  menjangkau  pengguna  lain 
tanpa  harus  berteman  secara  daring.  Kemudahan 
ini 
memberikan  kebebasan  bagi  siapa  pun  untuk  berkomentar.  
Perbedaan  pendapat  yang  terjadi  berpotensi  untuk  memicu 
perselisihan  antarpengguna.  Kebebasan  berekspresi  di  dunia 
maya  mengakibatkan  pengguna  lebih  mudah  melakukan 
perundungan  tanpa  harus  takut  akan  reaksi  langsung  dari 
seseorang  yang  menjadi  targetnya.  Sebagai  media  sosial, 
Twitter masih belum bisa menciptakan lingkungan yang ideal 
dan tidak jarang  membawa ketidaknyamanan bagi pengguna-
penggunanya.  Masih  banyak  konten  yang  tidak  diinginkan 
bersifat menyinggung dan menyakiti di jejaring sosial Twitter. 
Menurut  laporan  dari  Virtual  Police,  sampai  dengan  Maret 
2021,  media  sosial  yang  mengandung  ujaran  kebencian 
terbanyak  adalah  Twitter  [4].    Selain  itu  berdasarkan  data 
laporan  dari  Patroli  Siber,  laporan  konten  negatif  pada  tahun 
2020 meningkat hampir tiga kali lipat dari tahun sebelumnya. 
Laporan  tidak  hanya  untuk  cyberbullying  namun  lebih  luas 
terkait penipuan, hoax, pornografi dan sebagainya [3]. 

Cyberbullying  merupakan  isu  yang  serius  pada  kehidupan 
sosial yang perlu mendapatkan perhatian karena efeknya dapat 
menyebabkan  depresi,  harga  diri  rendah,  tidak  mampu 
berkonsentasi  di  kelas,  turunnya  nilai  akademik,  cemas,  dan 
bahkan  bunuh  diri  [5].  Dampak  yang  diterima  korban 
cyberbullying dapat menyerang psikologis yakni stres, depresi, 
kesepian,  ansietas,  dan  gejala  psikosomatis  bagi  korban  [6]. 
Berdasarkan  survei  yang  dilakukan  oleh  FIKKes  Universitas 
Muhammadiyah  Semarang,  74,3%  mahasiswa  korban 
cyberbullying mengalami depresi ringan dan 18,6% mengalami 
depresi sedang [7]. 

Dengan  berbagai  kerugian  yang  ditimbulkan,  membiarkan 
masalah ini terjadi dapat mengakibatkan lebih banyak korban 
yang mengalami hal tersebut. Selain itu, membiarkan perilaku 
ini terjadi terus menerus tanpa adanya tindakan yang jelas dapat 
menurunkan  standar  norma  berperilaku  di  sosial  media 
khususnya  Twitter.    Sebagai  langkah  dasar  untuk  mengatasi 
masalah  perundungan  maya  di  Twitter,  penelitian 
ini 
bermaksud  untuk  memberikan  gambaran 
terhadap  pola 
perilaku  perundungan  maya  di  jejaring  sosial  Twitter.  Dalam 

 1 / 8 

 
 
 
 
 
 
penelitian  ini  akan  dibangun  model  klasifikasi  data  Twitter 
untuk  melihat  apakah  sebuah 
twit  mengandung  unsur 
perundungan atau tidak. Penelitian ini juga melakukan prediksi 
terhadap  karakteristik  demografi  pengguna 
twitter  yang 
menggunakan  kata  kunci  cyberbullying  dalam  twit  yang 
diunggahnya. 

Dari  penelitian  ini  diharapkan  akan  diketahui  gambaran 
perilaku  cyberbullying  berdasarkan  karakteristik  demografi 
gender  dan  kelompok  umur.  Hasil  dari  penelitian  ini  dapat 
digunakan  sebagai  pengetahuan  terhadap  pola  komunikasi 
masyarakat  khususnya  dalam  kasus  cyberbullying  di  Twitter. 
ini  dapat  menjadi 
Diharapkan  untuk  kedepannya  data 
pertimbangan  sebagai  dasar  untuk  pembangunan  kebijakan 
terkait penggunaan media sosial. Penulis mengharapkan hasil 
dari  penelitian  ini  dapat  meningkatkan  kesadaran  pengguna 
sosial media terhadap isu perundungan maya dan menurunkan 
potensi 
tersebut  untuk  menciptakan 
lingkungan berkomunikasi yang lebih baik. 

terjadinya  perilaku 

II.  TUJUAN PENELITIAN 

Berdasarkan identifikasi masalah di atas, tujuan yang ingin 

dicapai dari penelitian ini adalah sebagai berikut. 
1.  Memprediksi  karakteristik  demografi  pengguna  Twitter 
mencakup gender, kelompok umur, serta jenis akun pribadi 
atau organisasi.  

2.  Melakukan  klasifikasi  twit  cyberbullying  dengan  metode 

Lexicon Based dan Multinomial Naïve Bayes. 

3.  Melakukan 

cyberbullying 
pengguna Twitter. 

analisis  deskriptif 
berdasarkan 

terhadap  data 

twit 
demografi 

karakteristik 

III. PENELITIAN TERKAIT 

Beberapa  penelitian  sebelumnya  telah  membahas  tentang 
cyberbullying  di  media  sosial  Twitter.  Penelitian  [8]  telah 
menganalisis  kata-kata  bullying  Bahasa  Indonesia  di  Twitter 
dan  menemukan  pola  penindasan  di  Indonesia  dengan 
menggunakan text mining. Penelitian ini juga membandingkan 
pola  bullying  di  Jakarta  dan  Surabaya  serta  mengidentifikasi 
beberapa  istilah  yang  merupakan  tren  pola  bullying  di 
Indonesia di Twitter.  

satu 

Salah 

dokumen 

penelitian 

klasifikasi 

yang 
menggunakan teknik semi-supervised learning dilakukan oleh 
[9].  Penelitian  ini  menggunakan  teknik  Multinomial  Naïve 
Bayes  dan  Pseudo  Labeling  dalam  mengklasifikasikan 
dokumen  terkait  medis.  Penggunaan  metode  Multinomial 
Naïve  Bayes 
dalam 
mengklasifikasi  dokumen  berhasil  digunakan  pada  penelitian 
ini dengan akurasi paling tinggi 85%. 

semi-supervised 

learning 

dan 

Penelitian [10] menggunakan metode Lexicon Based untuk 
mengklasifikasi dokumen. Analisis sentimen digunakan untuk 
mengevaluasi tingkat kinerja pengajaran berdasarkan komentar 
umpan  balik  tekstual  dari  siswa.  Database  kata  sentimen 
bahasa Inggris dibangun untuk mengidentifikasi polaritas kata 
sebagai  sumber  leksikal.  Setiap  kata  opini  dalam  database 
diberi  nilai.  Nilai  sentimen  berkisar  antara  −3  hingga  +3. 
Metode ini menganalisis secara otomatis komentar umpan balik 
siswa ke kategori negatif, positif, atau netral. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

IV. METODE PENELITIAN  

A.  Metode Pengumpulan Data 

Data  diperoleh  dari  hasil  crawling  menggunakan  library 
twint pada bulan Januari 2021. Twit yang diambil adalah twit 
berbahasa Indonesia dengan kata kunci yang tertera pada Tabel 
I. Data disimpan dalam bentuk csv dengan masing-masing kata 
kunci  yang  kemudian  digabungkan  dan  disimpan  dalam  satu 
file  yang  sama.  Twit  yang  dikumpulkan  dalam  penelitian  ini 
berdasarkan 13 kata kunci yang diambil dari penelitian [8]. 

TABEL I 
KATA KUNCI YANG DIGUNAKAN DALAM PENELITIAN 

No 
(1) 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 

Kata Kunci 
(2) 

Anjing 
Babi 
Bajingan 
Bangsat 
Brengsek 
Buta 
Gembel 
Goblok 
Jelek 
Keparat 
Monyet 
Setan 
Tolol 

Seluruh  kata  kunci  berdasarkan  penelitian  [8]  diperiksa 
terlebih  dahulu  secara  manual  dengan  menggunakan  Twitter 
Search Bar, apakah twit-twit tersebut relevan atau tidak. Kata 
kunci yang diambil untuk digunakan pada penelitian ini adalah 
kata  kunci  yang  memiliki  twit  cyberbullying  yang  paling 
relevan. 

sosial 

Inggris  pada  media 

Pemakaian  Bahasa  Inggris  oleh  penduduk  Indonesia  di 
media  sosial  sudah  menjadi  tren  di  era  globalisasi  ini. 
Khususnya  generasi  Y  dan  Z  cenderung  memiliki  preferensi 
penggunaan  Bahasa 
[11]. 
Berdasarkan  hal 
tersebut,  memungkinkan  bagi  pelaku 
cyberbullying  untuk  menulis  twit  dalam  Bahasa  Inggris  di 
Twitter. Namun pada penelitian ini, kata kunci yang digunakan 
hanya  kata  kunci dalam  Bahasa Indonesia.  Dalam praktiknya 
library  twint  yang  digunakan  dalam  proses  crawling  tidak 
memberikan  hasil  terbaik  untuk  mengambil  variabel  lokasi 
sehinga  penambahan  kata  kunci  Bahasa 
Inggris  akan 
mengikutsertakan  twit  dari  pengguna  negara  lain.  Karena 
penelitian  ini  difokuskan  untuk  perilaku  cyberbullying  di 
Indonesia,  maka  peneliti  tetap  mempertahankan  kata  kunci 
yang ada sebagai dasar pengambilan data  twit. Variabel yang 
dikumpulkan dari Twitter dengan proses crawling ditunjukkan 
pada Tabel II. 

 2 / 8 

 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

TABEL II 
VARIABEL YANG DIKUMPULKAN 

No 
(1) 
1 

id 

Variabel 
(2) 

2 
3 

4 
5 
6 

7 

8 

9 

user id 
username 

date 
time 
tweet 

likes_count 

replies_count 

retweet_count 

Keterangan 
(3) 
Id unik untuk twit yang memiliki 
panjang 19 karakter 
id unik untuk pengguna 
Pengguna  yang  telah  memiliki 
akun pada media social twitter 
Tanggal twit diunggah pengguna 
Waktu twit diunggah pengguna  
Teks  dengan  panjang  280 
karakter 
Jumlah  suka  dari 
diunggah pengguna 
Jumlah  balasan  dari  twit  yang 
diunggah pengguna 
Jumlah  retweet  dari  twit  yang 
diunggah pengguna 

twit  yang 

memprediksi  demografi  bervariasi  di  berbagai 
jenis 
karakterstik, skor F1 0,90 untuk Organisasi, 0,92 untuk gender, 
dan 0,52 untuk usia. Pada penelitian ini dilakukan pengecekan 
terhadap akurasi prediksi demografi untuk pengguna Twitter di 
Indonesia.  Peneliti  mengumpulkan  250  akun  Twitter  dari 
seluruh  data  pengguna  yang  sudah  terkumpul,  terdiri  dari  76 
akun organisasi dan 174 akun pribadi yang telah diprediksi oleh 
m3inference. Pemilihan sampel tersebut dilakukan berdasarkan 
popularitas pengguna dengan mempertimbangkan kemudahan 
mendapatkan informasi gender dan usia.  

Hasilnya,  dalam  memprediksi  jenis  akun  organisasi  dan 
bukan organisasi, akurasi  yang didapat sebesar 97,60%,  pada 
prediksi gender sebesar 91,01%, dan akurasi yang paling kecil 
didapat  ketika  memprediksi  kelompok  umur  yaitu  65,73%. 
Kesalahan yang paling sering terjadi adalah saat memprediksi 
tahun  ke  bawah  namun  pada 
pengguna  berumur  18 
kenyataannya adalah pengguna dengan umur 19-29 tahun. 

C.  Metode Pengolahan Data 
Eksplorasi Data 

Semua  twit  yang  berasal  dari  masing-masing  kata  kunci 
digabungkan  dan  record  yang  duplikat  dihapus.  Dataset 
kemudian dilakukan klasifikasi pengguna menjadi akun pribadi 
dan akun organisasi dengan menggunakan library m3inference. 
Data twit yang diolah adalah twit dengan minimal mendapatkan 
satu kali interaksi dengan catatan twit yang tidak ada interaksi 
berarti tidak memberikan pengaruh terhadap pengguna lainnya. 
jumlah 
Data  yang 
pengguna sebanyak 39.540. Kemudian data mengalami proses 
pemilahan  dengan  tidak  mengikutsertakan  twit  yang  berasal 
dari akun organisasi  sehingga jumlah  record  yang digunakan 
dalam  penelitian  ini  sebanyak  54.436  twit  dengan  jumlah 
pengguna  sebanyak  37.909.  Proses  pemilihan  tersebut  juga 
memberikan hasil inferensi sebagai variabel baru yang tertera 
pada Tabel III. 

terkumpul  sebanyak  58.118  dengan 

TABEL III 
VARIABEL HASIL INFERENSI KARAKTERISTIK PENGGUNA 

No  Variabel 
(1) 
1 

(2) 
account 

gender 
age 

2 
3 

. 

Keterangan 
(3) 
Jenis akun organisasi atau non-organisasi 
hasil prediksi inferensi 
Gender pengguna hasil prediksi inferensi 
Kelompok  umur  pengguna  hasil  prediksi 
inferensi 

B.  Prediksi Inferensi Karakteristik 

Prediksi inferensi karakteristik demografi pengguna Twitter 
dilakukan  dengan  menggunakan  m3inference  yang  dibangun 
dalam penelitian [12]. M3inference mengambil input visual dan 
teks antara lain gambar profil, nama, nama pengguna, dan teks 
pendek 
(biografi  Twitter).  Dalam 
pengoperasiannya  m3inference  dibangun  dalam  32  bahasa 
utama  yang  sebagian  besar  adalah  bahasa  yang  digunakan  di 
Eropa dan tidak mencakup Bahasa Indonesia.   

deskriptif 

diri 

 Dalam  penelitian 

telah  menggunakan 
[13]  yang 
m3inference,  menyebutkan  bahwa  kinerja  m3inference  dalam 

Sebelum  melakukan 

text  preprocessing,  beberapa 
pemilahan  dilakukan  sebagai  tahapan  cleaning  data  awal.  
Kolom  search  bar  Twitter  memiliki  fitur  rekomendasi  kata 
serupa.  Contohnya  ketika  ingin  menulusuri  twit  dengan  kata 
juga  akan 
kunci  “bangsat”  maka  hasil  penelusuran 
menunjukkan twit yang mengandung kata “bangsa”. Karena itu 
twit yang tidak mengandung kata kunci yang dimaksud dalam 
penelitian  ini  dibuang.  Kemudian  fitur  bahasa  yang  terdapat 
pada  library  twint  tidak  sepenuhnya  menyaring  twit  yang 
berbahasa  Indonesia.  Untuk  itu  dilakukan    proses  pemilahan 
kembali  dengan  menggunakan 
langdetect  untuk 
menghapus  data  yang  menggunakan  bahasa  selain  Bahasa 
Indonesia.  Satu  twit  dapat  mengandung  lebih  dari  satu  kata 
kunci. Oleh karena itu twit yang memiliki record serupa karena 
berasal  dari  dua  atau  lebih  kata  kunci  dihapus  untuk 
menghindari duplikat record. 

library 

Twit  yang  berasal  dari  akun  portal  berita  atau  organisasi 
akan  dipisahakan  sehingga  penelitian    ayi  terfokus  kepada 
pengguna  individu.  Contoh  dari  jenis  akun  tersebut  yaitu 
detikcom,  kompas,  tirtoid,  dll.  Pemisahan  dilakukan  dengan 
menggunakan library m3inference yang memanfaatkan user id 
untuk  melakukan  fetching  langsung  dari  basis  data  Twitter. 
Tahapan ini juga menghasilkan variabel gender dan kelompok 
umur  pengguna 
inferensi 
karakteristik  demografi  yang  merupakan  fasilitas  dari  library 
tersebut. Beberapa pengguna saat dilakukannya proses inferensi 
(Maret  2021)  sudah  dihapus  sehingga  tidak  dapat  diprediksi 
karakteristik  akunnya.  Pengguna-pengguna  yang  dimaksud 
kemudian dihapus dari dataset. 

sebagai  hasil  dari  prediksi 

Label  diberikan  secara  manual  dengan  dua  kategori  yaitu 
bully dan non-bully. Label diberikan terhadap sejumlah 1.406 
dokumen  secara  manual  oleh  penulis.  Twit  yang  diberi  label 
bully  yaitu  twit  yang  mengandung  unsur  kebencian  bersifat 
menyinggung,  menakuti,  membuat  marah,  mempermalukan 
atau mengancam. Sedangkan  twit yang diberi label non-bully 
yaitu twit yang tidak mengansung unsur kebencian atau netral. 

 3 / 8 

 
 
 
 
 
 
Text Preprocessing 

Tahapan  yang  dilakukan  pada  text  preprocessing  yaitu 

sebagai berikut.  
a.  Case folding, yaitu mengubah semua huruf menjadi huruf 
kecil (lower case), menghapus karakter selain huruf/tanda 
baca, menghapus angka, dan menghapus karakter kosong. 
b.  Link/url, username, hashtag, emoji, dan stopword removal, 
yaitu penghapusan link/url, username, hashtag, emoji, dan 
stopword  yang  terdapat  pada  sebuah  twit.  Stopword 
merupakan kata yang memiliki informasi rendah.  

c.  Normalisasi,  yaitu  memperbaiki  kata  yang  tidak  baku 

berdasarkan Kamus Besar Bahasa Indonesia. 

d.  Stemming  kata,  yaitu  proses  mengubah  kata  menjadi  kata 

dasarnya.  

e.  Tokenisasi  kata,  yaitu  proses  pemisahan  kalimat  menjadi 

potongan-potongan kata yang disebut token. 

Ekstraksi Fitur 

Ekstraksi  fitur  dilakukan  dengan  membandingkan  metode 
TF-IDF dan Count Vectorizer untuk model klasifikasi dengan 
metode  Multinomial  Naïve  Bayes  dengan 
teknik  semi 
supervised learning. 

D.  Metode Analisis 

Metode  klasifikasi  dengan  menggunakan  Lexicon  Based 
memanfaatkan sebuah kamus leksikon yang berisi daftar kata 
positif  dan    aying  n  berikut  dengan  nilai  polaritasnya. 
Lexicon based classification memanfaatkan metode rule based 
cara 
method 
membandingkan  jumlah  kata  positif  dan  jumlah  kata  negatif 
pada satu dokumen. Kamus untuk pendekatan berbasis leksikon 
dapat dibuat secara manual atau secara otomatis menggunakan 
kata-kata inti untuk memperluas daftar kata [14]. 

untuk  mengklasifikasi 

dengan 

data 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

juga  akan  ditampilkan  dalam  bentuk  word  cloud.  Selain  itu, 
karakteristik  demografi  hasil  inferensi  dengan  m3inference 
juga  ditampilkan  untuk  melihat  kecenderungan  pelaku 
cyberbullying pada Twitter berdasarkan gender dan kelompok 
umur.  Serta  dilakukan  juga  perhitungan  dasar  untuk  korelasi 
antara  variabel  numerik  yang  mencakup  jumlah  retweet, 
balasan,  dan  suka  dengan  skor  sentimen  yang  didapat  dari 
metode Lexicon Based Classification. 

V.  KERANGKA PIKIR 

dapat 

kata-kata 

Penelitian  ini  berawal  dari  semakin  banyaknya  pengguna 
Twitter  yang  menyampaikan  pemikirannya  secara 
liar. 
Kemudahan beropini di Twitter memberikan kesempatan bagi 
pengguna  untuk  mengungkapkan  pandangannya 
tanpa 
menimbulkan 
yang 
menyaring 
ketidaknyamanan bagi pihak lain. Kebebasan berkomunikasi di 
Twitter melahirkan ujaran kebencian yang dapat menyinggung 
bahkan menyakiti korban. Ujaran kebencian tersebut biasanya 
tidak  pantas  saat 
diikuti  dengan  kata-kata  kasar  dan 
mengungkapkan ketidaksetujuan atau menentang sesuatu. Twit 
yang  mengandung  ujaran  kebencian  adalah  perilaku 
cyberbullying yang dapat dideteksi lalu diklasifikasikan dengan 
metode  Lexicon  Based  dan  Multinomial  Naïve  Bayes. 
Selanjutnya  hasil  teks  prediksi  tersebut  dianalisis  bagaimana 
kecenderungan 
dalam  melakukan 
cyberbullying berdasarkan karakteristik demografi gender dan 
rentang umur. Kerangka pikir penelitian ini diilustrasikan pada 
Gambar 1. 

pengguna  Twitter 

Dalam  penelitian  ini  penulis  menggunakan  kamus  data 
leksikon  yang  telah  dibangun  oleh  Evan  Martua.  Penulis 
penggunakan  dua  label  di  mana  kategori  positif  dan  netral 
menjadi  satu  kategori  sebagai  label  bukan  cyberbullying 
twit 
sedangkan  kategori  negatif  diberi 
cyberbullying.  

label  sebagai 

Multinomial Naïve Bayes  

Pemodelan  dengan  metode  Multinomial  Naive  Bayes 
dilakukan  dengan 
learning  dan 
teknik  semi-supervised 
menerapkan algoritma Pseudo Labeling. Klasifikasi dilakukan 
dengan  dua  tahap  yang  masing-masing  menghasilkan  satu 
model.  Model  pertama  dibangun  menggunakan  data  berlabel 
dan  digunakan  untuk  memprediksi  data  tidak  berlabel.  Label 
hasil  prediksi  disebut  pseudo-label  yang  kemudian  dianggap 
sebagai  label  sebenarnya  dari  data.  Model  classifier  kedua 
dibangun  menggunakan  data  berlabel  dan  data  pseudo-label 
kemudian  digunakan  untuk 
yang  digabungkan  dan 
memprediksi twit cyberbullying. 

Analisis Desktiptif 

Untuk melihat gambaran umum tentang twit cyberbullying 
dilakukan  analisis  deskriptif.  Dalam  penelitian  ini  akan 
ditunjukkan frekuensi twit cyberbullying berdasarkan hari dan 
waktu.  Kata-kata  yang  sering  muncul  pada  twit  cberbullying 

Gambar 1. Kerangka pikir penelitian 

. 

VI. HASIL DAN PEMBAHASAN 

A.  Pengumpulan Data 

Dari  hasil  crawling  yang  terkumpulkan,  dilakukan  proses 
pelabelan  terhadap  data  twit  dengan  kategori  bully  dan  non-
bully. Twit yang diberi label bully yaitu twit yang mengandung 
unsur  kebencian  bersifat  menyinggung,  menakuti,  membuat 
marah,  mempermalukan  atau  mengancam.  Sedangkan  twit 
yang diberi label non-bully yaitu twit yang tidak mengansung 
unsur kebencian atau netral. Proses pelabelan dilakukan secara 
manual oleh penulis terhadap 1.406 dari 54.436 data twit yang 
sudah dikumpulkan.  

 4 / 8 

 
 
 
 
 
 
 
 
Dari  1.406  data  yang  diberi  label  secara  manual,  terdapat 
39,19% twit yang dikategorikan sebagai bully. Kategori bully 
dan  bukan  non-bully  divisualisasikan  ke  dalam  word  cloud 
untuk  melihat  kata-kata  yang  sering  digunakan  pada  masing-
masing kategori twit. Visualisasi word cloud twit yang diberi 
label bully ditunjukkan pada Gambar 2. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

[16],  yang  meneliti  tentang  bentuk  dan  fungsi  pemakaian 
umpatan.  Umpatan  selain  sebagai  bentuk  sindiran,  juga 
memiliki  fungsi  sebagai  bentuk  ekspresi  marah  atau  jengkel, 
bentuk  sapaan  atau  pengakraban,  dan  bentuk  ekspresi 
keterkejutan. 

B.  Predeksi Karakteristik Demografi Pengguna  

Prediksi inferensi pengguna Twiter memerlukan data  user 
id  untuk  dilakukan  fetching  langsung  ke  Twitter.  39.540  dari 
39.545  data  pengguna  berhasil  diprediksi.  Sedangkan  lima 
pengguna tidak dapat dilakukan proses inferensi  karena akun 
lagi  pada  Twitter.  Tabel  IV. 
sudah 
menunjukkan  jumlah  data  pengguna  yang  diprediksi  berikut 
dengan hasil prediksinya. 

tidak  ditemukan 

Gambar 2. Word cloud twit yang diberi label bully 

Hasilnya dari visualisasi di atas, twit bully sering digunakan 
untuk  hal  yang  mengarah  kepada  suatu  pihak  atau  oknum 
seperti  “kadrun”,  “abu  janda”,  dan  “pemerintah”.    Kata  seru 
“dasar”  dan  “otak”  sering  digunakan  saat  menyerang  target 
secara  verbal.  Penyerangan  verbal  ditunjukan  untuk 
merendahkan  harga  diri 
[15]. 
Penyerangan  dapat  dilakukan  dengan  dua  cara  yaitu  
menyerang  secara  langsung  dengan  konteks  bercanda  namun 
bersifat  menghina  dan  menyerang  secara  tidak  langsung 
dengan sindirian.  

seseorang  atau  pihak 

Sedangkan pada twit yang tidak dikategorikan sebagai bully, 
beberapa  kata  kunci  digunakan  sebagai  cara  berekspresi  atau 
ungkapan yang membuat makna frasa menjadi lebih kuat. Word 
cloud  twit  yang  dikategorikan  sebagai  non-bully  ditampilkan 
pada Gambar 3. 

TABEL IV 
HASIL INFERENSI KARAKTERISTIK PENGGUNA 
Hasil Prediksi 

Gender 

Kelompok Umur 

Jenis Akun 

Laki-laki 
Perempuan 
0-18 tahun 
19-29 tahun 
30-39 tahun 
40 tahun ke atas 
Organisasi 
Pribadi 

Jumlah Pengguna 
22.454 
15.455 
18.079 
15.233 
1.824 
2.773 
1.631 
37.909 

C.  Klasifikasi twit dengan Lexicon Based 

Klasifikasi  dengan  metode  Lexicon  Based  memberikan 
akurasi sebesar 51,71%. Confussion matrix klasifikasi dengan 
Lexicon Based ditunjukkan pada Tabel V. dengan nilai presisi, 
recall,  dan  f1  yang  dapat  dilihat  pada  Tabel  VI.  Rendahnya 
akurasi  dapat  terjadi  karena  terdapat  banyak  kata  kunci  yang 
ditangkap  sebagai  kata  negatif  pada  kamus 
lexicon, 
mengesampingkan  kenyataan  bahwa  kata-kata  umpatan 
tersebut  dapat  digunakan  untuk  mengekspresikan  kemarahan, 
rasa sakit, kegembiraan, frustrasi, atau keterkejutan [17]. 

TABEL V 
CONFUSSION MATRIX KLASIFIKASI DENGAN LEXICON BASED  
SEBELUM KATA KUNCI DIHAPUS 

(1) 

Prediksi 

(2) 

Bully 
Non-bully 
Jumlah 

Aktual 
Bully  Non-bully 
(3) 
448 
103 
551 

576 
279 
855 

(4) 

Jumlah 
(5) 
1024 
382 
1406 

Gambar 3. Word cloud twit yang diberi label bukan bully 

Contohnya  pada  gambar  di  atas,  penggunaan  kata-kata 
kasar  saat  mengatakan  suka,  takut,  sedih,  atau  lelah  sebagai 
cara  meluapkan  perasaan  dan  emosi  yang  berlebihan.  Ini 
ditunjukkan  pada  visualisasi  word  cloud  yang  mengandung 
kata-kata  seperti  “suka”,  “cantik”,  “sayang”,  “ganteng”, 
“menangis”, “capek”, “takut”. Hal ini sejalan dengan penelitian 

 TABEL VI 
PRESISI, RECALL, DAN FI DENGAN LEXICON BASED  
SEBELUM KATA KUNCI DIHAPUS 

Label 
(1) 

Bully 
Non-bully 
Akurasi 

Presisi 
(2) 
0.44 
0.73 
0.63 

Recall 
(3) 
0.81 
0.33 

F1-score 
(4) 

0.57 
0.45 

Untuk itu dilakukan klasifikasi kembali setelah menghapus 
kata  kunci  pada  twit  yang  sudah  dilakukan  preprocessing 
tadinya 
sebelumnya.  Pada  klasifikasi  kedua, 

twit  yang 

 5 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

diprediksi  sebagai  cyberbullying  namun  pada  kenyataanya 
bukan,  berkurang  sebanyak  265  twit.  Hal  ini  menyebabkan 
akurasi  meningkat  menjadi  62,59%.  Confussion  matrix 
klasifikasi  dengan  Lexicon  Based  setelah  kata  kunci  dihapus 
ditunjukkan pada Tabel VII. dengan nilai presisi, recall, dan f1 
yang dapat dilihat pada Tabel VIII. 

Pemodelan II 

Model kedua yang terbentuk dengan metode ekstraksi fitur 
TF-IDF  menunjukkan  rata-rata  akurasi  62,55%  dan  standar 
deviasi 0,0099. Metode count vectorizer memberikan rata-rata 
akurasi yang lebih baik sebesar 72,94% dengan standar deviasi 
0.0114 yang dapat dilihat pada Tabel X. 

TABEL VII 
CONFUSSION MATRIX KLASIFIKASI DENGAN LEXICON BASED  
SETELAH KATA KUNCI DIHAPUS 

(1) 

(2) 

Prediksi 

Bully 
Non-bully 
Jumlah 

Aktual 
Bully  Non-bully 
(3) 
336 
215 
551 

311 
544 
855 

(4) 

Jumlah 
(5) 

647 
759 
1406 

TABEL VIII 
PRESISI, RECALL, DAN FI DENGAN LEXICON BASED  
SETELAH KATA KUNCI DIHAPUS 

Label 
(1) 

Bully 
Non-bully 
Akurasi 

Presisi 
(2) 
0.52 
0.71 

Recall 
(3) 
0.61 
0.63 

F1-score 
(4) 

0.56 
0.67 

D.  Klasifikasi twit dengan Multinomial Naïve Bayes  

Klasifikasi 

twit  cyberbullying  selanjutnya  dilakukan 
dengan  menggunakan  metode  Multinomial  Naïve  Bayes 
dengan  menerapkan  5-fold  cross  validation.  Metode  ini  akan 
diterapkan  ke  dalam  teknik  semi-supervised  learning  yang 
dilakukan dengan dua tahap pemodelan. 

Pemodelan I 

Pemodelan  pertama  dilakukan  pada  1.406  data  twit  hasil 
crawling  yang  diberi  label  secara  manual.  Model  yang 
terbentuk dengan metode ekstraksi fitur TF-IDF menunjukkan 
rata-rata  akurasi  65,36%  dengan  standar  deviasi  0.0095. 
Sedangkan dengan Count Vectorizer, model memberikan hasil 
rata-rata  akurasi  sebesar  70,70%  dan  standar  deviasi  0.0216 
yang dapat dilihat pada Tabel IX. 

Iterasi ke- 

Akurasi model 
dengan TF-IDF 

TABEL IX 
NILAI AKURASI PADA ITERASI CROSS VALIDATION PEMODELAN I  
Akurasi model 
dengan Count 
Vectorizer 
(3) 
70,21% 
74,37% 
70,46% 
70,81% 
67,61% 

(2) 
67,38% 
74,38% 
69,75% 
66,55% 
66,19% 

(1) 
1 
2 
3 
4 
5 

Selanjutnya,  prediksi  dilakukan  pada  setiap  25%  data  twit 
yang  belum  diberi  label.  Sebanyak  4.437  dari  13.257  data 
diprediksi sebagai twit cyberbullying. Label hasil prediksi akan 
dianggap sebagai label sebenarnya dari data. 

TABEL X 
NILAI AKURASI PADA ITERASI CROSS VALIDATION PEMODELAN II 

Iterasi ke- 

(1) 
1 
2 
3 
4 
5 

Akurasi model 
dengan TF-IDF 
(2) 
63,12% 
64,18% 
62,06% 
62,06% 
61,35% 

Akurasi model dengan 
Count Vectorizer 
(3) 
73,22% 
71,56% 
74,64% 
73,46% 
71,80% 

Pemodelan kedua dengan dilakukan untuk memprediksi data 
twit  yang  belum  diberi  label.  Hasil  prediksi  menunjukkan 
10.029  data  dikategorikan  sebagai  cyberbullying  dan  29.744 
data bukan cyberbullying. 

E.  Visualisasi dan Analisis 
Korelasi antara Variabel Numerik 

Nilai  sentimen  didapatkan  dari  hasil  klasifikasi  dengan 
metode  Lexicon  Based.  Analisis  ini  bertujuan  untuk  melihat 
apakah  ada  korelasi  antara  nilai  sentimen  dengan  jumlah 
interaksi  yang  mencakup  jumlah  balasan,  jumlah  suka,  dan 
jumlah  retweet.  Hasil  menjunjukan  bahwa  tidak  ada  korelasi 
antara  nilai  sentimen  dengan  jumlah  interaksi.  Positif  atau 
negatifnya  sentimen  dari  sebuah  twit  tidak  ada  hubungannya 
dengan  seberapa  banyak  keterlibatan,  ketertarikan,  dan 
interaksi yang didapatkan. Variabel numerik yang berkorelasi 
kuat hanya jumlah retweet dan jumlah suka sebesar 0,91. 

Cyberbullying berdasarkan hari dan waktu 

Cyberbullying  di  Twitter  paling  sering  terjadi  pada  akhir 
pekan dan malam hari. Hari Jumat, Sabtu dan Minggu menjadi 
hari  dengan  twit  cyberbullying  terbanyak  sementara  sore 
sampai  malam  hari  menjadi  waktu  terjadi  twit  cyberbullying 
paling  sering.  Waktu-waktu  tersebut  adalah  waktu  di  mana 
orang-orang  tidak  banyak  melakukan  rutinitas  sehari-hari. 
Salah satu motif yang mendasari tindakan cyberbullying adalah 
ketika  pelaku  cyberbully  bosan  dan  mencari  hiburan  [18]. 
Sehingga  kurangnya  aktivitas  yang  bermakna  yang  memicu 
kebosanan  dapat  memicu  pelaku  untuk  mencari  hiburan  di 
media  sosial  dengan  mengganggu  pihak  lain.  Gambar  4.  dan 
Gambar  5.  berturut-turut  menunjukkan 
twit 
cyberbullying berdasarkan waktu dan hari. 

frekuensi 

 6 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

hal  bullying  dan  cyberbullying.  Laki-laki  lebih  cenderung 
menjadi pelaku bully dan cyberbully daripada perempuan [19].  
Selain  itu,  perbandingan  jumlah  pelaku  cyberbullying 
berdasarkan  gender  ini  tidak  terlalu  berbeda    dengan  hasil 
inferensi  karakteristik  gender  pengguna  Twitter  sebelum  twit 
dilakukan  klasifikasi.  Kata  kunci  yang  digunakan  dalam 
penelitian ini, yang umumnya merupakan kata umpatan, lebih 
banyak digunakan oleh laki-laki daripada perempuan. Hal ini 
dijelaskan  oleh  penelitian  yang  menyebutkan  bahwa  laki-laki 
memiliki kecenderungan untuk mengucapkan kata-kata makian 
yang  lebih  banyak  menampilkan  tindakan  agresif  daripada 
itu  dapat  berkaitan  dengan 
perempuan.  Kecenderungan 
penemuan  yang  menekankan  bahwa  perempuan  memiliki 
volume korteks frontal orbital yang lebih besar daripada pria, 
yang mana dapat memodulasi kemarahan dan agresivitas lebih 
baik  [20].  

Sedangkan berdasarkan kelompok umur seperti yang terlihat 
pada  Gambar  7.,  pelaku  cyberbullying  didominasi  oleh 
kelompok  umur  kurang  dari  18  tahun  sebanyak  43,25%  dan 
diikuti dengan kelompok umur 19-29 tahun sebanyak 39,01%. 

Gambar 7. Frekuensi cyberbullying berdasarkan kelompok umur 

Hasil  ini  tidak  jauh  berbeda  dengan  penelitian  terdahulu 
sisi  psikologi, 
cyberbullying  dari 
yang  menyelidiki 
menjelaskan  bahwa  cyberbullying  atau  serangan  agresif  di 
internet  paling  banyak  mempengaruhi  remaja  di  sekolah 
menengah  pertama  dan  dapat  meluas  ke  periode  sekolah 
menengah  atas 
lain 
menyebutkan orang dewasa muda yang berumur 18–25 tahun 
mengalami  tingkat  cyberbullying  tertinggi  dan  prevalensi 
cyberbullying lebih rendah di antara kelompok usia yang lebih 
tua [23]. 

[22].  Sementara  penelitian 

[21], 

VII. 

PENUTUP 

A.  Kesimpulan 

Berdasarkan  hasil  penelitian  di  atas,  dapat  diperoleh 

kesimpulan sebagai berikut. 
1.  Hasil prediksi inferensi pengguna Twitter menujukkan twit 
dengan  kata  kunci  cyberbullying  dan  minimal  mendapat 
satu  kali  interaksi  berasal  dari  pengguna  yang  didominasi 
oleh  gender  laki-laki  sebanyak  59,23%  dan  kelompok 
kurang dari 18 tahun sebanyak 47,69%.  

2.  Klasifikasi  dengan  metode  Lexicon  Based  memberikan 
akurasi sebesar 51,71% sebelum kata kunci dihapus. Selain 
untuk  tujuan  menyindir  atau  mengujar  kebencian,  kata 

 7 / 8 

Gambar 4. Frekuensi cyberbullying berdasarkan waktu 

Gambar 5. Frekuensi cyberbullying berdasarkan hari 

Cyberbullying  berdasarkan  karakteristik  demografi  pengguna 
Twitter  

Berdasarkan karakteristik gender, sebanyak 65,25% pelaku 
cyberbullying  di  Twitter  yang  mendapatkan  minimal  satu 
interaksi pada Januari 2021 adalah laki-laki dan 34,75% adalah 
perempuan.  Gambar  6.  menunjukkan 
twit 
cyberbullying berdasarkan gender. 

frekuensi 

Gambar 6. Frekuensi cyberbullying berdasarkan gender 

Dari  hasil  visualisasi  di  atas  dapat  dilihat  bahwa  laki-laki 
lebih cenderung melakukan cyberbullying daripada wanita. Hal 
ini  sejalan dengan penelitian  yang  menjelaskan bahwa ketika 
gender dipertimbangkan, terdapat perbedaan signifikan dalam 

 
 
 
 
 
 
 
 
 
kunci  yang  merupakan kata-kata  umpatan juga digunakan 
oleh pengguna Twitter untuk mengekspresikan kemarahan, 
rasa  sakit,  kegembiraan,  frustrasi,  atau  keterkejutan. 
Dengan  tidak  mengikutsertakan  kata  kunci  cyberbullying, 
klasifikasi  dengan  metode  Lexicon  Based  memberikan 
akurasi  sebesar  62,59%.  Sedangkan  model  Multinomial 
Naïve  Bayes  dengan  teknik  semi-supervised  Learning 
menunjukkan  akurasi  yang  paling  baik  sebesar  74,64% 
dalam megklasifikasi twit cyberbullying.  

3.  Korelasi  antara  variabel  numerik  menunjukkan  tidak  ada 
korelasi  antara  nilai  sentimen  dengan  jumlah  interaksi. 
Positif atau negatifnya sentimen dari sebuah twit tidak ada 
hubungannya  dengan  seberapa  banyak  keterlibatan, 
ketertarikan, dan interaksi yang dia dapatkan. 

4.  Perilaku cyberbullying di Twitter paling sering terjadi pada 
akhir  pekan  dan  malam  hari  oleh  gender  laki-laki  dan 
kelompok usia kurang dari 18 tahun diikuti oleh kelompok 
usia 19-29 tahun dengan perbedaan yang tidak signifikan. 

B.  Saran 

Beberapa  saran  yang  dapat  dipertimbangkan  untuk 

penelitian berikutnya adalah sebagai berikut. 
1.  Memisahkan  twit  dengan  kata  kunci  yang  memiliki  arti 
harfiah  seperti  nama-nama  hewan  yang  tidak  ditujukan 
untuk disampaikan dalam makna lain. 

2.  Penambahan  kata  kunci  baru  berbahasa  Inggris  dengan 
mempertimbangkan 
penduduk 
semakin 
Indonesia kelompok usia muda yang menggunakan Bahasa 
Inggris di media sosial. 

banyaknya 

3.  Menggunakan alat inferensi demografi lain yang mencakup 
pengguna  Twitter  di  Indonesia  mengingat  keterbatasan 
m3inference  yaitu  hanya  mengkhususkan  32  bahasa  yang 
tidak  termasuk  Bahasa  Indonesia.  Selain  itu,  pengecekan 
akurasi  m3inference  dalam  penelitian  ini  bersifat  lemah 
karena  sampel  pengguna  dipilih  berdasarkan  subjektivitas 
kemudahan mendapatkan informasi.  

4.  Menambah data manual labeling serta pelabelan dilakukan 
dengan kriteria yang lebih spesifik agar akurasi meningkat. 
5.  Penggunaan  slangword  dan  stopword  dari  Indonesia 
Lexicon Word (Evan Martua) belum mampu mengganti dan 
menghapus seluruh slangword dan stopword yang ada pada 
teks, terlihat dari banyaknya kata di word cloud yang belum 
terstandar.  Untuk 
dapat 
perbaikan 
menggunakan sumber lain atau memodifikasi sumber data 
dengan menambahkan daftar kata pada kamus.  

selanjutnya 

DAFTAR PUSTAKA 
[1]  V.  D.  Pozza,  A.  Di  Pietro,  S.  Morel,  and  E.  Psaila,  “Cyberbullying 

[2] 

Among Young People,” Parlam. Eur., pp. 1–194, 2016. 
“Digital  2020:  Indonesia  —  DataReportal  –  Global  Digital  Insights.” 
https://datareportal.com/reports/digital-2020-indonesia  (accessed  Apr. 
01, 2021). 

[3]  R.  Ameliah,  “Pencegahan  Cyberbullying:  Penguatan  Literasi  Digital 
Melalui Digital Parenting - PDF Free Download,” 2020, Accessed: Jun. 
14,  2021. 
[Online].  Available:  https://docplayer.info/195150048-
Pencegahan-cyberbullying-penguatan-literasi-digital-melalui-digital-
parenting.html. 

[4]  Republika,  “Virtual  Police:  Konten  Ujaran  Kebencian  Terbanyak  di 
Twitter 
2021. 
Republika 
https://www.republika.co.id/berita/qputx3383/emvirtual-policeem-
konten-ujaran-kebencian-terbanyak-di-twitter (accessed Jun. 15, 2021). 

Online,” 

| 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

[5]  M.  Teasley,  “Cyberbullying,  Youth  Behavior  and  Society,”  J.  Child 
Adolesc. Behav., vol. 02, no. 01, pp. 1–2, Dec. 2013, doi: 10.4172/2375-
4494.1000119. 

[6]  M. A. Campbell, “Cyber Bullying: An Old Problem in a New Guise?,” 
Aust.  J.  Guid.  Couns.,  vol.  15,  no.  1,  pp.  68–76,  Jul.  2005,  doi: 
10.1375/ajgc.15.1.68. 

[7]  K. Aini and R. Apriana, “Dampak Cyberbullying Terhadap Depresi pada 
Mahasiswa Prodi Ners,” J. Keperawatan Jiwa, vol. 6, no. 2, p. 91, Jan. 
2019, doi: 10.26714/jkj.6.2.2018.91-97. 

[8]  H. Margono, X. Yi, and G. K. Raikundalia, “Mining Indonesian cyber 
bullying patterns in social networks,” Conf. Res. Pract. Inf. Technol. Ser., 
vol. 147, no. ACSC, pp. 115–124, 2014. 

[9]  D.  H.  Fudholi  and  K.  P.  Juwairi,  “Classifying  Medical  Document  in 
Bahasa  Indonesia  using  Semi-Supervised  Learning,”  IOP  Conf.  Ser. 
Mater. Sci. EFudholi, D. H., Juwairi, K. P. (2021). Classifying Med. Doc. 
Bhs. Indones. using Semi-Supervised Learn. IOP Conf. Ser. Mater. Sci. 
Eng. 1077(1), 012015. https//doi., vol. 1077, no. 1, p. 012015, 2021, doi: 
10.1088/1757-899x/1077/1/012015. 

[10]  K. Z. Aung and N. N. Myo, “Sentiment analysis of students’ comment 
using  lexicon  based  approach,”  in  Proceedings  -  16th  IEEE/ACIS 
International Conference on Computer and Information Science, ICIS 
2017, Jun. 2017, pp. 149–154, doi: 10.1109/ICIS.2017.7959985. 
[11]  N. Syawallina and S. P. Suganda, “Kecenderungan Penggunaan Bahasa 
Inggris  pada  Instragrammer  —  Universitas  Indonesia,”  pp.  217–227, 
[Online].  Available: 
10, 
2019,  Accessed: 
https://scholar.ui.ac.id/en/publications/kecenderungan-penggunaan-
bahasa-inggris-pada-instragrammer. 

2021. 

Jun. 

[12]  Z. Wang et al., “Demographic Inference and Representative Population 
Estimates  from  Multilingual  Social  Media  Data,”  vol.  12,  2019,  doi: 
10.1145/3308558.3313684. 

[13]  R. Crilley, M. Gillespie, B. Vidgen, and A. Willis, “Understanding RT’s 
Audiences: Exposure Not Endorsement for Twitter Followers of Russian 
State-Sponsored  Media,” 
J.  Press.,  Dec.  2020,  doi: 
10.1177/1940161220980692. 

Int. 

[14]  M. Taboada, J. Brooke, M. Tofiloski, K. Voll, and M. Stede, “Lexicon-
basedmethods for sentiment analysis,” Comput. Linguist., vol. 37, no. 2, 
pp. 267–307, 2011, doi: 10.1162/COLI_a_00049. 

[15]  N. Chotijah, “Representasi Kekesaran Verbal Pelajar dalam Film Dilan 
1990 (Analisis Semiotika Charles Sandres Peirce),” Feb. 2020. 
[16]  P. S. S. Maksyur, “Bentuk dan Fungsi Pemakaian Umpatan oleh Siswa 
SMA Negeri 2 Majene Sulawesi Barat dalam Situasi Nonformal,” Oct. 
2013, Accessed: Jun. 11, 2021. [Online]. Available: http://lib.unair.ac.id. 
[17]  M.  F.  S.  Nicolau  and  K.  E.  Sukamto,  “Male  and  Female  Attitudes 
towards  Swear  Words:  A  Case  Study  at  Binus  International  School,” 
k@ta, vol. 16, no. 2, pp. 71–76, Dec. 2014, doi: 10.9744/kata.16.2.71-
76. 

[18]  M. M. Pandie and I. T. J. Weismann, “Pengaruh Cyberbullying Di Media 
Sosial  Terhadap  Perilaku  Reaktif  Sebagai  Pelaku  Maupun  Sebagai 
Korban Cyberbullying Pada Siswa Kristen SMP Nasional Makassar,” J. 
Jaffray, vol. 14, no. 1, p. 43, Mar. 2016, doi: 10.25278/jj71.v14i1.188. 
[19]  X.  Li  and  B.  Yang,  “A  Pseudo  Label  based  Dataless  Naive  Bayes 

Algorithm for Text Classification with Seed Words.” 

[20]  E. Güvendir, “Why are males inclined to use strong swear words more 
than  females?  An  evolutionary  explanation  based  on  male  intergroup 
aggressiveness,” Language Sciences, vol. 50. Elsevier Ltd, pp. 133–139, 
Jul. 01, 2015, doi: 10.1016/j.langsci.2015.02.003. 

[21]  R.  M.  Kowalski  and  S.  P.  Limber,  “Psychological,  physical,  and 
academic  correlates  of  cyberbullying  and  traditional  bullying,”  J. 
Adolesc.  Heal.,  vol.  53,  no.  1  SUPPL,  pp.  S13–S20,  Jul.  2013,  doi: 
10.1016/j.jadohealth.2012.09.018. 

[22]  P.  K.  Smith,  J.  Mahdavi,  M.  Carvalho,  S.  Fisher,  S.  Russell,  and  N. 
Tippett,  “Cyberbullying:  Its  nature  and  impact  in  secondary  school 
pupils,” J. Child Psychol. Psychiatry Allied Discip., vol. 49, no. 4, pp. 
376–385, Apr. 2008, doi: 10.1111/j.1469-7610.2007.01846.x. 

[23]  M. J. Wang, K. Yogeeswaran, N. P. Andrews, D. R. Hawi, and C. G. 
Sibley,  “How  Common  Is  Cyberbullying  among  Adults?  Exploring 
Gender,  Ethnic,  and  Age  Differences 
the  Prevalence  of 
Cyberbullying,”  Cyberpsychology, Behav. Soc. Netw.,  vol. 22,  no. 11, 
pp. 736–741, Nov. 2019, doi: 10.1089/cyber.2019.0146. 

in 

 8 / 8 

 
 
 
 
"
221709653,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pembangunan Sistem Informasi Penghitungan 
Pengeluaran Konsumsi Rumah Tangga Triwulanan 

Efriandi (221709653, 4SI1) 
Dosen Pembimbing: Firdaus, MBA. 

cukup 

Ringkasan— Produk Domestik Bruto (PDB) merupakan salah 
satu indikator utama untuk menentukan pertumbuhan ekonomi 
suatu negara. PDB dapat dihitung melalui tiga pendekatan, yaitu 
pendekatan  produksi,  pendekatan  pendapatan  dan  pendekatan 
satu  komponen  penyumbang  PDB 
pengeluaran.  Salah 
signifikan  adalah 
pendekatan  pengeluaran  yang 
Pengeluaran Konsumsi Rumah Tangga (PKRT). PKRT dihitung 
oleh  BPS  RI  pada  Subdirektorat  Neraca  Rumah  Tangga  dan 
Institusi  Nirlaba.  Perhitungan  PKRT  triwulanan  selama  ini 
menggunakan  aplikasi  Microsoft  Excel,  dimana  dirasa  belum 
cukup  efisien.  Beranjak  dari  permasalahan  itu,  penelitian  ini 
dibuat untuk membangun sistem informasi penghitungan PKRT 
triwulanan  sehingga  nantinya  akan  dibuat  sistem  penghitungan 
PKRT  yang  mampu  menyelesaikan  isu  yang  ada,  serta  efisien 
dalam  pengerjaannya.  Penelitian 
ini  menggunakan  metode 
analisis  permasalahan  dan  PIECES  untuk  analisa  kebutuhan. 
Untuk  testing  dan  evaluasi,  peneliti  menggunakan  black  box 
testing dan SUS. 

Kata  Kunci—  Pembangunan  Sistem,  Sistem  Infromasi,  PDB 

Pengeluaran, PKRT Triwulanan 

I.  LATAR BELAKANG 

Salah  satu  indikator  penting  untuk  mengetahui  kondisi 
ekonomi  disuatu  negara  dalam  suatu  periode  tertentu  adalah 
data  Produk  Domestik  Bruto  (PDB).  PDB  pada  dasarnya 
merupakan jumlah nilai tambah yang dihasilkan oleh seluruh 
unit  usaha  dalam  suatu  negara,  atau  merupakan  jumlah  nilai 
barang  dan  jasa  akhir  yang  dihasilkan  oleh  seluruh  unit 
ekonomi [1]. 

PDB  dapat  dihitung  melalui 

tiga  pendekatan  yaitu 
pendekatan produksi, pendekatan pendapatan, dan pendekatan 
pengeluaran.  Secara  konsep  ketiga  pendekatan  tersebut  akan 
menghasilkan angka yang sama. PDB pengeluaran merupakan 
besaran nilai produk barang dan jasa (output) yang dihasilkan 
di dalam wilayah domestik untuk digunakan sebagai konsumsi 
akhir oleh rumah tangga, Lembaga Non-profit yang melayani 
Rumah  Tangga  (LNPRT),  dan  pemerintah  ditambah  dengan 
investasi,  serta  ekspor  neto.  Untuk  menghitung  PDB 
Pengeluaran maka salah satu komponen yang diperlukan yaitu 
komponen Pengeluaran Konsumsi Rumah Tangga (PKRT) [2]. 
Badan  Pusat  Statistik  adalah  Lembaga  Pemerintah  Non 
langsung  kepada 
Kementerian  yang  bertanggung 
Presiden.  Dimana  salah  satu  kegiatan  BPS  yaitu  menghitung 
dan  menerbitkan  data  PDB  Indonesia.  Subdirektorat  Neraca 
Rumah  Tangga  dan  Institusi  Nirlaba  adalah  salah  satu 
Subdirektorat dari Direktorat Neraca Pengeluaran yang ada di 
BPS  RI,  yang  salah  satu  kegiatan  rutinnya  yaitu  melakukan 
penyusunan  PDB  pengeluaran  komponen  Pengeluaran 
Konsumsi Rumah Tangga (PKRT) secara triwulanan. 

jawab 

Berdasarkan berita Resmi Statistik terbaru yang dikeluarkan 
oleh BPS, komponen PKRT merupakan salah satu penyumbang 
terbesar  terhadap  PDB  pengeluaran,  yaitu  sebesar  56,52% 
terhadap  total  PDB  pengeluaran  [3].  PKRT 
triwulanan 
didapatkan  dari  hasil  penghitungan  dengan  menggunakan 
beberapa inputan data, seperti data PKRT tahunan, data Indeks 
Perkembangan PKRT, data indikator pendukung, dan lainnya.  
Data 
ini  diperoleh  BPS  dengan  melakukan 
pengumpulan data dari internal BPS dan dari eksternal BPS.  

inputan 

rekonsiliasi 

akan  dilakukan  proses 

Data yang telah dikumpulkan akan diinput ke dalam sistem 
untuk  selanjutnya  dilakukan  penghitungan  angka  PKRT 
triwulanan, setelah angka PKRT triwulanan didapatkan maka 
selanjutnya 
atau 
penyesuaian  data  yang  bertujuan  untuk  menurunkan 
diskrepansi  (perbedaan)  antara  dua  data.  Dalam  hal  ini 
dilakukan  rekonsiliasi  untuk  menurunkan  perbedaan  antara 
data  PKRT  triwulanan  (nasional)  yang  telah  didapatkan  dari 
penghitungan  menggunakan  data  inputan  dan  data  PKRT 
regional yang merupakan akumulasi PKRT triwulanan dari 34 
provinsi. Rekonsiliasi dilakukan berkali-kali sampai memenuhi 
angka maksimal diskrepnasi (perbedaan) yang telah ditetapkan. 
Terakhir  angka  PKRT  triwulanan  yang  sudah  dilakukan 
rekonsiliasi akan dikirimkan kepada Subdirektorat Konsolidasi 
Neraca  Pengeluaran  yang  nantinya  akan  digunakan  dalam 
melakukan penghitungan PDB pengeluaran. 

Kegiatan  penyusunan  angka  PKRT  triwulanan  selama  ini 
menggunakan  aplikasi  Microsoft  Excel  di  komputer  lokal. 
Proses  entri  data  dilakukan  oleh  tiga  staff  seksi  dan  akan 
dilakukan  pengecekan  data  oleh  kasubdit  dan  kepala  seksi. 
Dengan menggunakan sistem yang sekarang terdapat beberapa 
isu yang menjadi permasalahan, seperti kerentanan akan human 
error seperti salah input data ataupun data tidak sengaja terubah, 
isu lain yaitu tidak tercatatnya riwayat rekonsiliasi yang mana 
data  riwayat  ini  dibutuhkan  oleh  subject  matter.  Dari  segi 
penyimpanan data, sistem yang sekarang menyimpankan data-
data  triwulanan-triwulanan  sebelumnya  dalam  satu  file Excel 
yang sama sehingga semakin lama file Excel ini akan semakin 
banyak datanya dan kemungkinan tingkat human error pastinya 
juga meningkat. File Excel hanya dilakukan back-up data pada 
harddisk staff seksi. 

Maka dari itu diperlukan pembangunan sistem penghitungan 
PKRT triwulanan untuk menyelesaikan isu permasalahan yang 
ada sehingga membuat proses penghitungan PKRT triwulanan 
lebih efisien. 

Berdasarkan 

II.  TUJUAN PENELITIAN 
latar  belakang  yang 

sebelumnya, 

tujuan  dari  penelitian 

telah  dituliskan 
ini  adalah  untuk 

 1 / 8 

 
 
 
 
mengembangkan sistem penghitungan PKRT triwulanan yang 
mampu  menyimpan  riwayat  rekonsiliasi,  serta  menyimpan 
data-data triwulanan sebelumnya pada database untuk menjaga 
integritas data sehingga proses penghitungan PKRT triwulanan 
lebih efisien. 

III. PENELITIAN TERKAIT 

Ada  beberapa  penelitian  terkait  yang  dijadikan  peneliti 

sebagai bahan referensi. 

Penelitian  terkait  yang  pertama  berjudul  “Pembangunan 
Sistem Informasi Terintegrasi Konsolidasi Neraca Pengeluaran 
Web”  yang  ditulis  oleh  Gibran  Sansadewa  Asshadiqi  pada 
tahun 2019. Penelitian yang dilakukan adalah pengembangan 
sistem  penghitungan  PDB  pada  Subdit  Konsolidasi  Neraca 
Pengeluaran  BPS  yang  sebelumnya  menggunakan  Excel  dan 
dirasa  belum  efisien.  Hasil  penelitian  berupa  aplikasi  web 
penghitungan  PDB  yang  membuat proses  penghitungan  PDB 
lebih efisien. Perbedaan antara penelitian terkait dan penelitian 
yang  akan  diteliti  yaitu  ada  pada  subject  penelitian,  dan  tipe 
aplikasi yang dikembangkan (web dan desktop) [4]. 

Penelitian  terkait  terkahir  yang  peneliti  gunakan  adalah 
penelitian  dengan 
judul  “Sistem  Aplikasi  Entri  dan 
Rekonsiliasi  Data  Produk  Domestik  Regional  Bruto 
Triwulanan  Provinsi”.  Penelitian  ini  ditulis  oleh  Nur  Ilmi 
Hasbah  pada  tahun  2017.  Hasil  dari  penelitian  ini  adalah 
aplikasi web yang mempermudah pegawai dalam proses entri 
data,  updating,  dan  melihat  hasil  entrian,  serta  menyajikan 
visualisasi  data  untuk  memudahkan  pengguna  dalam 
memperoleh  informasi.  Penelitian  ini  memberikan  gambaran 
peneliti tentang proses aplikasi entri data dan rekonsiliasi [5]. 

IV. METODE PENELITIAN  

A.  Ruang Lingkup Penelitian 

Ruang  lingkup  penelitian  ini  mencakup  pembangunan 
sistem  penghitungan  PKRT  triwulanan  yang  dilakukan  oleh 
Subdirektorat  Neraca  Rumah Tangga dan  Institusi  Nirlaba  di 
BPS  RI.  Kegiatan  penghitungan  PKRT  triwulanan  meliputi 
indikator  yang  dibutuhkan, 
penginputan  data-data  dan 
rekonsiliasi,  dan  finalisasi.  Saat  ini,  keseluruhan  proses 
dilaksanakan  menggunakan  Microsoft  Office  Excel  di 
komputer lokal. Penelitian ini nantinya akan menjadikan proses 
penghitungan PKRT triwulanan lebih efisien. 

B.  Metode Pengumpulan Data 

Metode  pengumpulan  data  yang  digunakan  dalam 

penelitian ini yaitu : 
1.  Wawancara 

Wawancara dilakukan kepada kepala dan staff Seksi 
Neraca  Institusi  Nirlaba.  Wawancara  dilakukan 
untuk  mengetahui  sistem  yang  sedang  berjalan 
mulai  dari  input  data,  rekonsiliasi  dan  finalisasi 
angka  PKRT  triwulanan.  Serta  untuk  mengetahui 
sistem seperti apa yang diharapkan selama ini. 

2.  Observasi 

Observasi  dilakukan  untuk  membandingkan  cara 
penghitungan PKRT triwulanan dengan teknis kerja 
yang sudah dirancang sebelumnya. 

3.  Studi Pustaka 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Studi pustaka dilakukan nutuk menemukan fondasi 
ilmiah  penelitian,  dilakukan  dengan  cara  mencari 
artikel,  jurnal,  publikasi,  panduan,  dan  penelitian-
penelitian  yang  berkaitan  dengan  kegiatan 
penghitungan PKRT triwulanan. 

4.  Kuesioner 

Metode  kuesioner  yang  digunakan  adalah  System 
Usability Scale (SUS). SUS dilakukan dengan cara 
melakukan 
survei  kepada  orang-orang  yang 
berkaitan  dengan  sistem  penghitungan  PKRT 
triwulanan dimana hasil dari survei akan digunakan 
sebagai evaluasi sistem yang dikembangkan. 

C.  Metode Pengembangan Sistem 

Metode  analisis  yang  digunakan  pada  penelitian  ini 
adalah  metode  System  Development  Life  Cycle  (SDLC). 
SDLC  model  waterfall  merupakan  model  pengembangan 
sistem  yang  dimodelkan  secara  sistematik  dan  linear, 
dimana  keluaran  dari  tahap  sebelumnya  akan  menjadi 
masukan  untuk  tahap  berikutnya.  Tahap-tahap  SDLC 
model waterfall adalah sebagai berikut : 
1.  Tahap Perencanaan (Planning) 

Pada tahap perencanaan peneliti menentukan ruang 
lingkup 
Informasi 
dikumpulkan dengan metode yang telah dijelaskan 
pada subbab sebelumnya. 

penelitian. 

tujuan 

dan 

2.  Tahap Analisis Sistem (Requirement Analysis) 

tahapan 

Dalam 
ini  dilakukan  pengumpulan 
informasi tentang sistem yang sedang berjalan pada 
kegiatan  penghitungan  PKRT  triwulanan  yang 
dilakukan oleh Subdirektorat Neraca Rumah Tangga 
dan Institusi Nirlaba, lalu dari informasi yang telah 
didapat  dilakukan  analisis  masalah  dan  analisis 
kebutuhan. 

3.  Tahap Desain (Design) 

Tahap  desain  merupakan 
tahapan  perencanaan 
sistem usulan berdasarkan hasil dari analisis sistem 
sebelumnya. 
berjalan  yang 
Tahapan ini meliputi : perancangan arsitektur sistem, 
perancangan proses bisnis sistem, perancangan basis 
data, dan perancangan User Interface (UI) sistem. 

sudah  dilakukan 

4.  Tahap Implementasi (Implementation) 
implementasi 

adalah 

tahap 

rancangan 

sistem  yang 

proses 
Tahap 
telah 
pembangunan 
implementasi 
direncanakan 
.NET  framework  dengan  bahasa 
menggunakan 
pemrograman  C#,  teks  editor  Visual  Studio,  dan 
Microsoft 
sebagai  Database 
Server 
SQL 
Management System (DBMS). 

sebelumnya, 

5.  Tahap  Uji  Coba  dan  Evaluasi  (Testing  and 

Evaluation) 
Dalam tahapan ini dilakukan uji coba sistem usulan 
kepada  subject  matter  terkait  yaitu  Subdirektorat 
Neraca  Rumah  Tangga  dan  Institusi  Nirlaba. 
Tahapan ini dilakukan untuk menguji kelayakan dan 
kesesuaian sistem dengan masalah yang ada. Sistem 
dievaluasi dengan menggunakan metode Black Box 
Testing, dan System Usability Scale (SUS). 

 2 / 8 

 
 
 
 
D.  Analisis Sistem Berjalan 

Analisis  sistem  berjalan  dilakukan  dengan  metode 
wawancara  dengan  kepala  Subdirektorat  Neraca  Rumah 
Tangga dan Institusi Nirlaba. Berdasarkan hasil wawancara, 
proses  bisnis  kegiatan  penghitungan  PKRT  triwulanan 
digambarkan sebagai berikut : 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

ada  pada 

Masalah  yang 
sistem  berjalan  dapat 
divisualisasikan  dengan  Fishbone  Diagram.  Fishbone 
Diagram  mengkategorikan  masalah  menjadi  tiga  bagian 
yaitu  Manusia  (People),  Informasi  (Information),  dan 
Metode  (Methods).  Masalah  yang  ditemukan  dalam 
Fishbone Diagram digambarkan pada gambar di bawah ini :

Gambar 2. Analisis Permasalahan 

F.  Analisis Kebutuhan Sistem 

Analisis kebutuhan dilakukan dengan mencari solusi 
atas  masalah-masalah  yang  ditemukan  dalam  sistem 
berjalan.  Analisis  kebutuhan  dapat  dipetakan  dengan 
metode PIECES sehingga lebih mudah untuk diidentifikasi 
dengan mengkategorikan masalah dan usulan menjadi enam 
kategori  yaitu  Performance, 
Information,  Economy, 
Control, Efficiency, dan Services. Analisis kebutuhan untuk 
sistem usulan adalah sebagai berikut : 
-  Performance 

Sistem  menyediakan 

fasilitas 

: 

Gambar 1. Analisis Sistem Berjalan 

TABEL I 

TABEL URAIAN PROSES SISTEM BERJALAN 

Uraian Proses 

Kode 
Proses 

ini  adalah  angka  PKRT 

Staff Seksi melakukan input data penyusun angka PKRT 
triwulanan ke lembar kerja Excel. Hasil dari pengolahan 
inputan  data 
triwulanan 
sementara 
Dilakukan  rekonsiliasi  (penyesuaian)  angka  PKRT 
triwulanan  sementara  dengan  menginputkan  angka 
rekonsiliasi 
rekonsiliasi,  dimana  ketika  hasil 
ini 
lebih  dari  5% 
(perbedaan) 
memiliki  diskrepansi 
dibandingkan dengan angka PKRT triwulanan regional 
maka proses rekonsiliasi akan dilakukan lagi 
Kasubdit  dan  Kepala  Seksi  melakukan  pengecekan 
angka  inputan  dan  pengolahan  yang  dilakukan  oleh 
seksi. Jika angka inputan dan pengolahan sudah sesuai 
maka  Kasubdit  dan  Kepala  Seksi  akan  memberikan 
persetujuan kepada Staff Seksi untuk mengirimkan data, 
jika  tidak  maka  Kasubdit  dan  Kepala  Seksi  akan 
meminta Staff Seksi untuk melakukan revisi data 
Staff  Seksi  melakukan 
pengolahan 
Staff Seksi mengirimkan angka PKRT triwulanan final 
ke Subdirektorat Konsolidasi Neraca Pengeluaran 

inputan  dan 

revisi  data 

1 

2 

3 

4 

5 

E.  Analisis Permasalahan 

Berdasarkan proses bisnis sistem berjalan yang  telah 
dipaparkan,  ditemukan  beberapa  masalah  yang  terjadi. 

- 

penghitungan PKRT yang lebih mudah 
Information : Data yang diperlukan untuk penghitungan 
PKRT  triwulanan  cukup  besar.  Sistem  yang  dibangun 
akan menyimpan data-data di database 

-  Economy : Sistem dapat menghemat waktu pengerjaan 

penghitungan PKRT triwulanan 

-  Control 

:  Sistem  berbasis  desktop  supaya 

tetap 

melindungi kerahasiaan data-data yang digunakan 

-  Efficiency 

: 

Sistem  mempercepat 

pengerjaan 

- 

penghitungan PKRT triwulanan 
Services : Sistem dibangun lebih user friendly sehingga 
mempermudah pengerjaan 

V.  KERANGKA PIKIR 

Kerangka pikir yang digunakan pada penelitian ini terdiri 
dari  6  bagian, yaitu  problems,  opportunity,  approach,  system 
development, system implementation, system measurement, dan 
result.  Pertama  peneliti  mencari  permasalahan  yang  terjadi 
pada sistem yang sedang berjalan. Dari masalah tersebut dilihat 
peluang  yang  bisa  digunakan  untuk  menyelesaikan  masalah 
menggunakan  teori-teori  secara  ilmiah  dimana  ditetapkanlah 
sebuah  solusi  yaitu  mengembangkan  sistem  penghitungan 
PKRT  Triwulanan 
yang 
dikembangkan  dibangun  dengan  menggunakan  bahasa 
pemrograman  C#  dengan  .NET  framework,  dan  basis  data 
Microsoft SQL Server. Setelah pembuatan sistem selesai, maka 
selanjutnya  dilakukan  pengujian  menggunakan  Black  Box 
Testing dan SUS. 

desktop.  Sistem 

berbasis 

 3 / 8 

 
 
 
 
 
 
 
 
 
 
Gambar 3. Kerangka Pikir 

VI. HASIL DAN PEMBAHASAN 

A.  Rancangan Bisnis Proses Sistem Usulan 

Berdasarkan  analisis  sitem  berjalan  yang 
telah 
dipaparkan sebelumnya, maka solusi untuk menyelesaikan 
permasalahan adalah dengan membuat sistem penghitungan 
PKRT triwulanan berbasis desktop. Sistem yang diusulkan 
memiliki proses bisnis seperti berikut : 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 4. Rancangan Bisnis Proses Sistem Usulan 

TABEL II 
TABEL URAIAN PROSES SISTEM USULAN 

Uraian Proses 

Kode 
Proses 

1 

2 

3 

4 

5 

6 

7 

8 

9 

10 

triwulanan 

Staff Seksi melakukan input data penyusun angka PKRT 
triwulanan pada aplikasi yang telah dirancang 
Sistem menyimpan inputan data Staff Seksi ke database 
Sistem  mengolah  data  yang  telah  diinput,  dimana  hasil 
dari  pengolahan  ini  berupa  angka  PKRT  triwulanan 
sementara 
Sistem  melakukan  rekonsiliasi  (penyesuaian)  angka 
PKRT 
sementara  menggunakan  angka 
rekonsiliasi.  Jika  diskrepansi  (perbedaan)  angka  PKRT 
triwulanan  sementara  dan  angka  PKRT 
triwulanan 
regional diatas 5% maka sistem akan meminta Staff Seksi 
untuk  melakukan  rekonsiliasi  lagi  (menginputkan  angka 
rekonsiliasi yang baru) 
Staff  Seksi  menginput  angka  rekonsiliasi  yang  baru 
sehingga nantinya diskrepansi akan semakin mengecil 
Sistem  menyimpan  riwayat  angka  rekonsiliasi  dan 
diskrepansi sebelumnya 
Kasubdit dan Kepala Seksi melakukan pengecekan angka 
inputan dan pengolahan yang dilakukan oleh seksi melalui 
sistem.  Jika  angka  inputan dan pengolahan sudah  sesuai 
maka  Kasubdit  dan  Kepala  Seksi  akan  melakukan 
finalisasi, jika tidak maka Kasubdit dan Kepala Seksi akan 
meminta Staff Seksi untuk melakukan revisi data 
Staff Seksi melakukan revisi data inputan 
Sistem  melakukan  finalisasi  angka  PKRT  triwulanan 
sehingga Staff Seksi tidak dapat mengubah data yang telah 
diinput 
Staff Seksi mengunduh angka PKRT triwulanan final pada 
sistem dan mengirimkannya ke Subdirektorat Konsolidasi 
Neraca Pengeluaran 

 4 / 8 

 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

B.  Use-Case Sistem Usulan 

Fungsi yang ada pada sistem usulan adalah mengentri 
data  komponen  penghitung  angka  PKRT  triwulanan, 
melakukan  rekonsiliasi  angka  PKRT  triwulanan,  melihat 
data  hasil  penghitungan  sistem,  finalisasi  angka  PKRT 
triwulanan,  mengelola  user,  dan  mengontrol  nilai 
diskrepansi.  Adapun  rancangan  use  case  sistem  usulan 
seperti gambar berikut : 

Gambar 5. Use Case Sistem  Usulan 

C.  Activity Diagram 

Activity  diagram  digunakan  untuk  menjelaskan  aktivitas 
dari use case yang telah digambarkan. Pada penelitian ini, 
activity diagram nya sebagai berikut : 

1.  Proses Entri 

Gambar 7. Activity Diagram Proses Rekonsiliasi 

3.  Proses Finalisasi 

Gambar 8. Activity Diagram Proses Finalisasi 

4.  Proses Kelola User 

Gambar 6. Activity Diagram Proses Entri 

D.  Implementasi Sistem 

2.  Proses Rekonsiliasi 

Gambar 9. Activity Diagram Proses Kelola User 

 5 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 10. Halaman Login 

Sebelum bisa masuk ke dalam aplikasi, user diminta untuk 
login terlebih dahulu. 

Gambar 13. Tampilan Entri PKRT Tahunan 

Gambar 11. Menu Beranda 

Menu Beranda 
Setelah berhasil login, maka user akan masuk ke tampilan 
Beranda.  Pada  Beranda,  sistem  akan  menampilkan 
informasi  terkait  data  PKRT  triwulanan  pada  tahun  dan 
triwulan  yang  dipilih.  Informasi  yang  ada  yaitu  informasi 
akan sudah atau belumnya data di finalisasi, informasi akan 
diskrepansi  yang  ditetapkan  pada  tahun  dan  triwulan 
tersebut,  informasi  akan  progress  entrian  data  yang  telah 
dilakukan  oleh  staff,  serta 
informasi  akan  progress 
rekonsiliasi. 

Gambar 14. Form isian untuk mengentri data 

Gambar 15. List Menu Rekonsiliasi 

Menu Rekonsiliasi 
Setelah  semua  data  di  entri,  staff  melakukan  rekonsiliasi. 
Rekonsiliasi  dilakukan  dengan  memilih  menu  rekonsiliasi 
dan memilih rekonsiliasi apa yang mau dilakukan (atas dasar 
harga berlaku atau atas dasar harga konstan). 

Gambar 12. List Menu Entri  

Menu Entri 
Staff melakukan entri data dengan memilih menu entri dan 
memilih data apa yang mau di entri. 

Gambar 16. Tampilan Rekonsiliasi ADHB 

Selanjutnya 
staff  dapat  melihat  peringatan  apakah 
diskrepansi diatas angka diskrepansi yang telah ditentukan, 
jika iya maka sistem akan menampilkan peringatan. 

 6 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Data  yang 
selanjutnya dapat di export ke dalam bentuk excel. 

telah  dilakukan  pengolahan  oleh  sistem 

E.  Evaluasi 

Tahapan  akhir  dalam  penelitian  ini  adalah  evaluasi 
sistem dengan menggunakan Black Box Testing dan System 
Usability Scale. 
1.  Black Box Testing 

Black  box  testing  merupakan  metode  evaluasi  sistem 
untuk  melihat  apakah  sistem  yang  dibuat  sudah 
berfungsi  sesuai  yang  diharapkan 
tanpa  melihat 
potongan kode program. Pada penelitian ini black box 
testing dilakukan oleh peneliti, yang mana kesimpulan 
dari  uji  black  box  testing  ini  adalah  sistem  yang 
dibangun sudah sesuai dengan yang diharapkan. 

2.  System Usability Scale (SUS) 

SUS  merupakan  metode  evaluasi  sistem  yang 
bertujuan  untuk  menilai  kebermanfaatan  oleh 
pengguna.  SUS  dilakukan  dengan  memberikan 
kuesioner yang berisikan 10 pertanyaan dengan skala 
nilai  1  sampai  5.  Responden SUS  pada  penelitian  ini 
berjumlah 5 orang yang merupakan pegawai BPS yang 
akan menggunakan sistem ini nantinya. Hasil dari SUS 
ini bernilai 68,5 yang berarti sistem yang dibuat dapat 
diterima oleh pengguna.  

VII. 

PENUTUP 

Kesimpulan dari penelitian mengenai Pembangunan Sistem 

Informasi Penghitungan PKRT Triwulanan ini yaitu : 

1.  Penelitian  ini  telah  menghasilkan  aplikasi  desktop 
penghitungan  PKRT  triwulanan  yang  dapat  menjaga 
integritas  dan  keamanan  data  yang  digunakan  serta 
dapat  mencatatkan 
setiap 
riwayat 
rekonsiliasi dilakukan. 

rekonsiliasi 

2.  Hasil  dari  black  box  testing  dan  SUS  menunjukkan 
bahwa sistem yang dibangun sudah sesuai harapan. 

Adapun saran yang dapat peneliti berikan untuk penelitian 

selanjutnya yaitu : 
1.  Penelitian 

ini  berfokus  pada  hasil  olahan  untuk 
dikirimkan  ke  Subdirektorat  Konsolidasi  Neraca 
Pengeluaran,  sehingga  olahan  data  lanjutan  masih 
belum tercakup pada penelitian ini. 

2.  Berdasarkan  evaluasi  bersama  subject  matter  dan 
pegawai  yang  terlibat  dalam  penghitungan  PKRT 
triwulanan, diharapkan  untuk  bisa  dikembangkan  fitur 
untuk  menampilkan  data  secara  tahunan  atau  semua 
tahun  yang  ada,  tidak  hanya  di  tahun  dan  triwulan 
tertentu. 

DAFTAR PUSTAKA 
[1]  BPS. Produk Domestik Bruto (Lapangan Usaha). Diakses pada : 4 Januari 
[Online].  Available:  https://www.bps.go.id/subject/11/produk-

2021. 
domestik-bruto—lapangan-usaha-.html 

[2]  BPS.  Produk  Domestik  Bruto  (Pengeluaran).  Diakses  pada  :  4  Januari 
2021.  [Online].  Available:  https://www.bps.go.id/subject/169/produk-
domestik-bruto--pengeluaran-. 

 7 / 8 

Gambar 17. Form isian untuk mengentri angka untuk rekonsiliasi 

Rekonsiliasi  dilakukan  dengan  mengentri  nilai  rekonsiliasi 
sehingga  nantinya  diskrepansinya  akan  mengeci.  Ketika 
telah  dilakukan  rekonsiliasi  maka  sistem  akan  menyimpan 
data riwayat rekonsiliasi sebelumnya. 

Gambar 18. List Menu Data 

Menu Data 
Digunakan  untuk  melihat  data-data  yang  telah  di  entrikan  
dan dilakukan pengolahan oleh sistem. 

Gambar 19. Tampilan Data Kirim 

Data Kirim merupakan salah satu sub menu Data. Data ini 
merupakan angka PKRT triwulanan final. 

Gambar 20. Hasil Export Data Kirim 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

[3]  Tim  BPS.  (2019).  Pertumbuhan  Ekonomi  Indonesia  Triwulan  III-2019. 

Jakarta : BPS 

[4]  G. S. Asshadiqi. Pembangunan Sistem Informasi Terintegrasi Konsolidasi 
Neraca  Pengeluaran  Berbasis  Web.  [SKRIPSI].  Jakarta  :  Politeknik 
Statistika STIS, 2019. 

[5]  N.  I.  Hasbah.  Sistem  Aplikasi  Entri  dan  Rekonsiliasi  Data  Produk 
Domestik Bruto Regional Bruto Triwulanan Provinsi (Studi Kasus: Subdit 
Konsolidasi Neraca Pengeluaran Regional). [SKRIPSI]. Jakarta : Sekolah 
Tinggi Ilmu Statistik, 2017. 

 8 / 8 

 
 
 
 
"
221709647,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Ekstraksi Informasi Kecelakaan Lalu Lintas 
dari Teks Berita Online Berbahasa Indonesia 
(Studi Kasus: Provinsi Jawa Timur Tahun 2020) 

Dwi Nissa Ananda (221709647, 4SD2) 
Dosen Pembimbing: Budi Yuniarto, SST., M.Si. 

yaitu 

kelemahan, 

tersebut  memiliki 

Ringkasan—  Tingginya  jumlah  penduduk  di  Provinsi  Jawa 
Timur  membuat  angka  kecelakaan  provinsi  ini  tertinggi  di 
Indonesia.  Korlantas  Polri,  sebagai  penyelenggara  fungsi  lalu 
lintas  memiliki  dashboard  Statistik  Laka  yang  menyediakan 
informasi  laka  lantas  per  polda  di  seluruh  Indonesia.  Namun 
dashboard 
tidak 
menampilkan  peta  titik  lokasi  kecelakaan.  Serta  tampilan 
dashboard  dalam  format  harian  menyebabkan  data  terus 
terganti  setiap  harinya.  Ini  membuat  publik  kehilangan 
informasi historis untuk memahami pola kejadian kecelakaan di 
Jawa Timur tahun 2020. Untuk mengatasi kelemahan data pada 
dashboard  Korlantas  Polri,  diperlukan  pendekatan  lain.  Berita 
online  dapat  menjadi  alternatif  pengumpulan  data  kecelakaan. 
Penelitian  ini  memanfaatkan  model  NER  dengan  pendekatan 
deep  learning  BiLSTM-CNNs  untuk  mengekstraksi  informasi 
kecelakaan  dari teks berita  online. Diperoleh rata-rata f1-scores 
model  NER  dalam  5-fold  CV  sebesar  85,145%.  Hasil  ekstraksi 
kecelakaan 
telah  berhasil 
divisualisasikan  ke  dalam  dashboard.  Dibangun  pula  model 
decision  tree  untuk  menentukan  apakah  kecelakaan  berpeluang 
menimbulkan korban tewas, dengan rata-rata f1-scores dalam 5-
fold  CV  sebesar  68,84%.  Ditemukan  bahwa  kecelakaan  yang 
melibatkan sepeda motor berpeluang 64% menimbulkan korban 
tewas  dan  di-support  dengan  60%  sampel,  sehingga  pengendara 
sepeda motor sebaiknya berhati-hati saat berkendara. 

(termasuk  peta 

lokasi) 

titik 

Kata  Kunci—  kecelakaan, 

lalu 

lintas,  ekstraksi,  NER, 

BiLSTM-CNNs 

I.  LATAR BELAKANG 

Peningkatan  jumlah  penduduk  turut  berdampak  pada 
peningkatan  mobilitas  masyarakat  dan  resiko  kecelakaan  lalu 
lintas.  Berdasarkan  data  Badan  Pusat  Statistik  (BPS),  jumlah 
penduduk  di  Provinsi  Jawa  Timur  menempati  urutan  kedua 
terbanyak  (39.698.600  jiwa)  serta  jumlah  kecelakaannya 
menempari  urutan  pertama  terbanyak  (25.622  peristiwa)  di 
Indonesia pada 2019[1]. 

Korlantas Polri sebagai penyelenggara fungsi lalu lintas[2], 
diharapkan  mampu  meningkatkan  kewaspadaan  masyarakat 
lewat  penyediaan  informasi  laka  lantas  yang  transparan. 
Korlantas  Polri  memiliki  dashboard  Statistik  Laka  yang 
menyediakan  informasi  kecelakaan  lalu  lintas  per  Polda  di 
seluruh  Indonesia[3].  Namun  dashboard 
tidak 
menampilkan peta titik lokasi kecelakaan. Padahal hal tersebut 
bisa  menambah  kehati-hatian  pengendara  ketika  melintasi 
jalan  tersebut.  Serta  tampilan  data  dashboard  dalam  format 
harian  menyebabkan  data  terus  terganti  setiap  harinya.  Ini 
informasi  historis  untuk 
membuat  publik  kehilangan 

tersebut 

menganalisis  pola  kejadian  kecelakaan  di  Jawa  Timur  Tahun 
2020.  Sehingga  untuk  mengatasi  kelemahan  data  pada 
dashboard  Korlantas Polri,  diperlukan sumber data  lain  yang 
tanpa 
titik 
bisa  menyediakan 
kehilangan informasi historis bagi masyarakat. 

lokasi  kecelakaan  dan 

teks  berita, 

tersebut.  Di  dalam 

ini  memanfaatkan  deep 

Berita  online  bisa  menjadi  alternatif  untuk  mengatasi 
kekurangan 
terkandung 
informasi  terkait  kejadian  kecelakaan  lalu  lintas  yang  bisa 
diambil  dengan  melakukan  Information  Extractions  (IE). 
Named Entity Recognition (NER), merupakan tugas dasar dari 
IE  yang  mampu  mengenali  entitas[4].  Kemampuan  NER  ini 
bisa  dimanfaatkan  untuk  mengekstraksi  informasi:  lokasi, 
waktu  kejadian,  jenis  kendaraan  yang  terlibat,  penyebab 
kecelakaan,  jumlah  korban  (luka  dan  tewas),  serta  usia  yang 
terlibat kecelakaan. 
Penelitian 

learning  untuk 
melakukan  ekstraksi,  sebab  memiliki  performa  yang  lebih 
baik dibandingkan shallow learning[5]. Metode deep learning 
yang digunakan adalah NER dengan Bidirectional Long Short 
Term  Memory  Convolutional  Neural  Networks  (BiLSTM-
CNNs).  Ini  dikarenakan  BiLSTM  mampu  menganalisis 
hubungan  antar  kata  di  dalam  urutan  dengan  memperhatikan 
arah 
token  maju  dan  mundur,  serta  CNNs  mampu 
memodelkan informasi sampai level karakter[6]. Penelitian ini 
juga  melakukan  POS  Tagingg  sebagai  fitur  tambahan  bagi 
model  NER  untuk  meningkatkan  performa[7].  Setelah 
mengekstraksi  informasi  dengan  NER,  barulah  atribut-atribut 
dalam  berita  kecelakaan  diambil  menggunakan  metode  Rule 
Based,  yakni  sekumpulan  aturan  yang  dibuat  sendiri  oleh 
peneliti berdasarkan pengetahuan linguistik[8]. 

Akhirnya, dari hasil ekstraksi berita kecelakaan yang telah 
didapat,  data  divisualisasikan  ke  dalam  dashboard  dan 
dilakukan  analisis  deskriptif.  Selain  itu  untuk  menghindari 
terjadinya  laka  lantas  yang  menyebabkan  tewasnya  korban, 
maka  dibuat  model  Pohon  Keputusan  (Decision  Tree)  yang 
dapat  menarik  beberapa  aturan  keputusan  untuk  menentukan 
apakah kejadian kecelakaan berpeluang menyebabkan korban 
tewas  di  Provinsi  Jawa  Timur  tahun  2020.  Metode  Decision 
Tree  dipilih  karena  prosesnya  mirip  dengan  proses 
pengambilan  keputusan  yang  dilakukan  manusia,  sehingga 
lebih mudah untuk dipahami dan diinterpretasi. 

Atas  alasan  itulah,  peneliti  tertarik  melakukan  ekstraksi 
informasi kecelakaan lalu lintas di Provinsi Jawa Timur tahun 
2020  dari  teks  berita  online  Berbahasa  Indonesia,  kemudian 
memvisualisasikan  hasilnya  ke  dalam  dashboard,  membuat 
analisis deskriptif serta menganalisis pola kecelakaan. 

 1 / 8 

 
 
 
 
 
II.  TUJUAN PENELITIAN 

Tujuan penelitian ini antaralain: 

1.  Merancang  dan  melakukan  tahapan  ekstraksi  informasi 
kecelakaan lalu lintas di Provinsi Jawa Timur tahun 2020 
dari teks berita online 

2.  Melakukan  visualisasi  dan  analisis  deskriptif  data 
kecelakaan lalu lintas di Provinsi Jawa Timur tahun 2020 
dari hasil ekstraksi teks berita online. 

3.  Mengetahui  aturan  keputusan  untuk  menentukan  apakah 
kecelakaan  lalu  lintas  berpeluang  menyebabkan  korban 
tewas  di  Provinsi  Jawa  Timur  tahun  2020  dari  hasil 
ekstraksi teks berita online dengan Decision Tree. 

III. PENELITIAN TERKAIT 

yang 

pernah 

dilakukan.  Penelitian 

Terdapat  penelitian  tentang  ekstraksi  informasi  berita 
kecelakaan 
[9], 
mengekstraksi  informasi  kecelakaan  dari  teks  berita  online 
Berbahasa 
Inggris  menggunakan  model  NER  untuk 
mendapatkan lokasi dan waktu kecelakaan, sementara plat dan 
tipe  kendaraan  diperoleh  dari  regular  expression.  POS 
Tagging ditambahkan sebagai input untuk model NER. Angka 
jumlah  korban  yang  tewas  dan  terluka  didapat  dengan 
yang 
melakukan 
mengidentifikasi  predikat  dalam  kalimat.  Bila  ditemukan 
angka  berformat  kata,  akan  dikonversi  ke  format  angka 
dengan algoritma word to number. 

Semantic  Role 

Labelling 

(SRL) 

Penelitian  kali ini  secara umum  juga  melakukan  hal  yang 
mirip,  yaitu  mengekstraksi  berita  kecelakaan  dengan  model 
NER.  Namun  terdapat  beberapa  perbedaan,  yakni  penelitian 
kali  ini  mengekstraksi  berita  berbahasa  Indonesia,  yang 
tentunya  memiliki  perbedaan  struktur  bahasa  dibanding  teks 
berbahasa  Inggris.  Serta  dalam  mencari  jumlah  korban  luka 
dan  tewas,  penelitian  ini  tidak  melakukan  SRL,  melainkan 
menggunakan rule based dengan memanfaatkan hasil prediksi 
tag POS, tag NER serta kamus kosakata yang didefinisikan. 

Klasifikasi  menggunakan  deep  learning  memiliki  akurasi 
yang lebih baik daripada shallow learning [5]. Penelitian [10], 
pernah  membandingkan  model  deep  learning  NER  dengan 
metode:  BiLSTM,  BiLSTM-CNNs,  BiLSTM-LSTM  serta 
BiLSTM-CNNs-LSTM.  Diperoleh  nilai 
terbaik 
sebesar 79.43% bila menggunakan BiLSTM-CNNs, sehingga 
ditarik kesimpulan bahwa model paling rumit tidak menjamin 
memberi  performa  paling  baik.  Penting  untuk  menyesuaikan 
kompleksitas model terhadap  jenis data yang digunakan. 

f1-score 

[7], 

Penelitian 

pernah  melakukan  NER 

dengan 
Multinomial Naïve Bayes untuk mendeteksi entitas dari  tweet 
Berbahasa  Indonesia  dengan  POS  Tagging  sebagai  fitur 
tambahannya.  Didapatkan  bahwa  penambahan  fitur  POS 
Tagging mampu meningkatkan performa model, dengan nilai 
f1-scores mencapai 80%. 

Penelitian [11], melakukan ekstraksi artikel musik dengan 
terlebih  dahulu  melakukan  NER.  Setelah  itu,  hasil  ekstraksi 
diambil  dengan  Rule  Based  yang  berupa  sekumpulan  aturan 
informasi. 
yang  didefinisikan  peneliti  untuk  mengambil 
Entitas  yang  diambil  antara  lain:  nama  acara,  nama  artis, 
waktu dan tempat. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

terhadap 

‗relevan‘  atau 

‗tidak  relevan‘ 

Sebelum  melakukan  ekstraksi,  perlu  dipastikan  relevansi 
berita  yang  dianalisis  dengan  mengklasifikasikannya  sebagai 
topik 
kategori 
kecelakaan lalu lintas.  Tugas  ini tidak perlu dilakukan secara 
manual,  karena  bisa  memanfaatkan  bantuan  model.  Semi-
supervised learning dapat meningkatkan kinerja model hanya 
dengan  menggunakan  sedikit  data  latih.  Penelitian  [12], 
pernah  mengklasifikasikan  teks  berita  berbahasa  Indonesia 
menggunakan  Naïve  Bayes.  Teknik  semi-supervised  learning 
dengan algoritma self-training dijalankan dengan mencobakan 
nilai threshold dari rentang 0.7 sampai 0.9 pada dataset. Nilai 
f1-score  terbaik  untuk  data  split  80%  train  dan  20%  test, 
diperoleh  dengan  memberi  label  pada  20%  data  training  dan 
menetapkan threshold 0.75. 

isi  konten, 

Teks  kecelakaan  pada  berita  online  tidak  lepas  dari 
kemungkinan  duplikasi.  Satu  kejadian  kecelakaan  bisa  saja 
diceritakan  secara  berulang  di  berita  lain.  Perlu  diperiksa 
kesamaan  setiap  pasangan 
lalu  menghapus 
duplikasi.  Penelitian  [13],  pernah  membandingkan  cosine 
dengan  jaccard  similarity  untuk  mengelompokkan  dokumen 
skripsi dan memperoleh tingkat akurasi cosine similarity lebih 
tinggi  dibandingkan 
jaccard  similarity,  yakni  94,98%. 
Penelitian  [14],  melakukan  deteksi  plagiarisme  karya  ilmiah 
dengan  memeriksa  nilai  cosine  similarity  dokumen  dan 
menetapkan  threshold  0.6  sebagai  batas  parameter  tingkat 
plagiarisme yang patut dicurigai. 

IV. METODE PENELITIAN  

A.  Ruang Lingkup Penelitian 

Penelitian  ini  dibatasi  hanya  menganalisis  kecelakaan  di 
Provinsi  Jawa  Timur  pada  2020.  Terdapat  delapan  variabel 
yang akan dicoba untuk diekstraksi, yakni: lokasi, hari/tanggal, 
waktu, kelompok umur korban, jenis kendaraan yang terlibat,  
jumlah  korban  tewas  dan  luka  serta  penyebab  kecelakaan. 
Jenis  kendaraan  yang  diteliti  terbatas  pada  enam  kategori: 
sepeda, sepeda motor, mobil, bus, truk serta kereta api. 

B.  Metode Pengumpulan Data 

Data  dikumpulkan  dengan  melakukan  scraping  berita 
online  di  dua  website  daerah  (jatimnow.com,  jawapos.com), 
dan  satu  website  nasional  (detik.com).  Terdapat  2  kata  kunci 
yang di-input, yaitu: ‗kecelakaan‘ dan ‗tabrak‘. 

C.  Metode Analisis 
C.1  Preprocessing 

Preprocessing  meliputi 

serangkaian 

tahapan  yang 

dilakukan pada data sebelum sampai ke tahap ekstraksi. 

Data Cleaning 

Dilakukan  pembersihan  awal  pada  berita  hasil  scraping, 
meliputi:  menghapus  berita  dengan  missing  content;  filter 
kolom,  menyeragamkan  format  tanggal  tiap  website,  serta 
merging  data  berdasarkan  URL.  Perlu  diperhatikan,  Content 
(masih 
berita  hasil  data  cleaning 
mengandung  karakter  non-ASCII  dan  teks  tambahan  yang 
biasanya terdapat pada artikel berita online). 

ini  masih  mentah 

 2 / 8 

 
 
 
 
 
 
 
 
Menghilangkan Konten yang Tidak Relevan 

terhadap 

‗relevan‘  atau 

‗tidak  relevan‘ 

Sebelum  mengekstraksi,  penting  untuk  memastikan 
sebagai 
relevansi  berita  dengan  mengklasifikasikannya 
kategori 
topik 
kecelakaan  lalu  lintas,  kemudian  berita  yang  tidak  relevan 
dihapus. Tugas ini tidak perlu dilakukan  manual, karena  bisa 
memanfaatkan bantuan model. Dipilih teknik klasifikasi Naïve 
Bayes Semi-Supervised Learning, karena meskipun sederhana 
Naïve  Bayes  memiliki  akurasi  yang  tinggi[15]  dan  semi-
supervised learning dapat meningkatkan kinerja model hanya 
dengan sedikit data latih[12]. 
  Preprocessing 

Content  yang  telah  melalui  tahap  data  cleaning,  di-
preprocess  dengan  melakukan:  case  folding,  menghilangkan 
teks 
tokenizing, 
tambahan  dan  karakter  non-ASCII, 
menghapus stopwords dan punctuation, serta stemming. 
  Process 

Beberapa data dilabeli secara manual, lalu di-split dalam 5-
fold  CV  (80%  train  set,  20%  test  set)  untuk  mengukur 
performa model. Dari data training, dipilih 20% data berlabel 
(L)  dan  80%  data  tidak  berlabel  (U).  Algoritma  self-training 
dijalankan secara iteratif di tiap lipatan, dimana hasil prediksi 
dengan  nilai  probabilitas                  akan  dianggap 
sebagai  data  pseudo-label[12].  Model  akhir  dibangun  dari 
gabungan 
training  dan  dipakai  untuk 
memprediksi relevansi seluruh Content terhadap kejadian laka. 
Hasil prediksi yang berlabel ‗tidak relevan‘ dibuang dan tidak 
diikutkan pada analisis berikutnya. 

seluruh  data 

Menghilangkan Konten yang Mirip 

Pada penelitian ini, satu berita diasumsikan mewakili satu 
kejadian  unik  kecelakaan,  sehingga  duplikasi  berita  harus 
similarity  untuk 
dihilangkan.  Dipilih  metode 
memeriksa  kemiripan  konten  berita,  sebab  metode  cosine 
memiliki performa yang lebih unggul dibanding jaccard[13]. 
  Preprocessing 

cosine 

Content yang telah melalui preprocessing pada klasifikasi 

Naïve Bayes, dilakukan tokenizing. 
  Process 

Content  berita  divektorisasi  dengan  TF-IDF,  kemudian 
nilai  kemiripan  (cosine)  antardokumen  diukur  dan  disimpan 
ke  dalam  matriks.  Pasangan  berita  yang  mirip  (nilai 
cosine>0.6)[14]  dikelompokkan  bersama,  kemudian  seluruh 
berita didalam kelompok  tersebut dibuang  kecuali  satu  berita 
yang  pertamakali  diunggah.  Output  pada  tahapan  ini,  adalah 
isi content hasil data cleaning (berupa teks asli) yang di-filter 
tersisa  dari  matriks  cosine 
dengan 
similarity 
Ini  dilakukan  untuk 
mempertahankan  keutuhan  Content  (kata,  kapitalisasi,  tanda 
baca,  dll.),  sehingga  model  NER  nantinya  dapat  memahami 
konteks dengan baik saat proses ekstraksi. 

(bebas  dari  duplikasi). 

indeks  berita  yang 

POS Tagging 

Berita  selanjutnya  diberi  tag  POS  berupa pengelompokan 
kata kedalam part-of-speech, seperti: seperti kata kerja (verb), 
kata benda (noun), dsb. Hasil tag POS nantinya menajdi fitur 
tambahan untuk meningkatkan akurasi model NER[7]. Selain 
itu, tag POS juga dipakai untuk mengekstraksi jumlah korban 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

karena  dapat  mengenali  angka  yang  berformat  kata.  POS 
Tagging  diimplementasikan  dengan  Flair,  sebuah  library 
yang  memudahkan  tugas  POS  Tagging  dan  mengungguli 
metode  NLP  lainnya.  Peneliti  tidak  perlu  menyiapkan  data 
training 
telah  menyediakan  corpus 
mereka[16].  Metode  yang  digunakan  Flair  adalah  BiLSTM-
CRF[17]. 
  Prerprocessing 

lagi,  karena  Flair 

Output berita dari tahap cosine similarity di-preprocessing 
dengan:  menghilangkan  teks  tambahan  dan  karakter  non-
ASCII, serta tokenizing pada level kata dan kalimat 
  Process 

Dari  corpus  yang  ada,  dibuat  dictionary  dan  embeddings 
untuk  input  model.  Model  dilatih  menggunakan  parameter: 
lerning_rate=0.1,  mini_batch_size=32,  max_epoch=10[18], 
lalu model dipakai untuk memprediksi tag POS seluruh token. 

C.2  Process 

Setelah  melakukan  serangkaian  tahapan  preprocessing, 
akhirnya  teks  berita  memasuki  tahap  process,  yang  meliputi 
tahapan ekstraksi sampai dengan analisis. 

Ekstraksi dengan NER BiLSTM-CNNs 

Named Entity Recognition (NER) merupakan tugas utama 
ini,  sebab  NER  dapat  mengekstraksi 
dalam  penelitian 
informasi  kecelakaan  dengan  mengidentifikasikan  entitas 
kedalam  kelas-kelas  yang  telah  didefinisikan,  seperti:  orang, 
lokasi,  dsb..  Dipilih  pendekatan  NER  deep  learning  dengan 
Bidirectional Long Short Term Memory Convolutional Neural 
Networks (BiLSTM-CNNs), sebab Bidirectional LSTM dapat 
memperhitungkan  urutan  token  dalam  dua  arah  dan  CNNs 
mampu memodelkan informasi sampai level karakter[6]. 
  Preprocessing 

NER  memerlukan teks asli tanpa  penghilangan  stopwords 
untuk  memahami  konteks.  Lemmatisasi  dan  stemming  juga 
tidak  diterapkan  untuk  mencegah  terpotongnya  informasi 
sensitif[19]. Preprocessing dilakukan dengan menyusun token 
kedalam tabel untuk memudahkan anotasi manual. 
  Process 

Sampel  berita  diambil  secara  acak,  kemudian  token 
dianotasi. Entitas yang diekstraksi meliputi: LOC, ORG, PER, 
MISC, DATE, TIME, VEHICLE, PLATE, AGE dan CAUSE 
yang  masing-masing diberi notasi BIO (Begin, Inside, Other) 
untuk  menandai  urutan.  Data  dipecah  kedalam  array  per 
kalimat,  dimana  tiap  array  menyimpan  informasi  mengenai: 
word,  character,  tag  NER  dan  tag  POS  tiap  token  dalam 
kalimat  tersebut.  Array  dibagi  dalam  5-fold  untuk  mengukur 
kebaikan  model.  Seluruh  array  dipetakan  kedalam  vector 
berdasarkan  dictionary  dan  embeddings.  Array  kemudian 
dikelompokkan  kedalam  ukuran  batch berdasarkan kesamaan 
jumlah tokennya. 

Model  NER  dibangun  dengan  parameter:  50  epochs,  0.5 
dropout,  200  LSTM  state  size,  3  convolutional  width  serta 
Nadam  optimizer[20].  Pada  penelitian 
ini,  ditambahkan 
additional feature berupa pos input layer (hasil POS Tagging) 
untuk meningkatkan performa model NER. Sehingga baseline 
model  terdiri  atas  empat  input,  yakni:  character-level  input 
(CNNs),  word-level  input,  casing  input  dan  pos  input. 

 3 / 8 

 
 
 
 
 
Awalnya  keempat  input  tersebut  diproses  secara  independen, 
kemudian digabung untuk diproses bersama. 

Gambar 1. Arsitektur model NER BiLSTM-CNNs 

1.  Character Level Input 

Pengenalan  level  karakter  dapat  menangani  kata 

diluar vocab (OOV) serta kesalahan pengejaan.  
a)  Character Embeddings Layer 

Lapisan 
ini  memetakan  seluruh  karakter  dalam 
embeddings  30  dimensi  yang  diinisiasi  secara  acak 
menggunakan  distribusi  Uniform  (-0.5,  0.5).  Setiap 
susunan  karakter  diberikan  zero-padding  di  akhir 
urutan dengan panjang maksimum 52 karakter untuk 
mencegah terbuangnya informasi. 

b)  Dropout Layer 

Lapisan dropout dengan rate 0.5 diterapkan ke level 
karakter sebelum dan setelah memasuki CNNs 

c)  Convolutional Neural Networks (CNNs) 

Lapisan  CNNs  terdiri  atas:  1D  convolutional  layer, 
maxpooling  layer,  serta  flatten  layer.  Lapisan  1D 
convolutional  dibentuk  dengan  kernel  size  3,    filters 
30,  fungsi  aktivasi  tanh,  serta  stride  1.  Kemudian, 
input  character  masuk  ke  maxpooling 
layer 
berukuran window=52 yang mereduksi dimensi input 
(downsampling)  dengan  hanya  mengekstrak  nilai 
maksimum  dari  area.  Kemudian,    proses  dilanjutkan 
ke flatten layer, untuk mengubah data menjadi array 
satu dimensi sebagai input bagi layer berikutnya. 

2.  Word Level Input 

Pada 

ini  kosakata  dipatakan  kedalam 
embeddings 50 dimensi menggunakan GloVe[21], dimana 

lapisan 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

bobot  dimensi  input  seukuran  dengan  output,  yakni  50. 
Embeddings  diinisiasi 
acak  menggunakan 
distribusi Uniform (-0.5,0.5). 
3.  Casing Input 

secara 

Lapisan  ini  memetakan  casing  menggunakan  bobot 
caseEmbeddings.  Dimensi  input  dan  output  seukuran 
dengan  dimensi  caseEmbeddings.  Embeddings  diinisiasi 
menggunakan distribusi Uniform (-0.5,0.5). 
4.  POS Input 

Lapisan  ini  memetakan  tag  pos  menggunakan  bobot 
posEmbeddings.  Dimensi  input  dan  output  seukuran 
dengan  dimensi  posEmbeddings.  Embeddings  diinisiasi 
menggunakan distribusi Uniform (-0.5,0.5). 
5.  Concatenation 

Lapisan  ini  menggabungkan  vektor  character-level, 

word-level, serta casing dan pos menjadi sebuah vector. 
6.  Bidirectional LSTM (BiLSTM) 

Lapisan  ini  mengubah  data  hasil  penggabungan 
(concatenation) menjadi 2 vector dengan 200 unit LSTM 
state  size:  satu  input  menerapkan  rekusri  maju,  dan 
mundur.  Digunakan  dropout  sebesar  0.5  dan  dropout 
recurrent sebesar 0.25. 
7.  Dense Output Layer 

Fungsi  softmax  diterapkan  untuk  menghasilkan 
output  prediksi  dari  kata  yang  diinput.  Dimensi  dense 
seukuran dengan jumlah label. 

Ekstraksi dengan Rule Based 

Sekumpulan  aturan  (rule)  didefinisikan  untuk  mengambil 
informasi  lokasi,  tanggal,  waktu,  kelompok  umur,  jenis 
kendaraan  yang 
jumlah  korban,  dengan 
memanfaatkan fitur: tag NER, tag POS, serta kamus kosakata. 
  Rule Based untuk Ekstraksi Lokasi Kecelakaan 

terlibat  serta 

Bila token mengandung tag NER ―LOC‖ dan diawali kata 
―di‖,  maka  token  dalam  kalimat  tersebut  diambil  sebagai 
lokasi kecelakaan dan proses dihentikan. String lokasi diubah 
ke koordinat (geocoding) untuk dipetakan dalam dashboard. 
  Rule Based untuk Ekstraksi Tanggal Kecelakaan 

Bila  token  mengandung  tag  NER  ―DATE‖,  maka  token 
dalam  kalimat  tersebut  dikonversi  ke  format  datetime.  Bila 
berhasil terkonversi, token diambil sebagai tanggal kecelakaan 
dan proses dihentikan. 
  Rule Based untuk Ekstraksi Waktu Kecelakaan 

Bila  token  mengandung  tag  NER  ―TIME‖,  maka  token 
dikonversi ke format datetime, digolongkan ke kategori waktu 
(pagi, siang, sore, malam) dan proses dihentikan. Namun, bila 
token  langsung  menyebut  salahsatu  kategori  waktu,  token 
diambil sebagai waktu kecelakaan dan proses dihentikan. 
  Rule Based untuk Ekstraksi Kelompok Umur yang 

Terlibat 
Pengecekan  dilakukan  sampai  akhir  kalimat  berita.  Bila 
token  mengandung  tag  NER  ―AGE‖,  maka  token  dikonversi 
menjadi angka dan ditambahkan ke variabel penyimpan umur. 
  Rule Based untuk Ekstraksi Jenis Kendaraan yang 

Terlibat 
Pengecekan  dilakukan  sampai  akhir  kalimat  berita.  Jenis 
kendaraan yang terlibat digolongkan ke enam kategori (mobil, 
bis,  truk,  sepeda,  sepeda  motor,  ka).  Bila  token  mengandung 

 4 / 8 

 
 
 
 
 
tag  NER  ―VEHICLE‖  dan  word  mengandung  salahsatu  dari 
kategori  jenis  kendaraan,  maka  kategori  kendaraan  tersebut 
diubah keterangannya menjadi ―ditemukan‖. 
  Rule Based untuk Ekstraksi Jumlah Korban Tewas dan 

Terluka 
Awalnya  diperiksa  apakah  terdapat  angka  yang  langsung 
menyebutkan  jumlah  korban  kecelakaan.  Angka  berformat 
kata  yang  terdeteksi  dari  tag  POS  ―NUM‖,  diubah  ke  digit 
dengan  algoritma  text2int.  Namun  jika  tidak,  cara  lainnya 
adalah  memetakan  seluruh  nama  orang  dan  dihubungkan 
dengan  jenis  kendaraan  pada  posisi  token  terdekat.  Nama 
orang  kemudian  diisi  keterangan,  dengan  memerhatikan 
kedekatan kata ―tewas‖ dan ―luka‖ terhadap nama orang/jenis 
kendaraannya.  Keterangan  tersebut  bisa  terisi  dan  bisa  juga 
kosong.  Bila  keterangan  kosong,  maka  diasumsikan  orang 
dalam keadaan selamat (tidak mengalami luka, ataupun tewas). 

Menghilangkan Berita Duplikat dari Hasil Ekstraksi 

Nilai  threshold  0.6  pada  cosine  similarity  tidak  bisa 
menghilangkan  duplikasi  berita  secara  sempurna.  Sehingga 
setelah  hasil  ekstraksi  berita  didapatkan,  duplikasi  dihapus 
lagi dengan ketentuan: berita yang memiliki kesamaan tanggal, 
waktu  dan  tempat  akan  dianggap  sebagai  satu  kejadian  yang 
sama, sehingga duplikasi dihilangkan 

Visualisasi dan Analisis Deskriptif dari Hasil Ekstraksi 

Hasil  ekstraksi  yang  telah  didapat  divisualisasi  kedalam 
dashboard  berbasis  web  menggunakan:  HTML,  CSS, 
Bootstrap,  Chart.js  serta  platform  Mapbox.  Dibuat  pula 
analisis deskriptif data kecelakaan dari visualisasi tersebut. 

Aturan Keputusan yang Menentukan Apakah Kecelakaan 
Berpeluang Menimbulkan Korban Tewas 

Akan  dicari  aturan  yang  menentukan  apakah  kecelakaan 
berpeluang  menimbulkan  korban  tewas  dengan  membuat 
Decision  Tree.  Metode  decision  tree  yang  dipilih  adalah 
CART  (Classification  and  Regression  Trees)  sebab  dapat 
lebih  mudah 
dipakai  pada  skala  kategorik  dan  rasio, 
diinterpretasi  dan  lebih  akurat[22].  Yang  dijadikan  variabel 
input,  yakni:  hari,  waktu,  kelompok  umur  serta 
jenis 
kendaraan  yang  terlibat.  Sementara  variabel  target  berupa 
kategori: ‘Die’ bila kejadian menimbulkan korban tewas, serta 
‘No Die’ bila kejadian tidak menimbulkan korban tewas. Data 
dibersihkan dari missing value dan variabel multilabel diubah 
ke  bentuk  dummy.  Dilakukan  5-fold  CV  untuk  mengukur 
performa  model,  kemudian  seluruh  data  digabung  untuk 
membangun model akhir decision tree. 

V.  KERANGKA PIKIR 

Kerangka  pikir  penelitian 
ditunjukkan pada Gambar 2 dibawah 

ini 

sebagaimana  yang 

Angka kecelakaan di 
Provinsi Jawa Timur 
terbanyak se-
Indonesia 

Dashboard Statistik Laka dari 
Korlantas Polri tidak menampilkan 
peta lokasi kecelakaan, serta data 
berformat harian membuat 
informasi historis hilang 

Diperlukan sumber data lain: Berita online 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Tujuan Penelitian 

Mengekstraksi 
informasi 
kecelakaan dari teks 
berita online 

Melakukan visualisasi 
dan analisis deskriptif 
data kecelakaan hasil 
ekstraksi 

Mengetahui aturan yang menentukan 
apakah kecelakaan berpeluang 
menimbulkan korban tewas 

Metode Penelitian 

Preprocessing 

1. Data Cleaning 
2. Hilangkan data tidak relevan 
3. Hilangkan data yang mirip 
4. POS Tagging 

Process 

1. Ekstraksi dengan NER 
2. Ekstraksi dengan Rule Based 
3. Hilangkan kembali duplikasi 
4. Visualisasi dan analisis hasil 
5. Bangun Decision Tree 

Hasil 

Dashboard informasi 
kecelakaan dengan peta titik 
lokasi dan tanpa kehilangan 
informasi historis 

Analisis 
deskriptif dari 
visualisasi 

Model Decision Tree yang 
menentukan apakah kecelakaan 
berpeluang menimbulkan korban tewas 

Gambar 2. Kerangka pikir penelitian 

VI. HASIL DAN PEMBAHASAN 

A.  Pengumpulan Data 

Berita  dikumpulkan  dengan  melakukan  scraping  pada 
website jatimnow, jawapos dan detik menggunakan keywords 
‗kecelakaan‘ dan ‗tabrak‘. Namun fitur advanced search pada 
website  utama jawapos.com  hanya bisa  mendeteksi  keywords 
dari  judul  berita,  tanpa  memerhatikan  isi  konten.  Sehingga 
berita juga diambil dengan Google News. Akan tetapi, banyak 
didapat  berita  dari  Google  News  yang  tidak  relevan  dengan 
peristiwa  kecelakaan.  Scraping  diimplementasikan  dengan 
library  selenium,  requests  dan  BeautifulSoup  di  python. 
Diperoleh total berita hasil scraping sebanyak 4303 dokumen. 

B.  Analisis 
B.1  Preprocessing 

Data Cleaning 

Data hasil scraping melalui serangkaian tahapan cleaning. 
Total  berita  berkurang  menjadi  4297  dokumen  setelah 
penghilangan missing content, menjadi 4143 dokumen setelah 
filtering tanggal, dan menjadi 3855 dokumen setelah merging. 

Menghilangkan Data yang Tidak Relevan 

Digunakan  model  Klasifikasi  Naïve  Bayes  dengan  Semi 
Supervised  Learning  untuk  memprediksi  relevansi  berita 
terhadap  peristiwa  kecelakaan 
lintas,  kemudian 
membuang  hasil  prediksi  yang  berlabel  ‗tidak  relevan‘. 
Sebanyak  1928  dokumen  dilabel  secara  manual.  Algoritma 

lalu 

 5 / 8 

 
 
 
 
 
 
 
 
 
  
 
 
  
self-training  Naïve  Bayes  dijalankan  secara  iteratif  dengan 
fungsi  multinomialNB()  di  python.  Rata-rata 
f1-score 
diperoleh sebesar 95%. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

TABEL I 
NILAI KEBAIKAN MODEL SELF TRAINING NIVE BAYES 
Accuracy 
0.972 
0.956 
0.953 
0.971 
0.966 
0.964 

Precision 
0.957 
0.942 
0.93 
0.964 
0.95 
0.949 

F1-score 
0.961 
0.939 
0.936 
0.961 
0.954 
0.95 

Fold 
1 
2 
3 
4 
5 
Mean 

Recall 
0.964 
0.936 
0.943 
0.957 
0.957 
0.951 

Model akhir dipakai untuk memprediksi relevansi seluruh 
sebanyak  1408  dokumen 
berita.  Dari  hasil  prediksi, 
terhadap  kasus 
‗relevan‘ 
sebagai  berita 
teridentfikasi 
kecelakaan  lalu  lintas,  dan  2447  sisanya  ‗tidak  relevan‘. 
Hanya  data    yang  relevan  saja  yang  dianalisis  ke  tahap 
selanjutnya.  

Menghilangkan Konten yang Mirip 

Tahap 

ini  menghilangkan  duplikasi  berita  dengan 
menghitung  nilai  kemiripan  (cosine)  setiap  pasang  dokumen, 
kemudian  menghilangkan  kelompok  dokumen  yang  mirip 
(cosine>0.6)[14].  Matriks  nilai  cosine  ditunjukkan  pada 
Gambar 3 dibawah. 

Gambar 4. Precision, recall dan f1-scores model POS Tagging 

B.2  Process 

Ekstraksi dengan NER BiLSTM-CNNs 

NER  merupakan  tugas  utama  pada  penelitian  ini  yang 
dapat mengekstraksi informasi kecelakaan melalui pengenalan 
entitas bernama. Diambil 60205 token (228 berita) secara acak 
untuk dianotasi manual. Data dipecah kedalam sentence yang 
dibagi dalam 5-fold untuk dievaluasi. Setelah memvektorisasi 
setiap sentence, model dibangun dan dilatih dalam 50 epochs. 
Nilai kebaikan model pada tiap fold disimpan secara otomatis 
ke  file  .txt.  Rata-rata  f1-scores  model  NER  terhadap  data 
testing sebesar 85,15%. 

TABEL II 
NILAI F1-SCORES MODEL NER BiLSTM-CNNs 

Fold 

1 
2 
3 
4 
5 
Mean 

F1-score 

Train set 
0.90777 
0.93198 
0.93192 
0.92762 
0.92971 
0.9258 

Test set 
0.84474 
0.85233 
0.84534 
0.85629 
0.85853 
0.85145 

Gambar 3. Matriks cosine setiap pasang dokumen berita 

Cosine  bernilai  antara  0  sampai  1,  dimana  semakin 
mendekati 1, artinya isi content pasangan berita semakin mirip. 
Tahapan  ini  mengurangi  data  hingga  tersisa  1247  artikel, 
kemudian  Content  asli  dari  tahap  data  cleaning  di-filter 
dengan indeks berita yang masih tersisa. 

POS Tagging 

Tag POS nantinya dipakai sebagai additional feature bagi 
model  NER  serta  untuk  mendapat  jumlah  korban.  POS 
tagging  diimplemetasikan  dengan  library  Flair  di  python. 
Model  dilatih  dengan  corpus  yang  disediakan  Flair[16]. 
Pelatihan  dilakukan  di  Google  Colab  karena  prosesnya  yang 
cukup berat. Diperoleh f1-scores terhadap data testing sebesar 
92,32%.  Performa  model  dalam  mengenali  tiap  class  dapat 
dilihat  pada  Gambar  4.  Model  tersebut  kemudian  digunakan 
untuk memprediksi tag POS seluruh token. 

TABEL III 
NILAI F1-SCORES MODEL NER BiLSTM-CNNs SETIAP ENTITAS 

F1-scores 

Entitas 

Fold-
1 

Fold-
2 

Fold-
3 

Fold-
4 

Fold-
5 

Mean 

AGE 
CAUSE 
DATE 
LOC 
MISC 
ORG 
PER 
PLATE 
TIME 
VEHICLE 

0.96 
0.37 
0.98 
0.85 
0.73 
0.67 
0.9 
0.91 
0.96 
0.86 

0.94 
0.4 
0.97 
0.85 
0.69 
0.85 
0.9 
0.98 
0.86 
0.91 

0.95 
0.25 
0.97 
0.89 
0.64 
0.69 
0.87 
0.97 
0.9 
0.92 

0.96 
0.48 
0.95 
0.91 
0.69 
0.68 
0.9 
0.98 
0.93 
0.89 

0.96 
0.36 
0.98 
0.89 
0.79 
0.76 
0.92 
0.93 
0.94 
0.89 

0.95 
0.37 
0.97 
0.88 
0.71 
0.73 
0.9 
0.95 
0.92 
0.89 

Entitas CAUSE (yang menerangkan penyebab kecelakaan) 
memiliki  nilai  f1-score  yang  rendah,  yaitu  sebesar  37%.  Hal 
ini  dikarenakan,  penyebab  kecelakaan  seringkali  ambigu  dan 
sulit  untuk  ditentukan  saat  proses  anotasi.  Sehingga  model 

 6 / 8 

 
 
 
 
 
 
 
 
 
 
 
yang  dihasilkan  menjadi  kurang  mampu  memahami  pola 
untuk  mengenali  penyebab  kecelakaan.  Model  akhir 
digunakan untuk memprediksi tag NER pada seluruh data. 

Ekstraksi dengan Rule Based 

Sekumpulan  rule  dibuat  untuk  mengambil  informasi: 
lokasi,  tanggal,  waktu,  umur,  jenis  kendaraan  dan  jumlah 
korban  dari  hasil  ekstraksi  NER.  Koordinat  kecelakaan 
diperoleh dengan melakukan geocoding menggunakan library 
geopy di python. 

Menghilangkan Berita Duplikat dari Hasil Ekstraksi 

Pada  tahap  ini  dilakukan  penghapusan  kembali  duplikasi 
berita  berdasarkan  hasil  ekstraksi  karena  nilai  ambang  batas 
kemiripan  0.6  pada 
tidak  bisa 
menghilangkan  seluruh  berita  yang  mirip.  Sehingga  kini 
tinggal tersisa sebanyak 1232 dokumen berita. 

tahap  cosine  similarity 

Visualisasi dan Analisis Deskriptif dari Hasil Ekstraksi 

Hasil  ekstraksi  divisualisasikan  kedalam  dashboard 
berbais  web.  Dari  geocode,  hanya  dipetakan  lokasi  yang 
memiliki  keterangan  jalan  dan  terletak  di  Prov.  Jawa  Timur. 
Koordinat lokasi kecelakaan dikonversi ke GeoJSON sebelum 
dipetakan melalui platform Mapbox. Tampak pada Gambar 5, 
titik lokasi kecelakaan dari hasil ekstraksi berita online sudah 
bisa dipetakan ke dalam dashboard. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

 

 

 

Jumlah korban luka tertinggi di Bulan Januari (183 orang) 
dan terendah di Bulan April (44 orang) 
Jumlah korban tewas tertiggi di Hari Senin (2013 orang) 
dan terendah di Hari Sabtu (86 orang) 
Jumlah  korban  luka  tertinggi  di  Hari  Kamis  (198  orang) 
dan terendah di Hari Sabtu (117 orang). 

pada 
Dashboard 
https://dwinissaa.github.io/dashboard-laka/ 

diakses 

dapat 

link 

berikut: 

Aturan Keputusan yang Menentukan Apakah Kecelakaan 
Berpeluang Menimbulkan Korban Tewas 

Untuk melihat aturan keputusan yang menentukan apakah 
kecelakaan  berpeluang  menimbulkan korban  tewas, dibangun 
model  Decision  Tree  dengan  metode  CART.  Data  terlebih 
dahulu  dibersihkan  dari  missing  value  dengan  complete 
deletion, sehhingga tersisa 745 data untuk dianalisis. Decision 
tree diimplementasikan di Rstudio dengan fungsi rpart. 

TABEL IV 
NILAI KEBAIKAN MODEL DECISION TREE DALAM 5-FOLD CV 
Recall 
0.8578 
0.9378 
0.7237 
0.8311 
0.7689 
0.8239 

F1-score  Accuracy 
0.6968 
0.7116 
0.6694 
0.6775 
0.6865 
0.6884 

Precision 
0.5866 
0.5734 
0.6226 
0.5719 
0.6201 
0.5949 

Fold 
1 
2 
3 
4 
5 
Mean 

0.6242 
0.6174 
0.6353 
0.6018 
0.6465 
0.6250 

Diperoleh  f1  scores  sebesar  68,84%.  Kemudian  seluruh 
data digabungkan untuk membentuk model akhir decision tree, 
sebagaimana pada Gambar 6. 

Gambar 5. Tampilan dashboard yang bisa memetakan titik lokasi kecelakaan 

Dari visualisasi dalam dashboard juga ditemukan bahwa: 
  Berita kecelakaan paling banyak di Bulan November (135 

berita) dan paling sedikit di bulan Mei (47 berita) 

Gambar 6. Decision Tree yang menentukan apakah kecelakaan lalu lintas 
akan menimbulkan korban tewas 

  Berita kecelakaan paling banyak di Hari Senin (202 berita) 

Dari  decision  tree  yang  telah  terbentuk,  dapat  ditarik 

dan paling sedikit pada Hari Jumat (129 berita) 

  Berita  kecelakaan  paling  banyak  pada  pagi  hari  (468 

berita) dan terendah di sore hari (85 berita) 

  Kecelakaan secara berturt-turut paling banyak melibatkan 
kelompok usia 20-24 tahun (303 orang), 25-29 tahun (301 
orang), 35-39 (260 orang) dan 15-19 tahun (243 orang) 
  Berita  kecelakaan  paling  banyak  melibatkan  motor  (685 

 

berita), mobil (570 berita) dan truk (555 berita) 
Jumlah  korban  tewas  tertinggi  di  Bulan  Januari  (121 
orang) sedangkan terendah di Bulan Mei (44 orang) 

beberapa aturan sebagai berikut: 
  Kecelakaan  yang  melibatkan  sepeda  motor,  berpeluang 
64%  dapat  menimbulkan  korban  tewas,  dan  di-support 
dengan 60% sampel. 

  Kecelakaan  yang 

tidak  melibatkan  sepeda  motor, 
melibatkan  kereta  api  dan  terjadi  di  hari  Selasa,  Kamis, 
Sabtu dan Minggu berpeluang 89% menimbulkan korban 
tewas, dan di-support dengan 2% sampel. 

  Kecelakaan  yang 

tidak  melibatkan  sepeda  motor, 
melibatkan  kereta  api  dan  terjadi  selain  di  hari  Selasa, 

 7 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
Kamis,  Sabtu  dan  Minggu  berpeluang  69% 
tidak 
menimbulkan  korban  tewas,  dan  di-support  dengan  2% 
sampel 

  Kecelakaan  yang  tidak  melibatkan  sepeda  motor,  tidak 
melibatkan  kereta  api,  melibatkan  kelompok  usia   60 
tahun  dan  terjadi  di  hari  Senin,  Selasa  dan  Kamis 
berpeluang  73%  menimbulkan  korban  tewas,  dan  di-
support dengan 3% sampel 

  Kecelakaan  yang  tidak  melibatkan  sepeda  motor,  tidak 
melibatkan  kereta  api,  melibatkan  kelompok  usia   60 
tahun  dan  terjadi  selain  di  hari  Senin,  Selasa  dan  Kamis 
berpeluang  86%  tidak  menimbulkan  korban  tewas,  dan 
di-support dengan 2% sampel 

  Kecelakaan  yang  tidak  melibatkan  sepeda  motor,  tidak 
melibatkan  kereta  api  dan  tidak  melibatkan  kelompok 
usia   60  tahun  berpeluang  77%  tidak  menimbulkan 
korban tewas, dan di-support dengan 31% sampel 

VII. 

PENUTUP 

  Dari penelitian yang telah dilakukan ditarik kesimpulan: 

2. 

1.  Telah  berhasil  dilakukan 

titik 

ekstraksi 

tentang:  peta 

informasi 
kecelakaan lalu lintas  dengan  model NER BiLSTM-
CNNs dan memperoleh f1-scores sebesar 85,145%; 
Informasi 
lokasi  kecelakaan, 
jumlah  korban  menurut  kelompok  umur  yang 
terlibat,  jumlah  korban  tewas  dan  terluka,  jumlah 
berita  kecelakaan  menurut  hari  dan  bulan,  jumlah 
berita kecelakaan menurut waktu, serta jumlah berita 
kecelakaan  menurut  jenis  kendaraan  yang  terlibat 
dari hasil ekstraksi berita online di Prov. Jawa Timur 
Tahun  2020 
  ke  dalam 
telah  divisualisasikan 
dashboard dan dilakukan analisis deskriptif. 

3.  Telah  dibuat  pohon  keputusan  (decision  tree)  untuk 
melihat  aturan  yang  menentukan  apakah  kecelakaan 
lalu  lintas  akan  menyebabkan  korban  meninggal 
dunia atau  tidak di Provinsi Jawa Timur tahun 2020 
dari hasil ekstraksi teks berita online dengan nilai f1-
scores 
bahwa 
kecelakaan 
sepeda  motor 
berpeluang  64%  menimbulkan  korban  tewas  dan  di-
support  dengan  60%  sampel,  sehingga  pengendara 
sepeda motor sebaiknya berhati-hati saat berkendara. 

68,84%.  Ditemukan 

yang  melibatkan 

sebesar 

  Saran yang bisa diterapkan pada penelitian selanjutnya:  

1.  Sebaiknya  penelitian  selanjutnya  dapat  membuat 
dashboard  yang  bisa  melakukan  scraping  dan 
pemodelan pada teks berita online secara otomatis; 
2.  Akan  lebih  baik  bila  ditambahkan  sumber  data  lain 
untuk  menambah  jumlah  perolehan  berita,  sehingga 
analisis bisa lebih maksimal; 

3.  Penelitian  ini  masih  belum  berhasil  mengekstraksi 
penyebab  kecelakaan  dikarenakan  ambiguitas  yang 
dialami  peneliti  dalam  melakukan  pelabelan. 
Sehingga  penelitian  selanjutnya  diharapkan  dapat 
seperti 
mengatasi 
mengelompokkan  penyebab  kecelakaan  ke  dalam 
kategori. 

ambiguitas 

tersebut, 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

DAFTAR PUSTAKA 

[1]  BPS, ―Statistik Transportasi Darat 2019,‖ 2020. 
[2]  ―Struktur 

Korlantas 
dan 
https://korlantas.polri.go.id/profil/ (accessed Aug. 05, 2021). 

Fungsi 

Tugas 

Polri.‖ 

[3]  ―Statistik  Laka.‖  https://korlantas.polri.go.id/statistik-laka/  (accessed 

[4] 

Aug. 05, 2021). 
J.  Jiang,  ―Mining  text  data,‖  in  Mining  Text  Data,  vol.  9781461432, 
Springer Science+Business Media, LLC 2012, 2012, pp. 11–41. 

[5]  A.  Nurdin  and  N.  U.  Maulidevi,  ―5W1H  Information  Extraction  with 
CNN-Bidirectional  LSTM,‖  J.  Phys.  Conf.  Ser.,  vol.  978,  no.  1,  2018, 
doi: 10.1088/1742-6596/978/1/012078. 
J.  P.  C.  Chiu  and  E.  Nichols,  ―Named  Entity  Recognition  with 
Bidirectional LSTM-CNNs,‖ Trans. Assoc. Comput. Linguist., vol. 4, no. 
2003, pp. 357–370, Dec. 2016, doi: 10.1162/tacl_a_00104. 

[6] 

[7]  R. Rifani, M. A. Bijaksana, and I. Asror, ―Named Entity Recognition for 
an  Indonesian  Based  Language  Tweet  using  Multinomial  Naive  Bayes 
Classifier,‖ Indones. J. Comput., vol. 4, no. 2, pp. 119–126, 2019, doi: 
10.21108/indojc.2019.4.2.330. 

[8]  D.  W.  Wulandari,  P.  P.  Adikara,  and  S.  Adinugroho,  ―Named  Entity 
Recognition  (NER)  Pada  Dokumen  Biologi  Menggunakan  Rule  Based 
dan Naïve Bayes Classifier,‖ J. Pengemb. Teknol. Inf. dan Ilmu Komput., 
vol. 2, no. 11, pp. 4555–4563, 2018. 

[9]  K. Pahi and A. Shakya, ―Road Accident News Information Extraction,‖ 

ICAEIC-2019, vol. 2, no. 1, p. 65, 2019. 

[10]  W. Gunawan, D. Suhartono, F. Purnomo, and A. Ongko, ―Named-Entity 
Recognition for Indonesian Language using Bidirectional LSTM-CNNs,‖ 
Procedia  Comput.  Sci.,  vol.  135,  pp.  425–432,  2018,  doi: 
10.1016/j.procs.2018.08.193. 

[11]  J.  Rahmah,  Ekstraksi  Informasi  Artikel  Musik  Berdasarkan  Ontologi 
Menggunakan Named Entity Recognition dan Metode Rule Based. 2019. 
[12]  J. Santoso et al., ―Self-Training Naive Bayes Berbasis Word2Vec untuk 
Kategorisasi Berita Bahasa Indonesia,‖ J. Nas. Tek. Elektro dan Teknol. 
Inf., vol. 7, no. 2, pp. 158–166, 2018, doi: 10.22146/jnteti.v7i2.418. 
[13]  Sugiyamto,  B.  Surarso,  and  A.  Sugiharto,  ―Analisa  Performa  Metode 
Cosine  Dan  Jacard  Pada  Pengujian  Kesamaan  Dokumen,‖  J.  Masy. 
Inform., vol. 5, no. 10, pp. 1–8, 2014, doi: 10.14710/jmasif.5.10.1-8. 
[14]  F.  B.  Sejati,  P.  Hendradi,  and  B.  Pujiarto,  ―Deteksi  Plagiarisme  Karya 
Ilmiah  dengn  Pemanfaatan  Daftar  Pustaka  dalam  Pencarian  Kemiripan 
Tema  menggunakan  Metode  Cosine  Similarity  (Studi  Kasus:  Di 
Universitas Muhammadiyah  Magelang),‖  J. Komtika,  vol. 2,  no. 2,  pp. 
85–94, Jan. 2019, doi: 10.31603/komtika.v2i2.2594. 

[15]  F.  Handayani  and  S.  Pribadi,  ―Implementasi  Algoritma  Naive  Bayes 
Classifier  dalam  Pengklasifikasian  Teks  Otomatis  Pengaduan  dan 
Pelaporan  Masyarakat  melalui  Layanan  Call  Center  110,‖  J.  Tek. 
Elektro, vol. 7, no. 1, pp. 19–24, 2015, doi: 10.15294/jte.v7i1.8585. 
[16]  S. D. Larasati, V. Kuboň, and D. Zeman, ―Indonesian morphology tool 
(MorphInd):  Towards  an  Indonesian  Corpus,‖  Commun.  Comput.  Inf. 
Sci., pp. 119–129, 2011, doi: 10.1007/978-3-642-23138-4_8. 

[17]  A.  Akbik,  D.  Blythe,  and  R.  Vollgraf,  ―Contextual  String  Embeddings 
for  Sequence  Labeling,‖  Proc.  27th  Int.  Conf.  Comput.  Linguist.,  pp. 
Available: 
1638–1649, 
http://aclweb.org/anthology/C18-1139. 

[Online]. 

2018, 

[18]  P.  Kaban,  ―POS-Tagging  Bahasa  Indonesia  dengan  Flair  NLP.‖ 
https://puspitakaban.medium.com/pos-tagging-bahasa-indonesia-
dengan-flair-nlp-c12e45542860 (accessed Aug. 05, 2021). 

[19]  W. Belblidia, ―Named Entity Recognition with BiLSTM-CNNs,‖ 2019. 

https://medium.com/illuin/named-entity-recognition-with-bilstm-cnns-
632ba83d3d41 (accessed Jun. 04, 2021). 

[20]  M. Hofer, A. Kormilitzin, P. Goldberg, and A. Nevado-Holgado, ―Few-

shot Learning for Named Entity Recognition in Medical Text,‖ pp. 1–10, 
2018, [Online]. Available: http://arxiv.org/abs/1811.05468. 

[21]  I.  Hanif,  ―GitHub  -  irfanhanif/Mira:  Natural  Language  Sentence 
Classification Using Global Vectors for Word Representations (GloVe), 
Convolutional  Neural  Networks,  and  Transfer  Learning  for  Chatbots 
Application,‖  2018.  https://github.com/irfanhanif/Mira  (accessed  Mar. 
30, 2021). 

[22]  F.  E.  Pratiwi,  F.  E.  Pratiwi,  and  I.  Zain,  ―Klasifikasi  Pengangguran 
Terbuka  Menggunakan  CART  (Classification  and  Regression  Tree)  di 
Provinsi Sulawesi Utara,‖ J. Sains dan Seni ITS, vol. 3, no. 1, pp. D54–
D59, 
Available: 
http://www.ejurnal.its.ac.id/index.php/sains_seni/article/view/6129. 

[Online]. 

2014, 

 8 / 8 

 
 
 
 
 
"
221709643,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Analisis Kualitas Website Politeknik Statistika STIS 
Dengan Pendekatan Webqual 4.0 

Donovan Hutapea (221709643, 4SI1) 
Dosen Pembimbing: Lutfi Rahmatuti Maghfiroh 

Ringkasan— Politeknik Statistika STIS merupakan salah satu 
dari  sekian  banyaknya  perguruan  tinggi  kedinasan  yang 
menerapkan  teknologi  informasi.  Salah  satu  penerapannya 
adalah  website  Politeknik  Statistika  STIS  dengan  domain 
stis.ac.id.  Website  ini  berguna  sebagai  pemberi  informasi  untuk 
internal  dan  eksternal  Politeknik  Statistika  STIS.  Karena  hal 
tersebut,  maka  tujuan  dalam  penelitian 
ini  yaitu  untuk 
melakukan  analisis  kualitas  website  Politeknik  Statistika  STIS 
dengan  pendekatan  Webqual  4.0.  Instrumen  yang  digunakan 
dalam  analisis  kualitas  website  ini  merupakan  penerapan  dari 
metode Webqual 4.0, yang terbagi dalam 3 dimensi/variabel, yaitu 
variabel  kegunaan 
informasi 
(information  quality),  dan  variabel  kualitas  interaksi  layanan 
(service interaction quality), dan mengambil 18 dari 23 indikator. 
Teknik  analisis  menggunakan  analisis  kesenjangan/gap  analysis 
dan  importance-performance  analysis  (IPA).  Berdasarkan  hasil 
analisis kesenjangan/gap analysis menunjukan kesenjangan yang 
bernilai  negatif  yang  artinya  kualitas  website  masih  belum 
memenuhi keinginan penggunanya. Selanjutnya hasil analisis IPA 
(Importance-Performance  Analysis)  menunjukkan  bahwa 
terdapat 2 dari 18 indikator yang perlu dijadikan prioritas dalam 
perbaikan  ke  depannya  yaitu  Tampilan  dan  Informasi  Tepat 
Waktu.  

(usability),  variabel  kualitas 

Kata Kunci— Website, Kualitas Website, Webqual 4.0, Analisis 

Kesenjangan, Importance-Perfromance Analysis 

website  yang  dikelola  oleh  suatu  organisasi  merupakan 
gambaran di dunia maya dari organisasi itu sendiri [5]. 

Salah satu metode pengukuran kualitas website yang saat ini 
banyak  digunakan  yaitu  Analisis  Webqual  4.0.  [6].  Webqual 
pada  dasarnya  merupakan  salah  satu  teknik  pengukuran 
kualitas  website  yang  berdasar  pada  persepsi  dari  pengguna 
akhir. Webqual mengalami perkembangan dari masa ke masa, 
dimulai  dari  Webqual  1.0  hingga  yang  terbaru  saat  ini  yaitu 
Webqual  4.0.  Webqual  4.0  yang  dicetuskan  oleh 
Barnes&Vidgen  pada  tahun  2003  ini  terbagi  dalam  tiga 
dimensi/variabel yaitu variabel kegunaan (Usability), variable 
kualitas informasi (Information Quality) dan variable kualitas 
interaksi layanan (Service Interaction Quality) [7], [8].  

4.0 

dalam  Webqual 

Indikator-indikator 

dilakukan 
pengidentifikasian 
lebih  spesifik  guna  mengidentifikasi 
indikator-indikator mana yang perlu diperbaiki dan yang sudah 
memenuhi  harapan  dari  pengguna.  Analisis  yang  digunakan 
yaitu  Analisis  Kesenjangan/Gap  Analysis  dan  Importance-
Performance  Analysis  (IPA).  Referensi  [1]  menunjukkan 
bahwa  analisis  kesenjangan  menghitung  gap  antara  persepsi 
dan  ekspektasi  dari  pengguna,  sedangkan  analisis  IPA 
mengelompokkan indikator mana yang harus sudah memenuhi 
harapan dan yang masih perlu perbaikan lebih lanjut dilihat dari 
persepsi dan harapan dari pengguna. 

I.  LATAR BELAKANG 

II.  TUJUAN PENELITIAN 

Teknologi di era digital saat ini sudah tidak bisa lepas dari 
kehidupan manusia sehari-hari. Mulai dari bangun hingga tidur, 
manusia  pasti  menggunakan  teknologi  dalam  mempermudah 
aktivitasnya.  Salah  satunya  yaitu  penggunaan 
teknologi 
informasi.  Hal  ini  merupakan  akibat  dari  kebutuhan  manusia 
akan informasi yang dimana memiliki peranan besar pada saat 
ini.  Teknologi 
informasi  yang  banyak  digunakan  yaitu 
penggunaan internet yang dimana pemanfaatan internet paling 
banyak yaitu penggunaan website [1]. 

Website  adalah  keseluruhan  halaman  web  yang  terdapat 
dalam  domain  yang  berisi  informasi.  Sebuah  situs  web 
biasanya  dibangun  di  banyak  situs  web  yang  saling 
berhubungan  [2].  Website  untuk  sebuah  organisasi  memiliki 
peranan  penting  karena  memberikan  beberapa  keuntungan, 
seperti  pemberian  layanan  online  kepada  pelanggannya  [3]. 
Salah  satu  organisasi  yang  menggunakan  website  adalah 
Politeknik Statistika STIS yang berdomain stis.ac.id. 

Website  yang  bagus  biasanya  berfokus  pada  konten  dari 
website yang merupakan faktor penentu untuk menarik minat 
pengguna  agar  kembali  mengunjungi  website  tersebut  [4]. 
Dalam  sebuah  organisasi,  kualitas  website  yang  dikelola 
merupakan faktor utama yang harus diperhatikan dikarenakan 

Tujuan dari penelitian ini adalah untuk mengetahui kualitas 
website  Politeknik  Statistika  STIS  menggunakan  pendekatan 
Webqual 4.0. Tujuan lainnya yaitu untuk mengetahui prioritas 
perbaikan berdasarkan indicator-indikator dalam Webqual 4.0.. 

III. PENELITIAN TERKAIT 

Barnes & Vidgen melakukan beberapa kali penelitian yang 
bertujuan untuk mengembangkan instrument untuk mengukur 
kualitas  dari  suatu  website.  Penelitian  tersebut  menghasilkan 
suatu  metode  yang  dinamakan  Webqual.  Pengembangan 
Webqual yang terakhir yaitu Webqual 4.0 yang berfokus pada 
3  dimensi  yaitu  dimensi  usability,  information  quality  dan 
service  interaction  quality.  Usability  berisi  mengenai  desain 
website serta kegunaan website itu sendiri. Information Quality 
berisi  mengenai  kualitas  dan  kesesuaian  dari  informasi  serta 
konten  yang  dimuat  dalam  suatu  website.  Service  Interaction 
Quality berisi mengenai pengalaman yang dirasakan pengguna 
saat menggunakan website tersebut. 

Shia  et  al  (2016)  melakukan  penelitian  tentang  kualitas 
website dari AliexPress berdasar pada persepsi dan ekspektasi 
dari  pengguna  dengan  pendekatan  Webqual  dan  Importance-
Performance  Analysis.  Hasil  dari  penelitian  ini  menyatakan 

 1 / 7 

 
 
 
 
 
kualitas  website  AliexPress  belum  memenuhi  harapan  dari 
pengguna  website  tersebut.  Kesimpulan  tersebut  didapat 
setelah  melihat  munculnya  kesenjangan  yang  bernilai  negatif 
pada  keseluruhan  dimensi  Webqual.  Dimana  kesenjangan 
terbesar dimiliki oleh dimensi usability.  

Kemudian dalam penelitian Safira Nur Rahmaini pada tahun 
2018  yang  melakukan  penelitian  mengenai  analisis  kualitas 
website  menggunakan  pendekatan  Webqual  4.0  dan 
Importance-Performance  Analysis.  Penelitian  ini  berfokus 
pada  analisis  terhadap  kualitas  website  dari  lima  universitas 
yang  berbeda  yang  selanjutnya  tiap  universitas  dibandingkan 
hasil  analisisnya.  Hasil  penelitian  menghasilkan  kesimpulan 
dimana  dari  masing-masing  universitas,  terdapat  indikator-
indikator  yang  perlu  diperhatikan/diperbaiki  guna  memenuhi 
harapan para pengguna dari website universitas tersebut. Pada 
penelitian ini, peneliti juga menggunakan pengukuran Webqual 
Index (WQI) yang digunakan untuk membandingkan kualitas 
masing-masing universitas terhadap universitas lainnya. 

IV. METODE PENELITIAN  
Metode penelitian ini secara umum adalah metode penelitian 
kuantitatif.  Pengumpulan  data  penelitian  dilakukan  dengan 
metode  survey  menggunakan  kuesioner  yang  disebar  kepada 
responden.  Kuesioner  yang  digunakan  merupakan  penerapan 
dari metode Webqual 4.0. Pertanyan yang ada dalam kuesioner 
merupakan pertanyaan mengenai kualitas website saat ini dan 
juga harapan dari pengguna terhadap kualitas website. 

Dalam  penelitian  ini,  digunakan  skala  likert  dengan  lima 
poin.  Setiap  pertanyaan  diberikan  5  (lima)  jawaban  untuk 
dipilih.  Untuk  penilaian  terhadap  Persepsi/Perfromance,  pada 
poin (1) diberikan keterangan “sangat tidak setuju” dan poin (5) 
diberikan keterangan “sangat setuju”. Untuk penilaian terhadap 
diberikan 
Tingkat  Kepentingan/Importance, 
keterangan  “sangat  tidak  penting”  dan  poin  (5)  diberikan 
keterangan  “sangat  penting”  [1].  Penyebaran  kuesioner 
dilakukan melalui Email dan Whatsapp. Untuk mempermudah 
pengolahan data, tiap-tiap indikator diberikan label pengganti 
yang diambil dari nama variabel disertai dengan nomor. 

poin 

(1) 

Variabel 

Usability 

TABEL I 
INSTRUMEN PERTANYAAN 

Pertanyaan 
Webqual 
Saya menemukan 
situs ini mudah  
untuk dioprasikan 
Interaksi dengan 
situs jelas dan  
mudah dimengerti 
Situs memiliki 
petunjuk yang jelas 

Situs mudah 
digunakan 

Pertanyaan Kuesioner 

Website mudah untuk 
dipelajari dan 
dioperasikan 
Interaksi dengan 
website jelas dan 
dapat dimengerti 
Website memiliki 
navigasi/petunjuk yang 
jelas 
Website mudah 
digunakan 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Situs memiliki 
tampilan yang  
Menarik 
Desain sesuai 
dengan tipe 
situsnya 
Situs ini 
meningkatkan 
kompetensi/ 
Persaingan 

Situs ini 
memberikan 
pengalaman  
positif untuk saya 
Menyediakan 
informasi yang 
akurat 
Menyediakan 
informasi yang 
dapat  
Dipercaya 
Menyediakan 
informasi yang 
tepat  
waktu/up to date 
Menyediakan 
informasi yang 
relevan 
Menyediakan 
informasi yang 
mudah  
dimengerti 
Menyediakan 
informasi secara 
detail 
Memberikan 
informasi dalam 
format  
yang sesuai 
Memiliki reputasi 
yang baik 
Memberikan rasa 
memiliki terhadap  
organisasi 

Memberikan 
kemudahan untuk  

Website memiliki 
tampilan yang menarik 

Desain website sesuai 
dengan tipenya 
(website pendidikan) 
Keberadaan website 
ini dapat 
meningkatkan 
kompetensi dengan 
website pendidikan 
lain 
Website ini 
memberikan 
pengalaman positif 
bagi saya 
Website menyediakan 
informasi yang akurat 

Website menyediakan 
informasi yang dapat 
dipercaya 

Website menyediakan 
informasi tepat waktu 

Website menyediakan 
informasi yang relevan 

Website menyediakan 
informasi yang mudah 
untuk dipahami 

Website menyediakan 
informasi yang detail 

Website menyediakan 
informasi dalam 
format yang sesuai 

Website memiliki 
reputasi yang baik 
Setelah menggunakan 
website, membuat 
saya merasa menjadi 
bagian dari komunitas 
website tersebut 
Website memberikan 
kemudahan untuk 

 2 / 7 

Information 
Quality 

Service 
Interaction 
Quality 

 
 
 
 
berkomunikasi 
dengan organisasi 

berkomunikasi dengan 
pihak pengelola 

Gambar 1. Penamaan Indikator 

Setelah  pengumpulan  data  selesai  dilakukan,  selanjutnya 
akan disaring dengan bantuan perangkat lunak Ms. Excel 2016. 
Terpilih 100 data  untuk digunakan  yang  kemudian dilakukan 
analisis  kesenjangan/gap  analysis  menggunakan  Ms.  Excel 
2016 dan analisis IPA menggunakan IBM SPSS. 

Analisis  yang  digunakan  yaitu  Importance-Performance 
Analysis (IPA). Analisis IPA digunakan untuk mengidentifikasi 
indikator-indikator dari suatu layanan berdasarkan prioritasnya 
dari  penilaian  pengguna 
terhadap  Performance  dan 
Importancenya  [1].  Hasil  analisis  IPA  ditunjukkan  dalam 
bentuk grafik yang dimana indikator-indikator tersebut dibagi 
ke  dalam  empat  kuadran.  Sumbu  X  mewakili  indikator 
persepsi/perfromance  dan  sumbu  Y  mewakili 
indikator 
kepentingan/importance. Berikut adalah rincian mengenai tiap 
kuadran [10]: 

Gambar 2. Analisis IPA 

1.  Kuadran pertama: Pertahankan! 

Kuadran  pertama  ini  berisi  indikator-indikator  yang 
dinilai  tinggi  oleh  pengguna  dalam  hal  Perfromance 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Importancenya.  Maka 

indikator  yang 
maupun 
termasuk  ke  dalam  kuadran 
ini  perlu  dijaga 
kualitasnya  karena  apa  yang  diharapkan  pengguna 
terhadap  indikator  tersebut  sudah  sesuai  dengan 
harapan pengguna. 

2.  Kuadran kedua: Perlu Perhatian Lebih. 

Kuadran  kedua  ini  berisi  indikator-indikator  yang 
dinilai  tinggi  oleh  pengguna  dalam  hal  Importance 
namun 
rendah  dalam  Perfromancenya.  Maka 
pengelola  diharapkan  dapat  memberikan  perhatian 
lebih terhadap indikator-indikator pada kuadran kedua 
ini. 

3.  Kuadran  ketiga:  Perlu  Perbaikan  namun  Bukan 

Prioritas. 
Kuadran  ketiga  ini  berisi  indikator-indikator  yang 
dinilai  rendah  oleh  pengguna  dalam  hal  Importance 
maupun dalam Perfromancenya. Hal ini menunjukkan 
indikator-indikator dalam kuadran ini merupakan low 
priority yang dimana tidak perlu terlalu diperhatikan 
perbaikannya. 

4.  Kuadran keempat: Terlalu Berlebihan. 

Kuadran  keempat  ini  berisi  indikator-indikator  yang 
dinilai  rendah  oleh  pengguna  dalam  hal  Importance 
namun tinggi dalam Perfromancenya. Maka dari itu, 
fokus  sumber  daya  pada  indikator-indikator  dalam 
kuadran  ini  sebaiknya  dialokasikan  ke  tempat  lain 
dengan prioritas lebih tinggi. 

 3 / 7 

 
 
 
 
 
 
 
 
 
 
V.  KERANGKA PIKIR 

1.  18 Indikator terhadap Performance 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 4. Uji Reliabilitas Performance 

Nilai Cronbach’s Alpha > 0,6 yang dapat disimpulkan 
bahwa  instrumen  terbukti  reliabel/dapat  diandalkan 
terhadap Performance dari website yang diteliti [11]. 

2.  18 Indikator terhadap Importance 

Gambar 5. Uji Reliabilitas Importance 

Nilai Cronbach’s Alpha > 0,6 yang dapat disimpulkan 
bahwa  instrumen  terbukti  reliabel/dapat  diandalkan 
terhadap Importance dari website yang diteliti [11]. 

B.  Uji Validitas 

Instrumen penelitian yang digunakan diuji terlebih dahulu 
validitasnya sebelum masuk ke tahap analisis. Berikut hasil 
uji validitas. 

1.  Performance 

.

Gambar 3. Kerangka Pikir 

Kerangka pikir pada penelitian ini diawali dari masalah awal 
yaitu pentingnya kualitas website bagi suatu organisasi/institusi 
yang  dimana  menggambarkan  organisasi/institusi  tersebut  di 
dunia  maya.  Setelah  masalah  awal,  dilanjutkan  dengan 
pencarian  solusi.  Pencarian  solusi  berfokus  pada  pencarian 
literatur terkait kualitas website dan cara menganalisa kualitas 
website. 

literatur 

Setelah  mengumpulkan 

terkait,  dilakukan 
pengumpulan  untuk  data  guna  mendukung  penelitian  ini. 
Adapun data  yang digunakan pada penlitian ini dikumpulkan 
dari  pengguna  website  dan  dibatasi  sebesar  100  responden. 
Setelah data dikumpulkan, dilakukan analisis berdasarkan data 
tersebut. Analisis yang digunakan yaitu Analisis Kesenjangan 
dan  Importance-Performance  Analysis.  Terakhir,  membuat 
laporan  dalam  bentuk  makalah  skripsi  yang  berisi  mengenai 
hasil analisis kualitas website Politeknik Statistika STIS. 

VI. HASIL DAN PEMBAHASAN 

A.  Uji Reliabilitas 

Instrumen penelitian yang digunakan diuji terlebih dahulu 
reliabilitasnya  sebelum  masuk  ke  tahap  analisis.  Berikut 
hasil uji reliabilitas. 

Gambar 6. Uji Validitas Performance 

 4 / 7 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Gambar  A  menunjukan  hasil  uji  validitas  dari 
instrumen  pada  sampel  Performance.  Referensi  [11] 
menunjukkan bahwa pada kolom Corrected Item-Total 
(rhitung), nilai tersebut dapat digunakan untuk melihat 
validitasnya.  Jika  dilihat  dari  Gambar  A,  nilai  pada 
rhitung  semua  bernilai  >  0.195  (rtabel  untuk  jumlah 
sampel 100 pada taraf signifikansi 5%). Perbandingan 
ini menunjukkan bahwa instrument pada penelitian ini 
telah terbukti valid. Hal ini sesuai dengan teori dimana 
item tersebut dapat dikatakan valid saat nilai rhitung > 
nilai rtabel. 
Importance 

2. 

Gambar 7. Uji Validitas Importance 

Gambar  B  menunjukan  hasil  uji  validitas  dari 
instrumen  pada  sampel  Importance.  Referensi  [11] 
menunjukkan bahwa pada kolom Corrected Item-Total 
(rhitung), nilai tersebut dapat digunakan untuk melihat 
validitasnya.  Jika  dilihat  dari  Gambar  B,  nilai  pada 
rhitung  semua  bernilai  >  0.195  (rtabel  untuk  jumlah 
sampel 100 pada taraf signifikansi 5%). Perbandingan 
ini menunjukkan bahwa instrument pada penelitian ini 
telah terbukti valid. Hal ini sesuai dengan teori dimana 
item tersebut dapat dikatakan valid saat nilai rhitung > 
nilai rtabel. 

C.  Analisis Kesenjangan/Gap Analysis 

Analisis kesenjangan menghitung selisih/gap antara nilai 
rata-rata  dari nilai persepsi pengguna dan  nilai ekspektasi 
pengguna.  Proses  penghitungannya  dengan  rumus  Q 
(kesenjangan)  =  P(Persepsi) –  E(Ekspektasi).  Jika  nilai  Q 
bernilai  0,  artinya  kualitas  website  sudah  sesuai  dengan 
ekspektasi  penggunanya.  Apabila  nilai  Q  bernilai  positif, 
hal  ini  menggambarkan  kualitas  website  telah  melewati 
ekspektasi  pengguna.  Namun  apabila  nilai  Q  bernilai 
negatif,  menandakan  kualitas  website tidak sesuai  dengan 
ekspektasi yang dimiliki oleh pengguna. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

TABEL II 
GAP ANALYSIS 

Indikator 

Rata-Rata 
Persepsi 
(P) 

Rata-Rata 
Ekspektasi 
(E) 

Rata-Rata 
Gap (Q) 

X1.1 
X1.2 
X1.3 
X1.4 
X1.5 
X1.6 
X1.7 
X1.8 

Rata-
Rata X1 

X2.1 
X2.2 
X2.3 
X2.4 
X2.5 
X2.6 
X2.7 

Rata-
Rata X2 

X3.1 
X3.2 
X3.3 

Rata-
Rata X3 

3.96 
3.89 
3.91 
4.06 
3.64 
3.86 
3.76 
3.84 

3.87 
3.91 
4.08 
3.84 
3.90 
3.98 
3.74 
3.89 

3.91 
3.83 
3.44 
3.66 

3.64 

4.50 
4.35 
4.50 
4.50 
4.35 
4.20 
4.20 
4.00 

4.33 
4.61 
4.78 
4.35 
4.60 
4.60 
4.00 
4.58 

4.51 
3.85 
3.60 
3.60 

-0.54 
-0.46 
-0.59 
-0.44 
-0.71 
-0.34 
-0.44 
-0.16 

-0.46 
-0.70 
-0.70 
-0.51 
-0.70 
-0.62 
-0.26 
-0.69 

-0.60 
-0.02 
-0.16 
0.06 

3.68 

-0.04 

Berdasarkan pada Tabel II, bisa dilihat bahwa nilai rata-
rata gap dari tiap indikator dalam 3 variabel Webqual 4.0 
(X1,  X2  dan  X3).  Secara  keseluruhan,  tiap  indikator 
memiliki  nilai  gap  yang  bernilai  negatif  kecuali  untuk 
indikator  X3.3.  Hal  ini  menandakan  kualitas  website 
Politeknik Statistika STIS masih belum memenuhi harapan 
penggunanya [12].  

Nilai rata-rata gap untuk dimensi kegunaan (X1) adalah 
sebesar -0.46. Sedangkan nilai rata-rata gap untuk variabel 
kualitas informasi (X2) sebesar -0.61 dan kualitas interaksi 
layanan (X3) memiliki rata-rata gap sebesar -0.04. Dimana 
variabel  yang  memiliki nilai  gap tertinggi adalah variabel 
kualitas  informasi  (X2)  sedangkan  indikator  dengan  gap 
terbesar dimiliki oleh indikator mengenai tampilan website 
(X1.5) dengan nilai gap -0.71. 

 5 / 7 

 
 
 
 
 
 
 
 
 
 
 
D.  Importance-Performance Analysis (IPA) 

Gambar 8. Kuadran Analisis IPA 

Setelah  menghitung  rata-rata  nilai  untuk  tiap  indikator, 
dilakukan analisis IPA yang hasilnya dibagi menjadi 4 kuadran. 

Gambar 9. Indikator dalam Kuadran 

Dari hasil analisis IPA diatas, dapat dilihat bahwa indikator 
USA5 (Tampilan) dan INF3  (Informasi Tepat Waktu) adalah 
indikator  yang  saat  ini  memiliki  urgensi/Importance  tinggi 
menurut pengguna namun, memiliki kinerja/Performance yang 
tidak sesuai dengan keinginan pengguna. 

Hasil  dari  pengujian 

instrumen  penelitian 
menunjukkan bahwa instrumen yang digunakan sudah terbukti 
validitas  dan  reliabilitasnya.  Berdasarkan  hasil  analisis 

terhadap 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

kesenjangan dan analisis IPA, kualitas dari website Politeknik 
Statistika STIS terbukti belum memenuhi keinginan dari para 
penggunanya.  Hal  itu  terbukti  dari  nilai  kesenjangan  yang 
bernilai negatif untuk keseluruhan indikator. Adapun indikator-
indikator  harus dijadikan dalam perbaikan  ke depannya  yaitu 
indikator mengenai tampilan dan informasi yang tepat waktu. 

VII. 

PENUTUP 

Berdasarkan penelitian yang sudah berjalan sejauh ini, dapat 

diambil kesimpulan-kesimpulan sebagai berikut. 

1. 

Indikator  Indikator  dalam  metode  Webqual  4.0  pada 
penelitian ini yang digunakan sebagai instrument telah 
terbukti  validitas  dan 
ini 
menunjukkan bahwa indikator dalam metode Webqual 
4.0  dapat  digunakan  dalam  melakukan  analisis  lebih 
lanjut. 

reliabilitasnya.  Hal 

2.  Analisis  Kesenjangan/Gap  Analysis  yang 

sudah 
dilakukan  berhasil  memberikan  kesimpulan  dimana 
secara  keseluruhan  indikator  kualitas  website  yang 
diteliti  memiliki  nilai  kesenjangan  negatif.  Hal  ini 
menandakan kualitas website Politeknik Statistika STIS 
masih belum memenuhi harapan penggunanya. 

3.  Analisis IPA (Importance-Perfromance Analysis) yang 
telah  dilakukan  menunjukkan  indikator-indikator  apa 
saja yang telah memenuhi keinginan pengguna. Namun, 
masih  terdapat  dua  indikator  yaitu  Tampilan  dan 
Informasi  Tepat  Waktu  yang  memerlukan  perhatian 
lebih agar dapat sesuai dengan keinginan pengguna..  
Saran dari peneliti untuk penelitian terkait yang akan datang 

sebagai berikut. 

1.  Disarankan  melakukan  perbaikan  terhadap  website 
tersebut  berdasarkan  hasil  analisis  IPA.  Hal  yang 
diperbaiki  yaitu  dalam  hal  tampilan  website  serta 
ketepatan  waktu dari informasi  yang diberikan. Untuk 
indikator tampilan, dapat dilakukan analisis lebih lanjut 
untuk mengetahui tampilan seperti apa yang diharapkan 
oleh  pengguna.  Sedangkan  untuk  indikator  Informasi 
Tepat  Waktu,  perlu  dilakukan  pengkajian 
lebih 
mendalam untuk memperbaiki indikator tersebut. 
2.  Nilai  kesenjangan/gap  dari  variabel  kualitas  informasi 
merupakan  yang  terbesar.  Hal  ini  menjadi  bahan 
pertimbangan  ke  depannya,  dimana  kualitas  informasi 
yang  disajikan  harus  bisa  memenuhi  harapan  dari 
pengguna  baik  dalam  segi  ketepatan  waktu,  informasi 
yang detail, dsb. 

3.  Untuk  penelitian  selanjutnya  dapat  menggunakan 
indikator-indikator Webqual 4.0 yang digunakan karena 
telah  berhasil  terbukti  validitas  dan  reliabilitasnya. 
Disarankan  untuk  menggunakan  metode  analisis  lain 
agar  dapat  melihat  perbandingan  hasil  dengan  metode 
analisis pada penelitian ini. 

DAFTAR PUSTAKA 

[1]  Rahmaini,  S.  N.  (2018).  Analisis  Kualitas  Website  Akademik
Menggunakan Metode Webqual 4.0 Dan Importance-Performance
Analysis  (Ipa)  (Bachelor's  thesis,  Jakarta:  Fakultas  Sains  Dan
Teknologi UIN Syarif Hidayatullah). 

 6 / 7 

 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

[2]  Hidayat, R. (2010). Cara praktis membangun website gratis. Elex

[3] 

Media Komputindo. 
Shia,  B.  C.,  Chen,  M.,  &  Ramdansyah,  A.  D.  (2016).  Measuring
customer satisfaction toward localization website by WebQual and
importance performance analysis (case study on AliexPress Site in
Indonesia).  American  Journal  of 
Industrial  and  Business
Management, 6(02), 117. 

[4]  Rosen, D. E., & Purinton, E. (2004). Website design: Viewing the
web as a cognitive landscape. Journal of Business Research, 57(7),
787-794. 

[5]  Napitupulu,  D.  B.  (2016).  Evaluation  of  XYZ  university  website
quality  based  on  Webqual  approach.  Buletin  Pos  dan
Telekomunikasi, 14(1), 51-64. 

[6]  Hafiz, A. (2017, November). Mengukur Kualitas Website dengan
Pendekatan  Webqual  4.0  Modifikasi.  In  Prosiding  Seminar
Nasional Darmajaya (Vol. 1, No. 1, pp. 443-457). 

[7]  Waloyo, L. A. S. (2018). Mengukur Kualitas Website Universitas
Dengan  Pendekatan  Webqual  (Studi  Kasus:  Unika  Widya
Mandala Madiun). Prosiding SNST Fakultas Teknik, 1(1). 
[8]  Barnes,  S.,  &  Vidgen,  R.  (2000).  WebQual:  an  exploration  of

website quality. ECIS 2000 proceedings, 74.  

[9]  Barnes,  S.  J.,  &  Vidgen,  R.  (2003).  Measuring  web  site  quality
improvements: a case study of the forum on strategic management
knowledge exchange. Industrial management & Data systems. 
[10]  Martilla,  J.  A.,  &  James,  J.  C.  (1977).  Importance-Performance

Analysis. Journal of Marketing, 77-79. 

[11]  Ghozali, I. (2013). Aplikasi Analisis Multivariate dengan Program
SPSS  Edisi  Ketujuh.  Semarang:  Badan  Penerbit  Universitas
Diponegoro. 

[12]  Asogwa,  B.  E.,  Asadu,  B.  U.,  Ezema,  J.  U.,  &  Ugwuanyi,  F.  C.
(2014).  Use  of  ServQUAL  in  the  evaluation  of  service  quality  of
academic libraries in developing countries. Library Philosophy and
Practice, 0_1. 

 7 / 7 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
"
221709638,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Analisis Ulasan Objek Daya Tarik Wisata  
di Jawa Timur pada Google Maps 

Disya Pratistaning Ratriatmaja (221709638, 4SD1) 
Dosen Pembimbing: Nucke Widowati Kusumo Projo, S.Si., M.Sc., Ph.D. 

Ringkasan—  Pandemik  COVID-19  berdampak  pada  sektor 
pariwisata,  salah  satunya  pada  keberlangsungan  Objek  Daya 
Tarik Wisata (ODTW). Pemerintah Jawa Timur dengan jumlah 
ODTW terbanyak membuat Pergub No. 53 Tahun 2020 sebagai 
upaya  pencegahan  penyebaran  COVID-19  dengan  protokol 
kesehatan.  Tetapi  masih  banyak  pelanggaran  yang  terjadi, 
sehingga perlu adanya pengawasan khususnya di ODTW. Calon 
pengunjung  juga  memerlukan  informasi  penerapan  protokol 
kesehatan,  fasilitas  yang  tersedia,  dan  informasi  lainnya  terkait 
ODTW. Ulasan pada Google Maps sebagai Big Data menyimpan 
informasi ODTW berdasarkan pengalaman pengunjung. Analisis 
ulasan  berhasil  dilakukan  dengan  metode  Multinomial  Naïve 
Bayes (MNB), Term Frequency-Inverse Document Frequency (TF-
IDF),  dan  pseudo-labelling,  serta  asosiasi  kata.  Hasil  analisis 
menunjukkan  bahwa  protokol  kesehatan  telah  diterapkan, 
fasilitas  yang  tersedia  telah  baik,  dan  tidak  terjadi  perubahan 
ulasan pada saat pandemik di ODTW. Nilai Stuart-Kendall Tau-
c menunjukkan hubungan yang sangat lemah dengan arah positif 
antara  rating  dan  sentimen  ulasan.  Penghitungan  data  cocok 
dengan  metode  Haversine,  Jaro  Winkler,  dan  Levenshtein 
menunjukkan  bahwa  data  hasil  web  scraping  dapat  menjadi 
bahan pelengkap data BPS.  

Kata Kunci— pariwisisata, COVID-19, ulasan, Google Maps 

I.  LATAR BELAKANG 

Sektor  pariwisata  memiliki  pengaruh  besar  bagi 
perekonomian  di  Indonesia.  Devisa  yang  disumbangkan  oleh 
sektor  pariwisata  pada  tahun  2018  naik  menjadi  USD17,6 
miliar  dengan  jumlah  kunjungan  wisatawan  mancanegara 
sebanyak 15,8 juta [1].  Setiap tahunnya kegiatan pariwisata di 
Indonesia  mengalami  peningkatan  yang  dapat  dilihat  dari 
bertambahnya  jumlah  wisatawan,  baik  wisatawan  nusantara 
(wisnus)  maupun  wisatawan  mancanegara  (wisman).  Pada 
tahun  2018  penambahan  wisnus  dan  wisman  masing-masing 
sebesar 12 persen dan 5,4 persen dari tahun 2017 [2]. Namun 
pada akhir tahun 2019 terjadi pandemik COVID-19 di Wuhan, 
Republik Rakyat China dan dampaknya juga dialami sampai ke 
Indonesia. 

Gambar 1. Jumlah Wisman dan Kasus Positif COVID-19 per Bulan Tahun 
2020 

Di Indonesia kasus COVID-19 pertama kali ditemukan pada 
tanggal 2 Maret 2020 [3]. Sejak kasus pertama tersebut, kasus 
COVID-19 di  Indonesia  terus  bertambah.  Pandemik  COVID-
19 telah berdampak pada sektor pariwisata, yang jelas terlihat 
dari  jumlah  kedatangan  wisman.  Perkembangan  kunjungan 
wisman dan kasus COVID-19 berbanding terbalik ditunjukkan 
pada Gambar 1. Kasus positif COVID-19 semakin bertambah, 
sedangkan jumlah kunjungan wisman semakin menurun [4].  

cara 

Pemerintah 

agar 
Indonesia  mulai  mencari 
perekonomian  Indonesia  dapat  berjalan  kembali  beriringan 
dengan  COVID-19.  Masyarakat  harus  mulai  beradaptasi 
dengan adanya virus ini dan tetap berkegiatan secara produktif. 
Untuk  itu  dibuatlah  Keputusan  Menteri  Kesehatan  (KMK) 
Republik  Indonesia  Nomor  HK.01.07/MENKES/382/2020 
tentang  Protokol  Kesehatan  Bagi  Masyarakat  di  Tempat  dan 
Fasilitas  Umum  dan  berlaku  mulai  tanggal  19  Juni  2020. 
Protokol  kesehatan  yang  dimaksud  yaitu  aturan-aturan  bagi 
masyarakat  untuk  menjalankan  kegiatan  sehari-hari,  salah 
satunya  kegiatan  yang  dilakukan  di  ODTW  sebagai  tempat 
umum. 

Fokus  penelitian  dilakukan  untuk  mengumpulkan  data 
ODTW  di  Provinsi  Jawa  Timur.  Hal  ini  dilakukan  karena 
Provinsi Jawa Timur memiliki jumlah usaha ODTW terbanyak 
di  Indonesia.  Tahun  2018  jumlah  ODTW  di  Provinsi  Jawa 
Timur  yaitu  529  perusahaan/usaha  [5].  Pada  tanggal  4 
Sepetember  2020  ditetapkan  Peraturan  Gubernur  (Pergub) 
Jawa  Timur  No.  53  Tahun  2020  tentang  Penerapan  Protokol 
Kesehatan  dalam  Pencegahan  dan  Pengendalian  COVID-19. 
Tetapi  pada  tanggal  16-17  September  2020  masih  terdapat 
7.003 pelanggar protokol kesehatan [6]. Hal ini dapat menjadi 
peringatan  bagi  pemerintah  setempat  untuk  secara  berkala 
memantau 
jalannya  protokol  kesehatan  sebagai  bentuk 
kepatuhan terhadap pergub.  

Selain  itu,  calon  pengunjung  ODTW  juga  harus  tahu 
penerapan protokol kesehatan di ODTW yang menjadi tujuan 
wisata. Tidak hanya informasi  protokol kesehatan, tetapi juga 
informasi mengenai fasilitas yang tersedia di ODTW tersebut. 
Kondisi  fasilitas  yang  tersedia  dapat  mempengaruhi  tingkat 
minat pengunjung suatu tempat wisata [7]. Informasi-informasi 
tersebut  dapat  diperoleh  dari  pengalaman  kunjungan  wisata 
yang telah dilakukan oleh pengunjung sebelumnya. 

Peneliti  mengumpulkan  ulasan  dari  Google  Maps  dengan 
metode web scraping. Google Maps sebagai layanan pemetaan 
berbasis web, menyediakan fitur ulasan untuk para pengguna. 
Rekomendasi  berdasarkan  pengalaman pengguna tidak hanya 
sebagai  pilihan  informasi  perjalanan,  tapi  sumber  paling 
berpengaruh  terhadap  keputusan  perjalanan  [8].  Pengusaha 
juga dapat memanfaatkan ulasan online untuk melihat kekuatan 
dan  kelemahan  layanan  sebagai  bahan  untuk  manajemen 
hubungan pelanggan [9].  

 1 / 8 

 
 
 
 
 
Ulasan  pada  Google  Maps  ini  termasuk  dalam  Big  Data, 
karena dapat menyimpan data yang besar dan menyimpan nilai 
lebih  yang  dapat  dianalisis.  Ulasan  terhadap  suatu  ODTW 
mengandung informasi mengenai protokol kesehatan, fasilitas 
yang  tersedia,  dan  informasi  lain  terkait  ODTW.  Untuk 
menggali  lebih  jauh  mengenai  sentimen  pengunjung,  dapat 
dilakukan dengan melakukan analisis sentimen dengan metode 
Multinomial  Naïve  Bayes  (MNB)  sebagai  classifier,  Term 
Frequency-Inverse  Document  Frequency  (TF-IDF)  sebagai 
faktor  pembobot,  dan  algoritma  pseudo-labelling  dalam 
melabeli data.  

Badan Pusat Statistik (BPS) sebagai lembaga statistik yang 
bertugas  mengumpulkan  data  Indonesia  juga  mengumpulkan 
data  terkait  ODTW.  Pengumpulan  data  dilakukan  dalam 
kegiatan  Survei  Perusahaan/Usaha  Objek  Daya  Tarik  Wisata 
yang dilakukan setiap tahun. Data yang dikumpulkan meliputi 
data  keterangan  pokok,  pengeluaran,  dan  pendapatan  usaha 
ODTW. Salah satu publikasi hasil dari kegiatan survei ini, yaitu 
Direktori Perusahaan/Usaha Objek Daya Tarik Wisata. Seperti 
halnya  BPS  yang  mengumpulkan  informasi  umum  terkait 
ODTW,  pada  Google  Maps  juga  mencantumkan  beberapa 
informasi tersebut. Sehingga terdapat potensi bahwa data pada 
Google  Maps  dapat  menjadi  pelengkap  data  BPS.  Evaluasi 
potensi pemanfaatan Big Data sebagai pelengkap data BPS ini 
akan  dilakukan  dengan  pencocokan  atau  matching  lokasi  dan 
nama  ODTW  dari hasil  pengumpulan data dengan data  BPS, 
yaitu  Direktori  Perusahaan/Usaha  Objek  Daya  Tarik  Wisata 
2020.  

II.  TUJUAN PENELITIAN 

Tujuan  dari  penelitian 

ini  adalah  untuk  memberikan 
informasi-informasi  mengenai  keadaan  ODTW  sebelum  dan 
saat  pandemik  COVID-19,  termasuk  penerapan  protokol 
kesehatan di Provinsi Jawa Timur. Informasi ini dikumpulkan 
dari analisis ulasan pengunjung melalui Google Maps. Analisis 
yang  dilakukan  bermanfaat  sebagai  bahan pertimbangan  bagi 
pengunjung, pengelola/pengusaha, dan pemerintah pada sektor 
pariwisata.  Selain  itu,  Big  Data  yang  dikumpulkan  memiliki 
potensi menjadi bahan pelengkap data resmi yang dikumpulkan 
oleh BPS. Maka, didapatkan tujuan penelitian yang lebih rinci 
sebagai berikut 

1.  Menganalisis penerapan protokol kesehatan pada ODTW 

di Provinsi Jawa Timur saat pandemik COVID-19, 

2.  Menganalisis kondisi fasilitas yang tersedia pada ODTW 

di Provinsi Jawa Timur, 

3.  Menganalisis  perubahan  sentimen  pengunjung  terhadap 
ODTW  di  Provinsi  Jawa  Timur  pada  Google  Maps 
sebelum dan saat pandemik COVID-19, 

rating 

perubahan 

4.  Menganalisis 

diberikan 
pengunjung terhadap ODTW di Provinsi Jawa Timur pada 
Google Maps sebelum dan saat pandemik COVID-19, 
5.  Menganalisis  keeratan  hubungan  antara  rating  dan 
sentimen  ulasan  pengunjung  pada  ODTW  di  Provinsi 
Jawa Timur, 

yang 

6.  Menganalisis kecocokan data hasil web scraping dengan 

data pengumpulan yang dilakukan oleh BPS. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

ini  bermanfaat 

Seperti  yang  telah  disebutkan  sebelumnya,  penelitian  ini 
bermanfaat bagi pengunjung untuk bahan pertimbangan dalam 
memilih  ODTW  yang  akan  dikunjungi.  Bagi  pengelola/ 
pengusaha,  penelitian 
sebagai  bahan 
pertimbangan untuk memperbaiki dan mengembangkan usaha 
ODTW  yang  dimiliki.  Dan  pemerintah  dapat  memanfaatkan 
ini  sebagai  bahan  pertimbangan  untuk 
hasil  penelitian 
mengawasi  berjalannya  kegiatan  di  ODTW,  khususnya 
mengenai  kepatuhan  terhadap  Pergub  No.  53  Tahun  2020. 
Selain  itu,  data  ODTW  ini  diharapkan  dapat  menjadi  bahan 
pelengkap data sektor pariwisata di BPS. 

III. PENELITIAN TERKAIT 

yang 

metode 

mengaplikasikan 

Penelitian  tentang  analisis  ulasan  online  telah  banyak 
dilakukan seiring berkembangnya teknologi. Analisis sentimen 
metode 
sebagai 
pengklasifikasian  teks  menjadi  label-label  tertentu,  dapat 
dilakukan dengan berbagai algoritma yang ada. Pada penelitian 
R. Wongso, F. A. Luwinda, B. C. Trisnajaya, dan O. R. Rudy 
kombinasi  metode 
perbandingan 
[10],  melakukan 
pengklasifikasian  teks.  Dari  penelitian  yang  telah  dilakukan, 
diperoleh  kesimpulan  bahwa  kombinasi  metode  TF-IDF  dan 
MNB adalah metode yang terbaik.  

Kemudian  D.  H.  Fudholi  dan  K.  P.  Juwairi  [11] 
mengimplementasikan  kombinasi  metode  TF-IDF  dan  MNB. 
Penelitian  ini  menerapkan  algoritma  pseudo-labelling  untuk 
mempermudah  pelabelannya.  Hasilnya  menunjukkan  bahwa 
kombinasi metode dan algoritma yang dilakukan telah berhasil 
digunakan. 

S.  R.  Kurniasari  [12]  melakukan  penelitian  terkait  analisis 
sentimen  dan  asosiasi  kata.  Setelah  melakukan  analisis 
sentimen,  kemudian  dilakukan  penghitungan  asosiasi  kata. 
Asosiasi kata dilakukan untuk menemukan hubungan antar kata 
pada  masing-masing  klasifikasi  ulasan  positif  dan  ulasan 
negatif.  Diperoleh  hasil  bahwa  pengunjung  hotel  mayoritas 
memiliki persepsi yang baik terhadap hotel dilihat dari asosiasi 
kata yang berhubungan dengan layanan hotel. 

R.  Vusvitasari,  S.  Nugroho,  dan  S.  Akbar  [14]  melakukan 
kajian  untuk  menguji  beberapa  metode  menghitung  korelasi. 
Metode  korelasi  yang  diuji  yaitu  korelasi  untuk  menghitung 
data  dengan  tipe  data  ordinal.  Hasil  penelitian  menunjukkan 
bahwa koefisien korelasi Spearman-rho (ρ) dan Kendall-tau (τ) 
baik  digunakan  untuk  pasangan  pengamatan  yang  tidak 
berdistribusi normal. Sedangkan koefisien korelasi Pearson (r) 
baik digunakan jika data pengamatan berdistribusi normal dan 
skala data serendah-rendahnya adalah interval atau rasio. 

Semakin  besarnya  data  di 

internet,  semakin  banyak 
informasi  yang  tersimpan  di  dalamnya.  Data  yang  diperoleh 
dari internet ini  dapat menjadi  pelengkap  statistik resmi  yang 
dikumpulkan oleh Badan Pusat Statistik (BPS). Data Matching 
dilakukan  untuk  melihat  kecocokan  antara  data  dari  internet 
yang dikumpulkan melalui web scraping dengan data BPS.  

Miftahuddin, Umaroh, dan Karim [13] melakukan penelitian 
mengenai  pengukuran  jarak  dua  titik  untuk  menghitung 
kesamaan  data  lokasi.  Penghitungan  jarak  menggunakan  titik 
longitude  dan  titik  latitude  lokasi  tersebut.  Perbandingan 
metode  Haversine,  Euclidean,  dan  Manhattan  dalam 

 2 / 8 

 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

penghitungan  jarak.  Kesimpulan  dari  hasil  perbandingan  tiga 
metode  tersebut,  yaitu  metode  Haversine  memiliki  tingkat 
akurasi dan ketepatan aplikasi yang lebih baik daripada metode 
yang lain. 

Penelitian  mengenai  data  matching  antara  data  hasil  web 
scraping  dengan  data  BPS  telah  dilakukan  oleh  Annisa  dan 
Pramana [15]. Pencocokan data dilakukan dengan metode Jaro 
Winkler  dan  Haversine.  Dua  lokasi  dikatakan  sama  apabila 
memiliki nilai  Jaro  Winkler lebih  dari  sama  dengan  0,85  dan 
nilai Haversine kurang dari sama dengan 2 km. Hasil penelitian 
menunjukkan terdapat 68,22% data yang match. 

Frequency  (TF-IDF)  sebelum  mengklasifikasikan  ulasan 
dengan metode Multinomial Naïve Bayes (MNB). Berikut 
adalah rumus klasifikasi MNB dengan TF-IDF [16] 
𝑃(𝑐|𝑡𝑒𝑟𝑚 𝑑𝑜𝑘𝑢𝑚𝑒𝑛 𝑑) =  𝑃(𝑐)𝑥 𝑃(𝑡1|𝑐)𝑥 𝑃(𝑡2|𝑐) 𝑥 … 𝑥 𝑃(𝑡𝑛|𝑐) 

(1) 
Dengan  𝑃(𝑐|𝑡𝑒𝑟𝑚 𝑑𝑜𝑘𝑢𝑚𝑒𝑛 𝑑)  adalah  peluang  suatu 
dokumen termasuk kelas c,  𝑃(𝑐) adalah peluang prior dari 
kelas c, 𝑡𝑛 adalah kata pada dokumen d ke-n, dan 𝑃(𝑡𝑛|𝑐) 
adalah peluang kata ke-n dengan diketahui kelas c. 

Pelabelan data menggunakan algoritma pseudo-labelling. 
Peneliti  membuat  dua  model  klasifikasi  sentimen,  berikut 
penjelasan alur klasifikasi sentimen yang dilakukan, 

IV. METODE PENELITIAN  

1.  Studi Pustaka 

Studi pustaka merupakan teknik pengumpulan data yang 
dilakukan  dengan  cara  mengumpulkan  informasi  dari 
berbagai  sumber  maupun  referensi  ilmiah  yang  relevan 
dengan  topik  penelitian.  Referensi  ilmiah  yang  dapat 
dijadikan  sumber  tidak  hanya  dalam  bentuk  media  cetak, 
telah 
namun 
dikumpulkan  dijadikan landasan  teori  dan panduan  dalam 
membuat  rancangan  sistem  pengumpulan  data  maupun 
analisis data. 

juga  media  digital. 

Informasi  yang 

2.  Tahap Pengumpulan Data 

Data  ulasan  ODTW  dikumpulkan  dari  Google  Maps 
menggunakan  metode  web  scraping  dengan  package 
Selenium  di  Python.  Web  scraping  dilakukan  dalam  dua 
tahap,  yaitu  pengumpulan  data  ODTW,  kemudian  data 
ulasan. Scraping ODTW dilakukan dengan pencarian kata 
kunci, yaitu wisata alam, wisata buatan, dan wisata budaya 
beserta penyebutan nama setiap kabupaten/kota di Provinsi 
Jawa  Timur,  contohnya  “Wisata  alam  di  Kabupaten 
Nganjuk”. Kata kunci tersebut dibuat berdasarkan definisi 
dari  ODTW  menurut  Undang-undang  Nomor  10  Tahun 
2009. Di Provinsi Jawa Timur terdapat 38 kabupaten/kota, 
sehingga kata kunci yang dibuat sebanyak 114 kata kunci. 
Scraping ulasan dilakukan dengan mengakses kembali link 
objek wisata yang telah disimpan saat scraping objek wisata. 

3.  Tahap Pre-processing Ulasan 

Sebelum  membuat  analisis  ulasan dilakukan  tahap  pre-

processing teks ulasan pengunjung sebagai berikut, 

a.  Cleaning:  menghilangkan  angka,  tanda  baca,  spasi 
baris, spasi di awal dan di akhir kalimat, karakter non-
ASCII,  emoji  dan  mengubah  semua  huruf  teks 
menjadi huruf kecil, 

b.  Tokenizing: tahap pemecahan kata pada kalimat ulasan, 

pada umumnya dipisah dengan spasi, 

c.  Menghilangkan stop word dan slang word: stop word 
adalah kata umum yang sering muncul, dan dianggap 
tidak memiliki makna yang berarti. Slang word adalah 
bahasa gaul, 

d.  Stemming: mengubah kata menjadi kata dasar dengan 

menghilangkan imbuhan pada kata tersebut. 

4.  Klasifikasi Sentimen Ulasan 

Tahapan  ini  dilakukan  untuk  mengklasifikasikan  teks 
ulasan  ke  kelas  positif,  netral,  dan  negatif.  Dilakukan 
Frequency-Inverse  Document 
pembobotan 

Term 

Gambar 2. Alur Program Klasifikasi Sentimen 

Klasifikasi sentimen yang dibuat sesuai dengan alur pada 
Gambar 2, menghasilkan 2 model pengklasifikasian. Model 
1  menguji  data  yang  telah  berlabel  manual.  Sedangkan 
Model 2 menguji data gabungan dari data berlabel dan data 
pseudo. 

5.  Evaluasi Model 

Evaluasi  model  dilakukan  untuk  mengukur  seberapa 
akurat model memprediksi data testing pada model prediksi 
sentimen  yang  telah  dibuat.  Metode  evaluasi  model  yang 
digunakan sebagai berikut, 

-  Nilai  Akurasi:  jumlah  prediksi  yang  benar  dibagi 

dengan banyaknya data yang diprediksi, 

-  Nilai  Cohen  Kappa:  koefisien  yang  mengevaluasi 
kesepakatan  diantara  dua  penilai,  yaitu nilai  prediksi 
dan  nilai  sesungguhnya.  Penghitungan  nilai  Cohen 
Kappa dengan rumus [17] 
𝜅 = (𝑝0 −   𝑝𝑒)/(1 − 𝑝𝑒) 

(2) 
Dengan  𝜅  adalah  nilai  Cohen  Kappa,  𝑝0  adalah  rasio 
kesepakatan yang diamati, dan 𝑝𝑒 adalah kesepakatan yang 
diharapkan  ketika  kedua  penilai  menetapkan  label  secara 
acak.  Tabel  I  adalah  klasifikasi  interpretasi  nilai  akurasi 
Cohen Kappa [18], 

TABEL  I 
KLASIFIKASI NILAI COHEN KAPPA 
Nilai Cohen Kappa  Kesepakatan 
< 0,0 
0,01 – 0,20 
0,21 – 0,40 
0,41 – 0,60 
0,61 – 0,80 
0,81 – 0,99 

Buruk 
Rendah 
Adil 
Cukup 
Baik 
Sempurna 

Evaluasi model dilakukan untuk validasi model dengan 
10-fold Cross Validation. 10-Fold Cross Validation adalah 
metode evaluasi model dengan membagi data menjadi 10-
fold untuk memvalidasi akurasi model. 

 3 / 8 

 
 
 
6.  Asosiasi Kata 

Asosiasi  kata  menunjukkan  keeratan  hubungan  antara 
dua atau lebih variabel yang sifatnya kualitatif [19]. Dalam 
penelitian  ini  asosiasi  kata  digunakan  untuk  mencari  kata 
yang  berhubungan  dengan  kata  yang  menunjukkan 
penerapan  protokol  kesehatan  dan  fasilitas  yang  tersedia 
pada  ODTW.  Sehingga  dapat  diketahui  topik  apa  yang 
sering  muncul  dan  berkaitan  dengan  penerapan  protokol 
kesehatan dan fasilitas yang tersedia pada ODTW. Rumus 
asosiasi kata yang digunakan sebagai berikut [12] 
𝑁 ∑ 𝑋𝑖𝑌𝑖 − (∑ 𝑋𝑖) (∑ 𝑌𝑖)
2 − (∑ 𝑋𝑖)2}{𝑁 ∑ 𝑌𝑖

2 − (∑ 𝑌𝑖)2}

√{𝑁𝑋𝑖

(3) 

𝑟 =

Penghitungan  asosiasi  kata  dilakukan  pada  Rstudio 
dengan bantuan fungsi R, yaitu findAssocs() pada package 
“tm”. Pada penelitian ini menggunakan batas nilai korelasi 
minimal sebesar 0,26. Menurut Tabel II, nilai korelasi 0,26 
menunjukkan  adanya  hubungan  yang  cukup  antara  dua 
variabel [20]. 

TABEL  II 
KLASIFIKASI NILAI KORELASI 
Interpretasi 
Tidak ada korelasi 
Korelasi sangat lemah 
Korelasi cukup 
Korelasi kuat 
Korelasi sangat kuat 
Korelasi sempurna 

Nilai Korelasi 
0 
>0-0,25 
>0,25-0,5 
>0,5-0,75 
>0,75-0,99 
1 

7.  Korelasi Stuart-Kendall Tau-c (𝜏𝑐) 

Korelasi  Stuart-Kendall  Tau-c 

adalah  metode 
menghitung korelasi dua variabel dengan tipe data ordinal. 
Korelasi ini digunakan ketika jumlah kategori pada variabel 
1 tidak sama dengan jumlah kategori variabel 2 atau tabel 
silang tidak square (jumlah baris tidak sama dengan jumlah 
kolom). Nilai korelasi tau-c berada pada nilai −1 ≤ 𝜏𝑐 ≤ 1. 
Nilai korelasi negatif menunjukkan sifat korelasi yang tidak 
searah, sedangkan nilai korelasi positif menunjukkan sifat 
korelasinya  searah  [20].  Penghitungan  korelasi  tau-c  baik 
digunakan pada data yang tidak berdistribusi normal [14]. 
Berikut rumus menghitung nilai 𝜏𝑐 [21],  

𝜏𝑐 = 𝑚(𝑃 − 𝑄)/𝑛2(𝑚 − 1) 

(4) 

Dengan P adalah jumlah konkordansi, Q adalah jumlah 
ketidaksesuaian, n adalah jumlah total pengamatan, dan m 
sama dengan min(R,C). R adalah banyak baris, sedangkan 
C  adalah  banyak  kolom.  Penghitungan  korelasi  Stuart-
Kendall  Tau-c  dilakukan  pada  aplikasi  Rstudio  dengan 
fungsi StuartTauC() dari library DecsTools. 

8.  Data Matching 

Data  matching  atau  pencocokan  data  dilakukan  untuk 
melihat kecocokan ODTW hasil scraping dan direktori BPS. 
Metode  pencocokan  yang  digunakan  adalah  string 
similarity dengan Jaro Winkler dan Levenshtein, serta jarak 
titik koordinat dengan Haversine.  

Sebelum  melakukan  data  matching,  perlu  melakukan 
konversi  data  untuk  memperoleh  titik  koordinat  objek 
wisata.  Latitude  dan  longitude  dari  data  hasil  scraping 
diperoleh  dari  link  objek  wisata  pada  Google  Maps  yang 
mengandung 
tersebut. 
Sedangkan  pada  data  BPS,  titik  koordinat  diperoleh  dari 

titik  koordinat  objek  wisata 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

konversi  variabel  alamat  objek  wisata  menjadi  titik 
koordinat.  Konversi  teks  alamat  menjadi  titik  koordinat 
disebut  juga  sebagai  geocoding.  Geocoding  dilakukan 
dengan  package  geopy  dari  Python.  Dari  package  ini 
digunakan 
titik 
koordinat  dari  suatu  alamat  di  dunia.  Sehingga  titik 
koordinat  ODTW  dapat  diperoleh  dan  digunakan  untuk 
menghitung jarak lokasi. 

library  ArcGIS  untuk  menemukan 

) 

+  

+  

1
3

 𝑥 (

(5) 

𝑑𝑗 =  

𝑚
|𝑠1|

𝑚
|𝑠2|

Pencocokan  data  dilakukan  dengan  atribut  nama 
kabupaten, nama  ODTW,  dan  titik koordinat.  Dua tempat 
dikatakan  sama apabila memiliki nilai  Jaro  Winkler nama 
kabupaten = 1 atau nilai Levenshtein nama kabupaten = 100, 
nilai  Jaro  Winkler  nama  ODTW  ≥  0,85  atau  nilai 
Levenshtein nama ≥ 85, dan nilai Haversine ≤ 2 km [15]. 
Berikut adalah rumus penghitungan jarak Jaro Winkler [22], 
𝑚 − 𝑡
𝑚
Dengan 𝑑𝑗  adalah  jarak  jaro, m adalah  jumlah  karakter 
yang  sama,  |𝑠1|  adalah  panjang  string  1,  |𝑠2|  adalah 
panjang string 2, dan 𝑡 adalah jumlah transposisi. Sehingga 
rumus penghitungan jarak Jaro Winkler seperti di bawah ini 
[22], 
𝑑𝑤 =   𝑑𝑗 + (𝑙𝑝(1 − 𝑑𝑗)) 
𝑑𝑗  adalah  jarak  jaro, 𝑙 adalah  panjang  prefiks  umum  di 
awal  string  dengan  nilai  maksimal  4  karakter.  (Panjang 
karakter  yang  sama  sebelum  ditemukan  ketidaksamaan 
maksimal  4),  dan  𝑝  adalah  konstantan  scaling  factor. 
Menurut  Winkler,  nilai  standar  𝑝  adalah  0,1.  Jarak  Jaro 
Winkler dihitung dengan bantuan fungsi jaro_winkler pada 
package jellyfish Python. 

(6) 

Berikut  rumus  Levenshtein  yang  digunakan  untuk 

pencocokan string [23], 

𝑙𝑒𝑣𝑎,𝑏(𝑖, 𝑗) =  

max(𝑖, 𝑗)                                        𝑗𝑖𝑘𝑎 min(𝑖, 𝑗) = 0

𝑚𝑖𝑛 {

𝑙𝑒𝑣𝑎,𝑏(𝑖 − 1, 𝑗) + 1
𝑙𝑒𝑣𝑎,𝑏(𝑖, 𝑗 − 1) + 1
𝑙𝑒𝑣𝑎,𝑏(𝑖 − 1, 𝑗 − 1) + 1(𝑎𝑖≠𝑏𝑗)

{

𝑙𝑎𝑖𝑛𝑛𝑦𝑎.

(7) 

Dengan  𝑙𝑒𝑣𝑎,𝑏(𝑖 − 1, 𝑗) + 1  adalah  operasi  penghapusan, 
𝑙𝑒𝑣𝑎,𝑏(𝑖, 𝑗 − 1) + 1 adalah operasi penyisipan, 𝑙𝑒𝑣𝑎,𝑏(𝑖 − 1, 𝑗 − 1) 
adalah  operasi  penghapusan,  dan  1(𝑎𝑖≠𝑏𝑗)  bernilai  1  jika 
elemen  𝑎𝑖 ≠ 𝑏𝑗 .  Dan  penghitungannya  menggunakan 
library Fuzzy Wuzzy pada Python 

Sedangkan  untuk  menghitung  jarak  antara  dua  lokasi 
dengan  menghitung  jarak  titik  koordinatnya,  digunakan 
rumus jarak Haversine sebagai berikut [13]  

𝑎 = 𝑠𝑖𝑛2 (

∅2 − ∅1
2

) + cos(∅1) . cos(∅1) . 𝑠𝑖𝑛2(

𝜆2 − 𝜆1
2

) 

(8) 

𝑑 = 2𝑟. arcsin (√𝑎) 
Dengan 𝑑 adalah  jarak  Haversine, ∅1 & ∅2  adalah  titik 
latitude,  dan  𝜆1  &  𝜆2  adalah  titik  longitude.  Metode  ini 
dihitung dengan bantuan package math Python. 

(9) 

Hasil pencocokan data yang telah memenuhi threshold di 
atas, dievaluasi dengan string evaluation berupa Character 
Error Rate (CER). Penghitungan CER dengan rumus [24] 
(10) 

𝐶𝐸𝑅 = (𝑆 + 𝐼 + 𝐷)/𝑁 

 4 / 8 

 
 
 
 
 
 
 
 
 
S  adalah  banyaknya  karakter  yang  diganti/ditukar 
(Substitution), I adalah banyaknya karakter yang disisipkan 
(Insertion),  D  adalah  banyaknya  karakter  yang  dihapus 
(Deletion),  dan  N  adalah  karakter  pada  string  atau  kata 
referensi.  Penghitungannya  dilakukan  dengan  bantuan 
package  fastwer  Python.  Sehingga  diperoleh  nilai  akurasi 
CER, yaitu 100-CER. 

V.  KERANGKA PIKIR 

Penelitian  ini  berawal  dari  masih  banyaknya  pelanggaran 
protokol  kesehatan  yang  terjadi  di  Provinsi  Jawa  Timur. 
Google Maps menyediakan fitur ulasan online yang dianggap 
sebagai Big Data dan dapat digali informasinya untuk analisis 
lebih lanjut. Penelitian ini menggunakan data primer yaitu data 
ODTW  hasil  web  scraping  dari  Google  Maps,  dan  data 
sekunder  dari  Direktori  Perusahaan/Usaha  ODTW  2020. 
Kerangka pikir penelitian ini ditunjukkan pada Gambar 3. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

VI. HASIL DAN PEMBAHASAN 

Data  ODTW  yang  berhasil  dikumpulkan  sebanyak  13.708, 
dan  kemudian  dilakukan  pre-processing  sehingga  jumlahnya 
menjadi  2.694.  Sedangkan  data  direktori  ODTW  BPS 
menunjukkan,  Jawa Timur  memiliki  526  usaha  ODTW.  Data 
ulasan yang dikumpulkan sebanyak 645.875 ulasan, kemudian 
dilakukan  pre-processing  dan  penyamaan  periode  waktu 
sehingga jumlahnya menjadi 238.274 data ulasan. 

Analisis  sentimen  dilakukan  menggunakan  data  sebanyak 
2.383 untuk Model 1, data pseudo label sebanyak 47.179, dan 
memprediksi data sebanyak 188.712 dengan Model 2. Berikut 
hasil evaluasi model yang telah dibuat, 

TABEL  III 
HASIL EVALUASI MODEL KLASIFIKASI SENTIMEN 

Model 
Model 1 
Model 2 

Akurasi  Cohen Kappa 
76,37% 
92,29% 

39,14% 
59,12% 

Tabel III menunjukkan nilai akurasi dan nilai Cohen Kappa 
dari dua model yang telah dibuat. Kedua model menunjukkan 
hasil yang adil untuk Model 1 dan cukup untuk Model 2 [18]. 
Setelah memprediksi data tidak berlabel dengan Model 2, data 
digabungkan  kembali  menjadi  satu.  Diperoleh  hasil  akhir 
pengklasifikasian sentimen sebagai berikut, 

TABEL  IV 
HASIL AKHIR KLASIFIKASI SENTIMEN 

Data 
Hasil Akhir 

Positif  Netral  Negatif  Total Data 
219.322 

238.274 

15.929 

3.023 

Gambar 4. Klasifikasi Ulasan Berdasarkan Kelas Sentimen 

Banyaknya  ulasan  berdasarkan kelas  sentimen  ditunjukkan 
pada  Tabel  IV.  Dari  Gambar  4,  92,05%  data  berlabel  positif, 
1,27% data berlabel negatif, dan 6,69% data berlabel netral. 

Kata  kunci  fasilitas  umum  yang  tersedia  meliputi  fasilitas, 
parkir,  kamar  mandi,  dan  musala  [7].  Sedangkan  kata  kunci 
protokol kesehatan di tempat umum, diambil dari Pergub Jawa 
Timur  No.  53  tahun  2020,  meliputi  masker,  cuci  tangan, 
handsanitizer,  jaga  jarak,  cek  suhu,  prokes,  pandemi,  dan 
protokol.  Untuk  mencari  asosiasi  kata  fasilitas  yang  tersedia, 
digunakan data ulasan berlabel positif dan negatif. Sedangkan 
untuk mencari informasi berkaitan dengan protokol kesehatan, 
digunakan data yang ditulis setelah adanya Pergub Jawa Timur 
No. 53 Tahun 2020 untuk melihat kepatuhan ODTW terhadap 
tersebut.  Sebelum  menghitung  asosiasi  kata, 
peraturan 
dilakukan  penyaringan  data  kembali.  Penyaringan  data 
dilakukan  untuk  memilih  data  dengan  atribut  pre-processing 
yang  memiliki  kata-kata  kunci  di  atas.  Batas  korelasi  yang 
digunakan  adalah  sebesar  0,26.  Korelasi  0,26  menunjukkan 
hubungan yang cukup kuat [20]. 

 5 / 8 

Gambar 3. Kerangka Pikir Penelitian 

 Sektor pariwisata yang memiliki pengaruh besar terhadap perekonomian Indonesia ikut terdampak COVID-19, termasuk usaha ODTW. Ditetapkannya protokol kesehatan untuk pencegahan dan pengendalian COVID-19. Ulasan online pada Google Maps sebagai Big Data menyimpan informasi yang dibutuhkan Jawa Timur dengan jumlah ODTW terbanyak di Indonesia, masih banyak terjadi pelanggaran protokol kesehatan terhadap Pergub No 53 Tahun 2020. Perlu adanya pengawasan terhadap penerapan protokol kesehatan dan pengunjung butuh informasi terkait ODTW, seperti penerapan protokol kesehatab, fasilitas yang tersedia, dan informasi lain terkait ODTW. Metode yang digunakan: 1. Pengumpulan data dengan web scraping, 2. Data Preprocessing, 3. Analisis sentimen dengan pseudo labelling, TF-IDF dan Multinomial Naïve Bayes Classifier, 4. Asosiasi kata, 5. Analisis deskriptif rating pengunjung, 6. Korelasi Stuart-Kendall Tau-c 7. Kecocokan data dengan Jaro Winkler, Levenshtein dan jarak Haversine, Evaluasi model klasifikasi sentimen dengan nilai akurasi dan nilai Cohen’s Kappa Output penelitian: 1. Informasi terkait penerapan protokol kesehatan saat pandemik, 2. Informasi terkait fasilitas yang tersedia di ODTW, 3. Jumlah ODTW berdasarkan perubahan sentimen saat pandemik, 4. Jumlah ODTW berdasarkan perubahan rating saat pandemik, 5. Besar dan arah hubungan antara rating dan sentimen ulasan, 6. Jumlah ODTW yang cocok antara hasil web scraping dan data BPS, Data primer: Data ODTW dan ulasannya pada Google Maps Data sekunder: Direktori DTW BPS tahun 2020 Data pada Google Maps memiliki potensi menjadi pelengkap bagi data BPS  
 
 
  
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Informasi yang terkait dengan penerapan protokol kesehatan 
menggunakan  1.249 ulasan  yang  telah disaring.  Berikut hasil 
penghitungan asosiasi kata, 

rating  yang  diberikan  pengunjung  terhadap  masing-masing 
ODTW. 

TABEL  V 
ASOSIASI KATA PROTOKOL KESEHATAN 
Ulasan Pergub (corlimit= 0,26) 

0,61 

pakai 

masker 

protokol kesehatan 
0,32 
terap 
0,29 
patuh 

cuci_tangan 
0,36 
sedia 
0,34 
sabun 
Tabel  V  menunjukkan  kata  masker  yang  sering  muncul 
dengan  kata  pakai,  kata  cuci tangan  dengan  sedia dan  sabun, 
serta  kata  protokol  kesehatan  dengan  terap  dan  patuh. 
Penghitungan  asosiasi  kata  di  atas  menunjukkan  bahwa 
penerapan  protokol  kesehatan  telah  diterapkan  di  ODTW 
Provinsi  Jawa  Timur.  Protokol  kesehatan  tersebut  meliputi 
pemakaian  masker,  penyediaan  tempat  cuci  tangan  dengan 
sabun, dan penerapan protokol kesehatan yang telah dipatuhi. 
Untuk mencari informasi terkait fasilitas yang tersedia, data 
yang  digunakan  adalah  data  berlabel  positif  dan  negatif  saja, 
masing-masing  sebanyak  13.494  dan  422  ulasan.  Hasilnya 
sebagai berikut, 

TABEL  VI 
ASOSIASI KATA FASILITAS YANG TERSEDIA PADA DATA BERLABEL POSITIF 
Ulasan Positif (corlimit= 0,26) 

fasilitas 

parkir 

0,26 

parkir 

lengkap 

0,3  masuk 
Dari ulasan positif pada Tabel VI menunjukkan bahwa kata 
fasilitas  sering muncul  bersama dengan  kata  lengkap,  dengan 
nilai korelasi sebesar 0,3. Sedangkan kata parkir sering muncul 
bersama dengan kata masuk, dengan nilai korelasi sebesar 0,26. 
TABEL  VII 
ASOSIASI KATA FASILITAS YANG TERSEDIA PADA DATA BERLABEL NEGATIF 
Ulasan Negatif (corlimit= 0,26) 
kamar_mandi 
0,33 
kotor 
0,33 
renovasi 
kunci 
0,27 
Masih  terdapat  persepsi  pengunjung  mengenai  fasilitas 
yang tersedia yang bersentimen negatif yang ditunjukkan oleh 
Tabel  VII.  Kata  parkir  sering  muncul  bersama  dengan  kata 
masuk, dengan nilai korelasi sebesar 0,32. Kata kamar mandi 
sering muncul Bersama dengan kata kotor, renovasi, dan kunci, 
dengan  nilai  korelasi  masing-masing  sebesar  0,33,  0,33,  dan 
0,27.  Perlu  dilakukan  evaluasi  dan  perbaikan  untuk  fasilitas 
dengan sentimen negatif. 

masuk 

0,32 

Dari hasil penghitungan asosiasi kata fasilitas yang tersedia 
di  atas  menunjukkan  usaha  ODTW  di  Provinsi  Jawa  Timur 
memiliki  fasilitas  yang lengkap,  seperti  tempat  parkir. Tetapi 
masih  ada  fasilitas  yang  harus  mendapat  perhatian  khusus 
karena masih mendapat ulasan negatif, yaitu tempat parkir dan 
kamar mandi. 

Untuk  melihat  perubahan  ulasan  maupun  rating  sebelum 
dan  saat  pandemik,  data  dibagi  menjadi  dua.  Yang  termasuk 
dalam  data  sebelum  pandemik  adalah  ulasan  yang  memiliki 
atribut  publish  date  1  tahun  yang  lalu,  sedangkan  yang 
termasuk  dalam  saat  pandemik  adalah  ulasan  yang  memiliki 
atribut  publish  date  kurang  dari  1  tahun  yang  lalu.  Hal  ini 
dihitung sejak pengumpulan data selesai dilakukan.  Kemudian 
dihitung jumlah ulasan berdasarkan waktu dan label sentimen, 
dan perubahan label sentimen maupun perubahan rata-rata nilai 

Gambar 5. Jumlah Sentimen Berdasarkan Waktu dan Kelas Sentimen 
Gambar  5  menunjukkan  perubahan 

jumlah  sentimen 
berdasarkan  waktu  dan  kelas  sentimennya.  Pada  ketiga  kelas 
sentimen terjadi penurunan jumlah ulasan terhadap ODTW di 
Jawa  Timur  saat  pandemik.  Hal  ini  menunjukkan  selama  1 
tahun  terakhir  saat  pandemik,  jumlah  pengunjung  ODTW 
berkurang.  Hal  ini  disebabkan  adanya  pembatasan  kegiatan 
pada  fasilitas/tempat  umum  untuk  mencegah  penyebaran 
COVID-19. 

Gambar 6. Persentase ODTW Berdasarkan Perubahan Label Sentimen 
Dihitung dari jumlah kelas sentimen terbanyak pada setiap 
ODTW,  menunjukkan  tidak  terjadinya  perubahan  sentimen. 
Terdapat 76% ODTW berlabel positif yang tetap mendapatkan 
label  positif  dari  pengunjung  saat  pandemik,  seperti  pada 
Gambar 6. Selain itu, perubahan yang terjadi meliputi ODTW 
yang tidak ada ulasan atau kosong menjadi bersentimen positif 
saat  pandemik  dan  ODTW  yang  bersentimen  positif  menjadi 
tidak ada ulasan saat pandemik, dan perubahan-perubahan lain 
yang jumlahnya sangat sedikit. 

Gambar 7. Persentase ODTW Berdasarkan Perubahan Nilai Rata-rata Rating 

 6 / 8 

 
 
 
 
 
 
 
 
 
 
  
Dilihat  dari  nilai  rata-rata  rating  setiap  ODTW,  terjadi 
perubahan  naik  pada  56,53%  ODTW  saat  pandemik 
ditunjukkan  pada  Gambar  7.  Penilaian  yang  diberikan 
pengunjung  saat  pandemik  menjadi  lebih  baik  dibandingkan 
sebelum  pandemik.  Tetapi,  ada 
juga  yang  mengalami 
penurunan  nilai  rata-rata  rating,  atau  perubahan  penilaian 
pengunjung yang menurun saat pandemik. Selain itu, terdapat 
ODTW  dengan  penilaian  yang  sama  sebelum  pandemik 
maupun saat pandemik. 

Untuk melihat hubungan antara rating  dan label  sentimen 
pengunjung, peneliti melakukan penghitungan korelasi Stuart-
tabel 
Kendall  Tau-c  pada  aplikasi  Rstudio.  Membuat 
kontingensi dilakukan sebelum menghitung korelasi.  

TABEL  VIII 
TABEL KONTINGENSI 

Label Sentimen 
Positif  Netral  Negatif 
312 
411 
1.089 
701 
510 

4.594 
4.034 
19.869 
51.098 
139.727 

385 
277 
1.315 
3.222 
10.730 

Rating 

1 
2 
3 
4 
5 

Tabel VIII adalah tabel kontingensi nilai rating dengan label 
sentimen.  Penghitungan  korelasi  Stuart-Kendall  Tau-c  di 
Rstudio  menghasilkan  nilai  korelasi  sebesar  0,016.  Menurut 
Sarwono,  nilai  ini  termasuk  dalam  kategori  korelasi  yang 
sangat lemah dengan arah yang positif atau searah [20]. Arah 
korelasi  positif  menunjukkan  bahwa  ketika  pengunjung 
memberikan  nilai  rating  yang  baik,  maka  label  sentimennya 
juga  baik  atau  positif.  Tetapi  korelasi  antara  rating  dan 
sentimen  ulasan  sangat  lemah,  hal  ini  menunjukkan  bahwa 
keduanya  kurang  saling  berhubungan.  Ada  faktor-faktor  lain 
yang  mungkin  lebih  memiliki  kedekatan  hubungan  dengan 
rating dan sentimen. 

Konversi  data  untuk  memperoleh  titik  latitude  dan  titik 
longitude  dilakukan  pada  kedua  data  ODTW,  hasil  scraping 
maupun  direktori  BPS.  Kemudian  dilakukan  data  matching 
dengan hasil sebagai berikut, 

Gambar 8. Hasil Data Matching ODTW 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar  8  menunjukkan  hasil  metode  kombinasi  yang 
digunakan.  stringDari  penghitungan  akurasi  string  dapat 
diketahui bahwa metode matching string dengan Jaro Winkler 
lebih baik daripada metode Levenshtein. Akurasi string terbaik 
diperoleh dari kombinasi metode Jaro  Winkler dan Haversine 
dengan akurasi string sebesar 74%. Tetapi jumlah data cocok 
yang  paling  banyak  diperoleh  dari  implementasi  metode  Jaro 
Winkler, yaitu sebanyak 195 data atau 37% dari data direktori 
BPS dengan akurasi string sebesar 63%. Akurasi ini termasuk 
nilai  yang  tinggi  jika  dibandingkan  dengan  nilai  akurasi  dari 
kombinasi  metode  yang  lain.  Dapat  dikatakan  bahwa  data 
ODTW dari Google Maps ini memiliki potensi untuk menjadi 
bahan pelengkap data BPS, karena terdapat kecocokan antara 
keduanya. 

VII. 

PENUTUP 

Berdasarkan  penelitian  yang 

telah  dilakukan,  dapat 

disimpulkan bahwa, 

1.  Analisis  sentimen  dengan  metode  MNB  +  TF-IDF  dan 
menerapkan  algoritma  pseudo-labelling  telah  berhasil 
dilakukan untuk melabeli ulasan ODTW di Google Maps, 
2.  Protokol  kesehatan  sesuai  dengan  Pergub  No  53  Tahun 
2020  telah  diterapkan  di  ODTW  Provinsi  Jawa  Timur, 
salah satunya dengan penggunaan masker dan penyediaan 
tempat cuci tangan dengan sabun, 

3.  Telah  tersedia  fasilitas  yang  lengkap  di  ODTW,  seperti 
tempat  parkir  masuk.  Tetapi  masih  perlu  dilakukan 
perbaikan fasilitas kamar mandi di ODTW Jawa Timur, 
4.  Pandangan  pengunjung  terhadap  ODTW  Jawa  Timur 
tidak mengalami perubahan saat pandemik, hal ini dapat 
dilihat  dari  perubahan  sentimen.  Sentimen  pengunjung 
terhadap ODTW tetap positif saat pandemik, 

5.  Penilaian  pengunjung  terhadap  ODTW  Jawa  Timur 
mengalami  peningkatan,  dapat  dilihat  dari  banyaknya 
ODTW dengan rating yang semakin baik saat pandemik, 
6.  Penghitungan  korelasi  menunjukkan  terjadi  hubungan 
yang sangat lemah dengan arah positif antara rating dan 
sentimen pengunjung, 

Peneliti  menyarankan 

7.  Data  web  scraping  Google  Maps  dapat  berpotensi 
menjadi  data  pelengkap  bagi  BPS  ditunjukkan  dengan 
nilai match sebesar 37%. 
Dari uraian di atas menunjukkan bahwa sektor pariwisata, 
khususnya usaha ODTW menjadi usaha yang potensial saat 
pandemik  dengan  tetap  menerapkan  protokol  kesehatan 
sesuai dengan peraturan yang ada dan menyediakan fasilitas 
yang baik maupun memperbaiki fasilitas yang ada.  
pengusaha 

agar 
memperhatikan  kembali  fasilitas  yang  tersedia  pada  usaha 
ODTW  miliknya.  Perlu  dilakukan  perbaikan  atau  renovasi 
terhadap  fasilitas  yang  tersedia,  khususnya  kamar  mandi. 
Selain  itu,  baik  pengunjung  maupun  pengusaha  harus  tetap 
selalu  menjaga  penerapan  protokol  kesehatan  di  ODTW. 
Untuk  penelitian  selanjutnya  dapat  menggali  lebih  banyak 
informasi yang terkandung dalam ulasan pada Google Maps, 
tidak hanya tempat wisata. Hal ini mengingat potensi ulasan 
Google  Maps  yang  menyimpan  informasi  berdasarkan 
pengalaman pengunjung. 

untuk 

 7 / 8 

 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

[21]  S. Institute, SAS/STAT 9.2 User's Guide, Second Edition:: The FREQ 
Procedure (Book Excerpt) 2nd Edition, United States of America: SAS 
Publishing, 2009.  

[22]  A. Kurniawati, S. Puspitodjati and S. Rahman, ""Implementasi 

Algoritma Jaro-Winkler Distance untuk Membandingkan Kesamaan 
Dokumen Berbahasa Indonesia,"" Universitas Gunadarma, Depok, 
2010. 

[23]  D. P. Putra, ""Pemanfaatan Algoritma Pencocokan String dalam 

Pencarian Kata Tidak Baku pada KBBI,"" Sekolah Teknik Elektro dan 
Informatika, Bandung, 2020. 

[24]  J. Read, E. Mazzone and M. Horton, Recognition Errors and 

Recognizing Errors – Children Writing on the Tablet PC, Preston: 
University of Central Lancashire, 2005.  

DAFTAR PUSTAKA 

[1]   Kemenpar, Buku Saku Manajemen Krisis Kepariwisataan Kementerian 
Pariwisata Republik Indonesia, Jakarta: Kementerian Pariwisata 
Republik Indonesia, 2019.  

[2]   Kemenparekraf, ""Rencana Strategis Tahun 2020-2024,"" Kementerian 

Pariwisata dan Ekonomi Kreatif, Jakarta, 2020. 

[3]   Satuan Tugas Penanganan COVID-19, ""Perkembangan Kasus Per-hari 

Komite Penanganan COVID-19 dan Pemulihan Ekonomi Nasional,"" 
Jakarta, 2020. 

[4]   Badan Pusat Statistik (BPS), ""Berita Resmi Statistik,"" BPS RI, 2020. 
[Online]. Available: https://www.bps.go.id/. [Accessed 29 March 
2020]. 

[5]   Badan Pusat Statistik (BPS), ""Statistik Objek Daya Tarik Wisata 

2018,"" BPS RI, Jakarta, 2020. 

[6]   D. Kurniawan, ""Polda Jatim Jaring 7.000 Pelanggar Protokol 

Kesehatan, Denda Rp 63 Juta,"" Liputan6.com, Surabaya, 2020. 

[7]   Suchaina, ""Pengaruh Kualitas Fasilitas Sarana dan Prasarana Tergadap 
Peningkatan Jumlah Pengunjung Wisata Danau Ranu Grati,"" Jurnal 
Psikologi, vol. 2, no. 2, pp. 89-109, 2014.  

[8]   B. Pan, J. C. Crotts and T. Maclaurin, ""Travel Blogs and the 

Implications for Destination Marketing,"" Journal of Travel Research, 
vol. 46, no. 1, pp. 35-45, 2007.  

[9]   Y. Cho, I. Im and R. Hitlz, ""The Impact of E-SErvices Failures and 

Customer Complaints on Electronic Commerce Customer Relationship 
Management,"" Journal of Consumer Satisfication, Dissatisfication & 
Complaining Behavior, vol. 16, p. 106, 2003.  

[10]  R. Wongso, F. A. Luwinda, B. C. Trisnajaya and O. R. Rudy, ""News 

Article Text Classification in Indonesian Language,"" in 2nd 
International Conference on Computer Science and Computational 
Intelligence 2017, Bali, 2017.  

[11]  D. H. Fudholi and K. P. Juwairi, Pemanfaatan Teknik Semi-supervised 

Learning untuk Klasifikasi Dokumen Medis [SKRIPSI], Yogyakarta: 
Universitas Islam Indonesia, 2019.  

[12]  S. R. Kurniasari, Implementasi SVM dan Asosiasi untuk Sentment 

Analysis Data Ulasan The Phoenix Hotel Yogyakarta pada Situs 
TripAdvisor [TUGAS AKHIR], Yogyakarta: Universitas Islam 
Indonesia, 2018.  

[13]  Y. Miftahuddin, S. Umaroh and F. R. Karim, ""Perbandingan Mrtode 
Perhitungan Jarak Euclidean, Haversine, dan Manhattan dalam 
Penentuan Posisi Karyawan (Studi Kasus: Institut Teknologgi Nasional 
Bandung),"" Jurnal Tekno Insentif, vol. 14, no. 2, pp. 69-77, 2020.  

[14]  R. Vusvitasari, S. Nugroho och S. Akbar, ”Kajian Hubungan Koefisien 
Korelasi Pearson, Spearman-rho, Kendall-Tau, Gamma, dan Somers,” 
Gradien, vol. 4, nr 2, 2008.  

[15]  C. F. Annisa and S. Pramana, ""Kajian Pemanfaatan Data Google Maps 
dalam Official Statistics (Studi Kasus: Usaha Sektor Penyedia Makan 
Minum di Pulau Jawa dan Bali),"" Seminar Nasional Official Statistics 
2020, vol. 2020, 2020.  

[16]  A. Rahman, Wiranto and A. Doewes, ""Online News Classification 
Using Multinomial Naive Bayes,"" Jurnal Ilmiah Teknologi dan 
Informasi, vol. 6, no. 1, 2017.  

[17]  J. Cohen, ""A coefficient of agreement for nominal scales,"" Educational 
and Psychological Measurement, vol. 20, no. 1, pp. 37-46, 1960.  

[18]  Poesio, R. Artstein and Massimo, ""Inter-Coder Agreement for 

Computational Linguistics,"" Association for Computational Linguistics, 
vol. 34, no. 4, 2008.  

[19]  N. M. Ulwan, Pattern Recognition pada Unstructured Data Teks 

Menggunakan Support Vector Machine dan Association [TUGAS 
AKHIR], Yogyakarta: Universitas Gajah Mada, 2016.  

[20]  J. Sarwono, ""Korelasi,"" 2011. [Online]. Available: 

https://www.jonathansarwono.info/korelasi/korelasi.htm. [Accessed 29 
March 2020]. 

 8 / 8 

 
 
 
 
 
 
 
 
"
221709635,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pengembangan Mekanisme Identifikasi Duplikasi 
Akun Toko pada Pembentukan Repositori 
Marketplace untuk Penyusunan Statistik  
E-Commerce  

Dina Salsabila (221709635, 4SD2) 
Dosen Pembimbing: Takdir, SST., M.T. 

Ringkasan— Saat ini BPS mulai melakukan pengumpulan data 
marketplace  untuk  menggambarkan  keadaan  e-commerce  di 
Indonesia.  Dalam  proses  pengumpulan  data  sering  terjadi 
duplikasi data akun toko yang mempunyai akun toko lebih dari 
satu  marketplace  sehingga  berdampak  pada  hasil  analisis  data 
marketplace  menjadi  bias.  Untuk  mengurangi  duplikasi  data, 
maka  penelitian  ini  mengkaji  pengembangan  mekanisme  untuk 
mengidentifikasi  duplikasi  akun  toko  antar  marketplace  yang 
berbeda.  Algoritma  kesamaan  kata  yang  digunakan  yaitu 
Levenshtein  Distance  untuk  kesamaan  nama  toko  dan  Cosine 
Similarity  untuk  kesamaan  etalase,  deskripsi  dan  nama  produk. 
Hasil  pengujian  dengan  regresi  logistik  biner  diketahui  bahwa 
variabel  yang  memiliki  pengaruh  terbesar  terhadap  status 
kesamaan  akun  toko  berturut-turut  yaitu  nama  toko,  nama 
produk, etalase toko, dan deskripsi toko. Hasil prediksi kesamaan 
akun  toko  dengan  model  tersebut  memiliki  total  akurasi  untuk 
data  dengan  dan  tanpa  stemming  berturut-turut  sebesar  96.3% 
dan  96.2%.  Berdasarkan  hasil  tersebut  maka  model  ini  bisa 
digunakan  untuk  memprediksi  kesamaan  akun  toko  antar 
marketplace,  sehingga  bisa  membentuk  repositori  marketplace 
untuk penyusunan statistik e-commerce. 

Kata  Kunci—  Marketplace,  algoritma  kesamaan  kata, 

Levenshtein Distance, Cosine Similarity,  regresi logistik biner. 

I.  LATAR BELAKANG 
teknologi 

Pesatnya 

informasi 

perkembangan 

dan 
komunikasi  menyebabkan  terjadinya  banyak  perubahan  salah 
satunya  dalam  penyebaran  informasi  dengan  menggunakan 
survei  Asosiasi 
tercatat  pada  hasil 
internet.  Seperti 
Penyelenggara Jasa Internet Indonesia (APJII) periode 2019 - 
kuartal II/2020 [1], jumlah pengguna internet di Indonesia naik 
menjadi 73.7% dari total populasi 266.91 juta jiwa penduduk 
Indonesia  atau  mencapai  196,71  juta  pengguna.  Jumlah  ini 
meningkat 25,5 juta atau 8,9% dibandingkan pada 2018 lalu.  
Pertumbuhan internet dan teknologi informasi saat ini tentunya 
mempengaruhi  dalam  proses  pengumpulan  data,  pengolahan 
hingga penyajian data.  

Selain dalam hal pengumpulan data, perkembangan internet, 
teknologi dan informasi juga berpengaruh dalam segi ekonomi. 
Dalam aspek ekonomi lahir istilah baru yaitu ekonomi digital. 
Hal ini ditandai dengan banyaknya perubahan sistem penjualan 
dari  offline  menuju  online.  Konsep  ekonomi  digital  pertama 
kali  diperkenalkan  oleh  Tapscott  (1997),  yaitu  merupakan 
sebuah fenomena sosial yang mempengaruhi sistem ekonomi, 
dimana  fenomena  tersebut  mempunyai  karakteristik  sebagai 

instrument 

sebuah  ruang  intelijen,  meliputi  informasi,  berbagai  akses 
informasi  dan 
terhadap 
pemrosesan  informasi  [2].  Komponen  ekonomi  digital  yang 
berhasil  diidentifikasi  pertama  kalinya  yaitu  industry  TIK, 
aktivitas e-commerce, dan distribusi digital barang dan jasa.  

informasi,  kapasitas 

E-commerce  merupakan  salah  satu  bagian  dari  ekonomi 
digital.  E-commerce  berdasarkan  Organization  for  Economic 
Co-Operation  and  Development 
(OECD)  2011  adalah 
penjualan atau pembelian barang/jasa, yang dilakukan melalui 
jaringan  komputer  dengan  metode  yang  secara  spesifik 
dirancang  untuk  tujuan  menerima  atau  melakukan  pesanan, 
tetapi  pembayaran  dan  pengiriman  utama  barang/jasa  tidak 
harus  dilakukan  secara  online  [3].  Seiring  berjalannya  waktu 
dengan semakin banyaknya jumlah pengguna smartphone dan 
pengguna  internet,  semakin  banyak  pula  orang  yang  mulai 
berjualan  maupun  berbelanja  melalui  internet.  Penjualan  dan 
pembelian barang online bisa dilakukan secara langsung antara 
penjual dan pembeli melalui media sosial atau website, dan bisa 
menggunakan platform digital sebagai perantara antara penjual 
dan  pembeli.  Platform  digital  atau  disebut  juga  marketplace 
adalah  sebuah  lokasi  jual  beli  produk  dimana  penjual  dan 
konsumen  bertemu  di  suatu  marketplace/platform  digital. 
Dilansir  dari  Statista.com  [4],  tiga  platform  e-commerce 
terbesar di Indonesia yaitu Shopee, Tokopedia, dan Bukalapak. 
Adanya  marketplace  sebagai  platform  untuk  jual-beli  ini 
mempermudah masyarakat untuk melakukan kegiatan ekonomi 
secara  praktis  dan  cepat.  Dari  sisi  pembeli,  platform  ini 
memudahkan  pembeli  untuk  mendapatkan  informasi  suatu 
produk  dari  berbagai  toko  online  dan  bisa  memilih  serta 
membandingkan produk dengan mudah dan cepat. Sedangkan 
dari  sisi  penjual,  platform  ini  memudahkan  dalam  sistem 
berjualan  sehingga  penjual  tidak  perlu  memikirkan  strategi 
penjualan  dan  lainnya.  Dengan  berbagai  kemudahan  yang 
ditawarkan oleh berbagai marketplace ini, maka jumlah orang 
yang 
dengan 
mendaftarkan tokonya ke berbagai online marketplace.  

semakin  meningkat 

berjualan 

online 

satu 

Data marketplace dapat dianalisis untuk melihat banyaknya 
toko yang berjualan online, banyak barang terjual, serta omzet 
penjualan.  Salah 
tantangan  dalam  analisis  data 
marketplace adalah dalam menghitung jumlah akun toko yang 
sebenarnya di marketplace. Selain itu juga pada penghitungan 
jumlah barang terjual dan omzet penjualan dari toko tersebut. 
Hal  ini  disebabkan  karena  penjual  dapat  mempunyai  toko 
online lebih dari satu marketplace dengan nama akun toko yang 

 1 / 8 

 
 
 
 
mirip.  Banyaknya  duplikasi  data  berupa  akun  toko  dengan 
nama  yang  mirip  di  berbagai  marketplace  akan  berdampak 
pada analisis data marketplace tersebut. 

Berdasarkan permasalahan tersebut, penelitian ini mengkaji 
pengembangan  mekanisme  untuk  identifikasi  duplikasi  akun 
toko  dari  berbagai  marketplace  yang  berbeda.  Penelitian  ini 
menggunakan  empat  variabel  yang  dapat  mengidentifikasi 
kesamaan  akun  toko  yaitu  nama  toko,  etalase  toko,  deskripsi 
toko  dan  nama  produk.  Hasil  dari  penelitian  ini  adalah 
didapatkan mekanisme untuk identifikasi duplikasi akun toko 
dan mekanisme untuk membentuk daftar toko online yang unik 
berdasarkan kesamaan kepemilikan akun toko sehingga dapat 
terbentuk repositori marketplace untuk penyusunan statistik e-
commerce.  

II.  TUJUAN PENELITIAN 

Tujuan dilakukannya penelitian ini adalah sebagai berikut. 
1.  Merancang mekanisme untuk identifikasi akun toko yang 

sama dari berbagai marketplace yang berbeda. 

2.  Merancang  mekanisme  untuk  menghasilkan  daftar  akun 
toko yang unik berdasarkan kesamaan kepemilikan akun 
toko. 

III. PENELITIAN TERKAIT 

Penelitian  terdahulu  yang  menjadi  rujukan  penelitian  ini 
yaitu  penelitian  [5]  yang  melakukan  analisis  pada  data 
marketplace.  Penelitian  tersebut  mengidentifikasi  kesamaan 
akun  toko  berdasarkan kesamaan  nama  toko  dan  lokasi  toko. 
Selain itu, penelitian tersebut juga mengasumsikan kesamaan 
toko berdasarkan jenis barang yang dijual. Apabila jenis barang 
yang  dijual  memiliki  kesamaan  minimal  50%  maka  toko 
tersebut diidentifikasi merupakan toko yang sama.  

Berangkat dari hasil penelitian [5] tersebut, maka penelitian 
ini mengkaji mekanisme untuk identifikasi duplikasi akun toko 
berdasarkan  kesamaan  akun 
toko  dengan  menggunakan 
beberapa variabel yang menjadi penciri kesamaan akun toko. 
Algoritma yang digunakan pada penelitian ini adalah algoritma 
kesamaan kata untuk menghitung kesamaan dari variabel yang 
diuji. Beberapa penelitian terkait algoritma kesamaan kata yang 
digunakan pada penelitian ini dapat dijabarkan sebagai berikut. 
a. 

Levenshtein Distance 
Levenshtein  Distance  merupakan  algoritma  yang 
sederhana  untuk  mengukur  kesamaan  antara  dua  string. 
Algoritma  ini  menghitung  kesamaan  kata  berdasarkan 
banyaknya operasi yang dilakukan untuk mengubah suatu 
string  menjadi  string  yang  lain,  mencakup  pengubahan, 
penghapusan,  ataupun  penambahan  suatu  karakter  [6].  
Beberapa  penelitian  yang  menggunakan  Levenshtein 
Distance yaitu. 
1.  Penelitian  [6]  menguji  kemiripan  dokumen  teks 
menggunakan algoritma Levenshtein Distance untuk 
membantu menentukan plagiarisme. Hasil pengujian 
menunjukkan  nilai  similaritas  yang  tinggi  yaitu 
diatas  77%  sampai  100%  untuk  dokumen  yang 
tinggi.  Sedangkan  untuk 
tingkat  kemiripannya 
dokumen  dengan  tingkat  kemiripan  yang  rendah 
maka  nilai  similaritas  nya  dibawah  40%.  Hasil 
penelitian  juga  menunjukan  bahwa  data  dengan 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

preprocessing  membuat  nilai  similaritas  menjadi 
lebih  baik, 
filtering  stopword  dan 
penggunaan  stemming,  namun  juga  membutuhkan 
waktu yang lebih lama untuk proses pengujiannya. 

terutama 

2.  Penelitian 

kinerja 

[7]  mengukur 

algoritma 
Levenshtein  Distance  dalam  mendeteksi  kemiripan 
antar dokumen teks dengan melihat perbedaan dari 
penggunaan  stopword  removal,  stemming  dan 
sorting  pada  preprocessing  datanya.  Hasil  terbaik 
didapatkan dari data yang melalui proses  stopword 
removal, stemming dan sorting sekaligus.  

3.  Penelitian [8] membandingkan metode perhitungan 
kemiripan  kata  dengan  algoritma  Levenshtein 
Distance  dan 
Jaro-Winkler.  Hasil  pengujian 
menunjukkan  algoritma  Levenshtein  Distance 
memiliki  nilai  akurasi  yang  lebih  tinggi  daripada 
Jaro-Winkler.  Namun,  kedua  algoritma  ini  hanya 
dapat  menemukan  kemiripan  dari  kata-kata  mirip 
yang memiliki sedikit kesalahan penulisan dan tidak 
dapat  menangani  kata-kata  yang  mirip  dari  sisi 
makna kata. 

tersebut, 

penelitian 

Berdasarkan 

peneliti 
menggunakan  algoritma  Levenshtein  Distance  untuk 
mengukur  kesamaan  akun  toko  berdasarkan  nama  toko. 
Hal  ini  dikarenakan  nama  toko  cenderung  singkat 
sehingga  cocok  untuk  dihitung  kesamaannya  dengan 
Levenshtein  Distance.  Pada  pengujian  kesamaan  nama 
toko ini hanya dilakukan pengubahan kata menjadi huruf 
kecil  dan  tidak  dilakukan  preprocessing  data  lainnya 
seperti stopword removal dan stemming. 

b.  Cosine Similarity 

Cosine  Similarity  merupakan  salah  satu  algoritma  yang 
populer  untuk mengukur  kesamaan  teks.  Kelebihan dari 
algoritma  ini  adalah  tidak  terpengaruh  pada  panjang 
pendeknya  suatu dokumen dan memiliki tingkat akurasi 
yang tinggi [9]. Beberapa penelitian yang menggunakan 
Cosine Similarity yaitu. 
1.  Penelitian  [9]  mendeteksi  kemiripan  teks  skripsi 
menggunakan  algoritma  Cosine  Similarity  dan 
pembobotan TF-IDF. Hasil penelitian menunjukkan 
bahwa  pengujian  dengan  stemming  menghasilkan 
nilai kemiripan rata-rata  lebih tinggi 10% daripada 
tanpa  stemming.  Penelitian  ini  menghasilkan  nilai 
similaritas diatas 50% untuk dokumen yang tingkat 
kemiripannya  tinggi,  sedangkan  untuk  dokumen 
tidak 
dengan 
berplagiat  menghasilkan  nilai  similaritas  dibawah 
40%.  
2.  Penelitian 

tingkat  kemiripan 

rendah  atau 

[10]  melakukan 

pengklasifikasian 
komentar  secara  otomatis  dengan  menggunakan 
metode  Cosine  Similarity.  Metode  preprocessing 
yang  dilakukan  yaitu  case  folding,  tokenisasi, 
pengubahan  slang  word,  stopword  removal  dan 
stemming. Pengujian dilakukan dengan k-fold cross 
validation  dengan  k=10  dan  didapatkan  akurasi 
model klasifikasi sebesar 80.87%.  

3.  Penelitian  [11]  menggunakan  skema  pembobotan 
Indexing  yang  dikomparasi 

kata  Class-Based 

 2 / 8 

 
 
dilakukan 

dengan  skema  pembobotan  TFIDF  dan  TFICF. 
Selanjutnya 
penghitungan  Cosine 
Similarity  dan  dilakukan  pengklasifikasian  teks 
dengan  metode  K-Nearest  Neighbor.  Metode 
preprocessing yang dilakukan yaitu tokenisasi, case 
folding,  Nchars  filter,  number  filter,  punctuation 
filter, stopword removal dan membandingkan antara 
stemming  dan  tanpa  stemming.  Hasil  penelitian 
tersebut ditemukan bahwa akurasi optimal dilakukan 
stemming  dan  dengan 
dengan  preprocessing 
pengukuran  Cosine  Similarity  berbasis 
term 
weighting TFIDF.  

Berdasarkan  penelitian  tersebut,  algoritma  Cosine 
Similarity  digunakan  untuk  mengidentifikasi  kesamaan 
akun  toko  berdasarkan  variabel  etalase  toko,  deskripsi 
toko dan nama produk. Variabel tersebut melalui tahapan 
preprocessing  untuk  meningkatkan  nilai  kesamaannya. 
Penelitian 
kesamaan 
berdasarkan  data  yang  melalui  tahapan  stemming  dan 
tanpa  stemming  untuk  melihat  pengaruh  dari  proses 
stemming terhadap skor kesamaan akun toko. 

ini  membandingkan 

hasil 

Untuk penghitungan kesamaan akun toko dari keseluruhan 
variabel  maka  dilakukan  penyusunan  model  dengan  regresi 
logistik  biner.  Regresi  logistik  biner  adalah  metode  analisis 
data yang digunakan untuk mencari hubungan antara variabel 
respon Y yang bersifat biner dengan variabel prediktor X yang 
berskala  kategori  atau  interval  [12].  Penelitian  terkait  model 
regresi logistik biner dapat dijabarkan sebagai berikut. 
1.  Penelitian [12] menggunakan model regresi logistik biner 
dan  uji  proporsi  dua  populasi.  Variabel  yang digunakan 
pada  penelitian  ini  yaitu  5  variabel  independen  dan  1 
variabel dependen yaitu status usia muda (tidak berstatus 
NEET dan berstatus NEET). 

2.  Penelitian  [13]  menggunakan  metode  regresi  logistik 
biner untuk mengetahui faktor-faktor yang memengaruhi 
keputusan  penduduk  lansia untuk bekerja.  Penelitian  ini 
menggunakan  8  variabel  independen  dan  1  variabel 
dependen  yaitu  keputusan  lansia  dalam  memilih  untuk 
bekerja (bekerja atau tidak bekerja).  

Berdasarkan  penelitian  tersebut,  maka  penyusunan  model 
kesamaan akun toko dilakukan dengan model regresi logistik 
biner.  Hal  ini  dikarenakan  tujuan  dari  pembentukan  model 
adalah untuk memprediksi apakah suatu toko merupakan toko 
yang  sama  atau  toko  yang  berbeda,  dimana  variabel  tersebut 
terdiri  dari  dua  kategori  sehingga  cocok  untuk  diuji  dengan 
model regresi logistik biner.  

IV. METODE PENELITIAN  

A.  STUDI PUSTAKA 

Penelitian dilakukan dengan studi pustaka penelitian terkait 
algoritma  kesamaan  kata,  wawancara  ke  mahasiswa  Polstat 
STIS  yang  menjadi  tim  projek  PKL  Riset  Big  Data  Kajian 
Digital Ekonomi, dan diskusi dengan Subject Matter yaitu BPS 
Subdit Pengembangan Model Statistik. 

B.  SUMBER DATA 

Sumber  data  pada  penelitian  ini  merupakan  data  sekunder 
yang diambil dari hasil PKL Tahun Akademik 2019/2020 Prodi 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Diploma IV Riset Big Data Kajian Digital Ekonomi. Penelitian 
tersebut  mengumpulkan  data  marketplace  Tokopedia  dan 
Bukalapak  yang  ada  di  Provinsi  D.I.  Yogyakarta.  Data  yang 
digunakan  pada  penelitian  ini  adalah  daftar  toko  dan  daftar 
produk pada marketplace di bulan Februari 2020, serta daftar 
etalase  dan  deskripsi  toko  yang  dikumpulkan  secara  manual 
pada periode 25 Februari 2021 - 4 Maret 2021. 

C.  METODE ANALISIS  

Dalam  menyusun  mekanisme  untuk  identifikasi  duplikasi 
berdasarkan kesamaan akun toko antar marketplace dilakukan 
pengolahan  data  menggunakan  aplikasi  Jupyter  Notebook 
dengan bahasa pemrograman Python 3.8 dimana alur penelitian 
dapat dilihat pada Gambar 1. 

Gambar 1. Alur penelitian 

Pada  penelitian  ini  dilakukan  pembentukan  data  training 
terlebih  dahulu  untuk  mengidentifikasi  kesamaan  akun  toko. 
Data  training  ini  dibuat  dengan  menggunakan  algoritma 
kesamaan  kata  yaitu  algoritma  Levenshtein  Distance  untuk 
melihat  kesamaan  nama  toko.  Pada  tahap  ini,  peneliti  hanya 
mengambil daftar toko yang mempunyai perbedaan nama toko 
dengan nilai Levenshtein Distance yang dibatasi pada jarak 0 
(nol) sampai 1 (satu) dikarenakan keterbatasan penelitian untuk 
melakukan  anotasi  secara  manual.  Untuk  memastikan 
kebenaran  kesamaan  akun  toko  tersebut  maka  dilakukan 
pengecekan manual kesamaan akun toko antar marketplace dan 
mengumpulkan data etalase dan deskripsi toko dari daftar toko 
tersebut.  Setelah  didapatkan  data  etalase  dan  deskripsi  toko 
tersebut, kemudian dilakukan eliminasi akun toko yang berbeda 
dari data training tersebut.  

Tahapan selanjutnya yaitu menghitung kesamaan akun toko 
berdasarkan konten atau isi dari toko online tersebut. Variabel 
yang  digunakan  pada  kesamaan  konten  toko  ini  yaitu  etalase 
toko,  deskripsi  toko,  dan  nama  produk.  Pertama  dilakukan 
preprocessing data untuk tiap variabel tersebut. Preprocessing 
ini  mencakup  mengganti  seluruh  huruf  menjadi  huruf  kecil, 
menghapus  tanda  baca,  menghapus  non-ASCII  encoding, 
menghapus  spasi  yang  berlebih,  dan  membuang  stopwords. 

 3 / 8 

 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Penelitian  ini  dilakukan  dengan  membandingkan  dua  metode 
preprocessing  yaitu  dengan  stemming  dan  tanpa  stemming. 
Stemming adalah proses pengubahan kata berimbuhan menjadi 
kata dasar. Alur preprocessing data dapat dilihat pada Gambar 
2. 

dari 

akurasi 

Setelah didapatkan nilai Cosine Similarity, kemudian dilihat 
besarnya 
untuk 
mengidentifikasi  kesamaan  akun  toko  berdasarkan  variabel 
etalase,  deskripsi  dan  nama produk.  Tingkat  akurasi  dihitung 
dari banyaknya toko dengan nilai Cosine ≥ 0.5 dibagi dengan 
jumlah toko di data training tersebut.  

algoritma 

tersebut 

Langkah  selanjutnya  dilakukan  penyusunan  model  dari 
kesamaan  nama  toko  yang  merupakan  hasil  dari  algoritma 
Levenshtein Distance dan kesamaan konten toko yaitu etalase, 
deskripsi  dan  nama  produk  yang  merupakan  hasil  dari 
algoritma  Cosine  Similarity.  Penyusunan  model  dilakukan 
dengan  regresi  logistik  biner  dengan  menggunakan  aplikasi 
SPSS  versi  26.  Dalam  penyusunan  model  terdapat  empat 
variabel  independen  yaitu  nama  toko,  etalase  toko,  deskripsi 
toko dan nama produk, serta satu variabel dependen yaitu status 
kesamaan  akun  toko.    Pada  variabel  dependen  terdapat  dua 
kategori yaitu 0 jika merupakan toko yang berbeda dan 1 jika 
merupakan toko yang sama. 

D.  EVALUASI 

Evaluasi dari algoritma kesamaan akun toko menggunakan 
beberapa  pengukuran  yaitu  pengecekan  kesamaan  akun  toko 
secara manual dan persentase banyaknya toko yang memiliki 
nilai skor kesamaan akun toko ≥ 0.5 untuk kesamaan nama toko, 
etalase,  deskripsi  dan  nama  produk.  Adapun  evaluasi  dari 
pemodelan  yaitu  dengan  melihat  goodness  of  fit  dari  model 
yang dihasilkan dan total akurasi dari ketepatan klasifikasi oleh 
model yang terbentuk.  

V.  KERANGKA PIKIR 

Gambar 2. Alur preprocessing 

Selanjutnya dilakukan penghitungan nilai Cosine Similarity 
untuk  data  training  tersebut.  Untuk  menghitung  nilai  Cosine 
Similarity ini dilakukan dengan merubah teks menjadi vektor 
atau feature extraction dan selanjutnya dilakukan penghitungan 
nilai  Cosine  Similarity.  Alur  Cosine  Similarity  dapat  dilihat 
pada Gambar 3. 

Gambar 3. Alur Cosine Similarity 

Gambar 4. Kerangka pikir 

Penelitian  ini  dilakukan  dengan  mengidentifikasi  variabel 
yang  dapat  menjadi  penciri  untuk  kesamaan  akun  toko  antar 
marketplace  dengan  mengamati  halaman  toko.  Selanjutnya 

 4 / 8 

 
 
 
 
 
dilakukan  identifikasi  algoritma  pendukung  untuk  mengecek 
kesamaan akun toko yaitu algoritma Levenshtein Distance dan 
algoritma Cosine Similarity. Setelah didapatkan variabel yang 
berpengaruh untuk menjadi penciri kesamaan akun toko antar 
marketplace,  kemudian  dilakukan  penyusunan  model  untuk 
menghitung  kesamaan  akun  toko  antar  marketplace  yang 
berbeda.  Langkah  selanjutnya  adalah  melakukan  pengujian 
terhadap  model  yang  dibuat.  Setelah  model  diuji  kemudian 
dilakukan  pembentukan  repositori  marketplace  yang  berisi 
daftar  toko  yang  unik  sehingga  didapatkan  jumlah  toko 
sebenarnya di kedua marketplace tersebut. 

VI. HASIL DAN PEMBAHASAN 

Pada penelitian ini dilakukan penyusunan mekanisme untuk 
identifikasi  duplikasi  akun  toko  antar  marketplace  yang 
berbeda dengan melakukan pengujian menggunakan algoritma 
kesamaan  kata.  Variabel  yang  digunakan  untuk  pengujian 
kesamaan akun toko antar marketplace yaitu nama toko, etalase 
toko,  deskripsi  toko,  dan  nama  produk  yang  dijual.  Tahap 
pertama  dari  penelitian  ini  adalah  membentuk  data  training 
yang  merupakan  hasil  pengujian  kesamaan  akun 
toko 
berdasarkan  nama  toko.  Untuk  mengidentifikasi  kesamaan 
nama  toko  antar  marketplace  yang  berbeda,  digunakan 
algoritma  Levenshtein  Distance  dengan  nilai  distance 
maksimal 1.  

Hasil  penghitungan  Levenshtein  Distance  untuk  kesamaan 
nama  toko  pada  marketplace  Bukalapak  dan  Tokopedia  di 
Sleman didapatkan sejumlah 959 toko yang mempunyai nama 
yang mirip di kedua marketplace tersebut. Untuk memastikan 
kebenaran dari kesamaan akun toko pada data training, maka 
dilakukan pengecekan manual serta pengumpulan data etalase 
dan deskripsi toko berdasarkan daftar toko hasil penghitungan 
Levenshtein Distance tersebut. Dari 959 toko, terdapat 202 toko 
yang dieliminasi dari daftar data training, dengan rincian 143 
akun  toko  (14.91%)  yang  berbeda  kepemilikan  dan  59  toko 
(6.15%)  yang  berstatus  tidak  aktif,  tutup  sementara  ataupun 
toko tidak ditemukan. Kemudian terdapat 228 toko (23.77%) 
tetapi  dapat  dipastikan 
yang 
kesamaan  kepemilikan  tokonya  dan  529  toko  (55.16%)  yang 
aktif  dan  mempunyai  barang.  Sehingga  pada  penelitian  ini, 
digunakan  dua  model  data  training  yaitu  data  training  model 
pertama yang memuat daftar toko yang aktif dan mempunyai 
barang dan daftar toko yang tidak mempunyai barang sebanyak 
757  toko  dan data  training model  kedua  yang hanya  memuat 
daftar  toko  yang  aktif  dan  mempunyai  barang  sebanyak  529 
toko. 

tidak  mempunyai  barang 

Langkah selanjutnya dilakukan pembersihan data atau tahap 
preprocessing data untuk data etalase, deskripsi toko dan nama 
produk.  Setelah  data  bersih  dilakukan  vektorisasi  teks  dan 
dihitung  kesamaan  akun  toko  dengan  algoritma  Cosine 
Similarity.  Berikut  adalah  hasil  kesamaan  akun  toko  dari 
variabel etalase, deskripsi dan nama produk. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 5. Persentase kemiripan akun toko berdasarkan etalase 

Gambar 6. Persentase kemiripan akun toko berdasarkan deskripsi 

Gambar 7. Persentase kemiripan akun toko berdasarkan nama produk 

Berdasarkan  hasil  penghitungan  Cosine  Similarity  tersebut 
diketahui  bahwa  data  dengan  preprocessing  stemming 
mempunyai  persentase  nilai  Cosine  ≥  0.5  yang  lebih  tinggi 
dibandingkan  data  tanpa  preprocessing  stemming.  Perbedaan 
yang  cukup  signifikan  terdapat  pada  variabel  deskripsi  toko 
yang dapat dilihat pada Gambar 6 dengan selisih sekitar 1-2%. 
Hal ini dikarenakan deskripsi toko cenderung lebih panjang dan 
mempunyai  banyak  kata  yang  berimbuhan,  sehingga  proses 
stemming  menjadikan  nilai  kesamaan  akun  toko  berdasarkan 
deskripsi toko menjadi lebih tinggi. 

 5 / 8 

 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Hasil  penghitungan  tersebut  juga  menunjukkan  bahwa 
persentase nilai kemiripan ≥ 0.5 lebih tinggi untuk data training 
toko  yang 
model  kedua  dimana  hanya  memuat  daftar 
mempunyai barang di etalasenya. Adapun nilai kemiripan yang 
paling  besar  diperoleh  oleh  variabel  nama  produk  yang 
ditunjukkan  pada  Gambar  7  sebesar  57-78%.  Hal 
ini 
dikarenakan  toko  cenderung  memberikan  penamaan  produk 
yang  sama  pada  berbagai  tokonya.  Kemudian  disusul  oleh 
variabel etalase toko yang ditunjukkan pada Gambar 5 sebesar 
30-36%  karena  rata-rata  toko  mendaftarkan  produknya  ke 
dalam etalase. Selanjutnya nilai kemiripan akun toko terkecil 
berdasarkan  deskripsi  toko  yang  ditunjukkan  pada  Gambar  6 
sebesar  24-26%.  Hal  tersebut  disebabkan  tidak  semua  toko 
mencantumkan  deskripsi  tokonya  dan  bisa  saja  memberikan 
deskripsi yang berbeda antar marketplace. 

Rata-rata  skor  Cosine  Similarity  untuk  variabel  etalase, 

deskripsi dan nama produk dapat dilihat pada Tabel I. 

TABEL I 

RATA-RATA SKOR COSINE SIMILARITY KESAMAAN AKUN TOKO ANTAR 
MARKETPLACE YANG BERBEDA 

Metode 
preprocessing 

Variabel 

Data training 
model 1 

Data training 
model 2 

Tanpa stemming 

Etalase toko 

28.96% 

34.01% 

Deskripsi 
toko 

Nama 
produk 

27.38% 

28.46% 

52.04% 

70.77% 

Etalase toko 

29.12% 

34.10% 

𝐵𝑜𝑏𝑜𝑡 𝑆𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦 = (1 −

𝑑𝑖𝑠𝑡
max(𝑆,𝑇)

) 

(1) 

Keterangan: 
dist 

max(S,T) 

:  nilai  Levenshtein  Distance  antara  string  1 
dan string 2 
: panjang string terbesar antara string 1 dan 
string 2 

Selanjutnya dilakukan penyusunan model untuk menghitung 
kesamaan  akun  toko  dari  keempat  variabel  yaitu  nama  toko, 
etalase toko, deskripsi toko dan nama  produk. Pemodelan ini 
dilakukan  untuk  melihat  bobot  dari  tiap  variabel  kesamaan 
akun toko dan memprediksi status kesamaan akun toko dengan 
menggunakan model regresi logistik biner. Pada penelitian ini, 
kategori  dari  peubah  respon  nya  adalah  toko  yang  berbeda 
dengan label 0 dan toko yang sama dengan label 1. 

Data yang dilatih dalam model ini berjumlah 900 toko yang 
terdiri dari data training model pertama yang dapat dipastikan 
kesamaan  tokonya  sebanyak  757  toko  dan  data  toko  yang 
berbeda  sebanyak  143  toko. Pemodelan  juga  dijalankan pada 
dua data yang berbeda, yaitu data dengan variabel yang melalui 
proses stemming dan data dengan variabel tanpa melalui proses 
stemming. Berikut adalah hasil uji parsial dalam regresi logistik 
biner. 

TABEL II  

HASIL PENGOLAHAN UJI PARSIAL REGRESI LOGISTIK BINER 

Tanpa Stemming 

Dengan stemming 

Deskripsi 
toko 

Nama 
produk 

28.13% 

29.25% 

Variabel 

B 

P-value 

Exp(B) 

52.76% 

71.69% 

Etalase 

3.083 

0.178 

21.815 

Berdasarkan tabel diatas, dapat dilihat bahwa secara rata-rata 
skor  Cosine  Similarity,  terdapat  perbedaan  yang  cukup 
signifikan untuk variabel produk dimana data training model 2 
memiliki rata-rata skor kemiripan yang lebih tinggi sekitar 18% 
daripada  data  training  model  1.  Sedangkan  variabel  etalase 
memiliki  perbedaan  rata-rata  skor  sekitar  4-5%  lebih  tinggi 
pada  data  training  model  2,  dan  variabel  deskripsi  hanya 
memiliki perbedaan rata-rata skor sekitar 1% lebih tinggi pada 
data  training  model  2.  Sementara  itu  walaupun rata-rata  skor 
Cosine  Similarity  lebih  tinggi  pada  data  dengan  stemming, 
namun  perbedaannya  tidak  jauh  berbeda  dengan  data  tanpa 
stemming.  

Setelah didapatkan nilai kesamaan untuk tiap variabel yaitu 
Levenshtein Distance untuk nama toko, dan Cosine Similarity 
untuk  etalase  toko,  deskripsi  toko,  dan  nama  produk,  maka 
terlebih  dahulu  dilakukan  transformasi  nilai  Levenshtein 
Distance  menjadi  bobot  similarity.  Bobot  similarity  dari 
Levenshtein Distance diasumsikan pada rentang 0 (nol) hingga 
1  (satu),  dimana  nilai  1  adalah  nilai  maksimum  yang 
menunjukkan  bahwa  dua  kata  adalah  sama  identik  [7]. 
Transformasi tersebut dilakukan dengan rumus sebagai berikut. 

Deskripsi 

1.373 

0.186 

3.946 

Produk 

6.475 

0.000 

648.590 

Nama toko 

57.613 

0.000 

1.050E+25 

Constans 

-52.562 

0.000 

0.000 

Stemming 

Variabel 

B 

P-Value 

Exp(B) 

Etalase 

2.959 

0.197 

19.278 

Deskripsi 

1.327 

0.197 

3.770 

Produk 

6.328 

0.000 

560.097 

Nama toko 

57.758 

0.000 

1.214E+25 

 6 / 8 

 
 
Constans 

-52.699 

0.000 

0.000 

Berdasarkan  Tabel  II,  diketahui  bahwa  data  dengan 
stemming maupun tanpa stemming mempunyai hasil yang sama. 
Variabel etalase dan deskripsi toko mempunyai p-value > 0.05 
yang artinya gagal tolak H0 atau dapat dikatakan bahwa dengan 
tingkat signifikansi lima persen, tidak terdapat pengaruh yang 
signifikan  dari  variabel  etalase  dan  deskripsi  toko  terhadap 
status  kesamaan  toko.  Sedangkan  variabel  nama  produk  dan 
nama  toko  mempunyai  p-value  <  0.05  yang  artinya  tolak  H0 
atau  dapat  dikatakan  bahwa  dengan  tingkat  signifikansi  lima 
persen, terdapat pengaruh yang signifikan dari variabel nama 
produk dan nama toko.  

Model  persamaan  regresi  logistik  biner  untuk  data  tanpa 
stemming  ditunjukkan  pada persamaan  (3)  dan  (4)  dan untuk 
data dengan stemming ditunjukkan pada persamaan (5) dan (6): 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Toko yang 
berbeda 

Toko 
yang 
sama 

Total 
Ketepatan 
Klasifikasi 

127 

16 

88.8 % 

18 

739 

97.6 % 

Kesamaan 
Akun Toko 

Toko yang 
berbeda 

Toko yang 
sama 

Total Akurasi 

96.2 % 

Stemming 

Kesamaan Akun Toko 

Toko yang 
berbeda 

Toko 
yang 
sama 

Persentase 
Total 
Ketepatan 
Klasifikasi 

127 

16 

88.8 % 

17 

740 

97.8 % 

𝐿𝑜𝑔𝑖𝑡 (𝑝(𝑥)) =   𝛽0 + 𝛽1𝑋1 + 𝛽2𝑋2 + ⋯ + 𝛽𝑛𝑋𝑛 

(2) 

𝐿𝑜𝑔𝑖𝑡 (𝑝(𝑥)) =   −52.562 + 3.083𝑋1 + 1.373𝑋2 +

                                   6.475𝑋3 + 57.613𝑋4  

Kesamaan 
Akun Toko 

Toko yang 
berbeda 

Toko yang 
sama 

(3) 

Total Akurasi 

96.3 % 

𝑝(𝑥) =

𝑒(−52.562+3.083𝑋1+1.373𝑋2+6.475𝑋3+57.613𝑋4)
(1 + 𝑒(−52.562+3.083𝑋1+1.373𝑋2+6.475𝑋3+57.613𝑋4))

(4) 
𝐿𝑜𝑔𝑖𝑡 (𝑝(𝑥)) =   −52.699 + 2.959𝑋1 + 1.327𝑋2 +
(5) 
       6.328𝑋3 + 57.758𝑋4   

𝑝(𝑥) =

𝑒(−52.699+2.959𝑋1+1.327𝑋2+6.328𝑋3+57.758𝑋4)
(1 + 𝑒(−52.699+2.959𝑋1+1.327𝑋2+6.328𝑋3+57.758𝑋4))

(6) 

Keterangan:  
p(x) 
X1 
X2 
X3 
X4 

:  Predicted Probability hasil pemodelan [0-1] 
:  Nilai Cosine Similarity etalase 
:  Nilai Cosine Similarity deskripsi 
:  Nilai Cosine Similarity nama produk 
 :  Bobot  similarity  dari  Levenshtein  Distance  nama 
toko 

Hasil  klasifikasi  dari  pemodelan  dengan  regresi  logistik 
biner  dapat  dilihat  pada  Tabel  III.  Klasifikasi  untuk  status 
kesamaan  akun  toko  ini  didasarkan  dari  nilai  predicted 
probability yang didapatkan dari pemodelan diatas. Jika p(x) ≥ 
0.5 maka prediksi klasifikasinya adalah 1 atau toko yang sama, 
sedangkan jika p(x) < 0.5 maka prediksi klasifikasinya adalah 
0 atau toko yang berbeda [14]. 

Berdasarkan  tabel  diatas,  diketahui  bahwa  baik  untuk  data 
tanpa  stemming  maupun  dengan  stemming  mempunyai  hasil 
prediksi yang sama untuk toko yang berbeda dengan persentase 
ketepatan klasifikasi sebesar 88.8%. Namun terdapat perbedaan 
prediksi  kesamaan  toko  dari  daftar  toko  yang  sama,  dimana 
pada data tanpa stemming sebanyak 18 toko yang seharusnya 
merupakan toko yang sama tetapi diprediksi menjadi toko yang 
berbeda,  sedangkan  pada  data  dengan  stemming  sebanyak 17 
toko  yang  seharusnya  merupakan  toko  yang  sama  tetapi 
diprediksi  menjadi  toko  yang  berbeda.  Sehingga  persentase 
ketepatan  klasifikasi  dari  prediksi  toko  yang  sama  pada  data 
tanpa stemming sebesar 97.6 % dan pada data dengan stemming 
sebesar 97.8%. Adapun total akurasi untuk data tanpa stemming 
sebesar 96.2% dan pada data dengan stemming sebesar 96.3%. 
Berdasarkan  hasil  tersebut  diketahui  bahwa  dengan  adanya 
proses stemming dapat meningkatkan nilai skor kesamaan akun 
toko, sehingga persentase akurasi dalam pengklasifikasian juga 
semakin  meningkat.  Walaupun  perbedaan  tersebut  tidak 
signifikan, 
dapat 
meningkatkan total akurasi dari model yang digunakan untuk 
memprediksi  kesamaan  akun  toko  antar  marketplace  yang 
berbeda Namun, jika dilihat dari segi waktu, data dengan proses 
stemming  membutuhkan  waktu  dua  kali 
lama 
dibandingkan data tanpa stemming.  

stemming 

dengan 

adanya 

namun 

lebih 

TABEL III 

HASIL KLASIFIKASI REGRESI LOGISTIK BINER 

Tanpa Stemming 

Kesamaan Akun Toko 

Persentase 

Adapun untuk tahap evaluasi dari model dilakukan dengan 
melihat goodness of fit dari pengujian Hosmer and Lemeshow 
Test, dimana H0: model telah cukup menjelaskan data dan H1: 
model tidak cukup menjelaskan data. Nilai goodness of fit dari 
Hosmer  and  Lemeshow  Test  pada  data  tanpa  stemming  dan 
stemming  masing-masing  sebesar  0.999  dan  0.992.  Dengan 
nilai p-value > 0.05 maka keputusan yang diambil adalah gagal 
tolak  H0,  artinya  model  yang  terbentuk  telah  cukup  untuk 
menjelaskan  data.  Hal  ini  menunjukkan  bahwa  persamaan 

 7 / 8 

 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
regresi  logistik  biner  dapat  digunakan  untuk  menjelaskan 
hubungan  variabel  penciri  kesamaan  akun  toko  pada  status 
kesamaan akun toko. 

Setelah  didapatkan  model  untuk  memprediksi  duplikasi 
berdasarkan  kesamaan  akun  toko  antar  marketplace  yang 
berbeda,  selanjutnya  dilakukan  pembentukan 
repositori 
marketplace.  Berikut  adalah  ringkasan  dari  pembentukan 
repositori marketplace di Kabupaten Sleman. 

TABEL IV  

HASIL PEMBENTUKAN REPOSITORI MARKETPLACE 

Marketplace 

Raw data 
(data asli) 

Tanpa Stemming 

Stemming 

Toko sama  Toko beda  Toko sama  Toko beda 

Bukalapak 

3682 

Tokopedia 

57411 

755 

2927 

56656 

756 

2926 

56655 

Jumlah 

61093 

60338 

60337 

jumlah 

Berdasarkan  Tabel  IV  dapat  diketahui 

toko 
sebenarnya  dari  kedua  marketplace  pada  Kabupaten  Sleman 
untuk data tanpa stemming dan dengan stemming berturut-turut 
sebanyak 60338 dan 60337 akun toko dari total data asli (raw 
data)  sebanyak  61093  akun  toko,  dengan  asumsi  bahwa 
kesamaan akun toko diidentifikasi berdasarkan kesamaan nama 
toko dengan perbedaan maksimal satu karakter dan kesamaan 
etalase, deskripsi dan nama produk yang telah melalui tahapan 
analisis.  Hasil  repositori  marketplace  ini  kemudian  bisa 
digunakan untuk penyusunan statistik e-commerce. 

VII. 

PENUTUP 

Berdasarkan  pengujian  yang  dilakukan,  terdapat  empat 
variabel yang bisa digunakan untuk mengidentifikasi duplikasi 
berdasarkan  kesamaan  akun  toko  antar  marketplace  yang 
berbeda yaitu nama toko, etalase toko, deskripsi toko, dan nama 
produk.  Hasil  matching  berdasarkan  nama  toko,  didapatkan 
bahwa dari 959 toko yang nilai Levenshtein Distance maksimal 
1  terdapat  78.94%  toko  yang  dapat  dipastikan  kesamaan 
kepemilikannya.  Adapun  hasil  matching  berdasarkan  nama 
produk,  etalase,  dan  deskripsi  toko  dengan  Cosine  Similarity 
memiliki persentase nilai kemiripan ≥ 0.5 berturut-turut sebesar 
57-78%, 30-36%, dan 24-26%. Hasil pemodelan dengan regresi 
logistik  biner  diketahui  bahwa  variabel  yang  memiliki 
pengaruh terbesar terhadap status kesamaan akun toko berturut-
turut  adalah  nama  toko,  nama  produk,  etalase  toko  dan 
deskripsi  toko.  Adapun  prediksi  kesamaan  akun  toko dengan 
model  tersebut  memiliki  total  akurasi  untuk  data  dengan  dan 
tanpa  stemming  berturut-turut  sebesar  96.3%  dan  96.2%. 
Terlihat  bahwa  perbedaan  hasil  untuk  data  dengan  dan  tanpa 
stemming  tidak  jauh  berbeda,  namun  dengan  adanya  proses 
stemming pada preprocessing data dapat meningkatkan akurasi 
untuk  prediksi  kesamaan  akun  toko  walaupun  stemming 
membutuhkan  waktu  dua  kali  lipat  lebih  lama  dibandingkan 
tanpa  stemming.  Adapun  evaluasi  dari  model  dengan 
menggunakan  uji  Hosmer  and  Lemeshow  diketahui  bahwa 
model  telah  cukup  menjelaskan  data.  Berdasarkan  hasil 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

tersebut  maka  model  ini  bisa  digunakan  untuk  memprediksi 
kesamaan akun toko antar marketplace yang berbeda, sehingga 
bisa membentuk repositori marketplace yang berisi daftar toko 
yang  unik  sebagai  dasar  untuk  penyusunan  statistik  e-
commerce.  

DAFTAR PUSTAKA 

[1]   APJII,  “Survei  Internet  APJII  2019-2020  [Q2],”  [Online].  Available: 

https://www.apjii.or.id. [Diakses 11 2020]. 

[2]   D.  Tapscott,  The  Digital  Economy:  Promise  and  Peril  in  the  Age  of 

Networked Intelligence, New York: McGraw-Hill Inc, 1997.  

[3]   OECD,  OECD  Guide  to  Measuring  the  Information  Society  2011, 

OECD Publishing, 2011.  

[4]   Statista, “Top 10 e-commerce sites in Indonesia as of 4th quarter 2020, 
Available: 

by 
traffic,” 
https://www.statista.com/statistics/869700/indonesia-top-10-e-
commerce-sites. [Accessed 11 2020]. 

[Online]. 

monthly 

[5]   Politeknik Statistika STIS, “Pemanfaatan BIG DATA dalam Mengetahui 
Pertumbuhan Jumlah Akun Toko, Jumlah Barang Terjual dan Besaran 
Omzet Penjualan Marketplace,” Jakarta, 2020. 

[6]   N. H. Ariyani, S. and R. Ramadhan, “Aplikasi Pendeteksi Kemiripan Isi 
Teks  Dokumen  Menggunakan  Metode  Levenshtein  Distance,” 
semanTIK, vol. 2, no. 1, 2016.  

[7]   B. Pratama and S. Pamungkas, “Analisis Kinerja Algoritma Levenshtein 
Distance  dalam  Mendeteksi  Kemiripan  Dokumen  Teks,”  Jurnal 
LOG!K@, vol. 6, no. 2, 2016.  

[8]   R. Fauzan, J. Riadi and F. Sholihin, “Perbandingan Metode Perhitungan 
Kemiripan  Kata,”  in  SNRT  (Seminar  Nasional  Riset  Terapan), 
Banjarmasin, 2018.  

[9]   A.  Riyani,  M.  Z.  Naf'an  and  A.  Burhanuddin,  “Penerapan  Cosine 
Similarity  dan  Pembobotan  TF-IDF  untuk  Mendeteksi  Kemiripan 
Dokumen,” Jurnal Linguistik, vol. 2, no. 1, 2019.  

[10]  M. Habibi and Sumarsono, “Implementation of Cosine Similarity in an 
Automatic Classifier for Comments,” JISKa, vol. 3, no. 2, pp. 110-118, 
2018.  

[11]  S.  W.  Iriananda,  M.  A.  Muslim  and  H.  S.  Dachlan,  “Identifikasi 
Kemiripan  Teks  Menggunakan  Class  Indexing  Based  dan  Cosine 
Similarity  untuk  Klasifikasi  Dokumen  Pengaduan,”  MATICS  Jurnal 
Ilmu Komputer dan Teknologi Informasi, vol. 10, no. 2, 2018.  

[12]  D. N. Sari and I. Ahmad, “Analisis NEET pada Usia Muda di Indonesia 

tahun 2017,” Skripsi Politeknik Statistika STIS, Jakarta, 2019. 

[13]  M.  R.  Fachriyan  Nur  and  I.  S.  Oktora, “Determinan  Penduduk  Lansia 
dalam  Menentukan  Bekerja  di  Kawasan  Indonesia  Timur  Indonesia,” 
Skripsi Politeknik Statistika STIS, Jakarta, 2019. 

[14]  J. Harlan, Analisis Regresi Logistik, Depok: Gunadarma, 2018.  

 8 / 8 

 
 
 
 
 
 
"
221709624,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Analisis Konten dan Keterlibatan Pengguna (User 
Engagement) di Media Sosial Badan Pusat Statistik 

Diah Henisa (221709624, 4SD1) 
Dosen Pembimbing: Nori Wilantika, S.S.T., M.T.I 

Ringkasan—  Badan  Pusat  Statistik  (BPS)  sebagai  lembaga 
pemerintah  telah  memanfaatkan  media  sosial  sebagai  sarana 
sosialisasi  kegiatan  statistik.  Penelitian  ini  bertujuan  untuk 
menganalisis keterlibatan publik di ketiga akun media sosial BPS 
berdasarkan jumlah like, komentar, dan share, serta menganalisis 
pengaruh  topik  postingan,  tipe  postingan,  dan  waktu  posting 
terhadap like, komentar, dan share di media sosial BPS. Data yang 
digunakan berupa seluruh postingan di akun Facebook, Twitter, 
dan Instagram BPS. Data dikumpulkan menggunakan tools yang 
sudah  ada,  yaitu  facepager  (Facebook),  twint  (Twitter),  dan 
instaloader  (Instagram).  Preprocessing  data  teks  meliputi  case 
folding, cleaning, tokenizing, filtering, normalisasi, dan stemming. 
Variabel  topik  postingan  diperoleh  dari  hasil  pemodelan  topik 
menggunakan  metode  Latent  Dirichlet  Allocation  (LDA).  Uji 
banding  dilakukan  menggunakan  uji  Kruskal-Wallis  dan  uji 
Dunn  untuk  melihat  perbedaan  keterlibatan  pengguna  antara 
topik  postingan,  tipe  postingan,  dan  waktu  posting.  Hasil  dari 
penelitian  menunjukkan  bahwa 
Instagram  mendapatkan 
engagement paling tinggi dibanding Facebook dan Twitter. Selain 
itu,  topik  postingan,  tipe  postingan  dan  waktu  posting  (hari  dan 
jam) memiliki pengaruh signifikan terhadap user engagement. 

Kata  Kunci—  Media  sosial,  analisis  konten,  keterlibatan 

pengguna. 

I.  LATAR BELAKANG 

tentang  apa  yang  dilakukan  oleh 

Di  era  digital  saat  ini  keterbukaan  informasi  merupakan 
suatu  keharusan  yang  dilakukan  pemerintah.  Suatu  lembaga 
pemerintah  perlu  memberikan  informasi  kepada  publik  atau 
masyarakat 
lembaga 
pemerintah guna memperoleh dukungan dari masyarakat. Hal 
ini berkaitan dengan pengelolaan hubungan masyarakat dalam 
lembaga 
lembaga 
tersebut.  Hubungan  masyarakat  di 
pemerintah  atau  biasa  disebut  humas  pemerintah,  adalah  
lembaga  atau  praktisi  humas  pemerintah  yang  bertugas 
mengelola  dan  mengkomunikasikan  program  dan  kebijakan 
pemerintah  kepada  publik  secara  efektif  dan  efisien  melalui 
berbagai sarana kehumasan sehingga membangun kepercayaan 
publik  dalam  rangka  menjaga  citra  dan  reputasi  lembaga 
pemerintah [1]. 

Di tengah pesatnya perkembangan teknologi digital, humas 
pemerintah dituntut untuk dapat memanfaatkan beragam media 
komunikasi  yang  ada  dalam 
rangka  memaksimalkan 
penyebarluasan informasi kepada publik [2]. Salah satu media 
komunikasi  yang  berkembang  sangat  pesat  saat  ini  adalah 
media sosial. Berdasarkan Publikasi Statistik Telekomunikasi 
Tahun  2019,  pengguna  internet  paling  banyak  menggunakan 
internet  untuk  mengakses  media  sosial,  yaitu  sebesar  87,2 
persen dari total pengguna internet, dan meningkat dari tahun 
ke tahun [3]. Perkembangan penggunaan media sosial tersebut 

menunjukkan  bahwa  media  sosial  berpotensi  besar  sebagai 
media  komunikasi  dan  diseminasi  informasi  bagi  humas 
pemerintah.  Media  sosial  merupakan  media  komunikasi  dua 
arah  berbasis  internet  yang  memudahkan  pengguna  dalam 
berinteraksi,  berpartisipasi,  dan  berbagi  konten.  Media  sosial 
memberikan kemudahan untuk berpartisipasi dalam pertukaran 
informasi secara online. Media sosial berpotensi meningkatkan 
interaksi antara pemerintah dengan masyarakat. Oleh karena itu, 
lembaga  pemerintah  perlu  meningkatkan  pemanfaatan 
teknologi  media  sosial  sebagai  media  komunikasi  agar  dapat 
menjangkau  masyarakat  lebih  luas.  Hampir  seluruh  lembaga 
pemerintah  di  Indonesia  telah  menggunakan  media  sosial 
sebagai  media  komunikasi.  Namun,  penggunaannya  masih 
belum optimal [4]. 

sosial 

sebagai 

Badan  Pusat  Statistik  (BPS)  sebagai  lembaga  pemerintah 
telah  memanfaatkan  media 
sarana 
sosialisasi/publisitas data dan kegiatan statistik. Melalui media 
sosial,  diharapkan  data-data  yang  dihasilkan  BPS  mampu 
menjangkau  masyarakat  dengan  cepat,  mudah,  dan  murah 
dengan  bahasa  dan  tampilan  yang  lebih  sederhana  [5]. 
Pengelolaan media sosial BPS dilakukan oleh Biro Hubungan 
Masyarakat  dan  Hukum,  khususnya  Bagian  Hubungan 
Masyarakat  (humas  BPS).  Humas  BPS  bertanggung  jawab 
dalam pelaksanaan hubungan media massa, sosialisasi kegiatan 
statistik, dan pengelolaan opini publik [6]. Sosialisasi kegiatan 
BPS  dilakukan  melalui  berbagai  kegiatan 
termasuk 
penyebarluasan informasi di berbagai kanal media sosial BPS. 
Lembaga  pemerintah  perlu  mengoptimalkan  penggunaan 
media  sosial  sebagai  sarana  komunikasi  publik.  Optimalisasi 
penggunaan media sosial tidak  hanya sebatas pembuatan dan 
penyebaran  konten,  melainkan  harus  diikuti  dengan 
pemantauan dan proses evaluasi [7]. Penggunaan media sosial 
di lembaga pemerintah juga harus diiringi dengan peningkatan 
kemampuan  humas  pemerintah  dalam  mengelola  konten  dan 
Menyusun strategi kehumasan. Pengelolaan konten yang baik 
dapat mengoptimalkan penyebaran konten di media sosial [8]. 
Keberhasilan  suatu  konten  di  media  sosial  dapat  dilihat  dari 
keterlibatan  pengguna 
(user  engagement).  Keterlibatan 
pengguna  dapat  dilihat  dari  interaksi/respon  publik/pengguna 
terhadap suatu konten [9]. Respon publik di media sosial dapat 
berupa like, komentar, dan share . Masing-masing media sosial 
memiliki jenis respon publik yang berbeda-beda.  

Dalam  rangka  meningkatkan  keterlibatan  publik  di  media 
sosial,  pemerintah  perlu  memahami  perilaku  publik  di  media 
sosial dengan menggali preferensi publik terhadap konten dan 
media yang dihasilkan  [4]. Penelitian terdahulu menunjukkan 
bahwa jenis topik pada konten media sosial memiliki pengaruh 
pada  tingkat  keterlibatan  pengguna  [10].  Selain  itu,  tipe 

 1 / 8 

 
 
 
 
 
postingan  (teks,  foto,  video)  dan  waktu  posting  juga  dapat 
mempengaruhi  user  engagement.  Waktu  penayangan  konten 
berperan dalam meningkatkan efektivitas penyebaran konten di 
media  sosial  [7].  Perilaku  publik  antara  satu  platform  media 
sosial dan platform media sosial lainnya tentu berbeda karena 
adanya  perbedaan  karakteristik  antara  pengguna  di  masing-
masing platform. 

Sebagai 

lembaga  pemerintah  yang  bergerak  dibidang 
statistik dan dalam kegiatannya banyak melibatkan masyarakat, 
pemantauan dan proses evaluasi media sosial menjadi hal yang 
penting  bagi  BPS  untuk  memaksimalkan  penggunaan  tiap 
platform  media  sosial.  Jika  publik  tetap  terlibat  dalam  suatu 
media  sosial,  maka  pemerintah  dapat  terus  memberikan 
informasi  kepada  publik  tentang  kebijakan  dan  kegiatan 
lembaga. Dengan memahami perilaku publik terhadap konten 
dan media yang dihasilkan dapat  menjadi bahan evaluasi dan 
menjadi  umpan  balik  yang  berlaku  strategis  atau  sebagai 
masukan 
untuk  memaksimalkan 
penggunaan media sosial. Namun, dalam pemantauan aktivitas 
media sosial BPS, belum dilakukan analisis mendalam terhadap 
perilaku publik di masing-masing media sosial terhadap konten 
dan media yang diunggah.  

pemerintah 

kepada 

Berdasarkan penjelasan diatas, penelitian ini bertujuan untuk 
menganalisis keterlibatan publik terhadap postingan di masing-
masing media sosial BPS dan melihat pengaruh topik postingan, 
tipe  postingan,  dan  waktu  posting.  Penelitian  ini  diharapkan 
dapat memberi masukan kepada BPS, khususnya humas BPS, 
dalam  memantau  keberhasilan  konten  di  media  sosial  dan 
menjadi  bahan  evaluasi  dalam  penyusunan  strategi  bermedia 
sosial  serta  memaksimalkan  penggunaan  tiap  platform  media 
sosial.  

II.  TUJUAN PENELITIAN 

Berdasarkan 

latar  belakang  diatas,  maka 

tujuan  dari 

penelitian ini adalah sebagai berikut: 

1.  Menganalisis keterlibatan publik yang ditinjau melalui 
respon  yang  diberikan  terhadap  postingan  di  ketiga 
akun media sosial BPS. 

2.  Menganalisis  perbedaan  keterlibatan  pengguna  (user 
engagement)  antara  topik  postingan,  tipe  postingan, 
dan  waktu  posting  terhadap  postingan di  ketiga  akun 
media sosial BPS.  

III. PENELITIAN TERKAIT 

Penelitian  terkait  berisi  uraian  penelitian  terdahulu  yang 
dijadikan acuan dalam melakukan penelitian sehingga peneliti 
dapat  memperkaya  teori  yang  digunakan  dalam  mengkaji 
penelitian  yang  dilakukan.  Berikut  ini  beberapa  penelitian 
terkait  yang  dijadikan  sebagai  referensi  dalam  penelitian  ini, 
yaitu: 

Referensi  [11]  yang  berjudul  “Pengaruh  Konten  Post 
Instagram  terhadap  Online  Engagement:  Studi  Kasus  pada 
Lima  Merek  Pakaian  Wanita”.  Penelitian  ini  bertujuan  untuk 
melihat pengaruh waktu posting dan tipe post terhadap online 
engagement yang diukur berdasarkan jumlah like dan komentar. 
Peneliti melakukan uji banding untuk melihat perbedaan online 
engagement  antara  tipe  post    dan  waktu  posting.  Hasil  yang 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

diperoleh  menunjukkan  bahwa  terdapat  pengaruh  signifikan 
antara tipe post dan waktu posting terhadap online engagement. 
Referensi  [10]  yang  berjudul  “Analysis  of  content  topics, 
user  engagement  and  library  factors  in  public  library  social 
media  based  on  text  mining”.  Tujuan  utama  dari  penelitian 
tersebut  adalah  mengeskplorasi  topik-topik  yang  ada  di 
postingan suatu akun Facebook milik perpustakaan umum dan 
menguji hubungan antara jenis topik dan keterlibatan pengguna 
(user  engagement).  Jenis  topik  diperoleh  dengan  melakukan 
pemodelan  topik  Bi-term.  Penelitian  ini  menemukan  bahwa 
terdapat perbedaan user engagement (like, komentar, dan share) 
terhadap  jenis  topik.  Selain  itu,  penggunaan  media  pada 
postingan  memberikan  pengaruh  positif 
terhadap  user 
engagement. 

Referensi [12] yang berjudul “Managing Brand’s Popularity 
on Facebook: Post Time, Content, and Brand Communication 
Strategies”.  Penelitian  ini  bertujuan  untuk  melihat  pengaruh 
jenis postingan, waktu posting, dan komentar admin terhadap 
popularitas  suatu  brand  di  media  sosial.  Data  dianalisis 
menggunakan metode Seemingly Unrelated Regression (SUR), 
Kruskal-Wallis,  dan  Mann-Whitney.  Hasil  penelitian 
menunjukkan  bahwa  konten  berupa  video  dan  waktu  posting 
(hari) memiliki pengaruh signifikan dan meningkatkan jumlah 
like,  komentar,  share,  dan  view.  Selain  itu,  postingan  yang 
diunggah  di  hari  kerja  selama  jam  kerja  lebih  efektif  dalam 
meningkatkan popularitas brand.  

Referensi  [4]  yang  berjudul  “Social  Media  and  Local 
Government  in  Indonesia:  Adoption,  Use  and  Stakeholder 
Engagement”.  Penelitian  ini  bertujuan  untuk  memahami 
perilaku  publik  di  media  sosial  dengan  mengeksplorasi 
preferensi  publik  terhadap  konten  dan  media  postingan. 
Penelitian  ini  menggunakan  Stakeholder  Engagement  Index 
(SEI)  untuk  mempelajari  perilaku  publik  di  media  sosial. 
Berdasarkan  hasil  yang  diperoleh,  untuk  meningkatkan 
engagement, pemerintah harus mengunggah konten berupa foto 
dan  video  lebih  sering  dibanding  konten  dengan  tautan  atau 
teks saja. 

IV. METODE PENELITIAN  

A.  Metode Pengumpulan Data 

Penelitian ini menggunakan data berupa seluruh post yang 
diunggah oleh akun media sosial BPS. Data dikumpulkan dari 
tiga laman akun resmi media sosial BPS, yaitu akun Facebook 
BPS  (id:  Badan  Pusat  Statistik),  akun  Twitter  BPS  (id: 
bps_statistics),  dan  akun  Instagram  BPS  (id  :  bps_statistics). 
tanggal 
Periode  pengumpulan  data  berlangsung 
pembuatan  masing-masing  akun  resmi  media  sosial  BPS 
hingga Juni 2021. Data yang dikumpulkan di tiap media sosial 
meliputi  ukuran  keterlibatan  pengguna  (jumlah  likes,  jumlah 
komentar, jumlah shares) per post, isi postingan, tipe postingan, 
dan  waktu  posting.  Pengumpulan  data  Facebook  dilakukan 
menggunakan  aplikasi  web  crawler  bernama 
facepager. 
Pengumpulan  data  Twitter  dilakukan  menggunakan  library 
Python  yaitu  Twint.  Sedangkan  pengumpulan  data  Instagram 
dilakukan menggunakan library Python yaitu Instaloader. 

sejak 

B.  Variabel Penelitian 

 2 / 8 

 
 
Terdapat dua jenis variabel yang akan diteliti, yaitu variabel 
dependen dan  variabel  independen.  Variabel dependen  terdiri 
dari  jumlah  likes,  jumlah  komentar,  dan  jumlah  share  yang 
merupakan  representasi  dari  user  engagement.  Namun,  untuk 
Instagram hanya terdiri dari jumlah likes dan jumlah komentar. 
Sedangkan variabel independen dari penelitian ini  terdiri dari 
topik  postingan,  tipe  postingan,  dan  waktu  posting,  Waktu 
posting dibedakan menjadi hari dan jam. Peneliti membagi jam 
posting  menjadi  beberapa  empat  periode  berdasarkan 
Indonesia’s  Digital  Consumer  Behavior  Report  tahun  2016 
[13]. Deskripsi variabel dapat dilihat pada Tabel I. 

Variabel 
Jumlah likes 

Jumlah 
komentar 
Jumlah 
shares 
Topik 
postingan 

Tipe 
postingan 

TABEL I 
VARIABEL PENELITIAN 
Deskripsi 

Jumlah like yang diperoleh setiap 
post 
Jumlah komentar yang diperoleh 
setiap post 
Jumlah share yang diperoleh setiap 
post 
Post terdiri dari beberapa topik 
yang dikategorikan berdasarkan 
pemodelan topik 
Post terdiri dari beberapa tipe, 
seperti album, foto, video, dan 
lainnya 

Jenis Variabel 
Dependen 

Dependen 

Dependen 

Independen 

Independen 

Hari posting  Waktu unggah terdiri dari Senin, 

Independen 

Jam posting 

Selasa, Rabu, Kamis, Jumat, Sabtu, 
dan Minggu 
Jam unggah terbagi menjadi empat 
zona waktu, yaitu pukul 06:00-
09:59, 10:00-13:59, 14:00-17:59, 
18:00-05:59 

C.  Preprocessing Data 

Independen 

Preprocessing data dilakukan pada data teks yang meliputi 
isi  postingan  (Facebook),  isi  tweet  (Twitter),  dan  caption 
(Instagram). 
untuk 
mempersiapkan  teks  menjadi  data  yang  terstruktur  sehingga 
dapat  diproses  pada  tahapan  berikutnya  [14].  Preprocessing 
dilakukan melalui beberapa tahapan, yaitu: 

Preprocessing 

bertujuan 

data 

Case Folding: 
Pada  tahap  case  folding,  seluruh  huruf  dalam  dokumen 
diubah menjadi huruf kecil. Sehingga keseluruhan teks dalam 
dokumen dikonversi menjadi huruf kecil. 

Cleaning: 
Tahap  cleaning  bertujuan  untuk  menghilangkan  berbagai 
karakter  simbol  yaitu  tanda  baca  dan  angka,  menghilangkan 
komponen teks seperti username, hashtag, dan sebagainya. 

Tokenizing: 
Tokenizing  atau  tokenisasi  adalah  proses  memisahkan 
kalimat  pada  sebuah  dokumen  menjadi  ptotongan-potongan 
seperti  kata,  kata  kunci,  frasa,  dan  elemen  lain  yang  disebut 
oken. 

Filtering: 
Pada 

tahap 

filtering 

atau  penyaringan  dilakukan 
pengurangan  jumlah  kata  di  dalam  sebuah  dokumen  dengan 
menghapus  stopwords.  Stopwords  merupakan  kata-kata  yang 
tidak  informatif  namun  memiliki  frekuensi  kemunculan  yang 
tinggi  dalam  dokumen.  Kata-kata  tersebut  seperti  kata  ganti 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

orang, kata seruan dan kata lainnya yang tidak begitu memiliki 
arti dalam penentuan kelas topik suatu dokumen. 

Normalisasi:  
Pada tahapan ini dilakukan normalisasi kata seperti kata gaul, 
kata  singkatan,  dan  kata  yang  tidak  sesuai  dengan  aturan 
Kamus  Besar  Bahasa  Indonesia  (KBBI)  menjadi  kata  yang 
sesuai KBBI. 
Stemming: 
Stemming  adalah  proses  mengubah  kata  menjadi  kata 
dasarnya.  Stemming  dilakukan  dengan  menghilangkan  semua 
imbuhan  baik  itu  berupa  awalan  (prefiks),  akhiran  (sufiks), 
maupun kombinasi dari awalan dan akhiran (konfiks) yang ada 
pada  setiap  kata  dalam  data.  Stemming  berguna  untuk 
mengurangi  variasi  kata  yang  sebenarnya  berasal  dari  kata 
dasar yang sama. 

D.  Pemodelan Topik Postingan 

Salah  satu  variabel  yang  akan  dilihat  perbedaan  terhadap 
keterlibatan pengguna adalah topik postingan. Topik postingan 
tidak diperoleh langsung dari hasil pengumpulan data. Maka, 
perlu  dilakukan  pemodelan  topik  untuk  mendapatkan  topik-
topik apa saja yang terdapat pada konten di akun media sosial 
BPS.  Data  yang  digunakan  pada  tahapan  pemodelan  topik 
postingan  ini  berupa  data  teks  hasil  preprocessing.  Metode 
yang digunakan untuk mengekstrak topik dari kumpulan data 
adalah metode Latent Dirichlet Allocation (LDA). Sedangkan 
untuk  mengevaluasi  model  dan  selanjutnya  memilih  model 
terbaik,  digunakan  coherence  score.  LDA  merupakan  teknik 
yang umum digunakan untuk mengetahui topik yang dibahas 
dan berhasil digunakan di berbagai bidang [15]. Implementasi 
LDA  dilakukan  menggunakan  package  gensim  pada  python. 
Hasil  pemodelan  topik  kemudian  akan  digunakan  untuk 
mengkategorikan setiap postingan ke dalam topik yang sesuai 
berdasarkan topik dominan dari setiap postingan. 

E.  Analisis Data 

Analisis Deskriptif 
Analisis  deskriptif  adalah  metode  yang  digunakan  untuk 
menganalisis  data  dengan  cara  memberikan  deskripsi  atau 
gambaran  berdasarkan  data  yang  dikumpulkan.Analisis 
deskriptif  memberikan  statistik  deskriptif  tentang  profil  akun 
media sosial BPS, variabel dependen, dan variabel independen. 
Untuk  menganalisis  keterlibatan  publik  (engagement)  di 
ketiga  akun  media  sosial  BPS,  dilakukan  pengukuran 
Stakeholder Engagement Index (SEI). SEI merupakan metode 
alternatif  untuk  memahami  perilaku  publik  di  media  sosial 
[16],[4]. Perhitungan SEI dapat dilihat pada Tabel II. 

TABEL II 
RUMUS PERHITUNGAN ENGAGEMENT 

Konsep 

Simbol 

Rumus 

Keterangan 

Popularitas 
(Popularity) 

Komitmen 
(Commitment) 

P 

Total likes/total post  Rata-rata jumlah 

P* 

C 

(P/jumlah 
follower)x1000 

Total komentar/total 
post 

likes tiap 
postingan 
Popularitas yang 
ada di publik 

Rata-rata 
komentar tiap 
postingan 

 3 / 8 

 
 
 
 
  
  
Penyebaran 
(Virality) 

C* 

V 

V* 

(C/jumlah 
follower)x1000 

Total share/total 
post 

(V/jumlah 
follower)x1000 

Engagement 

E 

P*+C*+V* 

Komitmen 
komunikasi publik 

Rata-rata jumlah 
share tiap posting 

Penyebaran 
komunikasi publik 

Stakeholder 
engagement index 
(SEI) 

Uji Normalitas 
Uji  normalitas  dilakukan  untuk  mengetahui  apakah  data 
berdistribusi normal atau tidak. Uji normalitas dilakukan pada 
semua variabel independen yang meliputi: topik postingan, tipe 
postingan,  dan  waktu  posting.  Uji  normalitas  data  pada 
penelitian ini dilakukan dengan uji Kolmogrov-Sminov (K-S) 
dengan  menggunakan  R.  Data  dikatakan  berdistristribusi 
normal  jika  nilai  signifikansi    (p-value)  lebih  besar  dari  0,05 
pada taraf signifikansi α=0,05.  Hasil pengujian normalitas ini 
akan menentukan metode statistik yang digunakan selanjutnya. 
Jika  data  berdistribusi  tidak  normal  maka  metode  yang 
digunakan adalah statistik nonparametrik. 

Uji Homogenitas 
Uji  homogenitas  dilakukan  untuk  mengetahui  apakah  data 
bersifat homogen atau tidak. Uji homogenitas pada penelitian 
ini  dilakukan  dengan  menggunakan  uji  Levene.  Hasil 
perhitungan  dari  uji  ini  akan  menunjukkan  nilai  signifikansi 
dari kelompok data yang berbeda. Nilai signifikansi lebih besar 
dari  0,05  menunjukkan  bahwa  kelompok  data  berasal  dari 
populasi dengan varians yang sama (homogen). 

Uji Kruskal Wallis 
Uji  Kruskal-Wallis  adalah  salah  satu  teknik  statistik  non 
parametrik  yang  digunakan  untuk  menguji  hipotesis  awal 
bahwa  sampel  k  independen  berasal  dari  populasi  yang 
sama/identik. Uji Kruskall-Wallis juga biasa disebut dengan uji 
ANOVA satu arah non parametrik. Penelitian ini menggunakan 
uji  Kruskal  Wallis  karena  data  tidak  berdistribusi  normal, 
variabel independen berskala kategorik lebih dari dua kategori, 
dan  jumlah  data  per  kategori  tidak  sama.  Uji  Kruskal  Wallis 
dalam penelitian ini digunakan untuk menguji apakah terdapat 
perbedaan  yang  signifikan  antara  variabel  independen  (topik 
postingan, tipe postingan dan waktu posting) terhadap variabel 
dependen  (jumlah  like,  jumlah  komentar,  dan  jumlah  share). 
Berikut ini  merupakan hipotesis pada penelitian ini. 

Pada variabel topik postingan, hipotesis yang diuji adalah: 
H11:  Terdapat  perbedaan  like  antara  topik  postingan  yang 
diunggah oleh akun media sosial BPS 
H12: Terdapat perbedaan komentar antara topik postingan yang 
diunggah oleh akun media sosial BPS 
H13:  Terdapat  perbedaan  share  antara  topik  postingan  yang 
diunggah oleh akun media sosial BPS 

Pada variabel tipe postingan, hipotesis yang diuji adalah: 
H14: Terdapat perbedaan jumlah like antara tipe postingan yang 
diunggah oleh akun media sosial BPS 
H15:  Terdapat  perbedaan 
postingan yang diunggah oleh akun media sosial BPS 
H16:  Terdapat  perbedaan  share  antara  tipe  postingan  yang 
diunggah oleh akun media sosial BPS 

jumlah  komentar  antara 

tipe 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pada variabel waktu posting, hipotesis yang diuji adalah: 
H17: Terdapat perbedaan jumlah like antara waktu posting yang 
diunggah oleh akun media sosial BPS 
H18:  Terdapat  perbedaan  jumlah  komentar  antara  waktu 
posting yang diunggah oleh akun media sosial BPS 
H19:  Terdapat  jumlah  share  antara  waktu  posting  yang 
diunggah oleh akun media sosial BPS 

Uji Post Hoc 
Ketika uji Kruskal Wallis memberikan hasil tolak H0, yang 
berarti  terdapat  perbedaan  yang  signifikan  antara  variabel 
independen (topik postingan, tipe postingan dan waktu posting) 
terhadap variabel dependen (jumlah like, jumlah komentar, dan 
jumlah  share).  Namun  belum  diketahui  letak  perbedaannya. 
Maka dari itu, dilakukan uji post hoc untuk mengetahui di mana 
perbedaan tersebut berada. Uji post hoc yang digunakan dalam 
penelitian ini adalah Uji Dunn. Uji Dunn merupakan prosedur 
perbandingan  ganda  berpasangan  nonparametrik  yang  sesuai 
ketika uji Kruskal-Wallis ditolak [17]. 

F.  Analisis Dan Interpretasi Hasil 

Setelah  diperoleh  hasil  pengolahan  data  dari  pengujian 
hipotesis  menggunakan  uji  Kruskal  Wallis  dan  uji  post  hoc, 
maka  dilakukan  analisis  dan  interpretasi  dari  hasil  yang 
interpretasi  dilakukan  untuk 
diperoleh.  Analisis  dan 
menjelaskan perbedaaan keterlibatan pengguna di media sosial 
BPS  antara  beberapa  jenis  postingan,  dilihat  dari  topik 
postingan, tipe postingan, dan waktu posting. 

V.  KERANGKA PIKIR 

Kerangka  pikir  mengacu  pada  latar  belakang,  masalah, 
tujuan,  metode  penelitian,  dan  solusi  yang  dihasilkan  pada 
penelitian.  Penelitian  ini  dilatarbelakangi  oleh  pemanfaatan 
media  sosial  oleh  BPS  sebagai  media  sosialisasi/publisitias. 
Penggunaan  media  sosial  diharapkan  mampu  menjangkau 
masyarakat  lebih  luas.  Oleh  karena  itu,  Humas  BPS,  perlu 
pertimbangan  dalam  menyusun  strategi  media  sosial  agar 
memberikan  hasil  yang  optimal.  Perlu  dilakukan  evaluasi 
terhadap  konten  yang  sudah  ada  untuk  memahami  perilaku 
pengguna  di  media  sosial  dengan  menggali  preferensi  publik 
terhadap  konten  yang  ada  guna  meningkatkan  keterlibatan 
publik  terhadap  konten  di  media  sosial  BPS.  Kerangka  pikir 
penelitian ini dapat dilihat pada Gambar 1. 

VI. HASIL DAN PEMBAHASAN 

A.  Pengumpulan Data 

Pada tahap pengumpulan data, peneliti mengumpulkan data 
dari ketiga platform media sosial yaitu Facebook, Twitter, dan 
Instagram.  Peneliti  mengumpulkan  data  berupa  konten  atau 
postingan  yang  ada  di  akun  resmi  media  sosial  BPS  beserta 
ukuran  keterlibatan  publik  (user  engagement)  untuk  setiap 
postingan.  Periode  pengumpulan  data  berlangsung  sejak 
tanggal  pembuatan  masing-masing  akun  resmi  media  sosial 
BPS hingga Juni 2021. Jumlah postingan, likes, komentar, dan 
share  yang  dikumpulkan  dari  setiap  platform  media  sosial 
ditunjukkan pada Tabel III. 

 4 / 8 

 
 
  
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Berdasarkan hasil crawling data Instagram, diperoleh 1.410 
posts. Dari 1.410 posts BPS yang dikumpulkan, jumlah posts 
pada tahun 2017 adalah sebanyak 160 posts, pada tahun 2018 
sebanyak  322  posts,  tahun  2019  sebanyak  348  posts,  tahun 
2020 sebanyak 426 posts, dan tahun 2021 sebanyak 154 posts. 
Dapat dilihat bahwa terjadi peningkatan jumlah posts dari tahun 
2017 sampai tahun 2020. 

B.  Preprocessing Data 

Setelah  tahap  pengumpulan  data,  dilakukan  preprocessing 
data  terhadap  data  teks  yaitu  isi  postingan  (Facebook),  isi 
tweets  (Twitter),  dan  caption  (Instagram).  Pada  proses 
preprocessing  data,  terdapat  beberapa  tahapan,  yaitu  case 
folding,  cleaning, 
filtering,  normalisasi  dan 
stemming.  Hasil  dari  setiap  tahapan  preprocessing  teks  dapat 
dilihat pada Tabel IV. 

tokenizing, 

TABEL IV 
HASIL PROSES PREPROCESSING DATA TEKS  

Sebelum 
preprocessing  

Setelah 
case folding 

Setelah 
cleaning 

Setelah 
tokenizing 

Setelah 
filtering 

Setelah 
normalisasi 

Setelah 
stemming  

jan 

Impor  migas  Feb  2021  senilai  US$1,30  miliar,  turun 
15,95 persen dibandingkan Jan 2021, turun 25,37 persen 
dibandingkan Feb 2020. Sedangkan Impor nonmigas Feb 
2021  mencapai  US$11,96  miliar,  naik  1,54  persen 
dibandingkan Jan 2021, naik 22,03 persen dibandingkan 
Feb 2020. 
impor migas feb 2021 senilai us$1,30 miliar, turun 15,95 
persen  dibandingkan  jan  2021,  turun  25,37  persen 
dibandingkan feb 2020. sedangkan impor nonmigas feb 
2021  mencapai  us$11,96  miliar,  naik  1,54  persen 
dibandingkan jan 2021, naik 22,03 persen dibandingkan 
feb 2020. 
turun  persen 
impor  migas  feb  senilai  us  miliar 
dibandingkan 
turun  persen  dibandingkan  feb 
sedangkan impor nonmigas feb mencapai us miliar naik 
persen dibandingkan jan naik persen dibandingkan feb 
['impor',  'migas',  'feb',  'senilai',  'us',  'miliar',  'turun', 
'persen', 
'persen', 
'dibandingkan',  'feb',  'sedangkan',  'impor',  'nonmigas', 
'feb', 
'persen', 
'dibandingkan', 'jan', 'naik', 'persen', 'dibandingkan', 'feb'] 
['impor',  'migas',  'feb',  'senilai',  'us',  'miliar',  'turun', 
'persen', 
'persen', 
'feb', 
'dibandingkan', 
'mencapai',  'us',  'miliar',  'persen',  'dibandingkan',  'jan', 
'persen', 'dibandingkan', 'feb'] 
['impor', 'migas', 'februari', 'senilai', 'us', 'miliar', 'turun', 
'persen', 
'persen', 
'dibandingkan',  'februari',  'impor',  'nonmigas',  'februari', 
'mencapai', 
'dibandingkan', 
‘januari', 'persen', 'dibandingkan', 'februari'] 
['impor',  'migas',  'februari',  'nila',  'us',  'miliar',  'turun', 
'persen',  'banding',  ‘januari',  'turun',  'persen',  'banding', 
'februari',  'impor',  'nonmigas',  'februari',  'capai',  'us', 
'miliar',  'persen',  'banding',  ‘januari',  'persen',  'banding', 
'februari'] 

'turun', 
'nonmigas', 

'jan', 
'impor', 

'dibandingkan', 

'dibandingkan', 

'dibandingkan', 

'mencapai', 

‘januari', 

'persen', 

'miliar', 

'miliar', 

'turun', 

'turun', 

'naik', 

'feb', 

'jan', 

'us', 

'us', 

C.  Pemodelan Topik Postingan 

Pemodelan  topik  dilakukan  menggunakan  metode  Latent 
Dirichlet Allocation (LDA) dari paket gensim di python. Data 
hasil preprocessing dibuat menjadi kamus dan korpus sebagai 
input utama pada pemodelan topik LDA. Selain itu, input yang 
diperlukan pada pemodelan LDA adalah jumlah topik, passes, 
dan  iterations.  Dalam  menentukan  jumlah  passes,  dilakukan 
analisis  nilai  coherence  score  dengan  melakukan  beberapa 

 5 / 8 

Gambar 1. Kerangka Pikir Penelitian 

TABEL III 
RINGKASAN POSTING DAN INTERAKSI PENGGUNA DI MEDIA SOSIAL BADAN 
PUSAT STATISTIK (BPS) 
Likes 
495.286 

Comments 
20.421 

Platform 
Facebook 

Shares 
147.101 

Posts 

1.859 

Twitter 

Instagram 

6.628 

55.583 

7.882 

1.410 

1.352.823 

26.596 

42.094 

- 

Dari  hasil  pengumpulan  data  Facebook,  diperoleh  1.859 
posts.  Dari  total  postingan  yang  diperoleh,  jumlah  post  pada 
tahun 2014  adalah  sebanyak  2  post,  tahun 2015  sebanyak 39 
post, tahun 2016 sebanyak 92 post, tahun 2017 sebanyak 311 
post, pada tahun 2018 sebanyak 371 post, tahun 2019 sebanyak 
433  posts,  tahun  2020  sebanyak  457  post,  dan  tahun  2021 
sebanyak  154  post.  Dapat  dilihat  bahwa  terjadi  peningkatan 
jumlah post dari tahun 2015 sampai tahun 2020. 

Berdasarkan  hasil  crawling  data  Twitter,  diperoleh  6.628 
tweets.  Dari  6.628  tweets  BPS  yang  dikumpulkan,  jumlah 
tweets pada tahun 2015 adalah sebanyak 534 tweets, pada tahun 
2016  sebanyak  622  tweets,  tahun  2017  sebanyak  834  tweets, 
tahun 2018 sebanyak 1.319 tweets, tahun 2019 sebanyak 1.154 
tweets,  tahun  2020  sebanyak  1.464  tweets,  dan  tahun  2021 
sebanyak 701 tweets. Dapat dilihat bahwa terjadi peningkatan 
jumlah tweets dari tahun 2015 sampai tahun 2020. 

 
 
 
 
 
 
percobaan  terhadap  tiga  parameter  topik  (10,  15,  dan  20) 
dengan  nilai  passes  diuji  coba  mulai  dari  5  sampai  50. 
Diperoleh hasil coherence score cenderung stabil pada passes 
ke  20.  Setelah  diperoleh  nilai  passes,  dilakukan  beberapa 
simulasi untuk mendapatkan nilai coherence score yang terbaik 
dengan  mengkombinasikan  iterations    dan  jumlah  topik. 
Iterations  diuji  coba  mulai  dari  100  sampai  1000  (kelipatan 
100). Sedangkan jumlah topik diuji coba mulai dari 1 sampai 
30.  Kemudian  diambil  jumlah  topik  dengan  nilai  coherence 
score tertinggi. Dari simulasi tersebut diperoleh jumlah topik 
sama  dengan  tujuh  dengan  coherence  score  tertinggi,  yaitu 
sebesar  0,6167.  Untuk  menginterpretasikan  tema  setiap  topik 
yang  dihasilkan,  peneliti  membaca  dokumen  yang  paling 
banyak berkontribusi pada tiap topik dan menyimpulkan topik. 
Hasil pemodelan topik LDA dapat dilihat pada Tabel V.  

TABEL V 
HASIL PEMODELAN TOPIK LDA 

Topik 

Kata Teratas 

Tema 

T1 

T2 

T3 

T4 

T5 

T6 

T7 

bps, provinsi, ekonomi, indonesia, 
kepala, pulau, jawa, cuk, kabupaten, 
barat 
Banding, persen, us, ekspor, miliar, 
upah, indonesia, turun, capai, impor 
data, bps, statistik, indonesia, survei, 
hasil, bidang, informasi, deputi, 
kualitas 

bps, cuk, indonesia, kepala, stis, 
statistik, jakarta, suhariyanto, acara, 
pegawai 

persen, harga, indeks, triwulan, 
tingkat, tani, inflasi, rupiah, turun, 
naik 
Juta, kunjung, banding, persen, orang, 
turun, tumpang, indonesia, angkut, 
wisman 
sensus, duduk, indonesia, data, online, 
tugas, catat, sehat, partisipasi, 
pandemi 

Fakta data, Gerakan 
cinta data, Selayang 
pandang provinsi 
Perkembangan 
ekspor dan impor 
Informasi terkait 
hasil survei yang 
disampaikan deputi 
bidang dan kualitas 
data 
Kegiatan atau acara 
yang diadakan BPS, 
dihadiri oleh kepala 
dan pegawai BPS 
Rilis data BPS 
tentang inflasi, ntp, 
dan sebagainya 
Statistik pariwisata 
dan transportasi 

Kegiatan sensus 
penduduk 2020 

Setelah  diperoleh  tujuh  topik  hasil  pemodelan  topik, 
dilakukan  kategorisasi  terhadap  setiap  postingan.  Dari  1.859 
postingan Facebook BPS, ada sebanyak 151 postingan (8,1%) 
termasuk dalam topik 1, 127 postingan (6,8%) termasuk dalam 
topik  2,  477  postingan  (25,7%)  termasuk  dalam  topik  3, 410 
postingan  (22,1%)  termasuk  dalam  topik  4,  192  postingan 
(10,3%)  termasuk  dalam  topik  5,  52  postingan    (2,8%) 
termasuk dalam topik 6, dan 450 postingan (24,2%) termasuk 
ke dalam topik 7. Sedangkan dari 6.628 tweet BPS, diperoleh 
topik  1  sebanyak  666  (10%),  topik  2  sebanyak  828  (12,5%), 
topik 3 sebanyak 1575 (23,8%), topik 4 sebanyak 1002 (15,1%), 
topik  5  sebanyak  1061  (16%),  topik  6  sebanyak  441  (6,7%), 
dan  topik  7  sebanyak  1055  (15,9%).  Dari  1.410  postingan 
Instagram BPS, diperoleh topik1 sebanyak 111 (7,9%), topik 2 
sebanyak  108  (7,7%),  topik  3  sebanyak  338  (24%),  topik  4 
sebanyak 262 (18,6%), topik 5 sebanyak 160 (11,3%), topik 6 
sebanyak  58  (4,1%),  dan  topik  7  sebanyak  373  (26,5  %). 
Distribusi topik di ketiga akun media sosial BPS dapat dilihat 
pada Gambar 2. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 2. Diagram batang topik postingan per jenis media sosial 

D.  Analisis Data 

Analisis Deskriptif 
BPS memiliki akun resmi Facebook dan Twitter pada tahun 
2015  serta  akun  Instagram  pada  tahun  2017.  Sejak  awal 
pembuatan akun sampai bulan Juni 2021, akun resmi Facebook 
BPS  telah  mengunggah  sebanyak  1.859  post  dan  memiliki 
pengikut  sebanyak  128.113;  total  tweet  di  akun  twitter 
sebanyak  6.628  tweet  dan  telah  diikuti  oleh  45.588  akun; 
sedangkan  akun  instagram  BPS  telah  mengunggah  sebanyak 
1.410 post dan diikuti oleh 203.294 pengikut. 

Untuk melihat perbandingan keterlibatan pengguna di ketiga 
akun  media  sosial  BPS,  dilakukan  pengukuran  Stakeholder 
Engagement  Index  (SEI).  Hasil  pengukuran  SEI  di  ketiga 
media sosial dapat dilihat pada Tabel VI. Dapat dilihat bahwa 
Instagram  mendapat  engagement  paling  tinggi  dibanding 
Facebook  dan  Twitter.  Meskipun  total  postingan  di  Twitter 
lebih  banyak  dibanding  Facebook  dan  Instagram,  namun 
popularitas, komitmen,  dan viralitas  Facebook dan  Instagram 
lebih  tinggi  dibanding  Twitter.  Hal  ini  menunjukkan  jumlah 
postingan yang banyak belum tentu memberikan  engagement 
yang tinggi. 

TABEL VI 
HASIL PENGUKURAN ENGAGEMENT 

Konsep 

Simbol  Facebook  Twitter 

Instagram 

Popularitas 
(Popularity) 

Komitmen 
(Commitment) 

Penyebaran 
(Virality) 

Engagement 

P 

P* 

C 

C* 

V 

V* 

E 

266,43 

8,39 

959,45 

2,08 

10,98 

0,09 

79,13 

0,62 

2,78 

0,18 

1,19 

0,03 

6,35 

0,14 

0,35 

4,72 

18,86 

0,09 

0,00 

0,00 

4,81 

Uji Hipotesis 
Sebelum dilakukan uji hipotesis, perlu dilakukan pengujian 
asumsi normalitas dan homogenitas. Pengujian normalitas dan 
homogenitas  dilakukan  pada  tiap  variabel  independen  untuk 
ketiga media sosial. Hasil pengujian normalitas menggunakan 
Kolmogorov-Smirnov menghasilkan nilai p-value kurang dari 

 6 / 8 

 
 
 
 
 
 
  
  
  
 
tingkat  signifikansi  0,05  pada  variabel  topik  postingan,  tipe 
postingan,  waktu  posting  (hari  dan  jam)  baik  di  Facebook, 
Twitter, maupun Instagram. Sehingga asumsi normalitas tidak 
terpenuhi.  

Selanjutnya dilakukan uji Kruskal Wallis untuk mengetahui 
apakah  terdapat  perbedaan  signifikan  antara  setiap  variabel 
independen.  Nilai  signifikansi  (p-value)  kurang  dari  0,05 
menunjukkan  bahwa  terdapat  perbedaan  signifikan  antara 
setiap  variabel  independent.  Hasil  uji  Kruskal  Wallis  dengan 
menggunakan R dapat dilihat pada Tabel VII untuk Facebook, 
Tabel VIII untuk Twitter, dan Tabel IX untuk Instagram. 

TABEL VII 
HASIL UJI KRUSKAL-WALLIS (FACEBOOK) 

Variabel Independen 
Topik postingan 

Tipe postingan 

Waktu posting (hari) 

Waktu posting (jam) 

Variabel Dependen 
Like 
Komentar 
Share 
Like 
Komentar 
Share 
Like 
Komentar 
Share 
Like 
Komentar 
Share 

p-value 
0,0000 
0,0000 
0,0000 
0,0000 
0,0000 
0,0000 
0,0000 
0,0000 
0,0000 
0,0000 
0,0000 
0,0000 

TABEL VIII 
HASIL UJI KRUSKAL-WALLIS (TWITTER) 

Variabel Independen 
Topik postingan 

Tipe postingan 

Waktu posting (hari) 

Waktu posting (jam) 

Variabel Dependen 
Like 
Komentar 
Share 
Like 
Komentar 
Share 
Like 
Komentar 
Share 
Like 
Komentar 
Share 

p-value 
0,0000 
0,0000 
0,0000 
0,0000 
0,0000 
0,0000 
0,0000 
0,0000 
0,0000 
0,0000 
0,0000 
0,0000 

Hasil 
Diterima 
Diterima 
Diterima 
Diterima 
Diterima 
Diterima 
Diterima 
Diterima 
Diterima 
Diterima 
Diterima 
Diterima 

Hasil 
Diterima 
Diterima 
Diterima 
Diterima 
Diterima 
Diterima 
Diterima 
Diterima 
Diterima 
Diterima 
Diterima 
Diterima 

TABEL IX 
HASIL UJI KRUSKAL-WALLIS (INSTAGRAM) 

Variabel Independen 
Topik postingan 

Tipe postingan 

Waktu posting (hari) 

Variabel Dependen 
Like 
Komentar 
Like 
Komentar 
Like 
Komentar 

p-value  Hasil 
0,0000  Diterima 
0,0000  Diterima 
0,0000  Diterima 
0,0000  Diterima 
0,0000  Diterima 
0,0000  Diterima 

Hasil  uji  Kruskal  Wallis  menunjukkan  bahwa  nilai 
signifikansi  masing-masing  variabel  dependen  untuk  tiap 
variabel independen pada ketiga platform media sosial kurang 
dari 0,05. Sehingga dapat dikatakan bahwa terdapat perbedaan 
yang  signifikan  antara  jumlah  like,  komentar,  maupun  share 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

pada media sosial Facebook, Twitter, dan Instagram jika dilihat 
dari topik postingan, tipe postingan, dan waktu posting. 

Karena  pengujian  Kruskal  Wallis  menghasilkan  tolak  H0, 
maka  dilakukan  uji  post  hoc  menggunakan  uji  Dunn  untuk 
mengetahui  topik  postingan,  tipe  posting,  dan  waktu  posting 
yang  memiliki  perbedaan  paling  signifikan  terhadap  user 
engagement. Nilai signifikansi lebih kecil dari 0,05 pada taraf 
signifikansi  α=0,05  menunjukkan  bahwa  terdapat  perbedaan 
rata-rata signifikan. 

E.  Analisis Dan Interpretasi Hasil 

Topik Postingan: 
Berdasarkan  hasil  uji  post-hoc  menggunakan  uji  dunn, 
terdapat  perbedaan  like,  komentar,  dan  share  pada  beberapa 
topik postingan, baik di Facebook, Twitter, maupun Instagram. 
Konten  yang  paling  memiliki  perbedaan  signifikan  terhadap 
like,  komentar,  dan  share  dibanding  topik  postingan  lainnya 
adalah konten tentang kegiatan sensus penduduk. 

Tipe Postingan: 
Berdasarkan hasil pengolahan data menggunakan uji Dunn 
dapat  diketahui  bahwa  terdapat  perbedaan  rata-rata  like  di 
Facebook  antara  tipe  postingan  berupa  album  dan  video 
dibandingkan dengan tipe postingan foto dan lainnya. Namun 
tidak terdapat perbedaan like antara tipe postingan album dan 
video. Dari hasil yang diperoleh juga dapat dilihat bahwa tipe 
postingan video memiliki pengaruh paling signifikan terhadap 
like,  komentar,  dan  share    di  Facebook  dibanding  tipe 
postingan  berupa  album,  foto,  dan  lainnya.  Berbeda  dengan 
Facebook,  tipe  postingan  foto  memiliki  perbedaan  signifikan 
terhadap 
twitter 
dibandingkan tipe postingan lainnya. 

like,  komentar,  dan  share  di 

jumlah 

Sedangkan  pada  media  sosial  Instagram,  tidak  terdapat 
perbedaan  signifikan  antara  tipe  postingan  foto  dan  video 
terhadap  like  di  instagram.  Hal  ini  menunjukkan  bahwa  baik 
tipe  postingan  berupa  foto  maupun  video  memberikan 
pengaruh  yang  sama  terhadap  like  di  instagram.  Namun  tipe 
postingan  berupa  carousel  memiliki  perbedaan  signifikan 
terhadap like di instagram dibandingkan dengan tipe postingan 
foto  dan  video.  Sedangkan  tipe  postingan  video  memiliki 
perbedaan  signifikan 
instagram 
dibandingkan tipe postingan lainnya. 

terhadap  komentar  di 

Waktu Posting: 
Berdasarkan hasil pengolahan data menggunakan uji Dunn 
dapat  diketahui  bahwa  postingan  yang  diunggah  pada  hari 
Rabu,  Sabtu,  dan  Minggu  memiliki  perbedaan  signifikan 
terhadap  user  engagement  di  Facebook  dibandingkan  hari 
lainnya.  Sedangkan  Senin  dan  Selasa  merupakan  hari  yang 
kurang  memiliki  pengaruh  terhadap  user  engagement  di 
Facebook. Untuk jam posting, postingan yang diunggah pada 
jam  6  pagi  sampai  jam  2  siang  menunjukkan  perbedaan 
signifikan terhadap jumlah user engagement di Facebook. 

Sama seperti Facebook, waktu posting (hari) yang memiliki 
perbedaan  signifikan  terhadap  like  di  Twitter  adalah  Rabu, 
Sabtu  dan  Minggu.  Hari  yang  memiliki  perbedaan  signifikan 
terhadap  komentar  di  Twitter  adalah  hari  Sabtu dan  Minggu. 
Hari  yang  memiliki  perbedaan  signifikan  terhadap  share  di 
Twitter adalah hari Rabu dan Sabtu. Sehingga dapat diketahui 
bahwa  Sabtu merupakan hari yang memiliki pengaruh paling 

 7 / 8 

 
 
 
  
  
  
  
  
  
  
  
 
  
  
  
  
  
  
  
  
 
  
  
  
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

baik  variabel  dependen  yang  merepresentasikan  keterlibatan 
publik, maupun variabel independen yang dapat mempengaruhi 
keterlibatan publik. 

DAFTAR PUSTAKA 
[1]  Kemenpan-RB, “Peraturan Menteri Pendayagunaan Aparatur Negara Dan 
Reformasi Birokrasi Republik Indonesia Nomor 83 Tahun 2012 Tentang 
Pedoman Pemanfaatan Media Sosial Instansi Pemerintah Dengan,” 2012. 
[2]  C.  C.  Priyatna,  F.  A.  A.  Prastowo,  F.  Syuderajat,  and  A.  Sani, 
“Optimalisasi  teknologi  informasi  oleh  lembaga  pemerintah  dalam 
aktivitas komunikasi publik,” Jurnal Kajian Komunikasi, vol. 8, no. 1, pp. 
114–127, 2020. 

[3]  Badan Pusat Statistik, Statistik Telekomunikasi Indonesia. Jakarta: Badan 

Pusat Statistik, 2019. 

[4]  A.  D.  Santoso,  D.  K.  Rinjany,  and  O.  M.  Bafadhal,  “Social  Media  and 
Local  Government  in  Indonesia:  Adoption,  Use  and  Stakeholder 
Engagement,” Romanian Journal of Communication and Public Relations, 
vol. 22, no. 3, pp. 21–35, 2020, doi: 10.21018/RJCPR.2020.3.307. 

[5]  M.  Faris  and  N.  T.  Pratikto,  “Building  A  Statistics  Society  through 
Youtube,” 2020 Asia–Pacific Statistics Week A decade of action for the 
2030 Agenda: Statistics that leaves no one and nowhere behind, Bangkok, 
Thailand, 15-19 Juni 2020. 

[6]  Badan Pusat Statistik, “Peraturan Kepala Badan Pusat Statistik Nomor 7 
Tahun  2008  Tentang  Organisasi  dan  Tata  Kerja  Badan  Pusat  Statistik,” 
2008. 

[7]  Kominfo,  Memaksimalkan  Penggunaan  Media  Sosial  Dalam  Lembaga 
Pemerintah. Jakarta Pusat: Direktorat Jenderal Informasi dan Komunikasi 
Publik, Kementrian Komunikasi dan Informatika, 2018. 

[8]  H.  Setiawan  and  P.  Santoso,  “Model  Optimalisasi  Peluang  Pemanfaatan 
Media Jejaring Sosial dalam Implementasi E-Governance di Indonesia,” 
Seminar  Nasional  Informatika  2013  (semnasIF  2013),  UPN  ""Veteran"" 
Yogyakarta, 18 Mei 2013. 

[9]  M. Schreiner, T. Fischer, and R. Riedl, “Impact of content characteristics 
and emotion on behavioral engagement in social media: literature review 
and  research  agenda,”  Electronic  Commerce  Research,  2019,  doi: 
10.1007/s10660-019-09353-8. 

[10] S. Joo, K. Lu, and T. Lee, “Analysis of content topics, user engagement 
and library factors in public library social media based on text mining,” 
Online Inf. Rev., vol. 44, no. 1, pp. 258–277, 2020, doi: 10.1108/OIR-11-
2018-0345. 

[11] A.  Santoso,  “Pengaruh  Konten  Post  Instagram 

terhadap  Online 
Engagement:  Studi  Kasus  pada  Lima  Merek  Pakaian  Wanita,”  Jurnal 
Teknik ITS, vol. 6, no. 1, pp. 50–54, 2017. 

[12] T. Hanifawati, U. S. Ritonga, and E. E.  Puspitasari, “Managing Brands’ 
Popularity on Facebook: Post Time, Content, and Brand Communication 
Strategies,” Journal of Indonesian Economy and Business, vol. 34, no. 2, 
p. 185-207, 2019, doi: 10.22146/jieb.45755. 

[13] Daily  Social  id,  “Indonesia’s  Digital  Consumer  Behavior  Report 2016,” 
https://www.greenlabgroup.com/upload/file/indonesias-digital-

2016. 
consumer-behavior-report-2016.pdf. 

[14] I. Wahyudin, E. T. Tosida, and F. Andria, Teori dan Panduan Praktis Data 
Science dan Big Data. Bogor: Lembaga Penelitian dan Pengabdian pada 
Masyarakat Universitas Pakuan, 2019. 

[15] N. Trivedi, S. Singla, and S. Alaee, “Topic-Based Engagement Analysis: 
A Case Study,” Proceedings of Data Mining Techniques, Fall 2017, 2017 
(CS235), 8 halaman, doi: 10.1145/nnnnnnn.nnnnnnn. 

[16] A.  Haro-de-Rosario,  A.  Sáez-Martín,  and  M.  del  Carmen  Caba-Pérez, 
“Using social media to enhance citizen engagement with local government: 
Twitter or Facebook?,” New Media and Society, vol. 20, no. 1, pp. 29–49, 
2018, doi: 10.1177/1461444816645652. 

[17] A. Dinno, “Nonparametric pairwise multiple comparisons in independent 
groups using Dunn’s test,” Stata Journal, vol. 15, no. 1, pp. 292–300, 2015, 
doi: 10.1177/1536867x1501500117. 

signifikan  terhadap  like,  komentar,  dan  share  di  Twitter. 
Sedangkan  Senin  dan  Selasa  merupakan  hari  yang  kurang 
memiliki  pengaruh  terhadap  like,  komentar,  dan  share  di 
Twitter. Selain itu, tweets yang diunggah pada jam 6 pagi sampi 
jam  10  pagi  dan  jam  6  sore  sampai  jam  6  pagi  memberikan 
perbedaan yang signifikan terhadap user engagement di Twitter. 
Sedangkan di media sosial instagram, diperoleh hasil bahwa 
postingan yang diunggah pada hari Rabu dan Minggu memiliki 
perbedaan  signifikan  terhadap  user  engagement  di  Instagram 
dibandingkan  dengan  hari  Senin  dan  Selasa.  Sama  seperti 
Twitter, waktu posting yang memiliki pengaruh terhadap  like 
dan komentar di Instagram yaitu pada jam 6 sore sampai jam 6 
pagi. 

VII.  PENUTUP 

Berdasarkan hasil analisis data yang telah dilakukan, berikut 

beberapa kesimpulan yang dapat diambil dari penelitian ini: 

1.  Diantara ketiga akun media sosial yang dimiliki BPS, 
Instagram  memperoleh  engagement  paling  tinggi 
dibandingkan Facebook dan Twitter. 

2.  Topik  postingan  memiliki  perbedaan  signifikan 
terhadap  user  engagement.  Topik  postingan  yang 
paling memiliki pengaruh terhadap like, komentar, dan 
share, adalah topik postingan tentang kegiatan sensus 
penduduk. 

3.  Tipe  postingan  memiliki  perbedaan 

signifikan 
terhadap  user  engagement.  Tipe  postingan  yang 
berpengaruh  signifikan  terhadap  user  engagement 
adalah  video  (Facebook  dan  Instagram),  dan  foto 
(Twitter). 

4.  Waktu  post  (hari  dan 

jam)  memiliki  pengaruh 
signifikan  terhadap  user  engagement.  Hari  yang 
mempengaruhi  secara  signifikan  adalah  hari  Rabu, 
Sabtu,  dan  Minggu.  Sedangkan  waktu  posting  yang 
paling  mempengaruhi  user  engagement    yaitu  pada 
pukul 10:00-13:59 (Facebook), pukul 06:00-09:59 dan 
18:00-05:59 
(Twitter),  serta  pukul  18:00-05:59 
(Instagram). 

Dalam  rangka  memaksimalkan  penggunaan  media  sosial 
sebagai sarana komunikasi sehingga meningkatkan keterlibatan 
pengguna  terhadap  konten  yang  ada,  humas  BPS  perlu 
menyusun  strategi  bermedia  sosial.  Berdasarkan  hasil  yang 
diperoleh  pada  penelitian  ini,  humas  BPS  dapat  menerapkan 
strategi  pada  kegiatan  sosialisasi  SP2020  yang  memperoleh 
banyak  keterlibatan  pengguna  kepada  kegiatan  survei  dan 
sensus BPS lainnya. Humas BPS disarankan mendistribusikan 
topik-topik besar secara seimbang. Dalam mengunggah konten 
di  media  sosial,  humas  BPS  disarankan  untuk  menggunakan 
media  seperti  album,  video, 
lain-lain  untuk 
memperkaya  informasi  yang  terkandung  dalam  postingan 
tersebut.  Selain  itu,  humas  BPS  disarankan  mengunggah 
konten  pada  waktu  yang  tingkat  keterlibatan  penggunanya 
tinggi. Humas BPS dapat menggunakan aplikasi atau fitur post 
terjadwal  dalam  mendistribusikan  konten  agar  sesuai  jadwal 
dan tidak mengganggu waktu di luar jam kerja. 

link,  dan 

Penelitian  ini  dapat  dikembangkan  lagi  dengan  ruang 
lingkup yang lebih luas. Variabel penelitian juga bisa diperkaya 
maupun  diubah  sesuai  akun  media  sosial  yang  akan  diteliti, 

 8 / 8 

 
 
 
"
221709618,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Pengembangan Fitur Perjalanan Dinas Luar Negeri
pada Sistem Informasi Kerja Sama, Protokol dan
PMP (KSPM)

Devis Renaldo (221709618, 4SI1)

Dosen Pembimbing: Nucke Widowati Kusumo Projo, S.Si, M.Sc, Ph.D.

Ringkasan— Pengembangan fitur Perjalanan Dinas

luar
negeri pada Sistem Informasi Kerja Sama, Protokol, dan
Penyiapan Materi Pimpinan (PMP) (SI-KSPM) bertujuan untuk
mempermudah proses pengajuan ataupun pelaporan perjalanan
dinas luar negeri yang pada proses sebelumnya masih dilakukan
secara manual. Fitur ini dapat membantu seluruh proses
tahap pengajuan,
perjalanan dinas luar negeri mulai dari
pengambilaan keputusan dari
serta
negeri. Penelitian ini
pelaporan perjalanan dinas
menggunakan metode System Development Life Cycle (SDLC)
Waterfall
secara
dalam pengerjaannya
berurutan mulai dari tahap analisis hingga tahap pemeliharaan.

atasan, monitoring,

dilakukan

yang

luar

Kata Kunci— Sistem Informasi, Fitur, Perjalanan Dinas

I. LATAR BELAKANG

persidangan,

penyiapan materi

Badan Pusat Statistik (BPS) merupakan salah satu lembaga
penyedia dan pengolah data di Indonesia. BPS memiliki biro
yang bertugas melaksanakan penyelenggaraan kerja sama,
protokoler,
pimpinan,
hubungan masyarakat, hukum, dan organisasi yaitu biro
Hubungan Masyarakat dan Hukum. Dalam melaksanakan
tugasnya biro Humas dan Hukum BPS memiliki sistem
informasi Kerja Sama, Protokol, dan Penyiapan Materi
Pimpinan (PMP) (KSPM) untuk memudahkan biro Humas
dan Hukum dalam mengerjakan tugas. Namun, tidak semua
tugas yang ada pada biro Humas dan Hukum sudah tercakup
dalam Sistem Informasi KSPM, fungsi yang belum ada di
dalam sistem informasi KSPM adalah pengajuan dan
pelaporan perjalanan dinas luar negeri.

ini

Saat

oleh

pegawai

dilakukan

itu karena

sistem yang sudah ada tidak efisien untuk
pengajuan
digunakan oleh pengguna. Hal
perjalanan
dengan
dinas
menggunakan e-mail secara langsung ke biro Hubungan
Masyarakat dan Hukum. Pelaporan perjalanan yang telah
dilakukan pun masih dilaporkan dan di dokumentasikan
ini menyulitkan biro Hubungan
dengan cara manual. Hal
Masyarakat
dan
dalam mengajukan
melaporkan perjalanan dinas. Selain itu, pimpinan tidak bisa
memonitoring pegawai yang sudah atau sering ke luar negeri.

pegawai

beserta

Sistem informasi KSPM ini tidak hanya akan ditambahkan
fitur perjalanan dinas luar negeri saja, tetapi nantinya akan
dihubungkan dengan API database pegawai melalui Single
Sign On (SSO) SIMPEG BPS

II. TUJUAN PENELITIAN
1. Membuat coding untuk fitur perjalanan dinas luar negeri
untuk memudahkan pegawai dalam mengajukan perjalanan
dinas luar negeri kepada atasan yang bersangkutan.
2. Membuat coding untuk daftar berkas pengajuan perjalanan
dinas luar negeri yang telah diajukan oleh pegawai sehingga
atasan dapat memonitor serta membantu dalam mengambil
keputusan terhadap pengajuan perjalanan dinas luar negeri
yang dilakukan pegawai.
3. Membuat coding fitur
laporan perjalanan dinas yang
membantu pegawai serta biro Humas & hukum dalam
dokumentasi perjalanan dinas luar negeri

III. PENELITIAN TERKAIT

of Management
Penelitian

Penelitian-penelitian terdahulu yang menjadi

rujukan
dalam penelitian ini yaitu penelitian yang berjudul Model
Development
System of
Internship
ini menjelaskan metode
pembangunan sistem menggunakan metode Waterfall. Metode
pembangunan sistem Waterfall menggunakan 5 tahapan yaitu
analisis,
dan
pemeliharaan.

implementasi,

perancangan,

Information

coba,

[1].

uji

yang

adalah

penelitian

penelitian

Penelitian lainnya yang menjadi acuan peneliti dalam
berjudul
melakukan
Comparative study of Waterfall model with RAD model [2].
Penelitian ini membandingkan metode pembangunan sistem
menggunakan model Waterfall dengan model RAD. Oleh
lebih memilih menggunakan metode
karena itu, peneliti
pembangunan sistem model Waterfall karena metodenya yang
lebih sistematik dan cocok untuk diterapkan dalam penelitian
ini.

Untuk mengatasi masalah tersebut rancangan penelitian
yang akan dilakukan adalah dengan mengembangkan fitur
perjalanan dinas luar negeri pada sistem informasi yang telah
ada yaitu sistem informasi KSPM. Fitur ini memuat fungsi :
- Pengajuan perjalanan dinas luar negeri
- Daftar perjalanan dinas yang sudah pernah dilakukan
- Laporan hasil perjalanan dinas luar negeri.

1 / 6

No

Judul

1 Model

Development
of
Management
Information
System of
Internship

TABEL I
TABEL LITERATUR
Tertulis

Penulis,
Publikasi
W. Hardyanto,
A.
Purwinarko, I.
M. Sudana,
and E.
Supraptono,
International
Conference on
Science and
Education and
Technology
2018, vol.
247, 2018

Waterfall model
has the
primary function
to ensure a
project. The
Waterfall method
has several stages
in software
development,
which consists of
needs analysis,
design,
implementation,
testing, and
maintenance

2 Comparative

study of
Waterfall
model with
RAD model

Sunil D.
Mone,
International
Journal of
Modern
Trends in
Engineering
and Research,
vol. 2, Maret
2015

When systematic
development and
low risk(success
ratio) is expected
and user as well as
customer have
passions then one
can prefer
waterfall model
instead of RAD
model.

Komentar

Penelitian ini
merupakan acuan
peneliti dalam
menggunakan
metode Waterfall
dalam membangun
Sistem Informasi.
Metode ini
menggunakan 5
tahapan dalam
prosesnya yaitu :
analisis,
perancangan,
implementassi, uji
coba, dan
pemeliharaan
Penelitian ini
merupakan alasan
peneliti dalam
memilih metode
waterfall dibanding
metode-metode
lainnya, karena
metodenya yang
sistematik. Menurut
peneliti, metode ini
merupakan metode
yang cocok dengan
sistem informasi
yang akan
dikembangkan.

IV. METODE PENELITIAN

Metode pengembangan sistem yang akan digunakan adalah
menggunakan metode System Development Life Cycle (SDLC)
dengan model Waterfall. SDLC atau Systems Development
Life Cycle adalah proses pembuatan dan pengubahan sistem
serta model
untuk
mengembangkan sistem-sistem tersebut [3]. Model Waterfall
adalah salah satu model dalam pembangunan sistem di mana
proses pembangunannya mengalir secara bertahap dari satu
tahap ke tahap lainnya [4].

dan metodologi

digunakan

yang

Pengembangan sistem informasi KSPM menggunakan
bahasa pemrograman PHP dengan framework Laravel serta
manajemen basis data MySQL. Laravel adalah salah satu
framework untuk pengembangan aplikasi web dengan sintaks
yang ekspresif dan elegan dan memberikan solusi untuk
pengembangan dengan memfasilitasi tugas umum di sebagian
besar proyek web besar [5]. Adapun tahapan tahapan dari
pengembangan sistem informasi KSPM ini adalah:

1. Analisis Masalah dan Kebutuhan
Pada Tahapan ini, peneliti

telah melakukan identifikasi
masalah dengan melakukan wawancara ke subject matter
(Bagian Biro Humas dan Hukum BPS). Peneliti mendapatkan
beberapa kebutuhan sistem dalam mengembangkan sistem
KSPM ini. Adapun requirement yang dibutuhkan dapat dilihat
pada Gambar 1.

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Gambar 1. Identifikasi Masalah

Gambar

1 menunjukkan Fishbone Diagram yang
mengidentifikasikan masalah pada sistem sebelumnya baik
dari bagian metode, alat, dan manusia. Pada bagian metode,
masalah yang terjadi adalah pengajuan perjalanan dinas
sebelumnya masih menggunakan e-mail, sehingga pengecekan
masih dilakukan secara manual. Masalah lainnya adalah
laporan perjalanan dinas masih dibuat dengan harus melihat
satu per satu Riwayat pengajuan perjalanan dinas di email.
Sehingga pembuatan laporan memerlukan waktu yang lama
dan merepotkan.

untuk

Masalah yang terjadi pada bagian alat adalah terdapat
beberapa prosedur yang belum menggunakan alat yang
tertentu tidak
sehingga untuk prosedur
terkomputerisasi,
efisien
adalah masih
dijalankan. Contohnya
menggunakan berkas hardcopy dalam proses pengajuan
perjalanan dinas. Sehingga proses pendokumentasiannya
riskan dengan resiko tercecer atau hilang. Untuk itu perlu
dikembangkan sistem yang terkomputerisasi baik dari awal
proses pengajuan dan pendokumentasiannya supaya system
perjalanan dinas lebih baik dibandingkan dengan sebelumnya.

Dari Gambar 1 tersebut didapatkan User Requirement

untuk sistem informasi KSPM adalah :

1) Membuat fitur pengajuan perjalanan dinas luar negeri

pada Sistem Informasi KSPM.

2) Membuat fitur laporan perjalanan dinas luar negeri pada

sistem informasi KSPM.

3) Pada Sistem terdapat menu bagi atasan yang berisi
perjalanan dinas apa saja yang sudah pernah dilakukan oleh
pegawai BPS.

luar negeri pada

4) Dengan adanya fitur pengajuan dan laporan perjalanan
dinas
sistem, Bagian KSPM dapat
mengontrol anggaran yang digunakan pada perjalanan dinas
luar negeri.

2. Perancangan Sistem
Rancangan

akan

sistem yang

dikembangkan

akan
mengadopsi dari sistem manual yang telah ada agar sistem
yang telah dikembangkan nantinya tidak terlalu sulit untuk
digunakan
yang
khususnya
pengguna
menggunakan. Alur
dalam pengajuan
perjalanan dinas luar negeri yang telah digunakan sebelumnya
dapat dilihat pada Gambar 2.

sistem manual

pegawai

bagi

2 / 6

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Pada Data Flow Diagram (DFD) ini terdapat 4 proses yang
terjadi yaitu proses pengajuan perjalanan dinas luar negeri,
penyetujuan perjalanan dinas, perjalanan dinas ,dan laporan
perjalanan dinas. Pada proses pengajuan perjalanan dinas,
pegawai harus melengkapi formulir terkait dengan pengajuan
sistem akan mengirimkan
perjalanan dinas. Setelah itu,
formulir tersebut ke atasan yang mana pada sistem atasan
didefinisikan sebagai admin. Proses selanjutnya adalah atasan
dapat menyetujui ataupun menolak pengajuan yang dilakukan
pegawai BPS. Apabila pengajuan tersebut
telah disetujui
pegawai dapat melakukan perjalanan dinas ke negara
tujuannya. Selanjutnya, pegawai melaporkan perjalanan dinas
luar negerinya ke sistem, yang mana nantinya sistem akan
memberikan laporan tersebut ke admin baik untuk bagian biro
Humas dan Hukum khususnya subbagian kerjasama dan juga
untuk atasan.

b) Entity Relationship Diagram (ERD)
ERD (Entity Relationship Diagram) adalah model teknik
pendekatan yang menyatakan atau menggambarkan hubungan
suatu model. Didalam hubungan ini dinyatakan yang utama
dari ERD adalah menunjukan objek data (Entity) dan
hubungan (Relationship), yang ada pada Entity berikutnya.
Gambar 4 menunjukkan Diagram hubungan antara entitas
yang terjadi pada proses pengajuan perjalanan dinas luar
negeri. Terdapat beberapa entitas pada proses pengajuan yaitu
pegawai, kegiatan, pengajuan, direktorat, negara, dan satker.

Gambar 2. Alur Sistem Manual Pengajuan Perjalan Dinas

Batasan dari sistem yang dikembangkan ini hanya sampai
pada pengajuan dari pegawai BPS kepada Biro Humas &
yang
Hukum Sub Bagian Kerjasama
dibutuhkan untuk pengajuan perjalanan dinas luar negeri
adalah Form Aplikasi Visa, Paspor yang masih berlaku, Pas
foto, dan Surat Undangan.

saja. Formulir

a) Data Flow Diagram (DFD)
Dalam merancang sistem yang akan dikembangkan peneliti
menggunakan Data Flow Diagram (DFD) untuk melihat
bagaimana alur data dari sistem yang akan dikembangkan ini.
Data Flow Diagram (DFD) tersebut dapat dilihat pada Gambar
3.

Gambar 4. Entity Relationship Diagram

c) Use Case Diagram
Use Case merupakan permodelan untuk menggambarkan
kelakuan sistem yang akan dibuat. Use Case digunakan untuk
mengetahui fungsi apa saja yang ada didalam sebuah sistem
dan siapa saja yang berhak menggunakan fungsi-fungsi
tersebut [6].Use case diagram merupakan gambaran atau
representasi dari interaksi yang terjadi antara sistem dengan
pengguna. Dari use case diagram ini dapat dilihat apa saja
interaksi yang terjadi antara pengguna yang mana pada sistem

Gambar 3. Data Flow Diagram (DFD)

3 / 6

ini adalah pegawai, atasan, dan pegawai biro Humas dan
hukum bagian KSPM dengan sistem. Use Case Diagram pada
penelitian ini terdapat pada Gambar 5.

apakah terdapat masalah dengan fitur yang
dievaluasi
dikembangkan. Saat
ini belum dilakukan pengujian dan
evaluasi dikarenakan sistem yang sedang dikembangkan
belum selesai dikerjakan.

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

V. KERANGKA PIKIR

biro

kerja

yang mempunyai

Kerangka pikir penelitian ini dapat dilihat pada Gambar 6.
Badan Pusat Statistik (BPS) merupakan salah satu lembaga
penyedia dan pengolah data di Indonesia. BPS memiliki salah
tugas melaksanakan
satu
penyelenggaraan
persidangan,
sama,
penyiapan materi pimpinan, hubungan masyarakat, hukum,
dan organisasi yaitu biro Hubungan Masyarakat dan Hukum.
Dalam melaksanakan tugasnya biro Humas dan Hukum BPS
memiliki sistem informasi Kerja Sama, Protokol, dan PMP
(KSPM). salah satu permasalahan yang terjadi pada bagian
KSPM ini adalah proses pengajuan dan pelaporan perjalanan
dinas luar negeri yang masih manual.

protokoler,

Peneliti mengembangkan sistem informasi yang dapat
memudahkan pengguna dalam pengajuan serta pelaporan
perjalanan dinas luar negeri. Dalam mencapai tujuan tersebut
peneliti mengembangkan
dengan
menggunakan metode System Development Life Cycle (SDLC)
Waterfall yang mana dalam pengerjaannya sangat sistematis
dimulai dari tahap analisis hingga tahap pemeliharaan dan
evaluasi.

sistem informasi

Gambar 5. Use Case Diagram

dengan

Pegawai

pengguna

sistem informasi

dapat melakukan

Use case diagram pada Gambar 4 menunjukkan seluruh
yang
interaksi
dikembangkan.
pengajuan
perjalanan dinas, melihat perjalanan dinas yang pernah
laporan hasil perjalanan dinas.
dilakukan, serta membuat
Bagian KSPM dapat melihat perjalanan dinas yang pernah
dilakukan oleh pegawai dan menerima
laporan hasil
perjalanan dinas dari pegawai. Atasan dapat menerima atau
menolak pengajuan perjalanan dinas dari pegawai, melihat
perjalanan dinas yang pernah dilakukan pegawai, dan
menerima laporan hasil perjalanan dinas dari pegawai.

3. Implementasi
Pembangunan

sistem diimplementasikan

berdasarkan
sistem informasi yang sudah ada menggunakan PHP, dibantu
dengan Framework Laravel serta dalam manajemen basis data
menggunakan MySQL. Hal ini selanjutnya akan dipaparkan
pada bagian hasil dan pembahasan.

4. Uji Coba dan Evaluasi
Pengujian unit memiliki fokus untuk memverifikasi unit
terkecil dari sebuah desain perangkat lunak yaitu komponen
atau modul dari sebuah software [7]. Uji coba dilakukan untuk
melihat apakah fitur yang dikembangkan sudah berjalan
akan
sesuai dengan kebutuhan user yang selanjutnya

Gambar 6. Kerangka Pikir

VI. HASIL DAN PEMBAHASAN

1) Fitur Pengajuan Perjalanan Dinas Luar Negeri

Fitur ini memiliki fungsi yang menyediakan menu bagi
pegawai untuk mengajukan perjalanan dinas luar negeri
kepada atasan. Menu ini menyediakan form bagi pegawai
untuk mengajukan perjalanan dinas luar negeri. Setelah
dapat
pegawai

mengajukan

formulir,

atasan

4 / 6

mempertimbangkan
dapat
melakukan perjalanan dinas luar negeri. Contoh formulir
terlihat pada Gambar 7.

pegawai

tersebut

apakah

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

2) Fitur Monitoring Perjalanan Dinas Luar Negeri

Pada fitur ini atasan dapat melihat perjalanan dinas apa saja
yang pernah dilakukan oleh pegawai yang akan mengajukan
perjalanan dinas luar negeri
tersebut. Fungsi dari fitur ini
adalah sebagai bahan pertimbangan oleh atasan dalam
menyetujui pengajuan perjalanan dinas luar negeri yang
dilakukan oleh pegawai BPS. Pada fitur ini juga atasan dapat
melihat
laporan dari
perjalanan dinas luar negeri sebelumnya atau belum. Fitur ini
juga menyediakan daftar pegawai yang sedang melakukan
perjalanan dinas luar negeri. Code dapat dilihat pada Gambar
8. Sedangkan interface pada fitur ini belum berhasil dibuat.

apakah pegawai

telah membuat

Gambar 7. Formulir Pengajuan Perjalanan Dinas Luar Negeri

5 / 6

Gambar 8. Code Fitur Monitoring oleh atasan

3) Fitur Laporan Perjalanan Dinas Luar Negeri

Fitur ini merupakan fitur yang menyediakan laporan oleh
pegawai atas perjalanan dinas luar negeri yang telah dilakukan.
Laporan tersebut berupa bagaimana perjalanan dinas yang
dilakukan, lama waktu yang digunakan untuk perjalanan dinas
luar negeri, anggaran yang digunakan, dsb. Adapun Hasil
code dari fitur ini terdapat pada Gambar 9. Interface pada fitur
laporan perjalanan dinas luar negeri belum berhasil dibuat.

●

●

●

●

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Gambar 9. Code Fitur Laporan Perjalanan dinas Luar Negeri

VII.

PENUTUP

1) Kesimpulan

ini

fitur

sudah

pengajuan, monitoring

dapat mengidentifikasikan
Penelitian
kebutuhan Sistem Informasi Kerja Sama, Protokol, dan
PMP (SI-KSPM)
Fitur-fitur yang diperlukan dalam pengembangan Sistem
Informasi Kerja Sama, Protokol dan PMP (SI-KSPM)
yaitu
pelaporan
perjalanan dinas luar negeri yang dibutuhkan oleh biro
Humas dan Hukum sudah dapat diidentifikasikan dan
dirancang pembuatannya.
Code untuk semua
fitur pengajuan,
fitur, yaitu
monitoring, dan pelaporan perjalanan dinas sudah selesai
dibuat.
Fitur Pengajuan perjalanan dinas luar negeri pada Sistem
Informasi Kerja Sama, Protokol, dan PMP (SI-KSPM)
sudah dapat dijalankan.

dan

2) Saran

masih

Proses pengembangan system informasi KSPM yang
dilakukan
keterbatasan.
Pengembangan berikutnya adalah untuk menyelesaikan
interface
dalam
yang
pengembangan system informasi KSPM ini.

mengalami

fitur-fitur

dibuat

telah

International

I. M. Sudana,

Journal of Modern Trends

Sunil D. Mone, Comparative study of Waterfall model with
in

DAFTAR PUSTAKA
[1] W. Hardyanto, A. Purwinarko,
E.
Supraptono, Model Development of Management Information
System of Internship, International Conference on Science and
Education and Technology 2018, vol. 247, 2018
[2]
Rad model,
Engineering and Research, vol. 2, pp. 481484, Maret 2015
Patel, U. A., & Jain, N. K. (2013). New idea in waterfall
[3]
model for real-time software development, International Journal
of Engineering Research & Technology(IJERT), vol. 2, pp.
114119, April 2013.
P. Gajalakshmi, Software Development Liffecycle Model
[4]
(SDLC) Incorporated With Release Management, International
Research Journal of Engineering and Technology, vol. 03, 2016
[5] Vishal V. Parkar, Prashant P. Shinde, Sanket C. Gadade,
Prasad M. Shinde, Utilization of Laravel Framework for
Development of Web Based Recruitment Tool, IOSR Journal of
Computer Engineering, pp. 3641
[6] Al Gheffira, Zeivira Masri Inayah, Rizani Teguh, Della
Oktaviany, Sistem Informasi Manajemen Proyek Berbasis
Website Pada PT. AKM, Jurnal Teknik Informatika dan Sistem
Informasi, Vol. 6, No. 1, September 2019, Hal. 62-71
[7] Ardian Riftha Dhuha, Fajar Pradana, Bayu Priyambadha,
Pengembangan Sistem Aplikasi Manajemen Proyek Berbasis
Web (Studi Kasus: PT. Swadaya Graha), Jurnal Pengembangan
Informasi dan Ilmu Komputer, Vol. 1, No. 11,
Teknologi
November 2017, hlm. 1367-1375

6 / 6

"
221709613,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

ANALISIS SENTIMEN DAN TOPIK 
PEMBICARAAN PUBLIK TERHADAP 
PROGRAM PSBB DAN PPKM:  
STUDI KASUS PENGGUNA  TWITTER 

Deni Rahmat Ramdhani (221709613, 4SD1) 
Dosen Pembimbing: Nori Wilantika, S.S.T.,  M.T.I. 

Ringkasan—  Pemerintah  Indonesia  menerapkan  kebijakan 
untuk  menekan  angka  penyebaran  COVID-19,  yaitu  dengan 
kebijakan PPSB dan PPKM. Indikator  kesuksesan tidak  dapat 
sepenuhnya dilihat  dari angka statistik,  penilaian  tersebut  juga 
mempertimbangkan  masukan,  diantaranya  adalah  dengan 
melihat respon dan opini masyarakat terhadap program tersebut. 
Penelitian  ini  mengekstraksi  informasi  mengenai  Kebijakan 
PSBB dan PPKM pada media sosial Twitter dengan pendekatan 
analisis sentimen menggunakan metode Multinomial Naïve Bayes 
untuk  klasifikasi  persepsi  masyarakat  dan  Topic  Modelling 
menggunakan metode  Latent  Dirichlet  Allocation  (LDA)  untuk 
menentukan jumlah topik yang dibahas masyarakat beserta kata 
kuncinya.  Meskipun  tweet  pada  penelitian  ini  didominasi  oleh 
sentimen netral, namun diketahui bahwasannya sentimen negatif 
lebih  banyak daripada  sentimen  positif.  Model  klasifikasi  yang 
terbentuk  dari  Multinomial  Naïve  Bayes  adalah  sebesar  58,3% 
dan topik yang terbentuk adalah sebanyak 3 topik.  

Kata  Kunci—  COVID-19,  PPSB,  PPKM,  Sentimen,  Topic 

Modelling 

I.  LATAR BELAKANG 

Corona  Virus  Disease  2019  atau  disingkat  COVID-19 
merupakan wabah baru yang secara resmi dinyatakan sebagai 
pandemi oleh World Health Organization (WHO) pada tanggal 
11 Maret 2020 lalu. Hal ini disebabkan karena penyebarannya 
yang begitu  cepat dan luas hingga  ke wilayah jauh  dari pusat 
wabah, yaitu Kota Wuhan,  China, dan serta dampaknya yang 
bahaya  bagi  kesehatan,  seperti  pneumonia  yang  parah, 
gangguan  pernapasan,  hingga  kematian.  Sejak  awal  wabah 
COVID-19  ini  muncul  di  Kota  Wuhan,  China,  pada  bulan 
Desember  2019  lalu,  jumlah  korban  terus  bertambah  setiap 
harinya. Hingga  per  tanggal  17  Februari 2021,  telah  tercatat 
jumlah  kasus  sebanyak lebih  dari  109  juta  jiwa  dan  jumlah 
kematian sebanyak lebih 2,4 juta jiwa di seluruh dunia[1].  

Di  Indonesia sendiri, per tanggal  tersebut telah menduduki 
posisi 19 dunia dengan jumlah  kasus sebanyak lebih 1,2  juta 
jiwa  dan  jumlah  kematian  sebanyak  lebih  33  ribu  jiwa[1]. 
Kondisi  demikian  memberikan  dampak  kepada  harus 
dilakukannya  upaya  komprehensif  untuk  menekan  dan 
memutus  rantai  penularan  COVID-19.  Seluruh  masyarakat 
harus sadar dengan kondisi ini,  dengan menerapkan protokol 
kesehatan  yang  telah  ditetapkan  oleh  Komite  Penangan 
COVID-19  dan Pemulihan Ekonomi Nasional (KPC-PEN)[2], 
yaitu  dengan  menggunakan  masker,  menjaga  jarak  dengan 

orang  minimal  sejauh  satu  meter,  dan  mencuci  tangan 
menggunakan sabun atau hand sanitizer.  Dalam menyikapi hal 
tersebut,  Pemerintah  Indonesia  juga 
turut  aktif  dengan 
menerapkan  beberapa  kebijakan  untuk  menekan  angka 
penyebaran COVID-19.  

Dalam upaya menangani COVID-19  ini, langkah awal yang 
dilakukan Pemerintah  adalah menerapkan Pembatasan  Sosial 
Berskala Besar (PSBB)  sesuai Peraturan Pemerintah Nomor 21 
Tahun 2020 tentang Pembatasan Sosial Berskala Besar dalam 
Rangka  Percepatan Penanganan  Corona  Virus  Disease  2019 
(COVID-19)[3].  Dalam  pelaksanaannya, PSBB  yang menjadi 
tanggung  jawab  atas  pemerintah daerah ini  dilakukan secara 
ketat  dengan  melakukan pembatasan terhadap kegiatan  pada 
sektor yang krusial, seperti peliburan sekolah dan tempat kerja. 
Selain itu, PSBB  juga menghentikan kegiatan ibadah bersama 
di rumah ibadah, pembatasan kegiatan di tempat atau fasilitas 
umum, pembatasan kegiatan sosial budaya, hingga pembatasan 
moda transportasi. Hal  tersebut dianggap sebagai sektor yang 
memiliki peluang besar menaiknya angka kasus COVID-19.   

DKI  Jakarta yang merupakan salah satu wilayah episentrum 
menjadi  provinsi  pertama  dalam  menerapkan  PSBB,  yaitu 
dimulai  pada  tanggal  10  April  2020  hingga  24  April  2020. 
Menurut  data  dari  tim  riset  kawal.co.id[4],  pembaharuan 26 
April  2020,  perbandingan  rata-rata  pertumbuhan  kasus 
COVID-19  sebelum dan sesudah PSBB  di DKI  Jakarta pada 
periode 10-24 April 2020 terkonfirmasi adanya penurunan rata-
rata  penambahan  kasus  COVID-19  di  Indonesia  setelah 
diberlakukannya  PSBB  di  DKI  Jakarta.  Hal  ini  dibuktikan 
dengan adanya penurunan angka rata-rata pertumbuhan kasus 
nasional  sebanyak  3,98  persen  dari  angka 
rata-rata 
pertumbuhan sebelum penerapan PSBB, yaitu 14,60 persen[4]. 
Namun,  dengan  diberlakukannya  PSBB,  pertumbuhan 
ekonomi  Indonesia  mengalami  penurunan, karena  kebijakan 
PSBB  ini dinilai terlalu ketat dalam membatasi sektor ekonomi, 
seperti membatasi masyarakat untuk bekerja.  

Pada  awal  tahun  2021,  Pemerintah  menggunakan  istilah 
pemberlakuan  Pembatasan  Kegiatan  Masyarakat  (PPKM) 
sesuai Instruksi Menteri Dalam  Negeri  Nomor  1  Tahun 2021 
tentang  Pemberlakukan  Pembatasan  Kegiatan 
untuk 
Pengendalian  Penyebaran  Corona  Virus  Disease  2019 
(COVID-19)[5].  Berbeda  dengan  PSBB,  penerapan  PPKM 
hanya mencakup Pulau Jawa dan Bali. PPKM  dinilai cenderung 
lebih  longgar  ketimbang  PSBB.  Dalam  aturan  PPKM, 
karyawan diizinkan untuk hadir ke kantor untuk bekerja hingga 

 1 / 10 

 
 
 
 
 
25  persen  karyawan  di  kantor,  serta  membolehkan  kegiatan 
ibadah  bersama  hingga  50  persen  kapasitas  tempat  ibadah 
tersebut[5]. Namun,  pada dasarnya PPKM  ini  adalah turunan 
dari  PSBB  dengan  melakukan  beberapa  perubahan  dalam 
aturannya.  Penerapan 
jilid  pertama  kebijakan  PPKM 
dilaksanakan pada tanggal 11 Januari 2021 hingga  25 Januari 
2021[6]. Dalam evaluasi pelaksanaan PPKM tersebut, Presiden 
Jokowi  menyampaikan  bahwa  pelaksanaan  PPKM  belum 
efektif  dalam  menurunkan  penularan  COVID-19[7].  Hal  ini 
terlihat  dari  masih  banyaknya  penambahan  kasus  harian 
COVID-19.  Bahkan, kasus harian telah menembus angka 1 juta 
pada  periode  tersebut[8].  Hasil  evaluasi  tersebut,  membuat 
pemerintah  melakukan  modifikasi  PPKM  menjadi  PPKM 
berbasis  Mikro  dalam  upaya  menjangkau  hingga  wilayah 
terkecil.  Pelaksanaan  PPKM  berbasis  Mikro  pertama 
dilaksanakan pada tanggal 9 Februari 2021 hingga 22 Februari 
2021[9]. Dari  hasil evaluasi tersebut, Menteri  Dalam  Negeri, 
Airlangga,  mengungkapkan  bahwa  selama  penerapan  PPKM 
Mikro,  secara  nasional  jumlah  kasus  aktif  COVID-19 
mengalami  penurunan  yang  cukup  signifikan,  yaitu  minus 
17,27 persen dalam sepekan[10]. Namun, dari semua kebijakan 
yang  telah  dilakukan  pemerintah,  tidak  dapat  sepenuhnya 
dinilai  melalui  angka  statistik  kasus  COVID-19,  penilaian 
tersebut juga haruslah mempertimbangkan berbagai masukan, 
diantaranya adalah dengan melihat bagaimana respon dan opini 
masyarakat terhadap program yang dilakukan pemerintah.  

Masyarakat memberikan  respon  dan  opininya di  berbagai 
media, salah satunya yang banyak digunakan oleh masyarakat 
adalah media sosial. Media sosial telah dianggap menjadi suatu 
hal yang wajib dimiliki oleh seluruh masyarakat. Berdasarkan 
data  “Digital  2021  Indonesia”  di  website  datareportal.com, 
jumlah pengguna media sosial di Indonesia pada januari tahun 
2021  yaitu  sebanyak  170  juta  pengguna  atau  sebesar  61,8 
persen  dari  total  populasi  Indonesia[11].  Salah  satu  media 
sosial  yang  paling  banyak  digunakan  oleh  masyarakat 
Indonesia  adalah  twitter,  yang  penggunanya  sebanyak  63,4 
persen  dari  total  pengguna  media  sosial  yang  ada  di 
Indonesia[11]. Hal  ini  menunjukkan adanya peluang  sumber 
data  yang  sangat  besar  dan  dapat  dimanfaatkan  untuk 
menghasilkan 
bermanfaat[12]. 
Pengumpulan  data  dari media  sosial  dinilai lebih efektif dan 
efisiensi  apabila  dibandingkan  dengan  survei  tradisional. 
Efisiensi  tersebut mencakup  biaya,  data  yang  real  time,  dan 
informasi  yang  lebih  detail  untuk  menggambarkan  opini 
masyarakat yang sebenarnya[13]. Banyak penelitian-penelitian 
analisis  opini  masyarakat  dengan 
yang  melakukan 
menggunakan  data  dari  media  sosial  twitter,  misalnya 
penelitian tentang opini masyarakat terhadap vaksin  COVID-
19[12],  dan  opini  masyarakat  terhadap  pelayanan  GSM  di 
Indonesia[14].  

informasi  yang 

suatu 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

2.  Mengetahui  akurasi  dari  klasifikasi  sentimen  publik 

terhadap kebijakan PSBB dan PPKM.   

3.  Menemukan  kata  kunci  topik  yang  sering  dibicarakan 

terhadap PSBB  dan PPKM.   

III. PENELITIAN TERKAIT 

Penelitian terkait dengan topik analisis sentimen dan  topic 
modelling  yang  menjadi  referensi  penelitian  ini  antara  lain 
adalah:  

Judul 
Analisis Sentimen 
Pro dan Kontra 
Masyarakat 
Indonesia tentang 
Vaksin COVID-19 
pada Media Sosial 
Twitter 

TABEL I 
PENELITIAN TERKAIT 
Penulis 
Rachman, F. 
F., & 
Pramana, S. 
[12] 

Keterangan 
Pada penelitian tersebut, dilakukan 
analisis terhadap sentimen 
masyarakat yang pro dan kontra 
terhadap Vaksin COVID-19. Metode 
yang digunakan adalah LDA dan 
data yang didapatkan melalui 
scraping twitter. 

Twitter’s 
Sentiment Analysis 
on Gsm Services 
using Multinomial 
Naïve Bayes 

Susanti, A. 
R., Djatna, 
T., & 
Kusuma, W. 
A.[14] 

Pada penelitian tersebut, dilakukan 
analisis terhadap sentimen pada 
tweet di twitter Pelayanan Gsm. 
Metode yang digunakan adalah 
Multinomial Naïve Bayes. 

Garcia, K., & 
Berton, 
L.[15] 

Topic detection 
and sentiment 
analysis  in Twitter 
content related to 
COVID-19 from 
Brazil and the USA 

Penelitian ini melakukan analisis 
sentimen dan mendeteksi topik 
yang terkait pada COVID-19 di Brazil 
dan USA melalui konten twitter. 
Data didapatkan dengan cara 
Twitter Stream API dan metode 
yang digunakan adalah LDA. 

  Dari beberapa penelitian pada Tabel 1,  terdapat keterkaitan 
dengan  penelitian  ini,  sehingga  ada  kesamaan  metode  yang 
digunakan  pada  penelitian  ini.  Perbedaannya  adalah  pada 
penelitian  ini  berfokus  pada  analisis  sentimen  masyarakat 
terhadap  program  yang  telah  dijalankan  pemerintah  dalam 
upaya menekan angka COVID-19,  yaitu PSBB  dan PPKM. 

Pada 

analisis 

penelitian 

IV. METODE PENELITIAN  
sentimen 
ini,  dilakukan 
menggunakan  Multinomial  Naïve  Bayes  classifier dan  topic 
modelling  yang  dilakukan  dengan  menggunakan  Latent 
Dirichlet Allocation (LDA).  Data yang digunakan adalah data 
Twitter  yang  dikumpulkan  melalui  scraping  menggunakan 
package  twint  yang  ada  di  python[16]. Data  yang  sudah  di 
scraping,  dilakukan  manual  labelling.  Kemudian  dilakukan 
preprocessing  untuk  menghilangkan  noise.  Setelah  bersih, 
dilakukan ke pengolahan berikutnya. 

II.  TUJUAN PENELITIAN 

Berdasarkan  identifikasi  masalah  di  atas,  maka  tujuan 

penelitian ini adalah:  
1.  Mengetahui  sentimen  publik  terhadap kebijakan  PSBB 

dan PPKM  melalui media sosial Twitter.  

Untuk analisis sentimen, data dibagi menjadi  data training 
dan  data  testing.  Data  training  dibuatkan  model  dengan 
menggunakan  Multinomial  Naïve  Bayes  classifier.  Model 
klasifikasi yang sudah dibuat, dilakukan evaluasi dengan cara 
membandingkan  data  aktual  hasil  manual  labelling  dengan 
hasil  klasifikasi. Sedangkan  untuk  topic  modelling,  pertama 
adalah menentukan jumlah topik dengan membandingkan nilai 

 2 / 10 

 
 
 
 
 
 
koherensi dari beberapa percobaan. kemudian peneliti memilih 
jumlah  topik  berdasarkan  nilai  koherensi  yang  tertinggi. 
Selanjutnya,  dilakukan  visualisasi  dengan  menggunakan 
package pyLDAvis  untuk mengetahui  kata  kunci  pada topik 
tersebut. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

melakukan preprocessing. Dalam melakukan preprocessing ini, 
package python yang perlu disiapkan adalah package sastrawi. 
Dalam  penelitian ini, dilakukan tahapan preprocessing antara 
lain:  
a)  Menghapus link URL  seperti “http://abc/def”. 
b)  Menghapus username seperti “@abc”.  
c)  Memisahkan  tanda  tagar  dengan  konten  seperti  “#abc” 

menjadi “abc”.  

d)  Menghapus simbol retweet seperti “RT”.  
e)  Menghapus angka, termasuk angka dalam string.  
f)  Menghapus emoji, karakter aneh, dan tanda baca.  
g)  Menyetarakan  karakter  huruf  dalam  kalimat  menjadi 

karakter huruf kecil semua (lowercase).  

h)  Tokenisasi,  yaitu  memotong  kalimat  menjadi  kata-kata 

secara terpisah.  

j) 

i)  Normalisasi,  yaitu  memperbaiki  kata-kata  yang  tidak 
baku akibat penyingkatan kata atau typo dalam penulisan 
kalimat.  
Penghapusan  stopword  dengan  menggunakan  package 
sastrawi  untuk  menghapus  kata/karakter  yang  tidak 
diperlukan.  
Stemming,  yaitu  menghapus  adanya  imbuhan  kata  dan 
merubah  kata  menjadi  akar  kata  berdasarkan  kamus 
sastrawi. 
Remove duplicate, yaitu menghapus data yang duplikat. 

k) 

l) 

Gambar 1. Tahapan Penelitian 

Gambar  1  merupakan  tahapan  yang  dilakukan  pada 

penelitian ini. Untuk detailnya adalah sebagai berikut: 

1. Data Collection  

Pada  penelitian  ini,  pengumpulan  data  dilakukan  dengan 
cara scraping menggunakan package twint yang ada di bahasa 
pemrograman python. Keyword yang digunakan adalah “PSBB” 
dan  “PPKM”.  Pengambilan  data  dilakukan  dalam  rentang 
waktu 01 Januari 2021 hingga 28 Februari 2021. 

2. Manual Labelling 

Peneliti melakukan pelabelan data dengan cara input manual 
untuk semua data tweet. Pada dataset ditambahkan kolom baru 
untuk label.  Kategori  1  yaitu  untuk tweet  yang  mengandung 
sentimen positif dan kategori 2 untuk tweet yang mengandung 
sentimen  negatif.  Tweet  yang  berupa  berita  dan  tweet  yang 
tanggapannya  bersifat  netral  terhadap  PSBB  dan  PPKM, 
diberikan  kategori  0.  Manual  labelling  tersebut  digunakan 
untuk melatih model analisis sentimen dan sebagai evaluasi. 

3. Preprocessing  

Dari  dataset  scraping  tersebut,  masih 

terdapat  noise 
misalkan mengandung  emoji,  kata  berimbuhan, kata  hubung, 
tanda baca, link website, angka, dan lainnya. Hal tersebut dapat 
mempengaruhi  analisis  data  atau  memberatkan  performa 
tersebut  harus  dihilangkan  dengan 
selanjutnya.  Noise 

4. Analisis Sentimen 

Data  yang  sudah  bersih  dan  memiliki  label,  dilakukan 
pemisahan data secara proporsional sebanyak 70 persen untuk 
data  training  dan  30  persen  untuk  data  testing.  Sebelum 
dilakukan  modelling,  data  perlu  diubah  ke  dalam  bentuk 
numerik agar dapat diolah dan dihitung dengan cara dilakukan 
fitur  ekstraksi  Term  Frequency  dan  Inverse  Document 
Frequency (TF-IDF).   Metode ini digunakan untuk menghitung 
nilai TF  dan IDF  pada setiap kata di setiap dokumen. Tujuan 
penggunaan  TF-IDF 
adalah  sebagai  vektorisasi  untuk 
menghitung  bobot  setiap  kata  yang  pada  umumnya  sering 
digunakan. Hal  ini dikarenakan klasifikasi Multinomial Naïve 
Bayes hanya menerima input berupa numerik. Selanjutnya pada 
Multinomial  Naïve  Bayes  Classifier,  diperlukan  package 
python Scikit-learn[17]. Kemudian,  data training diolah oleh 
Multinomial Naïve Bayes  Classifier untuk membentuk model 
sentimen.  Setelah 
testing 
diklasifikasikan  oleh  model  yang  sudah  terbentuk.  Setelah 
selesai, hasil klasifikasi dilakukan tahap evaluasi. 

training  selesai,  data 

tahap 

5. Evaluasi Analisis Sentimen 

Metode evaluasi untuk analisis sentimen pada penelitian ini 
adalah dengan membandingkan data testing tweet yang sudah 
dilakukan  pelabelan  secara  manual  dengan  input  komputer 
yang  dilabeli  oleh  model  Naïve  Bayes  classifier  yang  telah 
dibangun. Package yang digunakan adalah sklearn.metrics[17]. 
Beberapa hal yang dilihat adalah tingkat akurasi, presisi, recall, 
serta  F1-Score.  Indikator  tersebut  berguna  untuk  melihat 
kinerja dari Naïve Bayes classifier. 

 3 / 10 

 
 
 
 
 
 
 
 
 
 
 
6. Topic Modelling 

modelling 

menggunakan 

Pada tahap ini, data yang digunakan adalah data yang sudah 
melalui  tahap  preprocessing.  Berikut  ini  adalah  tahapan 
penerapan topic modelling:  
a)  Data tweet yang sudah bersih dibuatkan sebuah dictionary 
dengan  memakai  package  Gensim.  Gensim  adalah 
package python yang dapat mengelolah bahasa alami dan 
Topic 
statistik. 
Dictionary/kamus  tersebut  merupakan  daftar  kata-kata 
unigram unik yang sering muncul dalam tiap data/tweet.  
b)  Dictionary/kamus  yang  sudah  dibuat,  dilakukan  filter. 
Token/kata  dihapus  ketika  token/kata  tersebut  muncul 
dalam kurang dari 10 tweet dan muncul di dalam lebih dari 
50  persen total  tweet.  Hal  ini  dilakukan agar  token/kata 
dalam kamus bisa representatif.  
Pembuatan  korpus.  kata  unigram  dalam  data  dibuatkan 
bag of words (bow). Korpus ini nantinya berisi komposisi 
kata-kata yang  hanya  ada  di  dalam  kamus  yang  sudah 
disaring beserta jumlah tiap tweet/dokumen.  

c) 

d)  Menentukan 

jumlah 

topik  yang  berdasarkan  nilai 

koherensi yang tertinggi.  
Implementasi LDA  model dengan gensim. 

e) 
f)  Visualisasi 

topic  modelling  dengan  menggunakan 

package pyLDAvis[18]. 

V.  KERANGKA PIKIR 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

program  tersebut. Pada  penelitian ini,  berupaya menganalisis 
sentimen  masyarakat  tersebut,  yang  kemudian  diklasifikasi 
menjadi sentimen positif,  negatif,  dan netral. Selain  itu,  juga 
dilakukan Topic Modelling yang  bertujuan untuk mengetahui 
jumlah  topik  bahasan  serta  kata  kuncinya.  Sehingga,  hasil 
penentuan  topik  ini  dapat  menjadi  bahan  evaluasi  terhadap 
upaya pemerintah menangani Pandemi COVID-19  ini. 

VI. HASIL DAN PEMBAHASAN 

Karakteristik Data, Manual Labelling dan Preprocessing 
1. Pengumpulan Data  

Pengumpulan  data  dilakukan  dengan  teknik  scraping 
melalui  package  di  python,  yaitu  twint.  Keyword  yang 
digunakan pada scraping ini adalah “psbb” dan “ppkm” dengan 
rentang  waktu  dari  awal  januari  sampai  akhir  februari. 
Pemilihan  keyword  tersebut dianggap  relevan dengan  tujuan 
yang ingin  dicapai, yaitu terkait PSBB  dan PPKM.  Meskipun 
masih terlalu luas, namun nantinya di bersihkan pada tahapan 
data  manual  labelling  dan  preprocessing.  Pada  awalnya, 
rencana  dalam  penelitian  ini  menggunakan  keyword  yang 
bertagar  saja,  yaitu  “#psbb”  dan  “#ppkm.  Namun  terdapat 
perbedaan yang cukup siginifikan antara yang bertagar maupun 
tidak  bertagar.  Jumlah  tweet  yang  menggunakan  keyword 
bertagar, hanya terdapat 4.038 tweet. Sedangkan scraping yang 
menggunakan  keyword  tanpa  tagar  terdapat  183.911  tweet. 
Alasan  tidak  menggunakan  keyword  yang  bertagar  adalah 
karena  tidak  semua  orang  yang  membicarakan  PSBB  dan 
PPKM  menggunakan  tagar,  kebanyakan  pengguna  twitter 
langsung menyelipkan kata PSBB  dan PPKM  dalam tweet-nya. 
Untuk lebih optimal dalam analisis nantinya, dalam penelitian 
ini diputuskan untuk menggunakan keyword tanpa tagar karena 
jumlah data yang didapatkan cukup banyak.  

Data  dikumpulkan  dengan  dua  kali  melakukan  scraping, 
yang pertama dengan keyword “psbb” dan berikutnya dengan 
keyword  “ppkm”.  Setelah  dilakukan  scraping  untuk  semua 
keyword,  kemudian  data  tersebut  digabungkan  menjadi  satu 
dataset. Gambar 5 berikut adalah histogram yang menampilkan 
jumlah tweet per harinya. 

Gambar 2. Kerangka Pikir 

Gambar 3. Histogram jumlah tweet 

Pada Gambar 2, terdapat alur kerangka pikir dari penelitian 
ini. Berawal  dari pandemi COVID-19,  kemudian upaya cepat 
tanggap dri pemerintah dalam menangani pandemi ini dengan 
menerapkan PSBB  dan PPKM.  Selama  penerapan PSBB  dan 
PPKM,  banyak  tweet  dari  masyarakat  dalam  menanggapi 

  Pada Gambar 3, terdapat lonjakan pada tanggal 10, 11, dan 
12  Februari  2021.  Hal  ini  disebabkan  penerapan  kembali 
PPKM  dengan modifikasi berskala mikro. Tentunya transisi ini 
mengundang  masyarakat  untuk  memberikan  opini  terkait 

 4 / 10 

 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

upaya pemerintah mengubah istilah kebijakan PSBB  menjadi 
PPKM. 

2. Karakteristik Data  
  Dari dataset yang sudah terkumpul, data memiliki 3 atribut, 
yaitu: id, date, dan tweet. Id merupakan kolom yang berisi kode 
unik  dari  suatu  tweet.  Date  merupakan  kolom  yang  berisi 
tanggal tweet dibuat. Tweet  merupakan kolom yang berisi teks 
tweet  itu  sendiri.  Tabel  2  adalah  sampel  data  dari  scraping 
menggunakan twint di python.  

berlabel positif merupakan tweet  yang  berisi  persepsi positif 
terhadap  kebijakan  PSBB  dan  PPKM.  Tweet  yang  berlabel 
negatif merupakan tweet yang berisi persepsi negatif terhadap 
kebijakan PSBB dan PPKM.  Tweet yang berlabel netral adalah 
tweet yang tidak ada hubungannya dengan kebijakan PSBB dan 
PPKM  atau tidak memberikan opini (bisa berupa berita). Label 
ini  diperlukan untuk membandingkan apakah hasil  pelabelan 
sentimen  pada  machine  learning  mendekati  pelabelan  yang 
diinput  manusia.  Contoh  hasil  dari  manual  labelling  dapat 
dilihat pada Tabel 3. 

TABEL 2 
CONTOH SAMPEL HASIL SCRAPING 

TABEL 3 
CONTOH HASIL MANUAL LABELLING 

id 
13577
52868
91724
0000 

13643
39074
41317
0000 

date 
06/02/
2021 

tweet 
8.580  Pelanggaran  Terjadi  selama  PPKM  di  Depok, 
Didominasi  Warga tidak  Pakai  Masker: Satuan  Polisi 
Pamong  Praja  (Satpol  PP)  Kota  Depok  mencatat 
banyak 
selama 
Pemberlakuan  Pembatasan  Kegiatan  Masyarakat 
(PPKM) sejak 11 Januari…  https://t.co/HnUpMvcXcQ 

pelanggaran 

terjadi 

yang 

24/02/
2021 

@Anonim  Mantap..👏   Indonesia  berhasil  menekan 
angka kasus Covid-19  dengan  terapkan PPKM  Mikro. 
#DisiplinPatuhiProkes  https://t.co/zU2JmVbPZT 

11 

-  25 

tweet 
Buat yang harus bepergerian jauh; Anda Harus Penuhi  Ini Ketika 
Januari  
Hendak  Bepergian  Selama  PPKM 
https://t.co/jdjhST77r9 
Buat  warga  Jawa  Tengah  nih  simak  penjelasan  Pak  Gubernur 
@ganjarpranowo soal PPKM.    Patuhi  selalu protokol  kesehatan 
pokoknya.  Satu  lagi  jangan  kumpul2  sama sapi  ini  pesan  saya 
pribadi.  https://t.co/V4gTXf35Ht 
Bukan  soal ganti nama  dari psbb  lock  down  mini  lock  down  ke 
ppkm  tapi tidak  tegasnya buk  @Menlu_RI   Dan @jokowi   buat 
peraturan  bagi  rakyat  anda  tapi  mengabaikan  peraturan  dari 
luar.. gimane donk.. 

Label 

0 

1 

2 

  Dari  sampel  data  yang  terdapat pada  Tabel  2,  didapatkan 
beberapa karakteristik data  yang  terlihat seperti  penggunaan 
bahasa  yang  tidak  terstruktur,  terdapat  penggunaan  emoji, 
singkatan,  dan  lainnya.  Oleh  karena  itu,  data  tersebut perlu 
dilakukan tahapan preprocessing agar  data  siap  diolah lebih 
lanjut. 

4. Preprocessing  
  Dataset yang sudah dilakukan manual labelling, selanjutnya 
dilakukan tahapan preprocessing untuk menghapus noise agar 
model yang dibuat lebih optimal. Kolom yang digunakan pada 
preprocessing ini adalah kolom tweet.  Hasil tahapan dari tiap 
preprocessing dapat dilihat pada Tabel 4.  

3. Manual Labelling  
  Dataset  yang  sudah  dikumpulkan,  selanjutnya  dilakukan 
tahapan manual  labelling. Pada  penelitian ini,  tahap  manual 
labelling dilakukan oleh  3  orang  dengan  membagi  rata  data 
mentahnya,  dan  juga  membutuhkan  waktu  yang  cukup lama 
jumlah  data  yang  banyak.  Manual  labelling 
mengingat 
dilakukan  atas  dasar  penilaian  subjektif  peneliti  terhadap 
persepsi  yang  terkandung  dalam  twitter.  Peneliti  menginput 
langsung label 0, 1, atau 2 pada kolom label yang telah dibuat 
secara manual. 

32614

9689

11461

Netral (0)

Positif (1)

Negatif (2)

Jumlah tweet
Gambar 4. Diagram batang jumlah tweet per label 

  Menurut Gambar 4, Dari total 53.764 data yang sudah diberi 
label manual, terdapat 32.64 tweet (60,6%) yang berlabel netral 
(label  0),  9.689  tweet  (18%)  berlabel  Positif  (label  1),  dan 
11.461  tweet  (21,4%)  berlabel  negatif  (label  2).  Tweet  yang 

TABEL 4 
PROSES PREPROCESSING 

Tweet awal 

@Anonim Mantap..👏  Indonesia berhasil menekan 
angka kasus Covid-19 dengan terapkan PPKM Mikro. 
#DisiplinPatuhiProkes  https://t.co/zU2JmVbPZT 

Menghapus  
username 

Mantap..👏  Indonesia berhasil menekan angka kasus 
Covid-19 dengan terapkan PPKM Mikro. 
#DisiplinPatuhiProkes  https://t.co/zU2JmVbPZT  

Menghapus 
link URL 

Memisahkan 
tanda tagar 

Menghapus 
angka 

Menghapus 
emoji, 
karakter aneh, 
dan tanda baca 

Lowercase 

Tokenisasi 

Mantap..👏  Indonesia berhasil menekan angka kasus 
Covid-19 dengan terapkan PPKM Mikro. 
#DisiplinPatuhiProkes   

Mantap..👏  Indonesia berhasil menekan angka kasus 
Covid-19 dengan terapkan PPKM Mikro. 
DisiplinPatuhiProkes   

Mantap..👏  Indonesia berhasil menekan angka kasus 
Covid- dengan terapkan PPKM Mikro. 
DisiplinPatuhiProkes   

Mantap Indonesia berhasil menekan angka kasus Covid 
dengan terapkan PPKM Mikro DisiplinPatuhiProkes   

mantap indonesia berhasil menekan angka kasus covid 
dengan terapkan ppkm mikro disiplinpatuhiprokes   

['mantap', 'indonesia',  'berhasil', 'menekan', 'angka', 
'kasus',  'covid', 'dengan', 'terapkan', 'ppkm', 'mikro'] 

Normalisasi 

['mantap', 'indonesia',  'berhasil', 'menekan', 'angka', 
'kasus',  'covid', 'dengan', 'terapkan', 'ppkm', 'mikro'] 

 5 / 10 

 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Stopword 

Stemming 

['mantap', 'indonesia',  'berhasil', 'menekan', 'angka', 
'covid', 'terapkan', 'ppkm', 'mikro'] 

1364339074
413170000 

24/02
/2021 

mantap  indonesia  hasil  tekan  angka  covid  terap 
ppkm mikro 

['mantap', 'indonesia',  'hasil', 'tekan', 'angka', 'kasus', 
'covid', 'dengan', 'terap', 'ppkm', 'mikro'] 

  Setelah dilakukan stemming, kata-kata tersebut digabungkan 
kembali  menjadi  kalimat.  Kemudian  dilakukan  remove 
duplicate, agar data yang duplikat dapat dibersihkan. 

TABEL 5 
CONTOH SAMPEL DATA SETELAH PREPROCESSING 

id 

1357752868
917240000 

date 

06/02
/2021 

tweet 

langgar ppkm  depok  dominasi  warga pakai  masker 
satu polisi pamong praja satpol pp kota depok catat 
langgar laku batas giat masyarakat ppkm januari 

  Tabel  5  merupakan  contoh  tweet  yang  sudah  dilakukan 
tahapan  preprocessing  dan  siap  dilanjutkan  ke  tahapan 
pengolahan berikutnya. 

Analisis Sentimen   
  Data yang sudah dilakukan pelabelan, selanjutnya dilakukan 
analisis untuk melihat kata-kata penting  yang  mempengaruhi 
tiap kategori label. Berikut adalah kata-kata penting per label: 

Gambar 5. Kata-kata penting untuk masing-masing sentimen 

dilakukan  pembuatan  model 

  Kata-kata pada Gambar  5  merupakan 10  kata yang  paling 
mempengaruhi  di 
tiap  kategorinya  berdasarkan  nilai 
pembobotannya.  Untuk  analisis  terkait  kata-kata  penting, 
dijelaskan pada subbab topic modelling. 
  Selanjutnya, data yang sudah melalui tahapan pelabelan dan 
preprocessing, 
dengan 
Multinomial  Naïve  Bayes  classifier  untuk  membandingkan 
apakah  hasil  pelabelan  pada  machine  learning  mendekati 
pelabelan yang diinput manusia. Pertama, dilakukan pemisahan 
data dengan proporsi 70% untuk data training dan 30% untuk 
data  testing. Pemisahan  data  dilakukan dengan  random  oleh 
komputer  dengan  memperhatikan proporsi label.  Didapatkan 
jumlah  data  training  berjumlah  22.716  untuk  netral,  6.854 
untuk positif dan  8.064 untuk negatif.  Data  training tersebut 
lalu  dipelajari  oleh  komputer.  Setelah  model  didapat,  maka 
model  tersebut dipakai  untuk tahap  testing dengan  memakai 
sisa  data  yang  berjumlah  30%.  Untuk  mengetahui  apakah 
model hasil dari training sudah benar perlu dibutuhkan evaluasi. 
Evaluasi tersebut menggunakan confusion matrix. 

Confusion  matrix  yang  sudah  terbentuk dapat  dipakai  untuk 
menghitung  akurasi, presisi, recall,  dan F1-Score dari  model 
analisis  sentimen  hasil  dari  klasifikasi  Multinomial  Naïve 
Bayes. 

TABEL 7 
AKURASI, RECALL, PRESISI, F1-SCORE MODEL 
Overall accuracy: 0.583 

Recall 0: 0.828 

Recall 1: 0.403 

Recall 2: 0.437 

Precision 0: 0.539 

Precision 1: 0.687 

Precision 2: 0.625 

F1-Score 0: 0.653 

F1-Score 1: 0.508 

F1-Score 2: 0.514 

  Berdasarkan  Tabel  7,  didapatkan akurasi  dari  pemodelan 
klasifikasi Multinomial  Naïve  Bayes  sebesar 58,3%.  Hal  ini 
dapat  diartikan  bahwa  hasil  klasifikasi  Multinomial  Naïve 
Bayes kurang cukup baik digunakan dalam penyelesaian kasus 
sentimen  analisis  terhadap  penerapan  kebijakan  PSBB  dan 
PPKM,  karena sebanyak 41,7% tidak dapat memberikan hasil 
klasifikasi yang sama dibandingkan dengan input manual. Hal 
ini juga dapat dilihat melalui perbandingan pada Gambar 6.   

TABEL 6 
CONFUSION MATRIX HASIL KLASIFIKASI 

Predict: 0 

Predict: 1 

Predict: 2 

Actual: 0 
Actual: 1 

Actual: 2 

5331 
461 

645 

2254 
1949 

628 

2313 
425 

2124 

  Tabel 6 merupakan confusion matrix yang membandingkan 
hasil  klasifikasi  label  manusia  dengan  model  komputer. 

Gambar 6. Perbandingan klasifikasi label pada data testing 

 6 / 10 

 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

  Gambar 6 merupakan perbandingan data testing antara input 
manual  dengan  predict dari  model  Multinomial Naïve  Bayes 
classifier.  Dapat  dilihat  bahwa  sentimen  netral  pada  input 
manual  mengalami  pengurangan  sebesar  35%  setelah 
dilakukan predict dari model yang sudah terbentuk. 

Topic Modelling  
  Untuk menentukan hasil pemodelan, dapat dilakukan dengan 
cara melihat visualisasi dari grafik coherence score. Coherence 

score  merupakan  salah  satu  cara  yang  digunakan  untuk 
mengevaluasi Topic  Modeling.  Ketika  coherence score pada 
topik  nilainya  tinggi,  maka  model  yang  dihasilkan  tersebut 
adalah yang baik. Dalam  menentukan jumlah topik, dilakukan 
perbandingan coherence score  melalui  grafik  pada  beberapa 
percobaan dengan  menentukan batasan  topiknya.  Percobaan 
yang dilakukan pada nilai limit dan start, dimulai dari 0-11, 0-
21, 0-31. Perbandingan tersebut dapat dilihat pada Gambar 7. 

Gambar 7. Grafik perbandingan coherence score  pada limit topik 

TABEL 9 
5 KATA TERATAS TIAP TOPIK 

Topik 

Kata Kunci 

Topik 1 (topik tentang Pembatasan 
kebijakan PPKM) 
Topik 2 (topik tentang Penilaian masyarakat 
terhadap kebijakan PSBB dan PPKM) 
Topik 3 (topik tentang Perpanjangan tanggal 
pelaksanaan kebijakan  PPKM Mikro) 

ppkm, covid, batas, 
tekan, masyarakat 
gagal, jam, ganti, rakyat, 
tutup 
mikro, rumah, kota, 
panjang, tanggal 

  Tabel 9 merupakan 5 kata yang merepresentasikan tiap topik 
dengan  jumlah  kata  tertinggi  bagi  tiap  topiknya.  Kata-kata 
tersebut dikelompokkan karena peluang munculnya kata-kata 
tersebut yang besar bagi sebuah topik. 

  Pada  Gambar  7,  dapat dilihat grafik  pada  limit  topik 0-11 
memiliki  nilai  coherence  score  tertingginya  sebesar  0.536. 
sedangkan pada limit topik 0-21 memiliki nilai coherence score 
tertingginya sebesar 0.516 dan pada limit topik 0-31 memiliki 
nilai coherence score tertingginya sebesar 0.524. Sehingga dari 
beberapa  percobaan  tersebut,  peneliti  memutuskan  untuk 
menggunakan hasil dari limit topik 0-11 dengan jumlah topik 
yang  digunakan sebanyak 3  karena memiliki coherence score 
tertinggi  yaitu  sebesar  0.536.  Dari  referensi  yang  peneliti 
dapatkan, jumlah topik yang semakin sedikit, maka topik yang 
ditampilkan  nantinya  lebih  umum.  Sehingga  jumlah  topik 
tersebut menjadi  acuan dalam  penelitian ini  untuk membuat 
model  selanjutnya.  Tabel  8  merupakan  perbandingan 
coherence score yang terdapat pada limit topik 0-11. 

TABEL 8 
COHERENCE SCORE 

Num Topics 

Coherence  Score 

Num Topics 

Coherence  Score 

1 
2 
3 
4 
5 

0.350606 
0.376463 
0.535789 
0.43957 
0.42699 

6 
7 
8 
9 
10 

0.441122 
0.486012 
0.461335 
0.442249 
0.436099 

  Selanjutnya  peneliti  membuat  model  LDA  berdasarkan 
banyak  topiknya  yaitu  3  beserta  dengan  jumlah  kata  yang 
ditampilkan dalam model yang memiliki bobot tertinggi di tiap 
topiknya. Tabel  7  menunjukkan topik yang  terbentuk dengan 
menggunakan 3 kluster tentang PSBB  dan PPKM. 

Gambar 8. Kluster topic modelling 

 7 / 10 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

  Gambar  8  merupakan  visualisasi  distribusi  dari  3  topik 
mengenai  PSBB  dan  PPKM.  Terlihat  semua  kluster  topik 
saling  berjauhan.  Hal  ini  menunjukkan  bahwasannya  topik 
yang didapatkan memiliki bahasan yang berbeda, sehingga ini 
cukup  untuk  merepresentasikan  keragaman  topik  bahasan 
masyarakat.  

Gambar 9. Kata yang relevan untuk topik 1 

  Berdasarkan  Gambar  9,  kata  yang  paling  sering  muncul 
untuk topik 1 ini adalah “ppkm”, “covid”, “batas”, ”tekan” dan 
“masyarakat”.  Kata 
dapat 
“ppkm” 
mengindikasikan suatu pembicaraan atau isu tentang kebijakan 
PPKM  dalam  menangani  COVID-19.  Kata  ”batas”,  “tekan” 
dan  “masyarakat” dapat mengindikasikan suatu pembicaraan 
atau isu tentang aturan dari PPKM  yang terlalu membatasi dan 
menekan aktivitas masyarakat. 

“covid” 

dan 

TABEL 10 
CONTOH TWEET TOPIK 1 
tweet 
Sekarang mending  kita  fokus  untuk  menghentikan  semua  pembatasan2 
seperti Lokdon, PSBB, PPKM, 3M, dan 3T. Penanganan pasien covid-19  ini 
jg  harus  diperbaiki,  spy  tingkat  kematian  maupun  tingkat perawatan di 
rumah sakit dpt ditekan 
Kalaupun  pemerintah  melaksanakan kebijakan  PPKM  Mikro,  maka  yang 
harus dilakukan  adalah menambahkan  pengetatan. Sehingga, penularan 
virus  di  tengah  masyarakat  bisa  benar-benar  ditekan  tanpa  perlu 
melakukan lockdown  .  https://t.co/hf07CpU4pd 
Sekarang mending  kita  fokus  untuk  menghentikan  semua  pembatasan2 
seperti Lokdon, PSBB, PPKM, 3M, dan 3T. Penanganan pasien covid-19  ini 
jg  harus  diperbaiki,  spy  tingkat  kematian  maupun  tingkat perawatan di 
rumah sakit dpt ditekan 

  Tabel  10 merupakan contoh tweet  yang  merepresentasikan 
kata  kunci  yang  terdapat  pada  topik  ini.  Topik  1  ini  juga 
merupakan topik dengan proporsi terbesar dari  seluruh tweet 
karena menampung 48,9% kata. 

Gambar 10. Kata yang relevan untuk topik 2 

  Berdasarkan Gambar  10,  kata  yang  paling  sering  muncul 
untuk topik 2 ini adalah “gagal”, “jam”, “ganti”, ”rakyat” dan 
“tutup”.  Kata  “gagal”  dapat  mengindikasikan  sentimen 
masyarakat  tentang  penilaian  negatif  kebijakan  PSBB  dan 
PPKM  dalam  menangani  COVID-19.  Kata  ”jam”,  “ganti”, 
“rakyat”  dan  “tutup”  dapat  mengindikasikan  isu  tentang 
tuntutan  rakyat  untuk  mengganti  aturan  dari  kebijakan 
pemerintah yang membatasi waktu aktivitas rakyat. 

TABEL 11 
CONTOH TWEET TOPIK 2 
tweet 
yang  gagal adala pemerinta.  rakyat gak perlu  diadu  pake  narasi pro  dan 
kontra  lockdown.  pemerinta  dari  mula  suda halu,  bikin  stetmen ngablu, 
gamau lockdown  karena gamau menjamin  kehidupan  rakyat selama awal 
pandemi. psbb abcd dst adala bullshit yg ujung2nya bikin tambah sengsara 
PSBB atau PSTKM  malah  justru dimanfaatin  oleh  klub  klub  motor berisik 
sama orang orang pacaran jam  segini, aku  kira pemerintah  gagal dengan 
program  ini,  udahlah  dicabut  aja  sama aja  ini  kondisinya  ga ngefek,  pas 
awal-awal pandemi itu malah benar-benar sepi 
yang  gagal adala pemerinta.  rakyat gak perlu  diadu  pake  narasi pro  dan 
kontra  lockdown.  pemerinta  dari  mula  suda halu,  bikin  stetmen ngablu, 
gamau lockdown  karena gamau menjamin  kehidupan  rakyat selama awal 
pandemi. psbb abcd dst adala bullshit yg ujung2nya bikin tambah sengsara 

  Tabel  11 merupakan contoh tweet  yang  merepresentasikan 
kata  kunci  yang  terdapat  pada  topik  ini.  Pada  topik  2  ini 
memiliki proporsi sebesar 36,5% kata. 

 8 / 10 

 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

masyarakat terhadap aturan pembatasan jam operasional untuk 
beraktivitas  bagi  para  pedagang  dalam  mencari  pelanggan. 
Dengan  adanya bahan evaluasi ini, harapannya ada perbaikan 
atas kekurangan yang langsung diutarakan masyarakat terhadap 
kebijakan  PSBB  dan  PPKM  dalam  mengatasi  penyebaran 
COVID-19  di Indonesia. 

VII. 

PENUTUP 

Berdasarkan hasil pembahasan pada bab sebelumnya, telah 
dilakukan  analisis  sentimen  dan  topic  modelling  mengenai 
Penerapan Kebijakan  PSBB  dan  PPKM  pada  data  twitter  di 
bulan  januari dan  februari.  Sehingga  didapatkan kesimpulan 
sebagai berikut: 
1. 

Pengumpulan  data  sentimen publik di  twitter  dilakukan 
dengan menggunakan  metode  scraping dengan package 
twint pada python. Kemudian dilakukan tahapan manual 
labelling  dan  preprocessing,  sehingga  data  akhir 
berjumlah  53.764  data,  terdapat  32.614  tweet  (60,6%) 
yang berlabel netral (label 0), 9.689 tweet (18%) berlabel 
Positif  (label  1),  dan  11.461  tweet  (21,4%)  berlabel 
negatif (label 2). 

2.  Model  dari  sentimen  publik dengan  Multinomial  Naïve 
Bayes classifier menghasilkan akurasi sebesar 58,3%. 
3.  Hasil  dari analisis topic modelling dengan  metode  LDA 
topik  dengan 
diperoleh  jumlah 
coherence  score  sebesar  0.536.  Topik  yang  dihasilkan 
yaitu sebagai berikut: 
a)  Topik 1: Pembatasan kebijakan PPKM.   

topik  sebanyak  3 

Kata kunci:   “ppkm”, “covid”, “batas”, ”tekan” dan 
“masyarakat”. 

b)  Topik  2:  Penilaian  masyarakat terhadap  kebijakan 

PSBB  dan PPKM.   
Kata  kunci:  “gagal”,  “jam”,  “ganti”,  ”rakyat” dan 
“tutup”. 

c)  Topik  3:  Perpanjangan 

tanggal  pelaksanaan 

kebijakan PPKM Mikro.  
Kata  kunci:  “mikro”,  “rumah”,  “kota”,  ”panjang”, 
dan “tanggal”. 

Dalam  penelitian ini  masih  banyak yang  harus  diperbaiki 
dan  dikembangkan  lagi.  Berikut  beberapa  saran  untuk 
penelitian ini: 
1.  Pada  penelitian ini,  tahapan manual  labelling dilakukan 
oleh  3  orang.  Namun  terdapat  kekurangan  pada  saat 
pemberian label karena adanya penilaian subjektif masing-
masing  yang  berbeda,  sehingga  kemungkinan  terjadi 
adanya  pemberian  label  yang  berbeda  pada  tweet  yang 
sejenis. Oleh karena itu, pada penelitian selanjutnya perlu 
dilakukan penyamaan pemahaman terkait sentimen netral, 
positif,  dan  negatif,  jika  dalam  tahap  manual  labelling 
dilakukan lebih dari 1 orang.  

2.  Pada  penelitian  ini,  metode  klasifikasi  yang  digunakan 
pada analisis sentimen hanya menggunakan  Naïve Bayes 
classifier,  namun  metode  ini  tidak  memberikan  hasil 
akurasi yang optimal, yaitu 58,3%.  Oleh karena itu, pada 
penelitian  selanjutnya  dapat  melakukan  perbandingan 
metode  analisis sentimen  dengan  algoritma  selain  Naïve 

 9 / 10 

Gambar 11. Kata yang relevan untuk topik 3 

  Berdasarkan Gambar  11,  kata  yang  paling  sering  muncul 
untuk topik 3 ini adalah “mikro”, “rumah”, “kota”, ”panjang”, 
“tanggal” dan “surabaya”. Kata “mikro”, “rumah”, “kota”, dan 
“surabaya”  dapat  mengindikasikan  pembicaraan  masyarakat 
tentang kebijakan PPKM  berskala mikro di kota surabaya dan 
beberapa kota kisaran jawa lainnya, mengingat ini pertama kali 
diterapkan di pulau jawa dan bali. Kata ”panjang” dan “tanggal” 
dapat  mengindikasikan  suatu  sentimen  masyarakat  tentang 
pelaksanaan  kebijakan  ini,  mengingat  pada  rentang  waktu 
januari dan februari telah dilaksanakan 2 kali kebijakan PPKM. 

TABEL 12 
CONTOH TWEET TOPIK 3 
tweet 
Udah  psbb  gausah diperpanjang2  perbulan  pertanggal berapa. Langsung 
aja terus terang perpanjang berapa tahun. Biar jls. 
pgn pulang kerumah  tp punya  keluarga begini. ga pulang  krumah psbb di 
perpanjang terus hidup dkosan terbatas 
Tiba2  ppkm,  tiba2  penutupan 
tiba2 
perpanjangan  ppkm/psbb,  tiba2  dirumah  saja  2hari,  tiba22222222....... 
cuma mempersulit mobilitas &amp 
Sedih  wakk, ketika lu  bisanya produktif  di  luar rumah trus di batasi dong 
jam malamnya. Eh ini malah di perpanjang psbb nya :"" 

jalan,  tiba2  psbb  berjilid2, 

  Tabel  12 merupakan contoh tweet  yang  merepresentasikan 
kata  kunci  yang  terdapat  pada  topik  ini.  Pada  topik  3  ini 
memiliki proporsi yang terendah, yaitu sebesar 14,6% kata. 
  Meskipun  tweet  pada  penelitian  ini  didominasi  oleh 
sentimen  netral,  namun  dapat  dilihat  bahwasannya sentimen 
negatif lebih banyak daripada sentimen positif. Dan juga secara 
keseluruhan,  topik  yang  dihasilkan  tersebut  lebih  banyak 
mengandung  kata-kata  yang  paling  sering  muncul  pada 
sentimen  negatif.  Hal  tersebut  dibuktikan  dengan  adanya 
lampiran  contoh  tweet.  Sehingga  perlu  adanya  evaluasi 
pemerintah  dalam  menerapkan  kebijkan  PSBB  dan  PPKM, 
mengingat  banyak  aspek  yang  perlu  diperhatikan  selain 
daripada  kesehatan,  terutama  pada  aspek  ekonomi  dan 
pendidikan.  Misalnya  dengan  contoh  tweet  “@KhofifahIP 
PPKM  gak  usah  diterusin  buk,  kasihan  yg  jualan  malam, 
berharap dapat satu atau dua pelanggan sekarang malah harus 
tutup  lebih  awal”,  dapat  mengindikasikan  adanya  keluhan 

 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

[14] 

[15] 

[16] 

[17] 

[18] 

media data,” J. Public Transp., vol. 16, no. 2, pp. 21–45, 2013, doi: 
10.5038/2375-0901.16.2.2. 
A. R. Susanti, T. Djatna, and W. A. Kusuma, “Twitter’s sentiment 
analysis on GSM services using Multinomial Naïve Bayes,” 
Telkomnika (Telecommunication Comput. Electron. Control., vol. 
15, no. 3, pp. 1354–1361, 2017, doi: 
10.12928/TELKOMNIKA.v15i3.4284. 
K. Garcia and L. Berton, “Topic detection and sentiment analysis in 
Twitter content related to COVID-19 from Brazil and the USA,” 
Appl. Soft Comput., vol. 101, p. 107057, 2021, doi: 
10.1016/j.asoc.2020.107057. 
Zacharias, C., “TWINT - Twitter Intelligence Tool,” github.com, 
2019. https://github.com/twintproject/twint (accessed Mar. 20, 
2021). 
Pedregosa et al., “Scikit-learn: Machine Learning in Python,” scikit-
learn.org, 2011. https://scikit-
learn.org/stable/modules/generated/sklearn.naive_bayes.Multinomia
lNB.html (accessed Mar. 20, 2021). 
Ben Mabey, “Welcome to pyLDAvis’s  documentation!,” 
pyldavis.readthedocs.io, 2015. 
https://pyldavis.readthedocs.io/en/latest/ (accessed Mar. 10, 2021). 

Bayes classifier, agar dapat mengetahui metode mana yang 
lebih baik digunakan.  

3.  Pada  penelitian  ini,  tweet  yang  bersifat  netral  terlalu 
mendominasi,  dan  tweet  netral  ini  juga  disertakan  saat 
tahapan topic modelling, sehingga topik pembicaraan yang 
terbentuk kurang  spesifik karena  banyaknya  tweet  yang 
berupa berita  atau  yang  bersifat netral.  Oleh  karena itu, 
tahapan  topic  modelling  pada  penelitian  selanjutnya 
diharapkan dapat fokus pada sentimen positif dan negatif 
saja,  agar  dapat  mengetahui 
topik 
pembicaraan  dan  bisa  menjadi  bahan  perbaikan  dari 
evaluasi tersebut. 

lebih  spesifik 

[1] 

[2] 

[3] 

[4] 

[5] 

[6] 

[7] 

[8] 

[9] 

[10] 

[11] 

[12] 

[13] 

DAFTAR PUSTAKA 

W. H. Organization, “WHO Coronavirus (COVID-19) Dashboard,” 
www.who.int, 2021. https://covid19.who.int/ (accessed Mar. 15, 
2021). 
KPCPEN, “Masyarakat dan Pemerintah Bersama Tekan Penularan 
COVID-19 Lewat Protokol Kesehatan,” covid19.go.id, 2021. 
https://covid19.go.id/p/berita/masyarakat-dan-pemerintah-bersama-
tekan-penularan-covid-19-lewat-protokol-kesehatan (accessed Mar. 
10, 2021). 
Pemerintah Pusat, “Peraturan Pemerintah Republik Indonesia 
Nomor 21 Tahun 2020,” Peraturan Pemerintah. 2021, doi: 
10.4324/9780367802820. 
E. K. Yazid and H. Palani, “Menakar Keberhasilan PSBB dalam 
Penanganan COVID-19: Data dan Peringatan bagi Pemerintah 
Daerah,” Csis, no. May, pp. 1–10, 2020, [Online]. Available: 
https://www.csis.or.id/publications/menakar-keberhasilan-psbb-
dalam-penanganan-covid-19-data-dan-peringatan-bagi-pemerintah-
daerah/%0Ahttps://www.csis.or.id/publications/menakar-
keberhasilan-psbb-dalam-penanganan-covid-19-data-dan-
peringatan-bagi-pemerint. 
Menteri Dalam Negeri, “Instruksi Menteri Dalam Negeri Nomor 03 
Tahun 2021.” 2021. 
H. Setkab, “Pemerintah Terapkan Kebijakan Pembatasan Kegiatan 
Masyarakat Pada 11-25 Januari 2021,” setkab.go.id, 2021. 
https://setkab.go.id/pemerintah-terapkan-kebijakan-pembatasan-
aktivitas-pada-11-25-januari-2021/ (accessed Mar. 21, 2021). 
CNN Indonesia, “PPKM Tak Efektif, Jokowi Instruksikan 
Pengetatan RT  dan RW,” www.cnnindonesia.com, 2021. 
https://www.cnnindonesia.com/nasional/20210203123329-20-
601716/ppkm-tak-efektif-jokowi-instruksikan-pengetatan-rt-dan-rw 
(accessed  Mar. 21, 2021). 
P. D. Jakarta, “JAKARTA TANGGAP COVID-19,” 
corona.jakarta.go.id, 2020. https://corona.jakarta.go.id/id (accessed 
Mar. 21, 2021). 
H. Setkab, “Tekan Kasus COVID-19, Pemerintah Terapkan PPKM 
Berbasis Mikro Mulai 9 Februari,” setkab.go.id, 2021. 
https://setkab.go.id/tekan-kasus-covid-19-pemerintah-terapkan-
ppkm-berbasis-mikro-mulai-9-februari/ (accessed Mar. 21, 2021). 
T. Maharani, “Airlangga Klaim Kasus Aktif Covid-19 Sepekan 
Turun Signifikan, hingga 17,27 Persen,” nasional.kompas.com, 
2021. 
https://nasional.kompas.com/read/2021/02/20/11513461/airlangga-
klaim-kasus-aktif-covid-19-sepekan-turun-signifikan-hingga-
1727?page=all (accessed  Mar. 10, 2021). 
Hootsuite; We Are Social, “DIGITAL 2021: INDONESIA,” 
datareportal.com, 2021. https://datareportal.com/reports/digital-
2021-indonesia (accessed  Jun. 04, 2021). 
F. F. Rachman and S. Pramana, “Analisis Sentimen Pro dan Kontra 
Masyarakat Indonesia tentang Vaksin COVID-19 pada Media 
Sosial Twitter,” Heal. Inf. Manag. J. ISSN, vol. 8, no. 2, pp. 2655–
9129, 2020, [Online]. Available: 
https://inohim.esaunggul.ac.id/index.php/INO/article/view/223. 
C. Collins, S. Hasan, and S. V. Ukkusuri, “A novel transit rider 
satisfaction metric: Rider sentiments measured from online social 

 10 / 10 

 
 
 
 
"
221709610,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pengembangan Fitur Pengajuan dan Persetujuan 
Proposal pada Sistem Informasi Unit Kegiatan 
Mahasiswa Politeknik Statistika STIS Berbasis Web 

Dea Anisa Irawan (221709610, 4SI1) 
Dosen Pembimbing: Yunarso Anang Sulistiadi, Ph.D. 

Ringkasan—  Untuk  melaksanakan  kegiatan,  UKM  (Unit 
Kegiatan  Mahasiswa)  atau  Bidang  membutuhkan 
izin 
berdasarkan  ketentuan  yang  berlaku.  Proses  pengajuan 
proposal/TOR yang saat ini berlaku membutuhkan banyak waktu 
dan  biaya. Sebelumnya,  sudah  ada  penelitian  yang  membangun 
sistem informasi berbasis web untuk mempermudah manajemen 
kegiatan  UKM/Bidang.  Namun,  fitur  yang  sudah  ada  dirasa 
belum  lengkap  untuk  menangani  pengajuan  dan  persetujuan 
proposal.  Berangkat  dari  permasalahan 
tersebut,  peneliti 
bermaksud  untuk  memperbaiki  dan/atau  menambahkan  fitur 
pada  sistem  sesuai  dengan  kebutuhan  pengguna,  salah  satunya 
ialah fitur pengajuan dan persetujuan proposal. Pengembangan 
sistem dilakukan dengan System Development Life Cycle (SDLC) 
Iterative  Model.  Sistem  dikembangkan  berbasis  web  dengan 
MySQL sebagai basis data, Codeigniter 3 sebagai kerangka kerja 
backend,  dan  Bootstrap  sebagai  frontend.  Uji  coba  dan  evaluasi 
sistem  dilakukan  dengan  Black  Box  Testing,  User  Acceptance 
Testing  (UAT),  dan  System  Usability  Scale  (SUS).  Subsistem 
pengajuan  tanda  tangan  proposal  kegiatan  mahasiswa  telah 
berhasil  dikembangkan.  Hasil  evaluasi  menunjukkan  subsistem 
yang  dikembangkan  sesuai  dengan  kebutuhan  pengguna  dan 
subsistem layak untuk digunakan. 

Kata Kunci— unit kegiatan mahasiswa, persetujuan, proposal, 

TOR 

I.  LATAR BELAKANG 

Politeknik Statistika STIS (selanjutnya disebut Polstat STIS) 
merupakan salah satu perguruan tinggi kedinasan di Indonesia 
yang  berada  di  bawah  Badan  Pusat  Statistik  (BPS).  Selain 
menjalankan kegiatan belajar mengajar yang diselenggarakan 
di kampus, mahasiswa Polstat STIS juga mengembangkan diri 
dengan  berbagai  macam  organisasi  dan  kegiatan  mahasiswa. 
Organisasi  mahasiswa  (Ormawa)  yang  ada  di  Polstat  STIS 
yaitu  Senat  Mahasiswa  (Sema)  dan  Dewan  Perwakilan 
juga  unit  kegiatan 
Mahasiswa  (DPM).  Selain 
mahasiswa  (UKM)  yang  berada  di  bawah  Sema.  Terdapat 
berbagai  bidang  yang  berada  di  bawah  UKM.  UKM/Bidang 
yang ada di Polstat STIS cukup beragam, mulai dari olahraga, 
kesenian,  kerohanian,  dan  lain  sebagainya.  UKM/Bidang 
tersebut merupakan wadah bagi mahasiswa untuk menjalankan 
hobi dan mengembangkan diri. 

itu,  ada 

Untuk  melaksanakan  kegiatan  di  kampus,  UKM/Bidang 
membutuhkan izin berdasarkan ketentuan yang berlaku. Selain 
juga  membutuhkan  dana  untuk 
itu,  beberapa  kegiatan 
menjalankan  kegiatannya,  terutama  kegiatan  yang  besar  dan 
melibatkan/mengajak  seluruh  mahasiswa  maupun  civitas 

akademika  lainnya.  Oleh  karena  itu,  UKM/Bidang  perlu 
mengajukan  perizinan  kegiatan  kepada  pihak  kampus. 
Perizinan  tersebut  dapat  berupa  proposal  ataupun  Term  of 
apabila  kegiatan 
Reference 
(TOR).  Proposal  dibuat 
UKM/Bidang  mencakup  eksternal  UKM/Bidang 
tersebut 
(seluruh mahasiswa, civitas akademika, atau pihak lain di luar 
Polstat  STIS)  dan  membutuhkan  dana  yang  bersumber  dari 
Imapolstat  atau  APBN.  Sedangkan  TOR  dibuat  apabila 
kegiatan  UKM/Bidang  mencakup 
internal  UKM/Bidang 
tersebut dan membutuhkan dana yang bersumber dari iuran/kas 
UKM/Bidang.  Perbedaan  lainnya  antara  proposal  dan  TOR 
terletak  pada  alur  pengajuan  persetujuannya.  Dalam  proses 
pengajuan  proposal,  proposal  dikoreksi  secara  berturut-turut 
oleh Ketua UKM, Sekretaris Sema, Bendahara Sema, dan DPM. 
Setelah  proposal  dikoreksi,  proposal  kemudian  dicetak  untuk 
ditandatangani  sebagai  bentuk  persetujuan.  Alur  pengajuan 
tanda  tangan  persetujuan  yaitu  Ketua  UKM,  Ketua  Sema, 
Dosen  Pendamping  UKM/Bidang,  Subkoor  Kemahasiswaan 
(BAAK), Bagian Umum (BU), dan Wakil Direktur III. Apabila 
telah  disetujui,  kegiatan  tersebut  dapat  dilaksanakan.  Alur 
pengajuan  TOR  kurang  lebih  sama,  hanya  saja  terdapat 
beberapa perbedaan. Pertama, TOR melalui proses koreksi oleh 
DPM. Kedua, TOR hanya membutuhkan tanda tangan sampai 
ke Dosen Pendamping UKM/Bidang. 

Proses  pengajuan  proposal/TOR  kegiatan  seperti  yang 
dijelaskan  di  atas  membutuhkan  banyak  waktu  dan  biaya. 
Waktu yang dibutuhkan dalam proses pengajuan proposal/TOR 
bergantung  pada  ketersediaan  pihak-pihak  terkait  di  tempat. 
Terlebih  lagi  dengan  adanya  pandemi  Covid-19  yang  terjadi 
saat 
tentu  saja  menghambat  pelaksanaan  kegiatan 
UKM/Bidang.  Kegiatan  UKM/Bidang  pun  mau  tidak  mau 
termasuk  pengajuan 
harus  dilaksanakan  secara  daring, 
proposal/TOR  kegiatan.  Untuk  mengatasi  hal 
tersebut, 
diperlukan  adanya  suatu  sistem  yang  mampu  memfasilitasi 
proses pengajuan proposal/TOR secara daring. 

ini 

Sebelumnya, sudah ada penelitian yang membangun sistem 
informasi  berbasis  web  untuk  mempermudah  manajemen 
kegiatan UKM/Bidang [1]. Sistem tersebut memiliki berbagai 
fitur  guna  menunjang  manajemen  kegiatan  UKM/Bidang,  di 
antaranya  yaitu  peminjaman  ruangan,  peminjaman  barang, 
presensi  kegiatan,  pengelolaan  proposal/TOR  dan  laporan 
informasi 
pertanggungjawaban,  pengarsipan,  berita,  serta 
UKM/Bidang 
pengelolaan 
keanggotaan. 
proposal/TOR dan laporan yang tersedia pada sistem tersebut 
hanya berfokus pada proses koreksi dan revisi proposal/TOR 

Fitur 

dan 

 1 / 8 

 
 
 
 
 
proposal/TOR, 

tersebut 
mengunggah 

dan laporan pertanggungjawaban oleh Sema sebelum nantinya 
terdiri  dari 
diajukan  ke  pihak  kampus.  Fitur 
laporan 
mengunggah 
pertanggungjawaban,  mengunduh 
ajuan  proposal/TOR, 
mengunduh  ajuan  laporan  pertanggungjawaban,  merevisi 
pertanggungjawaban, 
proposal/TOR,  merevisi 
laporan 
menyetujui 
pertanggungjawaban.  Berangkat  dari  permasalahan  tersebut, 
peneliti  bermaksud  untuk  mengembangkan  sistem  informasi 
yang sudah ada dengan menambahkan fitur penunjang lainnya 
yang dibutuhkan. 

dan  menyetujui 

proposal/TOR, 

laporan 

II.  TUJUAN PENELITIAN 

Penelitian  ini  bertujuan  untuk  mengevaluasi  sistem  yang 
sudah ada guna mengetahui penggunaannya dalam pelaksanaan 
kegiatan  mahasiswa  selama  Perkuliahan  Jarak  Jauh  (PJJ). 
Penelitian  ini  juga  bertujuan  untuk  memperbaiki  dan/atau 
menambahkan  fitur  pada  sistem  sesuai  dengan  kebutuhan 
pengguna, salah satunya ialah fitur pengajuan dan persetujuan 
proposal/TOR. 

III. PENELITIAN TERKAIT 

Nasrullah (2020) telah membangun sebuah Sistem Informasi 
Unit Kegiatan Mahasiswa  Politeknik Statistika STIS berbasis 
web yang dapat memenuhi kebutuhan-kebutuhan sistem [1]. 

Sadewa  dan  Siahaan  (2016)  melakukan  analisis  dan 
perancangan  Sistem  Informasi  Kegiatan  Mahasiwa  (UKM) 
berbasis web pada Universitas Batanghari. Penelitian tersebut 
menghasilkan sebuah prototipe sistem informasi unit kegiatan 
mahasiswa  berbasis  web  yang  menyediakan  layanan  berupa 
informasi  kegiatan  UKM,  anggota  dan  pengurus,  laporan 
rekapitulasi  kegiatan  dan  penggunaan  dana  serta  pendaftaran 
anggota secara daring [2]. 

Arhabi dan Ichdayanto (2013) membangun Sistem Informasi 
Unit  Kegiatan  Mahasiswa  Berbasis  Web  untuk  Japan  Genki 
Community.  Sistem  tersebut  adalah  aplikasi  yang  akan 
digunakan untuk membantu sistem di unit kegiatan mahasiswa, 
khususnya  Japan  Genki  Community.  Aplikasi  ini  dapat 
membantu  manajemen  aktivitas  kegiatan  mahasiswa  seperti 
manajemen  anggota  dan  komunitas,  manajemen  kegiatan, 
informasi mengenai Japan Genki Community dan lainnya [3]. 

IV. METODE PENELITIAN  

A.  Metode Analisis 

Pengembangan  sistem  dilakukan  dengan  System 
Development Life Cycle (SDLC) Iterative Model. Dalam 
mengembangkan sistem, basis data yang digunakan yaitu 
aplikasi  MariaDB  yang  dikembangkan  dari  MySQL. 
Administrasi basis data  dilakukan dengan menggunakan 
phpMyAdmin.  Pengelolaan  backend  dilakukan  dengan 
menggunakan  kerangka  kerja  Codeigniter  3  yang 
menggunakan  bahasa  pemrograman  PHP.  Sedangkan 
frontend  dibuat  dengan  menggunakan  kerangka  kerja 
Bootstrap. 

B.  Analisis Sistem Berjalan 

1.  Proses Bisnis di Sistem Informasi yang Sudah Ada 

Proses bisnis pada Gambar 1 merupakan proses bisnis 
informasi  yang  belum 

ada  pada 

sistem 

yang 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

diimplementasikan. Proses bisnis acuan yang digunakan 
dalam  pembangunan  sistem  merupakan  proses  bisnis 
yang  digunakan  pada  periode  kepengurusan  yang  lama, 
sehingga  proses  bisnisnya  tidak  sejalan  dengan  proses 
bisnis yang terbaru. Pada proses bisnis terbaru yang dibuat 
oleh  pengurus  Sema  periode  2020/2021,  pengoreksian 
proposal  dilakukan  tidak  hanya  oleh  Sekretaris  dan 
Bendahara Sema, tetapi juga oleh DPM. Sedangkan dalam 
proses  bisnis  sistem  yang  sudah  ada,  pengoreksian 
proposal  hanya  dilakukan  oleh  Sema.  Selain  itu,  sistem 
informasi  tersebut  hanya  memfasilitasi  sampai  proses 
pengoreksian  proposal,  sedangkan  proses  pengajuan 
proposal masih harus dilakukan secara manual. 

Gambar 1. Proses bisnis sistem yang sudah ada 

2.  Uji Coba Sistem Informasi yang Sudah Ada 

pengurus  UKM/Bidang 

Peneliti mengadakan uji coba terhadap fitur pengajuan 
proposal/TOR pada sistem informasi yang sudah ada. Uji 
coba  dilakukan  oleh  pengurus  Sema  dan  pengurus 
UKM/Bidang. Setelah uji coba  tersebut, pengurus Sema 
dan 
berpartisipasi 
memberikan umpan balik dengan mengisikan kuesioner. 
Dari umpan balik tersebut, masalah yang ditemukan pada 
sistem  tersebut  terkait  pengajuan  proposal  kegiatan 
mahasiswa yaitu: 
1)  Proposal harus diunduh sebelum dikoreksi. 
2)  Tidak  ada  keterangan  ekstensi  file  proposal/TOR 

yang 

yang dapat diunggah. 

3)  Ketika  menambahkan  kegiatan  gagal,  data  yang 
telah  dimasukkan  hilang  semua,  sehingga  harus 
diketik ulang. 
C.  Kebutuhan Sistem 

Berdasarkan  masalah-masalah  yang  dipaparkan  pada 
latar  belakang,  pengumpulan  data,  dan  analisis  sistem 
berjalan,  kebutuhan-kebutuhan  sistem  adalah  sebagai 
berikut. 
1.  Sistem  terdapat  pengoreksian  proposal  kegiatan 
mahasiswa sesuai dengan proses bisnis terbaru. 

2.  Sistem  terdapat  persetujuan  dan  tanda 

tangan 

persetujuan proposal/TOR kegiatan mahasiswa. 

 2 / 8 

 
 
 
3.  Sistem  mampu  digunakan  untuk  mengoreksi 
proposal/TOR  kegiatan  tanpa  perlu  mengunduh 
proposal/TOR. 

4.  Sistem 

terdapat 

keterangan 

ekstensi 

file 

proposal/TOR yang dapat diunggah. 

5.  Sistem  menyimpan  data  kegiatan  yang 

telah 
dimasukkan meskipun gagal menambahkan kegiatan. 

D.  Metode Pengujian dan Evaluasi 

Pengujian  dan  evaluasi  sistem  dalam  penelitian  ini 
dilakukan  dengan  menggunakan  tiga  metode,  yaitu 
sebagai berikut. 
1.  Black Box Testing 

Peneliti melakukan pengujian Black Box Testing untuk 
mengetahui  apakah  fungsi-fungsi  pada  subsistem  sudah 
berjalan  sesuai  dengan  yang  diharapkan.  Pengujian 
dilakukan  dalam  46  skenario  yang  mencakup  seluruh 
fungsi  yang 
terdapat  dalam  subsistem  pengajuan 
persetujuan proposal/TOR. 
2.  User Acceptance Testing (UAT) 

UAT  dilakukan  untuk  mengetahui  apakah  subsistem 
yang  telah  dikembangkan  sesuai  dengan  kebutuhan 
pengguna.  Peneliti  menggunakan  kuesioner  untuk 
mengumpulkan data. Pertanyaan yang diajukan di dalam 
kuesioner berupa test case yang disusun berdasarkan level 
pengguna. 
3.  System Usability Scale (SUS) 

SUS  digunakan  untuk  mengetahui  apakah  subsistem 
yang telah dikembangkan berguna dan mudah digunakan 
oleh  pengguna.  Peneliti  menggunakan  kuesioner  untuk 
mengumpulkan data. Data yang dikumpulkan mengikuti 
model  SUS  yaitu  dengan  menggunakan  sepuluh  item 
pertanyaan. 

V.  KERANGKA PIKIR 

Kerangka  pikir  merupakan  alur  pemikiran  peneliti  dalam 
melakukan  penelitian  ini,  seperti  yang  diilustrasikan  pada 
Gambar 2. Penelitian ini berawal dari ditemukannya  masalah 
terkait pengajuan proposal/TOR di masa pandemi. Sementara 
itu, Sistem Informasi UKM Polstat STIS yang telah dibangun 
belum  memfasilitasi  pengajuan  tanda  tangan  persetujuan 
proposal/TOR kegiatan mahasiswa. Berangkat dari hal tersebut, 
peneliti mengidentifikasi masalah-masalah yang terjadi dalam 
proses  pengajuan  tanda  tangan  persetujuan  proposal/TOR 
kegiatan  mahasiswa.  Untuk  mengatasi  masalah-masalah 
tersebut,  peneliti  mengusulkan  solusi  dengan  menambahkan 
tangan  persetujuan  proposal/TOR 
fitur  pengajuan 
kegiatan pada Sistem Informasi UKM Polstat STIS yang telah 
dibangun sebelumnya. Karena kebutuhan sistem sudah cukup 
jelas dan waktu pengerjaaannya relatif pendek, pengembangan 
sistem tersebut dilakukan dengan menggunakan metode SDLC 
model  iteratif.  Setelah  sistem  selesai  dikembangkan,  peneliti 
menguji  coba  dan  mengevaluasi  fitur  terbaru  pada  sistem 
dengan Black Box Testing dan Usability Testing. Berdasarkan 
hasil uji coba dan evaluasi, peneliti menarik kesimpulan terkait 
tercapai atau tidaknya tujuan penelitian dan memberikan saran-
saran  untuk  penelitian  dan/atau  pengembangan  sistem  lebih 
lanjut. 

tanda 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 2. Kerangka pikir 

VI. HASIL DAN PEMBAHASAN 

A.  Perancangan Sistem 

1.  Proses Bisnis Usulan 

Proses  bisnis  usulan  untuk  menambah  kegiatan  di 
dalam  sistem  tidak  jauh  berbeda  dengan  proses  bisnis 
yang  sudah  ada  di  dalam  sistem.  Hanya  saja,  ada 
tambahan proses dalam pengoreksian proposal/TOR dan 
pengajuan tanda tangan persetujuan proposal/TOR. Jadi, 
pada  awalnya  pengurus  UKM/Bidang  terlebih  dahulu 
menambah  kegiatan  dan  mengunggah  proposal/TOR. 
Setelah 
dan 
ditambahkan 
proposal/TOR  kegiatan  diunggah,  proses  selanjutnya 
ialah  pengoreksian  dan  persetujuan  proposal.  Alur 
pengoreksian dan persetujuan proposal secara garis besar 
terlihat pada Gambar 3. Sedangkan alur pengoreksian dan 
persetujuan TOR secara garis besar terlihat pada Gambar 
4. 

kegiatan 

sistem 

ke 

Gambar 3. Alur pengoreksian dan persetujuan proposal kegiatan 

apakah 

memutuskan 

Pengoreksian dan persetujuan pertama dilakukan oleh 
Ketua UKM. Ketua UKM akan membaca proposal/TOR, 
kemudian 
proposal/TOR 
memerlukan  revisi  atau  tidak.  Setelah  Ketua  UKM 
menyetujui 
selanjutnya 
proposal/TOR  akan  dikoreksi  oleh  Sekretaris  Sema. 
Setelah  Sekretaris  Sema  menyetujui  proposal/TOR 
kegiatan,  proposal/TOR  tersebut  akan  dikoreksi  oleh 
Bendahara  Sema.  Proses  pengoreksian  proposal/TOR 

proposal/TOR 

tersebut, 

 3 / 8 

 
 
 
 
oleh Ketua UKM, Sekretaris Sema, dan Bendahara Sema 
kurang lebih sama, seperti terlihat pada Gambar 5.\\\\ 

Gambar 4. Alur pengoreksian dan persetujuan TOR kegiatan 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Setelah  proposal  disetujui  oleh  DPM  ataupun  TOR 
disetujui  oleh  Bendahara  Sema,  proposal/TOR  akan 
masuk  ke  proses  pengajuan  persetujuan.  Pertama 
proposal/TOR  akan  dibaca  oleh  Ketua  Sema.  Apabila 
Ketua  Sema  menyetujui  proposal/TOR 
tersebut, 
proposal/TOR  kemudian  akan  dibaca  oleh  Dosen 
Pendamping  UKM/Bidang.  Apabila  Dosen  Pendamping 
menyetujui  TOR,  maka  TOR 
otomatis 
tertandatangani secara digital dan pengurus UKM/Bidang 
sudah  bisa  melaksanakan  kegiatan  tersebut.  Sedangkan 
apabila  Dosen  Pendamping  UKM/Bidang  menyetujui 
proposal, proposal selanjutnya akan dibaca oleh BAAK. 
Apabila  BAAK  menyetujui  proposal  tersebut,  proposal 
kemudian  akan  dibaca  oleh  BU.  Proses  persetujuan 
proposal  oleh  Ketua  Sema,  Dosen  Pendamping 
UKM/Bidang, BAAK, dan BU kurang lebih sama, seperti 
terlihat pada Gambar 7. 

akan 

Gambar 5. Proses bisnis usulan pengoreksian proposal/TOR oleh Ketua 
UKM, Sekretaris Sema, dan Bendahara Sema 

Apabila  Bendahara  Sema  telah  menyetujui  proposal 
tersebut,  DPM  akan  mengoreksi  proposal 
tersebut. 
Sedangkan  apabila  Bendahara  Sema  telah  menyetujui 
TOR, TOR akan masuk ke proses pengajuan persetujuan. 
Proses  pengoreksian  proposal  oleh  DPM  terlihat  pada 
Gambar 6. 

Gambar 6. Proses bisnis usulan pengoreksian proposal oleh DPM 

Gambar 7. Proses bisnis usulan persetujuan proposal/TOR oleh Ketua 
Sema, Dosen Pendamping UKM/Bidang, BAAK, dan BU 

Proposal yang telah disetujui oleh BU kemudian akan 
menunggu  persetujuan  dari  Wakil  Direktur  III.  Wakil 
Direktur III akan membaca proposal tersebut. Setelah itu, 
Wakil  Direktur  III  akan  memutuskan  untuk  menyetujui 
ataupun  menolak  proposal  tersebut.  Proses  persetujuan 
oleh Wakil Direktur III diilustrasikan pada Gambar 8. 

Seperti terlihat pada Gambar 5, Gambar 6, Gambar 7, 
dan  Gambar  8,  selain  memberikan  persetujuan,  pihak-
pihak  yang  terlibat  dalam  proses  pengoreksian  dan 
persetujuan  proposal  juga  dapat  memberikan  revisi 
proposal  apabila  dibutuhkan.  Apabila  salah  satu  pihak 
tersebut  memberikan 
pengurus 
revisi 
UKM/Bidang  harus  merevisi  proposal  tersebut  dan 
kemudian  mengunggah  proposal  yang  telah  direvisi, 
untuk  kemudian  masuk  kembali  ke  dalam  proses 
pengoreksian dan persetujuan proposal dari awal. 

proposal, 

Apabila  proposal  disetujui  oleh  Wakil  Direktur  III, 
proposal  akan  otomatis  tertandatangani  secara  digital. 
Pengurus  UKM/Bidang 
sudah  bisa  melaksanakan 
kegiatan  seperti  yang  telah  dijadwalkan.  Sementara  itu, 
apabila proposal ditolak oleh Wakil Direktur III, pengurus 
UKM/Bidang  dapat  menghapus  kegiatan  tersebut  dari 
sistem  dan  mengajukan ulang  atau  menambahkan ulang 
kegiatan tersebut di sistem. 

 4 / 8 

 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

berbeda-beda  menyesuaikan  dengan  status  kegiatan  dan 
level pengguna. 

Untuk  mengajukan  proposal/TOR  kegiatan,  pengurus 
UKM/Bidang terlebih dahulu menambahkan kegiatan ke 
dalam  sistem  informasi.  Tampilan  untuk  menambahkan 
kegiatan 
terlihat  pada  Gambar  10.  Field  untuk 
mengunggah  proposal/TOR  hanya  akan  muncul  jika 
memilih  Membutuhkan  Proposal  atau  Membutuhkan 
TOR pada field Jenis Kegiatan. Setelah melengkapi detail 
kegiatan  dan  menekan  tombol  Tambah,  kegiatan  akan 
ditambahkan  ke  basis  data  dan  proposal/TOR  akan 
otomatis terunggah. 

Gambar 10. Implementasi tampilan menambahkan kegiatan 

Setelah proposal/TOR terunggah, status kegiatan akan 
berubah  menjadi  Menunggu  Persetujuan  Proposal/TOR 
oleh Ketua UKM. Namun, jika dalam menambah kegiatan 
juga  sekaligus  meminjam  barang/ruang,  maka  statusnya 
akan  menjadi  Menunggu  Persetujuan  Peminjaman 
barang/ruang 
Barang/Ruang.  Setelah 
disetujui,  barulah  status  kegiatan  berubah  menjadi 
Menunggu Persetujuan Proposal/TOR oleh Ketua UKM. 
Tampilan  yang  muncul  ketika  Ketua  UKM  login  ke 
sistem kemudian menekan menu Panel Sema dan memilih 
tab Proposal/TOR terlihat pada Gambar 11. 

peminjaman 

Gambar 11. Implementasi tampilan pengoreksian proposal/TOR Ketua 
UKM 

Setelah  proposal/TOR  disetujui  oleh  Ketua  UKM, 
proposal/TOR akan masuk ke proses pengoreksian. Status 
kegiatan akan berubah menjadi Menunggu Pengoreksian 
Proposal/TOR  oleh  Sekretaris  Sema.  Apabila  Sekretaris 
Sema  login  ke  sistem,  kemudian  menekan  menu  Panel 
Sema  dan  memilih  tab  Proposal/TOR,  tampilannya 
kurang  lebih  sama  dengan  Ketua  UKM,  seperti  terlihat 
pada Gambar 12. Hanya saja, pada tampilan Ketua UKM, 
yang  muncul  hanyalah  proposal/TOR  dari  UKM  yang 
diketuainya.  Sedangkan  pada  tampilan  Sekretaris  Sema 
yang muncul proposal/TOR dari seluruh UKM/Bidang. 

 5 / 8 

Gambar  8.  Proses  bisnis  usulan  persetujuan  proposal  oleh  Wakil 
Direktur III 
2.  Rancangan Basis Data 

Basis  data  yang  dirancang  untuk  keseluruhan  sistem 
memiliki  23  entitas.  Rancangan  Entity  Relationship 
Diagram  (ERD)  basis  data  tersebut  seperti  terlihat pada 
Gambar 9. 

Gambar 9. Rancangan Entity Relationship Diagram (ERD) basis data 

B.  Implementasi Sistem 

Implementasi  basis  data  menggunakan  aplikasi 
MariaDB  yang  dikembangkan  dari  MySQL.  Tools  yang 
tersedia  pada  MariaDB  yang  digunakan  lebih  lanjut 
adalah phpMyAdmin untuk membuat struktur basis data. 
Untuk mengelola backend sistem menggunakan kerangka 
kerja  Codeigniter  3, 
sistem 
menggunakan Bootstrap. Berikut ini adalah tampilan hasil 
implementasi  fitur  persetujuan  proposal/TOR.  Tampilan 
untuk  pengoreksian  dan  persetujuan  proposal/TOR 
kurang  lebih  sama  untuk  setiap  level  pengguna.  Hanya 
saja, menu yang tersedia dan proposal/TOR yang tampil 

sedangkan 

frontend 

 
 
 
 
 
 
Gambar  12.  Implementasi 
Sekretaris Sema 

tampilan  pengoreksian  proposal/TOR 

Setelah proposal/TOR disetujui oleh Sekretaris Sema, 
status  kegiatan  akan  berubah  menjadi  Menunggu 
Pengoreksian  Proposal/TOR  oleh  Bendahara  Sema. 
Apabila  Bendahara  Sema  login  ke  sistem,  kemudian 
menekan  menu  Panel  Sema  dan  memilih 
tab 
Proposal/TOR,  tampilannya  kurang  lebih  sama  dengan 
Sekretaris Sema, seperti terlihat pada Gambar 12. 

Setelah  Bendahara  Sema  menekan  tombol  Setuju, 
status kegiatan akan otomatis berubah sesuai dengan jenis 
kegiatan. Apabila kegiatan menggunakan proposal, maka 
status  kegiatan  akan  berubah  menjadi  Menunggu 
Pengoreksian  Proposal  oleh  DPM.  Sedangkan  apabila 
kegiatan menggunakan TOR, maka status kegiatan akan 
berubah  menjadi  Menunggu  Persetujuan  Proposal/TOR 
oleh Ketua Sema. Ketika DPM login ke sistem, proposal 
kegiatan akan muncul di tampilannya seperti terlihat pada 
Gambar  13.  Setelah  DPM  mengoreksi  dan  menyetujui 
proposal  kegiatan,  status  kegiatannya  akan  berubah 
menjadi  Menunggu  Persetujuan  Proposal/TOR  oleh 
Ketua  Sema.  Apabila  Ketua  Sema  login  ke  sistem, 
tampilannya  kurang  lebih  sama  dengan  Sekretaris  dan 
Bendahara Sema seperti terlihat pada Gambar 12. 

Gambar 13. Implementasi tampilan persetujuan proposal DPM 

Setelah  proposal  disetujui  oleh  Ketua  Sema,  status 
kegiatan  akan  otomatis  berubah  menjadi  Menunggu 
Persetujuan  Proposal/TOR  oleh  Dosen  Pendamping. 
Ketika Dosen Pendamping UKM/Bidang login ke sistem, 
tampilannya  kurang  lebih  sama  dengan  DPM,  seperti 
terlihat pada Gambar 14. Hanya saja, pada tampilan DPM, 
yang  muncul  hanyalah  kegiatan  yang  menggunakan 
proposal.  Sedangkan  pada  tampilan  Dosen  Pendamping 
UKM/Bidang,  yang  muncul  hanya  kegiatan  dengan 
proposal  dan  kegiatan  dengan  TOR  dari  UKM/Bidang 
yang didampinginya. 

Ketika  Dosen  Pendamping  UKM/Bidang  menekan 
tombol  Setuju,  status  kegiatan  akan  berubah  sesuai 
dengan  jenis  kegiatan.  Apabila  kegiatan  menggunakan 
proposal,  maka  status  kegiatan  akan  berubah  menjadi 
Menunggu Persetujuan Proposal oleh BAAK. Sedangkan 
apabila  kegiatan  menggunakan  TOR,  maka  status 
kegiatan 
akan  berubah  menjadi  Kegiatan  Siap 
Dilaksanakan  dan  TOR  akan  tertandatangani  secara 
digital. Ketika BAAK login ke sistem dan menekan menu 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Persetujuan Proposal, akan muncul tabel berisi proposal 
yang perlu disetujui. Tampilannya terlihat seperti Gambar 
15. 

Gambar  14.  Implementasi  tampilan  persetujuan  Dosen  Pendamping 
UKM/Bidang 

Gambar 15. Implementasi persetujuan proposal BAAK 

Setelah  BAAK  menyetujui  proposal,  status  kegiatan 
akan  berubah  menjadi  Menunggu  Persetujuan  Proposal 
oleh BU. Apabila BU login ke sistem dan menekan menu 
Persetujuan Proposal, maka akan muncul tampilan seperti 
pada Gambar 16. 

Gambar 16. Implementasi tampilan persetujuan proposal BU 

Setelah  BU  menekan  tombol  Setuju,  status  kegiatan 
akan  otomatis  berubah  menjadi  Menunggu  Persetujuan 
Wakil  Direktur  III.  Ketika  Wakil  Direktur  III  login  ke 
sistem,  tampilan  yang  muncul  akan  terlihat  seperti 
Gambar 17. Apabila Wakil Direktur III menekan tombol 
Setuju, bukan hanya status kegiatan akan berubah menjadi 
Kegiatan  Siap  Dilaksanakan,  tetapi  proposal  juga  akan 
otomatis tertandatangani secara digital. 

Gambar 17. Implementasi tampilan persetujuan proposal Wakil Direktur 
III 

juga  mengusulkan 

Tanda  tangan  digital  yang  disisipkan  secara  otomatis 
baik ketika Dosen Pendamping UKM/Bidang menyetujui 
TOR  maupun  ketika  Wakil  Direktur  III  menyetujui 
proposal  berbentuk  Quick  Response  code  (QR  code). 
Sistem  akan  secara  otomatis  menyisipkan  tanda  tangan 
digital di halaman pertama proposal/TOR. Oleh karena itu, 
peneliti 
format 
proposal/TOR dengan memindahkan Lembar Pengesahan 
Proposal/TOR  ke  halaman  pertama  seperti  terlihat  pada 
Gambar  18  dan  Gambar  19.  Apabila  QR  code  yang 
disisipkan  dipindai,  akan  secara  otomatis  membuka  file 
proposal/TOR  di  sistem 
informasi  sebagai  bentuk 
verifikasi  bahwa  proposal/TOR  tersebut  benar-benar 
ditandatangani  melalui  sistem.  File  proposal/TOR  akan 
tampil  dengan  catatan  pihak  yang  memindai  QR  code 

perubahan 

 6 / 8 

 
 
 
 
 
 
 
 
sudah  login  ke  sistem.  Jika  belum,  maka  akan  terlebih 
dahulu diarahkan ke halaman login. 

Gambar 18. Proposal kegiatan yang telah ditandatangani secara digital 

Gambar 19. TOR yang telah ditandatangani secara digital 

Selama kegiatan masih berada dalam status Menunggu 
Pengoreksian atau Menunggu Persetujuan Proposal/TOR, 
pengurus  UKM/Bidang  dapat  mengirimkan  email 
pengingat atau notifikasi sesuai dengan status kegiatannya. 
Ketika pengurus UKM/Bidang login ke sistem, kemudian 
memilih  menu  Panel  UKM/Bidang  dan  memilih  tab 
Kegiatan, akan muncul tabel berisi kegiatan yang sedang 
dalam  proses  pengajuan  dan/atau  yang 
sedang 
dilaksanakan.  Setiap  kegiatan  memiliki  aksi  yang  dapat 
dilakukan,  salah  satunya  aksi  Kirim  Pengingat  untuk 
mengirimkan  email  notifikasi  sesuai  dengan  status 
kegiatannya.  Tampilannya  terlihat  seperti  pada  Gambar 
20. 

Gambar 20.  Implementasi  tampilan kegiatan dengan  status  Menunggu 
Pengoreksian atau Menunggu Persetujuan 

C.  Uji Coba dan Evaluasi Sistem 

1.  Black Box Testing 

Dari  46 

skenario  pengujian  yang  dilakukan, 
menunjukkan bahwa fungsi-fungsi dalam sistem berjalan 
sesuai dengan yang diharapkan. 

2.  User Acceptance Testing (UAT) 

Untuk  mengetahui  apakah  subsistem  yang 

telah 
dikembangkan  sesuai  dengan  kebutuhan  pengguna, 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

peneliti  melakukan  uji  coba  dan  evaluasi  dengan 
menggunakan  User  Acceptance  Test.  Uji  coba  dan 
evaluasi  dilakukan  oleh  23  responden  dengan  level 
pengguna  yang  berbeda-beda.  Di  antara  23  responden 
tersebut, 10 responden mewakili pengguna dari Pengurus 
UKM/Bidang,  7  responden  mewakili  pengguna  dari 
Ketua  UKM,  1  responden  mewakili  pengguna  dari 
Sekretaris  Sema,  2  responden  mewakili  pengguna  dari 
Bendahara  Sema,  1  responden  mewakili  pengguna  dari 
Ketua Sema, 1 responden mewakili pengguna dari DPM, 
dan  1  responden  mewakili  pengguna  dari  Dosen 
Pendamping UKM/Bidang. 

Test  case    yang  tertuang  di  dalam  kuesioner  terbagi 
menjadi lima berdasarkan level pengguna, yaitu Pengurus 
UKM/Bidang,  Ketua  UKM,  Pengurus  Sema,  DPM,  dan 
Dosen  Pendamping  UKM/Bidang.  Test  case  Pengurus 
UKM/Bidang  terdiri  dari  8  skenario  terkait  kebutuhan 
pengurus  UKM/Bidang  di  dalam  subsistem  pengajuan 
persetujuan  proposal/TOR.  Test  case  Ketua  UKM  dan 
Dosen Pendamping UKM/Bidang masing-masing  terdiri 
dari 6 skenario, sedangkan test case Pengurus Sema dan 
DPM masing-masing terdiri dari 5 skenario. 

Hasilnya,  dari  kelima  test  case  tersebut,  seluruh 
responden  dari  test  case  Ketua  UKM,  Pengurus  Sema, 
DPM,  dan  Dosen  Pendamping  UKM/Bidang  100% 
menjawab sesuai. Artinya, keseluruhan sistem yang telah 
dikembangkan  sesuai  dengan  kebutuhan  Ketua  UKM, 
Pengurus  Sema,  DPM,  dan  Dosen  Pendamping 
UKM/Bidang. Sementara untuk Pengurus UKM/Bidang, 
hasilnya seperti terlihat pada Tabel I sebagai berikut. 

TABEL I 
HASIL UJI UAT PENGURUS UKM/BIDANG 

No. 

(1) 
1 

2 

3 

4 

5 

6 

7 

8 

Skenario 

status 

revisi 
kegiatan 

(2) 
Ketika  saya  menambahkan  kegiatan, 
kegiatan muncul di tabel Kegiatan. 
Ketika  saya  hendak  mengunggah 
terdapat  keterangan 
proposal/TOR, 
ekstensi file yang dapat saya unggah. 
Ketika 
saya  mengunggah 
proposal/TOR, 
berubah. 
Ketika  saya  mengirim  pengingat, 
email pengingat berhasil terkirim. 
Ketika  saya  melihat proposal/TOR  di 
arsip, laman lihat proposal terbuka dan 
proposal/TOR 
telah 
ditandatangani terbuka sepenuhnya. 
Saya menerima email notifikasi ketika 
ada yang merevisi proposal/TOR. 
Saya menerima email notifikasi ketika 
Dosen Pendamping Bidang atau Wakil 
Direktur  III  menyetujui  TOR  atau 
proposal kegiatan. 
Saya menerima email notifikasi ketika 
Wakil Direktur III menolak proposal. 

yang 

Sesuai 

(3) 
100% 

Tidak 
Sesuai 
(4) 
0% 

100% 

0% 

100% 

0% 

100% 

0% 

90% 

10% 

60% 

40% 

60% 

40% 

50% 

50% 

Berdasarkan  tabel  tersebut,  terlihat  bahwa  terdapat 
empat  skenario  dengan  hasil  100%  sesuai,  sedangkan 
empat skenario lainnya hasilnya  bervariasi.  Hal  tersebut 
terjadi  karena  pelaksanaan  uji  coba  sistem  yang  tidak 
selesai  sampai  ke 
tahap  persetujuan  oleh  Dosen 
Pendamping UKM/Bidang atau Wakil Direktur III karena 

 7 / 8 

 
 
 
 
 
 
 
itu, 

tidak  ada  responden  dari  pengguna  BAAK,  BU,  dan 
Wakil  Direktur 
responden 
III.  Oleh  karena 
menyatakan bahwa skenario tersebut tidak sesuai. Meski 
begitu,  terdapat  responden  yang  menyatakan  bahwa 
skenario tersebut sesuai. Hal tersebut dikarenakan ketika 
pelaksanaan  uji  coba,  pengguna  yang 
tidak  ada 
respondennya  digantikan  oleh  orang  lain,  sehingga 
skenario tersebut dapat diselesaikan. Jika responden yang 
tidak  menyelesaikan  keseluruhan  tahapan  uji  coba  tidak 
diikutsertakan  dalam  analisis  hasil  UAT,  maka  hasilnya 
seluruh responden dari  test case Pengurus UKM/Bidang 
100% menjawab sesuai. 

Berdasarkan  keseluruhan  hasil  UAT  tersebut,  dapat 
dikatakan  bahwa  subsistem  secara  keseluruhan  sesuai 
dengan  kebutuhan  pengguna  dalam  hal  ini  Pengurus 
UKM/Bidang,  Ketua  UKM,  Pengurus  Sema,  dan  DPM. 
Sedangkan  kesesuaian  kebutuhan  pengguna 
lainnya 
seperti  Dosen  Pendamping  UKM/Bidang,  BAAK,  BU, 
dan Wakil Direktur III tidak dapat diukur karena tidak ada 
responden dari pengguna tersebut. 
3.  System Usability Scale (SUS) 

Penilaian kegunaan dari subsistem yang dikembangkan 
melalui  metode  System  Usability  Testing 
(SUS) 
melibatkan 23 responden yang sama dengan UAT. Hasil 
perhitungan  skor  SUS  dari  jawaban  responden  seperti 
terlihat pada Tabel II sebagai berikut. 

Responden 

(1) 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 

10 

9 

1 

2 

8 

3  4  5 

TABEL II 
HASIL UJI SUS TERHADAP SISTEM USULAN 
Pertanyaan 
7 
6 
(2) 
2 
5  2  4 
3 
2 
4  1  3 
2 
2 
4  3  2 
2 
2 
3  5  4 
5 
4 
4  1  4 
3 
1 
5  2  5 
1 
3 
4  2  4 
2 
3 
4  2  4 
3 
2 
4  3  4 
2 
1 
5  4  5 
1 
4 
4  2  4 
2 
1 
5  2  5 
2 
2 
4  2  4 
2 
3 
3  2  4 
3 
3 
4  2  4 
3 
4 
4  4  5 
5 
3 
5  3  4 
3 
1 
4  1  4 
1 
4 
4  4  4 
4 
2 
5  1  4 
2 
2 
4  1  4 
2 
2 
5  2  4 
2 
1 
1 
5  1  2 
Rata-rata skor SUS 

2 
4 
1 
2 
2 
1 
3 
3 
2 
1 
2 
1 
2 
2 
4 
2 
4 
1 
4 
1 
2 
2 
1 

5 
5 
4 
5 
5 
5 
4 
5 
4 
5 
4 
5 
4 
4 
3 
5 
4 
5 
4 
4 
4 
5 
4 

5 
4 
4 
4 
5 
5 
4 
5 
4 
5 
4 
5 
4 
3 
4 
5 
4 
5 
4 
4 
4 
5 
5 

5 
4 
5 
4 
5 
5 
4 
4 
5 
5 
5 
5 
4 
4 
5 
5 
5 
5 
4 
4 
5 
4 
5 

4 
1 
5 
5 
1 
1 
2 
4 
4 
3 
2 
3 
2 
3 
2 
2 
3 
1 
3 
1 
2 
3 
1 

Skor 
SUS 
(3) 

77,5 
75 
65 
52,5 
80 
97,5 
70 
67,5 
70 
87,5 
72,5 
90 
75 
62,5 
65 
67,5 
65 
95 
52,5 
85 
80 
80 
90 
74,8913 

Berdasarkan hasil dari perhitungan skor SUS tersebut, 
diperoleh  skor  74,891304  yang  mengindikasikan  bahwa 
subsistem layak digunakan karena berada di atas  cut off 
point  SUS  yaitu  68.  Namun,  jika  melihat  lebih  jauh 
jawaban-jawaban yang diberikan oleh responden, terlihat 
bahwa  beberapa  responden  memberikan  jawaban  yang 
jauh  berbeda  dari  sebagian  besar  responden  lainnya, 
terutama  pada  pertanyaan-pertanyaan  bernomor  genap. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Responden dengan beberapa jawaban yang jauh berbeda 
dari sebagian besar responden lainnya yaitu Responden 4, 
tanpa 
16,  dan  19.  Apabila  skor  SUS  dihitung 
mengikutsertakan  ketiga  responden  tersebut,  maka  skor 
akhirnya menjadi 77,5. 

PENUTUP 
Kesimpulan  dari  hasil  penelitian 

VII. 

ini 

ialah  subsistem 
pengajuan  tanda  tangan  proposal  kegiatan  mahasiswa  telah 
berhasil  dikembangkan.  Berdasarkan  Black  Box  Testing, 
fungsi-fungsi  di  dalam  subsistem  yang  dikembangkan  sudah 
sesuai dengan yang diharapkan. Berdasarkan User Acceptance 
Testing  (UAT),  fungsi-fungsi  di  dalam  subsistem  yang 
dikembangkan  secara  keseluruhan  sesuai  dengan  kebutuhan 
pengguna, dalam hal ini Pengurus UKM/Bidang, Ketua UKM, 
Pengurus Sema, DPM, dan Dosen Pendamping UKM/Bidang. 
Sedangkan  kesesuaian  kebutuhan  pengguna  lainnya  seperti 
BAAK, BU, dan Wakil Direktur III tidak dapat diukur karena 
tidak  ada  responden  dari  pengguna  tersebut.  Berdasarkan 
(SUS), 
evaluasi  menggunakan  System  Usability  Scale 
subsistem yang dikembangkan layak digunakan. 

Peneliti  menyadari  terdapat  beberapa  keterbatasan  dari 
subsistem  yang  dikembangkan.  Oleh  karena 
itu,  untuk 
penelitian  lebih  lanjut,  sebaiknya  menambahkan  kebutuhan-
kebutuhan  dari  pengguna  lainnya  yang  terlibat  dalam  proses 
persetujuan  proposal  selain  dari  pengurus  UKM/Bidang  dan 
pengurus  Sema.  Sistem 
login  sebaiknya  menggunakan 
otentikasi  akun  Sipadu  Politeknik  Statistika  STIS.  Alur 
pengajuan  persetujuan  proposal/TOR  sebaiknya  dikaji  lebih 
lanjut untuk menemukan kemungkinan masalah di dalam alur 
tersebut  dan  menentukan  solusi  untuk  alur  pengajuan 
persetujuan proposal/TOR yang lebih efektif dan efisien. Selain 
itu,  sebaiknya  menambahkan  berbagai  kebutuhan  sistem 
lainnya  yang  tidak  tercakup  di  penelitian  ini  dan  penelitian 
sebelumnya. 

DAFTAR PUSTAKA 
[1]  Nasrullah,  Sistem  Informasi  Unit  Kegiatan  Mahasiswa  Politeknik 
Statistika STIS Berbasis Web [Skripsi], Jakarta: Politeknik Statistika STIS, 
2020. 

[2]  I.  Sadewa dan K.  Siahaan, “Analisis  dan  Perancangan  Sistem  Informasi 
Unit  Kegiatan  Mahasiswa  (UKM)  Berbasis  Web  pada  Universitas 
Batanghari,” Jurnal Manajemen Sistem Informasi, vol. 1, no. 2, pp. 135-
146, Des 2016. 

[3]  H. F. Arhabi dan M. D. Ichdayanto, Mse, “Sistem Informasi Unit Kegiatan 
Mahasiswa Berbasis Web Studi Kasus Japan Genki Community,”  Tugas 
Akhir  Fakultas  Ilmu  Terapan  Program  Studi  D3  Teknik  Informatika 
Universitas Telkom, pp. 13-42, 2013. 

 8 / 8 

 
 
 
 
"
221709609,"Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik 

 Sentimen Analysis dan Topic Modelling pada Tweet 
terkait Vaksinasi COVID-19 di Indonesia  

Dea Aditya (221709609, 4SD2) 
Dosen Pembimbing: Nori Wilantika, S.S.T. , M.T.I. 

  COVID-19  di 

terkait  vaksinasi 

Ringkasan—  Kumpulan  tweet  masyarakat  terkait  vaksinasi 
COVID-19 di Indonesia dapat menjadi sumber knowledge untuk 
memberikan gambaran respon dan opini mereka terkait vaksinasi 
COVID-19  di  Indonesia.  Mengidentifikasi  respon  dan  opini 
melalui media sosial Twitter yang penggunaanya masif dari waktu 
ke  waktu  tentunya  membuat  pola  tweet  semakin  beragam, 
sehingga  cukup  sulit  jika  dilakukan  dengan  manual.  Untuk 
menyelesaikan  masalah 
tersebut,  peneliti  menggunakan 
pendekatan dengan text mining. yaitu menerapkan topic modeling 
dan  sentiment  analysis.  Topic modeling dengan  algoritma  Latent 
Dirichlet  Allocation  digunakan  memodelkan  opini  dari  tweet 
masyrakat 
Indonesia. 
Selanjutnya, penggunaan sentiment analysis dengan lexicon based 
untuk  mengukur  sentimen/respon  tweet  masyarakat  terkait 
vaksinasi  COVID-19.    Hasil  sentiment  analysis  menunjukkan 
bahwa  respon  masyarakat  selama  perode  perdana  vaksinasi 
COVID-19  didominasi  oleh  sentimen  netral.  Walaupun  respon 
didominasi sentimen netral, jumlah sentimen positif masih lebih 
banyak  dari  sentimen  negatif.  Kemudian,  hasil  topic  modeling 
menunjukkan  walaupun  program  vaksinasi  COVID-19  sudah 
dilakukan,  opini  sebagian  masyarakat  masih  ragu  dan  menolak 
terhadap  vaksinasi  COVID-19  karena  dipengaruhi  beberapa 
faktor diantaranya masalah keamanan dan status kehalalan dari 
vaksin  yang  diberikan.  Selain  itu,  model  topik  juga  menangkap 
opini terkait kasus penyebaran berita hoax dan penipuan terkait 
vaksinasi COVID-19.  
Kata  Kunci—  topic  modeling,  sentiment  analysis,  coherence, 
knowledge 

I.  LATAR BELAKANG 
Wabah  Coronavirus  2019  (COVID-19)  merupakan  penyakit 
menular yang telah menjadi pandemi di seluruh dunia. SARS-
CoV-2    yang  ditemukan  di  Wuhan  merupakan  patogen  jenis 
baru  penyebab COVID-19 [1]. Wabah COVID-19 secara resmi 
dinyatakan sebagai pandemi oleh Organisasi Kesehatan Dunia 
WHO pada 11 Maret 2020.  Berdasarkan data yang diperoleh 
dari  South  Morning  China  Pozzst,  kasus  pertama  COVID-19 
ditemukan  di  Wuhan  pada  akhir  November 2019  dan  setelah 
itu,  kasus  ini  menyebar  dengan  cepat  tidak  hanya  di  China 
tetapi  juga  secara  global  [2]  Terhitung  hingga  tanggal  17 
Januari  2021  sekitar    95,7  juta  kasus  orang  yang  telah 
terkonfirmasi terpapar COVID-19 di seluruh dunia dan angka 
kematian  mencapai  2  juta  kasus,  sedangkan  untuk  Indonesia 
masuk dalam 20 besar negara dengan jumlah kasus penularan 
tertinggi dengan jumlah sekitar 907 ribu kasus [3] . Tingginya 
angka  kejadian  COVID-19  tentunya  mengguncang  seluruh 
masyarakat di dunia termasuk di Indonesia. Hal tersebut juga 
membuat  Pemerintah  Indonesia  telah  melakukan  beberapa 
kebijakan  untuk  mengatasi  penyebaran  wabah  COVID-19  di 
Indonesia,  diantaranya  adalah  dengan  memberlakukan 
Pembatasan  Sosial  Berskala  Besar  (PSBB)  di  tiap  daerah. 
Namun,  pada  saat  yang  sama  justru  pemberlakuan  kebijakan 

tersebut memilki dampak yang buruk kepada kehidupan sosial 
dan  kegiatan  ekonomi  masyarakat.  Pelaksanaan  social 
distraction atau work from home menjadi salah satu penyebab 
krisis ekonomi pada masa pandemi COVID-19 [4] 

Jumlah kasus COVID-19 dari waktu-waktu semakin tinggi 
dan  dampak  masalah  yang  ditimbulkanya  terhadap  sektor 
ekonomi dan sosial membuat wabah ini harus ditangani dengan 
segera. Pembuatan vaksin oleh para ilmuwan merupakan salah 
satu cara dalam pencegahan penyebaran wabah yang semakin 
meluas [5]. Oleh karena itu, diharapkan dengan adanya vaksin 
aktivitas sosial dan ekonomi masyarakat dapat berjalan dengan 
normal.    Pembuatan  rencana  vaksinasi  yang  tepat  diperlukan 
untuk  membantu  bagaimana  nantinya  vaksin 
ini  dapat 
digunakan secara terorganisir dan aman. Pemerintah Indonesia 
melalui  Menteri  Kesehatan  Budi  Gunadi 
Sadikin 
mempercepatan  pelaksanaan  vaksinasi  dengan  melakukan 
koordinasi  dengan  beberapa  perusahaan  penyedia  vaksin  dan 
Pemerintah  menargetkan  akan  melakukan  vaksinasi  dengan 
400 juta dosis vaksin dalam dua periode, periode I dimulai dari 
Januari-April  2021,  sedangkan  periode  II  dimulai  dari  April 
2021 hingga Maret 2022 mendatang[6]. 

Indonesia.  Opini  dan 

Opini dan respon masyarakat terkait vaksinasi COVID-19 
di  Indonesia  dapat  menjadi  sumber  knowledge  yang  penting 
untuk  diketahui  oleh  para  pemerintah  dalam  memberi 
gambaran tentang situasi dan kondisi yang dialami masyarakat 
selama  proses  vaksinasi  COVID-19   di Indonesia.  Selain  itu, 
opini  dan  respon  tersebut  juga  dapat  dijadikan  sumber 
knowledge  untuk  memberikan  pemahaman  tentang  penilaian 
masyarakat  terhadap  pelaksaanaan  vaksinasi  COVID-19  di 
Indonesia  yang dapat  dijadikan  bahan  bagi  pemerintah untuk 
mengidentifikasi  adanya  indikasi  masalah  ataupun  sebagai 
pertimbangan  untuk  bahan  evaluasi  selama  pelaksanaan 
vaksinasi  COVID-19  di 
respon 
masyarakat  terhadap  vaksinasi  COVID-19  tersebut  tentunya 
berbeda-beda,  ada  yang  mendukung/positf,  menolak/negatif 
ataupun memilih bersikap netral. Saat ini kemajuan teknologi 
informasi membuat banyak orang menggunakan media sosial 
sebagai  platform  popular  dalam  mengemukakan  opini  atau 
pendapat mereka terkait suatu kejadian. Hal ini sesuai dengan 
yang  diungkapkan  oleh  [7],  bahwa  media  sosial  diguanakan 
sebagai platform paling populer untuk mendapatkan informasi 
tentang dunia dan penggunaanya semakin masif dari ke hari ke 
hari.  Media  sosial  merupakan  platform  yang  ampuh  dalam 
memberikan  akurasi  dan  kesegeraan 
real time [8].  Twitter adalah salah satu media sosial berbasis 
microblogging  populer  yang  digunakan  oleh  banyak  orang 
untuk saling mengungkapkan pendapat mereka terhadap topik 
yang sedang dibahas. Menurut riset yang dilakukan manajemen 
media  sosial  HootSuite  dan  agensi  marketing  sosial  We  Are 
Social,  pada  Januari  2020  tercatat  jumlah  pengguna  media 

informasi  secara                   

 1 / 10 

 
 
 
 
 
sosial  di  Indonesia  telah  mencapai  160  juta  pengguna  dan 
meningkat 12 juta (8,1 persen) antara April 2019 dan Januari 
2020 [9]. 

Pengumpulan  data  opini  dan  respon  masyarakat/publik 
melalui media sosial Twitter merupakan sebuah terobosan baru 
untuk  menggantikan  pengumpulan  data  survey  konvesional. 
Kecilnya biaya dan kecepatan dalam memeperoleh data secara 
real-time,  membuat  pengumpulan  data  opini  dan  respon 
melalui  media  sosial  lebih  efisien  dibandingkan  survey 
konvesional.  Opini  dan  respon  masyarakat  terkait  vaksinasi 
COVID-19  di  Indonesia  pada  media  sosial  Twitter  dapat 
menjadi sumber data real time untuk menggambarkan kondisi 
dan  situasi  yang  dialami  masyarakat  selama  prose  vaksinasi 
COVID-19. Namun, penggunaanya yang masif dari waktu ke 
waktu  membuat  pola  tweet  semakin  beragam,  sehingga  sulit 
untuk mengidnetifikasi  informasi opini dan respon  dari  tweet 
tersebut. Maka, penelitian ini mencoba untuk mengidentifikasi 
informasi  dari  kumpulan  tweet  yang  banyak,  sehingga  dapat 
membantu  dalam  memberi  knowledge  bermanfaat    terkait 
kondisi dan situasi selama pelaksanaan vaksinasi COVID-19  di 
Indonesia. 

II.  TUJUAN PENELITIAN 

Tujuan  dari  penelitian  ini  adalah  melakukan  analisis 
sentimen untuk mengukur respon masyarakat terkait vaksinasi 
COVID-19 di Indonesia. Kemudian, penelitian ini menerapkan 
metode  Latent  Dirichlet  Allocation  untuk  meengelompokkan 
opini  masyarakat  terkait  vaksinasi  COVID-19  di  Indonesia 
pada perbincangan di media sosial Twitter. Melalui respon dan 
opini  masyarakat  terhadap  vaksinasi  COVID-19  di  Indonesia 
diharapkan dapat memberi knowledge baru untuk mengetahui 
kondisi dan situasi selama vaksinasi COVID-19 di Indonesia. 
Selain  itu,  dapat  dijadikan  sebagai  bahan  penilaian    dan 
evaluasi  pemerintah  selama  proses  vaksnasi  COVID-19  di 
Indonesia. 

III. Penelitian Terkait 

Beberapa penelitian telah menerapkan topic modelling untuk 
mengungkap topik dari kumpuan  tweet terkait suatu kejadian 
tertentu.  Seperi  penelitian  yang  dilakukan  oleh  [10]  yang 
melakukan  analisis  teks  untuk  mengetahui  persepsi  publik 
terhadap  COVID-19  pada  media  sosial  Twitter  dengan 
menerapkan topic modelling dan analisis sentimen. Data tweet 
yang berhasil dikumpulkan adalah 107.990 tweet dari periode 
13  Desember  hingga  9  Maret  2020  dengan  menggunakan 
Twitter  API.  Adapun  kata  kunci  yang  digunakan diantaranya 
adalah  ""coronavirus"", ""covid_19"", ""2019-nCov"" dan ""COVID-
19"".  Hasil penelitian ini menunjukkan bahwa  dengan analisis 
sentimen  dan  emosi  menunjukkan  bahwa  masyarakat 
cenderung  memiliki  pandangan  negatif  terhadap  COVID-19. 
Kemudian,  hasil  topic  modelling  terkait  pandemi  COVID-19 
dengan  algoritma  Latent  Dirichlet  Allocation  dibagi  menjadi 
tiga  kategori  topik.  Topik  pertama  membicarakan  kondisi 
darurat yang ditimbulkan oleh pandemi COVID-19, kemudian 
situasi  wabah  dan  cara 
topik  kedua  membicarakan 
penanggulangannya,  dan  topik  ketiga  membicarakan  tentang 
pemberitaan dan pelaporan pada  media sosial tentang wabah. 

Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik 

Selain  itu,  penelitian  ini  juga  menunjukkan  bahwa  Twitter 
menjadi  media  sosial  yang  sangat  berguna  sebagai  saluran 
komunikasi  publik  dalam  memahami  kesadaran  publik 
terhadap pandemi COVID-19.  

topik 

sebelumnya 

Selanjutnya,  beberapa  penelitian 

telah 
melakukan  analisis  terhadap  opini  publik  terkait  pengadaan 
vaksin  COVID-19 dengan pendekatan  text mining. Penelitian 
pertama  adalah  peneltian  yang  dilakukan  oleh  [11]  tentang 
monitoring persepsi terhadap vaksin COVID-19 dengan topik 
model di Amerika Serikat. Penelitian ini menggunakan Latent 
Dirichlet  Allocation  untuk  memodelkan 
terkait 
pandangan  publik  pada  media  sosial  Twitter  terhadap  vaksin  
COVID-19  .  Tweet  dikumpulkan  pada  20  Mei  beberapa  hari 
setelah  pengumuman  dari  Presiden  Trump  terkait  ""Operasi 
Warp  Speed""  .  Proses  topic  modelling  dilakukan  secara 
terpisah,  yaitu  memodelkan  topik  untuk  tweet  terkait  anti- 
vaksin  dan        pro-vaksin.  Tweet  terkait  anti-vaksin  diperoleh 
melalui pencarian berdasarkan hashtag yang berkaitan dengan 
penolakan 
dan 
#vaccinesaregenocide,  sedangkan  tweet  terkait  pro-vaksin 
diperoleh  melalui  pencarian  berdaasarkan  hashtag  yang 
berhubungan  dengan  dukungan 
seperti 
#Vaccine4results  dan  #vaccineswork.  Hasil  penelitian  ini 
menunjukkan  bahwa  LDA  berhasil  mengekstrak  topik  dari 
masing-masing kategori. Tweet terkait anti-vaksin terdiri atas 4 
topik perbincangan, yaitu keamanan, konspirasi, ideologi, dan 
hilangnya  kebebasan  individu,  sedangkan  tweet  terkait  pro-
vaksin terdiri atas 3 perbincangan. 

#novaccinemandate 

atas  vaksin, 

vaksin, 

seperti 

Selanjutnya  adalah  penelitian  yang  dilakukan  oleh  [12] 
tentang analisis sentimen pro dan kontra masyarakat Indonesia 
tentang  vaksin  COVID-19  pada  media  sosial  Twitter. 
Penelitian  ini  menggunakan  Analisis  sentimen  dan  topic 
modelling Latent Dirichlet Allocation untuk mengetahui opini 
dan reaksi publik terkati rencana pengadaan vaksin COVID-19. 
Di  Indonesia.  Data  yang  dianalisis  adalah  tweet  sebelum 
diberlakukanya  proses  vaksinasi  COVID-19  di  Indonesia 
dengan  total  tweet  yang  terkumpul  adalah  5583  tweet.  Hasil 
penelitian  ini  menunjukkan  bahwa  sentimen  positif  lebih 
dominan  dibandingkan  sentimen  negatif  dan  model  Latent 
Dirichlet  Allocation  berhasil  mengungkap  pandangan 
masyarakat terkait  vaksin COVID-19 di Indonesia, diantaranya  
vaksin  merah  putih,  sertifikasi  halal  vaksin,  uji  layak  pakai 
vaksin,  harga  vaksin,  sampai pembicaraan  umum masyarakat 
seperti fungsi & objek vaksinasi.  

Berdasarkan  penelitian  yang dilakukan  sebelumnya,  masih 
respon 
sedikit  penelitian  yang  menungkap  opini  dan 
masyarakat setelah diberlakukanya vaksinasi perdana COVID-
19  di  Indonesia.  Adanya  kemungkinan  perbedaan  opini  dan 
respon  masyarakat  sebelum  dan  setelah  diberlakukannya 
vaksinasi  perdana  COVID-19  di  Indonesia  berpotensi  dapat 
menghasilkan  knowledge  baru  untuk  mengetahui  kondisi  dan 
situasi  selama  vaksinasi  COVID-19  di  Indonesia.  Selain  itu, 
dapat  dijadikan  sebagai  bahan  penilaian  dan  evaluasi 
pemerintah selama proses vaksnasi COVID-19 di Indonesia. 

 2 / 10 

 
 
 
 
 
 
IV. METODE PENELITIAN  
Gambaran  umum  rencana  penelitian  yang  akan  dilakukan 

disajikan pada gambar 1. 

Gambar 1. Metodologi Penelitian 

Tahap  awal  yang  dilakukan  adalah  dengan  melakukan 
pengumpulan  data  dengan  crawling  data  tweet.  Metode 
pengumpulan data memanfaatkan package twint pada python.   
Data tweet pada penelitian ini diperoleh berdasarkan kata kunci 
‘Vaksin Covid-19’ khusus berbahasa Indonesia. Pengumpulan 
data  dilakukan  per  minggu  sejak  periode  awal  vaksinasi 
COVID-19 di Indonesia dari 13 Januari 2021 hingga 30 April 
2021.  

Data  yang  berhasil  dikumpulkan  selanjutnya  dilakukan 
filtering bahasa (hanya tweet berbahasa Indonesia yang dipilih). 
Proses filtering bahasa dilakukan dengan menggunakan library 
langdetect.  Kemudian  akan  dilakukan  validasi  kembali 
terhadap tweet yang terklasifikasi sebagai bahasa asing secara 
manual. Hal tersebut dilakukan untuk memastikan bahwa tweet 
yang 
sebagai  bahasa  asing  benar-benar 
didominasi oleh kata  asing atau justru masih didominasi kata 
Indonesia.  Ketika  hasil  pengecekan  menunjukkan  manual 
bahwa tweet yang terklasifikasi sebagai bahasa asing tersebut 
masih  didominasi  oleh  kata-kata  asing,  maka  tweet  tersebut 
dibuang, begitupun sebaliknya.  

terklasifikasi 

Kemudian tahap selanjutnya adalah membuang tweet yang 
bukan berasal dari akun individu/masyarakat. Fokus penelitian 
ini  hanya  menganalisis  opini  dari  akun  individu/masyarakat, 
sehingga pada tahap ini semua tweet yang bukan berasal dari 
akun  individu  akan  dibuang.  Filtering  akun  non-individu 
dilakukan dengan membuat kamus  untuk mendaftarkan akun 
kedalam  beberapa  kategori  ,  diantaranya  adalah  akun  media, 
akun  pemerintah,  dan  swadaya  masyarakat,  kemudian 
dilakukan  labelling  dengan  potongan  program  yang  dibuat 
melalui  bahasa  pemrograman  python.  Setelah  proses 
pengkategorian  akun  selesai,  maka  akun  yang  masuk  dalam 
kategori  media,  pemerintah,  dan  swadaya  masyarakat  akan 
dibuang.  Kemudian,  dilakukan  pengecekan  manual  untuk 
memastikan lagi akun non-individu yang tidak tercakup dalam 

Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik 

kamus.  Pada  tahap  ini,  nama  akun  yang  mengandung  kata 
berhubungan  dengan  akun  media,  pemerintah,  dan  swadaya 
masyarakat  akan  dibuang.  Beberapa  contoh  kata  tersebut 
diataranya  adalah  ‘kabar’,  ‘info’,  ‘tv’,  ‘com’  ,  ‘_id’,  ‘news’, 
‘berita’,  ‘koran’,  ‘radio’,  ‘media’,  ‘terkini,  ‘update’,  ‘cerita’, 
‘24jam’, ‘redaksi’, ‘times’, ‘headline’, ‘_net’. Selain itu seluruh 
tweet repost dari akun non-individu juga dibuang.  

Setelah  data 

tweet  yang  bukan  berasal  dari  akun 
individu/masyarakat berhasil dibuang, maka tahap selanjutnya 
adalah  melakukan  persiapan  data  atau  preprocessing  data. 
Tahap pertama adalah cleaning data tweet yang bertujuan untuk 
membersihkan data  yang tidak diperlukan dalam analisis dan 
membuat data lebih terstruktur, sehingga data siap digunakan 
dalam  proses  analisis  selanjutnya.  Ada  beberapa  tahap  yang 
dilakukan  pada  proses  ini,  yaitu  case  foding,  menghilangkan 
link  url  ,  menghilangkan  hashtag,  menghialngkan  beberapa 
karakter  (mention,  emoji,  angka,  dan  spasi  ganda),  dan 
menghilangkan  punctuation  (tanda  baca).  Setelah  dilakukan 
cleaning  data  tweet,  tahap  selanjutnya  adalah  melakukan 
transformasi data tweet. Pada tahap ini, proses yang dilakukan 
pertama  kali  adalah  tokenizing,  yaitu  proses  yang  dilakukan 
untuk  memecah  data  tweet    kedalam  bentuk  token  kata. 
Selanjutnya,  hasil  dari  proses  tokenizing  ini  akan  digunakan 
pada proses normalisasi, penghapusan stopword, dan stemming. 
Proses  normalisasi  digunakan  untuk  mengubah  bentuk  kata 
kedalam bentuk baku dan kamus yang digunakan adalah kamus 
yang  dibuat  oleh  Salsabila,  2018  [13].  Kemudian  tahap 
penghapusan stopword  digunakan untuk menghapus kata-kata 
yang  tidak  memiliki  makna  berarti.  Adapun  kamus  stopword 
bahasa  Indonesia  yang  digunakan  adalah  berasal  dari  library 
NLTK.  Setelah  dilakukan  proses  stopword,  tahap  akhir  dari 
proses transformasi data tweet ini adalah proses stemming atau 
proses  mengubah  kata  kedalam  bentuk  kata  dasar.  Proses 
stemming  memanfaatkan  kamus  yang  berasal  dari  library 
Sastrawi.  [14].  Setelah  semua  data  tweet  berhasil  dilakukan 
preprocessing,  maka  selanjutnya  adalah  menghapus  bentuk 
duplikasi  data  tweet  dan  data  tweet  yang  memiliki  tingkat 
kemiripan 100%  dihapus. 

terkait  vaksinasi  COVID-19  yang 

Data yang telah berhasil melalui tahap preprocessing akan 
digunakan dalam proses analisis sentimen dan topic modelling. 
respon 
Analisis  sentimen  digunakan  untuk  mengukur 
telah 
masyarakat 
dilaksanakan di Indonesia pada perioede 13 Januari 2021 -  30 
April  2021.  Metode  yang  digunakan  pada  analisis  sentimen 
adalah dengan pendekatan berbasis leksikon. Analisis sentimen 
berbasis leksikon menafaatkan kamus dari kata  yang memiliki 
nilai skor polaritas tertentu [15].  

Setelah  hasil  sentimen  dari 

tweet  diperoleh,  maka 
selanjutnya  adalah  mengidentifikasi  topik  dari  tiap  kelas 
sentimen,  yaitu  kelas  negatif,  netral,  dan  positif.  Hal  ini 
dilakukan untuk melihat opini apa saja yang dibahas dari tiap 
kelas  senitmen.  Pembuatan  model  dilakukan  dengan  bantuan 
wrapper  dari  library  Gensim,  yaitu  LDA  Mallet.  Secara 
sederhana,  Mallet  atau  disebut  sebagai  Machine  Learning  for 
Language  Toolkit  merupakan  paket  berbasis  Java  untuk 
pemrosesan  bahasa  alami  yang  dirilis  oleh  UMASS  Amherst. 
Pekerjaan yang dapat dilakukan oleh Mallet diantaranya adalah 

 3 / 10 

 
 
 
 
 
klasifikasi dokumen, pemodelan topik, ekstraksi informasi, dan 
aplikasi pembelajaran mesin lainnya pada teks[16].  

Metode  evaluasi  dengan  coherence  score  dipilih  untuk 
pemilihan  model  terbaik  dari  jumlah  topik  yang  dibentuk. 
Coherence score digunakan untuk mengukur tingkat kecocokan 
tiap  kata-kata  pada  suatu  topik  yang  terbentuk  berdasarkan 
analisis semantik antara kata-kata yang terdapat pada dokumen 
[17].  Model  yang  baik  akan  menghasilkan  topik  dengan 
coherence score yang tinggi. 

terbaik, 

Setelah  diperoleh 

topik  model  yang 

tahap 
selanjutnya adalah melakukan interpretasi pada topik. Beberapa 
pekerjaan yang dilakukan pada tahap ini adalah  menganalisis 
tema topik dari tiap kelas sentimen dan melakukan visualisasi 
untuk mengetahui lebih jelas sebaran topik yang telah diperoleh. 
Pembuatan visualisasi sebaran topik dilakukan dengan bantuan 
library PyLDAvis, selain itu PyLDAvis juga dapat menampilkan 
kumpulan  kata  kunci  tiap  topik  yang  membantu  dalam 
menafsirkan topik. 

V.  KERANGKA PIKIR 

Kerangka  pikir  penelitian  ini  disajikan  pada  Gambar  2 

sebagai berikut :   

 Gambar 2. Kerangka Pikir 

Berdasarkan gambar 2, dijelaskan bahwa opini dan respon 
masyarakat  merupakan 
terkait  vaksinasi  COVID-19  di 
Indonesia dapat menjadi sumber knowledge yang penting untuk 
diketahui  oleh  para  pemerintah  dalam  memberi  gambaran 
tentang  situasi  dan  kondisi  yang  dialami  masyarakat  selama 
proses vaksinasi COVID-19  di Indonesia. Selain itu, opini dan 
respon tersebut juga dapat dijadikan sumber knowledge untuk 
memberikan  pemahaman 
tentang  penilaian  masyarakat 
terhadap pelaksaanaan vaksinasi COVID-19 di Indonesia yang 
dapat dijadikan bahan bagi pemerintah untuk mengidentifikasi 
adanya indikasi masalah ataupun sebagai pertimbangan untuk 
bahan  evaluasi  selama  pelaksanaan  vaksinasi  COVID-19  di 

Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik 

Indonesia.  Saat  ini  kemajuan  teknologi  informasi  membuat 
banyak  orang  menggunakan  media  sosial  sebagai  platform 
populer  dalam  mengemukakan  opini  dan  respon  pendapat 
mereka terkait suatu kejadian. Twitter adalah salah satu media 
sosial  berbasis  microblogging  populer  yang  digunakan  oleh 
banyak  orang  untuk  saling  mengungkapkan  opini  dan  respon 
mereka terhadap topik yang sedang dibahas. Opini dan respon 
masyarakat  terkait  vaksinasi  COVID-19  di  Indonesia  pada 
media sosial Twitter dapat menjadi sumber data real time untuk 
menggambarkan kondisi dan situasi yang dialami masyarakat 
selama  prose  vaksinasi  COVID-19.  Namun,  mengidentifikasi 
opini  dan  respon  melalui  media  sosial  Twitter  yang 
penggunaanya  masif  dari  waktu  ke  waktu  tentunya  membuat 
pola  tweet  semakin  beragam,  sehingga  cukup  sulit  jika 
dilakukan  dengan  manual.  Untuk  menyelesaikan  masalah 
tersebut, peneliti menggunakan pendekatan analisis dengan text 
mining, yaitu menerapkan topic modelling dan anaisis sentimen. 
Topic modelling dengan algoritma Latent Dirichlet Allocation 
digunakan  memodelkan  opini  dari  tweet  masyrakat  terkait 
vaksinasi    COVID-19  di  Indonesia.  Selanjutnya,  penggunaan 
analisis  sentimen  dengan  lexicon  based  untuk  mengukur 
sentimen/respon 
COVID-19 di Indonesia . 

tweet  masyarakat 

terkait 

vaksinasi                     

VI. HASIL DAN PEMBAHASAN 

4.1.   Pengumpulan Data 
 Pembagian 

tiga  waktu  periode  pengumpulan  data 
dilakukan untuk melihat pola data tweet tiap bulannya  Jumlah 
tweet yang berhasil terkumpul disajikan pada Tabel 1. 

                        JUMLAH TWEET HASIL CRAWLING 

 TABEL I 

No 
1 
2 
3 

Periode 
13 Jan – 12 Feb 2021 
13 Feb– 14 Mar 2021 
15 Mar – 30 Apr 2021 
Total 

Jumlah 
45284 
30784 
28546 
104.614 

Hasil pada Tabel 1 diatas bahwa jumlah data tweet  tiap 
periodenya  mengalami  penurunan,  dimana  jumlah  data  pada 
periode  pertama  ke  periode  kedua  terjadi  penurunan  sebesar 
0,030%,  sedangkan  periode  kedua  ke  periode  ketiga  terjadi 
penurunan sebesar 0,07%. 

4.2.   Filtering Bahasa 

Jumlah tweet dari tiap minggu setelah dilakukan filtering 

bahasa disajikan pada Tabel 2. 

                                         TABEL II 
         JUMLAH TWEET HASIL FILTERING BAHASA 

No 

1 
2 
3 

Periode 

Jumlah Setelah Dilakukan 
Filtering Bahasa 

13 Jan – 12 Feb 2021 
13 Feb – 14 Mar 2021 
15 Mar2021 – 30 Apr 2021 

45050 (-0,005%) 
30284 (-0,016%) 
28246 (-0,010%) 

 4 / 10 

 
 
 
 
 
                 
 
 
 
 
 
 
 
Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik 

 Hasil  pada  Tabel  4  diatas  menunjukkan  total  tweet  akhir 
yang  digunakan  dalam  proses  pembuatan  analisis  topic 
modelling  dan  analisis  sentimen  adalah  37992  tweet  atau 
berkurang sekitar 63% dari tweet awal pengumpulan data. 

4.5 Analisis Sentimen 

Hasil analisis sentimen dari tiap periode pengumpulan data 

disajikan pada Gambar 3 berikut. 

Proporsi sentimen setiap 
periode

0.6

0.5

0.4

0.3

0.2

0.1

0

negatif

positif

netral

13Jan-12Feb

13Feb-14Mar

15Mar-30Apr

            Gambar 3. Hasil Sentimen Setiap Periode 

Berdasarkan hasil analisis  sentimen yang telah dilakukan, 
dapat  disimpulkan  bahwa  respon  masyarakat  selama  perode 
perdana vaksinasi COVID-19 didominasi oleh sentimen netral. 
Walaupun respon didominasi sentimen netral, jumlah sentimen 
positif  masih  lebih  banyak  dari  sentimen  negatif.  diperoleh 
bahwa  proprosi  sentimen  setiap  bulannya  naik  turun.  Pola 
proporsi  pada  sentimen  negatif  memiliki  pola  naik  turun dan 
proporsinya cenderung lebih kecil dibandingkan sentimen lain 
tiap bulannya. Kemudian, proporsi sentimen positif cenderung 
memilki  pola  yang  menurun  tiap  bulannya.  Sedangkan, 
proporsi  sentimen  netral  cenderung  mengalami  kenaikan  tiap 
bulannya. Secara keseluruhan, proporsi sentimen netral masih 
cenderung lebih tinggi dibandingkan proporsi sentimen negatif 
dan  positif..  Hasil  penelitian  ini  menunjukkan  bahwa  pola 
sentimen  masyarakat  setelah  vaksinasi  perdana  COVID-19 
cenderung memiliki pola yang sama dengan hasil penelitian [12] 
yang  melakukan  analisis  pada  periode  sebelum  vaksinasi 
COVID-19. 

Hasil  Tabel  2    diatas  menunjukan  bahwa  tidak  tejadi 
penurunan  tweet  yang  cukup  signifikan  setelah  dilakukan 
filtering bahasa setiap periode waktu pengumpulan data. Dapat 
disimpulkan bahwa, hasil crawling data Twiter masih terdapat 
beberapa  tweet  yang  berbahasa  asing  walaupun  jumlahnya 
tidak  signifikan.  Periode  13  Feb  –  14  Mar  2021  merupakan 
periode yang memiiki tingkat pengurangan tweet tertinggi yaitu 
sebesar 0,016%. 

4.3.  Filtering Akun  

Jumlah  tweet  setelah  dilakukan  filtering  akun    disajikan 

pada Tabel 3. 

                                         TABEL III 
          JUMLAH TWEET HASIL FILTERING AKUN 

No 

Periode 

1 
2 
3 

13 Jan – 12 Feb 2021 
13 Feb– 14 Mar 2021 
15 Mar – 30 Apr 2021 

Jumlah Setelah Dilakukan 
Filtering Akun 
23626 (-0,47%) 
14937(-0,51%) 
14611(-0,48%) 

Hasil  pada  Tabel  III    diatas  menunjukan  bahwa  terjadi 
penurunan  tweet  yang  signifikan  (hampir  setengah  dari  tweet 
sebelumnya) setelah dilakukan filtering akun setiap periodenya. 
Dapat  disimpulkan  bahwa,  dari  hasil  crawling  data  Twiter 
separuh tweet berasal dari akun media ataupun akun pemerintah. 
Periode  13  Feb  –  14  Mar  2021  merupakan  periode  yang 
memiiki tingkat pengurangan tertinggi yaitu sebesar 0,51%. 

4.4.  Preprocessing data 
         Hasil  preprocessing  data  tweet  disajikan  pada  tabel  IV 
berikut. 
                                           TABEL IV 
                     CONTOH HASIL PREPROCESSING DATA TWEET 

No 

Proses 

Sebelum 

Sesudah 

1 

Preprocessi
ng data 

ayo vaksin 
takut vaksin 
indonesia 
bebas pandemi 
covid 

=â˜†= Ayoo vaksin 
dan jangan takut, 
Vaksin untuk 
Indonesia Bebas 
dari Pandemi Covid-
19 #NewNormal 
#GugusTugasCovid
19 #CoronaVirus 
#BersatuLawanCovi
d19 #Vaksin 
#stophoaxviruscoro
na  
https://t.co/5JQnBJS
F9A 

2 

Hapus 
Duplikasi 
data 

104.614 

37992 (-63%) 

 5 / 10 

 
 
 
 
 
 
 
 
 
 
 
 
Pola Proporsi Sentimen 
Tiap Harinya

Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik 

coherence score maka topik yang dihasilkan semakin optimal. 
Adapun  limit  topik  yang  digunakan  adalah  5-11,  5-21,  dan                
5-31. 

n
a
J
-
3
1

n
a
J
-
5
1

n
a
J
-
7
1

n
a
J
-
9
1

n
a
J
-
1
2

n
a
J
-
3
2

sentimen positif
sentimen negatif
b
b
b
b
b
b
b
r
n
n
n
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
p
p
p
p
p
a
a
a
e
e
e
e
e
e
e
M
M
M
M
M
M
M
M
M
M
M
M
M
M
M
A
A
A
A
A
J
J
J
F
F
F
F
F
F
F
-
-
-
sentimen netral
-
-
-
-
-
-
-
-
-
-
-
-
5
7
9
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
1
3
5
7
9
4
0
2
4
6
8
8
0
8
0
6
4
2
8
6
4
2
0
8
6
4
2
2
2
2
0
0
0
1
1
2
1
1
1
2
2
0
2
3
2
2
2
2
1
1
1
1
1
0
0
0
0

b
e
F
-
2
2

b
e
F
-
2
0

b
e
F
-
6
1

b
e
F
-
8
1

b
e
F
-
6
0

b
e
F
-
4
0

b
e
F
-
0
2

r
p
A
-
1
1

r
p
A
-
3
1

r
p
A
-
9
0

r
p
A
-
5
1

r
p
A
-
7
0

n
a
J
-
1
3

r
p
A
-
1
2

r
p
A
-
3
2

r
p
A
-
5
2

r
p
A
-
7
2

r
p
A
-
9
2

1.2

1

0.8

0.6

0.4

0.2

0

           Gambar 4. Pola Pergerakan Sentimen tiap Minggu 

Kemudian  Gambar  4  diatas  menunjukkan,  bahwa  pola 
pergerakan  sentimen  dari  13  Januari  2021  –  30  April  2021 
cukup fluktuatif.  Secara umum, pola proprosi sentimen negatif 
cenderung dibawah dengan proporsi sentimen netral dan positif. 
Kemudian,  secara  umum  pola  proporsi  sentimen  netral  dan 
positif  cenderung  sama,  namun  pada  periode  15  Maret-26 
Maret  terjadi  lonjakan  proporsi  pada  sentimen  netral  dan 
penurunan  proporsi  sentimen  positif  yang  cukup  signifikan, 
sehingga menyebabkan adanya perbedaan pola proporsi netral 
dan  positif  selama  periode  tersebut.  Jika  dilihat  per  harinya, 
pola  proporsi  sentimen  negatif  cenderung 
lebih  stabil 
dibandingkan  dengan  sentimen  positif  dan  netral  atau  dapat 
diartikan  bahwa  sentimen  negatif  tidak  mengalami  kenaikan 
atau penurunan respon yang signifikan setiap harinya. Rata-rata 
perubahan  proporsi  sentimen  negatif  setiap  harinya  sekitar 
0,04%,  lebih  rendah  dari  rata-rata  pada  proporsi  netral  dan 
positif. 

 Tingginya  tingkat  respon  netral  dari  tweet  masyarakat 
menunjukkan  bahwa  respon  dari  tweet  masyarakat  diperoleh 
bukan  hanya dari mereka  yang  menyatakan  apakah   pro  atau 
kontra  terhadap  wacana  vaksinasi  tersebut,  melainkan  juga 
banyak respon masyarakat lainnya sepeti pengetahuan, harapan, 
atau pendapat umum mereka [12]. 

4.6 Analisis Topic Modelling 

Pembuatan  analisis  topic  modelling  digunakan  untuk 
mengelompokkan  opini  masyarakat  berdasarkan 
respon 
mereka terhadap vaksinasi COVID-19 di Indonesia. Terdapat 3 
kelas  respon  masyarakat  yang  akan  dijadikan  bahan  analisis 
pada  topic  modelling  ini,  yaitu  kelas  sentimen  positif,  netral, 
dan  negatif.    Pendekatan  yang  dilakukan  untuk  menentukan 
jumlah  topik  optimal  adalah  dengan  pembuatan  limit  topik. 
Dari  limit  topik  tersebut  akan  dilihat  jumlah  topik  yang 
tertinggi,  semakin  tinggi 
menghasilkan  coherence  score 

Gambar 5. Pola Coherence Score Limit Topik 5-31 
Kelas Sentimen Negatif 

   Setelah  dilakukan  evaluasi  dengan  beberapa  limit  topik, 
diperoleh  bahwa  bahwa  jumlah  topik  yang  memiliki  nilai 
coherence score tertinggi adalah jumlah topik pada limit topik 
5-31, yaitu 13 topik dengan coherenece score sebesar 0,529561. 
Maka, jumlah topik optimal pada kelas sentimen negatif adalah 
berjumlah 13 topik. 

Gambar 6. Sebaran Topik Kelas Sentimen Negatif 
. 
  Kemudian jika dilihat dari Gambar 6 diatas, sebagian besar 
topik telah menyebar di seluruh kuadran (tidak memusat di satu 
kuadran),  selain  itu  tidak  ada  topik  yang  beririsan  penuh. 

 6 / 10 

 
 
 
 
 
 
 
 
 
 
Berdasarkan hasil tersebut, dapat disimpulkan model dengan 13 
topik sudah cukup baik. 

Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 7. Pola Coherence Score Limit Topik 5-21                  

Gambar 9. Pola Coherence Score Limit Topik 5-31                  

Kelas Sentimen Netral 

Kelas Sentimen Positif 

Setelah  dilakukan  evaluasi  dengan  beberapa  limit  topik, 
diperoleh  bahwa  bahwa  jumlah  topik  yang  memiliki  nilai 
coherence score tertinggi adalah jumlah topik pada limit topik 
5-21, yaitu 15 topik dengan coherenece score sebesar 0,485603. 
Maka,  jumlah  topik  optimal  pada  respon  kelas  positif  adalah 
berjumlah 15 topik. 

jumlah 

topik  24 

Setelah  dilakukan  evaluasi  dengan  beberapa  limit  topik, 
diperoleh  bahwa  bahwa  jumlah  topik  yang  memiliki  nilai 
coherence score tertinggi adalah jumlah topik pada limit topik 
5-31, yaitu 24 topik dengan coherenece score sebesar 0,475895. 
Namun, 
terlalu  banyak  dan  dapat 
menyebabkan banyak topik yang saling beririsan atau tumpang 
tindih.  Menurut  [18]  memiliki  terlalu  banyak  topik  akan 
menghasilkan model topik dengan lingkaran yang lebih kecil 
lebih banyak tumpang tindih, dan mengelompok menjadi satu 
kuadran. Maka, untuk menghindari hal tersebut alternatif yang 
dipilih  adalah  dengan  memilih  jumlah  topik  13  dengan 
coherence score sebesar 0,44301. 

Gambar 8. Sebaran Topik Kelas Sentimen Positif 

  Kemudian jika dilihat dari Gambar 8, sebagian besar topik 
telah  menyebar  di  seluruh  kuadran  (tidak  memusat  di  satu 
kuadran),  walaupun  masih  terdappat  beberap  topik  yang 
beririsan,  namun  tidak  ada  topik  yang  beririsan  penuh  atau 
tumpang tindih. Berdasarkan hasil tersebut, dapat disimpulkan 
model dengan 15 topik sudah cukup baik 

Gambar 10. Sebaran Topik Kelas Sentimen Netral 

  Kemudian jika dilihat dari Gambar 10 diatas, sebagian besar 
topik telah menyebar di seluruh kuadran (tidak memusat di satu 

 7 / 10 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
kuadran), namun masih terdapat 2 topik yang saling beririsan 
penuh/tumpang  tindih.  Berdasarkan  hasil  tersebut,  dapat 
disimpulkan model dengan 13 topik sudah cukup baik. 

4.6 Interpretasi Topik 

Setelah  model  topik  optimal  telah  ditentukan,  kemudian 
selanjutnya  yang  dilakukan  adalah  melakukan 
tahap 
interpretasi terhadap topik. Interpretasi topik dilakukan untuk 
menafsirkan  tema  dari  tiap  topik,  sehingga  dapat  memberi 
gambaran opini dari masyarakat terkait vaksinasi COVID-19 di 
Indonesia.  Kumpulan  kata  kunci  tiap  topik  digunakan  untuk 
membantu  dalam  menafsirkan  tema  dari  topik.  Jumlah  topik 
dan kata kunci disajikan pada  Tabel 5, Tabel 6, dan Tabel 7. 

Tabel V. Kata Kunci dari Topik Kelas Sentimen       

Negatif 

Topik 
0 

1 

2 

3 

4 

5 

6 

7 

8 

9 

10 

11 

12 

Kata Kunci 

suntik, takut, masyarakat, 
masyarakat_takut, 
takut_jarum 
mati,  akibat,  risiko,  angka, 
papar 
puskesmas,  laksana,  tahap, 
efek_samping, kota 
terima_suntik, 
suntik, 
program_imunisasi,  daftar,  
malaysia,  
orang, negara, kasih, dokter, 
lihat_suasana  
tipu, 
modus, 
berpurapura_nakes,waspada 
kena, rakyat, tolak, percaya, 
kesan,  

bayar, 

sakit,  virus,  tubuh,  corona, 
kebal 
vaksinasi, takut, aman, ragu, 
halal 
pandemi, 
salah, Indonesia 
lansia, hasil, pakai, prioritas, 
rentan 
sebar,  palsu,  berita,  hoax, 
masyarakat 

lawan,  negara, 

juta,  dosis,  bahan,  sinovac, 
bio_farma,  

Tema 
Efek jarum suntik 

Resiko kematian 

Efek 
vaksin 
Program 
Malaysia 

samping 

vaksin 

terkait 

vaksin 
kekebalan 

Keprihatinan  pada 
tenaga medis 
Penipuan 
vaksin 
Penolakan  vaksin 
oleh rakyat 
Dampak 
pada 
tubuh 
Keraguan 
keamanan vaksin 
Negara  melawan 
pandemi 
Pemberian  vaksin 
pada lansia 
Berita  hoax  terkait 
vaksin 
Produksi 
sinovac 
biofarma 

vaksin 
dan 

Berdasarkan hasil kata kunci tiap topik pada Tabel 5 diatas, 
dapat  simpulkan  terdapat  3  tema  besar  dari  sentimen  kelas 
negatif.  Tema  pertama  membahas  permasalahan  masyarakat 
yang takut untuk melakukan vaksinasi karena beberapa faktor 
diantaranya  terkait  keamanan  jarum  suntik,  kasus  kematian 
akibat  vaksinasi  dan  status  kehalalan  vaksin  yang  masih 
diragukan.  Tema  kedua  membahas  tentang  beredarnya  berita 
hoax dan penipuan terkait vaksinasi COVID-19. Tema ketiga 

Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik 

membahas keraguan dan penolakan masyarakat akan program 
vaksinasi  COVID-19  karena  beberapa  alasan,  diantaranya 
adalah keraguan masyarakat  atas vaksin yang dapat membuat 
tubuh kebal  dari virus,  dan  adanya  penolakan  pada vaksinasi 
usia lanjut.  

Tabel VI. Kata Kunci dari Topik Kelas Sentimen  

Positif       

Topik 
0 

1 

2 

3 

4 

5 

6 

7 

8 

9 

10 

11 

12 

13 

14 

Kata Kunci 
positif, daerah, pagi, info, 
edar 
sinovac,  uji,  efektif,  hasil, 
klinis 
virus, 
kebal, prokes 

corona, 

tubuh, 

aman,  halal,  vaksinasi, 
indonesia, sehat 
orang, 
herd_immunity, 
percaya 
mudah, 
program_vaksin, rantai 

cepat, 
efek, 

putus, 

jalan, 

lindung, 

masyarakat, 
bpom, lansia, selamat 
suntik,  presiden,  orang, 
terima, perdana 

jaga, 

protokol, 

sehat, 
patuh, cegah 
laksana,  aman,  distribusi, 
puskesmas, dinas_ 

tenaga, 

terima,  suntik,  malaysia, 
rakyat, negara 
sehat,  moga, 
nakes, alhamdulillah 
indonesia, bebas, pandemi, 
vaksinasi, harap 
dosis, 
juta, 
indonesia, 
lawan_pandemi,  
vaksinasi, 
sukses, dukung, gratis 

program, 

lawan, 

Tema 

Informasi terkait 
vaksin 
Uji klinis sinovac 

dan 

izin 

untuk 

Prokes 
menghindari 
penularan virus 
Keamanan 
kehalaln vaksin 
vaksin 
Pemberian 
dalam  mendukung 
herd imunity 
Vaksin  sebagai  cara 
rantai 
memutus 
wabah 
Pemberian 
vaksin pada lansia 
Penyuntikkan 
perdana 
presiden 
Pematuhan  protokol 
kesehatan 
Distribusi 
pengawasan 
vaksinasi 
Penyuntikan  vaksin 
di malaysia 
Dukungan 
tenaga kesehatan 
Harapan 
pandemi 
Jumlah dosis vaksin 

bebas 

pada 

pada 

dan 

Program 
gratis 

vaksin 

Berdasarkan hasil kata kunci tiap topik pada Tabel 6 diatas, 
dapat  simpulkan  terdapat  6  tema  besar  dari  topik    sentimen 
kelas positif.  Tema pertama membahas tentang efek pemberian 
vaksin  terrhadap  pembentukan  herd  imunity  dan  status 
kehaalan  vaksin.  Tema  kedua  membahas  tentang  proses 
penyuntikkan  vaksin  perdana  Presiden  Jokowidodo.  Tema 
ketiga  membahas  tentang  menjaga  kepatuhan  pada  protokol 
kesehatan  di  tengah  proses  vaksinasi. 
  Tema  keempat 
membahas tentang izin pemberian vaksinasi COVID-19 pada 
lansia.  Tema  kelima  membahas  tentang  berbagai  dukungan 

 8 / 10 

 
 
 
 
  
 
 
 
 
 
terhadap  lancarnya  program  vaksinasi,  termasuk  dukungan 
pada tenaga kesehatan, masyarakat yakin vaksinasi COVID-19 
dapat  dengan  cepat  membantu  melawan  pandemi  dan 
memulihkan kondisi ekonomi yang terdampak akibat pandemi. 
Selain  itu,  masyarakat  juga  mendukung  pemberian  vaksin 
gratis. 

Tabel VII. Kata Kunci dari Topik Kelas Sentimen 

Netral 

Kata Kunci 

indonesia,  puasa, 

negara, astrazeneca, efek_samping, orang, 
jenis 
juta, dosis, bio_farma, indonesia, daerah 
sinovac, lansia, bpom, izin, merah_putih 
vaksinasi,  batal_puasa, 
masyarakat 
terima_suntik,menteri_kesihatan,panglima_tni, 
selesai 
uji_klinis, terima_warga, tolak, terima, tahap 
pandemi, 
indonesia, percaya 
suntik, presiden, terima, rabu, januari 
program,  daftar,  gerak_daftar,  putus_rantai, 
perintah 
virus, tubuh, kebal, corona, sakit 
daftar, malaysia, rakyat,aplikasi, mysejahtera 
vaksinasi,  tahap,  program_vaksinasi,  sehat, 
tenaga_sehat 
laksana, proses, jawa_timur, maret,jawa 

lawan_pandemi, 

masyarakat, 

Topik 
0 

1 
2 
3 

4 

5 
6 

7 
8 

9 
10 
11 

12 

Berdasarkan hasil kata kunci tiap topik pada Tabel 7 diatas, 
dapat simpulkan terdapat 5 tema besar dari topik kelas sentimen 
netral.  Tema  pertama  membahas  tentang  pemahaman  dan 
pengetahuan  masyarakat  terkait  hasil  uji  klinis  dan  efek 
samping  vaksin  astrazeneca.  Tema  kedua  membahas  tentang  
tentang pemberian izin penyuntikkan vaksin sinovac dan merah 
putih oleh BPOM pada lansia. Tema ketiga membahas tentang 
pemberian vaksin kepada menteri kesehatan serta tni dan polri.   
Tema keempat membahas tentang hukum pemberian vaksinasi 
saat melaksanakan ibadah puasa, beberapa menanggapi bahwa 
pemberian vaksinasi  dapat  membatalkan  ibadah  puasa.  Tema 
kelima membahas tentnag proses pendaftaran dan pengawalan 
pelaksanaan  vaksinasi  COVID-19  selama  bulan  maret  di 
daerah Jawa Timur. 

VII.  PENUTUP 

Dari tahapan penelitian yang telah dilakukan, berikut hasil 

yang dapat disimpulkan pada penelitian ini : 

1.  Metode  sentiment  analysis  dengan  TextBlob    dan  
topic modeling dengan LDA merupakan metode yang 
dapat  membantu  dalam  mengidentifikasi  respon  dan 
respon masyarakat terkait vaksinasi COVID-19 pada 
media sosial Twitter. 

2.  Berdasarkan  hasil  analisis  sentimen  yang 
dilakukan,  dapat  disimpulkan  bahwa 

telah 
respon 

Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik 

masyarakat selama perode perdana vaksinasi COVID-
19 didominasi oleh sentimen netral. Tingginya tingkat 
respon  netral  dari  tweet  masyarakat  menunjukkan 
bahwa respon dari tweet masyarakat diperoleh bukan 
hanya dari mereka yang menyatakan apakah  pro atau 
kontra terhadap wacana vaksinasi tersebut, melainkan 
juga  banyak  respon  masyarakat 
lainnya  sepeti 
pengetahuan,  harapan,  atau  pendapat  umum  mereka 
[12].  Walaupun  respon  didominasi  sentimen  netral, 
jumlah  sentimen  positif  masih  lebih  banyak  dari 
sentimen  negatif.  Jika  dilihat  per  harinya,  pola 
proporsi  sentimen  negatif  cenderung  lebih  stabil 
dibandingkan dengan sentimen positif dan netral atau 
tidak 
dapat  diartikan  bahwa  sentimen  negatif 
mengalami  kenaikan  atau  penurunan  respon  yang 
signifikan 
setiap  harinya.  Rata-rata  perubahan 
proporsi sentimen negatif setiap harinya sekitar 0,04%, 
lebih  rendah  dari  rata-rata  pada  proporsi  netral  dan 
positif. 

3.  Hasil  penelitian 

ini  menunjukkan  bahwa  pola 
sentimen  masyarakat  setelah  vaksinasi  perdana 
COVID-19  cenderung  memiliki  pola  yang  sama 
dengan hasil penelitian [12] yang melakukan analisis 
pada periode sebelum vaksinasi COVID-19.  

4.  Hasil  topic  modeling  menunjukkan,  opini  yang 
disampaikan  masyarakat  sebagian  besar  membahas 
tentang  penilaian  dan  pengetahuan  mereka  terhadap 
vaksinasi  COVID-19  di 
Indonesia.  Penilaian 
masyarakat  cenderung  mengarah  pada  persoalan 
kemanan  dan  status  kehalalan  vaksin  COVID-19, 
serta terkait pemberian vaksin COVID-19 pada lansia. 
Adanya  pembahasan  kedua  topik  tersebut  pada  dua 
kelas  sentimen  positif  dan  negatif  menunjukkan 
bahwa,  persoalan 
tersebut  masih  pro-kontra  di 
masyarakat. Selain itu, sebagian masyrakat masih ragu 
dan  menolak 
terhadap  vaksinasi  COVID-19. 
Kemudian, maraknya berita hoax dan kasus penipuan 
terkait  vaksinasi  COVID-19  dapat  menimbulkan 
masalah  di  masyarakat,  sehingga  dapat  menghambat 
jalanya proses vaksinasi COVID-19 di Indonesia..  

Penelitian  selanjutnya  diharapkan  dapat  mencari  metode 
analisis sentimen dan evaluasi topik yang lebih baik, sehingga 
dapat  meningkatkan  hasil  analisis.  Kemudian,  proses  analisis 
dapat dilakukan lebih mendalam, seperti dengan menganalisis 
tweet berdasarkan lokasi dan mengidentifikasi akun atau tokoh 
yang  paling  berpengaruh  dan  banyak  dibicarakan  dari  suatu 
topik.  

 9 / 10 

 
 
 
 
 
 
 [1] 

[2] 

[3] 

[4] 

[5] 

[6] 

[7] 

[8] 

[9] 

[10] 

[11] 

[12] 

[13] 

[14] 

[15] 

[16] 

[17] 

[18] 

DAFTAR PUSTAKA 

A. E. Gorbalenya et al., “Severe acute respiratory syndrome-related 
coronavirus: The species and its viruses – a statement of the 
Coronavirus Study Group,” bioRxiv, 2020, doi: 
10.1101/2020.02.07.937862. 
M. Novina, “Pasien Pertama COVID-19 Ditemukan, Bantu Lacak 
Sumber Virus Corona.” 
https://www.kompas.com/sains/read/2020/03/18/140300223/pasien-
pertama-COVID-19-ditemukan-bantu-lacak-sumber-virus-corona 
(accessed Jan. 17, 2021). 
Worldmeter, “COVID-19 Coronavirus Pandemi,” 2021. 
https://www.worldometers.info/coronavirus/ (accessed Jan. 17, 
2021). 
N. K. Muliati, “Pengaruh Perekonomian Indonesia di Berbagai 
Sektor Akibat Corona Virus Disease 2019 (Covid-19),” Widya 
Akunt. dan Keuang., vol. 2, no. 2, pp. 78–86, 2020, doi: 
10.32795/widyaakuntansi.v2i2.874. 
C. Liu et al., “Research and Development on Therapeutic Agents 
and Vaccines for COVID-19 and Related Human Coronavirus 
Diseases,” ACS Cent. Sci., vol. 6, no. 3, pp. 315–331, 2020, doi: 
10.1021/acscentsci.0c00272. 
Kemenkes, “Dimulai Januari, Berikut Jumlah Sasaran Vaksinasi 
COVID-19 di Indonesia,” 2020. 
https://www.kemkes.go.id/article/view/20122900009/dimulai-
januari-berikut-jumlah-sasaran-vaksinasi-covid-19-di-
indonesia.html (accessed Jan. 17, 2021). 
Shalvee and S. Sambhav, “Role of mass media and communication 
during pandemic,” Int. J. Creat. Res. Thoughts, vol. 8, no. 5, pp. 
3786–1790, 2020. 
L. Palen and S. Vieweg, “The emergence of online widescale 
interaction in unexpected events: Assistance, alliance & retreat,” 
Proc. ACM Conf. Comput. Support. Coop. Work. CSCW, pp. 117–
126, 2008, doi: 10.1145/1460563.1460583. 
B. Ludwianto, “Riset: 64% Penduduk Indonesia Sudah Pakai 
Internet,” 2020. https://kumparan.com/kumparantech/riset-64-
penduduk-indonesia-sudah-pakai-internet-1ssUCDbKILp/full 
(accessed Jan. 17, 2021). 
S. Boon-Itt, “A text-mining analysis of public perceptions and topic 
modeling during the COVID-19 pandemic using Twitter data.,” 
JMIR public Heal. Surveill., 2020, doi: 10.2196/21978. 
E. D’Andrea, P. Ducange, A. Bechini, A. Renda, and F. Marcelloni, 
“Monitoring the public opinion about the vaccination topic from 
tweets analysis,” Expert Syst. Appl., vol. 116, pp. 209–226, 2019, 
doi: 10.1016/j.eswa.2018.09.009. 
F. F. Rachman and S. Pramana, “Analisis Sentimen Pro dan Kontra 
Masyarakat Indonesia tentang Vaksin COVID-19 pada Media 
Sosial Twitter,” Heal. Inf. Manag. J. ISSN, vol. 8, no. 2, pp. 2655–
9129, 2020, [Online]. Available: 
https://inohim.esaunggul.ac.id/index.php/INO/article/view/223. 
N. Aliyah Salsabila, Y. Ardhito Winatmoko, A. Akbar Septiandri, 
and A. Jamal, “Colloquial Indonesian Lexicon,” Proc. 2018 Int. 
Conf. Asian Lang. Process. IALP 2018, pp. 226–229, 2019, doi: 
10.1109/IALP.2018.8629151. 
A. Librian, “High quality stemmer library for Indonesian Language 
(Bahasa),” GitHub, 2017. https://github.com/sastrawi (accessed 
Mar. 25, 2021). 
M. Z. Nafan and A. E. Amalia, “Kecenderungan Tanggapan 
Masyarakat terhadap Ekonomi Indonesia berbasis Lexicon Based 
Sentiment Analysis,” J. Media Inform. Budidarma, vol. 3, no. 4, p. 
268, 2019, doi: 10.30865/mib.v3i4.1283. 
A. K. McCallum, “MALLET: A Machine Learning for Language 
Toolkit,” 2002. http://mallet.cs.umass.edu. 
J. Stolee, “An Evaluation of Topic Modelling Techniques for 
Twitter,” arXiv, pp. 1–11, 2016. 
J. Le, M. Weber, and D. Wild, “Topic Modeling of COVID-19 
Open Research Dataset using Latent Dirichlet Allocation,” 2020, 
[Online]. Available: https://doi.org/10.1145/nnnnnnn.nnnnnnn. 

Makalah Sidang Skripsi – Program Studi D-IV Komputasi Statistik 

 10 / 10 

 
 
 
"
221709607,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Penerapan Algoritma Machine Learning untuk 
Pengelompokan Kesejahteraan Sosial 
(Studi Kasus : Kawasan Timur Indonesia Tahun 2019) 

Danu Abdi Saputra (221709607, 4SD2) 
Dosen Pembimbing : Dr. Tiodora Hadumaon Siagian, M. Pop., Hum. Res. 

Ringkasan—  Kesejahteraan  sosial  merupakan  salah  satu 
indikator  yang  menggambarkan  pembangunan  wilayah  di 
Indonesia.  Berbagai  program  serta  kebijakan  telah  dilakukan 
pemerintah dalam proses penyelenggaraan kesejahteraan sosial di 
Indonesia. Namun nyatanya, masih terdapat beberapa wilayah di 
Indonesia  yang  pelaksanaan  program 
serta  kebijakan 
kesejahteraan sosial belum sepenuhnya efektif dan tepat sasaran. 
Hal ini diindikasikan dengan masih terdapat enam belas provinsi 
di Indonesia yang persentase jumlah penduduk miskinnya di atas 
10%  berdasarkan  data  Badan  Pusat  Statistik  pada  Maret  2020 
dimana  diketahui  sembilan  dari  enam  belas  provinsi  tersebut 
merupakan provinsi di wilayah Kawasan Timur Indonesia (KTI). 
Selain  itu,  lima  provinsi  dengan  persentase  penduduk  miskin 
tertinggi  di  Indonesia  juga  merupakan  provinsi  yang  ada  di 
wilayah  KTI.  Berdasarkan  permasalahan 
tersebut,  pada 
ini  dilakukan  penerapan  algoritma  unsupervised 
penelitian 
machine 
learning  yaitu  clustering  untuk  pengelompokan 
kesejahteraan  sosial  pada  seluruh  kabupaten/kota  di  wilayah 
KTI.    Metode  clustering  yang  digunakan  merupakan  metode  k-
means  clustering  dan  fuzzy  c-means  dengan  jumlah  cluster  (k) 
yaitu  3  dan  4.  Dari  semua  hasil  pengelompokan  yang  telah 
diperoleh,  kemudian  dilakukan  evaluasi  dengan  menggunakan 
metode  davies-bouldin  index  dan  dunn  index  untuk  mencari 
algoritma  machine  learning  terbaik  dalam  penelitian  ini.  Hasil 
pengelompokan dari algoritma machine learning terbaik tersebut 
divisualisasikan  dalam  sebuah  dashboard  kesejahteraan  sosial 
yang  telah  dibangun  dengan  menggunakan  Bahasa  R  dengan 
framework Shiny. 

Kata  Kunci—  Kesejahteraan  sosial,  KTI,  unsupervised  machine 

learning, k-means clustering, fuzzy c-means. 

I.  LATAR BELAKANG 

Kesejahteraan  sosial  merupakan  indikator  penting  bagi 
banyak  negara  di  seluruh  dunia  termasuk  di  Indonesia. 
Kesejahteraan sosial menjadi penting karena menjadi salah satu 
indikator  kunci  keberhasilan  pembangunan  di  suatu  wilayah. 
Berdasarkan  UU  No.  11  Tahun  2009  tentang  Kesejahteraan 
Sosial  [1]  dalam  pasal  1  ayat  1,  menyatakan  bahwa 
“Kesejahteraan Sosial adalah kondisi terpenuhinya kebutuhan 
material,  spiritual,  dan  sosial  warga  negara  agar  dapat  hidup 
layak  dan  mampu  mengembangkan  diri,  sehingga  dapat 
melaksanakan fungsi sosialnya”.  

Pemerintah telah menyelenggarakan berbagai program serta 
kebijakan  sebagai  bentuk  tanggung  jawab  guna  mewujudkan 
kesejahteraan  sosial  di  Indonesia.  Berhasil  atau  tidaknya 
penyelenggaraan tersebut dapat diketahui salah satunya melalui 
indikator  tingkat  kemiskinan.  Sebagaimana  yang  dinyatakan 

Setiawan  [2]  bahwa  kemiskinan  menjadi  salah  satu  prioritas 
dalam  pembangunan  kesejahteraan  sosial.  Berdasarkan  data 
Badan Pusat Statistik (BPS) Maret 2020 dapat diketahui bahwa 
jumlah  penduduk  miskin  di  Indonesia  sebanyak  26.424.020 
jiwa  atau  sebanyak  9,78%  dari  jumlah  penduduk  Indonesia. 
Sebenarnya angka tersebut merupakan sebuah prestasi, karena 
angkanya  sudah  dibawah  10%.  Artinya  hal 
tersebut 
menunjukan  bahwa  pelaksanaan  program  serta  kebijakan 
kesejahteraan  sosial  yang  telah  dilakukan  pemerintah  di 
Indonesia  selama  ini  terbilang  sudah  cukup  berhasil.  Namun 
demikian, masih terdapat beberapa wilayah di Indonesia yang 
pelaksanaan  program  serta  kebijakan  kesejahteraan  sosialnya 
dipandang  masih  belum  sepenuhnya  efektif dan tepat sasaran 
karena  terdapat enam belas provinsi di Indonesia yang angka 
kemiskinannya  masih  diatas  10%.  Sembilan  dari  enam  belas 
provinsi  tersebut  merupakan  provinsi-provinsi  yang  ada  di 
wilayah  Kawasan  Timur  Indonesia  (KTI)  (lihat  Gambar  1). 
Pada Gambar 2 dapat dilihat juga bahwa lima provinsi dengan 
persentase penduduk miskin tertinggi di Indonesia merupakan 
provinsi-provinsi  yang  ada  di  wilayah  KTI.  Hal  tersebut 
mengindikasikan bahwa pelaksanaan program serta kebijakan 
kesejahteraan  sosial  yang  telah  dilakukan  pemerintah  di 
wilayah KTI belum sepenuhnya efektif dan tepat sasaran. 

Sumber : Badan Pusat Statistik (2020) 

Gambar 1. Persentase penduduk miskin wilayah Indonesia Timur, Maret 2020 

 1 / 9 

 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

3.  Mendapatkan 

peta 

hasil 
tematik 
pengelompokan  wilayah  dari  algoritma  machine  learning 
terbaik,  yang  kemudian  akan  ditampilkan  dalam  sebuah 
dashboard informasi. 

berdasarkan 

Sumber : Badan Pusat Statistik (2020) 

Gambar 2. Persentase penduduk miskin menurut provinsi di Indonesia, Maret 

2020 

Dalam merumuskan program maupun kebijakan yang efektif 
untuk  menangulangi  suatu  masalah,  maka  diperlukan 
pengetahuan  tentang  karakteristik  dari  wilayahnya.  Sehingga 
program  serta  kebijakan  yang  akan  dilakukan  dapat  tepat 
sasaran  dan  dapat  menyelesaikan  persoalan  kesejahteraan 
sosial  yang  ada.  Informasi  yang  valid  dan  relevan  mengenai 
karakteristik  dari  suatu  wilayah  sangat  dibutuhkan  oleh  para 
pengambil  kebijakan  guna  mewujudkan  penyelenggaraan 
kesejahteraan sosial yang terencana dan terarah.  

Karakteristik  dari  suatu  wilayah  tersebut  dapat  diketahui 
learning  seperti 
dengan  menerapkan  algoritma  machine 
clustering  merupakan 
algoritma 
clustering.  Metode 
unsupervised  machine 
learning  yang  digunakan  untuk 
melakukan  pengelompokan  objek  yang  didasarkan  pada 
persamaan  karakteristik  yang  dimiliki.  Dengan  menerapkan 
metode clustering, diharapkan dapat membantu para pengambil 
kebijakan  untuk  mengetahui  karakteristik  dari  wilayahnya 
berdasarkan visualisasi hasil pengelompokan. 

Terdapat  beberapa  penelitian  sebelumnya  yang 

telah 
melakukan kajian tentang kesejahteraan sosial maupun tentang 
pengelompokan  pada  berbagai  kasus  dengan  menggunakan 
berbagai  macam  metode  clustering  seperti  yang  dilakukan 
Setiawan  [2],  Ramadhani,  Hoyyi,  &  Mukid  [4],  Dwitiyanti, 
Selvia  &  Andrari  [5]  serta  Hidayat,  Wasono  &  Darsyah  [6]. 
Namun, 
belum 
mengakomodir  isu  pengelompokan  kesejahteraan  sosial  dan 
sampai  sekarang  belum 
informasi 
kesejahteraan sosial. 

tersedia  dashboard 

penelitian-penelitian 

tersebut  masih 

II.  TUJUAN PENELITIAN 

Berdasarkan  latar  belakang  di  atas,  maka  beberapa  tujuan 

dari penelitian dijabarkan sebagai berikut : 
1.  Melakukan pengelompokkan  wilayah dengan  menerapkan 
algoritma machine learning (k-means clustering dan fuzzy 
c-means)  berdasarkan  variabel-variabel  yang  berkaitan 
dengan kesejahteraan sosial 

2.  Mendapatkan  algoritma  machine  learning  terbaik  yang 
mampu  mengidentifikasi  kelompok-kelompok  wilayah 
berdasarkan tingkat kesejahteraan sosialnya dengan tepat 

III. PENELITIAN TERKAIT 

Dari  kajian  literatur  diketahui  terdapat  beberapa  penelitian 
terkait  yang  menjadi  dasar  dalam  proses  penelitian  yang 
dilakukan.  Setiawan  melakukan  penelitian  untuk  menghitung 
indeks  kesejahteraan  sosial  yang  dijadikan  sebagai  ukuran 
keberhasilan  pembangunan  sosial  yang  telah  dilakukan  di 
Indonesia  [2].  Indeks  kesejahteraan  sosial  yang  dihasilkan 
tersebut dibangun berdasarkan dimensi objektif dan subyektif. 
Selanjutnya  Ramdhani,  Hoyyi  &  Mukid  melakukan 
pengelompokan  kesejahteraan  masyarakat    33  provinsi  di 
Indonesia  pada  tahun  2012  menggunakan  metode  k-means 
cluster dengan jumlah cluster (k) optimal berdasarkan davies-
bouldin index [4].  

Dwitiyanti,  Selvia  &  Andrari  melakukan  pengelompokan 
provinsi  di  Indonesia  berdasarkan  indikator  kesejahteraan 
rakyat  tahun 2017 dengan menerapkan metode fuzzy c-means 
[5].  Kemudian  Hidayat,  Wasono  &  Darsyah  melakukan 
penelitian  yang  bertujuan  untuk  melakukan  analisis  faktor-
faktor yang mempengaruhi kemiskinan di Jawa Tengah dengan 
menggunakan  pendekatan  clustering  [6].  Metode  yang 
digunakannya  adalah  k-means  clustering  dan  fuzzy  c-means. 
Perbandingan  untuk  mencari  metode  terbaik  antara  kedua 
metode 
tersebut 
menggunakan rasio SW dan SB. 

digunakan 

penelitian 

dalam 

yang 

sosial 

IV. METODE PENELITIAN  
Data  yang  digunakan  dalam  penelitian  ini  merupakan  data 
tahun  2019  dari  232 
dimensi  kesejahteraan 
kabupaten/kota yang ada di 17 provinsi pada wilayah KTI yaitu 
Kalimantan  Barat,  Kalimantan  Tengah,  Kalimantan  Selatan, 
Kalimantan  Timur,  Kalimantan  Utara,  Nusa  Tenggara  Barat, 
Nusa  Tenggara  Timur,  Sulawesi  Utara,  Sulawesi  Selatan, 
Sulawesi Barat, Sulawesi Tenggara, Gorontalo, Maluku Utara, 
Maluku,  Papua  Barat,  Papua.  Dari  proses 
identifikasi 
berdasarkan  dimensi  kesejahteraan  sosial  pada  penelitian-
penelitian sebelumnya, terdapat enam variabel yang digunakan 
dalam penelitian ini (Lihat Tabel 1). Data dari variabel-variabel  
tersebut  merupakan  data  sekunder  dari  BPS  (Badan  Pusat 
Statistik)  yaitu  pada  publikasi  Statistik  Kesejahteraan  Sosial 
dan Provinsi dalam Angka.  

TABEL I 
VARIABEL-VARIABEL PENELITIAN 

Dimensi 
(1) 
Pendapatan 

Kesehatan 

Pendidikan 

Perumahan 

Pendapatan 
Ketenagakerjaan 

Variabel 
(2) 
X1 

X2 

X3 

X4 

X5 
X6 

Deskripsi Variabel 
(3) 

Persentase penduduk miskin 
Persentase penduduk yang mempunyai 
keluhan kesehatan selama sebulan terakhir  
Rata-rata lama sekolah 
Persentase rumah tangga yang tinggal di 
bangunan tempat tinggal milik sendiri 
Pengeluaran Per Kapita Disesuaikan 
Tingkat Pengangguran Terbuka 

 2 / 9 

 
 
 
Pengelompokkan 

di  wilayah  KTI 
kabupaten/kota 
berdasarkan data dimensi  kesejahteraan sosial yang terbentuk 
dilakukan  dengan  menerapkan  algoritma  machine  learning 
yaitu 2 metode clustering (metode k-means clustering dan fuzzy 
c-means).  Kedua  metode  clustering  yang  digunakan  tersebut 
merupakan  metode  non-hierarki.  Metode  non-hierarki 
merupakan  metode  clustering  dimana  jumlah  cluster  yang 
digunakan ditentukan di awal proses clustering [7]. Hair, dkk 
dalam  Ningrat,  Maruddani,  &  Wuyandari  [8]    menyatakan 
bahwa  terdapat  dua  asumsi  yang  harus  terpenuhi  sebelum 
melakukan  proses  clustering  yaitu  asumsi  kecukupan  sampel 
dan asumsi non-multikolinieritas.  

K-Means  Clustering  merupakan  metode 

clustering 
sederhana yang biasa digunakan dalam proses clustering. Salah 
satu pengembangan dari metode tersebut adalah metode fuzzy 
c-means,  dimana  terdapat  penambahan  logika  fuzzy  didalam 
metodenya sehingga algoritma ini memiliki akurasi yang tinggi 
serta  waktu  komputasi  yang  cepat  [9].  Pada  Gambar  3  dan 
Gambar  4  dibawah  dijelaskan  mengenai  algoritma  k-means 
clustering dan fuzzy c-means yang digunakan dalam penelitian 
ini.  

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Setelah  dilakukan  proses  clustering,  tahapan  selanjutnya 
adalah melakukan analisis karakteristik dari kelompok wilayah 
yang  terbentuk  dari  hasil  clustering.  Kemudian,  dilakukan 
pencarian  algoritma  machine  learning  terbaik  yang  mampu 
mengidentifikasi  kelompok  wilayah  berdasarkan 
tingkat 
kesejahteraan  sosialnya  dengan  tepat  dan  efisien.  Terdapat  2 
metode  yang  digunakan  untuk  mencari  metode  clustering 
terbaik dalam penelitian ini yaitu dengan menggunakan metode 
davies-bouldin index dan dunn index. Setelah diperoleh metode 
clustering  terbaik,  tahapan  selanjutnya  dalam  penelitian  ini 
adalah  membangun  dashboard  informasi  yang  menampilkan 
diseminasi hasil dari metode clustering terbaik yang digunakan 
dalam  penelitian  ini.  Pembangunan  dashboard  informasi 
dilakukan  dengan  menggunakan  package  shiny  yang  tersedia 
pada bahasa pemrograman R.  

V.  KERANGKA PIKIR 

Berdasarkan  kajian  literatur,  penelitian  ini  memilih  5 
dimensi pada kesejahteraan sosial, yaitu kesehatan, pendidikan, 
perumahan,  pendapatan  dan  ketenagakerjaan.  Variabel  pada 
setiap  dimensi  ini  dapat  dilihat  pada  Tabel  1.  Gambar  5  di 
bawah ini menyajikan kerangka pikir penelitian yang memuat 
5  dimensi  dari  kesejahteraan  sosial.  Kerangka  pikir  tersebut 
yang  menjadi  landasan  dalam  penerapan  algoritma  machine 
learning  untuk  pengelompokan  kesejahteraan  sosial  wilayah 
KTI tahun 2019. 

                                           Sumber : Prasetyo (2012) 

Gambar 3. Algoritma K-Means Clustering 

               Sumber : Bezdek, Ehrlich, & Full (1984) 

   Gambar 4. Algoritma Fuzzy C-Means 

Gambar 5. Kerangka Pikir 

VI. HASIL DAN PEMBAHASAN 

6.1 Hasil Pengelompokan Wilayah 

1.  Data Preprocessing  

Data  Preprocessing  adalah  proses  manipulasi  data 
yang dilakukan sebelum proses pengolahan data.  Pada 
penelitian ini, ditemukan 2 missing value pada variabel 
Tingkat Pengangguran Terbuka (X6) untuk Kabupaten 
Puncak  dan  Intan  Jaya.  Untuk  mengatasi  hal  tersebut, 
dilakukan  imputasi  data  dengan  menggunakan  metode 
median  substitution.  Selanjutnya  dilakukan  proses 
pengecekan tipe data dari tiap variabel dalam penelitian. 
Pengecekan 
tersebut  dilakukan  untuk  penyetaraan 
semua  tipe  data  dari  variabel  dalam  penelitian  ini. 
Tahapan  terakhir  dalam  preprocessing  data  ini  adalah 
standarisasi data. Hal tersebut dilakukan karena terdapat 
perbedaan skala atau satuan pada data yang digunakan 
dalam penelitian. 

 3 / 9 

 
 
 
 
 
 
   
2.  Asumsi 

a.  Asumsi Kecukupan Sampel 

Berdasarkan  perhitungan  nilai  KMO  yang  telah 
dilakukan, diperoleh nilai KMO sama dengan 0,80 
(> 0,5). Sehingga, dapat disimpulkan bahwa asumsi 
kecukupan sampel pada penelitian ini terpenuhi. 

b.  Asumsi Non-Multikolinieritas 

Berdasarkan  Tabel  2  dapat  diketahui  bahwa 
semua nilai VIF dari semua variabel < 10, sehingga 
asumsi    non  multikolinieritas  pada  penelitian  ini 
terpenuhi.   

TABEL II 
NILAI VIF 

Variabel 
X1 
X2 
X3 
X4 
X5 
X6 
   Sumber : Hasil pengolahan 

VIF 
2,21 
1,18 
2,75 
2,31 
3,34 
2,20 

3.  Penentuan Jumlah Cluster (k) Optimal 

a.  Metode Elbow 

Berdasarkan  Gambar  6  dibawah  dapat  diketahui 
bahwa  garis  membentuk  siku  pada  saat  k  =  4. 
Sehingga  dengan  menggunakan  metode  elbow, 
(k)  optimal  pada 
didapatkan 
penelitian ini adalah 4. 

jumlah  cluster 

                Sumber : Hasil pengolahan 

                 Gambar 6. Penentuan k optimal dengan metode elbow 

b.  Metode Koefesien Sihouette 

Berdasarkan  Gambar  7  dapat  dibawah  diketahui 
bahwa  dengan  menggunakan  metode  koefesien 
silhouette,  didapatkan  jumlah  cluster  (k)  optimal 
pada penelitian ini adalah 3. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

                   Sumber : Hasil pengolahan 

                Gambar 7. Penentuan k optimal dengan metode koefesien silhouette 

4.  Proses Clustering 

a.  K-Means Clustering dengan k = 3 

yang 

Berdasarkan 

telah 
pengelompokan 
dilakukan dengan menggunakan k-means clustering 
dengan  jumlah  cluster  (k)  sebanyak  3,  didapatkan 
hasil bahwa cluster 1 terdiri dari 46 kabupaten/kota, 
cluster 2 terdiri dari 159 kabupaten/kota dan cluster 
3 terdiri dari 27 kabupaten/kota.  

             TABEL III 
                    RATA-RATA HASIL K-MEANS CLUSTERING (K = 3) 

Cluster 
Cluster 1 
Cluster 2 
Cluster 3 

X1 
9,71 
12,59 
34,04 

X2 
27,83 
30,33 
16,87 

X3 
9,95 
7,84 
4,88 

X4 
66,69 
87,48 
91,73 

X5 
12.389,46 
9.184,48 
5.694,85 

X6 
7,30 
3,50 
1,93 

terendah  dari 

Berdasarkan  Tabel  3  di  atas,  warna  merah 
menunjukan nilai rata-rata tertinggi dan warna biru 
menunjukan  nilai  rata-rata 
tiap 
variabel yang dibandingkan dengan seluruh cluster. 
Berdasarkan  hal  tersebut,  diperoleh  karakteristik 
dari tiap cluster yang terbentuk sebagai berikut : 
1.  Cluster  1  adalah  cluster  yang  dikategorikan 
memiliki  tingkat  kesejahteraan  sosial  tinggi. 
Cluster ini merupakan kelompok kabupaten/kota 
yang memiliki persentase penduduk miskin dan 
persentase  rumah 
tinggal  di 
bangunan tempat tinggal milik sendiri terendah, 
persentase  penduduk  yang  mempunyai  keluhan 
kesehatan  selama  sebulan  terakhir  yang  berada 
pada posisi sedang, serta rata-rata lama sekolah, 
pengeluaran  per  kapita  disesuaikan  dan  tingkat 
tertinggi  di  antara 
pengangguran 
kelompok yang lain. 

tangga  yang 

terbuka 

2.  Cluster  2  adalah  cluster  yang  dikategorikan 
memiliki tingkat kesejahteraan sosial menengah. 
Cluster ini merupakan kelompok kabupaten/kota 
yang memiliki persentase penduduk miskin, rata-
rata lama sekolah, persentase rumah tangga yang 
tinggal  di 
sendiri, 
pengeluaran  per  kapita  yang  disesuaikan  dan 
tingkat pengangguran terbuka yang berada posisi 
serta  persentase  penduduk  yang 
sedang, 

tinggal  milik 

tempat 

 4 / 9 

 
 
 
 
 
 
 
mempunyai  keluhan  kesehatan  selama  sebulan 
terakhir tertinggi di antara kelompok yang lain. 
3.  Cluster  3  adalah  cluster  yang  dikategorikan 
memiliki  tingkat  kesejahteraan  sosial  rendah. 
Cluster ini merupakan kelompok kabupaten/kota 
yang  memiliki  persentase  penduduk  yang 
mempunyai  keluhan  kesehatan  selama  sebulan 
terakhir, rata-rata lama sekolah, pengeluaran per 
kapita  disesuaikan  dan  tingkat  pengangguran 
terbuka  terendah,  serta  persentase  penduduk 
miskin dan persentase rumah tangga yang tinggal 
tinggal  milik  sendiri 
tempat 
di  bangunan 
tertinggi di antara kelompok yang lain. 

Clustering 

kabupaten/kota  wilayah  KTI 
menggunakan  metode  k-means  clustering  dengan 
jumlah cluster (k) sama dengan 3 divisualisasikan ke 
dalam  bentuk  peta  pada  Gambar  8.  Berdasarkan 
gambar tersebut, dapat dilihat bahwa warna kuning 
mendominasi pada peta. Hal ini menunjukan bahwa 
sebagian  besar  wilayah  KTI  didominasi  oleh 
kabupaten/kota  yang  memiliki  kesejahteraan  sosial 
yang masuk ke kategori menengah. 

                        Sumber : Hasil pengolahan 

                     Gambar 8. Pengelompokan k-means clustering dengan k = 3 

b.  K-Means Clustering dengan k = 4 

yang 

Berdasarkan 

telah 
pengelompokan 
dilakukan dengan menggunakan k-means clustering 
dengan  jumlah  cluster  (k)  sebanyak  4,  didapatkan 
hasil bahwa cluster 1 terdiri dari 19 kabupaten/kota, 
cluster  2  terdiri  dari  38  kabupaten/kota,  cluster  3 
terdiri dari  90 kabupaten/kota dan cluster 4 terdiri 
dari 85 kabupaten/kota.  

      TABEL IV 
    RATA-RATA HASIL K-MEANS CLUSTERING (K = 4) 

Cluster 
Cluster 1 
Cluster 2 
Cluster 3 
Cluster 4 

X1 
35,53 
10,09 
17,48 
8,67 

X2 
15,09 
27,63 
26,82 
33,04 

X3 
3,84 
10,16 
7,69 
8,05 

X4 
93,77 
64,25 
88,98 
84,97 

X5 
5.396,84 
12.623,89 
7.832,34 
10.551,18 

X6 
1,38 
7,59 
3,36 
3,86 

Berdasarkan  Tabel  4  di  atas,  warna  merah 
menunjukan nilai rata-rata tertinggi dan warna biru 
menunjukan  nilai  rata-rata 
tiap 
variabel yang dibandingkan dengan seluruh cluster. 

terendah  dari 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Berdasarkan  hal  tersebut,  diperoleh  karakteristik 
dari tiap cluster yang terbentuk sebagai berikut : 
1.  Cluster  1  adalah  cluster  yang  dikategorikan 
memiliki  tingkat  kesejahteraan  sosial  sangat 
rendah.  Cluster 
ini  merupakan  kelompok 
kabupaten/kota  yang  memiliki  persentase 
penduduk  yang  mempunyai  keluhan  kesehatan 
selama  sebulan terakhir, rata-rata lama sekolah, 
pengeluaran  perkapita  yang  disesuaikan  dan 
tingkat  pengangguran  terbuka  terendah,  serta 
persentase  penduduk  miskin  dan  persentase 
rumah  tangga  yang  tinggal  di  bangunan  tempat 
tinggal milik sendiri tertinggi diantara kelompok 
yang lain. 

2.  Cluster  2  adalah  cluster  yang  dikategorikan 
memiliki  tingkat  kesejahteraan  sosial  sangat 
tinggi.  Cluster 
ini  merupakan  kelompok 
kabupaten/kota yang memiliki persentase rumah 
tangga yang tinggal di bangunan tempat tinggal 
milik  sendiri  terendah,  persentase  penduduk 
miskin 
rendah,  persentase  penduduk  yang 
mempunyai  keluhan  kesehatan  selama  sebulan 
terakhir  tinggi,  serta  rata-rata  lama  sekolah, 
pengeluaran  per  kapita  disesuaikan  dan  tingkat 
diantara 
pengangguran 
kelompok yang lain. 

tertinggi 

terbuka 

3.  Cluster  3  adalah  cluster  yang  dikategorikan 
memiliki  tingkat  kesejahteraan  sosial  rendah. 
Cluster ini merupakan kelompok kabupaten/kota 
yang  memiliki  persentase  penduduk  yang 
mempunyai  keluhan  kesehatan  selama  sebulan 
terakhir, rata-rata lama sekolah, pengeluaran per 
tingkat 
disesuaikan 
kapita 
pengangguran  terbuka  rendah,  serta  persentase 
penduduk  miskin  dan  persentase  rumah  tangga 
yang  tinggal  di  bangunan  tempat  tinggal  milik 
sendiri tinggi. 

yang 

dan 

4.  Cluster  4  adalah  cluster  yang  dikategorikan 
memiliki  tingkat  kesejahteraan  sosial  tinggi. 
Cluster ini merupakan kelompok kabupaten/kota 
yang  memiliki  persentase  penduduk  miskin 
terendah,  persentase rumah tangga yang tinggal 
di bangunan tempat tinggal milik sendiri rendah, 
rata-rata  lama  sekolah,  pengeluaran  per  kapita 
disesuaikan  dan  tingkat  pengangguran  terbuka 
yang 
serta  persentase  penduduk 
tinggi, 
mempunyai  keluhan  kesehatan  selama  sebulan 
terakhir tertinggi diantara kelompok yang lain. 

Clustering 

kabupaten/kota  wilayah  KTI 
menggunakan  metode  k-means  clustering  dengan 
jumlah cluster (k) sama dengan 4 divisualisasikan ke 
dalam  bentuk  peta  pada  Gambar  9.  Berdasarkan 
gambar tersebut, dapat dilihat bahwa warna biru dan 
kuning mendominasi pada peta. Hal ini menunjukan 
bahwa sebagian besar wilayah KTI didominasi oleh 
kabupaten/kota  yang  memiliki  kesejahteraan  sosial 
yang masuk ke kategori tinggi dan rendah. 

 5 / 9 

 
 
 
 
 
 
 
                        Sumber : Hasil pengolahan 

                     Gambar 9. Pengelompokan k-means clustering dengan k = 4 

c.  Fuzzy C-Means dengan k = 3 

yang 

Berdasarkan 

telah 
pengelompokan 
dilakukan  dengan  menggunakan  fuzzy  c-means 
dengan  jumlah  cluster  (k)  sebanyak  3,  didapatkan 
hasil  bahwa  cluster  1  terdiri  45  kabupaten/kota, 
cluster 2 terdiri dari  48 kabupaten/kota dan cluster 
3 terdiri dari 139 kabupaten/kota. 

                                              TABEL V 

     RATA-RATA HASIL FUZZY C-MEANS (K = 3) 

Cluster 
Cluster 1 
Cluster 2 
Cluster 3 

X1 
30,25 
9,61 
11,12 

X2 
22,29 
28,45 
30,15 

X3 
5,95 
9,92 
7,86 

X4 
91,21 
67,11 
87,25 

X5 
6.335,44 
12.357,38 
9.393,96 

X6 
2,32 
7,18 
3,57 

terendah  dari 

Berdasarkan  Tabel  5  di  atas,  warna  merah 
menunjukan nilai rata-rata tertinggi dan warna biru 
menunjukan  nilai  rata-rata 
tiap 
variabel yang dibandingkan dengan seluruh cluster. 
Berdasarkan  hal  tersebut,  diperoleh  karakteristik 
dari tiap cluster yang terbentuk sebagai berikut : 
1.  Cluster  1  adalah  cluster  yang  dikategorikan 
memiliki  tingkat  kesejahteraan  sosial  rendah. 
Cluster ini merupakan kelompok kabupaten/kota 
yang  memiliki  persentase  penduduk  yang 
mempunyai  keluhan  kesehatan  selama  sebulan 
terakhir, rata-rata lama sekolah, pengeluaran per 
kapita  disesuaikan  dan  tingkat  pengangguran 
terbuka  terendah,  serta  persentase  penduduk 
miskin dan persentase rumah tangga yang tinggal 
di  bangunan 
tinggal  milik  sendiri 
tempat 
tertinggi diantara kelompok yang lain. 

tangga  yang 

2.  Cluster  2  adalah  cluster  yang  dikategorikan 
memiliki  tingkat  kesejahteraan  sosial  tinggi. 
Cluster ini merupakan kelompok kabupaten/kota 
yang memiliki persentase penduduk miskin dan 
persentase  rumah 
tinggal  di 
bangunan tempat tinggal milik sendiri terendah, 
persentase  penduduk  yang  mempunyai  keluhan 
kesehatan  selama  sebulan  terakhir  yang  berada 
pada posisi sedang, serta rata-rata lama sekolah, 
pengeluaran  per  kapita  disesuaikan  dan  tingkat 
pengangguran 
diantara 
kelompok yang lain. 

tertinggi 

terbuka 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

3.  Cluster  3  adalah  cluster  yang  dikategorikan 
memiliki tingkat kesejahteraan sosial menengah. 
Cluster ini merupakan kelompok kabupaten/kota 
yang memiliki persentase penduduk miskin, rata-
rata lama sekolah, persentase rumah tangga yang 
tinggal  di 
sendiri, 
pengeluaran  per  kapita  yang  disesuaikan  dan 
tingkat pengangguran terbuka yang berada posisi 
sedang, 
serta  persentase  penduduk  yang 
mempunyai  keluhan  kesehatan  selama  sebulan 
terakhir tertinggi diantara kelompok yang lain. 

tinggal  milik 

tempat 

Clustering 

kabupaten/kota  wilayah  KTI 
menggunakan metode fuzzy c-means dengan jumlah 
cluster (k) sama dengan 3 divisualisasikan ke dalam 
bentuk  peta  pada  Gambar  10.  Berdasarkan  gambar 
tersebut,  dapat  dilihat  bahwa  warna  kuning 
mendominasi pada peta. Hal ini menunjukan bahwa 
sebagian  besar  wilayah  KTI  didominasi  oleh 
kabupaten/kota  yang  memiliki  kesejahteraan  sosial 
yang masuk ke kategori menengah. 

                       Sumber : Hasil pengolahan 

                     Gambar 10. Pengelompokan fuzzy c-means dengan k = 3 

d.  Fuzzy C-Means dengan k = 4 

yang 

Berdasarkan 

telah 
pengelompokan 
dilakukan  dengan  menggunakan  fuzzy  c-means 
dengan  jumlah  cluster  (k)  sebanyak  4,  didapatkan 
hasil  bahwa  cluster  1  terdiri    24  kabupaten/kota, 
cluster  2  terdiri  dari    36  kabupaten/kota,  cluster  3 
terdiri dari  93 kabupaten/kota dan cluster 4 terdiri 
dari 79 kabupaten/kota.  

  TABEL VI 
    RATA-RATA HASIL FUZZY C-MEANS (K = 4) 

Cluster 
Cluster 1 
Cluster 2 
Cluster 3 
Cluster 4 

X1 
34,43 
8,66 
16,44 
8,88 

X2 
16,81 
28,99 
31,11 
28,08 

X3 
4,49 
10,17 
7,61 
8,28 

X4 
92,88 
63,94 
89,50 
83,53 

X5 
5.485,71 
13.022,53 
8.252,76 
10.329,54 

X6 
1,74 
7,60 
3,16 
4,24 

Berdasarkan  Tabel  6  di  atas,  warna  merah 
menunjukan nilai rata-rata tertinggi dan warna biru 
menunjukan  nilai  rata-rata 
tiap 
variabel yang dibandingkan dengan seluruh cluster. 

terendah  dari 

 6 / 9 

 
 
 
 
 
 
  
 
 
Berdasarkan  hal  tersebut,  diperoleh  karakteristik 
dari tiap cluster yang terbentuk sebagai berikut : 
1.  Cluster  1  adalah  cluster  yang  dikategorikan 
memiliki  tingkat  kesejahteraan  sosial  sangat 
rendah.  Cluster 
ini  merupakan  kelompok 
kabupaten/kota  yang  memiliki  persentase 
penduduk  yang  mempunyai  keluhan  kesehatan 
selama  sebulan terakhir, rata-rata lama sekolah, 
pengeluaran  per  kapita  disesuaikan  dan  tingkat 
pengangguran terbuka terendah, serta persentase 
penduduk  miskin  dan  persentase  rumah  tangga 
yang  tinggal  di  bangunan  tempat  tinggal  milik 
sendiri tertinggi diantara kelompok yang lain. 
2.  Cluster  2  adalah  cluster  yang  dikategorikan 
memiliki  tingkat  kesejahteraan  sosial  sangat 
ini  merupakan  kelompok 
tinggi.  Cluster 
kabupaten/kota  yang  memiliki  persentase 
penduduk  miskin  dan  persentase  rumah  tangga 
yang  tinggal  di  bangunan  tempat  tinggal  milik 
sendiri  terendah,  persentase  penduduk  yang 
mempunyai  keluhan  kesehatan  selama  sebulan 
terakhir  tinggi,  serta  rata-rata  lama  sekolah, 
pengeluaran  per  kapita  disesuaikan  dan  tingkat 
pengangguran 
diantara 
kelompok yang lain. 

tertinggi 

terbuka 

lama 

terbuka 

3.  Cluster  3  adalah  cluster  yang  dikategorikan 
memiliki  tingkat  kesejahteraan  sosial  rendah. 
Cluster ini merupakan kelompok kabupaten/kota 
sekolah, 
rata-rata 
yang  memiliki 
pengeluaran  per  kapita  disesuaikan  dan  tingkat 
pengangguran 
rendah,  persentase 
penduduk  miskin  dan  persentase  rumah  tangga 
yang  tinggal  di  bangunan  tempat  tinggal  milik 
sendiri  tinggi,  serta  persentase  penduduk  yang 
mempunyai  keluhan  kesehatan  selama  sebulan 
terakhir tertinggi diantara kelompok yang lain. 
4.  Cluster  4  adalah  cluster  yang  dikategorikan 
memiliki  tingkat  kesejahteraan  sosial  tinggi. 
Cluster ini merupakan kelompok kabupaten/kota 
yang  memiliki  persentase  penduduk  miskin, 
persentase  penduduk  yang  mempunyai  keluhan 
kesehatan selama sebulan terakhir dan persentase 
rumah  tangga  yang  tinggal  di  bangunan  tempat 
tinggal milik sendiri rendah, serta rata-rata lama 
sekolah, pengeluaran per kapita disesuaikan dan 
tingkat pengangguran terbuka tinggi. 

Clustering 

kabupaten/kota  wilayah  KTI 
menggunakan metode fuzzy c-means dengan jumlah 
cluster (k) sama dengan 4 divisualisasikan ke dalam 
bentuk  peta  pada  Gambar  11.  Berdasarkan  gambar 
tersebut, dapat dilihat bahwa warna biru dan kuning 
mendominasi pada peta. Hal ini menunjukan bahwa 
sebagian  besar  wilayah  KTI  didominasi  oleh 
kabupaten/kota  yang  memiliki  kesejahteraan  sosial 
yang masuk ke kategori tinggi dan rendah. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

                        Sumber : Hasil pengolahan 

                     Gambar 11. Pengelompokan fuzzy c-means dengan k = 4 

6.2 Evaluasi Hasil Cluster 

Evaluasi hasil cluster merupakan tahapan yang dilakukan 
untuk  mencari  algoritma  machine  learning  terbaik  yang 
digunakan  dalam  proses  clustering  kesejahteraan  sosial 
kabupaten/kota  wilayah  KTI  yang  dilakukan  pada 
penelitian  ini.  Metode  evaluasi  hasil  clustering  yang 
digunakan pada penelitian ini adalah metode davies bouldin 
index dan dunn index. Berdasarkan perhitungan yang telah 
dilakukan, diperoleh hasil seperti pada Tabel 7. 

                                   TABEL VII 

                                       EVALUASI HASIL CLUSTERING 

Metode 
k-means clustering (k=3) 

k-means clustering (k=4) 
fuzzy c-means (k=3) 
fuzzy c-means (k=4) 

Davies Bouldin Index 
1,1126 
1,4388 
1,2668 
1,6228 

Dunn Index 
0,1179 
0,0817 
0,0640 
0,0940 

Berdasarkan  Tabel  7  di  atas,  dapat  dilihat  bahwa  hasil 
clustering  yang  memiliki  nilai  davies  bouldin  index 
terendah  dan  dunn  index  tertinggi  adalah  hasil  clustering 
dengan  menggunakan  metode  k-means  clustering  dengan 
jumlah  cluster  (k)  sama  dengan  3.  Sehingga,  dapat 
disimpulkan  bahwa  algoritma  machine  learning  terbaik 
yang  digunakan  dalam  penelitian  ini  adalah  metode  k-
means clustering dengan jumlah cluster (k) sebanyak 3. 

6.3 Dashboard Infomasi 

Dashboard informasi yang dibangun dalam penelitian ini 
merupakan  dashboard  yang  menampikan  mengenai  hasil 
pengelompokan  kesejahteraan 
sosial  dari  algoritma 
machine learning terbaik yang digunakan dalam penelitian 
yang  dilakukan.  Pembangunan  dashboard 
informasi 
tersebut sebagai bentuk visualisasi dari hasil yang diperoleh 
dalam penelitian ini,  yang bertujuan untuk mempermudah 
pemahaman  informasi  dari  hasil  yang  didapatkan  dalam 
ini.  Terdapat  4  menu  dalam  dashboard 
penelitian 
kesejahteraan sosial yang dibangun, yaitu :  

a.  Menu Beranda 

Halaman  Beranda  merupakan  halaman  awal  dari 
dashboard  informasi  yang  dibangun.  Sebagaimana 
dapat  dilihat  pada  Gambar  12  bahwa  halaman  ini 
menampilkan informasi singkat mengenai dashboard 

 7 / 9 

 
 
 
 
 
 
kesejahteraan  sosial  yang  dibangun  pada  penelitian 
ini. 

kabupaten/kota di tiap provinsi  yang  menjadi bagian 
dari wilayah KTI pada tahun 2019 (Lihat Gambar 15). 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

                   Sumber : Hasil pengolahan 

                    Gambar 12. Halaman Menu Beranda 

                   Sumber : Hasil pengolahan 
                   Gambar 15. Halaman Menu Tabel Data 

d.  Menu Konsep 

b.  Menu Peta Tematik 

Halaman  Peta  Tematik  merupakan  halaman  yang 
menampilkan  visualisasi  hasil  pengelompokan 
kesejahteraan  sosial  wilayah  KTI  tahun  2019  dari 
algoritma  machine  learning  terbaik  yang  digunakan 
dalam penelitian ini. Visualisasi hasil pengelompokan 
yang  ditampilkan  pada  halaman  ini  berupa  peta  dan 
tabel (Lihat Gambar 13 dan Gambar 14).  

Halaman  Konsep  merupakan  halaman  yang 
menampilkan mengenai konsep dan definisi dari kata 
penting 
dashboard 
kesejahteraan  sosial  yang  dibangun  dalam  penelitian 
ini (Lihat Gambar 16). 

digunakan 

dalam 

yang 

                    Sumber : Hasil pengolahan 

                   Gambar 13. Halaman Menu Peta Tematik [1] 

                Sumber : Hasil pengolahan 
              Gambar 16. Halaman Menu Konsep 

Evaluasi  terhadap  dashboard kesejahteraan  sosial  yang 
telah  dibangun  dilakukan  dengan  uji  usability  yang 
dilakukan  terhadap  10  responden  dengan  menggunakan 
instrumen  kuesioner  SUS  (System  Usability  Scale)  yang 
terdiri dari 10 pertanyaan. Berdasarkan Tabel 8, diketahui 
bahwa nilai SUS yang diperoleh adalah 86,75. Angka yang 
didapatkan tersebut lebih besar dari 68, yang artinya bahwa 
dashboard  yang  telah  dibangun  dinilai  cukup  baik  dan 
dapat digunakan oleh pengguna [12]. 

TABEL VIII 
HASIL UJI SUS (SYSTEM USABILITY SCALE) 

                    Sumber : Hasil pengolahan 

                   Gambar 14. Halaman Menu Peta Tematik [2] 

c.  Menu Tabel Data 

Responden 

1 
2 
3 
4 
5 
6 
7 
8 
9 
10 

1 
4 
4 
4 
3 
4 
3 
3 
3 
4 
2 

2 
4 
4 
3 
4 
4 
3 
4 
3 
4 
4 

3 
4 
4 
3 
3 
4 
3 
4 
4 
4 
4 

Nilai Pertanyaan 
7 
4 
4 
4 
4 
4 
3 
4 
3 
4 
4 

5 
4 
4 
4 
3 
4 
3 
3 
3 
3 
3 
Nilai SUS 

6 
4 
4 
3 
4 
3 
4 
3 
3 
3 
2 

4 
0 
4 
3 
4 
2 
3 
4 
4 
4 
4 

Halaman  Tabel  Data  merupakan  halaman  yang 
menampilkan data lengkap dari semua variabel yang 
berdasarkan 
penelitian 
digunakan 

dalam 

ini 

8 
4 
4 
4 
4 
3 
4 
4 
4 
4 
4 

9 
4 
4 
3 
4 
3 
3 
4 
4 
4 
3 

10 
4 
3 
3 
3 
2 
2 
4 
2 
3 
1 

Nilai 
Responden 
90 
97,5 
85 
90 
82,5 
77,5 
92,5 
82,5 
92,5 
77,5 
86,75 

 8 / 9 

 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

VII. 

PENUTUP 

Penelitian ini telah berhasil menerapkan algoritma machine 
learning  untuk  pengelompokan  kesejahteraan  sosial  wilayah 
KTI tahun 2019 dengan metode k-means clustering dan fuzzy 
c-means  dengan  jumlah  cluster  (k)  yaitu  3  dan  4.  Kemudian 
dari  hasil  proses  clustering  tersebut,  dilakukan  perbandingan 
kualitas  hasil  pengelompokan  yang  diperoleh  dan  didapatkan 
bahwa algoritma machine learning terbaik dalam penelitian ini 
adalah  k-means  clustering  dengan  jumlah  cluster  (k)  sama 
dengan 3. Visualisasi dari hasil pengelompokan kesejahteraan 
sosial dengan algoritma machine learning terbaik dalam bentuk 
dashboard  informasi  telah  dibangun  dengan  menggunakan 
package Shiny. 

Dari hasil penelitian ini, diharapkan nantinya dapat dijadikan 
referensi  bagi  para  pengambil  kebijakan  agar  dapat 
melaksanakan  program  serta  kebijakan  kesejahteraan  sosial 
yang efektif dan tepat sasaran di wilayah KTI. Untuk penelitian 
selanjutnya,  diharapkan  dapat  mengembangkan  penelitian  ini 
dengan  menambah 
dimensi 
kesejahteraan sosial yang belum tercakup, menerapkan metode 
clustering  lain  untuk  pengelompokan  kesejahteraan  sosial, 
serta  dapat  mengembangkan  dashboard  informasi  yang  telah 
dibangun. 

variabel-variabel 

dari 

DAFTAR PUSTAKA 
[1]  Kementerian  Sosial  (2019)  Undang-Undang  Nomor  11  Tahun  2009 

Tentang Kesejahteraan Sosial. 

[2]  Setiawan,  H.  H.,  “Merumuskan  Indeks  Kesejahteraan  Sosial  (IKS)  di 

Indonesia,” Sosio Informa, 5(3), 208-222, 2019. 
Kemiskinan 
Statistik. 

Pusat 

[3]  Badan 

dan 

Ketimpangan. 

https://www.bps.go.id/subject/23/kemiskinan-dan-ketimpangan.html. 
Diakses Pada Tanggal 10 September 2020. 

[4]  Ramdhani, F., Hoyyi, A., dan Mukid, M. A., “Pengelompokan Provinsi di 
Indonesia berdasarkan Karakteristik Kesejahteraan Rakyat menggunakan 
Metode K-Means Cluster,” Jurnal Gaussian, 4(4), 875-884, 2015. 

[5]  Dwitiyanti, N., Selvia, N., dan Andrari, A. R., “Penerapan Fuzzy C-Means 
Cluster  dalam  Pengelompokan  Provinsi  Indonesia  Menurut  Indikator 
Kesejahteraan Rakyat,” Faktor Exacta, 12(3), 201-209, 2019. 

[6]  Hidayat,  R.,  Wasono,  R.,  dan  Darsyah,  M.  Y.,  “Pengelompokan 
Kabupaten/Kota  di  Jawa  Tengah  menggunakan  Metode  K-Means  dan 
Fuzzy  C-Means,”  Seminar  Nasional  Pendidikan,  Sains  dan  Teknologi 
Fakultas  Matematika  dan 
Ilmu  Pengetahuan  Alam  Universitas 
Muhammadiyah Semarang, 240-250. 

[7]  Anuraga,  G.,  “Hierarchical  Clustering  Multiscale  Bootstrap  untuk 
Pengelompokan Kemiskinan di Jawa Timur,” Statistika, 1(3), 27-33, 2015. 
[8]  Ningrat, D. R., Maruddani, D. A. I., dan Wuryandari, T., “Analisis Cluster 
dengan  Algoritma  K-Means  dan  Fuzzy  C-Means  Clustering  untuk 
Pengelompokan Data Obligasi Korporasi,” Jurnal Gaussian, 5(4), 641-650, 
2016. 

[9]  Agustini,  F.,  “Implementasi  Algoritma  Fuzzy  C-Means  Studi  Kasus 
Penjualan  di  Sushigroove  Restaurant,”  Jurnal  Ilmu  Pengetahuan  dan 
Teknologi Komputer, 3(1), 127-132, 2017. 

[10] Prasetyo, E., “Data Mining : Konsep dan Aplikasi Menggunakan Matlab,” 

Yogyakarta : Andi Offset, 2012. 

[11] Bezdek,  J.  C.,  Enrlich,  R.,  dan  Full,  W.,  “FCM  :  The  Fuzzy  C-Means 
Clustering Algortihm,” Computer & Geosciences, 10(2-3), 191-203, 1984. 
[12] Bangor, A., Kortum, P. T. dan Miller, J. T., “An Empirical Evaluation of 
the  System  Usability  Scale,”  International  Journal  of  Human-Computer 
Interaction, 24(6), 574-594, 2008. 

 9 / 9 

 
 
 
"
221709604,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Penerapan Passive Mobile Positioning Data dalam 
Statistik Pariwisata dengan Pendekatan Point of 
Interest 
(Studi Kasus : Daerah Istimewa Yogyakarta, Indonesia) 
Dandy Adetiar Al Rizki (221709604, 4SD1) 
Dosen Pembimbing: Setia Pramana 

Ringkasan—Statistik pariwisata resmi digunakan sebagai tolok 
ukur  dalam  pembangunan  kepariwisataan.  Saat  ini,  dalam 
pengumpulan  statistik  pariwisata  resmi,  big  data  dimanfaatkan 
sebagai  sumber  data  lain.  Dalam  penelitian  pariwisata,  big  data 
yang  menggunakan  teknologi  pelacakan  lokasi  seperti  Mobile 
Positioning  Data  (MPD)  pada  statistik  pariwisata  resmi  masih 
sangat  sedikit.  Metode  pendekatan  dengan  passive  mobile 
positioning  data  merupakan  metode  yang  menjadi  mayoritas 
dalam  penerapan  MPD  di  sektor  pariwisata.  Penelitian  ini, 
menyusun  dan  memaparkan  algoritme  berbasis  sampel  acak 
MPD  pasif  yang  dapat  mengukur  jumlah  pengunjung  wisata 
dengan  pendekatan  point  of 
interest  yang  mendukung 
kepariwisataan  sebagai  kajian  penerapan  MPD  pasif  dalam 
statistik  pariwisata.  Dengan  mengimplementasikan  sampel  acak 
MPD  pasif  dari  operator  jaringan  seluler  dan  proxy  dari  PoI, 
algoritme  yang  dibangun  dievaluasi  melalui  pattern  suitability. 
Ditunjukkan  dari  kesesuaian  pola,  algoritme  dapat  digunakan 
untuk  mengukur  jumlah  pengunjung  di  suatu  area  geografis. 
Kemudian, hasil penerapan dievaluasi lebih lanjut dengan analisis 
untuk mengetahui insight dari output dan keandalan algoritme. 

Kata Kunci— pariwisata, big data, mobile positioning data, point 

of interest, pattern suitability. 

I.  LATAR BELAKANG 

Keberhasilan  pembangunan  kepariwisataan  di  Indonesia 
dapat  diketahui  melalui  informasi  tentang  kepariwisataan 
dalam statistik pariwisata resmi yang dikeluarkan oleh Badan 
Pusat  Statistik  (BPS).  Untuk    mengetahui  informasi  tentang 
kepariwisataan  dapat  dilakukan  dengan  survei  pengunjung  di 
transportasi[16]. 
tempat-tempat  wisata  utama  atau  pusat 
Namun,  survei  memiliki  beberapa  kekurangan  seperti  tingkat 
respons  yang relatif rendah dan keengganan responden  untuk 
menjawab[18], serta tidak dapat menunjukkan mobilitas orang-
orang  sebelum  dan  sesudah  kunjungan  dan  asal-usul  orang 
yang  mengunjungi 
ini  untuk 
mengatasi  kekurangan 
tersebut  digunakanlah  pendekatan 
teknologi dengan memanfaatkan big data[22]. 

tersebut[24].  Saat 

tempat 

Big  data  adalah  istilah  umum  untuk  ledakan  kuantitas  dan 
keragaman data dari high frequency digital data dan biasanya 
tidak berasal dari sumber tradisional[15]. Berdasarkan sumber 
data  dalam  penelitian  pariwisata,  big  data  terbagi  dalam  tiga 
kategori utama[12], yaitu data yang dihasilkan oleh pengguna 
(media  sosial); data  berdasarkan  perangkat  (lokasi,  jenis 
perangkat  seluler,  dll); dan  data  transaksi  (data  pemesanan 
online).  Data  tekstual  dan  foto  online  di  media  sosial, 
terkendala dengan keandalan data (misal adanya ulasan palsu) 

dan  faktor  penting  lainnya  dalam  dimensi  temporal  seperti 
waktu  dan  durasi  kunjungan[12].  Data  pada  perangkat 
persoalan utamanya  bagaimana  mendapatkan data  dan terkait 
kerahasiaan  pelanggan[15].  Kemudian  data  pemesanan  dan 
pembelian  online  (misal  layanan  akomodasi)  bukan  untuk 
jumlah 
menganalisis  kunjungan  aktual  atau  mengukur 
kunjungan aktual[11]. 

Terlepas dari kendala yang ada, data lokasi pada perangkat 
seperti Mobile Positioning Data (MPD) merupakan salah satu 
sumber  data  digital  baru  yang  digunakan  dalam  penelitian 
pariwisata yang memiliki potensi besar untuk digunakan dalam 
statistik  pariwisata  resmi  nasional  dan  untuk  alasan  tersebut 
juga  telah  diterapkan[16].  Sejak  tahun  2016,  BPS  mulai 
melakukan  eksplorasi  pemanfaatan  big  data  melalui  MPD 
lain  dalam  penyusunan  statistik 
sebagai  sumber  data 
pariwisata[4].  Potensi  penggunaan  MPD  dalam  statistik 
pariwisata resmi telah dicatat di tingkat Eropa oleh Eurostat[10] 
dan Internasional oleh UN Global Working Group on Big Data 
for Official Statistics[21].  

Secara  terminologi,  MPD  merupakan  data  hasil  pelacakan 
koordinat  lokasi  ponsel  melalui  jaringan  seluler.  MPD  dapat 
diperoleh  melalui  cara  “aktif”  atau  “pasif”.  Penentuan  posisi 
aktif menyiratkan pelacakan ponsel menggunakan permintaan 
khusus  dengan  izin  dari  pengguna  ponsel[16].  Sedangkan 
Passive Mobile Positioning Data atau MPD pasif dikumpulkan 
secara  otomatis  oleh  operator  jaringan  seluler  untuk  tujuan 
penagihan dalam log files meliputi panggilan masuk dan keluar 
serta pesan terkirim[16]. 
Penerapan  MPD 

telah  dilakukan  di  banyak  bidang, 
diantaranya: mengukur jumlah pengunjung asing dan domestik, 
serta  menganalisis  mobilitas  mereka  selama  pekan  Asian 
Games[24];  menggambarkan  wilayah  metropolitan  di 
Indonesia  sebagai  cara  untuk  menjelaskan  berbagai  proses 
transformasi  di  Statistics  Business  Process  Framework  and 
Architecture  (SBFA)[13];    bahkan  penggunaan  MPD  untuk 
mengukur pariwisata di perbatasan Indonesia[7]. 

Dalam  penelitian  kepariwisataan  yang  menggunakan  data 
jaringan seluler, terkait pada aktivitas ruang-waktu pengunjung, 
MPD  pasif  merupakan  metode  pendekatan  yang  menjadi 
mayoritas[19]. Untuk pengumpulannya, operator seluler dapat 
mengumpulkan data geografis anonim dari log files, yang pada 
akhirnya tidak melanggar identitas dan privasi pribadi[2]. 

Terlepas dari banyaknya penerapan tersebut dan kenyataan 
bahwa kemajuan teknologi telah membuka kemungkinan baru 

 1 / 8 

 
 
 
 
untuk  menganalisis  pergerakan  wisatawan  dalam  skala  yang 
besar,  masih  sangat  sedikit  penelitian  yang  dilakukan 
menggunakan teknologi pelacakan lokasi dalam skala nasional, 
terutama dalam fokus statistik pariwisata resmi[19]. 
Minimnya  penelitian  yang  menggunakan 

teknologi 
pelacakan 
lokasi  terutama  yang  berfokus  pada  statistik 
pariwisata resmi, menjadi dorongan untuk mengkaji penerapan 
MPD  pasif  dalam  statistik  pariwisata  dengan  data  kunjungan 
wisata  di  objek  tertentu  yang  mendukung  kepariwisataan 
nasional. Dalam hal ini, disebut sebagai Point of Interest (PoI) 
destinasi wisata, yaitu entitas geografis dalam pariwisata yang 
diabstraksi  sebagai  titik.  Menyusun  prosedur  atau  algoritme 
yang  dapat  mengukur  jumlah  pengunjung  wisata  pada  PoI 
destinasi  wisata  dengan  metode  yang  tepat  untuk  menangani 
dan memproses data adalah tugas yang menantang. Oleh karena 
itu, pendekatan big data menjadi penting dan digunakan dalam 
penelitian ini. 

II.  TUJUAN PENELITIAN 

Tujuan  yang  akan  dicapai  dalam  penelitian  ini  adalah 

sebagai berikut : 

1.  menyusun  pendekatan  berbasis  big  data  dalam 
statistik  pariwisata  melalui  pembangunan  algoritme 
yang  dapat  mengukur 
jumlah  pengunjung  dan 
menggambarkan  kepadatan  pengunjung  dari  sampel 
acak  MPD  pasif  pada  objek  atau  destinasi  wisata  di 
tingkat  kabupaten/kota  dengan  pendekatan  PoI 
destinasi wisata; dan 

2.  mengidentifikasi tipe pengunjung wisata berdasarkan 
kunjungan  pada  PoI  destinasi  wisata  melalui 
implementasi  algoritme  berbasis  MPD  pasif  dengan 
sampel acak. 

III. PENELITIAN TERKAIT 

Ada  berbagai  penelitian  mengenai  pemanfaatan  big  data 
yang  telah  menggunakan  MPD.  Ahas  dkk.[2]  sebagai  hasil 
penelitiannya  menunjukkan  MPD  pasif  sebagai  sumber  yang 
menjanjikan  untuk  penelitian  pariwisata,  diantaranya 
menggunakan statistik pengunjung, berdasarkan studi kasus di 
Estonia.  Dalam  penelitiannya,  sebagai  proses  validasi  MPD 
pasif  dibandingkan  dengan  statistik  akomodasi  berdasarkan 
kesesuaian  pola  (pattern  suitability)  dan  korelasi  bulanan 
berdasarkan daerah. 

Sikder  dkk.[20]  mengusulkan  metode  untuk  mendeteksi 
wisatawan  di  antara  total  populasi  dengan  menganalisis  data 
lokasi  CDR  telepon  seluler  (MPD  pasif)  melalui  seperangkat 
aturan,  yaitu  NegativeRules  untuk  menjelaskan  pengguna 
ponsel  tidak  dianggap  sebagai  wisatawan  dan  PositiveRules 
untuk  menjelaskan  pengguna  ponsel  dianggap  sebagai 
wisatawan. 

Dalam  kajian 

terhadap  statistik  komuter,  Pradika[14] 
menggunakan MPD untuk mengetahui variabel rumah melalui 
algoritme Density Based Spatial Clustering Applied with Noise 
(DBSCAN).  Hasil  penelitiannya  menunjukkan  penerapan 
algoritme DBSCAN dengan syarat memenuhi statistik hopkins 
memiliki  akurasi  hingga  90,48%  dalam  mendeteksi  lokasi 
rumah pada data MPD. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Berfokus  pada  pariwisata,  Raun[16]  membahas  langkah-
langkah metodologi penggunaan MPD pasif untuk menghitung 
statistik  pariwisata  (jumlah  pengunjung  dan  kunjungan)  yang 
digunakan  untuk  analisis  destinasi  wisata  seperti  berdasarkan 
frekuensi  dan  durasi.  Keterwakilan  hasil  penerapan  MPD 
dievaluasi  melalui  perbandingan  antara  hasil  yang  diperoleh 
dari MPD dengan jumlah bulanan  malam  yang dihabiskan  di 
tempat  akomodasi  di  Estonia,  menurut  negara  tempat  tinggal 
dari periode waktu yang sama. Evaluasi menunjukkan jumlah 
malam keseluruhan yang dihabiskan dari MPD sejalan dengan 
hasil yang diperoleh dari data akomodasi.  

Widyasanti  dkk.[24]  menyajikan  metodologi  penggunaan 
MPD untuk mengukur jumlah pengunjung asing dan domestik, 
serta  menganalisis  mobilitasnya  berdasarkan  point  of  interest 
(PoI) di lokasi Asian Games 2018, selama pekan Asian Games. 
Hasil  penelitiannya  memberikan  pemahaman,  bahwa  metode 
pengukuran  jumlah  pengunjung  berdasarkan  PoI  dengan 
menggunakan  MPD,  bermanfaat  dalam  melengkapi  sumber 
data konvensional. 

IV. METODE PENELITIAN  
Pembangunan  prosedur  dalam  penelitian  ini  menggunakan 

metode design science research. 

Gambar 1. Design science research process model[23]  

A.  Awareness of Problem 

Tahap 

proses 

pencarian 

ini  merupakan 

teknologi  pelacakan 

dan 
pengidentifikasian  masalah  melalui  kajian  pustaka  terkait 
penggunaan 
lokasi  pada  statistik 
pariwisata.  Output  yang  dihasilkan  adalah  proposal  berupa 
kajian  penerapan  MPD  pasif  dalam  statistik  pariwisata. 
Sebagai bagian dari tahap ini, peneliti mengusulkan proposal 
dilakukan dengan data kunjungan wisata di objek tertentu yang 
mendukung 
tingkat 
kabupaten/kota. 

kepariwisataan 

nasional 

hingga 

B.  Suggestion 

Setelah pengembangan proposal, dilakukan langkah kreatif 
fungsionalitas  baru  berdasarkan  konfigurasi  dari  elemen  baru 
dan/atau yang sudah ada. Output pada tahap ini adalah desain 
tentatif  berupa  prosedur  yang  mengukur  jumlah  pengunjung  
wisata  berbasis  big  data  di  lokasi  wisata  hingga  tingkat 
kabupaten/kota menggunakan sampel acak MPD Pasif dengan 
pendekatan PoI destinasi wisata.  

 2 / 8 

 
 
 
C.  Development 

Adapun variabel yang dikumpulkan dapat dilihat pada tabel 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Desain  tentatif  atau  sementara  dikembangkan  lebih  lanjut 
ini.  Pengembangan 
dan  diimplementasikan  dalam 
dilakukan  dengan  membentuk  artefak  berupa  algoritme  yang 
dimodelkan  menggunakan  flowchart.  Arsitektur  dibangun 
berdasarkan daerah asal; daerah tujuan; posisi dan lama objek 
yang  diukur  peneliti  saat  berada  dalam  area  penelitian  atau 
objek penelitian; serta proses agregasi. 

tahap 

berikut: 

TABEL I 
VARIABEL TERKUMPUL 

Variabel 

msisdn 
(identitas pelanggan operator seluler yang 
telah di-hashing dan bersifat unik) 

D.  Evaluation 

keabsahan/validitas 

Artefak  dievaluasi  sesuai  dengan  kriteria  yang  ditujukan 
untuk  mengetahui 
yang 
dikembangkan. Penyimpangan dari ekspektasi, baik kuantitatif 
maupun  kualitatif  harus  dijelaskan  secara  tentatif.  Evaluasi 
validasi dilakukan dengan kesesuaian pola (pattern suitability) 
fluktuasi melalui metode visual dan evaluasi lainnya dilakukan 
dengan analisis deskriptif yang dibantu dengan visualisasi. 

artefak 

E.  Conclusion 

ID(identitas) 

timestamp 
(waktu dan/atau tanggal) 

latitude (titik pada garis lintang) 

longitude (titik pada garis bujur) 

nama 
(kata yang menunjukkan sebutan suatu 
tempat) 

Sumber Data 

MPD Pasif 

Akomodasi 

MPD Pasif 

MPD Pasif, 
Akomodasi, ODTW 

MPD Pasif, 
Akomodasi, ODTW 

Akomodasi, ODTW 

Tahap ini merupakan akhir dari siklus penelitian. Akhir dari 
upaya  penelitian  merupakan  kepuasan  dari  hasil  penelitian, 
fakta  pengetahuan  dari  hasil  usaha  dalam  penelitian,  serta 
kemungkinan anomali yang mungkin berfungsi sebagai subjek 
penelitian lebih lanjut. 

tipe  
(kata yang menunjukkan jenis suatu tempat) 

Akomodasi, ODTW 

tanggal 

room_used  
(banyaknya ruangan/kamar yang digunakan) 

Akomodasi 

Akomodasi 

V.  KERANGKA PIKIR 

Melalui kajian pustaka ditemukan, mayoritas penelitian yang 
menggunakan teknologi pelacakan lokasi masih terfokus pada 
pertanyaan metodologi dan penggunaan data dalam penelitian 
analisis  perkotaan, 
transportasi  dan  mobilitas  manusia. 
Sehingga,  metode  untuk  mengukur  jumlah  pengunjung  pada 
destinasi wisata dalam statistik pariwisata, masih sangat sedikit. 
Penelitian  ini  bertindak  dengan  menyusun  prosedur  atau 
algoritme  berbasis  big  data  yang  dapat  mengukur  jumlah 
pengunjung wisata dalam statistik pariwisata, di objek tertentu 
yang  mendukung  kepariwisataan  nasional  hingga  tingkat 
kabupaten/kota. Prosedur ditentukan dengan beberapa metode 
diantaranya,  Reverse-Geocoding  (spatial  file  dan  nominatim) 
(DMDBSCAN)  untuk 
dan  Dynamic  Method  DBSCAN 
memperoleh  keterangan 
level 
kabupaten/kota;  serta  Distance  dan  Stay  Detection  sebagai 
syarat untuk bisa dikatakan sebagai pengunjung. 

lokasi  daerah  asal  di 

Dalam  perancangannya  metode  disusun  melalui  penerapan 
sampel acak MPD pasif dengan pendekatan kunjungan di area 
sekitar PoI. Passive Mobile Positioning Data (Gambar 3) yang 
digunakan  merupakan  data  sampel  acak  tahun  2020  yang 
dikumpulkan  dari  Universitas  Gadjah  Mada 
(UGM). 
Kemudian,  Point  of  Interest  menggunakan  proxy  dalam 
menentukan  kepadatan  pengunjung  diantaranya  objek  daya 
tarik  wisata  (ODTW)  selaku  daya  tarik  suatu  wilayah 
pariwisata dan akomodasi selaku industri kepariwisataan yang 
berperan  besar  dalam  pembangunan  pariwisata.  Data 
akomodasi  yang  digunakan  berbasis  big  data  yang 
dikumpulkan  dari  BPS  tahun  2020  melalui  data  penyedia 
layanan  pemesanan  akomodasi  secara  online  yang  diperoleh 
berdasarkan web scraping[1]. Untuk memperoleh data ODTW 
tahun  2020,  metode  web  scraping  diterapkan  pada  google 
maps. 

Secara  umum,  kerangka  pikir  pada  penelitian  ini  adalah 

sebagai berikut : 

Gambar 2. Kerangka pemikiran 

 3 / 8 

 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 5. Proses clustering, algoritme penentuan pengunjung wisata 

Setiap msisdn yang memenuhi pengujian dilanjutkan dengan 
pengelompokan  menggunakan  DMDBSCAN,  dengan  dua 
parameter yaitu Eps (radius) dan MinPts (jumlah minimum titik 
dalam 
radius).  MinPts  di  set  statis  yang  ditentukan 
menggunakan jumlah tetangga terdekat (kth nearest neighbor) 
dari  k-dist  plot  yaitu  k=3[8].  Kemudian  Ester  dkk.[9] 
menyebutkan  MinPts=k,  sehingga  nilai  MinPts  dalam 
algoritme  ini  adalah  3.  Sedangkan,  Eps  di  set  mengikuti 
kepadatan  datanya,  dengan  melihat  perubahan 
tajam 
(knee/elbow)  pada  k-dist  plot[8].  Artinya,  akan  berbeda  di 
setiap  msisdn  dengan  kemungkinan  terdapat  Eps  yang  sama 
antar  msisdn.  Sebagai  alternatif  ketika  sistem  tidak  dapat 
menentukan  knee  karena  keterbatasan  dukungan  perangkat, 
maka digunakan Eps=0,001[14]. 

Output pada proses ini berupa kumpulan data msisdn yang 
ditetapkan  sebagai  daerah  asal,  namun  belum  terdapat  lokasi 
ataupun nama daerah di dalamnya. 

3) Penentuan Nama Daerah: Proses penentuan nama suatu 
daerah berdasarkan latitude-longitude pada MPD pasif dan PoI. 
Metode reverse-geocoding diterapkan untuk menentukan nama 
lokasi di level kabupaten/kota. Untuk data yang bersumber dari 
MPD  pasif,  proses  ini  diterapkan  pada  hasil  clustering  dan 
msisdn non-clustering (hopkins ≥ 0,5). 

Gambar 3. Passive mobile positioning data sample. 

VI. HASIL DAN PEMBAHASAN   

A.  Algoritme Penentuan Pengunjung Wisata 

1) Data  Cleansing:  Proses  analisis  kualitas  dari  suatu  data 
mencakup  kontrol  kualitas  data  awal  yang  dilakukan  dengan 
cara mengubah; mengoreksi; atau menghapus data yang salah; 
tidak lengkap; tidak akurat;  memiliki duplikat; atau  memiliki 
format  yang  salah  dalam  basis  data,  untuk  mendapatkan 
informasi yang benar secara sistematis melalui algoritme. 

Gambar 4. Proses data cleansing, algoritme penentuan pengunjung wisata 

ini 

MPD  pasif  dalam  algoritme 

tidak  memerlukan 
penyesuaian tipe, karena dianggap memiliki tipe yang sama di 
setiap  datanya.  Namun,  PoI  memerlukan  penyesuaian  tipe 
objek,  hal  ini  dikarenakan  data  terdiri  dari  kumpulan  objek 
dengan  tipe  yang  berbeda.  Kemudian  untuk  menghindari 
perbedaan format atau format yang salah baik pada MPD pasif 
maupun  PoI,  maka  format  tanggal  perlu  disesuaikan  menjadi 
satu bentuk mengikuti format baku. 

Selanjutnya  agar  data  memiliki  kejelasan;  tidak  ada  data 
yang kosong; terdapat jangkauan atau kisaran nilai; keunikan 
data  dan  pola  pengekspresian  berdasarkan  pada  ketentuan 
standar yang sudah ditentukan, dilakukanlah penghapusan null 
value dan duplikat.  

2) Clustering:  Proses  pengelompokan  data  MPD  untuk 
menentukan daerah asal pengguna ponsel yang menggunakan 
jaringan  operator  seluler  terkait.  Clustering  dapat  diterapkan 
pada MPD pasif apabila sebelumnya memenuhi nilai statistik 
hopkins  yang  digunakan  untuk  mengukur  kecenderungan 
cluster yaitu kurang dari 0,5 pada tingkat kepercayaan 90%[3]. 
Semakin  hopkins cenderung  mendekati 0,  maka  hipotesis  nol 
(tidak  ada  cluster  yang  bermakna)  dapat  ditolak  dan 
disimpulkan  bahwa  dataset secara signifikan  merupakan data 
yang  dapat  dikelompokkan.  MPD  pasif  diuji  secara  berulang 
pada masing-masing msisdn untuk menghasilkan nilai statistik 
hopkins, dengan mengambil titik (latitude-longitude) pada data. 

Gambar 6. Penentuan  nama  daerah  asal  MPD  pasif  (msisdn),  algoritme 

penentuan pengunjung wisata 

Terdapat  dua  sub-proses  di  dalamnya  yaitu  proses  pada 
MPD  Pasif  dan  PoI.  MPD  pasif  hasil  clustering  (latitude-
longitude);  msisdn  non-clustering  (latitude-longitude),  dan 
spatial  file  (poligon-poligon  digital)  digabungkan  untuk 
mendapatkan  nama  kabupaten/kota  daerah  asal.  Apabila  dari 
spatial  file  nama  daerah  tidak  ditemukan,  maka  dilakukan 
pendekatan  nominatim  (penanda  pada  peta  digital)  melalui 
latitude-longitude  berdasarkan  application 
proses 

input 

 4 / 8 

 
 
 
  
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

programming interface (API) yang diterapkan OpenStreetMap, 
untuk  mendapatkan  alamat  yang  memuat  kabupaten/kota. 
Pendekatan  nominatim  dilakukan  untuk  memaksimalkan 
penggunaan seluruh data,  namun apabila pendekatan tersebut 
gagal  maka  data  yang  dimaksud  akan  dihapus.  Kemudian 
penetapan  sebagai  daerah  asal  dari  proses  ini,  ditetapkan 
dengan  melihat  frekuensi  terbanyak  dari  nama  lokasi  yang 
ditemukan. 

Proses  yang  hampir  sama  secara  keseluruhan  pada  MPD 
pasif juga diterapkan pada data PoI. Perbedaannya terletak pada 
output yang dihasilkan yaitu daerah tujuan. Perbedaan lainnya, 
pada MPD pasif untuk menghasilkan output ditentukan dengan 
melihat  frekuensi  nama  lokasi  terbanyak,  namun  pada  PoI 
proses tersebut tidak diperlukan, karena setiap satu objek PoI 
hanya memiliki satu nama lokasi. 

Gambar 7. Penentuan nama daerah tujuan MPD  pasif (msisdn) atau asal poi, 

algoritme penentuan pengunjung wisata 

4) Penentuan  Pengunjung:  Proses  terpisah  dari  clustering 
dan  penentuan  nama  daerah  (menggunakan  hasil  data 
cleansing).  Dalam  penentuan  pengunjung  wisata  (objek 
penelitian  yang  diukur),  berdasarkan  posisi  diterapkan 
haversine formula yang berguna untuk menghitung radius atau 
jarak  antara  dua  buah  titik  koordinat  (area  objek  penelitian) 
yang  ditetapkan  dari  batas  keakuratan  MPD  pasif  (200m). 
Haversine  dihitung  dengan  menginputkan  titik  (latitude-
longitude)  setiap  msisdn  (data  yang  tersebar)  dan  objek  PoI 
(titik tengah). 

Selanjutnya  penentuan  pengunjung  wisata  didasari  dengan 
lama  objek  saat  berada  dalam  area  penelitian.  Adapun  salah 
satu  ketentuannya  yaitu  msisdn  berhenti  di  sekitar  objek  PoI 
minimal 15 menit, yang dihitung dari waktu selesai dikurangi 
waktu  mulai  berhenti[6].  Untuk  menghitung  durasi  tersebut 
diperlukan  lebih  dari  satu  record  MPD  pasif  di  sekitar  objek 
PoI  setiap  harinya.  Ketentuan  lainnya,  saat  msisdn  berada  di 
sekitar  objek  PoI,  objek  tersebut  bukan  merupakan  tempat 
kesehariannya  seperti  tempat  tinggal  atau  tempat  bekerja. 
Berikutnya  pengunjung  dapat  langsung  ditetapkan  dengan 
memperhatikan  aturan  lamanya  berhenti  yaitu  maksimal 
selama  tujuh  hari  (per  bulannya  dalam  minggu  yang  sama) 
berada  di  area  tersebut[20].  Apabila  kondisi  tersebut  tidak 
terpenuhi  maka  dilakukan  pendekatan  dengan  melihat 
kunjungan  di  setiap  minggunya  yaitu  tinggal  atau  menetap 
kurang dari lima hari[17].  

Gambar 8. Identifikasi/penentuan  pengunjung  wisata,  algoritme  penentuan 

pengunjung wisata 

5) Pembentukan  Data  Agregat:  Proses  agregasi  yang 
dilakukan dengan menggabungkan output proses ke-3 dan ke-
4  yaitu  daerah  asal,  daerah  tujuan,  dan  pengunjung  wisata 
melalui  variabel  sejenis  disertai  variabel  pendukung  lainnya 
yang diperlukan untuk memudahkan dalam analisis. Sehingga, 
dapat  diperoleh  informasi  atau  insight  dari  analisis  yang 
dilakukan. 

Gambar 9. Proses  agregasi  atau  pembentukan  data  agregat,  algoritme 

penentuan pengunjung wisata 

B.  Implementasi Algoritme 

Implementasi  algoritme  merupakan  tahap  realisasi  dari 
prosedur yang telah disusun. Implementasi menggunakan data 
berbasis big data di Daerah Istimewa Yogyakarta (DIY) yang 
merupakan salah satu wilayah pariwisata andalan di Indonesia. 
MPD  pasif  yang  digunakan  terdiri  dari  entri  data  sampel 
acak  penggunaan  telepon  aktif  yang  direkam  oleh  operator 
jaringan  seluler  indosat.  Data  akomodasi  menggunakan  data 
dari  penyedia  layanan  pemesanan  akomodasi  secara  online 
terdiri  dari  informasi  seperti  lokasi  dan  jumlah  kamar  yang 

 5 / 8 

 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

digunakan.  Selanjutnya  data  ODTW  diperoleh  dengan 
menginput kata kunci pada proses web scraping: google maps. 
Kata  kunci  yang  digunakan  mengacu  pada  publikasi  BPS[6], 
diantaranya  wisata  buatan;  wisata  alam;  kawasan  pariwisata; 
dan enam lainnya. Rincian data yang digunakan adalah sebagai 
berikut : 

pada level provinsi (Gambar 10). Di level kabupaten/kota dua 
dari  lima  diantaranya  terlihat  memiliki  sedikit  perbedaan 
tersebut,  peneliti 
(Gambar  11).  Namun  dengan  hasil 
menetapkan  algoritme  dapat  mewakili  jumlah  pengunjung  di 
suatu area geografis hingga level kabupaten/kota. 

TABEL II 
DATA IMPLEMENTASI 

Data 
MPD Pasif 
(sampel 
acak) 

Akomodasi 

Sumber 

Telephone 
Company (telco) 
atau operator 
jaringan seluler 
Indosat 

Penyedia 
layanan 
pemesanan 
akomodasi 
secara online 

ODTW 

Google Maps 

Rincian 

 970 msisdn 
 6.728.745 record 
 Periode : Maret-Juni 2020 
 Variabel 

:  msisdn, 

timestamp, 

latitude, longitude. 

 2.549 objek 
 19 tipe 
 Periode : Maret-Juni 2020 
 Variabel : ID, nama, tipe, tanggal, 
longitude, 

latitude, 
room_used. 

 2.166 objek 
 170 tipe 
 Dikumpulkan pada tahun 2020 
 Variabel  :  nama,  tipe,  latitude, 
longitude. 

Gambar 10. Perbandingan  jumlah  pengunjung  sampel  acak  MPD  pasif  dan 

jumlah kamar yang terpakai di akomodasi, menurut provinsi  

Pengimplementasian  metode-metode  dalam  algoritme 
dilakukan  dengan  memanfaatkan  software  berbasis  python 
programming  language  beserta  package  atau  modul  di 
dalamnya yang berguna untuk mengotomatisasi penghitungan. 
Hasil 
implementasi  menunjukkan  sebanyak  689  msisdn 
ditetapkan  sebagai  pengunjung  dari  3.043  objek  PoI:  2.415 
akomodasi dengan 15 tipe (berdasarkan klasifikasi sumber data 
yang  digunakan)  dan  628  objek  daya  tarik  wisata  dengan  85 
tipe (berdasarkan klasifikasi google maps). 

Dengan  implementasi  menggunakan  big  data,  algoritme 
yang  diusulkan  dapat  mengukur  jumlah  pengunjung  dan 
kunjungan  di  suatu  area  geografis  hingga  daerah  sulit  akses 
seperti  wisata  alam.  Statistik  pariwisata  inilah  yang  berguna 
sebagai salah satu input data untuk analisis objek yang menjadi 
daya  tarik  wisata.  Kemudian  dengan  proses  analisis  data  ini, 
keberhasilan  pembangunan  kepariwisataan  di  suatu  wilayah 
baik  wilayah  administratif  maupun  non-administratif  dapat 
diukur secara aktual dan temporal. 

C.  Evaluasi Algoritme 

dalam  mengukur 

1) Validasi:  Untuk  menggunakan  sampel  acak  MPD  pasif 
dalam statistik pariwisata, perlu dilakukan validasi kualitasnya. 
Proses  evaluasi  dilakukan  dengan  melihat  keterwakilan 
implementasi 
pengunjung. 
Menggunakan pattern suitability melalui metode visual dengan 
garis  (line  chart),  fluktuasi  hasil  implementasi  algoritme 
dibandingkan  dengan  jumlah  kamar  yang  dipakai  dalam 
akomodasi yang menjadi salah satu  proxy PoI, dengan waktu 
(hari) dan objek yang sama. 

jumlah 

Fluktuasi  antara  sampel  acak  MPD  pasif  dan  akomodasi 
terlihat tidak jauh berbeda Hal ini menunjukkan bahwa sampel 
acak MPD pasif dapat menunjukkan kepadatan pengunjung di 
suatu  wilayah  atau  area  geografis  di  mana  akomodasi  berada 

Gambar 11. Perbandingan  jumlah  pengunjung  sampel  acak  MPD  pasif  dan 

jumlah kamar terpakai di akomodasi, menurut kabupaten/kota 

data 

Meskipun 

pembanding 

untuk  mengevaluasi 
keterwakilan  kunjungan  pada  objek  wisata  tidak  tersedia. 
Namun, perbandingan antara data kunjungan sampel acak MPD 
pasif  hasil  algoritme  pada  akomodasi  sejalan  dengan  jumlah 
kamar yang digunakan dalam akomodasi. 

2) Analisis  Deskriptif:  Untuk  mendeskripsikan  output 
algoritme  berdasarkan  hasil  implementasi  yang  dilakukan. 
Analisis  ini  berguna  dalam  mengetahui  manfaat  dan  kendala 
yang  mungkin  dari  algoritme.  Ahas  dkk.[2]  membuktikan 
analisis  berupa  ringkasan  statistik  dan  deskripsi  sederhana  di 
tingkat kota dan kabupaten berguna untuk menyusun rencana 
pembangunan 
lokal  khususnya  pembangunan  destinasi 
pedesaan  yang  kekurangan  sumber  daya  pemasaran  dan 
menarik pengunjung wisata. Berlandaskan hal tersebut, analisis 
ini dilakukan hingga level terkecil agar insight yang diperoleh 
dari algoritme dan implementasinya lebih maksimal. 

Analisis  hanya  terbatas  pada  data  sampel  yang  digunakan. 
Kemudian  analisis  ini  tidak  melakukan  uji  populasi  sehingga 
tidak 
ataupun 
menggambarkan keadaan populasi yang sebenarnya dari angka 
pasti yang disajikan. Akan tetapi, dari hasil validasi secara pola 

dapat  menunjukkan,  membuktikan 

 6 / 8 

 
 
 
 
 
pergerakan  algoritme  dapat  mewakili  jumlah  pengunjung  di 
suatu area geografis. 

Dari  689  pengunjung  wisata  hasil 

implementasi  di 
kabupaten/kota Daerah Istimewa Yogyakarta, 75% diantaranya 
mengunjungi  kedua  proxy  yang  digunakan  yaitu  akomodasi 
dan  ODTW.  Hal  ini    menimbulkan  persepsi  bahwa  sebagian 
besar  pengunjung  wisata  yang  mengunjungi  ODTW  juga 
melakukan  kunjungan  pada  akomodasi  terkait,  begitu  pula 
sebaliknya. Algoritme menetapkan pengunjung apabila berada 
disekitar PoI, sehingga pengunjung dalam hal ini belum tentu 
menginap pada objek akomodasi yang dikunjunginya. 

Dari banyaknya tipe pada setiap proxy yang digunakan, 10 
tipe  dengan  kunjungan  terbanyak  baik  berdasarkan  bulan 
maupun  secara  kumulatif  pada  periode  Maret-Juni  2020  di 
masing-masing  proxy. Tipe tujuan  wisata  pada  proxy ODTW 
(klasifikasi  google  maps)  memiliki  selisih  yang  cukup  besar 
dibandingkan dengan tipe-tipe lainnya. Selisih tersebut diduga 
terjadi karena  adanya generalisasi tipe  dari sumber data  yang 
tidak tertangkap pada algoritme.  

Pergerakan  jumlah  pengunjung  ODTW  di  Provinsi  D.I. 
Yogyakarta dari data sampel acak yang diterapkan cenderung 
meningkat pada akhir bulan yaitu minggu dan tanggal terakhir 
pada  tiap  bulan.  Kemudian,  untuk  mengetahui  waktu-waktu 
ramai atau padat pengunjung  ODTW melalui heatmap dalam 
visualisasi data dengan satuan jam dan hari secara kumulatif. 
Semakin  hijau  warna  yang  ditunjukkan  maka  pengunjung 
semakin ramai atau padat, sebaliknya jika semakin merah maka 
semakin  sepi  pengunjungnya.  DariGambar  12, 
terlihat 
kunjungan terpadat berada pada hari minggu yang dimulai dari 
jam 8 hingga sebelum jam 20, dengan puncak keramaian terjadi 
pada jam 12 keatas hingga sebelum jam 15 dan sore hari sekitar 
jam 16. 

Gambar 12. Heatmaps waktu padat pengunjung ODTW kumulatif Maret–Juni 

2020, menurut provinsi 

Analisis dilakukan hingga level terkecil yaitu objek di dalam 
proxy.  Berdasarkan  analisis  diketahui  bahwa  masih  terdapat 
kekurangan  dalam  pengimplementasian  algoritme 
terkait 
proses  data  cleansing.  Hal  ini  ditunjukkan  melalui  ODTW 
dengan nama “Wisata Keraton Jogja” yang bertipe museum dan 
“Keraton  Yogyakarta”  yang  bertipe  istana,  di  mana  kedua 
objek  tersebut  merupakan  objek  yang  sama.  Data  cleansing 
pada ODTW dilakukan berdasarkan keunikan karakter nama; 
tipe;  dan  titik  koordinat  (secara  exact),  yang  ternyata  setelah 
melalui proses tersebut masih belum bisa membedakan ODTW 
sesuai dengan pengetahuan umum di masyarakat. 

Analisis  menunjukkan  bahwa  output  algoritme  dari  hasil 
implementasi dapat mengetahui pergerakan atau pertumbuhan 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

objek  wisata  baik  secara  kolektif  ataupun  khusus.  Selain  itu 
output  juga  dapat  menunjukkan  rata-rata  kunjungan  seluruh 
ODTW; suatu tipe ODTW; salah satu ODTW; atau dalam suatu 
wilayah administratif. Dalam pembentukan algoritme terdapat 
waktu  lamanya  berada  disekitar  PoI  yang  disimpan  kedalam 
suatu  variabel.  Lama  waktu 
tersebut  digunakan  untuk 
menganalisis  lama  kunjungan  ODTW.  Hasil  analisis  dari 
implementasi  data  menunjukkan  pengunjung  yang  berada 
disekitar  atau  mengunjungi  salah  satu  ODTW  paling  cepat 
selama  15,02  menit.  Secara  rata-rata,  minimal  sekitar  1  jam 
setiap  pengunjung  berada  disekitar  atau  mengunjungi  setiap 
ODTW  per  harinya.  Kemudian,  diketahui  selama  23,22  jam 
adalah  lama  maksimal  pengunjung  mengunjungi  salah  satu 
ODTW. Lama maksimal berpotensi dianggap/menjadi anomali, 
dengan  dugaan  terdapat  kekurangan  dalam  algoritme  yaitu 
kurang dapat mendeteksi 2 kali kunjungan atau seterusnya pada 
hari dan objek/lokasi yang sama. 

Berdasarkan  daerah  asal  dan  daerah  tujuan  dilakukan 
identifikasi  untuk  mengetahui  banyaknya  pengunjung  wisata 
yang  tergolong  sebagai  wisatawan  nusantara  (wisnus)  dan 
pengunjung  setempat.  Dikatakan  sebagai  wisnus  apabila 
pengunjung mengunjungi PoI yang menjadi tujuannya berada 
diluar  kabupaten/kota  asalnya  dan  bukan  pekerja  di  lokasi 
tujuannya. 
adalah 
pengunjung  yang  mengunjungi  PoI  tujuannya,  berada  dalam 
kabupaten/kota  yang  sama  dengan  daerah  asalnya  dan  bukan 
pekerja  di  lokasi  tujuannya.  Pengidentifikasian  dilakukan 
berdasarkan  daerah  tujuan  dengan  cakupan  Kota  Yogyakarta 
dan  Kabupaten  lainnya  di  DIY  yaitu  Bantul;  Gunung  Kidul; 
Kulon Progo; dan Sleman, dan berdasarkan daerah asal dengan 
cakupan se-Indonesia. 

pengunjung 

Sedangkan 

setempat 

Hasil identifikasi pada data sampel menunjukkan sebanyak 
475  dari  689  pengunjung  digolongkan  menjadi  wisnus  yang 
mengunjungi kabupaten/kota di provinsi Yogyakarta dan 523 
dari  689  pengunjung  digolongkan  sebagai  pengunjung 
setempat secara kumulatif pada periode Maret-Juni 2020. Dari 
hasil ini diketahui bahwa sebagian besar pengunjung setempat 
juga  merupakan  wisnus  yang  masuk  ke  Daerah  Istimewa 
Yogyakarta. 

Menurut  analisis  dengan  cakupan  yang  lebih  kecil  yaitu 
dengan daerah asal dan tujuan hanya meliputi DIY, pengunjung 
setempat  dari  implementasi  data  sampel  terbanyak  berada  di 
Sleman  dan  wisnus  DIY  terbanyak  juga  berasal  dari  Sleman 
dengan objek tujuan terbanyaknya berada di Kota Yogyakarta 
(Gambar 13). Namun, berdasarkan pelaku kunjungan dari total 
msisdn,  kota  yogyakarta  merupakan  daerah  dengan  pelaku 
kunjungan terbanyak yaitu  99% diantaranya merupakan pelaku 
kunjungan wisata di daerahnya. Lebih detail lihat Tabel III: 

Kab/Kot 

Bantul 
Gunung Kidul 
Kulon Progo 
Sleman 
Yogyakarta 
Total 

TABEL III 
PELAKU KUNJUNGAN 

Jumlah 
msisdn 

Jumlah Pengunjung 
Setempat (msisdn) 

176 
66 
67 
257 
92 
658 

155 
29 
16 
232 
91 
523 

Pelaku 
Kunjungan 
88% 
44% 
24% 
90% 
99% 

 7 / 8 

 
 
 
 
Gambar 13. Circos plot pengunjung, berdasarkan daerah asal-tujuan 

Analisis  hasil 

implementasi  menunjukkan 

statistik 
pariwisata yang dihasilkan dapat digunakan untuk mengetahui 
jumlah  pengunjung,  waktu  padat  pengunjung;  pertumbuhan 
pengunjung; proporsi pelaku kunjungan; lama waktu kunjung; 
bahkan  mengidentifikasi  tipe  pengunjung  wisata  berdasarkan 
daerahnya.  Dengan  demikian  statistik  pariwisata  yang 
merupakan  output  dari  algoritme  dapat  melengkapi  statistik 
pariwisata resmi.  Adanya hal ini diharapkan dapat digunakan 
sebagai  tolok  ukur  untuk  mengetahui  apa  saja  yang  harus 
diperbaiki dan dibenahi baik secara kolektif maupun pada objek 
atau destinasi wisata terkait. 

Meskipun  algoritme  saat  ini  cukup  bisa  mewakili  jumlah 
pengunjung,  namun  dari  analisis  ditemukan  bahwa  algoritme 
saat ini hanya mampu mendeteksi pengunjung yang melakukan 
sekali kunjungan pada objek yang sama dalam hari yang sama. 
Kemudian  pada  implementasi  klasifikasi  tipe  objek  tidak 
diperhatikan lebih lanjut, sehingga analisis terhadap tipe objek 
kurang maksimal. Untuk itu kedepannya pengklasifikasian tipe 
objek  perlu  diperhatikan  lebih  lanjut  agar  analisis  dapat 
dilakukan secara maksimal serta terhindar dari ambiguitas dan 
duplikasi. 

VII. 

PENUTUP 

Kajian  ini  memberikan  pemahaman  bahwa  penerapan 
sampel  acak  MPD  pasif  dengan  pendekatan  PoI  di  sektor 
pariwisata,  dapat  menghasilkan  statistik  pariwisata  berupa 
jumlah  pengunjung  dan  kunjungan  wisata  di  suatu  area 
geografis.  Penelitian  ini  berhasil  membangun  suatu  prosedur 
atau  algoritme  berdasarkan  big  data  yang  dapat  mengukur 
kepadatan 
jumlah 
pengunjung  pada  objek  atau  destinasi  wisata  hingga  tingkat 
kabupaten/kota  dan  mengidentifikasi  tipe  pengunjung  wisata 
berdasarkan  kunjungan  pada  PoI,  yaitu  sebagai  wisatawan 
nusantara dan pengunjung setempat.  

dan  menggambarkan 

pengunjung 

Selanjutnya  algoritme  ini  dapat  dikembangkan  menjadi 
algoritme  yang  dapat  mendeteksi  pengunjung  dengan 
kunjungan lebih dari sekali di objek yang sama dalam hari yang 
sama, misalnya dengan memperhatikan trajectory atau lintasan 
dari data yang digunakan. 

DAFTAR PUSTAKA 

[1]  Adhinugroho, Y., Putra, A. P., Luqman, M., Ermawan, G. Y., Takdir, 
Mariyah, S., & Pramana, S. (2020). Development of online travel Web 
scraping for tourism statistics in Indonesia. Information Research, 25(4). 
doi:10.47989/irpaper885 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

[2]  Ahas, R., Aasa, A., Roose, A., Mark, U., & Silm, S. (2008). Evaluating 
passive mobile positioning data for tourism surveys: An Estonian case 
study. Tourism Management, 29(3), 469-486. 

[3]  Banerjee, A. (2004). Validating clusters using the Hopkins statistic. 2004 

IEEE International Conference on Fuzzy Systems (IEEE Cat. 
No.04CH37542) (hal. 149-153). Budapest: IEEE. 
doi:10.1109/FUZZY.2004.1375706. 

[4]  BPS. (2018, November 8). Pemanfaatan Big Data dalam Survei 

Wisatawan Nusantara. Diambil kembali dari Kegiatan Statistik Badan 
Pusat Statistik: 
https://www.bps.go.id/news/2018/11/08/254/pemanfaatan-big-data-
dalam-survei-wisatawan-nusantara.html 

[5]  BPS. (2018). Statistik Objek Daya Tarik Wisata. Jakarta: Badan Pusat 

Statistik. 

[6]  BPS. (2019). Panduan Penggunaan Mobile Positioning Data (MPD) 

untuk Statistik Mobilitas. Jakarta: Badan Pusat Statistik. 

[7]  Dimulyo, S., Esko, S., Saluveer, E., & Lestari, T. (2018). The use of 

MPD for measuring tourism on the Indonesian border. Helsinki: EFGS. 
[8]  Elbatta, M. N. (2012). An Improvement for DBSCAN Algorithm for Best 

Results in Varied Densities. Gaza: The Islamic University-Gaza. 
[9]  Ester, M., Kriegel, H.-P., Sander, J., & Xu, X. (1996). A density-based 
algorithm for discovering clusters in large spatial databases with noise. 
KDD'96: Proceedings of the Second International Conference on 
Knowledge Discovery and Data Mining (hal. 226–231). AAAI Press. 
[10] Eurostat. (2014). Feasibility study on the use of mobile positioning data 
for tourism statistics. Luxembourg: Publications Office of the European 
Union. 

[11] Eurostat. (2017). Tourism statistics: Early adopters of big data? (2017 

ed.). Luxembourg: European Union. 

[12] Li, J., Xu, L., Tang, L., Wang, S., & Li, L. (2018). Big data in tourism 
research: A literature review. Tourism Management, 68, 301-323. 
[13] Noviyanti, I., Prabawa, P. D., Sari, D. P., Koswara, A., Lestari, T. K., 

Fahyuananto, M. H., & Setiawan, E. (2020). Towards big data as official 
statistics: Case study of the use of mobile positioning data to delineate 
metropolitan areas in Indonesia. Statistical Journal of the IAOS, 36(4), 
943-954. doi:10.3233/SJI-200750 

[14] Pradika, A. A. (2019). Kajian Mobile Positioning Data Untuk Statistik 
Komuter (Studi Kasus : Relawan Pegawai Badan Pusat Statistik). 
Jakarta: Politeknik Statistika STIS. 

[15] Pramana, S., Yuniarto, B., Kurniawan, R., Yordani, R., Lee, J., Amin, 
I., . . . Indriani, R. (2017). Big data for government policy: Potential 
implementations of big data for official statistics in Indonesia. 2017 
International Workshop on Big Data and Information Security (IWBIS) 
(hal. 17-21). Jakarta: IEEE. doi:10.1109/IWBIS.2017.8275097 

[16] Raun, J. (2020). Mobile positioning data for tourism destination studies 

and statistics. Tartu: University of Tartu Press. 

[17] Republik Indonesia. (2020). Undang-Undang Republik Indonesia Nomor 

11 Tahun 2020 tentang Cipta Kerja (pasal 77 ayat 2). Jakarta: 
Kementerian Hukum dan Hak Asasi Manusia. 

[18] Ruslani, A., Madjida, W. O., & Nughroho, A. R. (2019). The Use of 
Mobile Positioning Data to Obtain Accommodation Statistics: Case 
Study of Indonesia. Asia–Pacific Economic Statistics Week 2019. 
Bangkok: UN ESCAP. 

[19] Shoval, N., & Ahas, R. (2016). The use of tracking technologies in 

tourism research: the first decade. Tourism Geographies, 18(5), 587-606. 
doi:10.1080/14616688.2016.1214977 

[20] Sikder, R., Uddin, M. J., & Halder, S. (2016). An Efficient Approach of 
Identifying Tourist by Call Detail Record Analysis. International 
Workshop on Computational Intelligence (IWCI) (hal. 136-141). Dhaka, 
Bangladesh: IEEE. 

[21] UN. (2017). Draft: Handbook on the use of Mobile Phone data for 

Official Statistics. New York: United Nations. 

[22] UNWTO. (2010). International Recommendations for Tourism Statistics 

2008. New York: United Nations. 

[23] Vaishnavi, V., Kuechler, B., & Petter, S. (2017, Desember 20). Design 

Science Research in Information Systems. Diambil kembali dari 
Association for Information System: http://www.desrist.org/design-
research-in-information-systems 

[24] Widyasanti, A. A., Reno, A., Esko, S., Tiru, M., & Lestari, T. K. (2020). 
The Use of Mobile Positioning data to Measure Visitors of a Multisport 
Events: A Case study of ASIAN Games 2018 in Indonesia. 2020 Asia-
Pacific Statistics Week. Bangkok: UNESCAP. 

 8 / 8 

 
 
 
 
"
221709600,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Aplikasi Visualisasi Data Produk Domestik Regional 
Bruto (PDRB) dengan Analisis Shift Share dan 
Location Quotient Berbasis Web 

Chafri Fajar Erwandra (221709600, 4SI1) 
Dosen Pembimbing: Farid Ridho, S.S.T., M.T. 

Ringkasan—  Salah  satu 

indikator  penting  untuk  dapat 
mengetahui  kondisi  perekonomian  pada  suatu  wilayah  adalah 
Produk  Domestik  Regional  Bruto  (PDRB).  Analisis  terhadap 
PDRB  diperlukan  untuk  mengetahui  dan  mempelajari  faktor-
faktor  penyebab  terjadinya  perbedaan  dan  fluktuasi  ekonomi. 
Analisis PDRB yang sering dilakukan untuk perbandingan antar 
wilayah  adalah  analisis  shift  share  dan 
location  quotient. 
Penghitungan  analisis  tersebut  dilakukan  secara  manual  per 
satuan  wilayah  dan  periode  tertentu,  sehingga  menjadikannya 
kurang  efisien  karena  jika  ingin  melakukan  analisis  terhadap 
wilayah  dan  periode  lain  maka  perlu  dilakukan  penghitungan 
lagi.  Selain itu tidak dapat membandingkan hasil antar wilayah 
secara  langsung.  Untuk  mengatasi  masalah  tersebut,  dibuat 
sebuah  aplikasi  yang  dapat  mempermudah  penghitungan  dan 
dapat  memvisualisasikan  hasil  analisis  PDRB.  Metode  yang 
digunakan  untuk  membangun  aplikasi  ini  adalah  SDLC  model 
Waterfall  yang  kemudian  diuji  menggunakan  uji  Black  box  dan 
System  Usability  Scale  (SUS).  Hasil  pengujian  menunjukkan 
bahwa  semua  fitur  pada  aplikasi  telah  sesuai  seperti  yang 
diharapkan dan didapatkan skor SUS sebesar 86,87 yang berarti 
aplikasi sudah mampu memudahkan pengguna dalam melakukan 
analisis data. 

Kata  Kunci—  Visualisasi  Data,  PDRB,  Shift  Share,  Location 

Quotient, Website 

I.  LATAR BELAKANG 

Salah satu indikator penting untuk dapat mengetahui kondisi 
perekonomian  pada  suatu  wilayah  adalah  Produk  Domestik 
Regional  Bruto  (PDRB).  PDRB  adalah  nilai  pasar  seluruh 
barang  dan  jasa  yang  diproduksi  oleh  suatu  wilayah  pada 
periode  waktu  tertentu.  Pada  dasarnya,  PDRB  merupakan 
jumlah dari nilai tambah yang dihasilkan oleh semua unit usaha 
yang  ada  dalam  suatu  wilayah  tertentu  atau  jumlah  dari nilai 
barang dan jasa akhir yang dihasilkan oleh semua unit ekonomi 
[4].  

dan 

dari 

data 

PDRB 

Analisis  data  PDRB  meliputi  mengkaji,  mengartikan  atau 
menguraikan  makna 
juga 
membandingkannya  baik  antar  waktu,  antar  daerah,  ataupun 
antar  variabel.  Analisis  PDRB  perlu  dilakukan  untuk  dapat 
mengetahui dan memahami berbagai faktor yang menyebabkan 
terjadinya  perbedaan  dan  fluktuasi  ekonomi.  Selain  itu  juga 
terdapat analisis agregat makro lain yang dapat diturunkan dari 
PDRB.  Hasil  dari  analisis  dapat  digunakan  sebagai  masukan 
bagi  pemerintah  daerah  untuk  menentukan  kebijakan  dalam 
pembangunan  ekonomi.  Analisis  dengan  menggunakan  data 

PDB/PDRB mencakup monitoring perilaku ekonomi, analisis 
makro ekonomi dan perbandingan antar wilayah [1]. 

Analisis  PDRB  untuk  perbandingan  antar  wilayah  yang 
sering  dilakukan  adalah  analisis  shift  share  dan  location 
quotient.  Penghitungan  analisis  shift  share  dan  location 
quotient  dilakukan  dengan  menggunakan  aplikasi  pengolah 
tabel  seperti  excel.  Penghitungan  dengan  excel  yang  tidak 
terotomatisasi  membuat  pengguna  harus  mengidentifikasikan 
rumus  sendiri.  Hal  ini  akan  mengakibatkan  rawan  terjadi 
kesalahan yang berasal dari manusia / human error, yang akan 
berimbas  pada  hasil  penghitungan  analisis  yang  tidak  sesuai. 
Selain  itu  penghitungan  analisis  shift  share  dan  location 
quotient yang dilakukan dengan excel hanya dapat dilakukan 
per satuan wilayah dan periode tertentu, hal ini menjadi kurang 
efisien karena jika ingin melakukan analisis terhadap wilayah 
dan  periode  lain  maka  perlu  dilakukan  penghitungan  lagi. 
Selain  itu,  penghitungan  analisis  yang  dilakukan  tiap  satuan 
wilayah  dan  periode  membuatnya  tidak  bisa  dibandingkan 
dengan  hasil  analisis  pada  wilayah  dan  periode  lainnya. 
Sehingga  perbandingkan  hasil  analisis  antar  wilayah  secara 
makro susah untuk dilakukan.  

Hasil  dari  analisis  shift  share  dan  location quotient  adalah 
berupa angka yang disajikan dalam bentuk tabel. Namun untuk 
dapat melihat perbandingan hasil analisis dari masing-masing 
wilayah dan lapangan usaha, perlu dilakukan penghitungan dari 
data  PDRB  lapangan  usaha  yang  dihasilkan  oleh  masing-
masing BPS daerah. Sehingga jumlah data akan sangat banyak, 
menampilkan hasil analisis dalam bentuk tabel menjadi tidak 
efisien. Penyajian dengan tabel dapat menampilkan data secara 
spesifik dan detail sehingga pengguna dapat memperoleh data 
yang  terperinci.  Namun,  penyajian  data  menggunakan  tabel 
memiliki  beberapa  kelemahan  seperti  pengguna  tidak  dapat 
membandingkan  pola  data  secara  langsung  sehingga  menjadi 
kurang  efektif  dan  efisien  karena  pengguna  harus  melakukan 
usaha lebih dalam melakukan interpretasi data.  

Salah  satu  solusi  yang  dapat  digunakan  untuk  mengatasi 
permasalahan  tersebut  adalah  dengan  menyajikan  data  hasil 
dari  analisis  tersebut  secara  visual.  Visualisasi  data  adalah 
teknik  penyajian  data  kedalam  bentuk  visual  seperti  grafik, 
diagram,  atau  peta  sehingga  dapat  mempermudah  pengguna 
dalam  memahami  data  yang  ditampilkan.  Visualisasi  data 
bertujuan untuk membantu pemahaman manusia tentang data 
dengan  memanfaatkan  kemampuan  sistem  visual  manusia 
dalam  mengekspresikan 
informasi  dengan  melihat  pola, 
melihat  tren,  dan  mengidentifikasi  outlier  [7].  Representasi 

 1 / 9 

 
 
 
 
 
visual  yang  dirancang  dengan  baik  dapat  menggantikan 
perhitungan  kognitif  dengan  kesimpulan  persepsi  sederhana 
dan  meningkatkan  pemahaman,  memori,  dan  pengambilan 
keputusan [7]. 

Dashboard adalah tampilan visual dari informasi terpenting 
yang  diperlukan  untuk  meraih  satu  atau  lebih  tujuan,  yang 
digabungkan dan disusun dalam satu layar sehingga informasi 
dapat  dipantau  dalam  sekejap  [10].  Dashboard  merupakan 
salah satu bentuk visualisasi data dengan hanya menampilkan 
informasi-informasi  terpenting  pada  satu  halaman,  sehingga 
penggunaannya akan membuat visualisasi menjadi lebih efisien. 
Maka dari itu, pada penelitian ini akan dibuat sebuah aplikasi 
berbasis  web  yang  dapat  menghitung  dan  memvisualisasikan 
data  PDRB  dengan  analisis  shift  share  dan  location  quotient 
berbentuk dashboard interaktif.  

II.  TUJUAN PENELITIAN 

Berdasarkan latar belakang di atas, maka tujuan yang ingin 

diperoleh dari penelitian ini adalah sebagai berikut: 

1.  Membangun  aplikasi  yang  dapat  menghitung  dan 
memvisualisasikan  hasil  analisis  shift  share  dan 
location quotient dalam bentuk dashboard interaktif. 
2.  Membuat fitur visualisasi yang dapat menampilkan dan 
membandingkan  hasil  analisis  dari  masing-masing 
wilayah dan sektor secara makro. 

3.  Membuat  fitur  yang  dapat  mengklasifikasikan  hasil 

analisis kedalam kuadran Tipologi Klassen.   

4.  Membuat  fitur  input  data  yang  memungkinkan  untuk 
melakukan penginputan data berjumlah besar sekaligus. 

III. PENELITIAN TERKAIT 

Penelitian  yang  terkait  yang  digunakan  sebagai  referensi 

dalam penelitian ini antara lain adalah sebagai berikut: 

Penelitian oleh Ashshidiq (2020), yang berjudul “Visualisasi 
Data  Menggunakan  Sistem 
Informasi  Geografis  untuk 
Kegiatan  Ekspor  dan  Impor  Indonesia”  [9].  Penelitian  ini 
bertujuan  untuk  membuat  visualisasi  data  ekspor  dan  impor 
menggunakan  sistem  informasi  geografis  interaktif  berbasis 
web. Penelitian ini menggunakan metode System Development 
Life Cycle (SDLC) dalam membangun aplikasi visualisasi dan 
menggunakan  System  Usability  Scale  (SUS)  untuk  menilai 
aplikasi visualisasi yang telah dibangun. Berdasarkan hasil uji 
coba  menggunakan  SUS  diperoleh  nilai  sebesar  78,86  yang 
berarti aplikasi sudah mampu memenuhi kebutuhan pengguna 
dan membantu  pengguna dalam interpretasi data yang dipilih 
oleh pengguna. 

Penelitian oleh Handigo (2020), yang berjudul “Visualisasi 
Data Menggunakan Sistem Informasi Geografis untuk Statistik 
Transportasi  Darat  di  Indonesia”  [6].  Penelitian  ini  hampir 
mirip dengan penelitian yang dilakukan oleh Ashshidiq (2020). 
Bertujuan untuk membuat visualisasi data statistik transportasi 
darat  menggunakan  sistem  informasi  geografis  berbasis  web. 
Penelitian ini menggunakan metode System Development Life 
Cycle  (SDLC)  Waterfall  dalam  pengembangan  sistem  dan 
menggunakan uji coba  Black-box dan  System Usability Scale 
(SUS)  untuk  menilai  sistem  visualisasi  data  yang  dibangun. 
Hasil  dari  uji  coba  Black-box  diperoleh  bahwa  fungsi-fungsi 
utama  pada  sistem  menghasilkan  output  yang  sesuai  dengan 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

input yang diberikan oleh pengguna. Uji coba System Usability 
Scale  mendapatkan  nilai  sebesar  81,59  yang  artinya  sistem 
dapat diterima dengan baik oleh pengguna. 

Penelitian  oleh  Winarko  (2016),  yang  berjudul  “Sistem 
Informasi  Geografis  Berbasis  Web  Dengan  Analisis 
Geographically  Weighted  Regression  (Studi  Kasus  Faktor-
Faktor  yang  Mempengaruhi  PDRB  Kabupaten/Kota  di 
Indonesia  Tahun  2010)”  [5].  Penelitian  ini  bertujuan  untuk 
membangun  Sistem  Informasi  Geografis  berbasis  web,  yang 
hasilnya  diimplementasikan  terhadap  data  indikator  ekonomi 
kabupaten/kota  di  Indonesia  sebagai  faktor-faktor  yang 
mempengaruhi  PDRB.  Penelitian  ini  menggunakan  uji  coba 
White-box, Black-box dan System Usability Scale (SUS) untuk 
menilai sistem yang telah dibangun. Hasil dari uji coba White-
box  menunjukkan  bahwa  sistem  sudah  berjalan  sebagaimana 
mestinya.  Dari  uji  Black-box  diperoleh  bahwa  fungsi-fungsi 
pada sistem dapat memberikan hasil yang sesuai terhadap input 
yang diberikan oleh pengguna. Uji coba System Usability Scale 
mendapatkan  nilai  sebesar  68,2  yang  artinya  sistem  berada 
pada rentang marginal high namun masih perlu perbaikan agar 
dapat masuk ke rentang acceptable. 

IV. METODE PENELITIAN  

A.  Metode Analisis Data 

tingkat 

ekonomi 

perkembangan 

Analisis  shift  share  adalah  analisis  yang  digunakan  untuk 
mengetahui 
dan 
kecenderungan transformasi struktur perekonomian pada suatu 
daerah.  Dengan  analisis  shift  share,  kita  dapat  mengetahui 
kemampuan kompetitif lapangan usaha pada suatu daerah dan 
dapat  mengetahui  lapangan  usaha  unggulan  pada  masing-
masing daerah. Selain itu, dengan analisis ini kita dapat melihat 
peranan  sumbangan  (share)  tiap  lapangan  usaha  terhadap 
perekonomian pada suatu daerah dan  melihat lapangan usaha 
yang  mengalami  kemajuan  selama  periode  pengukuran.  Jika 
suatu lapangan usaha memiliki nilai  shift share paling tinggi, 
maka  lapangan  usaha  tersebut  dapat  dikatakan  paling  unggul 
[1].  Analisis  shift  share  memiliki  tiga  komponen,  yaitu 
National Shift (Nij), Proportional Share (Mij), dan Differential 
Share (Cij). Persamaan 1 merupakan rumus dari perubahan nilai 
sektor pada provinsi, yang nilainya sama dengan pertambahan 
dari nilai National Shift, Proportional Share, dan Differential 
Share. Persamaan 2, 3, dan 4 secara berturut-turut merupakan 
rumus dari National Shift, Proportional Share, dan Differential 
Share. 

𝐷𝑖𝑗 = 𝑁𝑖𝑗 + 𝑀𝑖𝑗 + 𝐶𝑖𝑗   

𝑁𝑖𝑗 = 𝐸𝑖𝑗. 𝑅𝑛   

𝑀𝑖𝑗 = 𝐸𝑖𝑗(𝑅𝑖𝑛 − 𝑅𝑛)   

𝐶𝑖𝑗 = 𝐸𝑖𝑗(𝑅𝑖𝑗 − 𝑅𝑖𝑛)   

(1) 

(2) 

(3) 

(4) 

Keterangan: 
Dij  = Perubahan sektor i di tingkat provinsi 
Nij  = Pergeseran struktur sektor i di tingkat provinsi 
Mij = Bauran industri sektor i di tingkat provinsi 
Cij  = Keunggulan kompetitif sektor i di tingkat provinsi 
Eij  = PDRB sektor i di tingkat provinsi 
Rij  = laju pertumbuhan sektor i di tingkat provinsi 

 2 / 9 

 
 
 
 
 
 
 
 
 
 
 
Rin = laju pertumbuhan sektor i di tingkat nasional 
Rn  = laju pertumbuhan PDRB di tingkat nasional 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Analisis  Location  Quotient  (LQ)  adalah  analisis  yang 
digunakan  untuk  mengetahui  kapasitas  ekspor  perekonomian 
pada  suatu  daerah  dan  mengukur  derajat  kemandirian  (self-
sufficiency)  pada  suatu  lapangan  usaha.  Perhitungan  LQ 
menghasilkan  dua  kriteria.  Jika  LQ  lebih  besar  dari  1,  maka 
sektor  tersebut  merupakan  sektor  basis.  Dengan  kata  lain, 
produksi  komoditas  pada  daerah  tersebut  dapat  memenuhi 
kebutuhan sendiri, atau bahkan dapat diekspor ke luar. Jika LQ 
kurang  dari  sama  dengan 1, maka  sektor  tersebut  merupakan 
sektor  non  basis.  Dengan kata  lain,  produksi  komoditas  pada 
daerah  tersebut  tidak  dapat  memenuhi  kebutuhan  sendiri 
sehingga  perlu  impor  atau  pasokan  dari  luar  [1].  Rumus  dari 
LQ dapat dilihat pada persamaan 5. 
𝐿𝑄 =  

(5) 

𝑣𝑖/𝑣𝑡
𝑉𝑖/𝑉𝑡

Gambar 1. Ilustrasi model waterfall [3] 

Tahapan  pada  SDLC  model  waterfall  secara  garis  besar 

terdiri dari empat tahap utama, yaitu: 

1.  Analisis Kebutuhan 
analisis 

  Tahap 
untuk 
kebutuhan 
mendefinisikan  aplikasi  seperti  apa  yang  dibutuhkan 
oleh  pengguna.  Analisis  kebutuhan  dilakukan  dengan 
wawancara dan observasi terhadap permasalahan yang 
ada pada sistem berjalan. 

dilakukan 

Keterangan: 
𝑣i  = PDRB provinsi lapangan usaha ke-i  
𝑣t  = total PDRB provinsi  
𝑉i  = PDB lapangan usaha ke-i  
𝑉t  = total PDB 

Tipologi  Klassen  merupakan  grafik  yang  dipergunakan 
untuk  melihat  struktur  dan  pola  pertumbuhan  pada  tiap-tiap 
sektor  lapangan  usaha  untuk  melihat  prospek  pertumbuhan 
perekonomian  pada  suatu  daerah  di  masa  mendatang.  Pada 
Tipologi  Klassen,  tiap-tiap  sektor  lapangan  usaha  pada  suatu 
daerah  dapat  dikategorikan  sebagai  sektor  yang  terbelakang, 
potensial, berkembang, dan prima [11]. Tipologi Klassen yang 
digunakan pada penelitian ini adalah Tipologi Klassen dengan 
pendekatan  sektoral.  Tipologi  Klassen  pendekatan  sektoral 
membagi  sektor  pada  daerah  berdasarkan  hasil  perhitungan 
analisis  Shift  Share  dan  Location  Quotient  [8].  Tabel  I 
menunjukkan kuadran pada Tipologi Klassen [8]. 

Shift 
Share 

SS+ 

SS- 

TABEL I 
KUADRAN TIPOLOGI KLASSEN 

Location Quotient (LQ) 

LQ ≤ 1 

KUADRAN II 
(POTENSIAL) 

LQ > 1 

KUADRAN I 
(PRIMA) 

KUADRAN IV 
(TERBELAKANG) 

KUADRAN III 
(BERKEMBANG) 

B.  Metode Pengembangan Aplikasi 

Penelitian  dilakukan  dengan  menggunakan  alur  System 
Development  Life  Cycle  (SDLC)  dengan  model  Waterfall. 
Model Waterfall adalah model SDLC yang paling sederhana. 
Model  ini  cocok  untuk  pengembangan  aplikasi  dengan 
tidak  berubah-ubah.  Model  Waterfall 
spesifikasi  yang 
menyediakan  pendekatan  alur  pengembangan  aplikasi  secara 
terurut mulai dari analisis, desain,  pengodean, pengujian, dan 
tahap  pendukung  (support)  [3].  Gambar  1  menunjukkan 
gambar alur proses pada model Waterfall. 

2.  Desain / Perancangan 

  Tahap  perancangan  dilakukan  untuk  memberikan 
gambaran  terhadap  aplikasi  yang  akan  dikerjakan, 
dilakukan  dengan  menerjemahkan  kebutuhan  pada 
tahap analisis kebutuhan ke bentuk representasi desain. 
Tahap ini dilakukan dengan membuat rancangan proses 
visualisasi data, basis data, dan antarmuka. 

3.  Pengodean / Implementasi 

Tahap implementasi dilakukan untuk merealisasikan 
rancangan pada tahap sebelumnya. Tahap implementasi 
berupa  pengodean  dengan  menggunakan  bahasa 
pemrograman 
dengan 
dan 
memanfaatkan  framework  Laravel  dan  library  open 
source  seperti  chart.js  dan  highcharts  untuk  membuat 
visualisasi data.  

Javascript 

PHP 

4.  Pengujian 

Tahap  pengujian  dilakukan  untuk  mengetahui 
kelayakan  aplikasi  terhadap  pengguna.  Uji  dilakukan 
dengan  menggunakan  uji  Black-box  dan  System 
Usability Scale (SUS). 

C.  Metode Visualisasi Data 

Terdapat beberapa metode klasifikasi visualisasi data  yang 
II  menampilkan  metode  klasifikasi 

digunakan.  Tabel 
visualisasi data yang sesuai dengan data PDRB. 

Metode 
Membandingkan 
kategori 

Menilai hierarki 
dan hubungannya 
dengan populasi 

TABEL II 
METODE VISUALISASI DATA 
Tujuan 
Untuk membandingkan ukuran dari nilai kategorik 
[2]. Visualisasi yang digunakan adalah bar chart 
dan sankey diagram. 
Untuk memberikan rincian nilai kategorik dalam 
hubungannya dengan nilai populasi [2]. Visualisasi 
yang digunakan adalah pie chart dan tree map. 

Menampilkan 
perubahan antar 
waktu 

Untuk menunjukkan perubahan tren dan pola dari 
nilai selama periode yang berkelanjutan [2]. 
Visualisasi yang digunakan adalah line chart. 

 3 / 9 

 
 
   
 
 
 
 
Menggambarkan 
koneksi dan 
hubungan 

Memetakan data 
geospasial 

Untuk menilai asosiasi, distribusi, dan pola yang 
ada pada kumpulan data multivariat [2]. Visualisasi 
yang digunakan ada scatter plot dalam bentuk 
tipologi klassen. 
Untuk menggambarkan dan menyajikan data 
dengan atribut geospasial dalam bentuk peta [2]. 
Visualisasi yang digunakan adalah choropleth map. 

Gambar 2 menunjukkan contoh gambaran visualisasi yang 

akan dipakai untuk memvisualisasikan data PDRB 

b) 
Gambar 2. a) sankey diagram [2]; b) tree map [2]; c) choropleth map [2] 

a) 

c) 

V.  KERANGKA PIKIR 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Berdasarkan masalah-masalah yang ada pada latar belakang, 
peneliti  mencari  peluang  yang  dapat  dilakukan  oleh  peneliti 
untuk dapat mengatasi masalah tersebut. Disini peneliti melihat 
suatu  peluang  bahwa  belum  terdapatnya  aplikasi  yang  dapat 
menghitung  sekaligus  memvisualisasikan  data-data  tersebut. 
Sehingga  dengan  adanya  peluang  tersebut,  peneliti  ingin 
mengembangkan sebuah website visualisasi data dalam bentuk 
dashboard interaktif yang dapat meringkas data-data tersebut 
dalam bentuk visual.  

Untuk  mewujudkan  hal  tersebut,  dipilihlah  suatu  metode 
pengembangan SDLC model Waterfall. Model pengembangan 
waterfall dipilih karena masalah yang ada  bukan berasal dari 
subject  matter,  melainkan  melalui  observasi  yang  dilakukan 
peneliti terhadap masalah yang ada. Sehingga pengembangan 
model  Waterfall  cocok  digunakan  pada  kondisi  ini,  karena 
permasalahan ditentukan di awal dan kemungkinan terjadinya 
perubahan  kebutuhan  selama  proses  pengembangan  sangat 
kecil.  

Masalah-masalah  tersebut  kemudian  diperinci  lagi  melalui 
analisis  permasalahan  dengan  diagram  fishbone,  kemudian 
dilakukan analisis kebutuhan untuk mengatasi masalah tersebut. 
Pada tahap perancangan aplikasi, dilakukan perancangan pada 
alur kerja, basis data, dan antarmuka. Setelah melakukan tahap 
perancangan, kemudian dilakukan tahap implementasi berupa 
pengodean  dengan  memanfaatkan  framework  Laravel  dan 
library open source untuk membuat web visualisasi data. Data 
yang  digunakan  pada  tahap  implementasi  adalah  data  yang 
didapatkan  dari  publikasi  BPS  tiap  daerah  yaitu  PDRB  atas 
dasar  harga  konstan  menurut  lapangan  usaha.  Setelah  tahap 
implementasi  kemudian  dilakukan  evaluasi  terhadap  aplikasi 
yang  telah  dibuat  menggunakan  uji  Black-box  dan  SUS. 
Berdasarkan  hasil  evaluasi  tersebut,  akan  ditarik  kesimpulan 
yang menjelaskan keberhasilan aplikasi dalam menyelesaikan 
masalah-masalah yang telah diidentifikasikan di awal. 

VI. HASIL DAN PEMBAHASAN 

A.  Analisis Kebutuhan  

aplikasi  pengolah 

Analisis  kebutuhan  dilakukan  dengan  wawancara  dan 
observasi  mengenai  penghitungan  analisis  shift  share  dan 
location  quotient  yang  dilakukan  secara  manual  dengan 
excel. 
menggunakan 
Wawancara dilakukan kepada mahasiswa Politeknik Statistika 
STIS  yang  pernah  mendapatkan  materi  perkuliahan  tentang 
analisis  shift  share  dan 
location  quotient  sebelumnya. 
Berdasarkan  wawancara  dan  observasi  yang  telah  dilakukan, 
didapatkan  kebutuhan  yang  diperlukan  dalam  aplikasi  yang 
akan dibangun sebagai berikut: 

seperti 

tabel 

Gambar 3. Kerangka pikir penelitian 

Gambar 3 menunjukkan alur kerangka pikir yang digunakan 

pada penelitian ini. 

1.  Aplikasi  yang  dapat  melakukan  penghitungan  analisis 
shift  share  dan  location  quotient  secara  otomatis. 
Sehingga  pengguna  dapat  melakukan  analisis  dengan 
usaha  yang  lebih  kecil  dan  mudah,  serta  dapat 
meminimalisir  kemungkinan 
terjadinya  kesalahan 
hitung (human error). 

2.  Aplikasi yang dapat melihat dan membandingkan hasil 
analisis dari masing-masing wilayah dan sektor secara 
langsung.  Sehingga  pengguna  tidak  perlu  melakukan 

 4 / 9 

 
 
 
 
 
 
 
analisis satu per satu terhadap setiap wilayah yang ingin 
dibandingkan 

3.  Aplikasi  yang  mampu  mengelola  data  yang  banyak. 
Karena data yang diinputkan akan sangat banyak, maka 
dibutuhkan  aplikasi  yang  memungkinkan  pengguna 
untuk melakukan input banyak data secara sekaligus. 
4.  Aplikasi  yang  dapat  memvisualisasikan  hasil  analisis. 
lebih  memudahkan  pengguna  dalam 

Sehingga 
melakukan perbandingan antar data. 

B.  Perancangan Alur Kerja Visualisasi Data 

Alur  kerja  aplikasi  dalam  memproses  dan  menampilkan 
visualisasi data dapat dilihat pada Gambar 4. Data PDRB yang 
diinput oleh pengguna Admin dimasukkan ke dalam basis data. 
Kemudian data dalam basis data dipanggil dengan query yang 
berisi  kode  untuk  mengolah data  PDRB  dengan  analisis  shift 
share  dan  location  quotient.  Setelah  itu  data  ditampilkan 
menuju  antarmuka  berupa  visualisasi  yang  dapat  dilihat  oleh 
pengguna  secara  umum.  Ketika  pengguna  mengganti  pilihan 
view,  maka  query  akan  melakukan  pemanggilan  data  sesuai 
pilihan pengguna. 

Gambar 4. Alur kerja visualisasi data 

C.  Perancangan Basis Data 

Terdapat  5  entitas  yang  ada  dalam  basis  data  yang  akan 
dibangun, yaitu data, wilayah, kelompok wilayah, sektor, dan 
users.  Entitas  tersebut  memiliki  hubungan  dengan  entitas 
lainnya  yang  digambarkan  dengan  Entity  Relationship 
Diagram (ERD) pada Gambar 5. 

Gambar 5. Entity relationship diagram basis data rancangan 

D.  Perancangan Antarmuka 

Perancangan antarmuka dibangun dengan tampilan mockup 
yang  digunakan  sebagai  dasar  untuk  membangun  antarmuka 
aplikasi. Secara umum aplikasi yang akan dibangun memiliki 
dua halaman utama yang penting, yaitu halaman dashboard dan 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

halaman  kelola  data.  Halaman  dashboard  berfungsi  untuk 
menampilkan  visualisasi  data  hasil  analisis  berdasarkan  data 
yang telah di input. Sedangkan halaman kelola data berfungsi 
untuk melakukan tambah dan hapus data. Rancangan halaman 
dashboard dan kelola data dapat dilihat pada Gambar 6 dan 7. 

Gambar 6. Rancangan antarmuka dashboard 

Gambar 7. Rancangan antarmuka kelola data 

E.  Implementasi 

Tahap implementasi dilakukan dengan menggunakan bahasa 
pemrograman  PHP  dan  Javascript  dengan  memanfaatkan 
framework Laravel dan library open source seperti chart.js dan 
highcharts untuk membuat visualisasi data.  

Yang  menjadi  halaman  utama  pada  aplikasi  ini  adalah 
halaman  dashboard  yang  menampilkan  visualisasi  hasil 
analisis  secara  interaktif.  Halaman  dashboard  dapat  diakses 
secara umum tanpa perlu  login terlebih dahulu. Pada aplikasi 
ini  terdapat  dua  jenis  dashboard,  yaitu  dashboard  kategori 
sektor dan dashboard wilayah provinsi. 

Gambar  8  menunjukkan  tampilan  halaman  dashboard 
kategori  sektor.  Dashboard  kategori  sektor  merupakan 
halaman  yang  menghitung  dan  memvisualisasikan  hasil 
analisis  shift  share  dan  location  quotient  seluruh  provinsi 
secara  makro  berdasarkan  suatu  pilihan  sektor  tertentu. 
Pengguna dapat memilih pilihan kategori sektor, tahun analisis, 
dan tahun dasar pada  dropdown  di bagian atas untuk melihat 
dashboard berdasarkan kategori sektor dan periode tahun yang 
diinginkan.  

Visualisasi  yang  ditampilkan  dalam  dashboard  ini  antara 
lain  peta  choropleth,  pie  chart,  bar  chart,  sankey  diagram, 

 5 / 9 

 
 
 
 
 
 
treemap  dua  level  dan  scatter  plot  dalam  bentuk  tipologi 
klassen.  Peta  choropleth  menggambarkan  data  hasil  analisis 
location  quotient,  semakin  pekat  warnanya  maka  semakin 
tinggi  nilainya.  Pie  chart  menggambarkan  perbandingan 
jumlah  antara  wilayah  basis  dan  non  basis.  Bar  chart 
menggambarkan  besarnya  nilai  differential  shift  pada  tiap 
provinsi. Tipologi klassen mengklasifikasikan tiap provinsi ke 
dalam kuadran yang menentukan bahwa  sektor pada provinsi 
tersebut  termasuk  sektor  prima,  potensial,  berkembang,  atau 
terbelakang.  Sankey  diagram  menggambarkan  besarnya 
alokasi jumlah PDRB pada tiap kelompok wilayah. Tree map 
menggambarkan jumlah PDRB setiap provinsi yang besarnya 
direpresentasikan dengan ukuran persegi. Warna pada tree map 
merepresentasikan kelompok wilayah. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

dari  suatu  provinsi  yang  dipilih.  Pengguna  dapat  memilih 
pilihan wilayah provinsi, tahun analisis, dan tahun dasar pada 
dropdown di bagian atas untuk melihat dashboard berdasarkan 
provinsi dan periode tahun yang diinginkan. 

tren 

laju  PDRB.  Pie 

Visualisasi  yang  ditampilkan  dalam  dashboard  ini  antara 
lain line chart, pie chart, bar chart, sankey diagram, treemap 
satu  level,  scatter  plot  dalam  bentuk  tipologi  klassen.  Line 
chart  menggambarkan 
chart 
menggambarkan perbandingan jumlah antara sektor basis dan 
non basis.  Bar chart  menggambarkan besarnya nilai  national 
share, proportional shift, dan differential shift pada tiap sektor. 
Tipologi  klassen  mengklasifikasikan  tiap  sektor  ke  dalam 
kuadran sektor prima, potensial, berkembang, atau terbelakang. 
Sankey  diagram  menggambarkan  besarnya  alokasi  jumlah 
PDRB.  Tree  map  menggambarkan  jumlah  PDRB  tiap  sektor 
yang  besarnya  direpresentasikan  dengan  warna  dan  ukuran 
persegi. 

Gambar 8. Tampilan dashboard kategori sektor  

Gambar  9  menunjukkan  tampilan  halaman  dashboard 
wilayah  provinsi.  Dashboard  wilayah  provinsi  dapat 
menghitung dan memvisualisasikan hasil analisis berdasarkan 
pilihan  provinsi.  Data  yang  ditampilkan  adalah  hasil  analisis 

Gambar 9. Tampilan dashboard wilayah provinsi 

 6 / 9 

 
 
 
 
Pengelolaan  data  pada  aplikasi  ini  dilakukan  oleh  Admin 
dengan  login  terlebih  dahulu.  Gambar  10  menunjukkan 
tampilan antarmuka login. 

Gambar  13  menunjukkan  form  untuk  input  data  pada 
halaman  kelola  data.  Untuk  melakukan  input  data,  pengguna 
memilih  file  excel  yang  sesuai  format,  kemudian  menekan 
tombol “Tambah”. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 10. Tampilan login 

Gambar  11  menunjukkan  tampilan  halaman  kelola  data. 
Halaman ini berfungsi untuk mengelola data PDRB yang ada 
dalam  aplikasi  ini.  Halaman  ini  hanya  dapat  diakses  oleh 
Admin  ketika  telah  login  ke  dalam  aplikasi.  Pada  halaman 
kelola  data  terdapat  tiga  fungsi  utama  yaitu  generate  format, 
input data, dan hapus data yang ditampilkan dengan card yang 
berbeda.  

Gambar 13. Form tambah data 

Gambar  14  menunjukkan  form  untuk  hapus  data  pada 
halaman kelola data. Untuk melakukan hapus data,  pengguna 
dapat  memilih  data  yang  ingin  dihapus  berdasarkan  pilihan 
wilayah provinsi dan tahun tertentu. Setelah itu  aplikasi akan 
memunculkan  tampilan  view  data  sesuai  pilihan  wilayah 
provinsi dan tahun. Pengguna dapat menekan tombol “Hapus 
Data” jika ingin menghapus data tersebut. Pengguna juga dapat 
menggunakannya untuk melihat data dalam bentuk tabel. 

Gambar 14. Form hapus data 

Untuk  melihat  tata  cara  melakukan  input  dan  hapus  data, 
pengguna dapat menuju halaman panduan yang dapat diakses 
melalui  menu  pada  sidebar.  Halaman  ini  berisi  mengenai 
langkah-langkah  untuk  melakukan  input  dan  hapus  data. 
Tampilan halaman panduan ditunjukkan pada Gambar 15. 

Gambar 11. Tampilan kelola data 

format  dan 

Fungsi  generate 

input  data  dibangun 
menggunakan library Laravel Excel yang tersedia pada Laravel. 
Gambar  12  menunjukkan  form  untuk  generate  format  excel 
pada halaman kelola data. Untuk melakukan generate format, 
pengguna harus memilih wilayah provinsi dan tahun sesuai data 
yang  akan  diinputkan.  Kemudian  pengguna  menekan  tombol 
“Generate  Format”,  maka  aplikasi  akan  memberikan  umpan 
balik  berupa  file  excel  yang  berisi  format  input  yang  akan 
terunduh secara otomatis. 

Gambar 12. Form generate format 

Gambar 15. Tampilan halaman panduan 

Pada aplikasi ini juga terdapat halaman tentang yang berisi 
penjelasan  mengenai  aplikasi  ini.  Tampilan  halaman  tentang 
ditunjukkan pada Gambar 16. 

 7 / 9 

 
 
 
 
 
 
 
 
Gambar 16. Tampilan halaman tentang 

Aplikasi ini juga memiliki fitur pengaturan akun yang dapat 
diakses dengan menuju halaman user profile yang berada pada 
menu dropdown user di pojok kanan atas. Halaman user profile 
merupakan halaman yang menampilkan informasi tentang akun 
Admin  yang  digunakan  untuk  login  ke  dalam  aplikasi.  Pada 
halaman ini, terdapat berbagai macam fitur terkait pengaturan 
akun  seperti  mengganti  nama,  mengganti  email,  mengganti 
password, mengaktifkan  autentikasi dua  faktor, dan lain-lain. 
Tampilan halaman user profile ditunjukkan pada Gambar 17. 

Gambar 17. Tampilan halaman user profile 

F.  Pengujian 

Pengujian  dilakukan  untuk  mengetahui  kelayakan  aplikasi 
terhadap pengguna. Metode pengujian yang digunakan adalah 
uji Black-box dan System Usability Scale (SUS). 

Uji black box pada aplikasi ini dilakukan dengan uji coba 43 
skenario kejadian yang dilakukan oleh peneliti. Hasil pengujian 
didapatkan  bahwa  semua  skenario  yang  diujikan  telah  sesuai 
seperti yang diharapkan. 

Uji SUS dilakukan kepada 16 orang mahasiswa Politeknik 
Statistika  STIS yang pernah mendapatkan materi perkuliahan 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

tentang  shift  share  dan 
location  quotient  sebelumnya. 
Berdasarkan  hasil  pengujian,  diperoleh  rata-rata  skor  sebesar 
86,87  yang  berarti  aplikasi  sudah  mampu  memudahkan 
pengguna dalam melakukan analisis data. 

VII. 

PENUTUP 

Berdasarkan  hasil  penelitian  yang  telah  dilakukan,  maka 

dapat diambil kesimpulan sebagai berikut: 

ini 

dapat  mengotomatisasi 

1.  Telah  dibangun  aplikasi  yang  dapat  menghitung  dan 
memvisualisasikan  hasil  analisis  shift  share  dan 
location  quotient  dalam  bentuk  dashboard  interaktif. 
Aplikasi 
proses 
penghitungan analisis shift share dan location quotient, 
sehingga 
perlu  melakukan 
penghitungan untuk mendapatkan hasil analisis. Selain 
itu  hasil  analisis  pada  aplikasi  ini  disajikan  dalam 
bentuk  visualisasi,  sehingga 
lebih  menarik  dan 
memudahkan  pengguna  dalam  melakukan  analisis 
terhadap data yang disajikan. 

pengguna 

tidak 

2.  Aplikasi  pada  penelitian  ini  memiliki  fitur  visualisasi 
data berupa bar chart, sankey diagram, pie chart, tree 
map, line chart, peta choropleth, dan Tipologi Klassen 
yang  disajikan  dalam  dua  macam  dashboard  yang 
sektor  dan 
berbeda,  yaitu  dashboard  kategori 
dashboard  wilayah  provinsi.  Pada  dashboard  kategori 
sektor,  data  visualisasi  yang  ditampilkan  adalah  data 
hasil  analisis  dari  seluruh  provinsi  berdasarkan  sektor 
yang  dipilih.  Sehingga  visualisasi  yang  telah  dibuat 
pada  dashboard  tersebut  dapat  menampilkan  dan 
membandingkan hasil analisis secara makro.  

3.  Fitur  visualisasi  Tipologi  Klassen  yang  telah  dibuat 
dapat  mengklasifikasikan  hasil  analisis  kedalam  suatu 
kuadran  yang  menentukan  kondisi  sektor  pada  suatu 
daerah.  Sehingga  dapat  diketahui  apakah  sektor  pada 
suatu daerah tersebut termasuk sektor prima, potensial, 
berkembang, atau terbelakang.  

4.  Aplikasi  yang  dibangun  dilengkapi  dengan  fitur 
pengelolaan data yang terdiri dari generate format, input 
data,  dan  hapus  data.  Input  data  dibangun  dengan 
yang 
library 
Laravel 
memanfaatkan 
memungkinkan 
data 
untuk  melakukan 
menggunakan  file  excel,  sehingga  penginputan  data 
dalam jumlah besar sekaligus dapat dilakukan. 

Excel 
import 

DAFTAR PUSTAKA 
[1]  A. K. Monika, K. T. Wahyuni, O. Syafwil, dan R. N. T. Wahyuni, Modul 

Sistem Neraca Nasional I. Jakarta: Sekolah Tinggi Ilmu Statistik. 

[2]  A.  Kirk,  Data  Visualization:  a  successful  design  process.  Birmingham: 

Packt, 2012. 

[3]  A. S. Rosa dan M. Shalahuddin, Rekayasa Perangkat Lunak Terstruktur 

dan Berorientasi Objek. Bandung: Informatika, 2014. 

[4]  Badan  Pusat  Statistik.  (2020).  Penyusunan  PDB  Indonesia  Tahunan 
Menurut  Lapangan  Usaha  (2010=100),  2020  [Online].  Available: 
https://sirusa.bps.go.id/sirusa/index.php/dasar/view?kd=3452&th=2020 
[5]  G. R. Winarko, Sistem Informasi Geografis Berbasis Web Dengan Analisis 
Geographically  Weighted  Regression  (Studi  Kasus  Faktor-Faktor  yang 
Mempengaruhi PDRB Kabupaten/Kota di Indonesia Tahun 2010). Jakarta: 
Sekolah Tinggi Ilmu Statistik, 2016. 

 8 / 9 

 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

[6]  Handigo,  Visualisasi  Data  Menggunakan  Sistem  Informasi  Geografis 
untuk  Statistik  Transportasi  Darat  di  Indonesia.  Jakarta:  Politeknik 
Statistika STIS, 2020. 

[7]  J. Heer, M. Bostock, dan V. Ogievetsky, “A Tour through the Visualization 
Zoo,” Communications of the ACM, vol. 53, no. 6, pp. 59-67, Juni 2010. 
[8]  L.  T.  Puspitawati,  “Analisis  Perbandingan  Faktor-Faktor  Penyebab 
Ketimpangan  Pembangunan  Antar  Kabupaten/Kota  di  Kawasan 
Kedungsapur,”  Economics  Development  Analysis  Journal,  vol.  2, no.  2, 
12 Juni 2013. 

[9]  M.  H.  Ashshidiq,  Visualisasi  Data  Menggunakan  Sistem  Informasi 
Geografis  Untuk  Kegiatan  Ekspor  dan  Impor  Indonesia.  Jakarta: 
Politeknik Statistika STIS, 2020. 

[10] S. Few, “Dashboard Confusion,” Intelligent Enterprise, 20 Maret 2018. 
[11] T. Widodo, Perencanaan Pembangunan: Aplikasi Komputer (Era Otonomi 

Daerah). Yogyakarta: UPP STIM YKPN, 2006. 

 9 / 9 

 
 
 
 
"
221709591,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Perancangan Kembali Antarmuka Pengguna Web
Portal Mahasiswa SIPADU STIS dengan Metode
User Centered Design

Banu Burkhairi (221709591, 4SI1)

Dosen Pembimbing: Lutﬁ Rahmatuti Maghﬁroh

Ringkasan—Politeknik Statistika STIS memiliki sistem infor-
masi terpadu yang bernama SIPADU STIS. Salah satu sub-
sistemnya ialah Web Portal Mahasiswa SIPADU STIS yang selan-
jutnya disebut SIPADU Mahasiswa. Walaupun sudah lama digu-
nakan sejak 2010, SIPADU Mahasiswa belum memenuhi standard
web design. Padahal perkembangan teknologi dalam web desain
sudah semakin baik. Dari masalah tersebut dilakukan survei
untuk mengetahui bagaimana penilaian pengguna terhadap an-
tarmuka SIPADU Mahasiswa. Hasil mengatakan bahwa SIPADU
Mahasiswa memiliki nilai dibawah nilai terbaiknya, sehingga
dibutuhkan peracangan kembali antarmuka. Oleh karena itu,
dilakukan perancangan kembali antarmuka pengguna SIPADU
Mahasiswa dengan metode user centered design dan melakukan
evaluasi hasil rancangan kembali antarmuka pengguna SIPADU
Mahasiswa dengan melihat kepuasan pengguna. Perancangan
akan dilakukan dengan tiga kali iterasi dengan membuat model
prototype dan dievaluasi setiap iterasinya. Setelah rancangan
berhasil dibuat akan dilakukan perbandingan antarmuka lama
dan baru untuk mengetahui bagaimana perbandingan penila-
ian antarmuka sebelum dan sesudah dirancang kembali. Hasil
dari penelitian ini pun berhasil membuat Perancangan kembali
SIPADU Mahasiswa dengan mendapatkan antarmuka barusesuai
dengan kepuasan pengguna yang berbentuk dan evaluasi ter-
hadap antarmuka baru dan menunjukkan bahwa antarmuka
baru memiliki penilaian lebih baik dari antarmuka lama.

Kata Kunci—Perancangan Kembali Antarmuka Pengguna,

Web Portal Mahasiswa SIPADU STIS, User Centered Design

I. LATAR BELAKANG

informasi dan komunikasi

“Sistem Informasi Terpadu merupakan sistem informasi
berbasis teknologi
terkini yang
dibangun oleh jurusan Komputasi Statistik dengan mengin-
tegrasikan semua proses pembuatan data pengelolaan data,
dan pengaksesan data pada suatu framework standar sistem
informasi STIS” [1]. SIPADU dibangun sejak tahun 2010
oleh mahasiswa Politeknik Statistika STIS untuk memudahkan
civitas akademika Politeknik Statistika STIS dalam mengelola
kegiatan akademik di Politeknik Statistika STIS. SIPADU
dapat diakses melalui aplikasi berbasis desktop untuk unit
kerja di Politeknik Statistika STIS dan web untuk mahasiswa,
dosen, dan pegawai. Adapun peneliti akan membahas Web
Portal Mahasiswa SIPADU STIS yang selanjutnya disebut
SIPADU Mahasiswa. Alasan peneliti memilih SIPADU Maha-
siswa ini karena SIPADU Mahasiswa yang memiliki pengguna
terbanyak, yaitu seluruh mahasiswa Politeknik Statistika STIS.
SIPADU Mahasiswa adalah website yang memudahkan
mahasiswa untuk mengakses informasi dalam mengelola

kegiatan perkuliahan. Sayangnya, SIPADU Mahasiswa belum
memenuhi standard web design. Menurut Nielsen [2], stan-
dard web design memiliki standar elemen seperti logo disudut
kiri atas halaman,
tidak adanya
splash page, dan menampilkan breadcrumbs secara horizontal.
Dapat dilihat bahwa SIPADU tidak memenuhi standar menurut
Nielsen, seperti tampilan header yang belum disudut kiri atas
halaman secara full width, belum adanya kotak pencarian, dan
tidak adanya breadcrumbs.

terdapat kotak pencarian,

SIPADU Mahasiswa sendiri

sudah dilakukan evaluasi
heuristik dalam penelitian Maghﬁroh [3]. Maghﬁroh meny-
atakan bahwa SIPADU Mahasiswa memiliki satu masalah
pada setiap aspek yang dinilai pada evaluasi heuristik, se-
hingga disarankan untuk dilakukan perancangan ulang an-
tarmuka. Aspek yang harus dilakukan perancangan ulang
adalah pada aspek kontrol dan kebebasan pengguna, pencega-
han kesalahan, pengenalan langkah-langkah, penggunaan yang
ﬂeksibilitas dan eﬁsiensi, keestetikan desain, serta bantuan
pengguna untuk mengenali, berdialog, maupun memulihkan
dari kesalahan.

Untuk menambah dasar yang lebih terkini, penelitian ini
melakukan survei pendahuluan untuk mengetahui bagaimana
sistem ini berjalan bagi para pengguna. Survei dilakukan
dengan dua tahap, yaitu evaluasi heuristik dan Questionnaire
for User Interface Satisfaction (QUIS). Pada evaluasi heuristik
di tabel ?? menunjukkan bahwa kategori yang paling banyak
adalah kategori “Baik”. Ada 63,08% dari seluruh responden
yang menilai bahwa kalau SIPADU-STIS termasuk kategori
“Baik”. Yang artinya pengguna sudah dapat menggunakan
SIPADU Mahasiswa dengan relatif mudah dan juga sebagian
besar bisa menyelesaikan tugas yang sulit pada sistem tersebut
[4]. Namun antarmuka pengguna masih perlu ditingkatkan.

Walaupun memiliki sudah dikatakan memiliki sistem yang
baik, terdapat lima indikator dari empat kriteria yang memiliki
pengaruh lebih rendah. Empat kriteria itu adalah fungsi dan
ﬁtur, navigasi, konten dan tesk, serta pencarian. Sementara itu
hasil QUIS menunjukkan bahwa nilai kepuasan pengguna pada
SIPADU Mahasiswa adalah 7,66 dari sepuluh. Berdasarkan
penelitian sebelumnya [5], menyatakan nilai dibawah delapan
termasuk aplikasi masih perlu dikembangkan antarmukanya.
Kuesioner QUIS memiliki enam kategori dan 32 indikator
pertanyaan. Yang mana terdapat tiga indikator yang mendapat
skor di bawah tujuh dan sembilan belas indikator di bawah

1 / 9

delapan.

Pada survei pendahuluan juga menambahkan pertanyaan
subjektif mengenai hal apa yang disukai, tidak disukai, dan
ﬁtur tambahan dari SIPADU Mahasiswa. Hal yang disukai
adalah kemudahan pemakaian, praktis, informatif, dan sesuai
target penggunaannya. Dan hal yang tidak disukai adalah
tampilan yang tidak menarik, tampilan yang kaku, tampilan
yang kuno, dan kurang ﬂeksibel. Dan pada terakhir ﬁtur
tambahan yang disarankan adalah tampilan yang lebih user
friendly, pembaruan tampilan, konten terkini, mendukung
tampilan mobile, ﬁtur panduan pengguna, dan tata letak yang
lebih baik.

Berdasarkan hasil survei pendahuluan dikatakan bahwa
antarmuka SIPADU Mahasiswa perlu dilakukan peracangan
kembali. Perancangan kembali akan dilakukan dengan metode
user centered design. Metode user centered design adalah
metode yang melakukan proses desain dengan melibatkan
pengguna secara langsung dan memengaruhi bagaimana desain
terbentuk [6]. Hal ini didasarkan karena metode user centered
design memiliki keuntungan untuk melibatkan pengguna yang
memastikan produk akan sesuai dengan tujuan sistem itu
dibangun, sehingga sistem akan lebih efektif, eﬁsien, dan
aman [6]. Dan juga pada penelitian sebelumnya oleh [7]
berhasil melakukan perancangan ulang web portal meng-
gunakan metode user centered design dengan memperoleh
hasil yang memenuhi kebutuhan pengguna dan menyelesaikan
masalah pada sistem.

II. TUJUAN PENELITIAN

Dari masalah yang telah dijelaskan pada latar belakang,
maka tujuan dari penelitian ini adalah melakukan perancangan
kembali antarmuka pengguna SIPADU Mahasiswa dengan
metode user centered design dan melakukan evaluasi hasil
rancangan kembali antarmuka pengguna SIPADU Mahasiswa
dengan melihat kepuasan pengguna.

III. PENELITIAN TERKAIT

Penelitian ini memiliki dua penelitian terkait, yaitu peneli-
tian milik Maghﬁroh [3] dan Simamora [8]. Penelitian Magh-
ﬁroh [3] melakukan evaluasi rancangan dan interaksi antar-
muka dengan pengguna SIPADU Mahasiswa menggunakan
evaluasi heuristik. Penelitian tersebut menyimpulkan bahwa
SIPADU Mahasiswa disarankan untuk dilakukan perbaikan
ulang dalam rangka meningkatkan pelayanan kepada peng-
guna. Hal yang disarankan untuk dilakukan pengembangan
adalah aspek kontrol dan kebebasan serta ﬂeksibilitas dan
eﬁsiensi penggunaan.

Kaitan penelitian tersebut dengan penelitian ini adalah
kesamaan pada melakukan evaluasi heuristik pada SIPADU
Mahasiswa. Kesimpulan penelitian tersebut menjadi salah satu
masalah pada latar belakang penelitian ini untuk melakukan
perancangan kembali. Sedangkan perbedaannya pada peng-
gunaan kuesioner dan populasi yang digunakan. Kuesioner
penelitian ini menggunakan kuesiner milik Turner [4]. Popu-
lasi pada penelitian ini adalah mahasiswa Politeknik Statistika
STIS tahun ajaran 2020/2021.

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Penelitian Simamora [8]. melakukan perancangan kem-
bali antarmuka pengguna sistem pegawai BPS menggunakan
metode user centered design. Dari penelitian tersebut, dije-
laskan bagaimana tahapan penggunaan metode dari analisis
awal hingga akhir. Penelitian tersebut menerapkan metode
user centered design dan menyimpulkan bahwa metode yang
digunakan berhasil meningkatkan antarmuka sesuai dengan
kebutuhan pengguna.

Kaitan penelitian tersebut dan penelitian ini adalah penggu-
naan metode dan alur penelitiannya. Detail metode dan alur
pada penelitian tersebut digunakan pada penelitian ini sebagai
acuan. Yang membedakan adalah pada tahap proses setiap
iterasinya, yaitu mockup, wireframe, dan prototype. Penelitian
ini membuat prototype dari setiap iterasi yang dilakukan
sebanyak tiga kali karena peneliti ingin penilai dapat langsung
menilai bagaimana rancangan yang diusulkan oleh peneliti.

IV. METODE PENELITIAN
Metode yang digunakan dalam penelitian ini adalah user
centered design. Tahapan penelitian dan alat yang digunakan
pada setiap proses diperoleh dari penelitian terkait milik
Simamora [8]. Metode ini digunakan agar dalam proses per-
ancangan kembali antarmuka pengguna SIPADU Mahasiswa
sesuai dengan kebutuhan pengguna. Yang mana tahapan user
centered design yang digunakan dalam penelitian ini akan
dijelaskan sebagai berikut.

Gambar 1. Metode Penelitian

Yang pertama studi literatur, tahapan ini akan diperoleh
dasar toeri sebagai acuan dalam perancangan antarmuka dan

2 / 9

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

metode pengerjaan yang digunakan untuk menyelesaikan per-
masalah pada penelitian ini.

akhir masing-masing indikator diperoleh dari rata-rata setiap
responde dari indikator tersebut.

Yang kedua ada analisis kebutuhan, hal ini dilakukan untuk
mendapatkan informasi kebutuhan pengguna yang akan diran-
cang pada SIPADU Mahasiswa. Proses ini akan memilik dua
tahap, yaitu:

1) Specify the context of use
Pada tahap ini pengguna sebagai populasi target yang pada
penelitian ini adalah mahasiswa Politeknik Statistika STIS.
Dimana pengguna yang menggunakan SIPADU Mahasiswa
jadwal kuliah
untuk mengakses informasi, seperti melihat
dan perubahan jadwal kuliah, melihat nilai, sampai dengan
penyusunan skripsi. Yang mana selanjutnya akan dilakukan
survei pendahuluan untuk mengetahui keadaan sistem saat ini.
Survei pendahuluan akan dilaksanakan dengan metode
convenience sampling dengan mendapatkan 260 responden
untuk mengisi kuesioner evaluasi heuristik. Selanjutnya di-
lakukan 260 responden itu dikelompokkan berdasarkan tiga
tingkat pemahaman yang ditentukan melalui
tiga variabel,
yaitu bagaimana pengetahuan pengguna terhadap seluruh ﬁtur
SIPADU Mahasiswa, bagaimana pengalaman pengguna ter-
hadap antarmuka pengguna, dan apakah pengguna memiliki
pengetahuan tentang antarmuka pengguna secara baik.

Selanjutnya pengguna akan ditanyakan apakah mereka
bersedia untuk menjadi evaluator dalam penelitian ini. Dari
yang bersedia kemungkinan diambil masing-masing lima dari
setiap tingkat pemahaman pengguna terhadap antarmuka yang
dilakukan sebelumnya. Yang mana lima belas evaluator ini
akan menjadi responden pada tahap kedua survei pendahuluan
untuk mengisi QUIS dan analisis pengguna serta menjadi
perwakilan pengguna lainnya untuk menilai rancangan pada
setiap proses desain berlangsung.

2) Specify the user and organizational requirements
Proses ini dilakukan untuk mengidentiﬁkasi kebutuhan
pengguna. Tahap ini dilakukan melalui survei pendahuluan.
Survei pendahuluan dilakukan dengan dua tahap. Tahap per-
tama memiliki kuesioner yang diisi untuk pengkategorian re-
sponden dan evaluasi heuristik. Tahap kedua dilakukan dengan
QUIS dan analisis pengguna dengan memberikan pertanyaan
mengenai apa yang pengguna sukai, tidak sukai, dan saran
mengenai tampilan antarmuka pengguna SIPADU Mahasiswa.
Evaluasi heuristik bertujuan untuk menilai kriteria usability
antarmuka pengguna. Kuesioner terdiri dari sebelas bagian
dengan 45 pertanyaan. Untuk mengolah hasil kuesioner akan
diambil modus dari skor total indikator pada masing-masing
responden. Untuk mengetahui apakah tampilan memiliki nilai
yang baik atau tidak, digunakan persentase modus skor total
indikator dengan skor penuh dari indikator tersebut.

Tahap kedua dilakukan dengan pengisian kuesioner QUIS
dan analisis pengguna, yang mana sebelumnya telak dilakukan
pengambilan responden sebanyak lima belas orang menjadi
evaluator. QUIS dilakukan untuk mengukur kepuasan ter-
hadap antarmuka pengguna. Kuesioner ini terdiri dari enam
bagian dengan jumlah pertanyaan sebanyak 32 pertanyaan.
Untuk memperoleh hasil akhir secara keseluruhan diambil
rata-rata dari hasil akhir setiap responden. Dan untuk hasil

Selanjutnya ialah memasuki tahap perancangan. Tahap ini
dilakukan dengan menggunakan permodelan UML (Uniﬁed
Modeling Languange). Dengan metode user centered design
maka tahapan menyesuaikan dengan metode yang digunakan
dan kebutuhan penelitian yang dilakukan, yaitu:

1) Product design solutions
Tahap ini peneliti membuat perancangan antarmuka
berdasarkan hasil analisis kebutuhan. Proses desain ini di-
lakukan untuk bisa dicoba implementasikan dalam bentuk
prototype agar dapat mengetahui secara langsung bagaimana
design solution diimplementasikan. Landasan yang digunakan
dalam perancangan adalah ten rule of thumb, hasil evaluasi
heuristik, hasil QUIS, dan hasil analisis pengguna. Hal itu
digunakan untuk membuat model melalui media kertas. Dan
selanjutnya akan diterjemahkan ke pemrograman untuk dibuat
prototype.

2) Evaluate designs
Selanjutnya hasil dari design solutions yang berhasil dibuat
akan dievaluasi oleh responden yang terlibat pada pengisian
QUIS sebelumnya. Evaluasi dilakukan untuk mengetahui
apakah sudah sesuai dengan kebutuhan atau belum dan terjadi-
lah iterasi. Jika belum sesuai peneliti akan kembali melakukan
perancangan berdasarkan hasil evaluasi sebelumnya. Namun,
pada iterasi terakhir, evaluasi dilakukan dengan menambah
lima responden dari yang tidak mengikuti proses desain
berlangsung. Skenario ujicoba yang digunakan pada ketiga
model sama, yaitu mengakses model, menjelajahi model ke-
mudian mengisi kuesioner yang telah diberikan.

Setelah semua iterasi sudah dilakukan dan memiliki Pro-
totype terakhir. Langkah selanjutnya ialah melakukan per-
bandingan antara antarmuka pengguna yang lama dengan yang
baru. Tujuan dari tahapan ini adalah membandingkan hasil
rancangan antarmukan terakhir dengan antarmuka SIPADU
Mahasiswa saat ini. Perbandingan menggunakan hasil evaluasi
heuristik dan QUIS yang terakhir dilakukan evaluasi. Hasil
perbandingan akan ditampilkan dalam bentuk graﬁk.

Tahap terakhir adalah pengambilan kesimpulan. Hal
ini
dilakukan untuk mengetahui apa saja yang harus diperbaiki
kedepannya pada perancangan. Hasil perancangan dan evaluasi
akan menjadi isi dari kesimpulan.

V. KERANGKA PIKIR

Penelitian ini dimulai dari identiﬁkasi masalah. Masalah
yang ditemukan adalah tampilan antarmuka pengguna yang
sudah ketinggalan dan sudah lama tidak mengalami perkem-
bangan tampilan. Dan setiap aspek pada SIPADU Maha-
siswa masih terdapat masalah. Dari masalah itu, pengguna
kesulitan untuk lebih interaktif dalam menggunakan SIPADU
Mahasiswa. Oleh karena itu, peneliti melakukan perancangan
kembali antarmuka pengguna yang melibatkan pengguna se-
cara langsung untuk meningkatkan kepuasan pengguna dan
membuat pengguna lebih interaktif lagi dalam menggunakan
sistem.

3 / 9

Untuk melakukan itu, penelitian ini menggunakan metode
user centered design. Metode ini menggunakan analisis kebu-
tuhan pengguna secara langsung untuk memperoleh kepuasan
pengguna secara tepat. Setelah itu, penelitian akan memiliki
tiga iterasi yang setiap iterasinya akan dilakukan evaluasi pada
rancangan sebelumnya. Yang mana diawali dengan melakukan
pada tahap survei pendahuluan yang akan menjadi dasar iterasi
pertama, lalu diakhiri dengan survei akhir membandingkan
tampilan antarmuka yang lama dengan yang baru. Setiap
iterasi akan langsung diimpelementasikan menjadi prototype
yang bisa langsung dicoba oleh sampel pengguna untuk dinilai
setiap evaluasinya. Penelitian ini pun dilakukan untuk mencari
hasil penilaian terbaik dari proses tiga iterasi tersebut. Berikut
Gambar 2 merupakan kerangka pikir yang telah dijabarkan
sebelumnya.

Gambar 2. Kerangka Pikir Penelitian

VI. HASIL DAN PEMBAHASAN

A. Survei Pendahuluan

Survei pendahuluan dilakukan diawal untuk melihat kon-
disi antarmuka pengguna pada SIPADU Mahasiswa saat ini
dan memperbarui hasil evaluasi heuristik pada penelitian se-
belumnya [3]. Survei ini akan memiliki dua tahap. Tahap
pertama melakukan pengisian kuesioner evaluasi heuristik dan
memberikan pertanyaan untuk dilakukan pengkategorian, serta
pertanyaan apakah bersedia menjadi evaluator pada penelitian
ini.

Target responden pada survei ini adalah pengguna SIPADU
Mahasiswa, yaitu mahasiswa Politeknik Statistika STIS tahun
akademik 2020/2021. Kerangka sampel pada survei ini adalah
data mahasiswa Politeknik Statistika STIS tahun akademik
2020/2021. Selanjutnya, populasi, unit observasi, dan unit
sampling adalah seluruh mahasiswa Politeknik Statistika STIS
tahun akademik 2020/2021. Jumlah sampel mengikuti penda-
pat dari Kuncoro [9] yang menyatakan survei untuk deskriptif
cukup memiliki 10% dari populasi.

Metode sampling yang digunakan ialah Convenience Sam-
pling. Hal ini dilakukan dengan menyebar kuesioner melalui
email ke seluruh populasi, lalu menunggu ketersedian mereka
untuk mengisinya selama satu pekan. Dari survei itu sudah
ada responden sebanyak 260 respon, yang mana itu sudah
memenuhi jumlah sampel yang diinginkan, yaitu 10% dari
populasi. Berikut gambar 3 adalah sebaran dari responden
berdasarkan tingkat masa perkuliahan.

1) Evaluasi Heuristik
Evaluasi akan dilakukan kepada pengguna secara langsung
yang akan menjadi responden. Ada sebanyak 260 responden
yang mengisi kuesioner sesuai pada pengambilan sampel.
Setelah melakukan pengolahan data pada evaluasi heuristi,
didapatkan distribusi hasil menurut kategori pada tabel I.

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Gambar 3. Jumlah Responden Berdasarkan Tingkat Perkuliahan

TABEL I
HASIL EVALUASI HEURISTIK

Kriteria
Sangat Buruk

Rentang Nilai
0-29

Jumlah
0

Persentase
0,00%

Buruk

Biasa

Baik

Antara 29 dan 49

Antara 49 dan 69

Antara 69 dan 89

Sangat Baik

89-100

Total

3

56

164

37

260

1,15%

21,54%

63,08%

14,23%

100%

Hasil pada tabel I menunjukkan bahwa kategori yang paling
banyak adalah kategori “Baik”. Ada 63,08% dari seluruh
responden yang menilai bahwa kalau SIPADU-STIS termasuk
kategori “Baik”. Yang artinya pengguna sudah dapat meng-
gunakan Web Portal SIPADU-STIS dengan relatif mudah dan
juga sebagian besar bisa menyelesaikan tugas yang sulit pada
sistem tersebut [4]. Walaupun sudah dikatakan memiliki sistem
yang baik, dari sepuluh kriteria dan 45 indikator usability
yang diukur dalam evaluasi heuristik. Ada empat kriteria dan
lima indikator yang memiliki pengaruh lebih rendah . Untuk
melakukan peningkatan pada antarmuka dilakukan kepada
indikator yang pengaruhnya lebih rendah dibanding indikator
lain [4]. Kelima indikator tersebut dapat dilihat pada tabel II
berikut.

tersebut

Berdasarkan tabel

II, kelima indikator

tidak
memenuhi skor penuh dari indikator yang dimiliki. Yang mana
indikator yang memenuhi itu harus memiliki penilaian yang
memenuhi skor penuh tersebut. Indikator-indikator tersebut
adalah kriteria fungsi dan ﬁtur, navigasi, konten dan teks,
serta pencarian. Indikator-indikator ini akan dilakukan desain
untuk diperbaiki secara bertahap pada proses perancangan
berlangsung.

Sebelum memasuki tahap selanjutnya, survei pendahuluan
melakukan pengkategorian pengguna berdasarkan level penge-
tahuan pengguna mengenai sisten dan antarmuka pengguna.
Hasil kategori dapat dilihat pada gambar 4.

Gambar 4 menunjukkan kategori responden berdasarkan
pengalaman terhadap SIPADU Mahasiswa dan pengetahuan
mengenai antarmuka pengguna. Level 1 user menunjukkan

4 / 9

TABEL II
TABEL INDIKATOR BERPENGARUH LEBIH RENDAH

sebelumnya. Sehingga didapat gambar 5.

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Gambar 5. Distribusi Responden untuk Keterangan Tambahan

Responden untuk QUIS ini sendiri berjumlah lima be-
las orang yang mana dipilih dari masing-masing kategori
sebanyak lima orang yang memilih bersedia yang mengisi
kuesioner sebelumnya. Hasil perhitungan QUIS menunjukkan
bahwa rerata nilai kepuasan pengguna SIPADU Mahasiswa
bernilai 7,66 dari skala penuh 10. Yang mana itu sudah
dikategorikan dalam sistem yang baik tetapi masih perlu
dikembangkan. Dari 6 kategori dan 32 indikator, terdapat 3
indikator yang mendapat skor di bawah tujuh dan 19 indikator
di bawah delapan. Secara lengkap dapat dilihat pada tabel III
Pada tabel ??, indikator yang memiliki skor dibawah de-
lapan ditandai dengan warna oranye, sedangkan yang memi-
liki skor dibawah enam tujuh ditandai dengan warna merah.
Kategori yang memiliki skor indikator dibawah delapan se-
banyak enam, yaitu umum (3 indikator), layar (2 indikator),
Terminologi dan Sistem Informasi (3 indikator), Kemudahan
Mempelajari Aplikasi (3 indikator), kinerja sistem (3 indika-
tor), dan antarmuka pengguna (5 indikator). Sementara itu,
kategori yang memiliki skor indikator dibawah tujuh sebanyak
tiga, yaitu layar, kemudahan mempelajari aplikasi, dan kinerja
sistem. Yang mana ketiga kategori hanya memiliki masing-
masing satu indikator yang bernilai dibawah tujuh. Sisanya
memiliki indikator diatas delapan. Indikator-indikator. Selan-
jutnya, indikator - indikator yang memiliki skor dibawah dela-
pan dan tujuh akan dilakukan perbaikan yang akan dilakukan
pada tahap desain. Yang mana hasil QUIS ini menjadi dasar
dalam perbaikan dalam tahap selanjutnya.

3) Analisis Pengguna
Tahap ini akan melakukan analisi pertanyaan apa yang
pengguna sukai,
tidak sukai, dan sarankan pada SIPADU
Mahasiswa. Dari hasil pengisian kuesioner ini, pada per-
tanyaan apa yang disukai dari SIPADU Mahasiswa ialah re-
sponden menyukai kemudahan pemakaian, praktis, informatif,
dan sesuai target penggunaannya. Dan pada bagian pertanyaan
yang tidak disukai dari SIPADU Mahasiswa ialah tampilan
yang tidak menarik, tampilan yang kaku, tampilan yang kuno,
kurang ﬂeksibel dan sebagainya. Dan pada pertanyaan terakhir,
yaitu tentang ﬁtur tambahan. Hasilnya yang diinginkan ialah
tampilan yang lebih user friendly, pembaruan tampilan, konten

5 / 9

Gambar 4. Distribusi Responden Berdasarkan Kategori

responden adalah pengguna awam. Yang artinya pengguna
masih butuh waktu untuk menguasai SIPADU Mahasiswa
dan kurangnya pengetahuan mengenai antarmuka pengguna.
Level 2 user adalah pengguna tingkat menengah, pengguna
sudah bisa menguasai SIPADU Mahasiswa dengan baik, tetapi
kurangnya pengetahuan mengenai antarmuka pengguna. Level
3 user adalah expert frequent user, atau pengguna yang sudah
mahir menggunakan SIPADU Mahasiswa dan memahami teori
antartmuka pengguna dengan baik. Dari hasil gambar 4 me-
nunjukkan bahwa pengguna SIPADU Mahasiswa pada tahun
ajaran 2020/2021 sudah memiliki kemampuan terhadap meng-
gunakan SIPADU dan pengalaman yang baik pada antarmuka
pengguna. Hal itu membuat mereka tidak merasa kesulitan
untuk mempelajari antarmuka pada sistem ini.

2) Questionnaire for User Interface Satisfaction (QUIS)
Tahap kedua survei ini adalah melakukan pengisian kue-
sioner QUIS dan analisis pengguna dengan menambahkan
pertanyaan apa yang pengguna sukai, tidak sukai, dan sarankan
pada SIPADU Mahasiswa. Dalam pengambilan sampel sendiri
didasarkan pada pendapat [10] yang menyatakanlima orang
sudah cukup untuk memberikan hasil tes dan umpan balik
yang memadai. Pengambilan sampel pun dilakukan masing-
masing lima dari tiap kategori. Responden pada kuesioner
ini ialah orang-orang yang mengisi bersedia pada kuesioner

TABEL III
HASIL PENGHITUNGAN QUIS

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

diberika pada input atau pesan error, tampilan yang sulit di-
pahami oleh pengguna, tampilan cenderung mengurangi peng-
guna untuk menggunakan sistem secara eﬁsien, dan tampilan
antarmuka yang dirasa masih belum memenuhi standar, seperti
masih adanya space kosong pada kiri dan kanan halaman. Dari
hasil analisis pengguna, aspek yang memiliki masalah adalah
tampilan antarmuka yang dirasa kaku dan kuno, tampilan yang
tidak responsif, tidak ﬂeksibel, dan tata letak.

2) Desain
Pada tahap ini dibuat model prototype untuk halaman be-
randa, jadwal kuliah, perkuliahan, data diri, dan data orangtua.
Halaman-halaman tersebut telah tercakup sesuai dengan anal-
isis fungsional. Model yang dibuat akan disesuaikan dengan
antarmuka lama, yaitu menggunakan ﬁtur navigasi samping
atau selanjutnya disebut sidebar dan navigasi atas atau yang
selanjutnya disebut navbar.

terkini, mendukung tampilan mobile, ﬁtur panduan pengguna,
dan tata letak yang lebih baik. Dari hasil tersebut, peneliti
akan mempertahankan hal yang disuka dari pengguna dan
akan melakukan perbaikan pada hal yang tidak disukai. Dari
hal yang disarankan juga akan dicoba diterapkan pada design
solution yang dibangun.

B. Iterasi I - Prototype I

1) Analisis Fungsional
Pada analisis ini, setiap elemen yang dirancang ulang akan
dihubungkan dengan struktur sistem secara keseluruhan. Pada
hasil evaluasi heuristik, sistem memiliki kendala pada empat
bagian. Yang paling utama adalah pada Pencarian, kolom
pencarian pada SIPADU dirasa tidak dimiliki oleh SIPADU
Mahasiswa. Yang mana ﬁtur tersebut hanya terdapat pada
ﬁtur pencarian dosen dan pencarian judul skripsi. Bagian
selanjutnya adalah perlunya dilakukan pengembangan pada
bagian konten, fungsi dan ﬁtur serta navigasi. Contohnya
ﬁtur absensi pada tampilan beranda yang dirasa kurang mem-
berikan informasi kepada pengguna sesuai tujuan.

Hasil QUIS memberikan aspek yang bermasalah pada be-
berapa bagian. Bagian tersebut adalah kurangnya menyoroti
ﬁtur utama dari suatu halaman, kurangnya infromasi yang

Gambar 6. Tampilan Beranda SIPADU Mahasiswa

Gambar 7. Tampilan Beranda pada Iterasi-I

lebih memenuhi

Tampilan dibuat

layar perangkat. Yang
mana sebelumnya SIPADU Mahasiswa terdapat area kosong
pada bagian kiri dan kanan halamannya. Hal
ini didasari
dengan menerapkan magazine style, model yang memberikan
space yang cukup untuk menampilkan konten yang beragam
dan kompleks [11]. Hal ini cocok untuk membuat SIPADU
Mahasiswa menerapkan tampilan full page web. Tampilan

6 / 9

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

dibuat memiliki tampilan sidebar, navbar, dan konten yang
ditampilkan yang baru. Yang mana template ini menyesuaikan
pada kebutuhan pengguna pada analisis pengguna dan hasil
QUIS.

Design solution yang diusulkan pada halaman beranda juga
melakukan perbaikan pada aspek yang bermasalah pada hasil
evaluasi heuristik, yaitu ﬁtur absensi yang dirasa kurang mem-
berikan informasi. Design solution yang dibuat membuat kon-
ten yang baru dengan menggantinya dari sebelumnya berben-
tuk seperti speedometer menjadi ﬁllbar yang lebih mendetail
pada setiap mata kuliahnya

3) Uji Coba
Setelah design solution selesai dibuat, uji coba dilakukan
dengan skenario yang sudah dijelaskan pada Bab Metodologi,
yaitu melakukan evaluate against requirement dan menam-
bahkan pertanyaan mengenai bagaimana tanggapan dan saran
perbaikan yang perlu dilakukan pada design solution yang
diberikan. Hasil uji coba dapat dilihat pada tabel IV.

card yang perlu dikembangkan lagi karena membuat tata letak
tampilan menjadi kurang stabil seperti penggunaan tabel dan
jadwal kuliah saat ini. Dan juga mempertimbangkan untuk
membuat halaman khusus untuk ﬁtur cari dosen yang dirasa
pengguna tidak cocok disatukan dengan konten perkuliahan.

2) Desain
Pada tahap ini model prototype – II dibuat sebagian hala-
man. Tujuan pemodelan ini adalah untuk melihat bagaimana
kepuasan pengguna terhadap design solution yang diberikan
setelah dilakukan perbaikan dari seluruh hasil evaluasi se-
belumnya. Perbaikan dilakukan pada bagian tata letak tampi-
lan, tampilan tabel, pembaruan pada penggunaan navigasi,
penambahan ﬁtur pencarian, dan penggunaan warna diper-
barui. Hal ini sesuai dengan hasil pada analisis pengguna
pada iterasi sebelumnya, seperti perubahan warna. Perubahan
warna dilakukan karena hasil analisis pengguna mengatakan
jika penggunaan warna pada iterasi sebelumnya nyentrik.
Yang selanjutnya warna tema pada iterasi kedua akan diganti
menjadi warna putih.

TABEL IV
HASIL ITERASI-1

No
1

2

3

4

5

6

Tampilan
Beranda

Jadwal

Perkuliahan

Proﬁl

OrangTua

Keseluruhan

Setuju
11

Tidak Setuju
4

12

10

14

14

14

3

5

1

1

1

Dari

tabel

IV dapat dilihat evaluator sebagian besar
menyetujui hasil dari design solution yang pertama ini. secara
keseluruhan 14 dari 15 evaluator menyutujui design solution
berdasarkan tata letak,
tampilan responsif dan pembaruan
tampilan. Evaluasi Iterasi-I ini juga melakukan menanyakan
mengenai saran perbaikan. Saran perbaikan tersebut antara lain
:

• Penambahan hover pada navigasi yang dimiliki,
• Memperbaiki penggunaan warna,
• Penyederhanaan tampilan tabel,
• Membuat halaman khusus pada ﬁtur cari dosen,
• Membuat identitas halaman
Hasil evaluasi dan saran perbaikan ini akan menjadi dasar

perbaikan pada iterasi selanjutnya.

C. Iterasi II - Prototype II

1) Analisis Fungsional
Hasil uji coba pada iterasi – I menunjukkan bahwa hal-
hal yang perlu diperhatikan adalah Terdapat ﬁtur yang tidak
dipahami pengguna, pemindahan ﬁtur cari dosen, penamba-
han breadcrumbs dan membuat tata letak lebih stabil lagi.
Setelah ditelusuri ternyata penggunaan navbar membuat peng-
guna mengalami kesulitan memahami ﬁtur yang ada. Banyak
pengguna juga mengalami kesulitan untuk mengetahui sedang
berada dimana pengguna sekarang. Hal itu dikarenakan tidak
adanya identitas halaman yang jelas. Tampilan konten dengan

Gambar 8. Tampilan Beranda

Gambar 8 menunjukkan perbaikan untuk menghilangkan
navbar. Yang mana ﬁtur itu dipindahkan pada sidebar menjadi
sub menu, sehingga sidebar terdiri dari menu dan sub menu.
Hal ini membuat pengguna dapat lebih memahami bagaimana
penggunaan ﬁtur untuk mengganti halaman, dikarenakan peng-
guna SIPADU Mahasiswa yang sudah biasa menggunakan
menu sidebar. Navbar tidak sepenuhnya dihilangkan, hal itu
dimanfaatkan pada ﬁtur lainnya, sehingga tidak menjadi basis
menu lagi. Bahkan, ada penambahan ﬁtur pada navbar, yaitu
adanya tombol untuk melakukan pencarian. Yang mana hal ini
disesuaikan berdasarkan indikator yang rendah pada evaluasi
heuristik pada survei pendahuluan sebelumnya.

3) Uji Coba
Setelah proses desain selesai, iterasi-II dilakukan uji coba
dengan evaluator yang sama seperti dengan iterasi-I dan yang
mengisi QUIS. Untuk melakukan penilaian pada design so-
lution ini dilakukan dengan mengisi kuesioner QUIS untuk
melihat bagaimana kepuasan pengguna dengan desain tersebut.
Hasil QUIS dapat dilihat pada tabel V

Berdasarkan hasil tabel V didapatkan satu indikator yang
masih memiliki skor dibawah delapan. Indikator tersebut

7 / 9

TABEL V
HASIL QUIS ITERASI-II

ketika dilakukan refresh halaman karena kesalahan algoritma
yang digunakan.

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

2) Desain

Tahap ini adalah tahap perancangan model antarmuka yang
lebih lengkap. Rancangan dibuat untuk sebagian besar hala-
man. Tujuan pembuatan model ini adalah memperbaiki model
yang sudah dibuat sebelumnya. Juga ditambahkan halaman-
halaman baru, seperti halaman login.

adalah indikator ”Sistem mempertimbangkan pengguna baru”
yang masuk dalam kategori Kinerja Sistem. Secara rata-rata
juga skor dari hasil QUIS bernilai 8,7. Dan juga adanya
penambahan pertanyaan pada kuesioner tersebut untuk menge-
tahui apa saja yang perlu diperbaiki untuk iterasi selanjutnya.
Saran perbaikan tersebut adalah

• Memperbaiki ﬁtur darkmode,
• Menambah ﬁtur penunjuk error, dan
• Menambah pop up konﬁrmasi

D. Iterasi III - Prototype III

1) Analisis Fungsional
Analisis ini menggabungkan hasil uji coba pada iterasi
II, iterasi I, dan survei pendahuluan yang sudah dilakukan
sebelumnya. Hasil uji coba pada iterasi
II menyebutkan
bahwa pengguna sudah merasa puas dengan design solution
yang dibangun. Hasil QUIS juga menunjukkan hasil yang
memuaskan setiap indikatornya. Hanya memiliki satu indika-
tor yang bernilai dibawah delapan, yaitu pertimbangan untuk
pengguna awam. Hal yang dirasa harus diperbaiki adalah
adanya bug pada beberapa ﬁtur, seperti ﬁtur dark mode dan
pesan error.

Pertimbangan pengguna awam memiliki nilai yang dirasa
belum terlalu baik dikarenakan belum adanya pesan-pesan
error bila terjadi kepada kesalahan pengguna. Terjadinya bug
pada beberapa ﬁtur terjadi karena kesalahan pemrograman
yang ditulis. Seperti pada dark mode yang tidak status dark

Gambar 9. Halaman Cari Skripsi pada Penyusunan Skripsi

Gambar 9 menunjukkan tampilan penyusunan skripsi pada
design solution. Dari gambar 9, perbaikan pada bug su-
dah berhasil dilakukan. Fitur dark mode bisa digunakan se-
mestinya. Yang mana sebelumnya, darkmode akan aktif jika
diaktifkan terlebih dahulu. Sekarang sudah dapat menyimpan
statusnya setiap berganti halaman. Bug pada ﬁtur pencarian
juga berhasil diperbaiki dengan bisa menampilkan hasil pen-
carian. Yang mana pada sebelumnya pencarian tidak dapat
ditampilkan ketika tombol cari ditekan.

3) Uji Coba

Uji coba dilakukan dengan responden sebanyak dua puluh
orang. Yang mana terdiri dari
lima belas responden yang
selama ini menjadi evaluator dan mengikuti rangkaian survei
dan lima orang yang tidak mengikuti rangkaian tersebut.
Penilaian pada iterasi ketiga ini dengan melakukan evaluasi
heuristik dan menambahkan pertanyaan mengenai ketersediaan
menggunakan antarmuka ini pada SIPADU Mahasiswa. Hasil
evaluasi heuristik menunjukkan sudah memenuhi seluruh ni-
lai penuh dari setiap indikator yang ada. Dan juga 61,54%
mengatakan antarmuka baru memiliki penilaian bagus dan
38,46%nya sangat bagus.

Selain melakukan evaluasi heuristik, pengguna juga mem-
berikan tanggapan mengenai apakah bersedia menggunakan
desain yang telah dirancang pada model ini. Hampir seluruh
pengguna mengatakan bahwa perubahan antarmuka desain
sudah lebih menarik dan sederhana serta baik untuk diterapkan
pada sistem yang sekarang. Hal yang paling banyak ditanggapi
positif oleh pengguna adalah pada ﬁtur melihat kehadiran
setiap mata kuliahnya.

8 / 9

E. Perbandingan Antarmuka Lama dan Baru

VII. PENUTUP

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Hal yang dapat disimpulkan pada penelitian ini adalah

sebagai berikut.

1) Perancangan kembali antarmuka web portal SIPADU
Mahasiswa berhasil dilakukan dengan mendapatkan an-
tarmuka baru sesuai dengan kepuasan pengguna yang
berbentuk.

2) Evaluasi berhasil dilakukan terhadap antarmuka baru
dan menunjukkan bahwa antarmuka baru memiliki pe-
nilaian lebih baik dari antarmuka lama.

Hal yang dapat disarankan pada penelitian ini adalah seba-

gai berikut.

1) Pada penelitian selanjutnya dapat dikembangkan lagi
dengan melakukan perancangan kembali dengan meng-
gunakan probability sampling yang memiliki nilai bias
lebih kecil dari metode pengambilan sampel yang digu-
nakan pada penelitian ini.

2) Pembangunan prototype yang dapat dikonversikan ke
berbagai pengembang pemrograman manapun, agar
hasil rancangan dapat diterapkan secara langsung.

DAFTAR PUSTAKA

[1] E. T. Widodo, Pengembangan Service-Oriented ARCHITECTURE (SOA)

SIPADU-STIS. Sekolah Tinggi Ilmu Statistik: Skripsi, 2012.

[2] J. Nielsen. (2004, 9) The need for web design standards. [Online].
https://www.nngroup.com/articles/the-need-for-web-design-

Available:
standards/

[3] L. R. Maghﬁroh, “Observation and heuristics evaluation of student web-
based application of sipadu-stis,” in Journal of Physics: Conference
Series, vol. 1511, no. 1.

IOP Publishing, 2020, p. 012019.

[4] N. Turner. (2011, 2) A guide to carrying out usability reviews. [Online].

Available: http://www.uxforthemasses.com/usability-reviews/

[5] K. Moumane, A. Idri, and A. Abran, “Usability evaluation of mobile
applications using iso 9241 and iso 25062 standards,” SpringerPlus,
vol. 5, no. 1, pp. 1–15, 2016.

[6] C. Abras, D. Maloney-Krichmar, J. Preece et al., “User-centered de-
sign,” Bainbridge, W. Encyclopedia of Human-Computer Interaction.
Thousand Oaks: Sage Publications, vol. 37, no. 4, pp. 445–456, 2004.
[7] A. Al Ghiffari, E. Darwiyanto, and D. Junaedi, “Perancangan ulang user
interface website politeknik kesehatan makassar menggunakan metode
user-centered design,” eProceedings of Engineering, vol. 6, no. 1, 2019.
[8] A. B. M. Simamora, Perancangan Kembali Antarmuka Pengguna Sistem
Pegawai BPS dengan Metode User Centered Design. Sekolah Tinggi
Ilmu Statistik: Skripsi, 2020.

[9] M. Kuncoro, “Metode riset untuk bisnis dan ekonomi,” Jakarta: Er-

langga, vol. 52, 2003.

[10] J. Nielsen, Usability engineering. Morgan Kaufmann, 1994.
[11] H. Kuswanto, “Analisis prinsip layout and composition pada web design
perusahaan pt. bank rakyat indonesia, tbk dan pt. ﬁf group berdasarkan
buku “the principle of beautifull website design by jason beaird”,” Elinvo
(Electronics, Informatics, and Vocational Education), vol. 2, no. 1, pp.
1–7, 2017.

Perbandingan dilakukan dengan membandingkan hasil eval-
uasi heuristik dan QUIS pada survei pendahuluan dengan hasil
evaluasi akhir. Perbandingan hasil evaluasi heuristik kedua
antarmuka dapat dilihat pada gambar 10

Gambar 10. Graﬁk Perbandingan Hasil Evaluasi Heuristik Antarmuka Lama
dan Baru

Gambar 10 menunjukkan bahwa hasil evaluasi heuristik
antarmuka lama mengalami peningkatan pada hasil evaluasi
heuristik antarmuka baru. Jumlah indikator yang mengalami
peningkatan ada 29 Indikator. Selain 29 indikator yang men-
galami peningkatan, enam belas indikator lainnya memiliki
persentase tetap. Oleh karena itu, dapat dikatakan indikator-
indikator tersebut telah meningkatkan pengaruhnya pada sis-
tem secara keseluruhan.

Sementara itu perbandingan hasil QUIS antarmuka lama dan

baru dapat dilihat pada gambar 11

Gambar 11. Graﬁk Perbandingan Hasil QUIS Antarmuka Lama dan Baru

Berdasarkan gambar 11 dapat dilihat bahwa hasil QUIS
rancangan antarmuka baru mengalami peningkatan pada se-
mua indikator. Perubahan skor tertinggi terjadi pada indika-
tor ”kekacauan sistem dan visual noice”. Sedangkan yang
mengalami perubahan skor terendah adalah indikator ”aplikasi
memberikan informasi tentang yang sedang dilakukan”. Hasil
perbandingan evaluasi heuristik dan QUIS antarmuka lama
dan baru menunjukkan bahwa rancangan antarmuka baru lebih
baik dari antarmuka lama.

9 / 9

"
221709590,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pengembangan Sistem Perpustakaan Digital Berbasis 
Web dengan Quick Response Code di Perpustakaan 
Pusdiklat BPS 

Baihaqi Ilham Syah (221709590, 4SI1) 

Dosen Pembimbing : Yunarso Anang, Ph.D. 

Ringkasan— Perkembangan teknologi informasi yang semakin 
modern telah memberi banyak manfaat bagi berbagai pihak, tak 
terkecuali  Pusdiklat  BPS.  Pusdiklat  BPS  terus  meningkatkan 
pelayanannya  dalam  pendidikan  dan  pelatihan,  salah  satunya 
melalui  digitalisasi  perpustakaan  dan  penyediaan  akses  koleksi 
digital  seperti  e-book.  Pengembangan  web  perpustakaan  digital 
(digital library) ini merupakan bentuk peningkatan efektivitas dan 
efisiensi  pengelolaan  dan  pelayanan  perpustakaan  sehingga 
seluruh informasi mudah dikelola dengan baik. Selain itu, bahan 
pustaka  seperti  karya-karya  diklat  dapat  diakses  dalam  bentuk 
digital.  Penelitian  ini  memanfaatkan  Quick  Response  Code  (QR 
Code)  yang  dimaksudkan  untuk  memudahkan  akses,  pencarian 
perpustakaan. 
dan 
Pengembangan  sistem  pada  penelitian  ini  menggunakan  System 
Development Life Cycle (SDLC) dengan model Waterfall. Melalui 
penelitian  ini  pula  diharapkan  petugas  dapat  dengan  mudah 
mengelola  data  buku  di  perpustakaan  dan  peserta  diklat  dapat 
mengakses karya-karya diklat secara daring 

penambahan 

informasi 

buku 

di 

Kata Kunci— digital library, e-book, QR Code. 

I.  LATAR BELAKANG 

Pusat  Pendidikan  dan  Pelatihan  (Pusdiklat)  Badan  Pusat 
Statistik  (BPS)  merupakan  unsur  pelaksana  BPS  di  bidang 
pendidikan  dan  pelatihan  yang  bertanggung  jawab  langsung 
kepada  Kepala  BPS.  Dalam menjalankan  tugasnya,  Pusdiklat 
menyediakan perpustakaan yang bisa dikunjungi peserta diklat 
untuk mencari dan membaca bahan  pustaka yang disediakan. 
Menurut  amanat  Peraturan  Kepala  Badan  Pusat  Statistik 
Nomor  116  Tahun  2014,  penyelenggaraan  pendidikan, 
pelatihan,  dan  pengembangan  SDM  di  pusdiklat  perlu 
memerhatikan pemenuhan sarana dan prasarana kerja yg layak 
termasuk  Teknologi  Informasi  dan  Komunikasi  (TIK)  [1]. 
Sistem yang berjalan di perpustakaan Pusdiklat saat ini secara 
keseluruhan  masih  seperti  perpustakaan  konvensional  pada 
umumnya,  seperti  penyediaan  koleksi  bahan  pustaka  dengan 
buku tercetak  dan pengunjung dapat melakukan peminjaman 
pada beberapa jenis bahan pustaka. Dari sisi pengelolaan, data 
bahan  pustaka  dikelola  melalui  berkas  di  aplikasi  Microsoft 
Excel,  sedangkan  pencatatan  tamu  dan  peminjaman  oleh 
pengunjung di buku. Perekapan peminjaman secara manual di 
buku yang sulit dipantau sering kali mengakibatkan buku yang 
dipinjam tertinggal di kamar maupun terbawa oleh peminjam. 
Oleh  karena  itu,  perpustakaan  pusdiklat  terus  berusaha 
meningkatkan 
dengan 
menyediakan  akses  bahan  bacaan  digital  untuk  karya  tulis 
alumni  peserta-peserta  diklat  sebagai  referensi.  Widiasa 

pelayanannya, 

satunya 

salah 

menyebutkan  tugas  pokok  perpustakaan  adalah  menghimpun 
bahan pustaka yang meliputi buku dan nonbuku sebagai sumber 
informasi,    mengolah  dan  merawat  bahan  pustaka,  dan 
memberikan layanan bahan pustaka [2]. 

Perpustakaan  di  Pusdiklat  BPS  saat  ini  masih  berorientasi 
pada  penyediaan  bahan  pustaka  dalam  bentuk  fisik  seperti 
dokumen  atau  buku 
Ibrahim, 
tercetak.  Menurut  Ali 
teknologi 
pengembangan  model  perpustakaan  berbasis 
informasi  secara  digital  bukan  berarti  buku  atau  dokumen 
tercetak harus ditinggalkan, melainkan terdapat alternatif untuk 
pemanfaatan  dokumen  tersebut,  yaitu  dengan  pengemasan 
informasi  produk  tercetak menjadi  produk  informasi berbasis 
teknologi digital. Pengemasan informasi adalah kegiatan yang 
dimulai  dengan  menyeleksi  berbagai  informasi  dari  sumber 
yang  berbeda,  mendata  informasi  yang  relevan,  menganalisis 
dan  menyajikan  informasi  yang  sesuai  dengan  kebutuhan 
pengguna [3]. Penggunaan e-book sebagai alternatif dari buku 
cetak seringkali dilakukan karena mudah dan praktis. Sebuah 
penelitian  yang  mengkaji 
tentang  persepsi  dan  pola 
penggunaan  e-book  pada  mahasiswa  ilmu  perpustakaan  dan 
informasi di Nigeria menunjukkan bahwa responden memiliki 
persepsi  yang  positif  terhadap  penggunaan  e-book    karena 
meningkatkan  efektifitas  belajar,  membantu  menyelesaikan 
tugas-tugas perkuliahan, dan tampilan ilustrasi grafis lebih jelas 
[4]. 

Kemajuan teknologi informasi yang pesat membawa banyak 
manfaat  dan  perubahan  pada  berbagai  aspek  kehidupan 
manusia  salah  satunya  adalah  perpustakaan  digital  yang 
memanfaatkan QR Code. Menurut Diana, perpustakaan digital 
adalah  suatu  perpustakaan  dimana  seluruh  isi  koleksi  dan 
proses  pengelolaan,  serta  layanannya  berupa  kumpulan  data 
dalam  bentuk  digital  [5].  Sedangkan  Susanto  mengatakan 
bahwa  perpustakaan  digital  merupakan  sebuah  sistem 
perpustakaan  yang  menggunakan  media  elektronik  dalam 
menyampaikan  informasi  dari  sumber  yang  dimiliki  dan 
menggabungkan  koleksi-koleksi,  layanan,  dan  sumber  daya 
manusia untuk mendukung penuh sikluspenciptaan, diseminasi, 
serta 
pemanfaatan  dan  penyimpanan  data 
pengetahuan dalam format digital yang telah dievaluasi, diatur, 
diarsip dan disimpan, melalui komputer stand alone, intranet, 
atau internet [6]. Selain itu, dengan memanfaatkan smartphone 
atau kamera yang terhubung dengan sistem perpustakaan, QR 
Code  dapat  memberikan  informasi  dengan  mudah  mengenai 
koleksi perpustakaan, situs web perpustakaan, pengunjung, dan 
petunjuk  tentang  perpustakaan.  Adanya  sistem  pengelolaan 

informasi, 

 1 / 8 

 
 
 
 
 
perpustakaan dengan QR Code ini sangat memungkinkan untuk 
dibuat  suatu  sistem  agar  memudahkan  pekerjaan  yang 
berhubungan dengan identifikasi barang maupun input barang 
sehingga setiap barang dapat terstruktur dengan rapi [7]. 

Sistem  yang  menunjang  penyediaan  bahan  bacaan  secara 
digital juga sangat diperlukan akibat adanya pandemi COVID-
19  yang  mengubah  pemebelajaran  pada  diklat  yang  semula 
berlangsung  secara  tatap  muka  menjadi  pembelajaran  jarak 
jauh (distance learning). Sebuah penelitian juga menyebutkan 
bahwa  pandemi  COVID-19  akan  membuat  perpustakaan 
digital  banyak  diminati  karena  menyediakan  konten-konten 
elektronik  yang  bisa  diakses  secara  online  dan  kedepannya 
penggunaan layanan ini akan terus meningkat [8]. Oleh karena 
itu,  pembangunan  sistem  perpustakaan  digital  berbasis  web 
dengan QR Code ini diharapkan dapat membuat peserta diklat 
semakin mudah dalam mengakses dan membaca bahan bacaan 
yang diinginkan secara digital.  

II.  TUJUAN PENELITIAN 

Berdasarkan rumusan masalah, tujuan umum penelitian ini 
adalah meningkatkan efisiensi dan efektifitas pengelolaan dan 
pelayanan  perpustakaan  pusdiklat  BPS.  Tujuan  khusus 
penelitian  ini  yaitu  merancang  dan  mengembangkan  sistem 
perpustakaan digital berbasis web dengan QR Code. 

III. PENELITIAN TERKAIT 

1)  Manajemen  Perpustakaan  Sekolah  dari 

Jurnal 
Perpustakaan  Sekolah,  Tahun  2007  oleh  I  Ketut 
Widiasa 

Penelitian  pertama  menjelaskan  bahwa 
tugas  pokok 
perpustakaan,  yaitu  menghimpun  bahan  pustaka  yang 
meliputi  buku  dan  nonbuku  sebagai  sumber  informasi, 
mengolah  dan  merawat  bahan  pustaka,  dan  memberikan 
layanan  bahan  pustakaan.  Dalam  menjalankan 
tugas 
pokoknya 
tersebut,  perpustakaan  perlu  memanfaatkan 
teknologi  informasi  untuk  meningkatkan  efiesiensi  dan 
efektifitas pelayanan dan pengelolaannya. 

2)  Pengembangan Model Perpustakaan Berbasis 

Teknologi Informasi untuk Meningkatkan Kinerja 
Layanan Perpustakaan dan Mewujudkan Perpustakaan 
Ideal Berbasis Digital Di Fasilkom Unsri, dari 
Prosiding KNTIA Tahun  2011 oleh  Ali Ibrahim 
Penelitian  ini  mengemukakan  bahwa  pengemasan  produk 
berbasis teknologi pada perpustakaan bisa dilakukan dengan 
isi  dari 
membangun  sebuah  digital 
perpustakaan  digital  berada  dalam  suatu  komputer  server 
yang bisa ditempatkan secara lokal, maupun di lokasi yang 
jauh,  namun  dapat  diakses  dengan  cepat  dan  mudah  lewat 
jaringan  komputer  sehingga  sehingga  prosesnya  menjadi 
lebih efisien, dan efektif. 

library,  dimana 

3)  Perception and usage pattern of e-books among library 

and information science students in selected 
universities in Nigeria, dari Journal of Library & 
Information Technology Tahun  2018 oleh  Adeyinka 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Penelitian  ini  menunjukkan  bahwa  responden  memiliki 
persepsi  yang  positif  terhadap  penggunaan  e-book    karena 
meningkatkan efektifitas belajar, membantu menyelesaikan 
tugas-tugas  perkuliahan, dan tampilan  ilustrasi  grafis  lebih 
jelas 

4)  QR Code in Library Practice Some Examples dari 
International Journal of Engineering Sciences & 
Research Technology Tahun  2017 oleh  A.S. Mishra 

Penelitian  ini  menjelaskan 
  bahwa  QR  Code  adalah 
teknologi  inovatif  untuk  perpustakaan  yang  membantu 
mengubah informasii ke dalam bentuk piksel persegi hitam 
dan  putih.  Setelah  itu  dapat  dipindai  melalui  gawai  atau 
perangkat  serupa  sehingga  penggunaan  QR  Code  sangat 
membantu  dalam  mempercepat  akses  terhadap  koleksi 
digital  beserta  isinya.  penelitian  ini  juga  menganalisis 
struktur kode QR dan menjelaskan cara kerjanya. 

5)  Desain dan Standar Perpustakaan Digital, dari Jurnal 

Pustakawan Indonesia Tahun  2010 oleh  Setyo Edi 
Penelitian ini menjabarkan perpustakaan digital secara lebih 
luas. Seperti pemanfaatannya, masalah hak cipta, desain, dan 
standar  yang  perlu  dipertimbangkan  dalam  pengembangan 
sistemnya. 

6)  COVID-19 and digital library services – a case study 

of a university library, dari Digital Library 
Perspectives. Tahun  2020 oleh  D. Mehta 

Digital  library  dapat  membantu  penyediaan  bahan  pustaka 
secara online, sebagai alternatif mengunjungi perpustakaan 
dikala pandemi maupun setelah pandemi berakhir. 

IV. METODE PENELITIAN  
Metodologi  yang  digunakan  dalam  pengembangan  sistem 
perpustakaan digital berbasis web dengan QR Code ini adalah 
System Development Life Cycle (SDLC) model waterfall yang 
terdiri dari beberapa tahapan sebagai berikut :  

1. Perencanaan 
Kegiatan  penelitian  diawali  dengan  pengumpulan  fakta-
fakta  di  lapangan  dengan  cara  observasi  dan  wawancara 
langsung  dengan  pemangku  kepentingan,  termasuk  kepada 
petugas  perpustakaan.  Hal  tersebut  guna  mendapat  detail 
informasi mengenai masalah, batasan cakupan, ruang lingkup 
serta  tujuan  akhir  dari  sistem  yang  akan  dibuat.  Kemudian 
melakukan  studi  pustaka  dengan  mencari  informasi  terkait 
dengan penelitian yang sudah ada sebelumnya. 

2. Analisis 
Berdasar dari informasi dan data yang diperoleh pada tahap 
sebelumnya, analisis dilakukan terhadap sistem yang ada saat 
ini  untuk  mendeskripsikan  permasalahan  dari  sistem  berjalan 
melalui  diagram  fishbone.  Penentuan  solusi  menggunakan 
metode analisis PIECES yang terdiri dari 6 indikator penilaian 
yaitu Performance, Information, Economy, Control, Eficiency, 
dan 
untuk 
mengidentifikasi dan memecahkan permasalahan yang terjadi 

Service.  Analisis 

ini  merupakan 

cara 

 2 / 8 

 
 
 
 
 
 
 
 
suatu 

pada 
ini  akan   
sistem  berjalan.  Dari  analisis 
menghasilkan  identifikasi  masalah  utama  dari  suatu  sistem   
serta  memberikan 
rekomendasi dan perbaikan melalui sistem usulan [9]. 

solusi  untuk  kemudian  dilakukan 

3. Desain 

Perancangan  dengan  menggunakan  diagram  Use  Case  dan 
Bussiness Process guna mendeskripsikan kondisi sistem yang 
berjalan  dan  sistem  usulan.  Tiap-tiap  Use  case  nya  akan 
digambarkan  melalui  activity  diagram  sehingga  terlihat  alur 
process kegiatan yang terdapat dalam aplikasi. 

4. Implementasi 
informasi 

Sistem 
ini  dikembangkan  pada  Framework 
CodeIgniter4 (CI4) dengan bahasa pemrograman PHP. Untuk 
tampilannya  menggunakan  template  Open  Source  yaitu  SB 
Admin  2  dan  Bootstrap  4.  Sedangkan  database  sistem 
menggunakan  MySQL  yang  dikelola  melalui  phpMyAdmin 
dan didesain dengan A5:SQL Mk-2. 

5. Uji Coba dan Evaluasi 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Di  masa  pandemi,  seluruh  kegiatan  masyarakat  dibatasi. 
Kebijakan  Pembatasan  Sosial  Berskala  Besar 
(PSBB) 
diterapkan  sebagai  bentuk  pencegahan  penyebaran  virus 
Corona.  Kebijakan 
ini  yang  kemudian  menjadi  dasar 
pelaksanaan  belajar  dari  rumah  secara  daring  dengan 
memanfaatan  teknologi  informasi  yang  ada.  Pusdiklat  BPS 
yang  merupakan  unsur  pelaksana  BPS  di  bidang  pendidikan 
dan pelatihan pun juga menerapkan metode Pembelajaran Jarak 
Jauh  (PJJ).  Karya-karya  diklat  yang  umumnya  membahas 
permasalahan selama bekerja di kantor BPS seringkali menjadi 
bahan  referensi  peserta  diklat  lain  yang  menemukan  masalah 
terkait. Peserta diklat yang biasa mengakses langsung referensi 
tersebut  di  perpustakaan  kini  tidak  dapat  dilakukan  karena 
belum 
layanan  akses  digital  pada  koleksi 
perpustakan. Hal tersebut yang menjadi awal penulusuran dan 
penggalian  informasi  lebih  lanjut  mengenai  perpustakaan  di 
Pusdiklat BPS. 

tersedianya 

Terakhir akan dilakukan uji coba untuk melihat seberapa jauh 
tujuan  pembuatan  sistem  telah  tercapai  dalam  mengatasi 
permasalahan yang ada pada sistem berjalan. Pengujian dengan 
blackbox testing digunakan untuk melihat keberhasilan fungsi 
mengeluarkan  output  sesuai  dengan  yang diinginkan.  Setelah 
itu, pengguna dan petugas akan mencoba menggunakan sistem 
secara langsung dan diarahkan untuk mengisi kuesioner terkait 
System  Usability  Scale  (SUS).  Selanjutnya,  pengguna  dan 
petugas  memberi 
terkait 
peningkatan  efisiensi  dan  efektifitas  pada  pelayanan  dan 
pengelolaan  di  perpustakaan  Pusdiklat  BPS.  Tanggapan 
pengguna dan petugas terhadap sistem penting untuk diketahui 
karena menurut [10] efisiensi dan efektifitas sistem dipengaruhi 
oleh kepuasaan pengguna. 

tanggapan 

terhadap 

sistem 

V.  KERANGKA PIKIR 

Gambar 2. Proses bisnis sistem berjalan di perpustakaan Pusdiklat BPS 

Selain  itu,  ditemukan  juga  masalah  lain  yaitu  mengenai 
proses bisnis dan pengelolaan perpustakaan yang kurang efektif 
dan  efisien  seperti  data-data  buku  yang  terpisah  di  beberapa 
berkas, data yang tidak up to date, informasi buku yang tidak 
lengkap  seperti kode  buku  atau  kode  rak  yang kosong  sering 
menyebabkan  buku  sulit  dicari  maupun  dikembalikan  ke 
tempat  semula.  Pencatatan  peminjaman  yang  masih  di  buku 

Gambar 1. Peta kerangka pikir 

 3 / 8 

 
 
 
      
sulit untuk dilakukan pengecekkan dan buku yang sudah lama 
dipinjam sering terlupakan sehingga menyebabkan buku hilang, 
terbawa peminjam, atau tertinggal di kamar. Oleh karenanya, 
Pusdiklat  mendapat  masukkan  tentang  pengembangan  sistem 
perpustakaan  digital  berbasis  web  yang  multi-platform  dan 
dengan  menerapkan  QR  Code  pada  aplikasinya.  Dengan 
demikian,  diharapkan  efisiensi  dan  efektifitas  pengelolaan 
perpustakaan digital dapat ditingkatkan dan dapat menyediakan 
akses layanan terhadap koleksi perpustakaan secara digital. 

VI. HASIL DAN PEMBAHASAN 

A. Analisis Sistem Berjalan dan Masalah 

Pengidentifikasian  sistem  berjalan  serta  masalah  dan 
kebutuhan  dari  sistem  perpustakaan  telah  dilakukan  melalui 
wawancara  secara  daring  kepada  pegawai  TI  Pusdiklat  terkait 
masukan  pengembangan  sistem  perpustakaan  digital  dan 
wawancara  langsung  dengan  penanggung  jawab  perpustakaan. 
Diperoleh  informasi  bahwa  proses  bisnis  yang  berjalan  di 
perpustakaan Pusdiklat BPS saat ini tidak jauh berbeda dengan 
perpustakaan konvensional pada umumnya. 

Adapun  permasalahan  yang  ditemukan  yaitu  pengelolaan 
perpustakaan  Pusdiklat  BPS  yang  kurang  efektif  dan  efisien, 
seperti  pemantauan  peminjaman  yang  sulit  karena  tercatat  di 
buku  tulis,  data-data  koleksi  perpustakaan  yang  sulit  dicari 
karena tidak terorganisir dalam satu berkas, informasi buku yang 
tidak  lengkap  dan  sistem  web  perpustakaan  terdapat  banyak 
ketidaksesuaian fungsi dan tampilan.  

Selain itu, terdapat masalah pada proses bisnis yang berjalan 
saat ini. Pada proses peminjaman tidak didapati informasi status 
ketersediaan  bahan  pustaka  untuk  dipinjam,  seperti  beberapa 
bahan pustaka yang telah dipinjam oleh orang lain, pemantauan 
data transaksi peminjaman dan pengembalian yang sulit karena 
dicatat melalui buku juga seringkali menyebabkan bahan pustaka 
hilang,  terbawa,  atau  tertinggal  di  asrama.  Tidak  tersedianya 
akses  bahan  pustaka  secara  digital  membuat  peserta  diklat 
seringkali membaca bahan pustaka di perpustakaan hingga larut 
malam,  hal  tersebut  tentu  membuat  petugas  perpustakaan 
kewalahan  dan  mengizinkan  pengunjung  meminjam  bahan 
pustaka yang statusnya tidak boleh dipinjam. 

Berbagai  permasalahan  yang 

telah  dijelaskan  diatas  

digambarkan pada fishbone diagram berikut. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

B. Analisis Kebutuhan 

Penelitian  ini  menggunakan  analisis  PIECES  yang  dapat 
mengidentifikasi  kebutuhan  dari  sistem  yang  berjalan  dan 
menganalisis solusi dari tiap-tiap aspeknya untuk sistem usulan 
yang akan dibuat.  

TABEL I 
TABEL ANALISIS PIECES 
Sistem Berjalan 

No 

Aspek 

1 

Perfomance  Pengelolaan 

dan 
bahan 
yang 

pencarian 
pustaka 
konvensional 
memakan waktu yang 
cukup lama. 
Informasi status 
ketersediaan bahan 
pustaka tidak ada 
Penyediaan 
komputer dan rak 
tambahan untuk 
pengunjung akan 
menambah biaya 

Data 
peminjaman 
yang tercatat di buku 
sulit  dikontrol  dan 
diawasi  

di 

secara 
buku 

Perekapan 
seluruh 
aktivitas  pengunjung 
perpustakaan 
dilakukan 
manual 
sehingga 
membutuhkan  waktu 
yang lama 
Layanan 
bahan 
pustaka hanya berupa 
karya 
cetak  yang 
jumlahnya terbatas. 

Solusi 
Pengelolaan dan pencarian 
bahan pustaka dilakukan  
melalui sistem 

Informasi status 
ketersediaan dan dapat 
dilihat melalui sistem 
Perangkat pribadi yang 
dimiliki pengunjung dapat 
dimanfaatkan dalam 
aktifitas di perpustakaan 
sehingga menghemat biaya 
operasional 
Pencatatan dan penyediaan 
informasi peminjaman 
dapat disaring melalui 
sistem 
Perekapan aktivitas 
pengunjung perpustakaan 
dilakukan oleh sistem 

Penyediaan akses bahan 
pustaka digital yang lebih 
lengkap melalui website. 

2 

Information 

3  Economy 

4  Control 

5  Efficiency 

6  Service 

C. Rancangan Sistem Usulan 

Berdasarkan  kebutuhan  untuk  mengatasi  permasalahan  yang 
telah  dianalisis  pada  bagian  sebelumnya,  maka  dilakukan 
pengembangan  web  perpustakaan  yang  mudah  diakses  dari 
berbagai jenis perangkat petugas dan pengunjung perpustakaan. 
Penerapan  QR  Code  digunakan  untuk  mempercepat  proses 
pengidentifikasian  informasi  buku.    Sistem  yang  diusulkan 
dalam  penelitian  ini  menggunakan  pendekatan  object-oriented 
dengan permodelan sistem usulan yang didesain menggunakan 
Unified Modeling Language (UML). Proses bisnis dan use case 
dari  sistem  yang  diusulkan  pada  perpustakaan  Pusdiklat  BPS 
dapat dilihat pada gambar berikut. 

Gambar 3. Fishbone diagram 

 4 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Urairan dari diagram use case sistem usulan dapat dilihat 

pada tabel di berikut ini. 

TABEL II 
TABEL PENJELASAN DARI DIAGRAM USE CASE SISTEM USULAN  

No. 

Use Case 

Deskripsi 

1 

2 

3 

4 

5 

6 

7 

8 

9 

Login 

Admin dan pengguna masuk ke dalam 
sistem untuk mengakses fitur sistem lebih 
lanjut. 

Mengelola data 
bahan pustaka 

Admin dapat menambah, mengubah, dan 
menghapus data informasi  bahan pustaka 

Mengelola data 
pengguna 

Admin dapat mengunggah, menambah, 
mengubah, dan menghapus data 
informasi pengguna 

Mengelola 
peminjaman bahan  
pustaka 

Admin dapat melihat daftar peminjaman 
dan mengubah status peminjaman bahan 
pustaka pengguna. 

Melihat data 
pengunjung di 
buku tamu 

Admin dapat melihat daftar seluruh 
pengunjung perpustakaan yang berisikan 
identitas dan tujuan pengunjung 

Membuat 
pengumuman 

Admin  dapat  menambah,  mengubah, 
atau menghapus informasi pengumuman 
yang  akan  ditampilkan  di  halaman 
Beranda pengguna 

Mengelola data rak 
dan label 

Admin dapat menambah, mengubah, dan 
menghapus data rak dan keterangan 
labelnya 

Berinteraksi 
dengan bahan 
pustaka 

Pengguna dapat melihat detil bahan 
pustaka, membaca bahan pustaka, dan 
menyimpan bahan pustaka yang akan 
ditampilkan di halaman Favorit pengguna 

Melihat katalog 
buku perpustakaan 

Pengunjung dapat melihat daftar buku 
yang tersedia di perpustakaan 

10 

Mengisi buku tamu 

Pengunjung dapat melakukan absensi 
perpustakaan melalui halaman Buku Tamu 

11 

12 

13 

Melakukan 
peminjaman bahan 
pustaka 

Pengguna dapat mengisi form 
peminjaman bahan pustaka tercetak 
melalui scan QR Code 

Melihat riwayat 
peminjaman 

Pengguna dapat melihat daftar 
peminjaman yang pernah dilakukan. 

Mengelola bahan 
pustaka yang 
disimpan 

Pengguna dapat menghapus dan 
membuka detail bahan pustaka 
tersimpan melalui halaman favorit.  

Desain  basis  data 

telah  mengalami  beberapa  kali 
penyesuaian  terhadap  kondisi  data  yang  tersedia  saat  ini, 
terutama  pada  bagian  tabel  buku  yang  cukup  banyak 
ditemukan  kekosongan  value  pada  bagian  kode  buku  dan 
beberapa kasus buku memiliki kode buku yang sama. 

 5 / 8 

Gambar 4. Proses bisnis sistem usulan perpustakaan Pusdiklat BPS 

Unit  yang  terlibat  dalam  seluruh  aktifitas  perpustakaan  di 
Pusdiklat  BPS  yaitu  pengunjung,  pegawai  perpustakaan, 
Subbagian Rumah Tangga, dan Bagian TI. Namun, pada sistem 
usulan ini hanya akan menunjang aktifitas unit pengunjung dan 
pegawai  perpustakaan  untuk  keperluan  pengelolaan  dan 
pelayanan  di  perpustakaan.  Akhirnya,  diperoleh  rancangan  use 
case dari sistem usulan sebagai berikut. 

    Gambar 5. Use Case sistem usulan perpustakaan Pusdiklat BPS 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pengunjung  perpustakaan  diwajibkan  untuk  mengisi 
formulir  buku  tamu  yang  berisikan  identitas  dan  tujuan 
berkunjung di halaman buku tamu seperti gambar 8. Informasi 
identitas pada tersebut akan terisi otomatis saat pengguna telah 
melakukan  login  dan  hanya  mengisi  tujuan  berkunjung  ke 
perpustakaan.  Pengisian  formulir  buku  tamu  ini  juga  menjadi 
syarat bagi pengguna sebelum mengajukan peminjaman buku di 
perpustakaan. 

 Gambar 8. Halaman formulir buku tamu 

Halaman  Beranda  untuk  Pengguna  berisikan  informasi 
peminjaman  buku  yang  sedang  dilakukan  serta  informasi 
pengumuman perpustakaan yang dibuat oleh Petugas.  

Gambar 9. Halaman Beranda untuk Pengguna 

Halaman  Katalog  Buku  untuk  pengguna  yang  telah  login 
memiliki  fitur 
tambahan  seperti  menyaring  daftar  buku 
berdasarkan label rak dan melihat detail buku dengan menekan 
judul buku seperti yang terlihat pada gambar 10.  

Gambar 6. ERD sistem usulan perpustakaan Pusdiklat BPS 

D. Implementasi Sistem 

Implementasi pada penelitian ini masih terus dilakukan dan 
telah membuat beberapa halaman Admin dan Pengguna seperti 
contoh berikut. 

Gambar 7. Halaman landing page 

Gambar 10. Halaman Katalog untuk Pengguna 

 6 / 8 

 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

menekan tombol “baca” untuk menampilkan isi bahan pustaka 
yang dapat dilihat pada gambar 14.  

Gambar 11. Halaman Peminjaman untuk Pengguna 

Pada halaman detail buku terdapat tombol untuk pengajuan 
peminjaman yang jika pengguna telah mengisi buku tamu, akan 
diarahkan ke  formulir peminjaman untuk mengisi durasi lama 
peminjaman  seperti  gambar  11.  Hal  tersebut  juga  dapat 
dilakukan  dengan  menekan  tombol  bersimbol  QR  Code  di 
topbar halaman pengguna untuk memindai QR Code pada label 
buku tercetak di perpustakaan. 

        Gambar 14. Tampilan baca bahan pustaka digital 

             Gambar 15. Halaman Beranda untuk Admin 

Admin  dapat  melihat  isi,  menambah,  mengubah,  dan 
menghapus  data  rak  beserta  informasi  label  keterangannya. 
Namun sebelum menghapus rak, data buku di dalamnya perlu 
dikosongkan terlebih dahulu. 

Gambar 12. Halaman detail bahan pustaka digital 

  Gambar 13. Halaman bahan pustaka digital 

Pada koleksi digital  terdapat beberapa jenis bahan pustaka 
yang yang berisikan daftar bahan pustaka digital seperti gambar 
12.  Pengguna  dapat  menekan  judul  untuk  menampilkan 
halaman detail bahan pustaka tersebut yang terlihat pada gambar 
13.  Pada  halaman  tersebut  pengguna  dapat  menyimpan  bahan 
pustaka  dengan  menekan  tombol  hijau  bersimbol  favorit  dan 

Gambar 16. Halaman Kelola Rak dan Label 

Setiap rak menampilkan jumlah buku yang terdaftar di rak 
tersebut, dan keterangan label rak untuk informasi jenis buku di 
dalamnya.  Saat  label  ditekan,  seluruh  buku  yang  ada  di  rak 
dengan  label  yang  sama  akan  di  tampilkan.  Pada  halaman 
katalog  atau  daftar  buku  tersebut  Admin  dapat  menambah, 
mengubah, dan menambah data buku seperti pada gambar 17. 

 7 / 8 

 
 
 
 
  
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

pelayanan yang berorientasi pada bahan pustaka tercetak 
membuat sebagian besar karya alumni peserta diklat tidak 
dapat  terakses  oleh  pengunjung  karena  keterbatasan 
tempat dan rak di perpustakaan pusdiklat BPS. 

2.  Pada penelitian ini telah dikembangkan web perpustakaan 
digital  dengan 
telah  disesuaikan 
fitur-fitur  yang 
berdasarkan  kebutuhan  untuk  mengatasi  masalah 
tersebut. 

3.  Implementasi  dari  sistem  yang  dikembangkan  telah 
meningkatkan  efisiensi  dan  efektivitas  pengelolaan  data 
informasi buku dan peminjaman, serta dapat menyediakan 
layanan akses bahan pustaka digital. 

B. Saran 

Saran  pengembangan  sistem  yang  mungkin  bisa 

dilakukan untuk  penelitian selanjutnya adalah: 

1.  Pengembangan 

atau 
peringatan  pada  peminjaman  buku  oleh  pengguna 
yang sudah habis masa peminjamannya. 

pemberitahuan 

notifikasi 

2.  Mengimplementasikan  server-side  datatables  dalam 
menampilkan data buku buku yang besar agar proses 
memuat menjadi lebih cepat. 

DAFTAR PUSTAKA 
[1]  Pusat  Pendidikan  dan  Pelatihan  Badan  Pusat  Statistik.  “Renstra 

Pusdiklat BPS 
 [ONLINE]  Tersedia: 
https://pusdiklat.bps.go.id/files/2020/monev/Renstra%20Pusdiklat%
20BP S%202020-2024.pdf 

2020-2024”. 2020.

[2]  I.  K.  Widiasa.  ""Manajemen  Perpustakaan  Sekolah.""  Jurnal 

Perpustakaan Sekolah, Hal 5-7. 2007. 

[3]  A.  Ibrahim,  dan  A.  Mira.  ""Pengembangan  Model  Perpustakaan 
Berbasis  Teknologi  Informasi  Untuk  Meningkatkan  Kinerja 
Layanan Perpustakaan dan mewujudkan Perpustakaan Ideal Berbasis 
digital.""  Prosiding  Konferensi  Nasional  Teknologi  Informasi  dan 
Aplikasinya (KNTIA2011). C87-C94. 2011. 

[4]  T.  Adeyinka,  O.  Dare,  S.  A.  Oguntayo,  and  M.  L.  Akanbi 
""Perception  and usage  pattern  of  e-books  among  library  and 
information  science  students  in  selected  universities  in  Nigeria."" 
Journal of  Library  &  Information  Technology 38, no. 2. 132-140. 
Mar 2018. 

[5]  T. Diana, D.H. Samosir, dan I. M. Widiyasa ""Pengembangan digital 
library  perpustakaan  universitas  atmajaya  Yogyakarta.""  Seminar 
Nasional Informatika (SEMNASIF). Vol. 1. No. 5. 2015. 

[6]  S. E. Susanto, “Desain Dan Standar Perpustakaan Digital”, Jurnal 

Pustakawan Indonesia, Vol. 10 No. 2. 2010. 

[7]  A.S. Mishra, S.K. Umre, & P.K. Gupta, “Qr Code in Library Practice 
Some Examples”, International Journal of Engineering Sciences & 
Research Technology, 6(2), 319-326. Feb 2017. 

[8]  D. Mehta and X. Wang, ""COVID-19 and digital library services – a 
case  study  of  a  university  library"",  Digital  Library  Perspectives, 
2020.  Tersedia:  10.1108/dlp-05-2020-0030  [Diakses  4  November 
2020]. 

[9]  Asbar,  Y.,      dan      Saptari,    M.A.,  “Analisa      Dalam      Mengukur 
Kualitas  Pelayanan  Terhadap  Kepuasan  Konsumen  Menggunakan  
Metode  PIECES”, Jurnal  Visioner  dan  Strategi, Volume 6, No. 2, 
2017, pp. 39-47. 

[10] Perdanawati, L. P. V. I., Rasmini, N. K., & Wirama, D. G. (2014). 
Pengaruh  unsur-unsur  kepuasan  pengguna  pada  efisiensi  dan 
efektivitas  kerja  pengguna  aplikasi  sistem  akuntansi  instansi  di 
satuan kerja pendidikan tinggi di provinsi Bali. E-Journal Ekonomi 
Dan Bisnis Universitas Udayana, 3(8), 478-493. 

 8 / 8 

      Gambar 17. Halaman Katalog Buku untuk Admin 

Tombol  bersimbol  QR  Code  di  topbar  halaman  Admin 
digunakan untuk memindai QR Code pada label buku tercetak 
di  perpustakaan.  Saat  data  buku  ditemukan,  halaman  akan 
dialihkan untuk melihat detail buku. Sedangkan jika data tidak 
ditemukan,  halaman  akan  dialihkan  ke  formulir  input  buku 
untuk  kemudian  diperiksa  kesesuaian  informasi  dan  disimpan 
sebagai data buku perpustakaan. 

      Gambar 18. Halaman Input Buku BMN 

F. Hasil Pengujian dan Evaluasi 

Pada penelitian ini pengujian sistem dilakukan dengan Black 
Box Testing dan System Usability Scale (SUS). Hasil black box 
testing menunjukkan bahwa seluruh fungsi pada tiap fitur telah 
sesuai dan berjalan dengan semestinya. Survei evaluasi uji coba 
sistem  dilakukan  terhadap  12  responden,  yang  terdiri  dari  2 
petugas  perpustakaan  Pusdiklat  BPS,  7  Alumni  Polstat  STIS 
Angkatan 58 dan 57 yang bersedia menguji coba sistem serta 3 
pengunjung perpustakaan. Hasil penilaian SUS didapatkan skor 
sebesar 76,87 yang termasuk dalam kategori baik. Tanggapan 
terhadap sistem didapatkan 10 dari 12 responden setuju bahwa 
dengan  adanya  sistem  ini  dapat  membuat  pelayanan  dan 
pengelolaan perpustakaan Pusdiklat BPS menjadi lebih efisien 
dan efektif dibanding sistem yang berjalan sebelumnya. 

VII. 

PENUTUP 

A. Kesimpulan 

Berdasarkan penelitian ini, hal-hal yang dapat disimpulkan 

adalah sebagai berikut: 
1.  Perpustakaan Pusdiklat BPS memiliki pengelelolaan dan 
proses bisnis yang kurang efektif dan efisien. Pengelolaan 
tersebut  meliputi  pencatatan  informasi  bahan  pustaka, 
buku  tamu,  dan  peminjaman  bahan  pustaka.  Selain  itu, 

 
 
 
 
 
"
221709571,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Pembangunan Aplikasi Quality Gates Berbasis Web
di Badan Pusat Statistik

Ardyanto (221709571, 4SI1)

Dosen Pembimbing: Dr. Margaretha Ari Anggorowati, M.T.

Ringkasan— Badan Pusat Statistik merupakan Lembaga
fungsi untuk
Pemerintah Non-Kementerian yang memiliki
melaksanakan tugas pemerintahan di bidang kegiatan statistik
sesuai dengan ketentuan peraturan perundang-undangan.
Sebagai
lembaga penghasil statistik tentu saja penjaminan
kualitas atau Quality Assurance dari statistik yang dihasilkan
BPS penting dilakukan. BPS sendiri telah menerapkan Quality
Assurance sejak tahun 2015 dalam bentuk National Quality
Assurance Framework (NQAF) dalam rangka menjamin kualitas
dari statistik yang dihasilkan. Akan tetapi salah satu instrumen
penjaminan kualitas dari NQAF yaitu Quality Gates (QG) belum
memiliki aplikasi khusus untuk pelaksanaannya. Pelaksanaan
QG saat ini hanya dibantu Google Spreadsheet. Pada penelitian
kali ini peneliti membangun aplikasi untuk pelaksanaan QG di
Badan Pusat Statistik. Aplikasi yang dibangun menggunakan
framework CodeIgniter versi 4 dengan metode pengembangan
yang diterapkan adalah metode Rapid Application Development.
Selanjutnya
dibangun
aplikasi
menggunakan Black Box Testing dan System Usability Scale
(SUS).
Kata Kunci— Quality Gates, NQAF, Quality Assurance

pengujian

telah

yang

I. LATAR BELAKANG.
Statistik

Badan Pusat

(BPS) merupakan Lembaga
Pemerintah Non-Kementerian yang memiliki fungsi untuk
melaksanakan tugas pemerintahan di bidang kegiatan statistik
sesuai dengan ketentuan peraturan perundang-undangan.
Dalam melakukan tugas-tugasnya, BPS memiliki sebuah visi
yaitu “Penyedia Data Statistik Berkualitas untuk Indonesia
Maju”. Salah satu usaha BPS dalam mewujudkan visi tersebut
adalah menyediakan statistik berkualitas yang berstandar
nasional dan internasional sebagaimana yang terdapat pada
misi Badan Pusat Statistik[1].

Berdasarkan visi dan misi BPS yang disampaikan di
paragraf sebelumnya, BPS mengedepankan kualitas dari data
yang dihasilkan. Penekanan mengenai kualitas statistik juga
dapat kita jumpai pada UU No 16 Tahun 1997 pasal 4 yang
berbunyi “Kegiatan statistik bertujuan untuk menyediakan
data statistik yang lengkap, akurat, dan mutakhir dalam rangka
mewujudkan Sistem Statistik Nasional yang andal, efektif, dan
efisien guna mendukung pembangunan nasional.”. BPS
sendiri telah menerapkan Quality Assurance sejak tahun 2015
dalam bentuk National Quality Assurance Framework (NQAF)
dalam rangka menjamin kualitas dari statistik yang dihasilkan.
NQAF yang digunakan BPS diadopsi dari UN-NQAF yang
dikeluarkan oleh United Nations (UN). Dalam rekomendasi
tersebut dijelaskan bahwa penggunaan Quality Assurance
untuk survei dan sensus yang dibentuk berdasarkan 10 prinsip
fundamental statistik dasar[2]. NQAF BPS terdiri dari tiga
yaitu Pengukuran Kualitas Mandiri,
instrumen

utama

Monitoring Kualitas, dan Quality Gates atau QG. Saat ini
baru Pengukuran Kualitas Mandiri dan Monitoring Kualitas
yang memiliki
pelaksanaannya
sedangkan untuk QG belum memiliki aplikasi khusus untuk
pelaksanaannya.

aplikasi

khusus

untuk

Quality gates adalah strategi mitigasi risiko statistik yang
dirancang untuk meningkatkan deteksi dini kesalahan atau
kekurangan di bagian mana pun dari siklus proses statistik,
baik itu pengumpulan, pemrosesan, analisis, atau diseminasi[3].
Dalam kasus BPS, QG bisa diumpamakan sebagai sebuah
gerbang atau gates pada titik-titik kritis pelaksanaan kegiatan
statistik. Pada setiap gates akan terdapat ukuran-ukuran yang
harus dipenuhi untuk melewati gates
ini
bertujuan untuk memastikan kualitas dari proses statistik yang
sedang berjalan dengan melakukan mitigasi terhadap risiko
proses sedini mungkin, sehingga dampak dari risiko yang
mungkin terjadi dapat dikontrol dan dapat segera diambil
keputusan dengan tidak membiarkan permasalahan berlanjut
ke tahap proses berikutnya[4]. Ketika pelaksanaan kegiatan
statistik telah melalui gate tersebut maka dapat diputuskan
bahwa proses pelaksanaan kegiatan statistik telah sesuai
dengan standar yang ditetapkan.

tersebut. Hal

Berdasarkan wawancara dengan narasumber, pelaksanaan
QG saat ini hanya dilakukan dengan bantuan aplikasi pihak
ketiga berupa Google Spreadheet. Penggunaan Google
Spreadheet dirasa kurang efisien karena tidak mampu
menangani keseluruhan
dari pelaksanaan QG. Google
Spreadsheet hanya digunakan untuk menangani proses
manajemen risiko.

dengan menuliskan

Proses pelaksanaan manajemen risiko dengan Google
pertanyaan-
Spreadsheet
dilakukan
risiko yang mungkin
pertanyaan untuk mengidentifikasi
terjadi. Kemudian setiap link akan dibagikan ke daerah yang
menjadi sampel pelaksanaan manajemen risiko. Pihak yang
berwenang akan menjawab pertanyaan tersebut sesuai dengan
pertanyaan yang ditujukan kepadanya berdasarkan jabatan dan
fungsi yang dimiliki. Jawaban akan dirangkum dan dianalisis
sesuai
dan
menghasilkan report berupa matriks risiko. Report yang
dihasilkan hanya tersedia untuk setiap sampel karena file pada
setiap sampel berbeda dan tidak ada hasil report untuk
keseluruhan sampel.

sebelumnya

ditetapkan

aturan

yang

telah

Pada tahap implementasi QG juga diperlukan sebuah
aplikasi khusus untuk pelaksanaannya karena belum ada
tahap ini. Pada
aplikasi yang mampu untuk menangani
pelaksanaan QG akan dilakukan pengecekan proses kegiatan
statistik yang dilakukan sesuai ukuran kualitas di setiap gate.
Pengecekan juga tidak hanya dilakukan di pusat tetapi juga

1 / 8

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

ada kemungkinan untuk dilakukan pengecekan kualitas di
level provinsi atau kabupaten/kota. Proses pengecekan juga
akan menentukan apakah pelaksanaan kegiatan statistik dapat
dilanjutkan ke tahap selanjutnya atau tidak.

II. TUJUAN PENELITIAN

Tujuan dari penelitian ini adalah untuk membangun sebuah
aplikasi Quality Gates berbasis web untuk dalam rangka
melakukan quality assessment dengan instrumen Quality
Gates pada pelaksanaan kegiatan statistik di Badan Pusat
Statistik. Aplikasi hanya fokus untuk penjaminan kualitas
yang sedang berjalan di Badan Pusat Statistik

III. PENELITIAN TERKAIT

Gambar 1. Alur pengembangan menggunakan RAD

Tahapan pengembangan menggunakan metode RAD terbagi
menjadi 3 tahap. Proses yang dilakukan peneliti di setiap tahap
sebagai berikut.

1. Requirement Planning

Review,

tersebut

dijelaskan

penerapan

Evaluation

risiko potensial

and
pada NSO Swedia.

Paul Biemer, dkk(2014) melakukan penelitian tentang “A
System for Managing the Quality of Official Statistics”. Pada
penelitian
sistem untuk
peningkatan kualitas data dari official statistics. Sistem yang
dimaksud di sini adalah ASPIRE atau A System for Product
yang
Improvement.
Sistem ini
diimplementasikan
menitikberatkan pada deteksi
terhadap
kualitas data untuk produk yang ditinjau dan mengevaluasi
upaya organisasi untuk memahami dan melakukan mitigasi
risiko ini melalui
studi evaluasi dan perbaikan proses.
ASPIRE melakukan penilaian risiko yang kemudian akan
diberi skor untuk menentukan prioritas pada risiko tersebut.
Hasil penelitian ini menjelaskan bahwa ASPIRE dapat
mengidentifikasi dan menilai risiko secara efektif sehingga
dapat diambil
tindakan untuk pencegahan atau perbaikan
sedini mungkin[5]. Konsep ASPIRE mirip dengan Quality
Gates yang ada di BPS akan tetapi pada ASPIRE tidak ada
apakah
pengecekan yang dilakukan untuk mengetahui
prosedur kegiatan statistik yang dilakukan sudah sesuai atau
belum.

Penelitian lain dilakukan oleh Geoff Neideck (2007)
dengan penelitian yang berjudul “A Framework for the
Accuracy Dimension of Data Quality for Price Statistics”.
Pada penelitian tersebut dijelaskan mengenai bagaimana
konsep quality gates berguna dalam mengidentifikasi poin-
poin penting untuk menilai kualitas data sebelum data
berpindah dari satu tahap ke tahap berikutnya. Pemilihan
proses yang akan dipantau dengan quality gates juga didasari
oleh risiko yang mungkin terjadi di setiap tahap. Tahap
dengan kemungkinan risiko yang besar perlu dipantau lebih
intensif. Geoff juga menjelaskan bahwa setiap gates memiliki
indikator yang harus diterima dan telah disetujui bersama
sebelum lanjut ke proses selanjutnya[6].

IV. METODE PENELITIAN

ini

Metode yang akan digunakan dalam pembangunan aplikasi
Quality Gates
adalah metode Rapid Application
Development atau RAD. Salah satu keuntungan RAD
keterlibatan user
semakin meningkat karena merupakan
tim secara keseluruhan sehingga user dapat
bagian dari
langsung memberi umpan balik terhadap aplikasi atau sistem
yang dibangun[7]. Tahapan RAD yang akan dilakukan dapat
dilihat pada gambar 1.

Pada tahap ini, peneliti dan user berdiskusi untuk
membahas goals dari aplikasi yang akan dibangun.
Pada tahap ini juga dibahas mengenai kebutuhan dan
tujuan dari aplikasi yang dibangun. Pada penelitian ini,
user yang berdiskusi pada tahap ini dengan peneliti
Subdirektorat Konsistensi
hanya
Statistik
dari
subdirektorat tersebut yang paham mengenai Quality
Gates BPS.

dari
dikarenakan

pegawai

pegawai

hanya

2. RAD Design Workshop

yang

kebutuhan

berdasarkan

Pada tahap ini peneliti membuat prototype dari
aplikasi
telah
diidentifikasi. Selanjutnya prototype akan dievaluasi
dan diberi masukan dari user jika prototype yang
dibangun belum sesuai. Pada tahap ini juga peneliti
mendapat beberapa masukan dari Direktorat Sistem
Informasi Statistik BPS. Masukan yang diberikan
khusus pada kebutuhan aplikasi agar sesuai dengan
server yang ada di BPS.
Implementation
Pada tahap ini prototype aplikasi telah disetujui yang
berarti aplikasi yang dibangun sudah sesuai dengan
kaidah dan aturan yang telah ditetapkan di BPS.
Aplikasi juga melalui pengujian berupa Black Box
Testing dan System Usability Scale.

3.

V. KERANGKA PIKIR

Gambar 2. Kerangka Pikir Penlitian

2 / 8

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

risiko akan dituliskan pada Google Spreadsheet. Selanjutnya
link Google Spreadsheet akan dibagikan kepada petugas yang
berwenang untuk mengisikan jawaban sesuai aturan yang
telah ditetapkan sebelumnya. Dari isian-isian tersebut akan
dianalisis dan menghasilkan tingkat risiko yang mungkin
terjadi pada setiap tahapan. Hasil dari manajemen risiko
risiko yang selanjutnya akan digunakan
adalah matriks
sebagai bahan untuk identifikasi peletakan gates dan
penyusunan rekomendasi QG. Setelah rekomendasi QG
selesai, selanjutnya diserahkan ke pelaksana survei untuk
dilengkapi dan diimplementasikan. Proses implementasi QG
akan diserahkan sepenuhnya ke pelaksana survei.

Dari analisis

sistem berjalan ditemukannya beberapa
permasalahan yang mungkin terjadi. Gambar 4 menunjukkan
fishbone diagram yang menampilkan permasalahan yang
terdapat empat
dijumpai. Pada fishbone diagram tersebut
pokok masalah yaitu machine, man, method, dan measurement.

Penelitan ini berawal dari kebutuhan BPS sebagai lembaga
penghasil data untuk melakukan Quality Assurance pada
survei dan sensus yang dilaksanakan. Quality Gates sebagai
salah satu instrumen dalam Quality Assurance BPS belum
memiliki aplikasi khusus untuk pelaksanaannya. Kemudian
terdapat peluang yaitu tersedianya berbagai macam framework
yang gratis dan mudah digunakan untuk pembangunan
aplikasi. Selanjutnya yang dilakukan di penelitian ini adalah
akan membangun sebuah aplikasi untuk pelaksanaan Quality
Gates di BPS. Aplikasi yang dibangun berbasis web dan akan
dibangun menggunakan framework CodeIgniter versi 4. Selain
itu akan digunakan framework Bootstrap untuk CSS dan
jQuery untuk JavaScript untuk memudahkan dalam proses
pembangunan aplikasi. Pengujian terhadap aplikasi yang
sudah dibangun akan menggunakan metode Black Box Testing
dan System Usability Scale (SUS). Hasil yang diharapkan dari
penelitian ini berupa aplikasi Quality Gates yang dibangun
dapat bekerja sesuai dengan kaidah dan aturan yang telah
ditetapkan BPS. Alur dari kerangka pikir penelitian ini dapat
dilihat pada gambar 2.

VI. HASIL DAN PEMBAHASAN

A. Analisis Sistem Berjalan

Gambar 4. Fishbone Diagram

B. Rancangan Arsitektur Sistem

Aplikasi yang dibangun terdiri dari tiga jenis user yaitu
admin, operator pusat, dan operator daerah. Masing-masing
operator dapat mengakses aplikasi QG pada server BPS
melalui jaringan internet. Aplikasi QG akan dibangun dengan
back-end menggunakan PHP dengan bantuan framework
CodeIgniter versi 4 dan penyimpanan data menggunakan
MariaDB. Front-end akan menggunakan HTML 5 dan untuk
membantu proses pengembangan akan digunakan framework
CSS Bootstrap untuk mempercantik tampilan dari aplikasi.
Penggunaan framework JavaScript berupa jQuery digunakan
untuk pengolahan informasi di sisi client. Aplikasi QG juga
dapat berkomunikasi dengan aplikasi NQAF melalui API yang
telah disediakan oleh web NQAF. API ini berfungsi untuk
melakukan update informasi yang berasal dari aplikasi QG
untuk ditampilkan ke aplikasi NQAF. Rancangan arsitektur
sistem akan ditampilkan pada gambar 5 berikut.

Gambar 5. Rancangan Arsitektur Sistem

3 / 8

Gambar 3. Proses Pelaksanaan QG

Gambar 3 menunjukkan proses bisnis pelaksanaan QG pada
BPS saat ini. Proses QG dimulai dari memilih survei atau
sensus untuk pelaksanaan QG. Setelah memilih survei atau
sensus akan dilanjutkan ke tahap perencanaan dan penyusunan
manajemen risiko. Penyusunan manajemen risiko dilakukan
dengan membuat pertanyaan-pertanyaan untuk mendeteksi
kemungkinan risiko. Setelah penyusunan manajemen risiko
adalah pengisian manajemen risiko dengan
selanjutnya
risiko yang telah
menjawab pertanyaan-pertanyaan terkait
dibuat
sebelumnya. Pengisian manajemen risiko masih
dilakukan dengan bantuan aplikasi pihak ketiga yaitu Google
Spreadsheet. Pertanyaan-pertanyaan mengenai manajemen

C. Analisis Kebutuhan

D. Rancangan Proses Bisnis

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Aplikasi yang dibangun bertujuan untuk pelaksanaan proses
asessment kualitas dengan instrumen Quality Gates dari
kegiatan statistik BPS. Kebutuhan aplikasi yang akan
dibangun terbagi menjadi dua bagian yaitu kebutuhan
fungsional dan kebutuhan non fungsional. Berikut kebutuhan
fungsional dari aplikasi yang akan dibangun.
1. Aplikasi dapat menangani keseluruhan proses QG dari

perencanaan hingga pelaksaanan QG

2. Aplikasi dapat melakukan filter terhadap hak akses user
sehingga user hanya dapat mengakses fungsi sesuai hak
akses yang dimiliki

3. Aplikasi dapat menampilkan report baik untuk pelaksanaan

manajemen risiko maupun pelaksanaan QG
Kebutuhan non fungsional dari aplikasi akan dijelaskan
menggunakan tabel PIECES. Tabel PIECES akan memecah
permasalahan menjadi enam bagian, yaitu Performance,
Information, Economic, Control, Efficiency, Service. Pada
tabel
I disajikan analisis kebutuhan menggunakan tabel
PIECES untuk aplikasi quality gates BPS.

Bagian
Performance

Information

Economic
Control

TABEL I

TABEL PIECES

Masalah
Beberapa tahap proses QG
di BPS masih dilakukan
manual.

Output untuk pelaksanaan
manajemen risiko hanya
tersedia
setiap
untuk
sampel,
tidak ada output
secara keseluruhan.

-
Proses implementasi QG
diserahkan sepenuhnya ke
ini
subject matter, hal
adanya
berpotensi
kesalahan
dalam
implementasi QG karena
tidak adanya pengawasan
langsung
dari
Subdirektorat Konsistensi
Statistik.

Efficiency

harus
Pengguna masih
mengecek
pertanyaan
mana yang harus dijawab
untuk manajemen risiko.

Service

-

seluruh

Solusi
Proses QG di BPS
dapat
dilakukan
dalam satu aplikasi
yang terintegrasi.
Menampilkan matriks
risiko untuk seluruh
sampel dengan nilai
risiko
kemungkinan
dampak
dan
skala
rata-rata
merupakan
isian
dari
operator.
-
Proses
implementasi
QG dapat dilihat oleh
fitur
admin melalui
yang telah tersedia.
dengan
Selain
itu
aplikasi
penggunaan
juga
dapat
menyeragamkan
implementasi
proses
QG yang telah sesuai
aturan
telah
yang
ditetapkan
sebelumnya.
Hanya menampilkan
atau
pertanyaan
survei
yang
merupakan tanggung
jawab petugas yang
bersangkutan
sehingga tidak perlu
mengecek
secara
manual.
-

Gambar 5. Swimlane Diagram Proses Bisnis usulan QG

4 / 8

Secara umum, proses bisnis pelaksanaan QG pada sistem
usulan terbagi menjadi dua bagian besar. Proses yang pertama
tahap
adalah proses penyusunan QG yang terdiri dari
pemilihan survei atau sensus untuk QG sampai pada operator
pusat melengkapi
rekomendasi QG. Proses yang kedua
merupakan tahap implementasi QG. Tahap implementasi
dimulai ketika operator pusat melakukan inisiasi pelaksanaan
QG sampai hasil dari pelaksanaan QG bisa dilihat oleh admin.
Pada
diagram untuk
5
memberikan gambaran mengenai
rancangan proses bisnis
aplikasi yang dibangun.

swimlane

disajikan

gambar

E. Use Case Diagram
Pada aplikasi QG terdapat

daerah. Admin

tiga aktor yaitu admin dan
operator yang terbagi menjadi dua yaitu operator pusat dan
untuk menyusun
operator
perencanaan QG, manajemen risiko, dan menyusun QG.
Selain itu admin juga dapat melihat hasil dari pelaksanaan
manajemen risiko berupa matriks risiko, dan summary dari
pelaksanaan QG.

bertugas

Operator terdiri dari dua yaitu operator daerah dan operator
pusat. Operator pusat memiliki lebih banyak fitur dibanding
operator daerah. Operator dapat melakukan pengisian terhadap
pertanyaan terkait manajemen risiko dan pengisian pada
tugas tambahan
pelaksanaan QG. Operator pusat memiliki
yaitu melengkapi rekomendasi ukuran kualitas QG dari admin
dan memulai pelaksanaan QG. Operator daerah dalam
melakukan pengisian QG harus menggunakan token. Token
akan berguna untuk melakukan pengenalan wilayah dari
operator daerah. Secara lengkapnya dapat dilihat pada use
case pada gambar 6 berikut.

Gambar 6. Use Case Diagram QG

F. Rancangan Database
pada

Database

aplikasi QG digunakan menyimpan
terkait pelaksanaan QG BPS. Aplikasi QG
informasi
menggunakan
penyimpanan
informasinya. Deskripsi tentang tabel-tabel tersebut disajikan
pada tabel II berikut.

sebanyak

untuk

tabel

14

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

TABEL II

TABEL DESKRIPSI TABEL DATABASE

Nama Tabel
qg_survei

qg_gates
qg_measures
qg_token

qg_report
qg_pertanyaan

qg_isian

qg_operator
qg_penanggung_jawab

qg_role
qg_sampel

qg_tahap

qg_kabupaten
qg_provinsi

Deskripsi
Menyimpan informasi seluruh survei yang telah
didaftarkan QG
Menyimpan daftar gates
Menyimpan daftar ukuran kualitas seluruh gates
Menyimpan daftar wilayah pelaksanaan QG dan
tokennya masing-masing
Menyimpan hasil pelaksanaan QG
Berisi pertanyaan-pertanyaan untuk pelaksanaan
manajemen risiko
Menyimpan isian pelaksanaan manajemen risiko
dari operator
Menyimpan informasi operator
Menyimpan informasi penanggung jawab untuk
mengisi pertanyaan terkait manajemen risiko
sekaligus menjadi tabel yang menjembatani antara
survei dan operator pada pelaksanaan manajemen
survei
Berisi daftar role di setiap survei
Berisi
manajemen risiko
Berisi
GSBPM
Berisi daftar kabupaten/kota di seluruh Indonesia
Berisi daftar provinsi di seluruh Indonesia

pelaksanaan

berdasarkan

tahapan

delapan

sampel

survei

daftar

untuk

Relasi antar tabel dan field pada database dapat dilihat pada

Entity Relationship Diagram pada gambar 7 berikut.

Gambar 7. Entity Relationship Diagram

G. Implementasi Kode Program

Penelitian ini menggunakan beberapa bahasa pemrograman
dalam pembangunan aplikasi. Bahasa-bahasa yang digunakan
antara lain HTML, CSS, JavaScript dan PHP. Pada bagian
implementasi kode program akan ditampilkan potongan-
potongan kode program yang digunakan dalam pembangunan
aplikasi. Pada makalah ini penulis hanya menampilkan
beberapa contoh dari
implementasi kode program pada
aplikasi yang dibangun.

1. Sidebar Operator

Pada gambar 8 ditunjukkan potongan kode untuk
tampilan sidebar operator. Aplikasi akan mengecek apakah
user yang login merupakan operator pusat atau daerah.

5 / 8

Selanjutnya aplikasi akan menampilkan menu sidebar
sesuai user yang login.

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Gambar 8. Potongan Kode Sidebar Operator

2. Menambahkan Survei

Gambar 9. Potongan Kode Menambahkan Survei

Pada gambar 9 menunjukkan potongan kode untuk
melakukan insert survei untuk pelaksanaan quality gates.
Fungsi
ini akan diakses oleh web NQAF melalui API
dengan method POST. Paramater yang dikirimkan ada
empat yaitu id, nama, deskripsi, dan id_unit. Parameter
tersebut akan disimpan pada basis data pada tabel qg_survei
dengan menggunakan metode insert. Apabila pada saat
melakukan insert, id yang dikirimkan telah terdapat pada
tabel qg_survei maka aplikasi akan memberikan informasi
error.

3. Generate Matriks Risiko

Pada gambar 10 ditampilkan potongan kode untuk
generate matriks risiko. Aplikasi menyimpan rata-rata dari
isian oleh operator untuk pertanyaan skala kemungkinan
dan skala dampak ke dalam array JavaScript. Selanjutnya
sistem akan mengecek setiap item dalam array tersebut
lokasi baris dan kolom
untuk menemukan informasi
pertanyaan
Setelah
risiko.
menemukan informasi baris dan kolom,
sistem akan
melakukan append nilai nomor rincian pertanyaan ke dalam
tabel atau matriks risiko.

pada matriks

tersebut

Gambar 10. Potongan Kode Generate Matriks Risiko

H. Implementasi Tampilan

Pada pembangunan sistem digunakan framework Bootstrap
tampilan yang menarik.

5 untuk mempermudah membuat
Berikut ini beberapa contoh tampilan untuk aplikasi QG.

Gambar 11. Halaman Login

Gambar 12. Halaman Perencanaan Survei

6 / 8

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Gambar 13. Halaman Manajemen Risiko

Gambar 17. Halaman Implementasi QG pada Admin

Gambar 14. Halaman Pengisian Manajemen Risiko

Gambar 15. Halaman Matriks Risiko

I. Pengujian Aplikasi

Evaluasi yang dilaksanakan pada aplikasi yang telah
dibangun menggunakan dua metode yaitu Black Box
Testing dan System Usability Scale (SUS).

a.

Black Box Testing
Black box testing adalah metode pengujian yang
didasarkan pada kebutuhan spesifikasi dan tidak perlu
melakukan pengecekan pada struktur dalam dari aplikasi[9].
Tabel
III berikut merupakan black box testing yang
dilakukan pada fungsi pelaksanaan QG.

TABEL III

TABEL BLACK BOX TESTING FUNGSI PELAKSANAAN QG

No

Skenario Pengujian

Hasil yang diharapkan

Kesimpulan

1

2

3

Memilih tolerance
dengan nilai merah
pada
pelaksanaan
QG

Memilih tolerance
dengan nilai merah
pada
pelaksanaan
QG

Memilih
tombol
detail pada ukuran
kualitas
yang
pengecekannya
berada
level
di
wilayah di bawah
level wilayah user

valid

valid

valid

gate

akan
Aplikasi
masukan
mengolah
akan
user,
warna
berubah
menjadi merah, tombol
input masih ada
akan
Aplikasi
masukan
mengolah
user,
akan
gate
berubah biru jika gate
belum selesai,
gate
akan berwarna hijau
selesai,
jika
tombol
pada
input
ukuran kualitas yang
bersangkutan
sudah
tidak ada
akan
Aplikasi
daftar
menampilkan
wilayah
bawah
di
wilayah user beserta
status apakah selesai
atau belum

gate

Gambar 16. Halaman Pelaksanaan QG

b.

System Usability Scale (SUS)

System Usability Scale atau SUS adalah survei yang cepat
dan mudah yang digunakan untuk menilai kegunaan produk
ini
atau layanan tertentu[10]. Pengujian SUS pada aplikasi
dilakukan pada 10 responden dengan menghasilkan skor SUS
[10], nilai SUS sebesar 77
sebesar 77. Menurut referensi

7 / 8

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

[3] Australian Bureau of Statistics. A Data Quality Framework For The
Australian Government’ S Direct Measure Of Income For Capacity. 2020
[4] Badan Pusat Statistik. Kerangka Penjaminan Kualitas Data Statistik,
Handbook of Quality Assurance Framework BPS. Jakarta: BPS, 2015.
[5] P. Biemer, D. Trewin, H. Bergdahl, and L. Japec. A System for
Managing the Quality of Official Statistics. Journal of Official Statistics,
Vol. 30, No. 3, 2014

[6] Neideck, G. A Framework for the Accuracy Dimension of Data Quality

for Price Statistics. September, 2007. 1–12.

[7] S. Aswati, M. S. Ramadhan, A. U. Firmansyah, K. Anwar. Studi Analisis
Model Rapid Application Development dalam Pengembangan Sistem
Informasi. Jurnal Matrik Vol. 16. No. 2, 2017

[8] A. Noertjahyana, ""Studi Analisis Rapid Aplication Development Sebagai
Salah Satu Alternantif Metode Pengembangan Perangkat Lunak"", Jurnal
Informatika Vol. 3, 74 - 79, Nopember 2002

[9] A. Bansal, ""A Comparative Study of Software Testing Techniques"",
International Journal of Computer Science and Mobile Computing, Vol. 3,
Issue. 6, 579 – 584, June 2014

[10] A. Bangor, P. T. Kortum, J. T. Miller, ""An Empirical Evaluation of the
System Usability Scale"", Journal of Human–Computer Interaction, 574–
594, 2008

menyatakan bahwa aplikasi yang dibangun sudah baik dan
dapat diterima.

VII.
penutup

PENUTUP
akan

Pada

bagian

dipaparkan mengenai
kesimpulan yang diperoleh pada penelitian ini dan saran untuk
pengembangan penelitian ini selanjutnya

A. Kesimpulan

Kesimpulan yang didapatkan pada pembangunan sistem

yang telah dilakukan adalah sebagai berikut:
1. Aplikasi yang dibangun fokus untuk pengecekan kualitas
kegiatan statistik di Badan Pusat Statistik dengan
instrumen Quality Gates dan dapat berjalan sesuai aturan
dan kaidah yang telah ditetapkan oleh BPS. Aplikasi
yang telah dibangun juga telah mampu menjawab
kebutuhan fungsional dan non fungsional berdasarkan
analisis kebutuhan yang telah dilakukan.

2. Aplikasi yang sudah dibangun telah melalui pengujian
Black Box Testing menunjukkan bahwa fungsi-fungsi
yang telah dibuat pada aplikasi dapat berjalan dengan
baik. Pengujian dengan menggunakan metode SUS
memperoleh skor sebesar 77 sehingga dapat disimpulkan
bahwa aplikasi yang dibangun sudah bisa diterapkan di
BPS.

B.

Saran
Aplikasi

yang

dibangun masih membutuhkan
penyempurnaan-penyempuranan dan penambahan fitur
untuk menambah nilai dari dari aplikasi yang dibangun.
Berikut beberapa saran dari peneliti untuk pengembangan
sistem ini kedepannya maupun saran dalam penggunaan
aplikasi yang dibangun.
1. Perlu dilakukan pengembangan lebih lanjut untuk
pengguna eksternal agar dapat menampung kebutuhan
dari pengguna yang berasal dari
luar Badan Pusat
Statistik mengingat aplikasi ini kedepannya direncanakan
untuk
juga
melakukan penjaminan kualitas dari kegiatan statistik
yang dilakukan. Pengembangan lebih lanjut juga dapat
dilakukan pada sisi tampilan dari aplikasi yang dapat
dibuat lebih responsif untuk seluruh ukuran perangkat.
2. Perlu adanya integrasi dengan sistem yang sudah ada di
Badan Pusat Statistik seperti sistem kepegawaian untuk
mempermudah user.

digunakan

pengguna

eksternal

oleh

3. Sebelum menggunakan aplikasi

ini, user diharapkan
memiliki pengetahuan mengenai Quality Gates BPS agar
mendapat gambaran untuk setiap proses di dalam
aplikasi ini. Selain itu user diharapkan untuk mengakses
aplikasi menggunakan perangkat laptop atau komputer
dikarenakan tampilan aplikasi belum dibuat responsif.

[1] Badan

Pusat

DAFTAR PUSTAKA
BPS
Tentang

Statistik.

[Online]. Available:

https://www.bps.go.id/menu/1/informasi-umum.html

[2] United Nations, Recommendations on quality assurance for official
statistics, United Nations National Quality Assurance Frameworks
Manual for Official Statistics, New York: United Nations, 2019, pp 10

8 / 8

"
221709561,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Kajian Penghitungan Indeks Harga Properti 
Perumahan dengan Menggunakan Data Situs 
Properti 

Anik Nurul ‘Izzati (221709561, 4SD1) 
Dosen Pembimbing: Nucke Widowati Kusumo Projo, S.Si, M.Sc, Ph.D 

langsung 

Ringkasan—  Badan  Pusat  Statistik  (BPS)  menghitung  Indeks 
Harga  Properti  Perumahan  (IHPP)  yang  merupakan  salah  satu 
indikator  untuk  melihat  pertumbuhan  ekonomi  di  bidang 
properti. Survei yang dilakukan untuk menghitung IHPP adalah 
Survei  Harga  Properti  Perumahan  (SHPP).  Dalam  melakukan 
survei, BPS masih menggunakan cara konvensional yaitu dengan 
wawancara 
terhadap  perusahaan  pengembang 
perumahan/apartemen. BPS mengalami kendala saat melakukan 
survei,  dilihat  dari  perbedaan  jumlah  terealisasi  dan  jumlah 
target pada pencacahan SHPP yang jauh berbeda. Penelitian ini 
dilakukan  untuk  mengatasi  masalah 
tersebut  dengan 
memanfaatkan  pendekatan  Big  Data.  IHPP  akan  dihitung 
menggunakan  data  yang  didapatkan  dari  scraping  data  yang 
dilakukan  pada  salah  satu  situs  properti  yaitu  rumah123.com. 
Penghitungan menggunakan Metode Matched Model dan Metode 
Hedonik.  IHPP  yang  dihasilkan  akan  dibandingkan  dengan 
IHPP  milik  BPS.  Berdasarkan  nilai  akurasi  yang  dihitung 
dengan  MAPE,  IHPP  dengan  penghitungan  Metode  Hedonik 
menghasilkan  nilai  akurasi  terkecil  yaitu  2,69%  untuk  properti 
rumah dan 4,75% untuk properti apartemen. 

Kata Kunci— Big Data, IHPP, WebScraping, Properti. 

I.  LATAR BELAKANG 

Kebutuhan  dasar  bagi  setiap  manusia  yang  harus  dipenuhi 
terdiri  dari  kebutuhan  sandang,  pangan,  dan  papan.  Pada 
zaman  modern  ini,  kebutuhan  manusia  semakin  beragam. 
Walaupun  demikian,  ketiga  kebutuhan  pokok  tersebut  akan 
tetap berada di urutan teratas dalam hal permintaan kebutuhan 
masyarakat.  Sebagai  manusia,  kebutuhan  papan  atau  rumah 
merupakan  suatu  tolak  ukur  kesejahteraan.  Rumah  menjadi 
tempat dalam menjaga kelangsungan kehidupan. 

Dikutip  dari  Kompas,  Direktur  Jenderal  Pembiayaan 
Perumahan  Kementerian  Pekerjaan  Umum  dan  Perumahan 
Rakyat,  Maurin  Sitorus,  memperkirakan  hingga  tahun  2025 
angka  kebutuhan  rumah  di  Indonesia  mencapai  30  juta  unit 
atau  diperkirakan  mencapai  1,2  juta  unit  per  tahun  [1]. 
Direktur  Eksekutif  Jakarta  Property 
Institute,  Wendy 
Haryanto, mengatakan sektor properti berkontribusi signifikan 
dalam  pertumbuhan  perekonomian  [2].  Untuk  Jakarta  saja, 
sektor  ini  menyumbang  sekitar  Rp  32,3  triliun  atau  17,61 
persen  pada  tahun  2019  [2].  Dengan  kontribusi  sebesar  itu, 
diharapkan  perekonomian  yang  terpuruk  dapat  dipulihkan 
melalui  sektor  properti  [2].  Hal  tersebut  tentunya  akan 
(developer)  untuk 
mendorong  perusahaan  pengembang 
membangun lebih banyak properti di berbagai wilayah dengan 
berbagai pilihan spesifikasi dan kisaran  harga tertentu. Harga 

properti  yang  beragam  perlu  untuk  dipantau  dengan 
menggunakan indeks harga. 

Badan  Pusat  Statistik  (BPS)  memiliki  Indeks  Harga 
Properti  Perumahan  (IHPP)  yang  didapat  dari  Survei  Harga 
Properti Perumahan (SHPP). Survei ini dilakukan oleh Subdit 
Statistik Harga Perdagangan Besar di  bawah  Bidang Statistik 
Distribusi  dan  Jasa.  Kegiatan  survei  merupakan  permintaan 
konsumen  data  sejak  tahun  2010.  BPS  memulai  survei 
tersebut pada tahun 2015 dengan cakupan 12 kota. Kemudian 
cakupan  SHPP  tiap  tahun  ditingkatkan  hingga  pada  tahun 
2018 menjadi 34 kota. 

IHPP  merupakan  salah  satu 

indikator  untuk  melihat 
pertumbuhan  ekonomi  khususnya  di  bidang  properti.  Selain 
itu,  indeks  ini  juga  digunakan  sebagai  deflator  pembentukan 
modal  tetap  bruto  dalam  neraca  nasional  [3].  Saat  ini  ada 
beberapa  pihak  yang  menghitung  IHPP,  diantaranya:  Bank 
Indonesia, Bank Tabungan Negara, dan Badan Pusat Statistik. 
Pembeda  IHPP  BPS  dari  yang  lain  yaitu,  BPS  menghitung 
IHPP  dengan  cakupan  tipe  properti  rumah  dan  apartemen 
yang  baru  dan  bukan  bekas.  Untuk  mendapatkan  indeks  ini, 
BPS  melakukan survei dengan wawancara  langsung terhadap 
perusahaan  pengembang  perumahan/apartemen  yang  masih 
aktif  dalam  melakukan 
transaksi  penjualan.  Dalam 
penghitungan  IHPP,  BPS  menggunakan  Metode  Matched 
Model. Data yang dihitung dengan  metode ini, perubahannya 
haruslah  memiliki  karakteristik  yang  identik  atau  spesifikasi 
yang sama [3].  

Berdasarkan  publikasi  BPS  yaitu  Aktivitas  Badan  Pusat 
Statistik  2019,  pada  Pencacahan  Survei  Harga  Properti 
Perumahan  terlihat  bahwa  jumlah  terealisasi  masih  di  bawah 
jumlah target seperti terlihat pada Gambar 1 [4]. Hal tersebut 
juga terjadi di tahun-tahun sebelumnya. Sehingga, secara tidak 
langsung  BPS  masih  mengalami 
dalam 
menyelenggarakan survei. 

kendala 

Gambar 1. Target dan Realisasi Sampel Kegiatan SHPP Tahun 2019 

 1 / 8 

 
 
 
 
 
 
 
Salah  satu  solusi  untuk  mengatasi  kendala  tersebut  adalah 
dengan  memanfaatkan  Big  Data.  Tahun  2020,  IMF  merilis 
buku  berjudul  RPPI  (Residential  Property  Price  Indices) 
Practical  Guide  yang  di  dalamnya  terdapat  perbandingan 
sumber  data  untuk  menghitung  RPPI.  Disebutkan  bahwa 
sumber data dari website memiliki ketepatan waktu yang lebih 
baik  daripada  data  yang  bersumber  dari  developer.  Sumber 
data  dari  website  juga  memiliki  ketersediaan  data  yang  baik, 
cepat, dan tidak memakan biaya [5]. Pada buku panduan RPPI 
ini,  juga  disebutkan  metode-metode  yang  direkomendasikan 
untuk  menghitung  indeks  harga  properti.  Salah  satunya 
menggunakan Metode Hedonik yang menilai perubahan harga 
properti  dari  efek  perubahan  harga  yang  timbul  akibat 
perbedaan karakteristik dan  lokasi properti  yang  dijual dalam 
relevan  dibandingkan  dengan  periode 
periode  yang 
sebelumnya [6].  

Seiring  berkembang  pesatnya  teknologi,  diketahui  bahwa 
Big  Data  dapat  memainkan  peran  penting  dalam  Official 
Statistics.  Penelitian  IMF  mengatakan  Big  Data  dapat 
dijadikan  sebagai  cara  baru  dalam  mengukur 
indikator 
ekonomi  seperti  harga,  kondisi  pasar  tenaga  kerja,  pasar 
perumahan,  sentimen  bisnis,  dan  lain-lain  [7].  Penerapan  Big 
Data  juga  merupakan  tantangan  BPS  untuk  mendukung 
ini  akan 
Official  Statistics.  Maka  dari 
memanfaatkan  Big  Data  yang  akan  dikembangkan  dalam 
penghitungan  IHPP  sebagai  perbandingan  IHPP  berbasis 
survei BPS yang sudah ada. 

itu,  penelitian 

Rumah123.com merupakan situs nomor 1 di Indonesia. Hal 
ini  berdasarkan  survei  dan  riset  Portal  Properti  Terbaik  di 
Indonesia yang dilakukan oleh Majalah Property and The City 
pada tahun 2016. Dari  survei dan riset tersebut menghasilkan 
bahwa  rumah123.com  merupakan  situs  properti  terbaik  di 
Indonesia dengan pengunjung harian terbanyak daripada situs 
properti lainnya [8]. Didirikan pada Maret 2007, situs properti 
ini  telah  membantu  banyak  warga  negara  Indonesia  maupun 
asing  dalam  melakukan 
itu 
rumah123.com  akan  digunakan  sebagai  objek  penelitian  atau 
website yang di-scraping.  

jual-beli  properti.  Untuk 

Sebelumnya, Subdirektorat Pengembangan Model Statistik 
di  BPS  sudah  melakukan  scraping  data  pada  website 
rumah123.com.  Scraping  tersebut  dilakukan  sebagai  upaya 
pemanfaatan Big Data untuk mendukung Official Statistics di 
BPS.  Maka  dari  itu  skripsi  ini  akan  menggunakan  hasil 
scraping data  yang sudah  mereka  lakukan. Dari sampel  hasil 
scraping  yang  telah  dilakukan,  terlihat  bahwa  iklan  properti 
terbanyak berada di Pulau Jawa seperti terlihat dalam Gambar 
2.  Sehingga  objek  penelitian  ini  dibatasi  akan  menggunakan 
data  hasil  scraping  website  rumah123.com  dengan  hanya 
wilayah kota-kota besar di Pulau Jawa. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 2. Grafik Data Sampel Hasil Scraping Website rumah123.com 
berdasarkan Provinsi 

Mulai 

perubahan 

tahun  2020,  dalam  Survei  Harga  Properti 
Perumahan,  BPS  melakukan 
frekuensi 
pengumpulan  data  dari  tahun  sebelumnya  yang  triwulanan 
menjadi  tahunan.  Hal  tersebut  membuat  perlunya  data  hasil 
scraping  dari  awal  tahun  2019  sampai  akhir  tahun  2020. 
Subdirektorat Pengembangan Model Statistik BPS baru mulai 
melakukan  scraping  data  di  akhir  tahun  2019  menjelang 
2020,  sehingga  penelitian  ini  akan  fokus  menggunakan  data 
pada  Triwulan  IV  tahun  2019  untuk  IHPP  BPS  sebagai  data 
sekunder dan data hasil  scraping  website rumah123.com dari 
bulan Juli 2019 sampai Desember 2019. Dalam penelitian  ini 
akan  digunakan  dua  metode  penghitungan  yaitu,  Metode 
Matched  Model,  metode  yang  digunakan  oleh  BPS  dan 
Metode  Hedonik,  metode  yang  banyak  digunakan  oleh 
berbagai  negara  untuk  penghitungan  indeks  harga  properti 
resmi mereka. 

II.  TUJUAN PENELITIAN 

Tujuan  dari  penelitian 

ini  adalah  menghitung  IHPP 
berdasarkan  pendekatan  Big  Data  yaitu  menggunakan  data 
hasil  scraping  situs  properti  website  rumah123.com  yang 
dilakukan  oleh  Subdirektorat  Pengembangan  Model  Statistik 
BPS.  Evaluasi  pada  data  yang  telah  dikumpulkan  akan 
dilakukan  untuk  melihat  apakah  data  hasil  scraping 
selanjutnya  dapat  digunakan  sebagai  Indeks  Harga  Properti 
Perumahan  milik  BPS dengan pendekatan data situs properti. 
Sehingga didapatkan tujuan penelitian yang lebih rinci sebagai 
berikut: 

1.  Menghitung  IHPP  di  Indonesia  berdasarkan  hasil 
scraping  dari  website  rumah123.com  pada  level  kota 
dengan  metode  penghitungan  BPS  yaitu  Metode 
Matched Model. 

2.  Menghitung  IHPP  di  Indonesia  berdasarkan  hasil 
scraping  dari  website  rumah123.com  pada  level  kota 
dengan Metode Hedonik. 

3.  Membandingkan 

dari 
pendekatan data situs properti dengan IHPP milik BPS.   

dan  menganalisis 

IHPP 

 2 / 8 

 
 
 
 
 
 
 
 
 
III. PENELITIAN TERKAIT 

Hasil  penelitian  sebelumnya  yang  berkaitan  dengan 

penelitian ini dibuat dalam Tabel I berikut ini.  

TABEL I 
Jurnal Terkait Big Data untuk Official Statistics dan Penghitungan Indeks 
Harga Properti 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

bermanfaat  untuk  menghitung  indeks  harga  properti  resmi  di 
berbagai negara. 

IV. METODE PENELITIAN  
Metode  yang  digunakan  dalam  penelitian  ini  terbagi 
menjadi  metode  pengumpulan  data  dan  metode  analisis 
sebagai berikut: 

1.  Metode Pengumpulan Data 

a.  Studi Pustaka 

Peneliti  mencari  dan  mempelajari  studi  pustaka 
yang  berkaitan  dengan  penelitian  melalui  media 
pembelajaran  seperti  buku,  jurnal,  dan  publikasi 
BPS. 

b.  Data Sekunder 

berupa 

variabel-variabel 

Metode  ini  digunakan  untuk  mendapatkan  data 
sekunder 
yang 
dibutuhkan  untuk  menghitung  IHPP.  Variabel-
tersebut  didapat  dari  hasil  scraping  
variabel 
(rumah123.com)  oleh 
website  situs  properti 
Subdirektorat  Pengembangan  Model  Statistik 
BPS. 
c.  Dokumen 
Dokumen 
dari 
Subdirektorat  Statistik  Harga  Perdagangan  Besar, 
hasil  publikasi  BPS,  dan 
lainnya. 
Dokumen  tersebut  seperti  Buku  Pedoman  Survei 
Harga  Properti  Perumahan  2019  dan  RPPI 
(Residential  Property  Price  Indices)  Practical 
Compilation Guide 2020. 

digunakan 

sumber 

berasal 

yang 

Tabel I berisikan beberapa penelitian yang telah melakukan 
pendekatan  Big  Data  untuk  Official  Statistics.  Wijaya  & 
Mariyah  (2019)  melakukan  scraping  data  beberapa  online 
retail di Indonesia dan melakukan penghitungan Indeks Harga 
Konsumen  (IHK)  menggunakan  data  tersebut.  Hasil  IHK 
berbasis  e-commerce  dilakukan  perbandingan  dengan  IHK 
BPS.  Hasil  penelitian  menunjukkan  penghitungan  IHK 
berbasis  e-commerce  dapat  dilakukan  dengan  pendekatan 
penghitungan BPS. Rachman (2019) memanfaatkan Big Data 
pada situs properti di Indonesia dan melakukan penghitungan 
Indeks  Harga  Properti  Residensial  Alternatif  (IHPRA)  untuk 
pasar  sekunder.  Metode  penghitungan  indeks  menggunakan 
Metode  Hedonik. 
tersebut  menghasilkan 
kesimpulan  bahwa  IHPRA  berbasis  situs  properti  dapat 
dilakukan.  Namun,  lokus  penelitian  terbatas  di  daerah  DKI 
Jakarta saja dan belum memperluas di kota-kota lain. Hill, dkk 
(2020) membahas Metode Hedonik Rolling Time Dummy dan 
terbukti 
memodifikasinya.  Metode 

digunakan 

Penelitian 

yang 

d.  Wawancara 

narasumber 

Wawancara  dilakukan  oleh  peneliti  dengan 
Pegawai 
beberapa 
Subdirektorat  Statistik  Harga  Perdagangan  Besar 
dan  Pegawai  BPS  yang  tergabung  Subdirektorat 
Pengembangan Model Statistik BPS. 

seperti 

2.  Metode Analisis 

Metode  analisis  dalam  penelitian  ini  dapat  diringkas 
seperti dalam Gambar 3. 

 3 / 8 

 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

1)  Menghitung 

relatif  harga 
menurut tipe untuk masing-masing kota [3] 

(RH) 

rumah 

𝑅𝐻𝑡(𝑖,𝑗,𝑘) =  

𝑃𝑡(𝑖,𝑗,𝑘)
𝑃𝑡−1(𝑖,𝑗,𝑘)

 𝑥 100 

Keterangan: 
𝑅𝐻𝑡(𝑖,𝑘)  =  relatif  harga  pada  triwulan  ke-t, 
kota ke-i, model ke-j, tipe ke-k 
𝑃𝑡(𝑖,𝑘) 
kota ke-i, model ke-j, tipe ke-k 
𝑃𝑡−1(𝑖,𝑘)  = harga rumah pada triwulan ke-(t-
1), kota ke-i, model ke-j, tipe ke-k 

=  harga  rumah  pada  triwulan  ke-t, 

2)  Menghitung  relatif  harga  (RH)  apartemen 
menurut tipe untuk masing-masing kota [3] 

𝑅𝐻𝑡(𝑖,𝑘) =  

𝑃𝑡(𝑖,𝑘)
𝑃𝑡−1(𝑖,𝑘)

 𝑥 100 

Keterangan: 
𝑅𝐻𝑡(𝑖,𝑘)   =  relatif  harga  pada  triwulan  ke-t, 
kota ke-i, tipe ke-k 
𝑃𝑡(𝑖,𝑘)  
ke-t, kota ke-i, tipe ke-k 
𝑃𝑡−1(𝑖,𝑘)   =  harga  apartemen  pada  triwulan 
ke-(t-1), kota ke-i, tipe ke-k 

=  harga  apartemen  pada  triwulan 

Dalam  penelitian 
ini  dilakukan  modifikasi 
penghitungan  IHPP  dari  formula  yang  telah  ada. 
Hal  ini  dilakukan  karena  keterbatasan  variabel 
data  hasil  scraping  yang  didapatkan.  Dari  data 
hasil  scraping,  variabel  dengan  keterangan  berisi 
model  rumah  yaitu  model  rumah  cluster  tidak 
berpagar,  model  rumah  cluster  berpagar,  dan 
model  rumah  non  cluster  yang  digunakan  untuk 
menghitung  IHPP  BPS,  tidak  tersedia.  Sehingga 
dalam  penghitungan  IHPP  ini,  model  rumah 
diasumsikan memiliki model yang sama. 
Setelah  dilakukan  penghitungan  menggunakan 
Metode  Matched  Model,  penghitungan  IHPP 
selanjutnya  dilakukan  dengan  Metode  Hedonik. 
Ada  beberapa  metode  untuk  membangun  indeks 
harga  hedonik.  Dalam  penelitian  ini  yang  akan 
digunakan  adalah  Metode  Time-Dummy.  Berikut 
merupakan formulanya [11]: 

𝐶

𝑇

𝑙𝑛 𝑙𝑛 𝑝ℎ =   ∑

𝛽𝑐𝑧ℎ𝑐 +  ∑

𝛿𝑡𝑑ℎ𝑡 +   𝜀ℎ𝑡  

𝑐=1

𝑡=2

Indeks  harga  untuk  periode  t  relatif  terhadap 
periode 1 kemudian dihitung sebagai berikut [11]: 

𝑃𝑡
𝑃1

= 𝑒𝑥𝑝 (𝛿̂𝑡) 

 4 / 8 

Gambar 3. Flowchart Tahapan Metode Analisis 

a.  Data Hasil Scraping 

Proses  analisis  diawali  dengan  scraping  data  dari 
situs  properti  yaitu  website  rumah123.com.  Pada 
tahap ini, peneliti tidak melakukan proses scraping 
sendiri,  proses  scraping  sudah  dilakukan  oleh 
Subdirektorat Pengembangan Model Statistik BPS. 

b.  Data Preprocessing 

Data  yang  sudah  didapat  dilakukan  preprocessing 
agar  didapatkan  kualitas  data  yang  bagus.  Pada 
proses  ini  terbagi  menjadi  penyaringan  variabel, 
pengecekan  tipe  data,  pengecekan  missing  value, 
pengecekan  duplikat,  dan  pengecekan  data  tidak 
wajar. 

c.  Penghitungan IHPP 

yang 

preprocessing 
sudah  melewati 
Data 
kemudian  dilakukan  penghitungan  IHPP  dengan 
menggunakan  Metode  Matched  Model  seperti 
yang  digunakan  oleh  BPS.  Indeks  Harga  Properti 
Perumahan  dengan  Metode  Matched  Model 
dihitung  menggunakan  formulasi  indeks  harga 
pada  umumnya.  Indeks  harga  rumah/apartemen 
pada  masing-masing  tipe  di  tiap  kota  dihitung 
tanpa menggunakan bobot. Secara umum, formula 
penghitungannya adalah sebagai berikut: 

 
 
 
 
 
 
 
 
d.  Analisis 

Pada  tahap  ini  dilakukan  analisis  mengenai  IHPP 
pendekatan  data  situs  properti  dan  IHPP  milik 
BPS.  Setelah  IHPP  dihitung,  dilakukan  akurasi 
menggunakan  MAPE.  MAPE  (Mean  Absolute 
Percent  Error)  merupakan  perhitungan  yang 
digunakan  untuk  menghitung  rata-rata  persentase 
kesalahan mutlak, dengan rumus [12]: 

𝑀𝐴𝑃𝐸 =   ∑

(

|𝐴𝑘𝑡𝑢𝑎𝑙 − 𝐹𝑜𝑟𝑒𝑐𝑎𝑠𝑡|
𝐴𝑘𝑡𝑢𝑎𝑙
𝑛

)  𝑥 100

MAPE (Mean Absolute Percent Error) digunakan 
jika  ukuran  variabel  peramalan  merupakan  faktor 
penting  dalam    mengevaluasi    akurasi    peramalan  
tersebut. MAPE  memberikan  petunjuk  seberapa  
besar    kesalahan  peramalan  dibandingkan  dengan 
nilai sebenarnya [12]. 

V.  KERANGKA PIKIR 

Kerangka  pikir  yang  digunakan  dalam  penelitian  ini  dapat 

dilihat pada Gambar 4. 

Gambar 4. Kerangka Pikir Penelitian 

Berdasarkan  Gambar  4,  penelitian  ini  bertujuan  untuk 
menghitung  IHPP  dari  pendekatan  data  situs  properti.  Untuk 
mencapai 
tersebut  dilakukan  prosedur  penelitian 
dengan tahapan Studi Pustaka dan Pengumpulan Data melalui 

tujuan 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

scraping  website  rumah123.com.  Data  yang  dipilih  sesuai 
dengan  variabel  yang  dibutuhkan  dalam  penelitian.  Data 
mentah  dari  hasil  scraping  tersebut  kemudian  dilakukan 
preprocessing  agar  sesuai  untuk  dilakukan  penghitungan 
IHPP.  Data  hasil  preprocessing  selanjutnya  dilakukan 
penghitungan  IHPP  menggunakan  Metode  Matched  Model 
dan  Metode  Hedonik.    Hasil  penghitungan  yang  diperoleh 
kemudian dianalisis dan dievaluasi. 

VI. HASIL DAN PEMBAHASAN 

A.  Scraping Data 

Memanfaatkan  data  hasil 

scraping  pada  website 
rumah123.com  yang  sudah  dilakukan  oleh  Subdirektorat 
Pengembangan Model Statistik BPS. Data hasil scraping yang 
digunakan adalah data dari Juli 2019 sampai Desember 2019. 
Dengan  wilayah  hanya  pada  Pulau  Jawa  yaitu  Provinsi  Jawa 
Barat,  DKI  Jakarta,  Jawa  Timur,  Banten,  Jawa  Tengah,  dan 
Daerah  Istimewa  Yogyakarta.  Variabel  yang  digunakan 
berupa  tipe  properti,  alur  properti,  harga  properti,  tanggal 
properti  diiklankan,  lokasi  properti  (provinsi  dan  kabupaten), 
jumlah  kamar  tidur,  jumlah  kamar  mandi,  luas  tanah,  luas 
bangunan,  dan  kondisi  properti.  Peneliti  mengolah  data  hasil 
scraping  menggunakan  RStudio.  Data  tersebut  berupa  csv 
yang terbagi antara data dari  Juli  – September 2019 dan data 
dari Oktober – Desember 2019.  

B.  Data Preprocessing 

Pembersihan  data  diawali  dengan  memberi  atribut  baru 
yang  bernama  triwulan  untuk  membedakan  triwulan  dari 
kedua  data.  Data  dari  Juli  –  September  2019  merupakan 
triwulan  tiga  dan  data  dari  Oktober  –  Desember  2019 
merupakan  triwulan  empat.  Kemudian  peneliti  mengambil 
data  yang  merupakan  rumah/apartemen  yang  dijual  dengan 
kondisi  yang  masih  baru.  Kedua  data  yang  awalnya  terbagi 
tiga  dan  empat  kemudian  peneliti 
menjadi 
menggabungkannya  menjadi  satu  data.  Peneliti  melakukan 
pengecekan untuk tipe atributnya.  

triwulan 

Setelah dilakukan pengecekan tipe data, terlihat bahwa ada 
beberapa  atribut  yang  tipenya  tidak  sesuai  sehingga  harus 
peneliti ganti. Seperti atribut kamar tidur yang pada data berisi 
numeric namun tipenya malah factor.  

Kemudian  peneliti  memisahkan  kembali  data  tadi  menjadi 
dua data berdasarkan tipe properti. Data pertama  dengan tipe 
tipe  properti 
properti  rumah  dan  data  kedua  dengan 
apartemen.  Ketika  dilakukan  pengecekan  summary  untuk 
kedua  data,  atribut  land_area  pada  data  apartemen  berisikan 
NA semua, sehingga atribut tersebut tidak digunakan.  

Dari  pengecekan  summary  masih  ada  kolom  yang  kosong. 
Hal  tersebut  berarti  ada  nilai  yang  hilang,  di  mana  iklan 
properti  tidak  memberikan  data  tentang  harga,  jumlah  kamar 
mandi,  jumlah  kamar  tidur,  luas  bangunan,  atau  luas  tanah. 
Peneliti  mengubah  kolom  kosong  tersebut  menjadi  NA  dan 
peneliti  menghilangkan  data  yang  NA  tersebut  karena  data 
yang missing tidak akan dapat digunakan.  

 5 / 8 

 
 
 
 
 
 
 
 
 
 
 
Setelah  nilai  NA  dihilangkan,  kemudian  peneliti 
menghilangkan data yang mengandung duplikat. Suatu rumah 
atau 
atau  apartemen  diasumsikan  merupakan 
apartemen yang sama apabila kabupaten, harga, jumlah kamar 
tidur,  jumlah  kamar  mandi,  luas  bangunan,  dan  luas  tanah 
memiliki nilai yang sama.  

rumah 

Apabila  data  duplikat  sudah  dihilangkan,  selanjutnya 
peneliti  menghilangkan  data  yang  ganjil  atau  tidak  normal. 
Penelitian  ini  menggunakan  beberapa  kriteria  agar  suatu  data 
properti dapat dianggap sebagai data tidak normal: 

a.  Rasio Luar Biasa 

Misalnya jumlah kamar tidur yang melebihi luas tanah 
dan  luas  bangunan.  Contohnya  pada  data  ditemukan 
ada properti yang luas bangunan seluas 1 m2 dan luas 
tanah 4.000 m2 dengan jumlah kamar tidur 30 buah. 

b.  Luas Tidak Biasa 

Misalnya luas bangunan lebih besar dari koefisien luas 
bangunan.  Contohnya  pada  data  ditemukan  ada 
properti yang luas tanah 260 m2 dan luas bangunannya 
260.378 m2. 
c.  Kamar Tidak Biasa 

Misalnya  jumlah  luas  kamar  mandi  dan  kamar  tidur 
lebih  besar  dari  luas  bangunan.  Contohnya  pada  data 
ditemukan  ada  properti  yang  memiliki  jumlah  kamar 
tidur  sebanyak  30  dan  jumlah  kamar  mandi  sebanyak 
931 dengan luas bangunan hanya 550 m2. 

d.  Harga Tidak Biasa 

Misalnya  harga  properti  lebih  kecil  dari  harga  yang 
seharusnya  dari  luas  tanah  dan  luas  bangunan  yang 
ada. Contohnya pada data ditemukan ada properti yang 
memiliki  luas  tanah  2.450  m2  dan  luas  bangunan  12 
m2 namun harganya hanya 170 juta. 

Gambar 5 merupakan sebaran data ganjil atau tidak normal 

yang akan dilakukan penghapusan. 

       a 

     b 

Gambar 5. Sebaran Data Ganjil pada Hasil Scraping rumah123.com   

Dari Gambar 5a terlihat bahwa untuk data properti rumah, 
data  ganjil  terbanyak  ada  pada  kategori  Luas  Tidak  Biasa 
sebanyak 77%. Kemudian dari Gambar 5b untuk data properti 
apartemen,  data  ganjil  terbanyak  ada  pada  kategori  Harga 
Tidak Biasa sebanyak 87%. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

TABEL II 
Rekaman Data 

 Tabel II menyajikan catatan data untuk tipe properti rumah 
dan  apartemen.  Setelah  dilakukan  preprocessing  yaitu 
penyaringan  data,  penghapusan  data  missing,  penghapusan 
data  duplikat,  dan  penghapusan  data  tidak  wajar,  terlihat 
hanya sekitar 1,68 persen total data yang digunakan dari data 
mentah hasil scraping website rumah123.com. 

C.  Penghitungan IHPP 

Pertama adalah menghitung IHPP dengan Metode Matched 
Model.  Dalam  menentukan  tipe,  penelitian  ini  menggunakan 
asumsi  bahwa  ketika  suatu  rumah  memiliki  luas  tanah,  luas 
bangunan,  kamar  tidur,  dan  kamar  mandi  yang  sama,  maka 
dianggap  memiliki  tipe  rumah  yang  sama.  Ketika  suatu 
apartemen  memiliki  luas  bangunan,  kamar  tidur,  dan  kamar 
mandi  yang  sama,  maka  dianggap  memiliki  tipe  apartemen 
yang sama. Setelah ditentukan tipenya, peneliti membagi data 
rumah 
untuk 
memudahkan penghitungan.  

berdasarkan 

apartemen 

kotanya 

dan 

Setelah  terbagi  datanya,  terlihat  bahwa  untuk  properti 
rumah,  kota  dengan  jumlah  observasi  di  bawah  30  adalah 
Tegal  sebanyak  18  dan  Banyumas  sebanyak  2.  Kemudian 
untuk  properti  apartemen,  kota  dengan  jumlah  observasi  di 
bawah  30  adalah  Tegal  sebanyak  0,  Surakarta  sebanyak  4, 
Banyumas  sebanyak  0,  Sukoharjo  sebanyak  1,  dan  Serang 
sebanyak  3.  Untuk  itu,  kota  tersebut  tidak  dapat  dihitung 
indeksnya.  Pada  data  BPS,  terdapat  kota-kota  yang  nilai 
indeksnya  tidak  ada,  kota-kota tersebut  untuk  properti  rumah 
adalah  Jakarta,  Bekasi,  Surakarta,  dan  Sukoharjo.  Kemudian 
untuk  apartemen  adalah  Jakarta,  Bekasi,  Tegal,  Surakarta, 
Banyumas,  Daerah  Istimewa  Yogyakarta,  Malang,  dan 
Serang.  Sehingga  untuk  kota 
tidak  dapat 
dibandingkan indeksnya.  

tersebut 

Dalam penghitungan IHPP menggunakan Metode Matched 
Model,  peneliti  membuat  sebuah  fungsi  pada  Rstudio  untuk 
menghitungnya  sesuai  dengan  rumus  yang  ada.  Peneliti 
membuat  dua  penghitungan  IHPP  pendekatan  situs  properti. 
Yaitu  tanpa  diberi  perlakuan  dan  dengan  diberi  perlakuan. 
Penghitungan  IHPP  dengan  perlakuan  di  sini  merupakan 
penghitungan 
indeks  dilakukan 
penghapusan. 

IHPP  ketika  outlier 

Metode  yang  dilakukan  selanjutnya  adalah  Metode 
Hedonik  dengan  Metode  Time-Dummy.  Model  Hedonik  ini 
memiliki  variabel  luas  tanah,  luas  bangunan,  jumlah  kamar 
tidur, dan jumlah kamar mandi sebagai variabel karakteristik. 
Jumlah  kamar  tidur  dan  jumlah  kamar  mandi  diperlakukan 
sebagai  variabel dummy. Dengan rincian untuk jumlah kamar 
tidur  adalah  1  sampai  2  kamar,  3  kamar,  dan  lebih  dari  4 
kamar. Untuk jumlah kamar  mandi adalah 1 kamar, 2 kamar, 
dan lebih dari 3 kamar. 

Seperti  yang  disebutkan  sebelumnya  bahwa  ada  beberapa 
kota  dengan  jumlah  observasi  tidak  dapat  untuk  menghitung 
indeks  dan  ada  indeks  IHPP  hasil  penghitungan  BPS  yang 

 6 / 8 

 
 
 
         
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

kosong, sehingga kota yang dihitung IHPP-nya untuk properti 
rumah  adalah  Bandung,  Semarang,  Daerah 
Istimewa 
Yogyakarta,  Surabaya,  Malang,  Serang,  dan  Tangerang. 
Gambar  6  memperlihatkan  IHPP  Rumah  Pendekatan  Situs 
Properti dan IHPP BPS. 

D.  Analisis 

Tabel III  merupakan  hasil IHPP Pendekatan Situs Properti 

pada rumah dan apartemen dengan akurasinya. 

TABEL III 
Hasil Penghitungan IHPP Rumah dan Tingkat Akurasinya 

TABEL IV 
Hasil Penghitungan IHPP Apartemen dan Tingkat Akurasinya 

Gambar 6. IHPP Rumah Pendekatan Situs Properti dan IHPP BPS 

  Secara  umum,  dari  Gambar  6  dapat  dilihat  bahwa  indeks 
melalui  pendekatan  situs  properti  dengan  Metode  Matched 
Model lebih tinggi dari  indeks  milik BPS. Sedangkan dengan 
Metode Hedonik, indeksnya hampir sama. Namun untuk kota 
Malang  dan  Serang,  indeksnya  cukup  berbeda.  Selanjutnya 
kota yang dihitung IHPP-nya untuk properti apartemen adalah 
Bandung, Semarang, Surabaya, dan Tangerang.  

Diketahui  bahwa  semakin  rendah  nilai  MAPE,  maka 
semakin  akurat  nilai  yang  dihitung  dari  nilai  sebenarnya. 
Berdasarkan  Tabel  III,  terlihat  bahwa  untuk  properti  rumah, 
nilai  MAPE  terkecil  merupakan  IHPP  Pendekatan  Situs 
Properti  Metode  Hedonik  sebesar  2,69  persen.  Begitu  juga 
berdasarkan  Tabel  IV  untuk  properti  apartemen,  nilai  MAPE 
terkecil  ada  pada  IHPP  Pendekatan  Situs  Properti  Metode 
Hedonik sebesar 4,75 persen. 

VII. 

PENUTUP 

Gambar 7. IHPP Apartemen Pendekatan Situs Properti dan IHPP BPS 

A.  Kesimpulan 

Gambar  7  memperlihatkan  IHPP  Apartemen  Pendekatan 
Situs Properti dan IHPP BPS. Dapat dilihat bahwa untuk kota 
Surabaya  dan  Tangerang,  nilai  indeksnya  hampir  sama. 
Sementara  itu  untuk  kota  Bandung  dan  Semarang,  hasil 
penghitungan  indeks  dengan  Metode  Matched  Model  lebih 
kecil  dari  indeks  milik  BPS  dan  Metode  Hedoniknya  lebih 
tinggi. 

1. 

2. 

IHPP  di  Indonesia  berdasarkan  hasil  scraping  dari 
website rumah123.com pada level kota dengan Metode 
sudah  dapat  dilakukan.  Hasil 
Matched  Model 
penghitungan  menunjukkan  bahwa  nilai  IHPP  secara 
umum lebih tinggi dari IHPP BPS. 
IHPP  di  Indonesia  berdasarkan  hasil  scraping  dari 
website rumah123.com pada level kota dengan Metode 
Hedonik  sudah  dapat  dilakukan.  Hasil  penghitungan 
menunjukkan  bahwa  nilai  IHPP  secara  umum  hampir 
sama dengan nilai IHPP hasil pengitungan BPS. 

3.  Nilai  akurasi  indeks  yang  dihitung  dengan  Metode 
Matched  Model  Tanpa  Perlakuan,  Metode  Matched 
Model  Dengan  Perlakuan,  dan  Metode  Hedonik  untuk 
properti rumah dan apartemen, nilainya lebih kecil dari 

 7 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

10  persen,  sehingga  penghitungan 
dikatakan memiliki kemiripan.  

indeks  dapat 

B.  Saran 

1.  Hasil  penelitian  menunjukkan  bahwa  BPS  dapat 
mempertimbangkan  penggunaan  data  situs  properti 
sebagai  salah  satu  sumber  penerapan  dalam  Official 
Statistics.  Terutama  dalam  pendekatan  menghitung 
Indeks Harga Properti Perumahan. Seperti untuk kota-
kota yang mempunyai kendala saat dihitung IHPP-nya. 
Kota-kota lain yang belum masuk dalam penghitungan 
IHPP  BPS,  dapat  didekati  dengan  kota  yang  datanya 
ada dalam situs properti. 

2.  Mengingat terbatasnya variabel yang peneliti dapatkan 
dari  data  hasil  scraping,  pada  penelitian  selanjutnya 
lain  (seperti  variabel 
dapat  ditambahkan  variabel 
jumlah  unit  yang  dibangun  pada  tiap  properti)  untuk 
didapatkan hasil yang lebih baik. 

DAFTAR PUSTAKA 
[1]  Kompas. (2016, 9). Hingga 2025 Kebutuhan Rumah di Indonesia Tembus 
30 
Available: 
https://money.kompas.com/read/2016/09/17/195151226/hingga.2025.keb
utuhan.rumah.di.indonesia.tembus.30.juta.unit. 

[Online]. 

Unit. 

Juta 

[2]  Kompas.  (2020,  7).  Sektor  Properti  Sumbang  Perekonomian  Jakarta 
Available: 
Rp32,3 
https://properti.kompas.com/read/2020/07/10/070000521/sektor-properti-
sumbang-perekonomian-jakarta-rp-32-3-triliun?page=all. 

[Online]. 

Triliun. 

[3]  Badan Pusat Statistik, Pedoman Survei Harga Properti Perumahan 2019. 

Jakarta: BPS, 2019. 

[4]  Badan Pusat Statistik, Aktivitas Badan Pusat Statistik 2019. Jakarta: BPS, 

2019. 

[5]  International Monetary Fund, Residential Property Price Index Practical 

Compilation Guide. Washington DC: IMF, 2020. 

[6]  Kunovac, Davor, dkk (2008). “Use  of the  Hedonic Method  to Calculate 
an  Index  of  Real  Estate  Prices  in  Croatia”.  Working  Papers.  Croatian 
National Bank, Republic of Croatia. 

[7]  Hammer,  Cornelia  L.,  D.  C.  Kostroch,  G.  Quirós,  and  STA  Internal 
Group  (2017).  “Big  Data:  Potential,  Challenges,  and  Statistical 
Implications”. IMF Staff Discussion Note, SDN/17/06, IMF, Washington 
DC. 

[8]  Property  and  the  City.  (2016,  6).  5  Top  Portal  Properti.  [Online]. 
Available: https://propertyandthecity.com/5-top-portal-properti-2. 
[9]  Wijaya,  A.  and  Mariyah,  S.  (2019).  “Study  of  Consumer  Price  Index 
based  on  E-Commerce  in  Indonesia”.  Asia-Pasific  Economic  Statistics 
Week. UN ESCAP. Bangkok, Thailand. 

[10] Rachman, A. (2019). “An Alternative Hedonic Residential Property Price 
Index for Indonesia Using Big Data: The Case of Jakarta”. International 
Conference on Real Estate Statistics. Eurostat, Luxembourg.  

[11] J. Hill, Robert, dkk (2020). “Rolling-Time-Dummy House Price Indexes: 
Window  Length,  Linking  and  Options  for  Dealing  With  the  Covid-19 
Shutdown”.  Graz  Economics  Papers.  Department  of  Economics, 
University of Graz, Austria. 

[12] J.H.Barus,  Ramli,  “Analisis  Peramalan  Ekspor  Indonesia  Pasca  Krisis 
Keuangan Eropa Dan Global Tahun 2008 dengan Metode Dekomposisi”, 
Jurnal Ekonomi dan Keuangan, vol. 1, no. 3, pp.117–133. 

 8 / 8 

 
 
 
"
221709559,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Penerapan Blockchain pada Pelacakan Alur
Pemrosesan Data Survei atau Sensus

Anggita Aufa Fakhri Zaidan (221709559, 4SD2)

Dosen Pembimbing:  Takdir, SST., M.T

Ringkasan— Permasalahan nonrespons pada survei/sensus
yang dilakukan BPS menuntut dilakukannya satu upaya
peningkatan kepercayaan melalui identifikasi petugas. Namun,
dari segi teknis situs resmi sensus masih tersentralisasi dan
masih belum dapat disajikannya transparansi
informasi
terkendala.
membuat upaya peningkatan kepercayaan ini
Blockchain merupakan teknologi basis data terdesentralisasi dan
tidak dapat dirusak sehingga lebih aman. Tahapan penelitian ini
mengacu pada metode DSRM. Perancangan sistem dilakukan
dengan mengidentifikasi sistem berjalan, merancang proses
sistem dengan menerapkan blockchain, menganalisis
bisnis
kebutuhan sistem, dan mendesain sistem menggunakan UML.
Penelitian
alur
pemrosesan data survei atau sensus dengan memanfaatkan
teknologi blockchain. Fitur yang berhasil dikembangkan adalah
input berkas informasi petugas oleh BPS dan pencarian berkas
informasi petugas untuk seluruh stakeholder yang terlibat.
Pemanfaatan teknologi
ini dapat meningkatkan kepercayaan
responden. Smart contract berhasil diimplementasikan dan
berfungsi dengan baik. Berdasarkan hasil pengujian sistem
menggunakan black box testing menunjukan kinerja sistem telah
sesuai dengan kebutuhan fungsional.

ini menghasilkan

rancangan

pelacakan

Kata

Kunci— Blockchain, Alur

Pemrosesan Data

Survei/Sensus, Sistem Pelacakan.

I. LATAR BELAKANG

Berdasarkan undang-undang Nomor 16 Tahun 1997 tentang
Statistik, salah satu peranan yang harus dijalankan Badan
Pusat Statistik (BPS)
adalah menyediakan data bagi
pemerintah dan masyarakat. Data ini didapatkan dari sensus
atau survei yang dilakukan sendiri dan juga dari departemen
atau lembaga pemerintahan lainnya sebagai data sekunder [1].
Data menjadi satu hal yang esensial di masa sekarang. Hal ini
didukung oleh pernyataan Presiden RI, Joko Widodo, bahwa
data memiliki nilai penting dalam pembangunan [2]. Melihat
pentingnya data, maka menjadi penting pula untuk menjaga
kualitas dari data yang dihasilkan. Data yang berkualitas dapat
diartikan juga sebagai kualitas survei [3].

Salah satu indikator yang dapat menyatakan kualitas survei
adalah rendahnya tingkat nonrespons. Sebab, hal ini akan
memengaruhi keterwakilan data pada saat pengambilan
kesimpulan dari sebuah survei, serta menyebabkan pendugaan
yang bias [4]. Kesalahan nonrespons dapat disebabkan oleh
beberapa sumber kesalahan, dua diantaranya yaitu unable to
answer, yakni ketika responden tidak memiliki atau tidak
memberi informasi dari pertanyaan yang diajukan dan the
hard core, yakni ketika responden dengan tegas menolak
untuk diwawancarai [5]. Kedua sumber tersebut tergolong
dalam permasalahan kepercayaan. Referensi [6] menunjukan
bahwa faktor yang signifikan meningkatkan tingkat respons

dapat

didefinisikan

ialah dengan meningkatkan kepercayaan responden kepada
BPS. Kepercayaan
suatu
keyakinan bahwa pernyataan pihak lain dapat diandalkan
untuk memenuhi kewajibannya. Rasa percaya atau tidak
percaya seseorang yang muncul dalam perilakunya ditentukan
dan
oleh
pengendalian [7].

faktor-faktor

informasi,

pengaruh,

sebagai

seperti

yang

Salah

dilakukan

dapat
responden

upaya
kepercayaan

dalam
satu
meningkatkan
ialah melalui
peningkatan keamanan data. Hal ini didukung oleh referensi
[8] bahwa persepsi keamanan dapat berpengaruh positif
terhadap kepercayaan. Saat ini, upaya yang dilakukan BPS
untuk meningkatkan kepercayaan responden untuk menjamin
keamanan data saat pelaksanaan survei/sensus ialah melalui
identifikasi petugas. Identifikasi ini dapat dilakukan dari segi
fisik dengan mengenakan atau identitas petugas seperti rompi,
tas punggung, tanda pengenal dan surat tugas. Sedangkan, dari
segi teknis adalah dengan pengecekkan petugas melalui nomor
identitas petugas yang bisa dilacak di situs resmi sensus
tersebut. Nomor ini berisikan beberapa atribut identitas seperti
foto, nama, kode, dan wilayah tugas petugas [9]. Akan tetapi,
sistem yang diterapkan dalam situs resmi sensus masih
tersentralisasi, yakni basis data disimpan pada server terpusat.
lembaga yang melakukan
pengumpulan data baik survei maupun sensus, masih belum
dapat menyajikan transparansi informasi lengkap terkait alur
pemrosesan data yang diberikan responden dari awal hingga
akhir publikasi. Padahal, transparansi merupakan salah satu
upaya meningkatkan integritas, di mana integritas merupakan
salah satu inti dari keamanan data.

Selain itu, BPS sebagai

Beberapa tahun terakhir telah bermunculan perkembangan
teknologi terkait pengamanan sistem, salah satunya adalah
teknologi blockchain sebagai sistem yang tahan penipuan dan
peretasan [10]. Dalam istilah sederhana, blockchain dapat
digambarkan sebagai basis data yang terdesentralisasi, tanpa
memerlukan adanya kepercayaan antar peserta dalam
bertransaksi di suatu sistem. Isi dari blok tersebut dikelola
sebagai daftar blok dengan transaksi yang terurut. Setiap blok
pada blockchain akan terhubung dengan blok sebelumnya
melalui hash sehingga transaksi dalam blockchain tidak dapat
diubah atau dihapus tanpa mengubah keseluruhan isi dari
blockchain
blockchain
diantaranya:
1.
2. Desentralisasi, tidak diperlukan pihak ketiga.
3. Konsensus, sebuah algoritma yang digunakan dalam
teknologi blockchain untuk menjaga konsistensi data
dalam jaringan terdistribusi.

Immutable, tidak dapat dihapus atau disunting.

[11]. Keunggulan

penerapan

1 / 8

4. Transparan, setiap peserta dapat melihat semua
ini membuat
pada

blockchain. Hal

blockchain

penerapan

pada

transaksi
keunggulan
ketertelusuran.

Saat ini, blockchain sudah diterapkan pada mata uang digital
layaknya bitcoin, pada manajemen rantai pasok blockchain
terbukti terpercaya membangun rantai pasokan makanan yang
transparan [12]. Berdasarkan keunggulan yang dimiliki
blockchain, maka blockchain dapat menjadi solusi dari
permasalahan di atas.

Namun, hingga saat ini belum terdapat penelitian penerapan
blockchain pada sebuah survei atau sensus. Oleh karena itu,
penelitian ini akan berfokus pada perancangan penerapan
blockchain pada alur pemrosesan survei atau sensus sebagai
upaya peningkatan kepercayaan responden kepada BPS.

II. TUJUAN PENELITIAN

Adapun tujuan dari penelitian ini adalah merancang sistem
pelacakan alur pemrosesan data pada survei atau sensus
dengan menerapkan teknologi blockchain.

III. PENELITIAN TERKAIT

Referensi [13] melakukan penelitian berisikan mengenai
perencanaan, sistem dan desain sistem. Pertama, Perencanaan
sistem dengan menerapkan blockchain ditujukan untuk
digunakan setiap anggota dalam rantai pasok kakao. Lalu,
tentang analisis kebutuhan dari segi
analisis sistem berisi
input, output,
tujuan, kendala, dan para stakeholder yang
berperan di rantai pasok kakao. Terakhir, dalam mendesain
sistem digunakan Unified Modelling Language (UML) untuk
memodelkan proses bisnis sistemnya,
jenis diagram yang
digunakan ialah Use Case Diagram dan Activity Diagram.
Kesimpulan dari penelitian tersebut ialah sistem blockchain
dapat diaplikasikan ke dalam jaringan rantai pasok, sehingga
alur informasi lebih efisien dari sebelumnya.

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

C. Perancangan dan Pengembangan

yang

beserta

aktor-aktor

Dalam tahap ini dilakukan beberapa proses. Pertama,
identifikasi sistem berjalan. Identifikasi ini berkaitan
dengan alur pemrosesan survei atau sensus yang sudah
ada
terlibat. Kedua,
perancangan sistem dengan menerapkan teknologi
blockchain. Ketiga, identifikasi kebutuhan. Identifikasi
kebutuhan
daftar
kebutuhan sistem yang selanjutnya dapat digunakan
dalam proses perancangan sistem. Keempat, mendesain
sistem dengan membentuk use case diagram untuk
melakukan penentuan akses kontrol (CRUD) terhadap
para aktor yang terlibat serta arsitektur sistem. Kelima,
merancang antarmuka sistem.
Implementasi

untuk mendapatkan

diperlukan

D.

Berdasarkan perancangan yang telah dibuat, untuk
menyusun solusi berdasarkan identifikasi masalah dan
tujuan
dengan
melakukan coding. Dalam proses pengimplementasian
sistem, diperhatikan juga spesifikasi perangkat yang
diperlukan untuk membangun sistem.

sistem dilakukan

implementasi

E. Evaluasi

testing

dilakukan melalui

Evaluasi bertujuan untuk menguji kesesuaian hasil
yang diperoleh dalam menyelesaikan permasalahan
pengujian
penelitian. Evaluasi
black-box
untuk pengujian fungsionalitas.
Pengujian ini melakukan evaluasi melalui pengecekan
izin aktivitas (CRUD) tiap aktor dan melihat apakah
izin
setiap aktor melakukan fungsi CRUD sesuai
aktivitasnya.
F. Komunikasi

Komunikasi hasil penelitian dilakukan melalui
penulisan skripsi. Sebelum menghasilkan buku skripsi
yang bisa dibaca publik, peneliti harus menyelesaikan
beberapa tahapan, yaitu seminar dan sidang skripsi.

IV. METODE PENELITIAN

V. KERANGKA PIKIR

Metode penelitian yang digunakan dalam penelitian ini
adalah Design Science Research Methodology (DSRM) yang
terdiri dari enam aktivitas yakni identifikasi masalah dan
motivasi,
dan
pengembangan, implementasi, evaluasi, dan komunikasi [14].
Berikut merupakan tahapan dari metode yang digunakan
dalam penelitian ini.
A.

Identifikasi Masalah dan Motivasi

perancangan

menentukan

tujuan,

Identifikasi masalah dapat dilakukan dengan studi
pustaka yang bersumber dari penelitian terkait, seperti
jurnal,
tesis, dan lain-lain. Selain itu, melalui studi
pustaka sistem yang menerapkan blockchain juga dapat
mempelajari bahasa-bahasa pemrograman penunjang.
lain dari proses identifikasi masalah yaitu
Manfaat
memotivasi peneliti dan pembaca untuk menerima solusi
dan hasil penelitian yang dilakukan.

B. Menentukan Tujuan

Setelah didapatkan pengidentifikasian masalah pada
tahap sebelumnya, maka selanjutnya ditentukan tujuan
penelitian. Tujuan penelitian ini bersifat rasional dan
berupa desain riset.

Kerangka pikir pada penelitian ini ditujukan pada Gambar 1
enam bagian, yaitu problems, approach,
terdiri dari
development,
implementation, measurement, dan result.
Pertama, penulis mencari permasalahan pada sistem survei
atau sensus yang sudah berjalan. Kemudian, penulis
melakukan kajian teori terkait pendekatan untuk membangun
sistem informasi pelacakan data berbasis blockchain dan
membangun serta mengembangan desain diagram alir,
tepat:
membandingkan
Ethereum atau Hyperledger, dan tampilan antarmuka
pengguna. Sistem ini akan diimplementasikan pada sensus
atau survei di
serta dievaluasi
menggunakan black-box testing. Hasil yang diharapkan dari
penelitian ini adalah dapat dilakukan penerapan blockchain
pada pelacakan alur pemrosesan data.

(purwarupa)

penggunaan

blockchain

Indonesia

yang

2 / 8

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

pelaksana kegiatan 3; pengawas sebagai pelaksana
kegiatan 4; serta responden hanya bisa melihat hasil
yang telah dipublikasikan oleh BPS.

b. Perancangan Sistem dengan Teknologi Blockchain

Sistem blockchain dibangun untuk keterlacakan
alur pemrosesan data survei/sensus. Keterlacakan ini
adalah kemampuan untuk menyajikan informasi
berkaitan dengan riwayat dan perpindahan suatu
objek melalui
setiap tahapan pemrosesan dan
distribusinya [15]. Sistem ini akan meningkatkan
transparansi pada rangkaian tahapan pemrosesan data
dapat
survei/sensus.
meningkatkan
terhadap
kemauan untuk dilakukannya wawancara karena
responden dapat mengakses informasi yang jelas
mengenai data yang diberikannya dilakukan proses
apa saja dan siapa pelaku pemrosesannya. Rancangan
pelacakan
survei/sensus
alur
ditunjukkan pada Gambar 2.

Adanya
kepercayaan

transparansi
responden

pemrosesan

data

Gambar 2. Proses bisnis sistem pelacakan berbasis blockchain

Pelacakan pemrosesan data survei/sensus oleh BPS
dilakukan dari awal penyusunan hingga publikasi.
Tiap tahapan proses ini akan dilakukan pencatatan
data-data yang nantinya akan dimasukkan ke dalam
jaringan blockchain. Data-data yang sudah tersimpan
nantinya akan dibangkitkan (generate) oleh sistem ke
dalam bentuk QR Code. QR Code ini akan berbentuk
cetakan ataupun digital sehingga dapat digunakan
responden untuk mengidentifikasi data yang telah
diberikan.

Pada seluruh tahapan kegiatan pemrosesan data,
tanggal dilakukannya
dilakukan pencatatan terkait
tahapan pemrosesan data, tempat dilaksanakannya
pemrosesan data, serta pelaku pemrosesan data.
Khusus bagian pencacahan dan pengawasan akan
ditampilkan petugas yang melakukannya.

c. Analisis Kebutuhan Sistem

Analisis dilakukan untuk mendapatkan gambaran
sistem yang akan dibuat sesuai dengan kebutuhan
pengguna. Berdasarkan analisis, sistem informasi
perlu memenuhi arsitektur dasar input-proses-output
komponen seperti yang ditunjukkan pada Gambar 3.

3 / 8

Gambar 1. Kerangka pikir penelitian

VI. HASIL DAN PEMBAHASAN

A. Perancangan Sistem
Proses

perancangan

sistem dilakukan

dengan
mengidentifikasi alur proses sistem pada survei atau
sensus yang telah dilakukan BPS. Kemudian, alur proses
tersebut dijadikan sebagai masukkan untuk melakukan
perancangan sistem yang akan diusulkan sebagai solusi
untuk mengatasi permasalahan penelitian.
a.

Identifikasi Sistem Berjalan

Analisis sistem berjalan dilakukan berdasarkan
proses survei atau sensus yang dilakukan oleh BPS.
bisa
Secara
ditarik
kesimpulan, bahwa
atau sensus
melakukan tahapan yang sama dengan runtutan
jadwal kegiatan sebagai berikut:

umum dalam prosesnya
setiap survei

1.
2.
3.
4.
5.

Penyusunan kuesioner dan buku pedoman
Pelatihan petugas lapangan
Pencacahan
Pengawasan/Pemeriksaan
Penerimaan
pengelompokan
(receiving),
(batching), dan penyuntingan/penyandian
(editing/coding) dokumen di BPS daerah
Pengiriman data ke BPS pusat
Pengolahan
Kompilasi dan tabulasi data
Publikasi

6.
7.
8.
9.
Anggota/aktor

primer

pemrosesan data
survei/sensus di BPS terdiri dari BPS pusat sebagai
pelaksana kegiatan 1, 7, 8, dan 9; BPS daerah sebagai
pelaksana kegiatan 2 dan 5; petugas cacah sebagai

dari

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

sistem. Setiap use case menyatakan spesifikasi
perilaku (fungsionalitas) dari sistem yang sedang
dijelaskan yang memang dibutuhkan oleh aktor untuk
memenuhi
case diagram
penelitian ini ditunjukan pada Gambar 4.

[16]. Use

tujuannya

Seluruh aktor selain responden, perlu login untuk
melakukan proses input data sesuai cakupan tahapan
masing-masing. Login digunakan untuk menangani
verifikasi dan hak akses dengan cara
sistem
memeriksa nama user dan password sehingga
muncul
form sesuai dengan hak akses. Khusus
responden bisa melakukan aktivitas melaporkan
petugas tanpa perlu melalui
login. Seluruh aktor
dapat mengakses alur pemrosesan data selama
mereka memiliki salinan atau cetak dari QR Code.

e. Arsitektur Sistem

Sistem akan dibangun pada penerapan blockchain
dengan menggunakan kerangka kerja ethereum
blockchain. Pada struktur blockchain ini dibangun
pada dua sisi yaitu frontend dan backend. Frontend
merupakan sisi yang tampak oleh pengguna sistem,
sedangkan sisi backend merupakan sisi yang tidak
ditampilkan
berfungsi
mengatur logika sistem dan pengolahan data-data.
Arsitektur sistem yang akan dibangun ditunjukkan
oleh Gambar 5.

pengguna

kepada

dan

Gambar 3. Diagram input output sistem

tempat

tanggal

Berdasarkan diagram tersebut, diketahui bahwa
sistem informasi yang dibangun membutuhkan input
sistem berupa data informasi dari pemrosesan data
diantaranya
dilakukannya
dan
pemrosesan data serta stakeholder yang melakukan
tersebut
pemrosesan tersebut. Setelah kedua hal
diinputkan akan terolah di sistem yang kemudian
menghasilkan output berupa desain situs web dan
purwarupa sistem pelacakan alur pemrosesan data
survei/sensus berbasis blockchain. Sistem input data
untuk
berisi
melakukan perekaman data. Input data hanya bisa
dilakukan oleh admin BPS pusat maupun daerah,
pengawas, dan petugas cacah sesuai kegiatan yang
dicakupnya. Oleh karenanya, untuk membatasi akses
dalam penginputan data dibuatlah sistem login.

beberapa menu

berfungsi

yang

d. Desain Sistem

Gambar 5. Arsitektur sistem

Gambar 4. Use case diagram dengan teknologi blockchain

Proses setelah menentukan kebutuhan sistem yaitu
mendesain sistem. Dalam penelitian ini digunakan
salah satu diagram dari Unified Modelling Language
case diagram. Penggunaan
(UML), yaitu use
diagram ini
untuk
kebutuhan (requirements) dari
mengilustrasikan

digunakan

karena

dapat

open-source

Pertama, terdapat aplikasi web digunakan untuk
mengirim permintaan ke layanan web hosting.
Setelah permintaan diterima oleh layanan web, data
akan dikirimkan ke jaringan blockchain dan disimpan
pada Interplanetary File System (IPFS). IPFS adalah
untuk
proyek
menyimpan dan berbagi data dengan identitas unik
pada
IPFS memiliki
keunggulan untuk menghapus duplikasi data atau
berkas yang tidak relevan sehingga IPFS dapat
penyimpanan yang aman
menyediakan model
berbasis blockchain [17]. Penggunaan IPFS dalam
arsitektur sistem ini menangani dua hal
terkait
melakukan input dan membaca data.

jaringan terdesentralisasi.

digunakan

yang

Komponen kedua, terdapat penampil berkas IPFS
yang membantu menelusuri berkas yang disimpan
secara
ini dilakukan dengan
mengkonfigurasikan node IPFS sehingga berkas akan

terdistribusi, hal

4 / 8

f.

ditampilkan. Penampil berkas ini bisa dilakukan
tanpa perantara, asalkan pengguna memiliki QR
Code atau hash dari berkas yang dihasilkan oleh
sistem.
Smart Contract

Referensi [18] menyatakan bahwa smart contract
adalah sebuah media untuk menyimpan semua aturan
dan kebijakan yang digunakan sesuai ketentuan
kontrak. Dengan kontrak ini, pelanggaran terhadap
perjanjian yang telah dibuat tidak dapat dilakukan.
Dalam ethereum blockchain, untuk menulis sebuah
contract dilakukan dengan menggunakan
smart
bahasa solidity. Smart contract bekerja tanpa melalui
pihak ketiga dan memiliki proses transaksi yang
kredibel sehingga tidak mudah untuk diretas ataupun
diubah [19]. Gambar 6 menunjukan diagram
pembentukan smart contract.

Gambar 6. Diagram pembentukan smart contract

Berikut penjelasan diagram alur pembentukan
smart
contract: membuat kode smart contract
dengan solidity, kode ini merupakan baris kode yang
logic pada sistem.
ditulis dan digunakan sebagai
Compile, untuk mengcompile contract yang telah
dibuat perlu memakai solidity compiler. Kontrak
kode yang telah dibuat berekstensi .sol kode dan
di-compile menggunakan solidity compiler versi
0.4.25.
untuk
memasukkan data ke jaringan blockchain sesuai
dengan kontrak yang telah ditentukan.

Selanjutnya,

digunakan

deploy

g. Rancangan Antarmuka

Antarmuka atau interface merupakan layanan yang
disediakan oleh sistem sebagai tampilan dan cara
sistem berkomunikasi
atau
mempermudah pengguna untuk berinteraksi dengan
sistem. Antarmuka yang baik akan membuat
pengguna menjadi nyaman dalam melaksanakan
tugas pada
adalah rancangan
antarmuka yang akan digunakan dalam sistem

sistem. Berikut

pengguna

dengan

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

alur

pelacakan
berbasis blockchain:
1. Halaman login.

pemrosesan

data

survei/sensus

Gambar 7. Rancangan antarmuka login

Gambar 7 merupakan antarmuka
pertama

login
merupakan
kali
ditampilkan kepada pengguna sistem selain
responden untuk melakukan input data.
2. Halaman input BPS Pusat dan BPS Daerah.

tampilan

yang

pertama

Pada antarmuka BPS terdapat dua menu yang
dapat digunakan untuk melakukan input data.
Menu
8) menunjukan
(Gambar
antarmuka untuk melakukan input pada sistem
alur pemrosesan data, sedangkan menu kedua
(Gambar
untuk
mengunggah berkas informasi detail petugas
atau pengawas pada survei/sensus.

antarmuka

berisikan

9)

Gambar 8. Rancangan antarmuka input BPS menu 1

Gambar 9. Rancangan antarmuka input BPS menu 2

3. Halaman input petugas dan pengawas.

5 / 8

Gambar 10. Rancangan antarmuka input petugas

4. Halaman pelacakan pemrosesan data.

Gambar 11. Rancangan antarmuka pelacakan pemrosesan data

Gambar 10 dan 11 merupakan antarmuka
informasi
bagi pengguna untuk mengakses
pelacakan alur pemrosesan survei/sensus. Cara
untuk mendapat informasi tersebut hanya dengan
melakukan input kode lacak atau memindai QR
Code yang diberikan kepada responden.

B.

yang

informasi

Informasi

antarmuka

Pada
terkait

Setelah berhasil memindai, responden akan
pelacakan
memasuki
tersebut.
disediakan ialah
tahapan-tahapan pemrosesan data yang telah
didapati
setiap
dilakukan.
informasi
pelaku
nama
pemrosesan,
tempat pemrosesan, dan tanggal
dilakukannya pemrosesan. Khusus pada tahapan
ditambahkan
pencacahan
informasi terkait informasi detail dari petugas
cacah atau pengawas yang bertugas di wilayah
tugas tersebut.

tahapan,
tahapan,

pengawasan

dan

5. Halaman lapor responden.

Antarmuka lapor pada Gambar 12 digunakan
oleh responden untuk melaporkan jika terdapat
informasi tidak relevan, misalnya petugas yang
datang tidak sesuai dengan yang terdapat pada
sistem pelacakan.

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Gambar 12. Rancangan antarmuka lapor responden

6. Halaman informasi petugas.

Informasi mengenai petugas disajikan dalam

format pdf yang tersimpan di IPFS.

Gambar 13. Rancangan antarmuka informasi petugas

Implementasi Sistem
Dalam bagian

ini

dijelaskan mengenai
akan
pengimplementasian sebagian dari perancangan sistem
yang telah dibuat pada poin sebelumnya yaitu pada
bagian mengunggah berkas informasi detil petugas ke
jaringan blockchain dan disimpan pada IPFS untuk
tahapan ketiga dan keempat dari pemrosesan data survei
atau sensus.
a. Kebutuhan Perangkat

Untuk mengimplementasi rancangan sistem yang
telah dibahas, dibutuhkan dukungan dari perangkat
keras dan piranti
lunak. Tabel 1 menjabarkan
spesifikasi dari perangkat yang digunakan dalam
proses implementasi ini.

TABEL I
DAFTAR SPESIFIKASI PERANGKAT UNTUK IMPLEMENTASI SISTEM

No

1

2

3

Jenis

Perangkat

Sistem Operasi

Windows 10 Home

Storage

Prosesor

500 GB

2 GHz dual cores

6 / 8

4

GPU

UHD Graphics 620

IPFS v0.4.0

Truffle v5.0.0-beta.1

Solidity v0.4.25

Web3.js v1.0.0

5

Blockchain

Git v2.32

Python v2.7

NodeJs v8.12.0

Ganache v1.2.1

b.

Implementasi Smart Contract

Smart contract dalam sistem ini digunakan untuk
menyimpan string yang berisi hash IPFS, uint yang
berisi waktu ditambahkannya berkas ke jaringan
blockchain dan IPFS, serta bool digunakan untuk
validasi. Validasi
ini digunakan untuk membantu
mengetahui apakah hash telah ditambahkan atau
belum.

Terdapat dua fungsi dalam smart contract ini ialah
fungsi menambahkan dan membaca. Pada fungsi
menambahkan akses hanya diberikan kepada admin,
BPS. Sedangkan, fungsi membaca bisa digunakan
oleh semua aktor berguna untuk mengetahui apakah
berkas informasi detail petugas yang disimpan di
IPFS telah tersedia.
Implementasi Antarmuka

Berikut adalah tampilan antarmuka yang berhasil
diimplementasikan dari perancangan pada Gambar 9,
Gambar 11, dan Gambar 13.
1. Antarmuka input informasi petugas.

c.

Gambar 14. Antarmuka input informasi petugas

2. Antarmuka pelacakan informasi petugas.

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Gambar 15. Antarmuka pelacakan informasi petugas

3. Antarmuka informasi petugas.

Gambar 16. Antarmuka informasi petugas

d. Hasil Pengujian

TABEL II
TABEL PENGUJIAN SISTEM

Nama
Fungsi

Tambah
berkas

Skenario

Hasil yang diharapkan

Hasil

Menambahkan berkas
informasi petugas

Data berhasil disimpan

Berhasil

Memunculkan
daftar
hash dari berkas yang
telah tersimpan

Berhasil

Menambahkan berkas
informasi petugas
yang telah tersimpan

Menampilkan
peringatan
berkas
dalam penyimpanan

bahwa
sudah ada di

Berhasil

Mencari berkas dari
hash yang diinputkan

Data berhasil dipanggil

Berhasil

Memunculkan
informasi waktu berkas
diunggah beserta tautan
ke berkas terkait

Berhasil

Cari
berkas

Mencari berkas yang
belum disimpan

Menampilkan
peringatan
bahwa
berkas tidak ditemukan

Berhasil

Mengklik tautan letak
IPFS

berkas

Mengalihkan tampilan
ke
informasi
detil petugas/pengawas
survei atau sensus

Berhasil

7 / 8

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Subsequent
Consumer Research. 33. pp. 203-211. 2006.

Impact on Market Response Outcomes,” Advances in

[9] Badan Pusat Statistik. Cek Sensus Penduduk 2020. [Online]. Available:

https://www.bps.go.id/sp2020/cek/

[10] J. Xu, “Are blockchains immune to all malicious attacks?”, Financial

Innovation. 2. 10.1186/s40854-016-0046-5, 2016.

[11] X. Xu, et al., “A Taxonomy of Blockchain-Based Systems for
Architecture Design,” IEEE International Conference on Software
Architecture (ICSA), 2017.

[12] S. Y. Lim, P. T. Fotsing, A. Almasri, O. Musa, M. L. M. Kiah, T. F. Ang,
and R. Ismail, “Blockchain Technology the Identity Management and
Authentication Service Disruptor: A Survey,” International Journal on
Advanced Science Engineering Information Technology, vol. 8, no. 4-2,
2018.

[13] D. A. Iswari, Y. Arkeman, and Muslich, “Analisis dan Desain Rantai
Pasok Kakao Berbasis Blockchain,” International Journal on Advanced
Science Engineering Information Technology, vol. 8, no. 4-2, 2018.
[14] K. Peffers, T. Tuunanen, M. A. Rothenberger, & S. Chatterjee, “A Design
Science Research Methodology for Information Systems Research,”
Journal of Management Information Systems, vol. 24, pp. 45-78, Aug
2007.

[15] T. Bosona & G. Grebesenbet, “Food traceability as an integral part of
logistics management in food and agricultural supply chain,” Food Contr
33: 32-48, 2013.

[16] A. T. Kurniawan, “Pemodelan use case (UML): Evaluasi

terhadap
beberapa kesalahan dalam praktik,” Jurnal Teknologi Informasi dan Ilmu
Komputer (JPTIK) 5 (1): 77-86, 2018.

[17] J. Benet, “IPFS - Content addressed, versioned, p2p file system
https://ipfs.io/ipfs/

(draft
QmR7GSQM93Cx5eAg6a6yRzNde1FQv7uL6X1o4k7zrJa3LX/ipfs.
draft3.pdf, 2014.

3),”

[18] Yulianton, H., Santi, R., Hadiono, K., & Mulyani, S, Implementasi
2018 Proceeding SINTAK,
in November

Sederhana Blockchain,
Semarang,  ISBN: 1-5.

[19] N. Atzei, M. Bartoletti, & T. Cimoli, A survey of attacks on ethereum
smart contracts (SoK), in April 2017 Proceeding of The 6th International
Conference on Principles of Security and Trust,Berlin .ISBN: 164–186.

Pada tahap ini merupakan pengujian dari hasil
sistem yang sudah dibuat sebelumnya. Metode
pengujian menggunakan metode black-box, yaitu
dengan melakukan percobaan keseluruhan sistem
dari
fungsi-fungsi aplikasi web. Hasil pengujian
dapat dilihat pada tabel berikut.

VII.

PENUTUP

Berdasarkan identifikasi sistem berjalan dari pemrosesan
data survei atau sensus oleh BPS menunjukan bahwa terdapat
lima stakeholder yang terlibat, yaitu BPS Pusat, BPS Daerah,
petugas cacah, pengawas, dan responden. Aktivitas yang
terjadi dalam pemrosesan data survei atau sensus antara lain
penyusunan kuesioner dan buku pedoman, pelatihan petugas
lapangan, pencacahan, pengawasan/pemeriksaan, penerimaan
(batching),
pengelompokan
(receiving),
penyuntingan/penyandian
dokumen,
pengolahan, kompilasi dan tabulasi data, serta publikasi.

(editing/coding)

Penelitian ini menghasilkan rancangan pelacakan alur
atau sensus dengan memanfaatkan
pemrosesan survei
teknologi blockchain. Fitur yang berhasil dikembangkan
adalah input berkas dari informasi petugas oleh BPS dan
pencarian berkas dari
informasi petugas oleh seluruh
stakeholder yang terlibat. Pemanfaatan teknologi ini dapat
membantu meningkatkan kepercayaan responden. Smart
contract berhasil diimplementasikan dan telah berfungsi
dengan
sistem
menggunakan black box testing menunjukan kinerja sistem
telah sesuai dengan kebutuhan fungsional.

Berdasarkan

pengujian

baik.

hasil

Adapun saran yang dapat digunakan untuk penelitian
selanjutnya adalah perlu adanya implementasi sistem yang
memanfaatkan teknologi blockchain untuk pelacakan alur
pemrosesan data secara lengkap dari tahap penyusunan hingga
tahap publikasi. Hal tersebut dikarenakan dalam penelitian ini
teknologi blockchain hanya diimplementasikan untuk berkas
informasi petugas, sehingga diharapkan penelitian selanjutnya
dapat memanfaatkan teknologi blockchain untuk sistem
keterlacakan tersebut.

[1] Badan

Pusat

DAFTAR PUSTAKA
Tentang

BPS.

Statistik.

[Online]. Available:

https://www.bps.go.id/menu/1/sejarah.html

[2] Presiden Republik Indonesia. (2020, 1). Jokowi Tekankan Pentingnya
Data
Available:
https://katadata.co.id/febrinaiskana/berita/5e9a498f59f50/jokowi-tekanka
n-pentingnya-data-dalam-pembangunan

Pembangunan.

[Online].

dalam

[3] P. P. Biemer, Latent Analysis of Survey Error. New Jersey: John Wiley &

Sons, Inc., 2011.

[4] J. G. Bethlehem and B. Bakker, “The impact of nonresponse on survey

quality,” Statist. J. Int. Ass. Off. Statist, 30, pp. 243-248, 2014 .

[5] W. G. Cochran, Sampling Technique ed., 3. Canada: John Wiley & Sons,

Inc., 1977.

[6] A. Adetia, I. Budi and F. Setiadi, ""Identification and Analysis of Factors
Affecting E-survey Response Rate at Central Bureau of Statistics,""
International Conference on Information Management and Technology
(ICIMTech), 2020, pp. 560-565, 2020.

[7] P. H. Schurr and J. L. Ozanne, “Influences on exchange processes:
buyers’ preconceptions of a seller’s trustworthiness and bargaining
toughness”, Journal on Consumer Research, 11(4), pp. 939-953, 1985.
[8] B. Jin & J. Y. Park, “The Moderating Effect of Online Purchase
Experience on the Evaluation of Online Store Attributes and the

8 / 8

"
221709552,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Pengembangan Sistem Survei Kebutuhan Data BPS:
Modul Output dan Manajemen Pengguna

Andre Fanni (221709552, 4SI1)

Dosen Pembimbing: Firdaus, M.B.A.

iterasi.

sistem SDLC model

Ringkasan— Penelitian Pengembangan Sistem SKD BPS
Modul output dan manajemen pengguna didasarkan dari belum
adanya monitoring kegiatan SKD. Pada Sistem berjalan, hasil
analisis SKD hanya ditampilkan dalam bentuk tabel dan tidak
interaktif. Untuk itu perlu adanya pengembangan sistem SKD
untuk memperbaiki dan meningkatkan kualitas web SKD.
Dalam melakukan pengembangan sistem survei kebutuhan data
output dan manajemen pengguna, peneliti
BPS modul
menggunakan yii2 basic sebagai
frame work php. Dalam
melakukan penelitian ini, peneliti menggunakan metode
Peneliti
pengembangan
menggunakan diagram Ishikawa sebagai metode analisis
masalah, dan analisa PIECES untuk analisis kebutuhan. Selain
itu juga, untuk melakukan uji coba dan evaluasi, peneliti
menggunakan black box testing dan SUS. Penelitian ini juga
rancangan sistem, dan
berisikan prosesi bisnis usulan,
implementasi dari rancangan sistem yang dibuat. Hasil dari
penelitian ini berhasil membuat sistem dashboard monitoring
kegiatan SKD yang mampu memperlihatkan data eksrim satker
BPS berdasarkan provinsi dan berdasarkan kabupaten/kota,
berhasil membuat sistem manajemen pengguna SKD yang dapat
melakukan penambahan pengguna baru, mengubah data
pengguna atau menonaktifkan pengguna, dan telah berhasil
migrasi sistem pendukung lain dalam kegiatan SKD seperti
menu analisis hitung cepat, kritik dan saran, rekapitulasi
pencacahan & pencapaian SKD, rawdata, dan rencana tindak
lanjut kegiatan SKD.

Kata Kunci— Survei Kebutuhan Data, yii2 basic, Monitoring,

Manajemen pengguna, SDLC, SUS.

I. LATAR BELAKANG

Kemajuan dan Perkembangan teknologi saat

ini yang
sangat pesat menuntut semua institusi pemerintahan maupun
swasta untuk bergerak mengikuti perkembangan agar tidak
tertinggal. Salah satu institusi pemerintahan yang selalu
adalah Badan Pusat Statistik (BPS), dalam
berinovasi
melakukan inovasi
tentunya harus ada penelitian terkait
apakah inovasi dan kinerja yang dilakukan lebih bermanfaat
atau tidak. Untuk itu perlu adanya survei layanan kepuasan
konsumen data BPS yang dapat diukur melalui Survei
Kebutuhan Data (SKD).

SKD merupakan tolok ukur BPS dalam peningkatan
kualitas data dan informasi statistik sebagai pelayanan publik.
SKD sendiri telah dilaksanakan sejak tahun 2005 dan saat ini
dilaksanakan
dan BPS
di BPS RI, BPS Provinsi,
Kabupaten/Kota. Responden SKD merupakan penggguna data
yang datang ke unit pelayanan BPS baik sebagai individu,
instansi, lembaga, ataupun institusi [1]. SKD adalah survei
yang diselenggarakan oleh Badan BPS untuk memahami
kebutuhan data statistik dan tingkat kepuasan konsumen
terhadap data dan pelayanan oleh BPS [2]. SKD sangat

penting karena digunakan untuk menjawab Indikator Kinerja
Utama (IKU) BPS, Indeks Kepuasan Konsumen (IKK) yang
tertuang dalam PermenpanRB nomor 17 tahun 2017, Indikator
SDGs, Indeks Persepsi Anti Korupsi (IPAK) yang berkaitan
dengan WBK/WBBM, Evaluasi layanan data dan informasi
statistik.

dari

penanggung

Menurut pegawai sub direktorat Rujukan Statistik yang
merupakan
pelaksanaan SKD
jawab
mengemukakan “Pelaksanaan SKD saat ini sendiri telah baik
dan konsisten dilakukan tiap tahun. dalam pelaksanaan nya
sendiri dilakukan secara online menggunakan CAWI yang
disebarkan melalui sosial media dan email. Pelaksanaan SKD
2020 yang menggunakan framework PHP Yii2 Advance
dirasa kurang sesuai digunakan dalam sistem SKD saat ini dan
belum adanya
suatu sistem yang dapat memanajemen
pengguna. Selain itu, ada desakan dari pimpinan BPS untuk
pembuatan sistem monitoring pengawasan kegiatan SKD”.

Penggunaan frame work yii2 advance pada Sistem SKD
saat ini tidak didasari kajian kepentingan terlebih dahulu. Hal
ini mengakibatkan sistem saat ini menjadi berantakan dalam
hal coding dan melakukan perbaikan tambal sulam sistem
setiap pelaksanaan SKD. Pelaksanaan SKD yang akan
dilakukan setiap tiga bulan sekali akan mengakibatkan tidak
efisien nya dalam melakukan updating coding. Untuk itu
subdir rujukan statistik sebagai pengelola dan penanggung
jawab
untuk melakukan
SKD memutuskan
kegiatan
pembangunan ulang sistem.

Sistem SKD tidak memisahkan antara web publik dan web
admin, dikarenakan masyarakat umum hanya dapat mengisi
kuisioner saja pada web SKD dan tidak diberikan akses menu
lain dalam web SKD.

Yii2 advance merupakan dua aplikasi yang berbeda
memisahkan antara web publik dan web admin. Frame work
ini pada dasarnya dibagi dua yaitu frontend dan backend. Yii2
advance juga tidak memiliki folder MVC (Model, View, dan
Controller) pada halaman root. Berbeda dengan Yii2 basic
yang tidak memisahkan antara web publik dan web admin
serta folder MVC berada pada root [7].

Oleh

karena

pengembangan

sistem SKD hanya
diperuntukan untuk admin saja dan tidak untuk publik maka
sesuai dengan pernyataan diatas, frame work PHP Yii2 basic
lebih cocok digunakan dibandingkan yii2 advance karena
frame work tersebut telah memenuhi kebutuhan subject matter
untuk membangun ulang sistem menggunakan frame work
PHP Yii2.

Dari hasil wawancara tersebut peneliti bermaksud untuk
mengembangkan sistem SKD BPS modul output dan
manajemen pengguna menggunakan frame work PHP Yii2

1 / 8

basic yang berfokus dalam pembuatan sistem monitoring
kegiatan SKD, manajemen pengguna, dan migrasi progrram
pendukung lain dalam sistem SKD.

B. Metode Pengumpulan Data

Metode pengumpulan data dalam penelitian ini terdapat 4

metode yaitu wawancara, observasi, studi pustaka, kuisioner.

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

II. TUJUAN PENELITIAN

latar

belakang

Berdasarkan

ini
bertujuan untuk mengembangkan sistem SKD yang dapat
meningkatan kualitas data dan informasi statistik. Adapun
fokus dari penelitian ini adalah:

penelitian

tersebut,

1. Pembuatan sistem dashboard monitoring kegiatan SKD
yang mempermudah dalam pengawasan kegiatan SKD
2. Pembuatan sistem manajemen pengguna yang dapat
manajemen

melakukan

dalam

mempermudah
pengguna

3. Melakukan migrasi program pendukung lainnya dalam

sistem SKD

C. Metode Pengembangan Sistem

Metode yang digunakan dalam penelitian pengembangan
sistem SKD adalah System development life cycle (SDLC)
model iterative. SDLC model iterative dilakukan modifikasi
dan penambahan fungsi baru secara berulang-ulang dengan
tahapan- tahapan nya
porsi penambahan yang lebih kecil.
(Analisis),
terdiri dari Planning (Perencanaan), Analysis
Design
(Desain), Implementation (Implementasi), Testing
(Pengujian). Tahapan-tahapan tersebut dilakukan secara
berulang-ulang untuk meningkatkan versi dari sistem sampai
semua kebutuhan dari sistem terpenuhi [7].

III. PENELITIAN TERKAIT

V. KERANGKA PIKIR

Kerangka pikir penelitian ini terdiri dari enam bagian yaitu,
masalah, pendekatan, solusi, system development, evalusi,
hasil [5].

Penelitian terdahulu terkait sistem SKD modul output dan

2)

manajemen pengguna:
Pembangunan
Sistem Informasi Helpdesk Modul
1)
Monitoring, Analisis, dan Evalusai oleh Dimas Tresna
Suhartono dari STIS tahun 2015. Penelitian ini
merupakan pembangunan sistem helpdesk sebagai solusi
untuk meningkatkan kualitas pelayanan publik oleh BPS.
Penelitian ini memonitoring modul rekomendasi statistik,
tanya jawab, dan search engine pada sistem helpdesk
tanyaBPS. Hasil dari penelitian ini adalah sistem mampu
menampilkan hasil web analytic dalam bentuk peta
tematik dan grafik-grafik berdasarkan wilayah secara
realtime [4].
Pengembangan Sistem Informasi Pengawasan Dokumen
untuk Perusahaan Pada Survei/Sensus BPS Berbasis
Web (Studi kasus: BPS Provinsi DKI Jakarta) oleh
Retno Fitriandari tahun 2018. Penelitiam ini merupakan
pengembangan sistem sebgai media pelaporan dan
survei/sensus
pengawasan
berlangsung. Selain itu juga penelitian ini bertujuan
untuk mengembangkan sistem pengawasan dokumen
kuisioner pada perusahaan dan sistem dapat mengawasi
beberapa survei sekaligus. Hasil dari penelitian ini
adalah sistem mampu menjdi media pelaporan dan
survei/sensus
pengawasan
berlangsung,
fasilitas
sistem dapat
pengawasan
dalam
Jakarta
memfasilitasi BPS
mengawasi progres beberapa survei sekaligus [3].

antar
sistem berhasil menjadi
dokumen

kuisioner,
Provinsi DKI

selama

selama

aktor

aktor

antar

IV. METODE PENELITIAN

A. Ruang Lingkup Penelitian

Ruang lingkup penelitian ini adalah pengembangan sistem
SKD modul output dan manajemen pengguna. Penelitian ini
berfokus pada pembuatan fitur dashboard monitoring kegiatan
SKD dalam memonitoring kegiatan SKD dan migrasi program
hasil kegiatan SKD.

Gambar 1. Kerangka Pikir

VI. HASIL DAN PEMBAHASAN

A. Analisis Sistem Berjalan

Berdasarkan hasil wawancara dengan subdirektorat
rujukan statiktik. Gambar 2 adalah alur dalam kegiatan

2 / 8

SKD khususnya
penyimpanan data ke rawdata.

dalam pengolahan

data

sampai

Gambar 2. Proses bisnis sistem berjalan

B. Analisis Kebutuhan

Analisis kebutuhan merupakan sebuah analisis lebih
lanjut dalam menanggapi permasalahan yang ada, dalam
analisis ini akan dipaparkan kebutuhan-kebutuhan dalam
pengembangan
analisis
kebutuhan terdapat 2 yaitu, analisis kebutuhan fungsional
dan analisis kebutuhan non fungsional [6].

sistem. Dalam melakukan

Kebutuhan non fungsional

TABEL I
ANALISA PIECES

Aspek

(1)

Kebutuhan Dalam Perbaikan Sistem

(2)

Performance Mampu meningkatkan performa sistem SKD

Information

Economics

Control

Efficiency

Service

Mampu memberikan informasi hasil SKD sekarang
dan SKD sebelumnya
Mampu membuat waktu pengawasan lebih cepat
sehingga waktu kerja pegawai dapat dimanfaatkan
untuk pekerjaan lain.
Mampu mengontrol dan mengawasi kegiatan SKD
serta mampu membatasi akses sistem.
Mampu membuat
memanajemen pengguna
Mampu menampilkan hasil SKD dalam bentuk
yang lebih menarik.

sistem SKD efisien dalam

C.

Proses Bisnis Sistem Usulan

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

pengguna dalam menggunakan dashboard. Proses bisnis ini
dimulai dari responden mengisi kuisioner SKD hingga selesai
lalu pemeriksa akan memverifikasi jawaban responden lalu
data yang telah diverifikasi akan disalin ke view database dan
database akan secara otomatis mengolah data tersebut lalu
data akan ditampilkan pada dashboard. Pengguna dapat
memonitoring progres SKD dan melihat progres terkecil dari
kegiatan SKD ini melalui menu dashboard.

Gambar 4. Proses bisnis usulan manajemen pengguna

Gambar 6 adalah diagram proses bisnis usulan pengembangan
sistem SKD BPS modul output dan manajemen pengguna
bagian manajemen pengguna. Proses bisnis ini menjelaskan
proses memanajemen pengguna yang dimulai dari pengguna
mengakses menu manajemen pengguna,
halaman
manajemen pengguna akan menampilkan daftar pengguna.
Selanjutnya pengguna dapat menambahkan pengguna baru,
apabila pengguna baru yang ditambahkan telah ada, maka
sistem akan memberitahukan bahwa pengguna tersebut sudah
ada dan jika tidak maka pengguna baru akan berhasil
ditambahkan. Lalu pengguna juga dapat mengubah data
pengguna
terdapat kesalahan dalam menambahkan
pengguna atau ingin menonaktifkan pengguna tersebut.

lalu

jika

D. Use Case Diagram
Use case digunakan dalam menggambarkan suatu hubungan
antara aktor dengan aktivitas dalam sistem yang akan
dibangun. Dalam sistem yang dibangun terdapat 7 role
admin
admin,
pengguna yaitu super
kabupaten/kota,
entri
pusat,
entri
kabupaten/kota, dan monev.

admin provinsi,
provinsi,
entri

Gambar 3. Proses Bisnis Usulan dashboard

Gambar 3 adalah diagram proses bisnis usulan pengembangan
sistem SKD BPS modul output dan manajemen pengguna
bagian dashboard. Proses bisnis ini menjelaskan rangkaian

Gambar 5. use case diagram

3 / 8

E.

Activity Diagram

Activity diagram menggambarkan aliran kerja atau proses
bisnis dari sistem. gambar 6-13 merupakan activity diagram
dari fitur yang dikerjakan dalam penelitian ini.

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Gambar 11. activity diagram menu
pencapaian

Gambar 6. activity diagram RTL

Gambar 10. activity diagram tabel
analisis

Gambar 7. activity diagram melihat
dashboard

Gambar 8. activity diagram melihat
kritik dan saran

Gambar 9. activity diagram melihat
data ekstrim

Gambar 12. activity diagram rawdata

Gambar 13. activity diagram
manajemen pengguna

4 / 8

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

F.

Implementasi User Interface
1) Halaman Login

3) Halaman Pengguna

Untuk mengatur pengguna yang dapat

Dalam mengakses semua menu yang ada dalam sistem,
makan
Pada
pengembangan web SKD, login hanya dapat dilakukan
dengan menggunakan SSO BPS seperti pada gambar 15

diperlukan

terlebih

dahulu.

login

diperlukan
gambar 18

halaman manajemen

login maka
seperti

pengguna

Gambar 15. tampilan halaman login SSO BPS

Pada saat login, tidak semua pegawai BPS dapat login.
ini dikarenakan sistem hanya memperbolehkan

Hal
pengguna yang terdaftar dan pengguna yang aktif.

2) Halaman Beranda

Halaman depan atau beranda pada web SKD dapat
dilihat pada gambar 16 yang menampilkan progres dari
suatu satker BPS RI dan BPS Provinsi.

Gambar 16. tampilan halaman beranda sebelum login

Saat setelah login menggunakan akun super admin
maka tampilan halaman beranda akan berubah gambar
17 yang mana ada tambahan menu untuk mengubah
daerah yang akan dipakai. Namun jika login dengan
selain akun super admin maka tampilan halaman beranda
akan seperti gambar 16.

Gambar 18. tampilan halaman pengguna

Pada

halaman

pengguna menampilkan

daftar
pengguna yang aktif maupun yang tidak aktif. Selain itu
juga halaman ini dapat menambahkan pengguna baru
dan mengubah data pengguna.

Gambar 19. tampilan menu tambah pengguna baru

dari
apabila

tampilan
baru,

19 merupakan
pengguna

halaman
Gambar
menambahkan
login
menggunakan super admin, maka tampilan yang diisi
seperti pada gambar 19, namun jika login selain dengan
akun super admin, maka pengguna hanya perlu mengisi
username, email, dan status aktif nya saja dikarenakan
untuk level pengguna dan daerahnya akan diatur sesuai
dengan pengguna yang menambahkan.

Gambar 17. tampilan halaman beranda setelah login

Gambar 20. tampilan menu edit pengguna

Gambar 20 merupakan tampilan dari mengedit atau
mengubah data dari pengguna yang mana admin dari
satker dapat menonaktifkan pengguna yang tidak terlibat
dalam kegiatan SKD pada tahun tersebut.

5 / 8

4) Halaman Dashboard

Halaman

dashboard merupakan
berfungsi untuk memonitoring progres dan hasil SKD.

halaman

yang

Gambar 21. tampilan halaman dashboard nasional

Gambar 21 merupakan tampilan halaman dashboard
secara nasional berisikan progres dari BPS Provinsi.
Selain itu juga di halaman ini akan ditampilkan hasil
hitung cepat dari IKK, IPAK, dan juga data ekstrim
satker
ini
secara nasional ke
mengelompokkan hasil analisis
beberapa
kepuasan
seperti
kelompok
layanan, perilaku anti korupsi, kebutuhan data, dan
kualitas data.

dan kabupaten/kota. Halaman

segmentasi,

provinsi

Gambar 22. tampilan halaman dashboard provinsi

Gambar 22 merupakan halaman dashboard satker
provinsi yang berisikan progres dan data ekstrim dari
BPS Kabupaten/Kota yang berada pada Provinsi tersebut.
Selain itu juga di halaman ini akan ditampilkan hasil
hitung cepat dari IKK dan IPAK untuk BPS Provinsi
juga mengelompokkan hasil
tersebut. Halaman ini
analisis BPS Provinsi
tersebut beberapa kelompok
seperti
segmentasi, kepuasan layanan, perilaku anti
korupsi, kebutuhan data, dan kualitas data.

Gambar 23. tampilan halaman dashboard kabupaten/kota

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Gambar 23 merupakan halaman dashboard satker
kabupaten/kota yang menampilkan hasil hitung cepat
dari IKK dan IPAK kabupaten/kota tersebut. Selain itu
juga, halaman ini mengelompokkan hasil
analisis
kabupaten/Kota tersebut ke beberapa kelompok seperti
segmentasi, kepuasan layanan, perilaku anti korupsi,
kebutuhan data, dan kualitas data.

5) Halaman Kritik dan Saran

Halaman ini merupakan kumpulan dari catatan pada
blok 4 yang diisi oleh responden. Pada halaman ini
catatan sudah dikelompokkan ke dalam beberapa tipe
catatan yaitu negatif, netral, dan postitif yang dapat
dilihat pada gambar 24.

Gambar 24. tampilan halaman kritik dan saran

Gambar 24 ditampilkan dua menu yaitu kritik & saran
per satker dan kritik & saran seluruh indonesia. Kedua
menu itu ditampilkan hanya pada pengguna super admin,
untuk pengguna
akan
menampilkan menu kritik & saran per satker masing-
masing.

admin hanya

selain super

6) Halaman Pencapaian

Halaman

pencapaian merupakan

yang
berisikan rekapitulasi pencacahan SKD 2021 dan progres
dari satker BPS.

halaman

Gambar 25. tampilan halaman pencapaian bagian rekapitulasi

Gambar 25 berisikan detail dari

jumlah responden,
pencacah dan pemeriksa berdasarkan satkernya. Selain
itu juga dapat mendownload rekapitulasinya dalam
bentuk pdf.

6 / 8

Gambar 26. tampilan halaman pencapaian bagian capaian suatu satker

Gambar 26 berisikan detail dari progres suatu satker
BPS pada halaman ini juga dapat mendownload hasil
dari progres per satker tersebut.

7) Halaman Analisis

Halaman ini merupakan halaman yang menampilkan
daftar analasis SKD. Tampilan dari halaman analisis
dapat dilihat dari gambar 27. pada halaman tersebut
pengguna dapat melihat detail ny dan juga dapat
mendownload hasil analisis SKD.

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

gambar 29 namun tidak mencakup tombol
rawdata.

tambah

Gambar 29. tampilan halaman rawdata berdasarkan satker

Pada gambar 30 merupakan tampilan halaman
seluruh indonesia yang hanya

download rawdata
ditampilkan untuk super admin.

Gambar 30. tampilan halaman rawdata seluruh indonesia

Apabila

tombol

tambah

rawdata

diklik

akan

menampilkan halaman seperti pada gambar 31.

Gambar 27. tampilan halaman analisis hasil SKD

Jika pengguna mengklik icon mata pada gambar 27.
maka akan menampilkan halaman seperti pada gambar
28 yang menampilkan detail dari analisis SKD, pada
halaman tersebut juga bisa langsung mendownload hasil
SKD dengan mengklik tulisan dowload.

Gambar 31. tampilan halaman tambah rawdata

9) Halaman Rencana Tindak Lanjut (RTL)

Halaman RTL berisikan rencana tindak lanjut dari
kegiatan
dapat
menambahkan, mengedit dan mendownload hasil RTL
sepeti yang ditampilkan pada gambar 32.

halaman

SKD,

pada

juga

ini

Gambar 28. tampilan halaman analisis hasil SKD bagian view tabel

8) Halaman Rawdata

Halaman rawdata merupakan halaman yang berisikan
menu download untuk mendownload rawdata dari SKD.
Pada halaman rawdata, pengguna selain super admin
hanya akan menampilkan rawdata per satker seperti pada

Gambar 32. tampilan halaman RTL

Gambar 32 merupakan tampilan untuk halaman

tambah RTL berdasarkan satker nya.

7 / 8

Gambar 33. tampilan halaman tambah rencana tindak lanjut

Gambar 34 merupakan tampilan dari halaman update rtl
yang berguna untuk mengubah isi dari rtl tersebut.

Gambar 34. tampilan halaman edit data rencana tindak lanjut

10) Halaman Data Ekstrim

Pada halaman data ekstrim akan menampilkan nilai
terendah dan tertinggi dari progres, IKK, dan IPAK.
Halaman ini hanya tampil pada pengguna super admin
dan admin provinsi, namun untuk admin provinsi hanya
menampilkan data ekstrim kabupaten/kota di provinsi
tersebut.

Gambar 35. tampilan halaman data ekstrim

E.

Evaluasi

Tahapan akhir dalam pengembangan sistem SKD adalah
evaluasi sistem dengan menggunakan black box testing dan
system usability scale.

1)

Black box testing

Black box testing adalah metode evaluasi sistem untuk
melihat apakah sistem yang dibuat berfungsi sesuai yang
diinginkan tanpa melihat source code. Dalam melakukan
uji black box pada pengembangan sistem ini dilakukan
oleh peneliti dan satu orang pegawai subdirektorat
rujukan statistik. Kesimpulan dari uji black box

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

menghasilkan bahwa sistem berfungsi sesuai dengan
yang diinginkan.
2)

System usability scale (SUS)

SUS adalah metode evaluasi sistem untuk menguji
persepsi kebermanfaatan sistem oleh pengguna. Uji SUS
dilakukan dengan cara memberikan kuisioner kepada
pengguna yang berisi sepuluh pertanyaan yang masing-
masing pertanyaan mempunyai rentang nilai 1-5. skor
dari uji SUS yang dilakukan kepada 11 orang pegawai
sub direktorat rujukan statistik, 1 orang pegawai BPS
Provinsi Sulawesi Tenggara, dan 1 orang pegawai BPS
kabupaten tasikmalaya menghasilkan nilai 78,08 yang
berarti bahwa sistem yang dibuat dapat diterima oleh
pengguna.

VII.

PENUTUP

Demikian Makalah ini dibuat untuk menyampaikan hasil
penelitian pengembangan Sistem SKD Modul output dan
manajemen pengguna. Penelitian ini telah berhasil membuat
dashboard monitoring, manajemen pengguna, dan berhasil
migrasi sistem yang lama.

Berdasarkan dari uji black box, sistem yang dibuat telah
berfungsi sebagai mana input yang dilakukan dan berdasarkan
uji SUS, sistem yang dikembangkan dapat diterima oleh
pengguna. Untuk rencana selanjutnya menyelesaikan draft
penelitian ini menjadi buku skripsi..

DAFTAR PUSTAKA

[1] BPS. Perka BPS Nomor 99 Tahun 2014 Pasal 7 tantang SKD
[2] BPS. Publikasi Analisis SKD. Jakarta: BPS, 2019.
[3] Fitriandari, Retno. Pengembangan Sistem Informasi Pengawasan
Dokumen untuk Perusahaan Pada Survei Sensus BPS Berbasis Web.
[SKRIPSI]. Jakarta: Politeknik Statistika STIS, 2018.

[4] Suhartono, Dimas Tresna. Pembangunan Sistem Informasi Helpdesk
Modul Monitoring, Analisis, dan Evalusai. [SKRIPSI]. Jakarta: Sekolah
Tinggi Ilmu Statistik, 2015.

[5] Wahono, Romi S (2012, 8), Kiat Menyusun Kerangka Pemikiran
Available:

Penelitian.
https://romisatriawahono.net/2012/08/07/kiat-menyusun-kerangka-
pemikiran-penelitian/

[Online]

[6] Whitten and Bentley, System Analysis & Design Methods. New York:

Mc.Graw-Hill,2007.

[7] Zakir, Ahmad., Implementasi Teknologi Framework Yii Pada Aplikasi
Berbasis Web. Jurnal Nasional Informatika Dan Teknologi Jaringan, 2(1),
45-48., 2017.

8 / 8

"
221709539,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Identifikasi Hoaks pada Berita Bahasa Indonesia 
Berbasis Teks 
Studi Kasus: Situs TurnBackHoax.id dan Detik.com 

Amanda Tabitha Bulan Panjaitan (221709539, 4SD2) 
Dosen Pembimbing: Ibnu Santoso, S.ST, M.T 

informasi 

semakin  maju 

Ringkasan—Teknologi 

tentu 
yang 
mendatangkan  banyak  kemudahan  bagi  para  penggunanya, 
mulai  dari  kemudahan  berkomunikasi,  bertukar  informasi  dan 
banyak  hal  positif  lainnya.    Akan  tetapi,  kemudahan  ini  justru 
mempercepat  penyebaran  berita  bohong  pada  internet.  Hoaks 
adalah 
sesat  dan  berbahaya  karena  dapat 
menyesatkan  persepsi  manusia  dengan  cara  memutarbalikkan 
fakta. Selain itu, berita palsu ini bertujuan untuk memengaruhi 
pembaca dengan menyajikan informasi yang salah sehingga para 
pembaca mempercayai berita tersebut dan mengambil tindakan 
berdasarkan  berita  tersebut.  Oleh  sebab  itu,  sebelum  menyebar 
lebih  luar  di  internet,  dibutuhkan  suatu  sistem  cerdas  yang 
bekerja dengan cara mengklasifikasi sebuah berita secara cepat. 
tidak 
Hal 
menyesatkan  para  pembacanya.  Pada  penelitian  ini,  dibangun 
sebuah  web  scraper  untuk  memperoleh  data  berita  berbahasa 
Indonesia.  Scraping  data  menghasilkan  dua  korpus  yang 
kemudian  dilakukan  klasifikasi  Random  Forest  dengan  metrik 
akurasi  sebagai  metode  evaluasi.  Diperoleh  tingkat  akurasi 
kedua korpus sebesar 88%. Model ini kemudian diimplementasi 
dalam  sistem  deteksi  hoaks  berbasis  web  yang  bertujuan  untuk 
mempermudah  masyarakat  dalam  mencari  tahu  kebenaran 
suatu berita. 

ini  bertujuan  agar  berita  bohong 

tersebut 

Kata Kunci—hoaks, klasifikasi, web scraping, data mining. 

I.  LATAR BELAKANG 

teknologi 

informasi 

Perkembangan  teknologi  informasi  saat  ini  sudah  semakin 
canggih  dan  merambah  ke  berbagai  bidang,  yang  tentunya 
membawa  banyak  dampak  positif  dalam  kehidupan  manusia. 
juga  memengaruhi 
Perkembangan 
hubungan  sosial  dalam  masyarakat,  baik  dalam  cara 
berkomunikasi  maupun  dalam  kehidupan 
sehari-hari. 
Perkembangan  teknologi  informasi  erat  kaitannya  dengan 
frekuensi  penggunaan  internet  yang  semakin  tinggi.  Era 
internet zaman ini mampu menghadirkan berbagai kemudahan 
yang  dapat  memenuhi  kebutuhan  masyarakat  akan  informasi 
maupun  pemanfaatan  untuk  kepentingan  sosial  ekonomi.   
Jarak  bukan 
lagi  penghalang  untuk  berkomunikasi  dan 
terdekat,  karena  era  digital 
berinteraksi  dengan  orang 
menawarkan  akses  internet  memungkinkan  siapa  saja  untuk 
tetap  bisa  dekat  dengan  mereka  yang  terpisah  oleh  jarak. 
Kemudahan  berinteraksi  melalui  internet  ini  mempermudah 
pertukaran informasi antar pengguna. Selain itu, banyak sekali 
kemudahan  yang ditawarkan melalui teknologi  yang semakin 
canggih. Teknologi ini tidak hanya dirasakan oleh masyarakat 
di perkotaan saja namun sudah merambat hingga perdesaan.  

Di  sisi  lain,  kehadiran  internet  juga  membuka  ruang  lebar 
bagi  penyebaran  informasi  atau  berita-berita  bohong  tentang 
suatu peristiwa yang meresahkan publik dikenal sebagai hoaks. 
informasi, 
Teknologi  memang  mempermudah  pertukaran 
namun tidak semua informasi tersebut merupakan fakta.  

Hoaks  adalah  sebagai  rangkaian  informasi  yang  memang 
sengaja  disesatkan,  namun  dijual  sebagai  kebenaran[1]. 
Dengan  kata  lain,  hoaks  juga  didefinisikan  sebagai  upaya 
pemutarbalikan  fakta  menggunakan  informasi  yang  seolah-
olah  meyakinkan  tetapi  belum  atau  tidak  dapat  diverifikasi 
kebenarannya.  Beberapa  informasi  hoaks  disebabkan  oleh 
perseorangan  dan  ada  pula  yang  disebabkan  oleh  organisasi 
yang  mengkhususkan dirinya  dalam bidang pembuatan berita 
serta  informasi  hoaks,  yang  kemudian  menyebarkannya  pada 
masyarakat luas. 

Beredarnya  berita  bohong  yang  kini  sudah  menjadi 
konsumsi  sehari-hari  masyarakat  dan  telah  dianggap  sebagai 
informasi  benar  atau  fakta  akibat  semakin  masifnya  berita 
hoaks saat ini. Selain itu, pengetahuan dan sumber masyarakat 
untuk  membedakan  suatu 
informasi  atau  berita  yang 
diperolehnya benar atau salah masih kurang, sehingga hal ini 
mengakibatkan  semakin  banyak  masyarakat  yang  termakan 
berita hoaks. Data yang disampaikan oleh Rudiantara, Menteri 
Komunikasi  dan 
(Menkominfo)  Republik 
Indonesia,  menunjukkan bahwa  penyebaran hoaks dan ujaran 
kebencian  diindikasikan  berasal  dari  800  ribu  situs  di 
Indonesia[2]. 

Informatika 

Misinformasi  ini  tentu  mendatangkan  banyak  keresahan 
bahkan  sampai  merenggut  korban  jiwa.  Seperti  yang  terjadi 
pada  tahun  2017  yakni  bentrok  ojek  online  dan  sopir  angkot 
akibat informasi simpang siur atau berita hoaks  yang beredar 
di antara kedua belah pihak[3]. Sesudah itu, pada tahun 2019 
tanah air digemparkan oleh Kerusuhan Wamena yang ternyata 
dipicu  oleh  hoaks  rasisme[4].  Kemudian,  pada  tahun  2020 
dalam  publikasi  The  American  Journal  of  Tropical  Medicine 
and  Hygiene  mengungkapkan  bahwa  misinformasi  terkait 
Covid-19 telah menelan setidaknya 800 korban jiwa di seluruh 
dunia[5]. 

Dosen  Departemen  Ilmu  Komunikasi  UGM,  Zainuddin 
Muda  Z.  Monggilo,  menyatakan  bahwa  literasi  media  dan 
informasi  di  zaman  internet  sangatlah  penting.  Di  zaman 
sekarang,  beliau  menyebut  masyarakat  kebingungan  untuk 
menentukan  apakah  suatu  berita  merupakan  fakta  atau 
hoaks[6].  Melalui  literasi  media,  masyarakat  menjadi  lebih 
kritis,  peka 
serta 
meningkatkan  kemampuan  kualitas  dan  kuantitas  intelektual. 

informasi  media  massa, 

terhadap 

 1 / 8 

 
 
 
 
 
 
Meningkatnya  daya  pikir  kritis  dan  analisis  yang  semakin 
tajam  diharapkan  dapat  menjadi  bekal  masyarakat  dalam 
menghadapi  masifnya  informasi  yang  beredar.  Masyarakat 
dapat  melakukan  analisis  unsur-unsur  bahasa  dalam 
mendeteksi berita bohong. Judul, teras, dan narasi berita yang 
diproduksi  dengan  sarana  bahasa,  dapat  digunakan  sebagai 
parameter  pendeteksi  gejala  sebuah  berita  hoaks  [7].  Berita 
yang  bersifat  provokatif,  menggunakan  tanda  baca  yang 
berlebihan  dan  kalimat  imperatif,  serta  susunan  bahasa  yang 
tidak  baku  mengindikasikan  bahwa  berita  tersebut  adalah 
hoaks. 

Selain  meningkatkan  literasi  masyarakat  dan  memahami 
ciri-ciri  berita  hoaks,  pemerintah  sendiri  telah  turun  tangan  
untuk  mengantisipasi  atau  mengurangi  jumlah  berita  bohong 
yang  beredar  di  masyarakat  dengan  meluncurkan  laman 
TurnBackHoax.id.  TurnBackHoax  bekerja  sama  dengan 
Masyarakat Anti Fitnah dan Hoax Indonesia (Mafindo) untuk 
membantah  ”kabar  burung”,  mengklarifikasi  informasi  yang 
simpang siur, dan memberikan penjelasan yang lugas. Metode 
identitas  atau  klasifikasi  yang  dilakukan  pada  situs  tersebut 
masih dilakukan secara manual, sehingga  menyulitkan proses 
klasifikasi  kebenaran  berita  dikarenakan  informasi  yang 
masuk  semakin  banyak.  Oleh  karena  itu,  dibutuhkan  sistem 
pendukung  untuk  membantu  proses  klasifikasi  berita  secara 
cepat dan tepat.  

Saat  ini,  berbagai  penelitian  telah  menerapkan  teknik 
machine  learning  untuk  mendeteksi  hoaks  yang  beredar. 
Namun, sejauh ini teknologi penangkal hoaks yang diterapkan 
dalam sistem pendeteksi hoaks masih jarang ditemukan 

Dari  permasalahan  yang 

telah  disampaikan  di  atas, 
ini  ditujukan  untuk  mencari  cara  bagaimana 
penelitian 
mendeteksi  hoaks  agar 
tidak  meresahkan  masyarakat.  
Penelitian  ini  membangun  web  deteksi  hoaks  yang  bertujuan 
agar  masyarakat  luas  dapat  melakukan  klasifikasi  terhadap 
berita  yang  mereka  terima  secara  cepat  dan  tepat  sehingga 
dapat  mencegah  penyebaran  berita  hoaks  di  internet.  Sistem 
menerima  input  berupa  tulisan  atau  paragraf  berbahasa 
tulisan  atau 
Indonesia,  kemudian  menganalisis  apakah 
paragraf tersebut merupakan berita hoaks atau bukan. 

Adapun  batasan  dari  penelitian  ini  bahwa  hasil  sistem 
hanya  mampu  memberi  label  ”1”  dan  ”0”  pada  suatu  berita, 
yakni menyatakan apakah berita tersebut hoaks atau fakta. 

II.  TUJUAN PENELITIAN 

Berdasarkan 

rumusan  masalah, 

tujuan  dilakukannya 

penelitian ini adalah: 

1.  Membangun  model  klasifikasi  untuk  memprediksi 
kebenaran  suatu  berita  yang  dipengaruhi  oleh 
karakteristik  berita  tersebut  dan  mengukur  kebaikan 
model. 

2.  Membandingkan  model  untuk  klasifikasi  secara 

otomatis. 

3.  Menyediakan dataset berita yang sudah terklasifikasi 

untuk publik. 
4.  Membangun 
Indonesia. 

sistem  deteksi  hoaks  berbahasa 

III. PENELITIAN TERKAIT 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar  1  merupakan  peta 

literatur  penelitian  yang 
dilakukan.  Secara  garis  besar,  kajian  literatur  mengenai 
klasifikasi  dokumen  terbagi  dua,  yakni  klasifikasi  dokumen 
secara  umum  dan  klasifikasi  berita  hoaks.  Literatur  yang 
berada  di  tengah  peta  membahas  tentang  bagaimana  cara 
mengatasi 
tidak 
seimbang. 

imbalanced  dataset  atau  dataset  yang 

Gambar 1. Peta Literatur (Literature Map) 

Tabel  di  bawah  ini  merupakan  penjelasan  lebih  lanjut 
mengenai peta literatur yang berkaitan dengan penelitian yang 
dilakukan. 

TABEL I 
TABEL PENELITIAN TERKAIT 

No 

Judul 

1  Deteksi 
Hoaks 
Pada Berita 
Berbahasa 
Indonesia 
Seputar 
COVID-19 
[8] 

2  Klasifikasi 
Hoax Pada 
Berita 
Kesehatan 
Berbahasa 
Indonesia 
Dengan 
Mengguna
kan 
Metode 
Modified 
K-Nearest 
Neighbor 
[9] 

Penulis, 
Publikasi 
Amanda 
Tabitha 
Bulan 
Panjaitan dan 
Ibnu Santoso 
pada Jurnal 
Ilmiah 
Teknik 
Informatika 
tahun 2021, 
Vol. 10, No. 
1, pp 76-85 

Prasetyo, 
Indriati, dan 
Adikara pada 
Jurnal 
Pengembanga
n Teknologi 
Informasi dan 
Ilmu 
Komputer 
tahun 2018, 
Vol 2, No.12, 
pp. 7466-
7473 

Tertulis 

Komentar 

Penelitian ini 
datasetnya 
terbatas pada 
topik seputar 
COVID-19 saja, 
dibutuhkan 
dataset dengan 
topik yang 
berbeda. 

Akurasi sudah 
tinggi namun 
alangkah lebih 
baik apabila 
dataset 
diperbanyak. 

Pada penelitian 
ini dilakukan 
perbandingan 
terhadap 
pengaruh 
penerapan feature 
engineering. 
Metode random 
forest dengan 
penerapan feature 
engineering 
menghasilkan 
tingkat akurasi 
sebesar 96,05%. 

Penelitian ini 
menggunakan 51 
berita yang telah 
dilabeli oleh 
pakar, 67 berita  
dilabeli oleh tim 
hoax buster dan 
52 berita sisanya 
dilabeli oleh situs 
portal berita itu 
sendiri. Hasil 
akurasi tertinggi 
adalah dengan 
dengan k-fold 
yang bernilai 7 
adalah sebesar 
94,12%.  
(pp. 7472-7473)   

3   Sistem 

Pendeteksi 

Berita 

Hoax di 

Media 

Munawar, 
Yosua Riadi 
Silitonga 
pada 
Jurnal Ilmu 
Komputer, 
Vol. 4, No. 2, 

Berita-berita yang 
ada di sosial 
media khususnya 
Twitter dan 
Facebook bisa 
diidentifikasi 
apakah fake 

Perlu 
penambahan 
beberapa 
klasifikasi untuk 
meningkatkan 
hasil analisis dan 
penambahan 

 2 / 8 

(Prasetyo et al., 2018)Klasifikasi Berita Hoaks dengan Machine LearningImbalanced Dataset(Bisri and Rachmatika, 2019)Klasifikasi dengan Machine Learning(Meenakshi et al., 2019)Klasifikasi DokumenKlasifikasi Berita Hoaks(Munawar and Silitonga, 2019)(Jehad and Yousif, 2020)(Deb et al., 2020)(Panjaitan and Santoso, 2021 
 
 
 
 
Sosial 

dengan 

Teknik 

Data 

Mining 

Scikit 

Learn 

[10] 

4  Fake News 

Classificati

on Using 

Random 

Forest and 

Decision 

Tree (J48) 

[11] 

5  A 

Comparati

ve Analysis 

Of News 

Categoriza

tion 

Using 

Machine 

Learning 

Approache

s  

[12] 

6  Credit 
Card 
Fraud 
Detection 
Using 
Random 
Forest 
[13] 

pp. 173-179  

Reham Jehad 
and Suhad A. 
Yousif pada 
ANJS, 
Vol.23, No. 
4, pp. 49-55 

Nabamita 
Deb, Vishesh 
Jha, Alok K 
Panjiyar, 
Roshan Kr 
Gupta 
International 
Journal of 
Scientific & 
Technology 
Research, 
Vol. 9, No. 1, 
pp. 2469-
2472 

Devi 
Meenakshi, 
Janani.  
Gayathri, 
Mrs. Indira 
International 
research 
journal of 
engineering 
and 
technology 
(IRJET), Vol. 
06, No.03, 
pp. 6662-
6666 

7 

Integrasi 
Gradient 
Boosted 

Bisri, A.,  
Rachmatika, 
R., pada 

(palsu) atau real 
(fakta) dengan 
membuat model 
klasifikasi dengan 
TF-IDF, 
CountVectorizer, 
Passive Agressive 
Classifier dan 
Support Vector 
Classifier.  

Penelitian ini 
memanfaatkan 
dua mesin yang 
berbeda 
mempelajari 
algoritma. 
Akurasi Decision 
Tree (J48) adalah 
sebesar 
89,11% 
sedangkan 
Random Forest 
84.97% 
Penelitian ini 
membandingkan 
metode Naive 
Bayes, Random 
Forest, Decision 
Tree, SVM dan 
Neural Networks 
dalam 
mengklasifikasi 
berita 
berdasarkan 
kategori topik 
berita tersebut. 

Hasil dari 
Algoritma 
Random Forest 
akan memberikan 
akurasi yang 
lebih baik apabila 
datasetnya 
diperbanyak, 
namun ini akan 
memperlambat 
kecepatan saat 
testing. 
Algortima SVM 
juga memberikan 
hasil yang kurang 
baik akibat 
imbalanced 
dataset. (pp. 
6666) 
Penerapan 
SMOTE dan 
bagging 

media sosial yang 
lain agar 
kesimpulan yang 
didapat bisa lebih 
tajam.  

Hasil penelitian 
mampu 
mengklasifikasi 
berita hoaks 
dengan baik 
namun belum ada 
pembangunan 
sistem deteksi 
hoaks pada 
penelitian ini. 

Penelitian ini 
melakukan 
perbandingan 
classifier untuk 
menentukan 
metode mana 
yang paling baik 
dalam 
mengklasifikasi 
jenis berita. 
Namun hanya 
sekedar 
membandingkan 
saja, tidak 
dilakukan 
pembangunan 
aplikasi untuk 
melakukan 
klasifikasi berita 
secara cepat. 

Diperlukan cara 
untuk mengatasi 
permasalahan 
imbalanced 
dataset, agar 
akurasi semakin 
baik dan 
kecepatan testing 
juga tidak 
memakan waktu 
yang lama. 

SMOTE dapat 
digunakan untuk 
mengatasi 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Jurnal 
JNTETI, Vol. 
8, No. 4, pp. 
209-314 

Trees 
dengan 
SMOTE 
dan 
Bagging 
untuk 
Deteksi 
Kelulusan 
Mahasiswa 
[14] 

ketidakseimbanga
n dataset karena 
pada data berita 
yang discrap 
nantinya 
kemungkinan 
besar akan terjadi 
ketidakseimbanga
n dataset. 

menghasilkan 
akurasi yang 
lebih baik yakni 
sebesar 80,57%. 
Hal  terbukti 
mampu 
memberikan 
solusi terhadap 
penanganan 
masalah 
ketidakseimbanga
n kelas dan dapat 
meningkatkan 
kinerja model 
klasifikasi GBT. 
(pp. 312) 

teks  beritanya  yakni  berasal  dari 

IV. METODE PENELITIAN  
Penelitian ini dimulai dengan melakukan pengumpulan data 
laman  web  yang 
dengan  cara  web  scraping.  Adapun 
laman 
dikumpulkan 
TurnBackHoax.id  dan  Detik.com.  Situs  TurnBackHoax.id 
sudah  melakukan 
yang 
dikumpulkan. Klasifikasi kebenaran berita dapat dilihat dapat 
judul  beritanya.    Jika  terdapat  tag  [Salah]  maka  akan  diberi 
label 1, jika terdapat label [Benar] maka diberi label 0. Untuk 
data  sumber berita  Detik.com diberi label 0. Berikut diagram 
alur penelitian: 

klasifikasi 

terhadap 

berita 

Gambar 2. Flowchart Scraping Teks Berita 

Penelitian  ini  menghasilkan  dua  korpus.  Korpus  pertama 
menggunakan  data  berita  hanya  dari  situs  TurnBackHoax.id 
dan  korpus  kedua  menggunakan  tambahan  data  berita  dari 
situs Detik.com. Langkah pertama yaitu membuka laman web 
media berita online tersebut. Setelah itu, melakukan pencarian 
berita dan pada halaman web akan ditampilkan hasil pencarian. 
URL  berita  kemudian  diekstrak  dan  dilakukan  scraping  data 
dengan  menggunakan  bahasa  pemrograman  Python.  Setiap 
judul  dan  narasi  dari  masing-masing  berita  diekstrak  dan 
disimpan  dalam  ekstensi  CSV.  Apabila  masih  terdapat  judul 
dan  narasi  URL  berita  yang  belum  diekstrak,  tahap  scraping 
dilanjutkan.  Apabila  sudah  tidak  ada,  proses  berhenti.  Data 
berita  kemudian  melewati 
tahap  preprocessing  dan 
pemrosesan seperti diagram alur sebagai berikut: 

 3 / 8 

 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

apakah  berita  yang  diinput  oleh  pengguna  merupakan  berita 
hoaks atau fakta. 

V.  KERANGKA PIKIR 

Saat  ini,  banyak  penelitian  yang  sudah  menerapkan  teknik 
machine  learning  untuk  mengklasifikasi  kebenaran  suatu 
dokumen  berita.  Bahkan,  terdapat  beberapa  penelitian  yang 
telah  mempelajari  klasifikasi  berita  hoaks  dengan  berbagai 
jenis  classifier.  Namun,  hanya  sedikit  yang  membangun  alat 
untuk  melakukan  klasifikasi  teks  berita  secara  otomatis. 
Penelitian ini tidak hanya mempelajari jenis klasifikasi terbaik 
untuk  mengelompokkan  teks  berita,  namun  juga  membangun 
web  untuk  melakukan  klasifikasi  secara  cepat  dan  otomatis 
terhadap input berita yang dimasukkan oleh pengguna. 

Penelitian  ini  diawali  dengan  melakukan  scraping  teks 
berita.  Kemudian,  dilakukan  preprocessing  terhadap  teks 
berita  dan  pemilihan  jenis  classifier  yang  akan  digunakan. 
Classifier  terpilih  merupakan  classifier  yang  menghasilkan 
akurasi,  presisi,  dan  f1-score  tertinggi.  Setelah  itu  dilakukan 
pembangunan web deteksi hoaks.  

teori  dengan  berbagai 

Kerangka  pikir  merupakan  model  konseptual 

tentang 
hubungan 
telah 
diidentifikasi  sebagai  masalah  yang  penting[15].  Gambaran 
kerangka  pikir  penelitian  ini  dijelaskan  melalui  gambar 
sebagai berikut: 

faktor  yang 

Gambar 3 Flowchart Pemrosesan Dataset 

feature  extraction,  dan 

Dataset yang didapatkan dari hasil scraping dibagi menjadi 
dua, yakni data latih dan data uji. Kedua dataset ini kemudian 
melewati  preprocessing, 
feature 
engineering. Setelah itu, diperoleh data siap latih dan data siap 
uji.  Selanjutnya,  dilakukan  pemodelan  dengan  beberapa  jenis 
classifier pada data siap latih sehingga terpilih model terbaik. 
Dari  akurasi  kelima  model  yang  diukur,  algoritma  Random 
Forest  merupakan  model  yang  terbaik.  Model  ini  kemudian 
diterapkan dan didapatkan hasil prediksi pada data siap uji. 

Selanjutnya,  setelah  model  terbaik  terpilih,  penelitian  ini 
memasuki tahap pembangunan web deteksi hoaks. Rancangan 
diagram alur pembangunan web dapat dilihat pada Gambar 4. 

Gambar 4. Flowchart Sistem Deteksi Hoaks 

Gambar 5. Kerangka Pikir 

pada 

stopword,  melakukan 

Sistem  deteksi  hoaks  dimulai  saat  pengguna  menginput 
judul  dan  teks  berita.  Sistem  ini  kemudian  melakukan 
penghapusan 
stemming, 
menghitung TF-IDF, serta menerapkan feature extraction dan 
feature  engineering  pada  input  judul  dan  teks  berita.  Data 
berita  yang  telah  melewati  tahap  preprocessing  kemudian 
diperiksa 
dengan  menerapkan  metode 
klasifikasi  Random  Forest.  Sistem  deteksi  hoaks  akan 
menampilkan  hasil  klasifikasi  berita  dengan  menentukan 

kebenarannya 

VI. HASIL DAN PEMBAHASAN 

Penelitian  dimulai  dengan  melakukan  pengumpulan  berita 
dengan  menggunakan  metode  web  scraping.  Dataset  yang 
dikumpulkan  ada  dua  jenis  yang  kemudian  disebut  dengan 
korpus.  Korpus  1  merupakan  dataset  berita  dari  situs 
TurnBackHoax  dan  korpus  2  merupakan  dataset  dari  situs 
TurnBackHoax  dan  Detik.  Total  data  berita  untuk  korpus  1 

 4 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

adalah  sebanyak  5236  berita  dan  korpus  2  sebanyak  7435 
berita.   

Setelah  dilakukan  pengumpulan  data  berita,  dataset 
kemudian dibagi menjadi dua dengan proporsi 70% untuk data 
latih  dan  30%  untuk  data  uji.  Kedua  dataset  ini  kemudian 
melalui  tahap  preprocessing.  Tahap  preprocessing  dimulai 
tanda  baca, 
dengan  melakukan  penghapusan  Stopwords 
karakter  non-ASCII,  angka  dan  emotikon,  Tokenizing,  dan 
Stemming.  

Penelitian  ini  menerapkan  feature  extraction  dengan  TF-
IDF. TF (Term Frequency) adalah frekuensi dari kemunculan 
sebuah  kata  dalam  dokumen.    Sedangkan  IDF  (Inverse 
Document  Frequency)  merupakan  sebuah  perhitungan  dari 
bagaimana  kata  tersebut  didistribusikan  secara  luas  pada 
dokumen yang bersangkutan.  

Setelah tahap feature extraction, dilakukan eksplorasi data. 
Tujuan  dari  eksplorasi  data  yakni  untuk  membantu  proses 
pemilihan  fitur  pada  tahap  feature  engineering.  Gambar  di 
bawah  ini  merupakan  hasil  dari  eksplorasi  data  yang  telah 
dilakukan. 

Gambar 6 dan 7 menunjukkan frekuensi penggunaan tanda 
baca dalam berita hoaks dan berita fakta. Dari grafik tersebut 
terlihat  bahwa  penggunaan  tanda  seru  dan  tanda  tanya  pada 
berita  hoaks  masing-masing  berada  pada  posisi  keempat  dan 
kelima.  Sedangkan  pada  berita  fakta,  tanda  tanya  menempati 
posisi  kesepuluh  dan  tanda  seru  menempati  posisi  kesebelas. 
Hal 
ini  menunjukkan  bahwa  berita  hoaks  cenderung 
menggunakan  tanda  seru  dan  tanda  tanya  pada  konten 
beritanya, berbeda dengan berita fakta yang cenderung jarang 
menggunakan kedua tanda baca ini. 

Setelah  melakukan  eksplorasi  data,  tahap  selanjutnya 
adalah 
feature  engineering.  Feature  engineering  adalah 
bagaimana peneliti menggunakan pengetahuan dan hasil studi 
sebelumnya  dalam  memilih  features  atau  membuat  features 
baru,  yang  bertujuan  agar  model  machine  learning  dapat 
bekerja  lebih  akurat  dalam  memecahkan  masalah.    Adapun 
features  dan  hasil  perbandingan  feature  engineering  yang 
digunakan dapat dilihat pada tabel berikut. 

TABEL II 
TABEL PERBANDINGAN FEATURE ENGINEERING 

Feature 

Corpus 1 

Corpus 2 

Hoaks 

Fakta 

Hoaks 

Fakta 

Berita dengan Awalan 
Bukan Huruf Kapital 

2870 

584 

2503 

2491 

Rata-Rata Kemunculan 
Tanda Seru 

Rata-Rata Kemunculan 
Tanda Tanya 

Cosine Similarity 

2,56 

1,98 

0,46 

2,02 

2,63 

1,63 

1,95 

1,78 

1,51 

0,41 

0,46 

0,36 

Feature  pertama  adalah  melihat  awalan  narasi  berita 
dengan  asumsi  teks  bukan  hoaks  biasanya  mengikuti  ejaan 
yang disempurnakan, termasuk penggunaan huruf kapital pada 
kata  pertama  dalam  suatu  kalimat.  Feature  selanjutnya  ada 
penggunaan  tanda  perintah  dan  tanda  tanya,  karena  berita 
hoaks  cenderung  berisi  kalimat-kalimat  provokatif  dan 
berunsur imperatif, baik berupa suruhan maupun larangan atau 
pertanyaan  yang  menggiring  opini.  Feature  terakhir  adalah 
cosine  similarity  untuk  menentukan  kemiripan  kata  antara 
judul dan narasi. 

Dari  Tabel  II  dapat  dilihat  perbedaan  karakteristik  berita 
hoaks  dan  fakta  dari  keempat  feature  yang  dibentuk.  Pada 
berita  hoaks,  lebih  banyak  narasi  yang  dituliskan  dengan 
awalan  huruf  kecil  dibandingkan  dengan  berita  bukan  hoaks. 
Salah satu hal yang dapat menjadi penyebabnya adalah berita-
berita  hoaks  umumnya  disebarkan  tidak  melalui  berita  resmi 
dan  tidak  mengikuti  aturan  penulisan  baku.  Kemudian,  rata-
rata  kemunculan  tanda  tanya  dan  tanda  perintah  pada  berita 
hoaks  lebih  banyak  dibandingkan  pada  berita  fakta.  Untuk 
nilai  cosine  similarity  judul  dengan  narasi  berita  hoaks 
nilainya lebih tinggi dibandingkan berita bukan hoaks, hal ini 
dapat  disebabkan  oleh  karakteristik  berita  hoaks  yang 

 5 / 8 

Gambar 6. Penggunaan Tanda Baca pada Berita Hoaks 

Gambar 7. Penggunaan Tanda Baca pada Berita Fakta 

 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

dan 

untuk 

mempercepat 

Setelah dilakukan pemilihan terhadap classifier terbaik dan 
analisis  data,  kemudian  dilakukan  pembangun  sistem  deteksi 
mempermudah 
hoaks 
pengklasifikasian  terhadap  berita.  Sistem  Deteksi  Hoaks 
merupakan  program  berbasis  web  yang  menjalankan 
pendeteksian terhadap berita yang dimasukkan oleh pengguna 
ke  dalam  sistem  untuk  diperiksa  apakah  berita  tersebut 
termasuk 
ini 
mengimplementasikan kedua model yang telah dibangun dari 
masing-masing  korpus.  Berikut  merupakan  tampilan  dari 
sistem berbasis web yang telah dibangun. 

Sistem 

hoaks 

berita 

fakta. 

atau 

cenderung  berasal  dari  broadcast  sehingga  judul  dan  narasi 
merupakan hal yang sama. 

Berita  hoaks  menggunakan  lebih  banyak  tanda  baca 
dibandingkan berita fakta. Pada berita hoaks, tanda baca yang 
paling  sering  digunakan  adalah  tanda  titik,  koma  dan  tanda 
seru. Hal ini dapat terjadi karena berita hoaks cenderung berisi 
kalimat-kalimat  provokatif  yang  menggunakan  banyak  tanda 
seru.    Hal  ini  sesuai  dengan  penelitian  oleh  Aribowo  yang 
menyatakan bahwa penggunaan bahasa berita hoaks umumnya 
tidak mengikuti kaidah penulisan Bahasa Indonesia yang baik 
dan  benar  dimana  penggunaan  tanda  bacanya  cenderung 
berlebihan[7]. 

Setelah melakukan pemilihan feature yang didasarkan pada 
hasil  eksplorasi  data,  maka 
tahap  selanjutnya  adalah 
melakukan  pemilihan  jenis  classifier  yang  menghasilkan 
  Tabel  III  menunjukkan  perbandingan 
akurasi 
pengukuran metrik classifier pada korpus 1 dan korpus 2. 

terbaik. 

TABEL III 
TABEL PERBANDINGAN AKURASI CLASSIFIER 
Classifier 
Random Forest 

Corpus 1 
88,15 

Corpus 2 
88,47 

Logistic Regression 

84,2 

KNN 

Naïve Bayes 

Tree 

84,02 

83,48 

80,43 

87,51 

77,90 

84,72 

81,17 

Kedua  korpus  menggunakan  Random  Forest  sebagai 
classifier-nya,  akurasi  korpus  1  adalah  sebesar  88,15%  dan 
akurasi  korpus  2  sebesar  88,47%.  Tabel  III  dan  IV 
menunjukkan laporan klasifikasi kedua korpus. 

TABEL IV 
TABEL LAPORAN KLASIFIKASI CORPUS 1 

Label 

Precision 

1 

0 

0,88 

0,88 

Recall 

0,99 

0,27 

F1-Score 

0,93 

0,41 

TABEL V 
TABEL LAPORAN KLASIFIKASI CORPUS 2 

Label 
1 

Precision 
0.92 

Recall 
0.83 

F1-Score 
0.88 

0 

0.86 

0.93 

0.89 

Dari  tabel  di  atas  dapat  dilihat  bahwa  korpus  1  yang 
menggunakan  data  berita  dari  TurnBackHoax  sangat  baik 
dalam mengklasifikasi berita hoaks namun kurang tepat ketika 
mengklasifikasi  berita  fakta.  Disisi  lain,  korpus  2  yang 
menggunakan  data  berita  dari  TurnBackHoax  dan  Detik 
mampu  mengklasifikasi  berita  fakta  dengan  baik  begitu  juga 
dengan berita hoaks,  namun laporan klasifikasi korpus 2 saat 
mendeteksi berita hoaks tidak sebaik korpus 1.  

Gambar 8. Tampilan Awal Web Deteksi Hoaks 

Gambar 9. Input Judul dan Narasi pada Korpus 1 

Gambar 10. Hasil Prediksi dengan Korpus 1 

 6 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 11. Input Judul dan Narasi pada Korpus 2 

Gambar 13. Download Dataset Korpus 1 dan 2 

Pada  halaman  awal  web  deteksi  hoaks  terdapat  link 
download  dataset  untuk  korpus  1  dan  2.  Dengan  adanya 
dataset  yang  tersedia  bagi  publik  ini,  diharapkan  dapat 
berguna untuk dikembangkan dalam penelitian lebih lanjut. 

VII. 

PENUTUP 

Berdasarkan  pembahasan  di  atas,  kesimpulan  yang 

diperoleh pada penelitian ini yakni sebagai berikut:  

1.  Berita-berita  yang  ada  bisa  diidentifikasi  apakah 
merupakan  hoaks  atau  fakta  dengan  membuat 
feature 
model  klasifikasi  dengan  TF-IDF, 
engineering, cosine similarity, dan Random Forest.   

2.  Hasil  klasifikasi  menggunakan  metode  Random 
Forest  pada  kedua  korpus  menunjukkan  akurasi 
sebesar 88%. Hal ini  menunjukkan bahwa  kinerja 
metode  yang  diusulkan  cukup  handal  untuk 
mendeteksi berita hoaks. 

3.  Dengan  adanya  penambahan  dataset,  model  yang 
dibangun  semakin  baik  pula.  Dataset  yang 
diperoleh  dapat  menjadi  bahan  pembelajaran 
untuk penelitian selanjutnya. 

4.  Sistem  deteksi  hoaks  yang  dibangun  berperan 
sebagai 
sistem  pendukung  keputusan  yang 
berguna  untuk  menangkal  berita  hoaks  agar  tidak 
menyebar luas di internet. 

Berdasarkan  kesimpulan  yang  telah  dijabarkan  di  atas, 
maka  berikut  adalah  saran  yang  diberikan  untuk  penelitian 
yang akan datang: 

1.  Mengembangkan  sistem  yang  dapat  menerima 
input  selain  teks  seperti  gambar,  video  maupun 
audio untuk dideteksi kebenarannya. 

2.  Diharapkan 

peneliti 

dapat 
cara 
penelitian 
mengembangkan 
mendeteksi  persentase  kebenaran  suatu  dokumen 
berita. 

selanjutnya 

dengan 

DAFTAR PUSTAKA 
[1]  Silvernan,  Craig,  “Lies,  Journalism:  A  Tow/Knight  Report.  Lies,  Damn 
[Online].  Available  doi: 

Lies  and  Viral  Content”,  Feb  2015. 
https://doi.org/10.7916/D8Q81RHH 

Gambar 12. Hasil Prediksi dengan Korpus 2 

Gambar  8  merupakan  tampilan  awal  web  deteksi  hoaks. 
Pengguna  hanya  perlu  memilih  korpus  mana  yang  akan 
digunakan sebagai dasar prediksi kebenaran berita.  

Gambar  9  dan  11  adalah  tampilan  input  judul  dan  narasi 
berita  pada  web  deteksi  hoaks.  Pengguna  memasukkan  judul 
berita  dan  narasi.  Kemudian,  pengguna  menekan  tombol 
prediksi  dan  hasil  klasifikasi  akan  ditampilkan.  Gambar  10 
dan  12  menunjukkan  hasil  prediksi  masing-masing  korpus. 
Seperti  yang  telah  dijelaskan  pada  batasan  penelitian,  sistem 
ini  hanya  melakukan  klasifikasi  terhadap  kebenaran  suatu 
berita  sehingga  outputnya  hanya  menunjukkan  apakah  berita 
yang diinput merupakan berita hoaks atau bukan.  

Sistem  ini  mengimplementasikan  model  machine  learning 
menggunakan  random  forest  yang  telah  dibangun.  Pada 
bagian  bawah  setelah  hasil  klasifikasi  ditampilkan  akurasi 
classifier pada tiap korpus. 

Sistem  pendeteksi  hoaks  berperan 

sistem 
ini, 
pendukung  keputusan.  Melalui  web  deteksi  hoaks 
masyarakat  luas  dapat  memanfaatkannya  untuk  melakukan 
langkah awal pencegahan penyebaran berita hoaks. Pengguna 
dapat  dengan  cepat  memprediksi  kebenaran  sebuah  cerita 
dengan model machine learning yang telah dibangun. 

sebagai 

[2]  Kominfo. 

(2017,  12).  Ada  800.000  Situs  Penyebar  Hoax  di 

Indonesia.[Online]. Available:  
https://kominfo.go.id/content/detail/12008/ada-800000-situs-penyebar-
hoax-di-indonesia/0/sorotan_media 

[3]  Liputan 6. (2017, 3). Biang Kerok Hoax di Balik Ricuh Ojek Online Vs 

Angkot.[Online]. Available:  

 7 / 8 

 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

https://www.liputan6.com/news/read/2898956/biang-kerok-hoax-di-
balik-ricuh-ojek-online-vs-angkot 

[4]  Kompas.  (2019,  9).  Polri  Sebut  Hoaks  Picu  Kerusuhan  di  Wamena. 

[Online]. Available: 
https://nasional.kompas.com/read/2019/09/23/12301861/polri-sebut-
hoaks-picu-kerusuhan-di-wamena.  

[5]  Forbes.  (2020,  8).  Report:  More  Than  800  Deaths  And  5,800 
Hospitalizations  Globally  May  Have  Resulted  From  COVID-19 
: 
Misinformation 
https://www.forbes.com/sites/markhall/2020/08/23/coronavirus-
misinformation/#dc3c9f01684e 

Available 

[Online]. 

Online. 

[6]  Liputan  6.  (2020,  8).  Pentingnya  Literasi  Media  untuk  Menangkal 
[Online].  Available: 

Informasi  Hoaks 
https://www.liputan6.com/cek-fakta/read/4342306/pentingnya-literasi-
media-untuk-menangkal-informasi-hoaks-di-media-sosial 

di  Media 

Sosial. 

[7]  E.  K.  Aribowo.  (2017,  10)  “Menelusuri  Jejak  Hoaks  dari  Kacamata 
Bahasa: Bagaimana Mendeteksi Berita Palsu Sedini Mungkin”. [Online]. 
Available: osf.io/preprints/inarxiv/k2at4. 

[8]  Panjaitan,  A.T.B,  Santoso,  I.,  Deteksi  Hoaks  Pada  Berita  Berbahasa 
Indonesia  Seputar  COVID-19,  Jurnal  Ilmiah  Teknik  Informatika  tahun 
2021, vol. 10, no. 1, pp 76-85, 2021. 

[9]  Prasetyo, A. R., Indriati, and Adikara, P. P, Klasifikasi Hoax Pada Berita 
Kesehatan Berbahasa Indonesia Dengan Menggunakan Metode Modified 
K-Nearest  Neighbor,  Jurnal  Pengembangan  Teknologi  Informasi  dan 
Ilmu Komputer, vol. 2, no. 12,  pp. 7466-7473, Dec 2018. 

[10] Munawar,  Silitonga,  Y.R.,  Sistem    Pendeteksi    Berita    Hoax    di  Media  
Sosial    dengan    Teknik  Data    Mining    Scikit    Learn,  Jurnal  Ilmu 
Komputer, vol. 4, no. 2, pp. 173-179, Dec 2019. 

[11] Jehad, R., Yousif, A.R., Fake News Classification Using Random Forest 
and Decision Tree (J48), ANJS, vol. 23, no. 4, pp. 49-55, Dec 2020. 
[12] Deb, N., Jha, V., Paniyar, A., and Gupta, R.K, A Comparative Analysis 
Of  News  Categorization  Using  Machine  Learning  Approaches, 
International Journal of Scientific & Technology Research, vol. 9, no. 1, 
pp. 2469-2472, Jan 2020. 

[13] Meenakshi, D.,  Janani, Gayathri, Mrs Indira, Credit card fraud detection 
using Random forest, International research journal of engineering and 
technology (IRJET), vol. 06, no.03, pp. 6662-6666 , Mar 2019. 

[14] Bisri,  A.,    Rachmatika,  R.,  Integrasi  Gradient  Boosted  Trees  dengan 
SMOTE dan Bagging untuk Deteksi Kelulusan Mahasiswa, JNTETI, vol. 
8, no. 4, pp. 309-314, Nov 2019. 

[15] Sugiyono, Metode Penelitian Kuantitatif, Kualitatif, dan R&D. Bandung : 

Alfabeta, CV,  2017. 

 8 / 8 

 
 
"
221709534,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Pengembangan Sistem Informasi Kompetensi
Pegawai Pusdiklat Badan Pusat Statistik

Alvin Pratama (221709534, 4SI1)

Dosen Pembimbing: Dr. Drs. Waris Marsisno M.Stat

Ringkasan— Sesuai dengan PP nomor 1 tahun 2017,
setiap PNS berhak mengikuti kegiatan pengembangan
kompetensi sekurang-kurangnya 20 jam pelajaran dalam
satu tahun. Untuk keperluan pemantauan perkembangan
dari pengembangan kompetensi yang telah diikuti oleh
setiap pegawai, Pusat Pendidikan dan Pelatihan BPS
mengembangkan suatu sistem informasi berbasis web.
Tujuan dari penelitian ini adalah menambahkan fitur
yang memungkinkan pegawai untuk melakukan entri data
tentang pengembangan kompetensi yang telah dan akan
diikutinya. Metode yang digunakan dalam pengembangan
ini adalah RAD. Pengembangan sistem telah berhasil
dilakukan dan dari hasil pengujian black box, semua
fungsi yang dirancang telah dapat berfungsi dengan
semestinya.

Kata Kunci— Pusdiklat, BPS, Sistem Informasi

I. LATAR BELAKANG

Pusat Pendidikan dan Pelatihan (Pusdiklat BPS) merupakan
unsur pelaksana Badan Pusat Statistik (BPS) di bidang
pendidikan dan pelatihan. Secara teknis operasional, Pusdiklat
berada di bawah dan bertanggung jawab kepada Kepala BPS
dan secara teknis administrasi dibina oleh Sekretaris Utama
BPS. Tugas Pusdiklat adalah melaksanakan penyelenggaraan
pendidikan dan pelatihan prajabatan dan kepemimpinan serta
pendidikan dan pelatihan teknis dan fungsional, sedangkan
fungsinya antara lain menyusun kebijakan teknis, rencana,
program, kegiatan, dan anggaran di bidang pendidikan dan
pelatihan, melaksanakan pendidikan dan pelatihan prajabatan,
kepemimpinan,
teknis, dan fungsional, serta melaksanakan
tugas administrasi Pusat pendidikan dan pelatihan.

tunjangan, fasilitas, cuti,

Berdasarkan Undang-Undang Nomor 5 Tahun 2014 tentang
Aparatur Sipil Negara, salah satu hak dari Pegawai Negeri
Sipil (PNS) adalah melakukan pengembangan kompetensi,
selain memperoleh gaji,
jaminan
pensiun, jaminan hari tua, dan perlindungan. Pengembangan
kompetensi
lain dapat berupa
dimaksud antara
keikutsertaan dalam pendidikan dan pelatihan, seminar, kursus,
dan penataran. Selain itu dapat juga dilakukan melalui praktik
lain di pusat dan daerah atau melalui
kerja di
pertukaran antara PNS dengan pegawai swasta dalam durasi
praktik dan pertukaran pegawai paling lama 1 (satu) tahun.

instansi

yang

Dalam Peraturan Pemerintah (PP) Nomor 1 Tahun 2017
disebutkan bahwa setiap PNS memiliki hak dan kesempatan
yang sama untuk diikutsertakan dalam pengembangan
kompetensi dengan memperhatikan hasil penilaian kinerja dan
penilaian kompetensi PNS yang bersangkutan. PP yang sama
juga menyebutkan bahwa setiap PNS wajib melakukan
pengembangan kompetensi paling sedikit 20 jam pelajaran
inilah yang
dalam satu tahun. Pengembangan kompetensi

kemudian menjadi dasar untuk pengembangan karier dan
merupakan salah satu dasar untuk pengangkatan dalam
jabatan.

tersebut,

kompetensi

Dalam upaya monitoring pemenuhan hak PNS dalam hal
pengembangan
BPS
melakukan pengumpulan data terkait riwayat pengembangan
pengembangan
kompetensi
kompetensi merupakan informasi mengenai pengembangan
kompetensi yang pernah diikuti oleh pegawai baik yang
diselenggarakan oleh internal
instansi BPS maupun yang
diselenggarakan oleh instansi di luar BPS.

Pusdiklat

pegawai.

Riwayat

setiap

Dari hasil wawancara dengan satu pegawai Bagian Tata
Pusdiklat BPS yang merupakan stakeholder dari
Usaha
proses
sistem pemantauan pengembangan kompetensi,
pengumpulan riwayat pengembangan kompetensi saat
ini
dilakukan dengan menggunakan sistem berbasis file, yaitu
dengan menggunakan aplikasi Microsoft Excel. Setiap unit
kerja di level eselon 2 mengumpulkan riwayat pengembangan
kompetensi pada unit kerjanya begitu juga pada unit kerja di
eselon 3 di bawahnya. Riwayat pengembangan
level
kompetensi yang sudah terkumpul kemudian digabungkan
menjadi satu file dan dikirimkan ke Pusdiklat BPS. Pusdiklat
BPS selanjutnya memeriksa file riwayat pengembangan
kompetensi yang telah dikirimkan dan menggabungkan
seluruh riwayat pengembangan kompetensi yang telah
diterima menjadi satu file. File tersebut menjadi dasar dalam
penyusunan
pengembangan
kompetensi pegawai di BPS.

laporan mengenai

capaian

dan

untuk

pengolahan

pengumpulan

Sebelumnya telah dilakukan penelitian oleh Muhammad
Faza (2020) yang berjudul Sistem Informasi Pengembangan
Kompetensi Pegawai berbasis web di Pusdiklat BPS sebagai
media
riwayat
pengembangan kompetensi dalam upaya meminimalkan
terjadinya kesalahan berupa duplikasi data, kesalahan input,
serta beragamnya format data.
Pada penelitian tersebut,
didapatkan skor SUS sebesar 53,33 yang dinilai sangat rendah.
rancangan desain
Berdasarkan informasi dari
sistem yang dikembangkan peneliti dianggap belum efektif
untuk menangkap seluruh data
riwayat pengembangan
kompetensi,
terutama kegiatan pengembangan kompetensi
yang dilakukan secara mandiri oleh pegawai.

responden,

yaitu

tersebut,

Sesuai dengan latar belakang tersebut maka dalam
penelitian ini dilakukan pengembangan terhadap sistem yang
ada
yang
memungkinkan seorang pegawai melakukan entri data riwayat
pengembangan kompetensi yang telah diikutinya secara
langsung,
tanpa melalui atasan di unit kerjanya masing-
masing.

dengan menambahkan

fitur

1 / 7

II. TUJUAN PENELITIAN
Tujuan dari penelitian ini adalah mengembangkan Sistem
Informasi Pengembangan Kompetensi Pegawai Berbasis Web
yang sudah ada dengan menambahkan fitur yang
memungkinkan setiap pegawai untuk melakukan self-
assessment tanpa harus melalui atasan di unit kerjanya.

III. PENELITIAN TERKAIT

Penelitian yang berkaitan dengan penelitian ini dilakukan
oleh Muhammad Faza (2020) dari Politeknik Statistika STIS,
mengenai sistem pengumpulan data riwayat pengembangan
kompetensi bagi pegawai BPS. Dalan penelitian tersebut, unit
eselon 2 dan eselon 3 memasukkan data pengembangan
kompetensi bagi setiap pegawainya.

Penelitian serupa juga pernah dilakukan oleh Anif, Dentha,
& Sindung (2017) dari Politeknik Negeri Semarang mengenai
sistem monitoring bagi mahasiswa yang sedang melakukan
magang. Dalam sistem tesebut mahasiswa melakukan
pelaporan setiap aktifitas yang mereka lakukan dalam satu
hari magang, kemudian pengawas di
tempat mahasiswa
tersebut magang dan pihak kampus, baik prodi dan dosen,
dapat melakukan pengawasan aktifitas yang dilaporkan.
Penelitian tersebut menggunakan kerangka kerja Laravel yang
berbasis PHP.

IV. METODE PENELITIAN

siklus

sebuah model

4.1 Metode Pengembangan Sistem
Metode pengembangan sistem yang digunakan di dalam
penelitian ini adalah Rapid Application Development (RAD).
(2002), RAD (Rapid Application
Menurut McLeod
Development) merupakan siklus hidup untuk pengembangan
yang lebih cepat dan mendapatkan hasil dengan kualitas yang
lebih baik dari
tradisional. Sedangkan menurut
Pressman (2012), Rapid Application Development(RAD)
adalah
perkembangan software
proses
sekuensial linier yang menekankan siklus perkembangan yang
sangat pendek. RAD dipilih karena
terhadap
perubahan model yang terjadi selama pengembangan sistem
dan peran serta stakeholder cukup intens sehingga umpan
bailk yang diberikan sangat membantu dalam pengembangan
sistem. Tahapan RAD terdiri atas perencanaan, analisis,
perancangan,
.
RAD fokus pada pembangunan model sistem dengan cepat,
mendapatkan umpan balik dari pengguna dan menggunakan
umpan balik tersebut untuk memperbaruhi sistem. Proses
tersebut dapat dilakukan berulang hingga sistem dapat
diterima pengguna.

implementasi, pengujuan dan peraawatan.

flexible

Gambar 1. Proses pengembangan tradisional

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Gambar 2. Proses pengembangan RAD

Menurut Whitten

keuntungan
dan Bentley
menggunakan metode RAD dalam pengembangan sistem
informasi antara lain :

(2007),

dan

aktif

dari pengguna

1. Mendorong partisipasi
manajemen sehingga
meningkatkan antusiasme pengguna akhir terhadap proyek.
2. Proyek memiliki visibilitas dan dukungan yang lebih
tinggi karena
keterlibatan pengguna yang luas di seluruh proses.
3. Pengguna dan manajemen lebih sering melihat hasil
pekerjaan solusi berbasis perangkat lunak.
4. Kesalahan dan kelalaian cenderung lebih cepat terdeteksi.
5. Pengujian dan pelatihan adalah produk sampingan alami
dari pendekatan prototyping yang mendasarinya.

Menurut Kendall (2010), terdapat tiga fase dalam RAD
yang melibatkan penganalisis dan pengguna dalam tahap
penilaian, perancangan, dan penerapan. Adapun ketiga fase
tersebut adalah requirements planning (perencanaan syarat-
syarat), RAD design workshop (workshop desain RAD), dan
implementation (implementasi). Sesuai dengan metodologi
RAD menurut Kendall (2010), berikut ini adalah tahap-tahap
pengembangan aplikasi dari
tiap-tiap fase pengembangan
aplikasi.
1)
Dalam fase ini, pengguna dan penganalisis bertemu untuk
mengidentifikasikan tujuan-tujuan aplikasi atau sistem serta
untuk megidentifikasikan
yang
ditimbulkan dari tujuan-tujuan tersebut. Orientasi dalam fase
ini
perusahaan.
Meskipun teknologi informasi dan sistem bisa mengarahkan
sebagian dari sistem yang diajukan, fokusnya akan selalu tetap
pada upaya pencapaian tujuan-tujuan perusahaan.

Requirements Planning (Perencanaan Syarat-Syarat)

adalah menyelesaikan masalah-masalah

syarat-syarat

informasi

2)
RAD Design Workshop (Workshop Desain RAD)
Fase ini adalah fase untuk merancang dan memperbaiki
yang bisa digambarkan sebagai workshop. Penganalisis dan
dan pemrogram dapat bekerja membangun dan menunjukkan
representasi visual desain dan pola kerja kepada pengguna.
Workshop desain ini dapat dilakukan selama beberapa hari
tergantung dari ukuran aplikasi yang akan dikembangkan.
Selama workshop desain RAD, pengguna merespon prototipe
yang ada dan penganalisis memperbaiki modul-modul yang
dirancang berdasarkan respon pengguna. Apabila sorang
pengembangnya merupakan pengembang atau pengguna yang
berpengalaman, Kendall menilai bahwa usaha kreatif ini dapat
mendorong pengembangan sampai pada tingkat terakselerasi.

Implementation (Implementasi)

3)
Pada fase implementasi ini, penganalisis bekerja dengan
para pengguna secara intens selama workshop dan merancang

2 / 7

aspek-aspek bisnis dan nonteknis perusahaan. Segera setelah
aspek-aspek ini disetujui dan sistem-sistem dibangun dan
disaring, sistem-sistem baru atau bagian dari sistem diujicoba
dan kemudian diperkenalkan kepada organisasi.

4.2 Metode Pengumpulan Data
Data dan informasi digunakan sebagai bahan dasar atau
acuan dalam pengambilan keputusan dan pemilihan solusi
dalam menghadapi masalah penelitian. Proses pengumpulan
data dan informasi pada penelitian ini dilakukan dengan
beberapa cara.

1. Studi Literatur
literatur
Studi

bertujuan mencari

dalam
pelaksanaan penelitian. Selain itu mempelajari
file dan
dokumen kerja organisasi dapat mempermudah dalam
memahami organisasi dan sistem yang sedang berjalan dengan
lebih baik. Dengan begitu tahap analisis
sistem dapat
dilakukan dengan lebih baik dan tepat.

referensi

literatur

Pada penelitian ini dilakukan studi

terhadap
beberapa buku terkait pengembangan sistem informasi. Hal
tersebut dilakukan agar langkah-langkah yang diambil dalam
pengembangan sistem informasi sesuai dengan teori dan
kaidah yang berlaku dan relevan. Selain itu studi literatur pada
penelitian juga dilakukan pada beberapa undang-undang dan
peraturan pemerintah mengenai pengembangan kompetensi
bagi pegawai negeri sipil. Kegiatan tersebut dilakukan guna
memperoleh pemahaman mengenai konteks dari penelitian ini
yang bisa dijadikan dasar dalam mendesain sistem informasi.

2. Wawancara
Menurut Joko Subagyo (2011), wawancara adalah “Suatu
secara
kegiatan dilakukan untuk mendapatkan informasi
langsung dengan mengungkapkan pertanyaanpertanyaan pada
para responden. wawancara bermakna berhadapan langsung
antara
kegiatannya
dilakukan secara lisan.

interview dengan

responden,

dan

Wawancara menjadi alternatif lain dalam upaya memahami
organisasi dan sistem yang sedang berjalan. Wawancara
dilakukan dengan bertanya kepada beberapa pihak yang
berkaitan dengan sistem yang sedang dikembangkan, yaitu
pegawai pusdiklat yang bersangkutan. Wawancara yang
dilakukan peneliti merupakan wawancara secara langsung
mendatangi dengan narasumber maupun wawancara secara
jarak jauh.

4.3 Metode Analisis
Digunakan beberapa metode analisis yang dilakukan dalam
tiap proses pengembangan sistem. Dilakukan analisis sistem
berjalan menggunakan bantuan kerangka bangun sistem
informasi serta diagram BPMN (Business Process Model and
Notation). Whitten dan Bentley (2007) menjelaskan dalam
bukunya yang berjudul ‘System Analysis and Design Methods’
bahwa kerangka bangun sistem informasi melihat suatu sistem
tiga aspek yaitu aspek pengetahuan, aspek
informasi dari
proses, dan aspek komunikasi. Kedua, tahap analisis masalah
dilakukan dengan menganalisis hasil wawancara dengan

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

pengguna. Ketiga, tahap analisis kebutuhan dilakukan dengan
membuat draf kebutuhan fungsional dan non-fungsional yang
diperoleh dari analisis perolehan data dan informasi dari studi
literatur dan wawancara kepada pengguna sistem yang
bersangkutan. Lalu pada tahap evaluasi sistem dilakukan
dengan menggunakan metode System Usability Scale Test.

V. KERANGKA PIKIR

ayat

Pada pasal 4

(1) dalam Peraturan Lembaga
Administrasi Negara Nomor 10 Tahun 2018 secara jelas
disebutkan bahwa setiap PNS memiliki hak dan kesempatan
yang sama untuk mengikuti pengembangan kompetensi
dengan memperhatikan hasil penilaian kinerja dan penilaian
kompetensi pegawai yang bersangkutan. Pengembangan
kebutuhan
pemenuhan
upaya
kompetensi merupakan
kompetensi PNS dengan standar kompetensi
jabatan dan
pengembangan karier.

Pengembangan kompetensi bagi setiap PNS dilakukan
paling sedikit 20 jam pelajaran (JP) dalam satu tahun.
Pengembangan kompetensi dilaksanakan pada tingkat instansi
maupun skala nasional. Pengembangan kompetensi yang
dimaksud berupa kegiatan pendidikan dan pelatihan. Pelatihan
pelatihan
dibedakan menjadi
nonklasikal. Pengembangan kompetensi klasikal dapat berupa
kegiatan seperti :

pelatihan

klasikal

dan

Pelatihan struktural kepemimpinan , manajerial,

fungsional dan pelatihan sosial kultural
Kegiatan

1.
teknis
2.
3.
sejenis
workshop atau lokakarya
4.
bimbingan teknis dan sosialisasi
5.
pelatihan klasikal lainnya.
Sedangkan pengembangan kompetensi non klasikal dapat

Jalur Pengembangan Kompetensi dalam bentuk

Kegiatan terfokus lain semisal , kursus , penataran ,

seminar/konferensi/sarasehan,

berupa kegiatan seperti :
1.
2.
3.
4.
5.

antara

Coaching, mentoring dan e-learning
Pelatihan jarak jauh dan detasering (secondment)
Pembelajaran alam terbuka (outbond)
Kegiatan patok banding (benchmarking)
pegawai
dengan
PNS
Pertukaran
swasta/Badan Usaha Milik Negara/ Badan Usaha
Milik Daerah
Kegiatan belajar
,
komunitas belajar , bimbingan di tempat kerja dan
magang/praktik kerja
Jalur Pengembangan Kompetensi dalam bentuk
pelatihan non klasikal lainnya.

belajar mandiri

lain seperti

6.

7.

Salah satu unsur manajemen PNS adalah pengembangan
karier. Manajemen karier bertujuan untuk memberikan
kejelasan dan kepastian karier kepada PNS, menyeimbangkan
antara pengembangan karier PNS dan kebutuhan instansi,
meningkatkan kompetensi dan kinerja PNS, serta mendorong
peningkatan mutu dan kualitas PNS. Dalam pelaksanaan
manajemen karier, setiap instansi pemerintah harus menyusun

3 / 7

profil PNS di mana salah satu komponennya adalah riwayat
pengembangan kompetensi.

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Statistik

Berdasarkan hal tersebut diatas maka dapat disimpulkan
termasuk Badan
bahwa semua instansi milik pemerintah,
Pusat
riwayat
(BPS),
pengembangan kompetensi yang merupakan hak dari setiap
dilakukan
pegawai BPS. Dalam penelitian
pengembangan sistem informasi pengembangan kompetensi
BPS.

perlu mengumpulkan

akan

ini

Gambar 4. Kerangka pikir

VI. HASIL DAN PEMBAHASAN

6.1 Proses Bisnis Sistem Berjalan
Analisis

data

sistem berjalan untuk memperoleh gambaran
tentang bagaimana sistem yang saat
ini dilakukan oleh
riwayat
dalam mengumpulkan
pusdiklat BPS
pengembangan kompetensi setiap pegawainya. Analisis ini
diperoleh dari hasil wawancara dengan pegawai pusdiklat.
Pengumpulan data pengembangan kompetensi di pusdiklat
masih dilakukan secara manual. Proses dimulai dari Pusdiklat
BPS yang melakukan permintaan data riwayat pengembangan
kompetensi pegawai kepada seluruh unit kerja level eselon 2
yang ada di BPS sesuai dengan format dan ketentuan yang
telah diberikan melalui email. Pada tahap selanjutnya, unit
kerja level eselon 2 selain melakukan pengumpulan data pada
lingkungan unit kerjanya, juga melakukan permintaan data
kepada unit kerja level eselon 3 di bawahnya. Unit kerja level
eselon 3 yang telah menerima format pengumpulan data
kemudian melakukan pengumpulan data pada lingkungan unit
kerjanya. Data yang telah terkumpul di lingkungan unit kerja
level eselon 3 dikirimkan kepada unit kerja level eselon 2 di
atasnya melalui email. Pihak unit kerja level eselon 2 yang
telah menerima data dari setiap unit kerja level eselon 3 di
bawahnya kemudian melakukan penggabungan data dan
mengirimkan data tersebut kepada Pusdiklat BPS melalui
email. Pusdiklat BPS akan menggabungkan dan mengolah
seluruh data yang telah diterima sebagai bahan pembuatan
laporan. Proses ini dilakukan setahun sekali.

Gambar 5. Sistem berjalan

Adapun sistem yang telah dibangun dan dirancang
sebelumnya oleh Muhammad Faza (2020) memiliki alur
sebagai berikut.

4 / 7

proses

pengecekan

Dalam sistem tersebut, media pengumpulan data diganti
dari menggunakan email menjadi menggunakan aplikasi web
data
sehingga
dilakukan oleh sistem. Pengguna dalam sistem 20JP terdiri
dari admin, eselon 2, dan eselon 3, dimana setiap pengguna
dapat melakukan
pengembangan
kompetensi, pengumpulan data pegawai, melihat dashboard,
dan mengunduh data pengembangan kompetensi pegawai.

penggabungan

pengumpulan

data

dan

Gambar 6.Rancangan Basis Data

6.2 Analisis Kebutuhan Sistem
Sistem yang dibuat harus memenuhi beberapa kebutuhan

berikut.

1. Sebagai media pengumpulan dan pengompilasi data
riwayat pengembangan kompetensi bagi setiap pegawai
BPS
2. Penyaringan data berdasarkan kriteria tertentu yang
ditentukan
3. Sebagai upaya mengurangi kesalahan informasi yang
dikumpulkan
kompetensi
4. Pengumpulan
pegawai yang mudah diakses dimana saja dan kapan saja
bagi setiap pegawai

pengembangan

data

6.3 Rancangan Sistem Usulan
Penelitian ini berfokus pada pengembangan sistem berupa
menambahkan akun pegawai sebagai self-assessment sehingga
tiap pegawai dapat menginput riwayat pelatihannya secara
mandiri, serta membuat modul untuk mengenerate sertifikat
pengembangan kompetensi.

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Gambar 7. Sistem Usulan

Gambar 8. Usecase diagram

6.4 Implementasi
Penelitian ini kerangka kerja Laravel versi 5.8 dan basis
data mysql untuk mengimplementasikan rancangan desain
sistem yang telah dibuat. Pada gambar 8 berikut merupakan

5 / 7

halaman login sistem yang muncul saat pertama kali pengguna
membuka aplikasi.

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Gambar 12. Data Pegawai

Gambar 9. Halaman login

Halaman dashboard untuk pegawai dapat dilihat pada
gambar 10 di bawah ini. Halaman ini merupakan yang
pertama kali ditampilkan pada saat pegawai berhasil masuk ke
dalam sistem.

6.5 Hasil Pengujian
dilakukan

Pengujian
untuk mengukur
sistem dalam melakukan
keberhasilan
Pengujian yang dilakukan menggunakan black box testing.
Hasil yang diharapkan Hasil uji
Precondition Condition
Keluar peringatan untuk
Berada pada
mengisi kembali sesuai
halaman login
data yang ada

efektivitas
tugas

Memasukkan
kombinasi username
dan password yang
tidak benar

Sesuai

dan
tertentu.

Berada pada
halaman login

Menekan tombol login
tanpa memasukkan
username dan password

keluar peringatan untuk
mengisi sesuai data
yang ada

Sesuai

Berada pada
halaman login

Memasukkan
kombinasi username
dan password milik
pegawai

Masuk ke dashboard
pegawai

Sesuai

Gambar 10. Dashboard

Pada gambar 11 berikut merupakan tampilan daftar riwayat
pengembangan kompetensi. Pengguna dapat melihat pelatihan
yang diikuti kemudian mendownload sertifikat pelatihan.

-

Masuk
sebagai
pegawai

Melihat data pribadi,
riwayat pengembangan
kompetensi

Sesuai

Pada tab
kompetensi

Memasukkan data
pengembangan
kompetensi

Data pengembangan
kompetensi berhasil
ditambah

Sesuai

Gambar11. Tampilan riwayat kompetensi

Halaman data pegawai dapat dilihat pada gambar berikut.

Pada tab
kompetensi

Mengklik tombol
download pada kolom
sertifikat

Sertifikat
pengembangan
kompetensi terunduh

Sesuai

VII.

PENUTUP

Berdasarkan tulisan di atas, terdapat kesimpulan:

6 / 7

Telah dilakukan pengembangan terhadap sistem yang
tiap

1.
ada sebelumnya berupa penambahan menu bagi
pegawai
2.
pada aplikasi telah berjalan sebagaimana mestinya.

Berdasarkan hasil pengujian black box, semua fungsi

DAFTAR PUSTAKA
[1] Mc.,Leod, R. Jr. System Development: A Project Management Approach.

New York: Leigh Publishing LLC, 2002.

[2] Whitten, J. L., & Bentley, L. D. System Analysis and Design Methods.

Boston: McGraw-Hill, 2007.

[3] Kendall, J.E. & Kendall, K.E. Analisis dan Perancangan Sistem. Jakarta:

Indeks, 2010.

[4] Pusat Pendidikan dan Pelatihan Badan Pusat Statistik : (2021, MARET

31). Tentang Pusdiklat BPS RI. Available:
https://pusdiklat.bps.go.id/index.php?r=site/page&view=about

[5] RI. Undang-Undang Republik Indonesia Nomor 5 tahun 2014 tentang

aparatur sipil negara. Jakarta:RI, 2014.

[6] RI. Peraturan Pemerintah Republik Indonesia Nomor 11 Tahun 2017.

Jakarta: RI, 2017.
[7] Faza, Muhammad,

SISTEM INFORMASI

PENGEMBANGAN
KOMPETENSI PEGAWAI
(20JP) PUSDIKLAT BADAN PUSAT
STATISTIK BERBASIS WEB. Jakarta: Politeknik Statistika STIS, 2020.
[8] Badan Kepegawaian Daerah Trenggalek : (2020, NOVEMBER 26). Jenis
Pengembangan Kompetensi PNS Secara Klasikal dan non Klasikal.
Available:
https://bkd.trenggalekkab.go.id/2020/11/26/pengembangan-
kompetensipegawai-negeri-sipil/

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

7 / 7

"
221709533,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pembangunan Sistem Presensi Daring Pegawai BPS 
Berbasis Location Based Services 

Alvin Berliansyah (221709533, 4SI1) 
Dosen Pembimbing: Nori Wilantika, S.S.T.,M.T.I. 

Ringkasan—  Kegiatan  presensi  merupakan  kegiatan 
pencatatan  kehadiran  yang  penting  dilakukan[1].  BPS  dalam 
menghadapi tantangan pandemi covid-19 menggunakan microsoft 
kaizala untuk presensi pegawainya. Namun penggunaan aplikasi 
kaizala memunculkan beberapa kendala. Akibatnya pengelolaan 
itu 
dan  pelaksanaan  kegiatan  presensi  terhambat.  Untuk 
diperlukan  pembuatan  aplikasi  presensi  online  sendiri  yang 
memenuhi  kebutuhan  presensi  pegawai  BPS.  Sistem  diterapkan 
kedalam  2  aplikasi  yaitu  aplikasi  mobile    dan  aplikasi  web. 
Aplikasi  mobile  digunakan  untuk  pelaksanaan  dan  monitoring, 
sedangkan  aplikasi  web  digunakan  untuk  rekapitulasi  presensi. 
Sistem yang dibangun berpotensi dapat diterima oleh pengguna 
dan  memudahkan  pengguna  dalam  menjalankan 
serta 
memonitoring kegiatan presensi pada lingkup pegawai BPS sesuai 
dengan  hasil  pengujian  UAT  berjenis  blackbox  testing  setelah 
sistem selesai dibangun.  

Kata Kunci— Presensi, BPS, LBS, Flutter, Pegawai 

I.  LATAR BELAKANG 

Kegiatan presensi merupakan kegiatan pencatatan kehadiran 
yang  penting  dilakukan  [1].  Bagi  perusahaan,  kegiatan  ini 
dilakukan  sebagai  dasar  dari  perhitungan  kedisiplinan  dan 
kinerja pegawai. Dalam lingkup sekolah kegiatan ini digunakan 
untuk sarana saling mengenal antara guru dan murid. Menurut 
Purwanto  dalam  [2],  presensi  merupakan  suatu  pencatatan 
kehadiran, bagian dari pelaporan aktivitas suatu institusi, atau 
komponen institusi itu sendiri yang berisi data kehadiran yang 
disusun  dan  diatur  sedemikian  rupa  sehingga  mudah  untuk 
dicari  dan  dipergunakan  apabila  sewaktu-waktu  diperlukan 
oleh pihak yang berkepentingan.  

Kegiatan  presensi  menggunakan  cara  yang  bermacam  - 
macam.  Pada  awal  perkembangannya  kegiatan  presensi 
menggunakan  kertas  sebagai  media  pencatatannya.  Kegiatan 
presensi  pada  kertas  menggunakan  tanda  pengenal  pribadi 
seperti tanda  tangan pada kertas. Presensi pada kertas mudah 
untuk  diaplikasikan  karena  hanya  membutuhkan  kertas  dan 
tinta.  Namun  presensi  menggunakan  kertas  menyebabkan 
boros  kertas  serta  pengolahan  dan  pencarian  data  kehadiran 
kurang  efisien.  Kemudian  adapun  kegiatan  presensi  yang 
menggunakan sidik jari. Menurut [3] ,sidik jari dapat digunakan 
sebagai alat identifikasi yang terpercaya. Presensi dengan sidik 
jari  dilakukan  dengan  cara  menempelkan  jari  yang  telah 
terdaftar ke alat indentifikasi sidik jari. Presensi dengan cara ini 
sangat efisien dilakukan dan sangat memudahkan pengolahan 
data  kehadirannya.  Namun  menurut  [2],mahalnya  alat  dan 
pembangunan sistem penyimpanan dan pengolahan yang rumit 
membuatnya  sulit  diimplementasikan.  Selanjutnya  presensi 
menggunakan  perangkat  mobile  menggunakan  QR  (Quick 
Response) code dan LBS (Location Base Service) sebagai alat 
identifikasi  dan  pencatatan.  Presensi  menggunakan  QR  code 

dan LBS mudah diimplentasikan karena fitur perangkat mobile 
saat  ini  dapat  digunakan  untuk  mengidentifikasi  QR  code 
maupun LBS. QR code menggunakan kamera pada perangkat 
mobile untuk melakukan scan kode yang telah disediakan untuk 
melakukan  kegiatan  presensi.  Sedangkan  LBS  menggunakan 
GPS (Global Positioning System) pada perangkat mobile [4]. 

Pandemi covid-19 menyebabkan kegiatan presensi tidak bisa 
dilakukan dengan media kertas dan sidik jari . Hal ini terjadi 
karena  kegiatan  presensi  dengan  media  tersebut  berkaitan 
dengan  kontak  fisik  antar  orang  yang  dapat  menyebabkan 
penularan  virus  Covid-19.  Menurut  Surat  Edaran  Menteri 
Pendayagunaan  Aparatur  Negara  dan  Reformasi  Birokrasi  
nomor  58  Tahun  2020,  presensi  yang  dianjurkan  untuk 
pemeriksaan  kehadiran  pegawai  adalah  presensi  online. 
Presensi online atau daring dianjurkan karena presensi daring 
dapat dilakukan tanpa melakukan kontak fisik  antar pegawai. 
Selain  itu  presensi  online  juga  dapat  memenuhi  aturan  wfo 
(work  from  office)  dan  wfh  (work  from  home)  pegawai  yang 
terdapat dalam Surat Edaran Menteri Pendayagunaan Aparatur 
Negara  dan  Reformasi  Birokrasi  Nomor  67  Tahun    2020 
tentang  sistem  kerja  pegawai  aparatur  sipil  negara  dalam 
tatanan normal baru. Dengan adanya aturan mengenai wfo dan 
wfh,  maka  presensi  online  yang  cocok  digunakan  adalah 
dengan metode LBS dari pada metode QR code.  

BPS  (Badan  Pusat  Statistik)  dalam  mengatasi  masalah 
presensi  pegawai  saat  pandemi  Covid-19  sudah  mengganti 
sistem  presensi  sidik  jari  menjadi  sistem  presensi  daring 
(online).  Presensi  daring  yang  digunakan  berbasis  mobile  
dengan  cara  LBS 
(Location  Base  Service)  dengan 
menggunakan  aplikasi  microsoft  kaizala.  Microsoft  Kaizala 
adalah aplikasi mobile manajemen pesan yang memungkinkan 
pengguna berkolaborasi dengan orang lain.  

Menurut Staf Seksi Perekaman Data, subdirektorat (subdit) 
Pengelolaan  Teknologi  dan  Informasi,  Direktorat  Sistem 
Informasi Statistik, alasan dipakainya produk microsoft adalah 
karena BPS sudah memiliki lisensi produk microsoft office 365 
yang terhubung pada microsoft kaizala dari sebelum pandemi 
Covid-19.  Namun  penggunaan  aplikasi  microsoft  kaizala 
menyebabkan berbagai kendala.  

Masih dalam wawancara dengan Staf Seksi Perekaman Data, 
subdit Pengelolaan Teknologi dan Informasi, Direktorat Sistem 
Informasi  Statistik,  Penggunaan  aplikasi  kaizala  tersebut 
mengakibatkan  data  presensi  pegawai  tidak  secara  langsung 
dimiliki oleh BPS akan tetapi berada pada penyimpanan yang 
dimiliki  kaizala.  Hal  ini  menjadikan  data  kehadiran  pegawai 
BPS 
yang 
direkomendasikan oleh ITU (International Telecommunication 
Union) X.800. Confidentiality mensyaratkan bahwa informasi 
(data)  hanya  bisa  diakses  oleh  pihak  yang  memiliki 
wewenang[5]. Dalam konteks ini pihak microsoft kaizala  yang 

tidak  memenuhi 

confidentiality 

syarat 

 1 / 9 

 
 
 
 
 
tidak  memiliki  wewenang  atas  data  kehadiran  pegawai  BPS 
dapat  menggunakan  data  tersebut  sesuai  keinginan  microsoft 
tersebut  berada  dalam 
kaizala  hanya  karena  data 
penyimpanannya.  

pengembangan sistem, framework yang digunakan, dan aturan 
-aturan BPS terkait presensi pegawai. Literatur yang berkaitan 
dengan penelitian ini penulis rangkum dalam tabel 1. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Data  kehadiran  yang  berada  pada  penyimpanan  microsoft 
kaizala juga bersifat data mentah hasil input pegawai melalui 
aplikasinya.  Sehingga  pengelola  harus  mengambil  data 
pegawai  secara  manual  dengan  mengunduhnya  kemudian 
mengolah  datanya.  Dari  segi  pengelola  presensi  berdasarkan 
wawancara dengan staff IPDS BPS Banyuwangi , data pegawai 
yang diunduh melalui aplikasi dengan waktu perhari memiliki 
informasi  lebih  detail  daripada  pengunduhan  perminggu  atau 
perbulan.  Namun  untuk  mengunduh  data  perhari  diperlukan 
upaya  dan  waktu  yang  lebih  besar  daripada  perminggu  atau 
perbulan. Akibatnya pengelola memutuskan untuk melakukan 
kemudian 
pengunduhan 
mengolahnya  yang  mengakibatkan  data  presensinya  kurang 
detail ketika di serahkan ke kasi umum.  

perminggu 

perbulan 

atau 

Dari sisi pengguna, penulis melakukan survei pendahuluan 
kepada  pegawai  BPS  untuk  mengetahui  pendapat  mereka 
tentang  kaizala  dan  kendala  yang  menyertainya.  Dari  135 
responden,  11,85%  berpendapat  bahwa  presensi  melalui 
kaizala  lebih  rumit  dari  pada  presensi  sidik  jari.  22,22% 
responden  merasa  rumit  ketika  menemukan  pranala  /  link 
undangan  presensi.  Separuh  responden,  tepatnya  54,07% 
pernah mengalami kegagalan melakukan presensi pada aplikasi 
kaizala.  4,44%  responden  merasa  tidak  pernah  mendapat 
pemberitahuan  presensi  dari  BPS.  Seperempat  responden 
tepatnya  25,92%  merasa  terganggu  terhadap  pemberitahuan 
aplikasi  kaizala  yang  tidak  berhubungan  dengan  kegiatan 
presensi.  Dan  9,63%  responden  merasa  aplikasi  kaizala 
memperlambat  performa  perangkat  yang  mereka  pakai. 
Berdasarkan fakta - fakta tersebut dapat dikatakan bahwa BPS 
perlu membuat sistem presensi daringnya sendiri.  

Penerapan presensi daring dengan QR code dan LBS sama – 
sama  memiliki  keuntungan  dalam  biaya  pengembangan  dan 
pengelolaan  presensinya.  Namun  penerapan  presensi  dengan 
metode LBS lebih cocok digunakan pada masa pandemi covid-
19.  Dengan  presensi  LBS  pegawai  dapat melakukan presensi 
diberbagai  lokasi  baik  itu  dikantor  atau  dirumah.  Sedangkan 
presensi QR code pegawai hanya dapat melakukan presensi di 
kantor  atau  ruang  yang  menyediakan  fasilitas  kode  untuk 
presensi seperti pada penelitan [6]. 

II.  TUJUAN PENELITIAN 

 Berdasarkan  latar  belakang  diatas,  tujuan  penelitian  ini 
adalah  untuk  membuat  dan  mengembangkan  sistem  presensi 
daring  berbasis  LBS  (Location  Base  Service).  Manfaat 
penelitian ini antara : 

•  Data presensi dimiliki sendiri oleh BPS 
•  Aplikasi 

dibangun 

yang 

memudahkan 

pengunduhan dan pengolahan data kehadiran 
•  Aplikasi juga membuat alur presensi lebih efisien  

III. PENELITIAN TERKAIT 
Literatur  yang  penulis  baca  mengenai  penelitian 

berkaitan 

dengan 

presensi 

secara 

ini 
umum,  metode 

No 

Judul 

1  Pembangunan 
Aplikasi 
Pengajuan 
Cuti Pegawai 
Dengan 
Flutter 

2  Aplikasi 
Presensi 
Online 
Berbasis 
Android 
Dengan Fitur 
Fingerprint 
Dan Location 
Based Service 
(LBS) Pada 
Lembaga 
Pendidikan 
XYZ 

Tertulis 

TABEL I. Penelitian terkait 
Penulis, 
Publikasi 
Stenly Frans, 
Waris 
Marsisno,[7]  

Aplikasi 
pengajuan cuti 
pegawai telah 
berhasil 
dibangun 
dengan 
menggunakan 
Flutter.  

Hasanudin,[8]   Pembangunan 

aplikasi 
presensi daring 
pada android 
dengan 
menggunakan 
fitur 
fingerprint dan 
location based 
service (LBS) 
berdasarkan 
masalah yang 
terjadi pada 
lembaga 
pendidikan 
XYZ terkait 
pengelolaan 
presensi 

Komentar 

Penelitian ini 
terjadi banyak 
penyesuaian 
dengan aturan-
aturan yang 
berlaku pada 
organisasi 
BPS.Kaitannya 
dengan 
penelitian ini 
adalah 
kesamaan 
metode 
penggunaan 
framework 
flutter 

Fitur Location 
Base Service 
pada penelitian 
tersebut juga 
akan 
digunakan 
dalam 
penelitian 
ini.Penelitian 
tersebut 
menggunakan 
fitur 
fingerprint 
guna menandai 
peserta didik 
pada lembaga 
pendidikannya 

IV. METODE PENELITIAN  
Penelitian dilakukan dalam 3 tahap yakni pengumpulan data, 
pembangunan  sistem,  serta  pengujian  dan  evaluasi  sistem. 
Pengumpulan  data  digunakan  untuk  memperoleh  data  terkait 
keadaan  yang  saat  ini  terjadi  pada  sistem  presensi  BPS. 
Pengumpulan  data  pada  penelitian  dilakukan  dengan 
wawancara,  studi  pustaka  dan  survei  pendahuluan.  Tahap 
kedua yakni pembangunan sistem, pembangunan ini dilakukan 
guna  menyelesaikan  masalah  yang  terbentuk  dari  tahap 
sebelumnya.  Pengembangan  sistem  pada  penelitian 
ini 
mengikuti  System  Development  Live  Cycle  (SDLC)  waterfall  
dimulai dengan tahap  perencanaan, analisis, desain, kemudian 
implementasi . Pendapat, saran dan kritikan yang didapat pada 
tahap  pengumpulan  data  diimplementasikan  kedalam  sistem 
yang dibangun supaya kebutuhan pengguna  terpenuhi.  Tahap 
terakhir  yaitu  pengujian  dan  evaluasi  sistem.  Pengujian 
dilakukan  dengan  beberapa  pengguna  menggunakan  teknik 
pengujian 
ini 
dilakukan  untuk  melihat  apakah  sistem  dapat  diterima  oleh 

  UAT(User  Acceptance  Test).  Pengujian 

 2 / 9 

 
 
 
  
pengguna.  Hasil  dari  tahap  ini  digunakan  untuk  menarik 
kesimpulan apakah sistem layak dipakai untuk atau tidak. 

. 

V.  KERANGKA PIKIR 

Penelitian  ini  dimulai  dengan  penemuan  masalah  yaitu 
karena  pandemi  covid-19  kegiatan  presensi  pegawai  pada 
organisasi  BPS  beralih  dari  yang  awalnya  menggunakan 
presensi  sidik  jari  menjadi  presensi  daring.  Namun  presensi 
daring  yang  digunakan  bukan  buatan  BPS  dan  menyebabkan 
banyak  kendala  penggunaannya.  Akibatnya  data  kehadiran 
yang  awalnya  otomatis  terintegrasi  menjadi  harus  diolah 
terlebih  dahulu  sebelum  dikirim.  Oleh  karena  itu  dibutuhkan 
pembangunan aplikasi presensi daring yang dapat terintegrasi 
dan memudahkan pegawai dalam melakukan presensi. 

Aplikasi presensi daring yang sesuai untuk masalah tersebut 
adalah  presensi  berbasis  LBS.  Presensi  berbasis  LBS  dapat 
menyesuaikan  kegiatan  presensi  sesuai  lokasi  pengguna  atau 
pegawai. Data lokasi tersebut kemudian diolah secara otomatis 
oleh aplikasi menjadi data kehadiran kemudian dikirim ke basis 
data BPS pusat untuk dijadikan arsip data pegawai. Kemudian 
setelah  aplikasi  selesai  dibuat,  penulis  melakukan  evaluasi 
dengan  menggunakan  UAT  dengan  jenis  black  box  testing 
untuk  menilai  kelayakan  sistem.  Kerangka  pikir  diatas 
disajikan pada Gambar 1. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 2. Alur presensi pegawai kaizala 

Setelah  kegiatan  presensi  dilaksanakan,  kemudian  dilakukan 
rekapitulasi presensi. Rekapitulasi ini salah satunya digunakan 
untuk pembayaran uang makan dan tunjangan kinerja masing -
masing  pegawai.  Kegiatan  ini  biasanya  dilakukan  perbulan. 
Alur rekapitulasi presensi digambarkan pada Gambar 3. 

Gambar 1. Kerangka Pikir 

VI. HASIL DAN PEMBAHASAN 

A.  Analisis Sistem Berjalan 

Sistem  presensi  yang  sekarang  berjalan  di  BPS  di  bagi 
menjadi  2  alur,  yaitu  presensi  pegawai  menggunakan  kaizala 
dan rekapitulasi presensi oleh admin presensi. Proses presensi 
pegawai  menggunakan  kaizala  memanfaatkan  fitur  minta 
kehadiran pada  aplikasi  mobile microsoft kaizala. Pembuatan 
undangan  permintaan  kehadiran  dilakukan  oleh  admin  pusat 
BPS.  Proses  bisnis  kegiatan  presensi  pegawai  menggunakan 
kaizala digambarkan pada Gambar 2.  

Gambar 3. Alur Rekapitulasi Presensi kaizala 

Dari gambar 2 dan 3 dapat diketahui masalah - masalah yang 
muncul akibat penerapannya yaitu pada proses dengan border 
berwarna  merah.  Solusi  dari  permasalahan  tersebut  penulis 
rangkum dalam Tabel II.  

TABEL II. Kebutuhan Sistem 

No. 

Kode 

Proses 

Berma

salah 

Kebutuhan sistem 

Kode 

Kebutuhan 

sistem 

1 

P1 

Secara otomatis memulai presensi tanpa 
perlu membuat undangan 

NF01 

 3 / 9 

PegawaiAdmin DaerahAdmin PusatMulaiP1Membuat link Undangan PresensiP2Memulai PresensiP10Mengakhiri PresensiP5Membuka AplikasiP6Membuka Grup ChatP7MembukaPranalaUndangan Permintaan Kehadiran Dari Grup BPSP8Menyetujui untukMembagikan Lokasi SaatIniP9MengirimDan Menyetujui Detail Laporan KehadiranP4Mengingatkan Pegawai Unit eseslon3 untukMelakukan PresensiSelesaiP3Mengingatkan SeluruhPegawai BPS RI untukMelakukan PresensiAdmin DaerahAdmin PusatP12MengunduhData Kehadiran pada website PBD BPSP13Mengolah Unduhan LaporanPresensiP14MenyerahkanLaporan Presensi ke Kasi umumP11Menyimpandata presensi dari kaizalake Server PBD BPSMulaiSelesai 
 
 
 
 
 
 
2 

3 

4 

5 

6 

P3 , P4  Membuat notifikasi khusus untuk 

presensi saja 
Pengguna tidak perlu membuka grup, 
langsung disajikan tombol mulai 
presensi di menu utama aplikasi 
Pembatas lokasi presensi dan validasi 
lokasi 
Penyimpanan langsung ke server 
presensi 
Data sudah dalam bentuk objek data 
yang mudah diolah 

P6 

P8 

P11 

P13 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

NF02 

F01 

NF03 

NF04 

F02 

B.  Analisis Kebutuhan Sistem 

Solusi dari permasalahan pada Tabel II dimasukkan kedalam 
kebutuhan  fungsional  dan  non  fungsional  sistem.  Sebagian 
permasalah  dimasukkan  kedalam  kebutuhan 
fungsional. 
Kebutuhan fungsional menjelaskan siapa saja pengguna sistem 
serta kegiatan apa saja yang dapat dilakukan pengguna didalam 
sistem  yang  akan  dibuat.  Untuk  memudahkan  pemahaman 
proses  pada  kebutuhan  fungsional  digambarkan  use  case 
diagram pada Gambar 4. 

Gambar 5. Rancangan Arsitektur Sistem Usulan 

Alur  presensi  sistem  usulan  yang  terbagi  kedalam  alur 

presensi (Gambar 6) dan alur rekapitulasi (Gambar 8). 

Gambar 4. Use case diagram 

Sedangkan  permasalahan  yang  masuk  kedalam  kebutuhan 

nonfungsional sistem antara lain: 

1.  Sistem dapat secara otomatis memulai presensi 
2.  Aplikasi  dapat menampilkan  pemberitahuan mengenai 

kegiatan presensi 

3.  Sistem dapat membatasi lokasi presensi pegawai dan 
4.  Data presensi disimpan langsung kedalam server. 

C.  Arsitektur dan Proses Bisnis Sistem Usulan 

Sistem presensi daring dibangun dalam dua bentuk aplikasi 
yaitu aplikasi web dan aplikasi mobile. Sistem memiliki 2 tipe 
pengguna  yaitu  pegawai  dan  pegawai  admin  presensi  seperti 
pada  Gambar  5.  Gambar 
tentang 
gambaran hubungan aktor dengan proses pada sistem usulan. 
Pegawai  berinteraksi  dengan  perangkat  mobile  miliknya. 
Sedangkan  Pegawai  admin  presensi  berinteraksi  dengan  web 
browser dan perangkat mobile-nya. Kemudian aplikasi mobile 
berinteraksi dengan aplikasi web menggunakan rest-api. 

tersebut  menjelaskan 

Gambar 6. Alur Presensi Sistem Usulan 

Alur pada Gambar 6 terlihat lebih sederhana dibandingkan 
dengan  alur  pada  Gambar  2  (sistem  berjalan).  Hal  ini 
menunjukkan sistem usulan memadatkan proses bisnis supaya 
lebih  efisien.  Untuk  lebih  menjelaskan  kegiatan  presensi  dan 
alur  proses  sistem  yang  terjadi  pada  sistem  usulan  dibuat  
activity diagram pada Gambar 7. 

Gambar  7  menjelaskan  alur  salah  satu  fitur  yang  dimiliki 
pada  sistem  yaitu  kegiatan  melakukan  presensi.  Pegawai 
melakukan  klik  atau  menekan  tombol  mulai  presensi  yang 

 4 / 9 

PegawaiMulaiMembuka AplikasiPresensi MobileMelakukan Login Menekan Tombol Mulai PresensiMengirimdan Menyetujui Detail Data PresensiSelesai 
 
 
 
 
 
 
 
 
 
 
berada  pada  aplikasi  mobile.  Kemudian  aplikasi  akan 
memverifikasi  apakah  izin  terhadap  lokasi  sudah  diaktifkan 
atau  belum.  Ketika  belum  diaktifkan  maka  pegawai  harus 
mengaktifkannya 
tampilan  aplikasi 
terlebih  dahulu  dan 
kembali  ke  menu  utama.  Sebaliknya  ketika  izin  sudah  aktif 
maka masuk menu presensi dimana aplikasi menampilkan peta 
dengan lokasi pegawai, jarak lokasi pegawai dengan kantor,dan 
tombol  kirim  presensi.  Ketika  posisi  pegawai  sudah  sesuai, 
maka  pegawai  menekan  tombol  kirim  untuk  menyimpan  dan 
mengirim data presensinya. Dari sini kegiatan presensi selesai 
dilakukan. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

D.  Rancangan Basis data 

Sistem  presensi  ini  menggunakan  beberapa  tabel  untuk 
menyimpan data dan informasi yang berkaitan dengan presensi 
pegawai.  Terdapat  5  tabel yang dibuat  dalam  sistem  presensi 
ini dengan judul antara lain tabel presensi, tabel pegawai, tabel 
wilayah_bps,  tabel  keys,  dan  tabel  limits.  Tabel  tersebut 
digambarkan pada gambar 9. 

Gambar 7. Activity diagram kegiatan presensi pegawai sistem usulan 

Alur rekapitulasi Gambar 8 memperbaiki rekapitulasi pada 
sistem  berjalan  (Gambar  3).  Alur  sistem  usulan  meniadakan 
peran admin pusat dalam kegiatan rekapitulasi presensi. Sistem 
presensi  ini  akan  memudahkan  pengolahan  data  sebab  data 
presensi sudah dalam bentuk objek data yang mudah diolah.  

Gambar 8. Alur rekapitulasi presensi 

Gambar 9. Rancangan basis data sistem usulan 

Tabel  presensi  pada  Gambar  9  berguna  untuk  menyimpan 
data presensi pegawai di dalam basis data. Variabel dari data 
presensi antara lain id, nip, waktu, latitude, longitude, address, 
catatan, dan date_created. Id presensi merupakan pengenal dari 
objek  data  presensi  yang  di  bentuk  secara  otomatis  dan  unik 
serta  merupakan  primary  key  dari  tabel  ini.  Variabel  nip 
merupakan variabel pengenal pegawai dari data presensi yang 
merupakan  foreign  key  dari  variabel  nip  pada  tabel  pegawai. 
Variabel waktu menunjukkan waktu setempat presensi pegawai 
dimasukkan ke database. Kemudian terdapat variabel latitude , 
longitude  dan  address  yang  menunjukkan  alamat  lokasi  dari 
data  presensi  yang  dibuat.  Variabel  catatan  berfungsi  untuk 
menambahkan catatan presensi yang telah dibuat oleh pegawai. 
Catatan ini digenerate secara otomatis melalui aplikasi mobile. 
Jarak  kantor,  Jam  kerja  dan  Status  presensi  dimuat  dalam 
variabel catatan ini. 

Tabel Pegawai menunjukkan data identitas pegawai di dalam 
sistem.  Tabel  pegawai  terdiri  dari  nip,  nama,  wil_id,  org_id, 
status,password,  createdby.  Variabel  nip  perperan  sebagai 
primary  key  untuk  identitas  pegawai  dan  yang  merupakan 
references  key  ke  variabel  nip  pada  tabel presensi.  kemudian 
ada  variabel  nama  untuk  menyimpan  nama  pegawai. 
Selanjutnya ada variabel wil_id yang merupakan singkatan dari 
wilayah  id  atau  pengenal  wilayah.  Variabel  ini  merupakan 
foreign  key  dari  variabel  wil_id  pada  tabel  wilayah_bps. 
Adapula  org_id  digunakan  untuk  menunjukkan  id  organisasi 
pegawai. Selain itu ada variabel status yang digunakan untuk 
menyimpan  status  pekerjaannya  pada  basis  data.  Variabel 
password  digunakan  untuk  menyimpan  password  pegawai. 
Terakhir ada variabel createdby yang berarti akun pegawai ini 
dibuat oleh siapa didalam sistem. 

Adapun Gambar 9 menunjukkan adanya tabel wilayah bps 
yang berisi tentang wilayah id, lokasi kantor , nama kantor dan 

 5 / 9 

Pegawai / Pegawai Admin PresensiAplikasi Presensi MobileServer PresensiMulaiMenekan tombol ""MulaiPresensi""Melakukanvalidasi izin akses lokasiMengaktifkanizin akses lokasiMerekamsekaligus menampilkan lokasi terkiniMemasukkan catatan (opsional)Menekan tombol""Kirim Presensi""Menyimpandata presensiMengirimdata  presensi SelesaiApakah diizinkan?TidakYaPegawai Admin PresensiMengunduhData Kehadiran pada Aplikasi WebMengolah Unduhan LaporanMenyerahkanLaporan Presensi ke Kepala Seksi umumMulaiSelesaiMelakukan Login pada Aplikasi Web Tabel PresensiTabel PegawaientityvariabelatributentityvariabelatributPKidint(11)NOT NULL NOT_INCREMENTPKnipvarchar(20)NOT NULLFKnipvarchar(20)NOT NULLnamavarchar(256)waktudatetime(2)NOT NULLFKwil_idvarchar(10)NOT NULLlatitudevarchar(25)NOT NULLorg_idvarchar(20)NOT NULLlongitudevarchar(25)NOT NULLstatusvarchar(5)addressvarchar(25)NOT NULLpasswordvarchar(256)NOT NULLcatatanvarchar(25)device_infovarchar(128)NOT NULLdate_createdtimestamp CURRENT_TIMESTAMPNOT NULLdate_createdtimestampNOT NULLTabel limitsTabel Wilayah_bpsentityvariabelatributentityvariabelatributPKidint(11)NOT NULLPKwil_idvarchar(10)NOT NULLurivarchar(255)NOT NULLnama_bpsvarchar(256)NOT NULLcountint(10)NOT NULLnip_adminvarchar(20)NOT NULLhour_startedint(11)NOT NULLlatitudevarchar(25)NOT NULLFKapi_keyvarchar(40)NOT NULLlongitudevarchar(25)NOT NULLjam_mulaiint(1)NOT NULLTabel keysjam_akhirint(1)NOT NULLentityvariabelatributjam_kerjaint(1)NOT NULLPKidint(11)NOT NULLjarak_wfoint(10)NOT NULLuser_idint(11)jarak_wfhint(10)NOT NULLkeyvarchar(40)NOT NULLlapanganbooleanNOT NULLlevelint(2)NOT NULLignore_limitstinyint(1)NOT NULLis_private_keytinyint(1)ip_addressestextdate_createdint(11)NOT NULL 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

masing  memiliki  fungsinya  sendiri.  Titik  akhir  presensi 
digunakan  untuk  mengelola  penggunaan  data  presensi  pada 
sistem.  Pengelolaan  ini  tercakup  memasukkan,  mengubah, 
menghapus  serta  menampilkan  data  presensi.  Kemudian  ada 
titik akhir pegawai, yang digunakan untuk mengelola akun dan 
pengaturan akun pegawai tersebut. Terakhir adalah  titik akhir 
wilayahbps,  titik  akhir  ini  digunakan  untuk  mengelola  data 
pengaturan  masing  -  masing  wilayah  bps.  pengaturan  ini 
mencakup pengaturan batasan lokasi, jam masuk presensi, jam 
kerja presensi dan jam pulang kegiatan presensinya. 

nip admin kantor. Variabel wilayah id merupakan primary key 
dari tabel ini.  Variabel nama BPS menunjukkan nama kantor 
BPS  wilayah  tersebut.  Guna  menyimpan  lokasi  kantor  BPS 
terdapat  variabel  latitude  dan  longitude.  Variabel  nip  admin 
menunjukkan  admin  pegawai  admin  presensi  yang  ada  pada 
wilayah tersebut. 

Tabel  limits  dan  tabel  keys  merupakan  tabel  yang  dibuat 
untuk  menggunakan  fitur  pembatasan  akses  key  pada 
codeigniter.  Tabel  tersebut  berisi  key  atau  kunci  untuk 
mengakses  webservice kemudian ada limit untuk pembatasan 
aksesnya. 
E.  Implementasi Sistem 

Sistem informasi yang telah dirancang, diterapkan dalam 2 
aplikasi yaitu aplikasi web dan aplikasi mobile. Aplikasi web 
digunakan  untuk  rekapitulasi  presensi.  Sedangkan  aplikasi 
mobile  digunakan  untuk  kegiatan  presensi  dan  monitoring 
presensi.  

untuk 

sebagai 

rancangan 

digunakan 

Pada aplikasi web, implementasi dilakukan dalam beberapa 
tahapan.  Pertama,  penulis  melakukan  konfigurasi  basis  data 
tempat 
sesuai 
penyimpanan data yang dibutuhkan pada sistem yang dibangun. 
Instalasi  dan  konfigurasi  codeigniter  selanjutnya  dilakukan 
supaya  bisa  digunakan  untuk  mengakses  basis  data, 
menjalankan  webservice  dan  menampilkan  aplikasi  web 
rekapitulasi. Konfigurasi yang dilakukan adalah mengaktifkan 
fitur-fitur pada codeigniter dan mengatur titik akhir (endpoint)  
webservice  sehingga  dapat  dikonsumsi  oleh  aplikasi  mobile. 
Titik akhir yang digunakan terdapat pada Tabel III. 

TABEL III. Daftar Titik Akhir Webservice 

Method 
GET 

Endpoint 
/presensi 

GET 

GET 

GET 

POST 
PUT 

/presensi?{nip} 

/presensi?{wil_id}{waktu} 

/presensi?{wilayah_id} 

/presensi 
/presensi 

DELETE 

/presensi 

GET 

GET 

POST 

PUT 

/wilayahbps 

/wilayahbps?{wil_id} 

/wilayahbps 

/wilayahbps 

DELETE 

/wilayahbps 

GET 
GET 

/pegawai 
/pegawai?{wil_id} 

Description 
List semua presensi 
yang ada di 
database 
List semua presensi 
oleh 1 user 
List semua presensi 
oleh 1 user pada 
waktu tertentu 
List semua presensi 
di 1 wilayah kerja 
Membuat presensi 
Update value 
presensi 
Menghapus 
presensi 
Daftar semua 
wilayah bps 
1 wilayah bps 
tertentu 
Membuat wilayah 
baru pada database 
Mengubah wilayah 
bps 
Menghapus wilayah 
tertentu 
List semua pegawai 
List pegawai 1 
wilayah 

Tabel diatas menampilkan daftar titik akhir yang digunakan 
untuk pengiriman dan pengelolaan data presensi pada sistem. 
Terdapat  3  titik  akhir  utama  yang  digunakan  yang  masing  - 

Gambar 10. Menu login aplikasi web 

Aplikasi web rekapitulasi memiliki 2 menu yaitu menu login 
untuk sarana masuk pegawai admin (Gambar 10) , dan menu 
utama  untuk  menampilkan  tabel  data  presensi  pada  wilayah 
kerja  admin  tersebut  (Gambar  11).  Implementasi  tampilan 
menu login aplikasi web dapat terlihat pada Gambar 10. Pada 
menu tersebut terdapat validasi nip dan password untuk masuk 
kedalam sistem. Hal ini menunjukkan tercapainya proses login 
pada diagram use case sistem pada bab sebelumnya.  

Tampilan kedua yakni tampilan menu utama pada aplikasi 
web  yang  dapat  dilihat  pada  Gambar  11.  Pada  tampilan  ini 
disajikan dropdown “pilih bulan” yang dapat digunakan admin 
presensi untuk memilih waktu data yang ingin ditampilkan atau 
direkap. Identitas nama, nip dan wilayah kerja admin presensi 
ditampilkan pada blok kanan atas. Tampilan tabel data presensi 
dan  tombol  unduh  ditampilkan  di  tengah  bawah  yang 
merupakan isi utama dari web rekapitulasi ini. Tombol unduh 
atau download digunakan untuk mengunduh data presensi yang 
telah dipilih kedalam bentuk excel. Implementasi tampilan web 
ini  merupakan  penerapan  proses  rekapitulasi  presensi  oleh 
admin pada diagram use case pada Gambar 4.  

 6 / 9 

 
 
 
 
 
 
 
Gambar 11.Tampilan menu utama aplikasi web 

Implementasi 

aplikasi  mobile  pada  penelitian 

ini 
menggunakan  kode  editor  Android  Studio  versi  4.1  untuk 
membuat  dan  mengelola  project  flutter  yang  telah  dirancang 
pada  penelitian  ini.  Pertama  penulis  melakukan  pembuatan 
project  flutter  yang  digunakan  sebagai  aplikasi  mobile  pada 
Android Studio. Setelah pembuatan selesai penulis melakukan 
inisiasi beberapa package atau paket yang dibutuhkan pada file 
pubspec.yml didalam file project flutter yang dibuat. Inisiasi ini 
bertujuan  untuk  mempermudah  aliran  data,  pembuatan 
tampilan dan fitur pada aplikasi presensi mobile yang dibangun. 

Gambar 12. Cuplikan kode back-end aplikasi mobile 

Cuplikan kode pada  Gambar 12 menunjukan implementasi 
kode class MainPage atau menu utama pada aplikasi mobile. 
MainPage disini bukan merupakan tampilan menu utama tapi 
kode program utama yang dijalankan ketika aplikasi di jalankan. 
Pada  kode  tersebut  terdapat  inisiasi  awal  atau    initState  yang 
menunjukkan  sistem  pemberitahuan  pada  aplikasi  yang 
dibangun  menggunakan  paket  dari  Onesignal.  Paket  ini 
serta 
digunakan 
menghilangkan  notifikasi  atau  pemberitahuan dalam  aplikasi. 
Kemudian  kembalian  pada  fungsi  pembangunan  widget  atau 

untuk  mengirim,  memunculkan 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

widget  build  menunjukkan  inisiasi  kode  untuk  menuju  page 
atau halaman tertentu pada aplikasi. Kode inilah yang disebut 
rute  halaman.  Penulis 
routing  page  atau  pemetaan 
menggunakan  paket  GetX  untuk  memudahkan  pengelolaan 
state dan pemetaan rute pada aplikasi. 

Gambar 13. Tampilan halaman aplikasi mobile 

Tampilan  menu  pada  Gambar  13  merupakan  halaman  – 
halaman  yang  menggambarkan  penerapan  use  case  pada 
Gambar  4  dalam  kebutuhan  fungsional  sistem.  Implementasi 
use case login diterapkan pada Gambar 13 nomor 1. Kemudian 
implementasi use case menampilkan riwayat presensi pegawai 
telah terjawab pada Gambar 13 nomor 2. Pada halaman ini juga 
terdapat tombol untuk melakukan presensi yang mengarah pada 
halaman  presensi  seperti  pada  Gambar  13  nomor  3  yang 
menjawab  interaksi  pengguna  dengan  proses  presensi  pada 
sistem.  

galat 

tombol 

ketika  menekan 

Implementasi kebutuhan non-fungsional diterapkan kedalam 
beberapa  fitur  aplikasi.  Kebutuhan  non-fungsional  sistem 
terkait batasan lokasi presensi diimplementasikan pada aplikasi 
di  bagian  menu  presensi.  Apabila  pengguna  melakukan 
presensi  diluar  wilayah  kerja  maka  akan  muncul  pesan  error 
atau 
yang 
mengindikasikan pengguna berada luar  wilayah kerja sehingga 
tidak  bisa  melakukan  presensi  (Gambar  14  nomor  1). 
dengan 
Kebutuhan 
pemberitahuan  atau  sistem  notifikasi  diimplementasikan 
menggunakan  paket  OneSignal.  Paket  ini  memungkinkan 
admin  untuk  melakukan  kostumasi  pesan  yang 
ingin 
disampaikan kepada pengguna terkait presensi. OneSignal juga 
memberikan layanan untuk notifikasi terjadwal menurut zona 
waktu masing-masing pengguna. Hal ini seperti pada Gambar 
14 nomor 2. 

non-fungsional 

berkaitan 

“kirim” 

yang 

 7 / 9 

 
 
 
  
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

4.  Memperlihatkan detail 

data pegawai lain 

5.  Mengubah status 

pegawai lain pada 

menu monitoring 

6.  Melakukan login pada 

aplikasi web 

7.  Memperlihatkan 

rekapitulasi presensi 

bulan mei 

5 

5 

5 

5 

0 

0 

0 

0 

3.6 detik 

3,4 detik 

- 

- 

Pengujian ini dilakukan pada 5 orang responden. Dari kelima 
responden,  semuanya  memakai  perangkat  berbasis  android. 
Kelima  responden  berhasil  menjalankan  task  yang  ada  pada 
Tabel  IV.  Tugas  yang  paling  cepat  dikerjakan  adalah  tugas 
untuk memperlihatkan detail presensi yaitu dengan rata-rata 3,2 
detik.  Task  yang  paling  lama  dikerjakan  adalah  melakukan 
login namun tidak ada keluhan responden terkait tugas tersebut. 
Selain tugas -tugas yang telah dikerjakan, beberapa tanggapan 
responden  mengenai  sistem  presensi  ini  antara  lain,  menurut 
responden  aplikasi  sudah  baik  dan  mampu  mengakomodir 
kegiatan presensi pegawai BPS. Responden juga beranggapan 
bahwa  alur  presensi  pada  sistem  presensi  ini  lebih  simpel 
dibandingkan  dengan  kaizala  khususnya  pada  jumlah  klik 
untuk mengirim data presensinya. 

task  atau 

IV  semua 

Pada  Tabel 

tugas  merupakan 
implementasi dari kebutuhan fungsional sistem yang tergambar 
pada Gambar 4. Sesuai dengan Gambar 4, pengujian kebutuhan 
fungsional  dilakukan  pada  2  aplikasi  yaitu  aplikasi  mobile 
(tugas  nomor  1-5)  dan  aplikasi  web  (tugas  nomor  6-7) 
Pengujian 
ini  dilakukan  untuk  memastikan  kebutuhan 
fungsional  yang  diimplementasikan  sudah  layak  dan  berhasil 
diterapkan.  

Pengujian  pada  kebutuhan  non-fungsional  juga  tercantum 
dalam  kuesioner  yang  dipakai.  Salah  satunya  terdapat  pada 
Tabel  4  dengan  tugas  bernomor  2  yang  digunakan  untuk 
menguji kebutuhan non-fungsional nomor 1. Responden tidak 
perlu  lagi  menunggu  pranala  undangan  yang  diberikan  oleh 
BPS  dan  bisa  langsung  melakukan  presensi.  Selain  itu  tugas 
pada Tabel 4 bernomor 7 secara tersirat  menjawab kebutuhan 
non-fungsional nomor 4 yaitu data presensi tersimpan langsung 
dalam  server.  Seluruh  responden  juga  mengaku  berhasil 
melihat  notifikasi  atau  pemberitahuan  untuk  melakukan 
presensi pada aplikasi. Hal tersebut sejalan dengan kebutuhan 
non-fungsional  nomor  2  yaitu  sistem  dapat  menampilkan 
pemberitahuan yang berkaitan dengan presensi. 

VII. 

PENUTUP 

Kesimpulan 

Berdasarkan hasil yang diperoleh dari proses pembangunan 
sistem  presensi  online  pegawai  dapat  ditarik  beberapa 
kesimpulan sebagai berikut. 

 8 / 9 

Gambar 14. Tampilan implementasi kebutuhan non-fungsional 

F.  Pengujian Sistem 

Pengujian sistem, penulis lakukan dengan UAT dengan jenis 
blackbox  testing.  Pengujian  dilakukan  dengan  wawancara 
bersama responden menggunakan kuesioner. Penulis membuat 
kuesioner  menggunakan 
dan  melakukan 
wawancara  pengujian  dengan  daring  menggunakan  google 
meet. Langkah-langkah pengujian yang akan dilakukan adalah 
sebagai berikut: 

googleform 

•  Menjelaskan  mekanisme,  tujuan  pengujian,  dan  apa 

yang diharapkan dari kegiatan pengujian. 

•  Pemberian waktu kepada responden untuk mengunduh 
aplikasi  sambil  menanyakan  pertanyaan  awal  pada 
kuesioner 

•  Pengerjaan 

skenario 

tugas-tugas 

yang 
dicontohkan  pada  Tabel  IV.  Proses  ini  diawasi  oleh 
penulis  agar  penulis  dapat  mendata  dari  kegiatan 
pengujian  tersebut  dan  memasukkan  data  kedalam 
kuesioner. 

seperti 

•  Wawancara  singkat 

tentang  bagaimana  pendapat 

responden tentang sistem yang dibangun. 

•  Penutupan. 
Dalam  pelaksanaan  kegiatan  pengujian,  data-data  yang 

dicari adalah sebagai berikut: 

Identitas responden yang diuji 

• 
•  Status  apakah  berhasil  atau  tidak  penyelesaian  tugas-

tugas tersebut 

•  Waktu pengerjaan setiap tugas-tugas 
•  Penyebab terjadinya kegagalan penyelesaian tugas bila 

pengguna tidak bisa menyelesaikan tugas 
•  Pendapat responden sebagai masukkan evaluasi 

TABEL IV. Hasil Pengujian Sistem 

No. 

Task 

Berhasil 

1.  Melakukan login 

2.  Melakukan presensi 

3.  Memperlihatkan detail 

5 

5 

5 

presensinya 

Tidak 
Berhasil 

0 

0 

0 

Rata-rata 
Waktu 
penyelesaian 
10,2 detik 

6,2 detik 

3,2 detik 

 
 
 
 
 
 
 
 
•  Sistem  presensi  online  pegawai 

telah  berhasil 
dibangun  dengan  menggunakan  Flutter  yang  dapat 
dijalankan  pada  platform  Android,  dan  berpotensi 
untuk  dijalankan  pada  platform  iOS,  Web,  maupun 
Desktop. Potensi dijalankan di perangkat iOS dengan 
cara  mengoreksi  ulang  dan  mengatur  plugin  flutter 
yang  cocok  di  iOS  tanpa  perlu  mengubah  kode 
program. 

•  Sistem yang dibangun berpotensi dapat diterima oleh 
pengguna  sesuai  dengan  hasil  pengujian.  Seluruh 
responden  berhasil  melaksanakan  tugas  –tugas  yang 
diberikan. 

•  Sistem  yang  dibangun  berpotensi  memudahkan 
pengguna  dalam  menjalankan  dan  memonitoring 
lingkup  pegawai  BPS. 
kegiatan  presensi  pada 
Terdapat  fitur  monitoring  ,  pembatasan  lokasi,  dan 
catatan presensi yang memperjelas dan memudahkan 
pendataan serta pengawasan kegiatan presensi. 

[1] 

[2] 

[3] 

[4] 

[5] 

[6] 

[7] 

[8] 

DAFTAR PUSTAKA 

M. Cahyaningtyas, “Seberapa Penting Kehadiran Karyawan di 
Kantor,” Mar. 14, 2019. 
https://www.gadjian.com/blog/2019/03/14/seberapa-penting-
kehadiran-karyawan-di-kantor/ (accessed Jan. 14, 2021). 

M. Al Satrio and L. A. Abdillah, “Aplikasi Presensi Mahasiswa 
dengan Menggunakan QR Code Berbasis Android pada Universitas 
Bina Darma,” pp. 7–12, 2017, doi: 10.31227/osf.io/nmhrx. 

P. K. Bose and M. J. Kabir, “Fingerprint: A Unique and Reliable 
Method for Identification,” J. Enam Med. Coll., vol. 7, no. 1, pp. 
29–34, 2017, doi: 10.3329/jemc.v7i1.30748. 

A. F. Okilas, S. D. Siswanti, and M. D. Rachman, “Location based 
service for information publication using GPS on android-based 
mobile phone,” Int. Conf. Electr. Eng. Comput. Sci. Informatics, 
vol. 1, no. August, pp. 190–197, 2014, doi: 10.11591/eecsi.1.373. 

I. T. Union, X.800. 1991. 

N. Hermanto, N. -, and N. R. D. R. Riyanto, “Aplikasi Sistem 
Presensi Mahasiswa Berbasis Android,” Simetris J. Tek. Mesin, 
Elektro dan Ilmu Komput., vol. 10, no. 1, pp. 107–116, 2019, doi: 
10.24176/simet.v10i1.2799. 

S. Frans and W. Marsisno, “Pembangunan Aplikasi Pengajuan Cuti 
Pegawai Dengan Flutter,” 2020. 

Hasanudin, “Aplikasi Presensi Online Berbasis Android Dengan 
Fitur Fingerprint Dan Location Based Service (Lbs) Pada Lembaga 
Pendidikan XYZ,” 2018. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

 9 / 9 

 
 
 
 
"
221709509,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Klasifikasi Berita Banjir Menggunakan Metode 
Support Vector Machine dan Ekstraksi Informasi 
Menggunakan Metode Name Entity Recognition 
pada Berita Online 
Studi Kasus: Situs Berita Online Detik.com 

Ahmad Nurhadi Ridwan (221709509, 4SD1) 

Dosen Pembimbing: Robert Kurniawan 

ini 

Penelitian 

bertujuan 

Ringkasan— 

untuk 
mengklasifikasikan  berita  banjir  menggunakan  metode  Support 
Vector  Machine  (SVM)  dengan  kernel  linear,  polinomial,  dan 
radial  basis  function  (RBF)  dan  menggunakan  proporsi  data 
training  dan  data  testing  60%,  70%,  80%,  dan  90%.  Pada 
pembentukan  model  klasifikasi  didapatkan  model  terbaik  yaitu 
model  SVM  dengan  kernel  RBF  dengan  proporsi  70% 
menghasilkan nilai akurasi sebesar 75,42%. Kemudian pada hasil 
klasifikasi  berita  banjir,  menggunakan  model  klasifikasi  SVM 
dengan kernel RBF, akan dilakukan ekstraksi informasi tentang 
kejadian  bencana  banjir  meliputi  lokasi  dan  waktu  terjadinya 
banjir.  Selanjutnya,  hasil  ekstraksi  informasi  akan  dilakukan 
pemetaan sehingga memudahkan interpretasi. 

Kata  Kunci—  klasifikasi,  Support  Vector  Machine,  ekstraksi 

informasi, berita online, banjir. 

I.  LATAR BELAKANG 

dapat 

banjir 

Banjir  merupakan  salah  satu  bencana  alam  yang  paling 
sering  terjadi  di  Indonesia.  Definisi  banjir  adalah  keadaan 
dimana  suatu  daerah  tergenang  oleh  air  dalam  jumlah  yang 
besar.  Kedatangan 
dengan 
memperhatikan curah hujan dan aliran air. Namun kadangkala 
banjir  dapat  datang  tiba-tiba  akibat  dari  angin  badai  atau 
tanggul  yang  biasa  disebut  banjir  bandang. 
kebocoran 
Berdasarkan  gambar  1,  banjir  merupakan  kejadian  bencana 
alam yang paling banyak terjadi di Indonesia dari tahun 2016 
hingga tahun 2020 [1]. Pada tahun 2020, jumlah kejadian banjir 
di Indonesia meningkat dengan pesat.  

diprediksi 

Selain  data  yang  bersumber  dari  BNPB  di  era  modern 
sekarang ini terdapat sumber data lain yang juga dapat berperan 
sebagai  komplementer  untuk  data  kejadian  bencana  alam. 
Sumber  data  tersebut  tersimpan  di  dunia  maya  yang  dikenal 
dengan  istilah  big  data  yang  mana  big  data  itu  sendiri  dapat 
didefinisikan  sebagai  sebuah  sistem  yang  mengintegrasikan 
dunia  nyata,  manusia,  dan  dunia  maya  [2].  Data  tersebut 
didapatkan melalui situs-situs berita yang terdapat di internet. 
Berita yang tersaji secara online melalui situs-situs di internet 
disebut  sebagai  berita  online.  Berita  online  dan  media  berita 
online  tumbuh  pesat  dari  tahun  1990  karena  sebagian  surat 
kabar  belum  memiliki  akses  onine  sebelum  berkembangnya 

internet dan media cetak sudah semakin tidak relevan dengan 
kehidupan  pembaca  kemudian digantikan  dengan  penyebaran 
informasi  berbasis  sistem  internet  [3].  Kelebihan  penyebaran 
berita  melalui  media  internet  adalah  kemudahan  penggunaan 
dan ketersediaan umum dan meskipun berita yang dirilis sudah 
tidak  update  namun  berita tersebut  tetap akan  bisa  dicari  dan 
dibaca  kembali  selama  artikel  berita  yang  ditulis  masih 
tersimpan di dalam portal berita online. 

Gambar 1. Grafik jumlah kejadian bencana alam di Indonesia 
berdasarkan data BNPB tahun 2016-2020 (Sumber data: BNPB) 

Kemudian  berdasarkan  data  SUSENAS  tahun  2017  yang 
dipublikasikan oleh Badan Pusat Statistik (BPS) menunjukkan 
bahwa  sepertiga  penduduk  Indonesia  mengunakan  internet 
pada tahun 2017. Penetrasi internet yang terendah ada di daerah 
Maluku  Utara,  Nusa  Tenggara  Timur  (17,88%),  dan  Papua 
(16,5%).  Sementara  penetrasi  tertinggi  ada  di  daerah  DKI 
Jakarta  (48,35%)  kemudian  disusul  dengan  Kepulauan  Riau 
(48,35%)  dan  Yogyakarta  (45,38%).  Dari  total  pengguna 
internet  hanya  65,9%  yang  menggunakan  untuk  mengakses 
berita online atau sekitar 50,7 juta orang. Sebanyak 74,7% ada 
di  perkotaan  dan  sisanya  di  perdesaan  [4].  Perkembangan 
internet  di  Indonesia  juga  berimbas  terhadap  media  baru 
penyampai informasi. Arus informasi tidak lagi dikuasai olehh 
surat kabar, majalah, tabloid, dan televisi. Portal berita menjadi 
sebuah  sumber  penyedia  informasi  yang  saat  ini  sedang  naik 
daun  seiringnya  dengan  pesatnya  kemajuan 
teknologi. 
Berdasarkan data dari Alexa.com tahun 2012, terdapat 13 portal 

1/8 

 
 
 
 
tertinggi 

berita  online  dengan  rata-rata  pageviews  dan  pengunjung 
harian 
Kompas.com, 
Vivanews.com,  Tribunnews.com,  Okezone.com,  Tempo.co, 
Republika  Online,  Antara  News,  Liputan6.com,  MetroTV, 
Kontan Online, Media Indonesia, dan Bisnis Indonesia [5]. 

Detik.com, 

adalah 

Konten  berita  online  merupakan  sebuah  unstructured  text, 
yang hanya bisa dibaca dan dimengerti oleh manusia, sehingga 
bisa  dilakukan  sebuah  metode  yang  memudahkan  untuk 
mendapatkan informasi secara ringkas menggunakan Machine 
Learning. Salah satu metode tersebut adalah text mining. Text 
Mining  merupakan  salah  satu  langkah  dalam  text  analyzing 
yang dilakukan secara otomatis oleh komputer untuk menggali 
informasi  yang  berkualitas  dari  suatu  rangkaian  teks  dalam 
sebuah  dokumen.  Sebelum  suatu  data 
teks  dianalisis 
menggunakan  metode  dalam  text  Mining  perlu  dilakukan 
text  diantaranya  adalah  Tokenizing,  Case 
preprocessing 
Folding,  Stopwords,  dan  Stemming.  Setelah  dilakukan 
preprocessing maka  selanjutnya dilakukan metode  klasifikasi 
dalam  mengelompokkan  kedalam  masing-masing  kategori. 
Analisis  klasifikasi  data  mining  adalah  menentukan  sebuah 
record data baru ke salah satu dari beberapa kategori yang telah 
didefinisikan  sebelumnya,  disebut  juga  dengan  supervised 
learning  [6].  Metode-metode  yang  telah  dikembangkan  oleh 
periset  untuk  menyelesaikan  kasus  klasifikasi,  antara  lain: 
Pohon  keputusan  (Decision  Tree),  Naïve  Bayes,  Jaringan 
Syaraf  Tiruan,  Analisis  Statistik,  Algoritma  Genetik,  Rough 
Sets,  k-Nearest  Neighbour,  Metode  Berbasis  Aturan  (Rule-
Based),  Memory  Based  Reasoning,  dan  Support  Vector 
Machine [6]. 

Untuk  mendapatkan  informasi  dari  sebuah  konten  berita, 
diperlukan  beberapa  tahapan  pada  text  Mining.  Salah  satu 
teknik  dalam  text  Mining  adalah  Information  Extraction  dan 
Feature Extraction. Information Extraction merupakan teknik 
dalam text mining untuk mendapatkan informasi dalam sebuah 
teks  dan  mengubah  unstructured  text  menjadi  structured  text 
[7].   Tujuan  ekstraksi  informasi  (IE)  adalah  untuk  mengubah 
teks  menjadi  format  terstruktur  dan  mengurangi  informasi 
dalam dokumen menjadi struktur tabel, teks yang tidak terlihat 
diambil  sebagai  masukan  untuk  menghasilkan  data  dengan 
format  tetap  dan  tidak  ambigu  sebagai  keluaran  (output). 
Informasi  yang  ditentukan  kemudian  dapat  diekstraksi  dari 
dokumen  yang  berbeda  dengan  representasi  yang  heterogen 
dan diringkas dan disajikan dengan cara yang seragam. [8] 

Name  Entity  Recognition  (NER)  atau  Name  Entity 
Recognition  and  Classification  (NERC)  adalah  salah  satu 
komponen  utama  dari  Information  extration  yang  bertujuan 
untuk  mendeteksi  dan  mengklasifikasikan  named-entity  pada 
suatu teks. NER umumnya digunakan untuk mendeteksi nama 
orang, nama tempat dan organisasi dari sebuah dokumen, tetapi 
dapat juga diperluas untuk identifikasi gen, protein dan lainnya 
sesuai kebutuhan [9]. 

Oleh  karena  itu  untuk  mendapatkan  data  kejadian  banjir 
yang  bersumber  dari  berita  online,  maka  penelitian  ini 
bertujuan untuk mengkaji tentang penerapan text mining untuk 
klasifikasi  berita  banjir  menggunakan  metode  Support Vector 
Machine  dan ekstraksi  informasi  menggunakan metode  name 
entity recognition pada teks berita online. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

II.  TUJUAN PENELITIAN 

Berdasarkan  identifikasi  masalah  pada  BAB  1,  maka 

penelitian ini bertujuan sebagai berikut 

1.  Mengklasifikasikan  berita  banjir  berdasarkan  berita 
online menggunakan Support Vector Machine (SVM). 
2.  Melakukan  ekstraksi  informasi  pada  hasil  klasifikasi 
berita banjir dan pemetaan dari hasil ekstraksi informasi 
tersebut. 

III. PENELITIAN TERKAIT 

Gambar 2. Diagram Alir Literatur Review 

terbaik 

Berdasarkan  gambar  2,  peneliti  mengumpulkan  beberapa 
penelitian 
terkait  metode  yang  digunakan.  Berdasarkan 
penelitian  dengan  topik  membandingkan  metode  Support 
Vector  Machine  dengan  metode  klasifikasi  lainnya  seperti 
Naive  bayes,  k-NN,  Decision  Tree,  dan  Random  Forest 
memberikan  kesimpulan  bahwa  metode  SVM  menghasilkan 
akurasi  yang 
[15][16][17][18][19].  Namun  ada 
beberapa penelitian yang menyimpulkan bahwa metode SVM 
kurang  baik  untuk  mengklasifikasikan  3  kelas  atau  lebih 
[13][14]. 
klasifikasi 
menggunakan  metode  SVM 
[20][21][22][23][24][25][26] 
dengan  optimasi  menggunakan  kernel linear,  polinomial,  dan 
radial  basis function  dan  name  entity  recognition  (ner)  untuk 
Information  Extraction  menghasilkan  nilai  akurasi  yang 
berbeda beda [27][28][29][30][31]. 

Selanjutnya, 

penelitian 

tentang 

Pada  penelitian  ini,  peneliti  melakukan  klasifikasi  berita 
banjir berdasarkan berita online menggunakan metode Support 
Vector Machine  dengan  kernel  linear, polinomial,  dan  radial 
basis  function.  Selanjutnya,  berita  banjir  hasil  klasifikasi 
sebelumnya akan dilakukan ekstraksi informasi kejadian banjir, 
yaitu  informasi  lokasi  dan  tanggal  terjadinya  banjir.  Metode 
yang  digunakan untuk ekstraksi informasi  adalah  name  entity 
recognition. 

IV. METODE PENELITIAN 

4.1.  Pengumpulan Data 

Pada  penelitian  ini,  berita  online  yang  diambil  adalah 
berita yang rilis dalam rentang waktu Januari 2016 sampai 
dengan Desember 2020 pada situs berita online Detik.com. 
Scraping data pada web berita online Detik.com dilakukan 
dengan menggunakan Library yang tersedia pada bahasa 
pemrograman  python.  Library  yang  digunakan  untuk 
scraping  tersebut  adalah  selenium  dan  scrapy.  Pada  saat 

2/8 

 
 
 
 
melakukan  scraping  berita  online,  hal  yang  pertama 
dilakukan  adalah  mengambil  seluruh  URL  berita  online 
berdasarkan  hasil  pencarian  dengan  keyword  ‘banjir’. 
Pengambilan  URL 
tersebut  dilakukan  menggunakan 
library  selenium  kemudian  disimpan  ke  dalam  bentuk 
excel. 

Alamat  url artikel  berita online  yang  telah didapatkan 
akan  dibuka  satu  persatu  secara  otomatis  dengan 
menggunakan  library  scrapy  agar  proses  lebih  cepat. 
Setiap  halaman  artikel  berita  online  yang  telah  dibuka 
memuat judul berita, tanggal berita diterbitkan, dan isi atau 
konten  berita  online.  Kemudian,  akan  dilakukan 
pengambilan  yang  selanjutnya  akan  dijadikan  sebagai 
dataset penelitian. 

4.2.  Preprocessing Text 

Sebelum  masuk  tahap  klasifikasi,  data  konten  berita 
online akan dilakukan preprocessing terlebih dahulu agar 
dapat  digunakan  sebagai  data  training  dan  testing. 
Preprocessing  data  yang  dilakukan  dimulai  dari  Case 
Folding, Stopword, Stemming, dan Tokenizing [12]. 
1.  Case Folding 

Data artikel berita online yang telah didapatkan masih 
mengandung banyak unsur-unsur tanda baca yang tidak 
diperlukan.  Pada  proses  ini,  akan  dilakukan  suatu 
proses  untuk  mengubah  semua  karakter  pada  teks 
menjadi  huruf  kecil.  Karakter  yang  diproses  hanya 
huruf  ‘a’  hingga  ‘z’  dan  selain  karakter tersebut  akan 
dihilangkan  seperti  tanda  baca  titik  (.),  koma  (,),  dan 
angka. 
2.  Stopword 

Selanjutnya,  akan  dilakukan  suatu  proses  untuk 
menghilangkan  kosakata  yang  bukan  merupakan  kata 
tidak 
unik  atau  ciri  pada  suatu  dokumen  atau 
menyampaikan  pesan  apapun  secara  signifikan  pada 
teks atau kalimat. Kosakata yang dimaksudkan adalah 
kata  penghubung  dan  kata  keterangan  yang  bukan 
merupakan  kata  unik  misalnya  “sebuah”,  “oleh”, 
“pada”, dan sebagainya. 

3.  Stemming 

Kemudian,  dilakukan  proses  untuk  mendapatkan  kata 
dasar  dengan  cara  menghilangkan  awalan,  akhiran, 
sisipan,  dan  confixes  (kombinasi  dari  awalan  dan 
akhiran). 
4.  Tokenizing 

Selanjutnya  dilakukan  proses  tokenizing,  merupakan 
proses memecah yang semula berupa kalimat menjadi 
kata-kata  atau  memutus  urutan  string  menjadi 
potongan-potongan  seperti  kata-kata  berdasarkan  tiap 
kata  yang  menyusunnya,  kemudian  dilakukan 
pembentukan pembobot menggunakan TF-IDF. 

4.3.  Pembentukan Model SVM 

dilakukan 

Pembentukan  model  SVM 

dengan 
training  untuk 
melakukan  pelatihan 
terhadap  data 
mendapatkan  nilai  pembobot  yang 
terbaik.  Nilai 
pembobot terbaik ditentukan dengan memasukkan semua 
kemungkinan  nilai  pembobot  hingga  dihasilkan  akurasi 
yang  terbaik.  Pada  penelitian  ini  data  training  dan  data 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

testing  dibagi  dengan menggunakan proporsi 60%, 70%, 
80%,  dan  90%.  Penggunaan  proporsi  tersebut  dilakukan 
terbaik,  yang 
agar  didapatkan  model  SVM  yang 
menghasilkan nilai akurasi tertinggi. 
 Fungsi  kernel  merupakan 

fungsi  yang 
memetakan  data  ke  ruang  dimensi  yang  lebih  tinggi 
dengan  harapan  data  akan  memiliki  struktur  yang  lebih 
baik  sehingga  lebih  mudah  dipisahkan.  Ada  3  fungsi 
kernel yang digunakan pada model SVM ini yaitu fungsi 
kernel  linear,  polinomial,  dan  radial  basis  function. 
Bentuk  formula  untuk  setiap  fungsi  kernel  dapat  dilihat 
pada tabel 1 [12]. 

suatu 

TABEL I 
TABEL FORMULA FUNGSI KERNEL 

Fungsi Kernel 
Linear 
Polinomial 
Radial Basis Function 

Formula 
𝐾(𝑥𝑖, 𝑥) = 𝑥𝑖

𝐾(𝑥𝑖, 𝑥) = (𝛾𝑥𝑖
2
𝐾(𝑥𝑖, 𝑥) = exp (−𝛾||𝑥 − 𝑥𝑖||

𝑇𝑥 
𝑇𝑥 + 𝑟)𝑝, 𝛾 > 0 
) 

4.4.  Menghitung Nilai Akurasi Model 

Nilai  akurasi  model  dihitung  berdasarkan 

tabel 
Confusion  Matrix.  Confusion  Matrix  merupakan  suatu 
metode  yang  umum  digunakan  dalam  melakukan 
perhitungan akurasi pada konsep Data Mining. Confusion 
Matrix  digambarkan  dengan  Tabel  II  yang  menyatakan 
jumlah  data  uji  yang  benar  diklasifikasikan  dan  jumlah 
data uji yang salah diklasifikasikan [11]. 

TABEL II 
TABEL CONFUSION MATRIX 

Classified as 

Predicted “+” 
True Positive 
False Positive 

Predicted “- “ 
False Negative 
True Negative 

Correct 
Classification 
Actual “+” 
Actual “- “ 

adalah 

Akurasi 

persentase 

yang 
diklasifikasikan  dengan  benar.  Semakin  tinggi  hasil 
akurasi, semakin efektif model algoritma klasifikasi yang 
diuji [11]. Akurasi dirumuskan pada persamaan 1. 

observasi 

Akurasi  =

𝑇𝑟𝑢𝑒 𝑃𝑜𝑠𝑖𝑡𝑖𝑣𝑒 + 𝑇𝑟𝑢𝑒 𝑁𝑒𝑔𝑎𝑡𝑖𝑣𝑒
Total Data

                                       (1) 

4.5.  Pembentukan Model NER 

Pada  data  hasil  klasifikasi  akan  dijadikan  sebagai 
dataset  untuk  proses  ekstraksi informasi.  Informasi  yang 
diekstrak  adalah  informasi  lokasi  dan  waktu  kejadian 
banjir.  Informasi  lokasi  akan  diekstrak  menggunakan 
metode NER dengan menggunakan library spacy yang ada 
pada  bahasa  pemrograman  python.  Sebelum  melakukan 
ekstraksi  informasi  terlebih  dahulu  membuat  model 
kosong. Kemudian menentukan pattern yang berisi entitas 
lokasi dan tanggal, dimana lokasi yang digunakan adalah 
lokasi  yang  ada  di  DKI  Jakarta  dari  level  kota  hingga 
kelurahan. 

4.6.  Pengecekan Berita Similar 

Terkadang berita yang didapatkan dari hasil scraping 
beberapa  ada  yang  mengandung  konten  atau  kasus  yang 
sama.  Hal  ini  dikarenakan  instansi yang  mengelola  situs 
berita  online  terus  menerus  mem-blow  up  berita  yang 
sekiranya  populer  pada  saat  itu.  Maka  dari itu  dilakukan 

3/8 

 
 
 
 
pengecekan terhadap  tiap  berita  untuk melihat  kesamaan 
antar  berita.  Dalam  penelitian ini,  suatu  berita  dikatakan 
serupa  jika  terdapat  hasil  ekstraksi  waktu  dan  lokasi 
kejadian yang sama.  

4.7.  Pemetaan Hasil Ekstraksi Informasi 

Pemetaan  dilakukan  berdasarkan  hasil  ekstraksi 
informasi menggunakan model NER. Pemetaan dilakukan 
untuk melihat  persebaran lokasi  banjir  yang  terjadi pada 
periode  tahun  2017  sampai  dengan  2020.  Pemetaan 
dilakukan menggunakan bantuan library folium yang ada 
pada  bahasa  pemrograman  python.  Sebelum  melakukan 
pemetaan  menggunakan  library  folium  terlebih  dahulu 
menentukan  nilai  longitude  dan  lattitude  masing  masing 
lokasi.  

Pada  penelitian 

V.  KERANGKA PIKIR 
ini  kerangka  pikir  yang  digunakan 
menunjukkan  tahapan  penelitian  secara  umum  yang  dapat 
dilihat  pada  gambar  3.  Penelitian  ini  menggunakan  data 
sekunder yang berupa data artikel berita online yang diperoleh 
dari  situs  web  berita  online  Detik.com  yang  diperoleh 
menggunakan metode web scraping. Data artikel berita online 
yang  diperoleh    kemudian  dilakukan  tahap  preprocessing, 
akan  dilakukan  klasifikasi  berita  banjir 
selanjutnya 
menggunakan  model  SVM.  Kemudian  akan  dilakukan 
ekstraksi informasi  pada dataset  hasil  klasifikasi  berita  banjir 
yang selanjutnya akan dilakukan pemetaan lokasi banjir. 

Gambar 3. Diagram Alir Kerangka Pikir 

VI. HASIL DAN PEMBAHASAN 

A.  Pengumpulan Data 

Dari hasil scraping konten berita online pada situs Detik.com 
tersebut,  didapatkan  data  sebanyak  9267  data  konten  berita 
online.  Data  konten  berita  online  tersebut  selanjutnya  akan 
diubah  kedalam  bentuk  excel  sehingga  memudahkan  proses 
analisis. 
B.  Analisis Data 

1.  Pembangunan Model Klasifikasi 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Penentuan Pembobot Model SVM 
a.  Kernel Linear 

Pada  kernel  linear  digunakan  parameter  C 
pada rentang 10−3 sampai dengan 103 untuk data 
Training. 

TABEL III 
TABEL NILAI AKURASI KLASIFIKASI SVM KERNEL 
LINEAR PADA DATA TRAINING 

Akurasi Klasifikasi (%) 

i

P
r
o
p
o
r
s

C 

60% 
70% 
80% 
90% 

100 
100 
100 
100 
100 

0.1 
76.11 
75.47 
75.81 
76.28 

10 
93.82 
99.49 
99.54 
99.28 

0.001 
55.55 
55.31 
55.42 
55.29 

1000 
1 
100 
86.31 
100 
89.72 
100 
89.24 
100 
88.78 
Berdasarkan Tabel III di atas dapat diketahui 
bahwa  dengan  menggunakan  kernel  linear  untuk 
setiap  word  vector  didapatkan  nilai  ketepatan 
paling  besar  100%  pada  semua  word  vector 
dengan  menggunakan  c  =  100  sampai  c  =  1000 
untuk  setiap  proporsi  data  training  dan  data 
testing.  Kemudian,  parameter  c  =  100  akan 
digunakan  dalam  model  SVM  yang  akan 
dilakukan uji validasi pada data testing. 

b.  Kernel Polinomial 

Selanjutnya  dilakukan  ketepatan  klasifikasi 
pada  kernel  polinomial  dengan  menggunakan 
parameter c pada rentang 10−3 sampai 103. 

TABEL IV 
TABEL NILAI AKURASI KLASIFIKASI SVM KERNEL 
POLINOMIAL PADA DATA TRAINING 

C 

60% 
70% 
80% 
90% 

P
r
o
p
o
r
s
i

Akurasi Klasifikasi (%) 

0.001 
55.72 
55.31 
55.42 
55.29 

0.1 
55.83 
55.37 
55.53 
55.53 

1 
99.58 
99.62 
99.62 
99.52 

10 
100 
100 
99.98 
99.98 

100 
100 
100 
100 
100 

1000 
100 
100 
100 
100 

Berdasarkan  Tabel  IV  tersebut  didapatkan 
parameter  c  untuk  kernel  polinomial  yaitu  c=10 
sampai  1000  untuk  setiap  proporsi  data  training 
dan data testing. Parameter c=100 akan digunakan 
pada  model  SVM  kernel  polinomial  agar  bisa 
dibandingkan dengan model SVM kernel linear. 

c.  Kernel Radial basis function 

Selanjutnya  dilakukan  ketepatan  klasifikasi 
pada  kernel  polinomial  dengan  menggunakan 
parameter c pada rentang 10−3 sampai 103 dengan 
parameter γ=1 

TABEL V 
TABEL NILAI AKURASI KLASIFIKASI SVM KERNEL 
RBF PADA DATA TRAINING 

Akurasi Klasifikasi (%) 

C 

60% 
70% 
80% 
90% 

0.001 
55.72 
55.31 
55.42 
55.29 

0.1 
71.58 
69.99 
72.24 
73.16 

1 
94.45 
96.03 
95.93 
95.51 

10 
100 
100 
100 
99.98 

100 
100 
100 
100 
100 

1000 
100 
100 
100 
100 

P
r
o
p
o
r
s
i

Berdasarkan Tabel V dapat diketahui bahwa 
nilai  terbaik  untuk  parameter  c  adalah  c=100 

4/8 

 
 
 
 
 
 
 
 
sampai  c=1000.  Sehingga  parameter  yang 
digunakan untuk pembentukan model SVM kernel 
RBF adalah c=100 dan γ=1. 

Melayu, Kecamatan Makasar, mengaku banjir 

DKI 

Jakarta, 

tahun  ini  parah  karena  gerak  air  yang  amat 

Cipinang 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Menghitung Nilai Akurasi Model SVM 
Berdasarkan Confusion Matrix masing-masing model 
didapatkan perbandingan nilai akurasi masing-masing 
model. 

TABEL VI 
TABEL PERBANDINGAN NILAI AKURASI MASING-
MASING MODEL 

Kernel 

Proporsi 

Linear 

Polinomial 

RBF 

60% 
70% 
80% 
90% 
60% 
70% 
80% 
90% 
60% 
70% 
80% 
90% 

Akurasi 
(c=100) 
70.1 
70.9 
71.2 
73.0 
69.02 
71.62 
71.3 
72.3 
75.16 
75.42 
75.23 
75.31 

Berdasarkan  Tabel  VI,  disimpulkan  bahwa  model 
SVM  yg 
terbaik  adalah  model  SVM  dengan 
menggunakan  kernel  RBF  dan  proporsi  data  training 
dan  data  testing  yang  digunakan  adalah  70%  dengan 
nilai akurasi model sebesar 75.42%. 

Informasi  kejadian  banjir  pada  hasil 

2.  Ekstraksi 
klasifikasi 
Pada hasil klasifikasi menggunakan model SVM yang 
terbaik  menghasilkan  sebanyak  3456  berita  online 
yang terklasifikasi sebagai berita banjir. 

Mereduksi Berita yang Memiliki Kemiripan 
Pada  beberapa  situs  berita,  terjadi  pemberitaan  yang 
semakin  marak  sehingga  menghasilkan  berita  yang 
beragam  namun  memberikan 
informasi  yang 
sama/mirip untuk konten yang dibahas. Sehingga pada 
penelitian ini akan dilakukan reduksi berita yang sama. 
Metode 
dengan 
digunakan 
membandingkan  hasil  ekstraksi  informasi  masing-
masing  berita.  Dua  buah  berita  dikatakan  sama  jika 
memiliki  informasi  yang  sama,  sehingga  reduksi 
dilakukan  dengan  cara  menghapus  informasi  yang 
duplikat.  Berikut  contoh  berita  yang  memiliki 
informasi yang sama dapat dilihat pada tabel V. 

adalah 

yang 

TABEL V 
TABEL CONTOH BERITA YANG MEMILIKI HASIL 
EKSTRAKSI INFORMASI YANG SAMA 

cepat  masuk  ke  perumahan. 

""Pernah 

Melayu, Makasar 

kebanjiran  2002,  2007.  Parahan  tahun  ini 

Ekstraksi 

soalnya gerak airnya cepet banget. Kan 2002 

tanggal: 

perlahan  tuh  masuk,  sekarang  cepet  banget 

1/1/2020 

(air masuk perumahan),"" kata warga atas nama 

Gilang Pramudhita (23) saat dihubungi, Rabu 

(1/1/2020).  Gilang  menyebut  sekitar  pukul 

05.00  WIB.  Tinggi  air  hingga  pukul  12.30 

WIB mencapai 1 meter.   ""1 meter lebih,"" kata 

Gilang.  Gilang  kini  mengungsi  ke  rumah 

tetangganya yang memiliki 2 lantai. Menurut 

Gilang,  warga  butuh  bantuan.  ""Bantuan  baru 

perahu  karet.  Makanan  belum  ada,""  sebut 

Gilang. 

Banjir  di  Cipinang  Melayu  Jaktim    Hingga 

Ekstraksi lokasi: 

siang  ini,  banjir  masih  merendam  kawasan 

Jakarta, Cipinang 

Cipinang  Melayu, 

Jakarta  Timur.  Ada 

Melayu,  Jakarta 

sepasang suami-istri  meninggal  dunia  karena 

Timur 

terjebak  banjir.  Keduanya  belum  dievakuasi. 

Ekstraksi 

""Korban  di  RW  04  2  orang  suami  istri 

tanggal: 

meninggal  karena  terjebak  banjir.  Lansia, 

1/1/2020 

sekitar umur 75 tahun. Mereka memang sakit 

sebetulnya,""  kata  Ketua  RW  04  Cipinang 

Melayu,  Irwan  Kurniadi,  Rabu  (1/1/2020). 

Selain  sepasang  suami-istri,  ada  seorang 

warga  lain  yang  meninggal  dunia.  Korban 

sudah  dievakuasi.  ""Meninggal  satu  orang, 

Sejumlah  wilayah  di  DKI  Jakarta  masih 

Ekstraksi lokasi: 

umur  sekitar  di  atas  60-an,  udah  tua.  Belum 

terendam 

.  Warga  Pangkalan  Jati,  Jalan 

dipastikan dia meninggal karena banjir, karena 

Harapan  II  Rt  04/010  Kelurahan  Cipinang 

informasi  yang  kita  dapat  dia 

sudah 

5/8 

 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 4. Grafik jumlah kejadian banjir di Jakarta 
berdasarkan konten berita online  
tahun 2017-2020 
Berdasarkan  gambar  4,  jumlah  kejadian  banjir  di 
Provinsi  DKI  Jakarta,  menurut  hasil  ekstraksi 
informasi  pada  berita  online,  meningkat  setiap 
tahunnya  dari  tahun  2018  hingga  2020.  Terjadinya 
peningkatan  jumlah  kejadian  banjir  yang  pesat  pada 
tahun  2020.  Hal  ini  sejalan  dengan  data  BNPB  yang 
mengatakan  bahwa jumlah  kejadian  banjir meningkat 
pesat pada tahun 2020. 

Gambar 5. Grafik jumlah kejadian banjir di Jakarta per 
kabupaten berdasarkan konten berita online  
tahun 2017-2020 

5, 

gambar 

Berdasarkan 
tersebut 
menggambarkan  tentang  jumlah  kejadian  banjir  di 
Provinsi  DKI  Jakarta  berdasarkan  Kota.  Dari  tahun 
2017  hingga  2020,  kota  Jakarta  Selatan  merupakan 
langganan banjir terbanyak setiap tahunnya. 

grafik 

meninggal.  Permintaan  evakuasi  ke  Damkar 

sekitar pukul 4 pagi. Evakuasi yang meninggal 

itu  sekitar  jam  9-an""  kata  Perwira  Piket 

Pemadam Kebakaran Jaktim, Supriyadi. 

Bendung Katulampa dinyatakan siaga 2 seusai 

Ekstraksi lokasi: 

hujan  deras  sejak  kemarin  sore.  meminta 

Jakarta, Cipinang 

warga  yang  berada  di  bantaran  sungai 

Melayu, Makasar 

mengantisipasi  limpahan  air  banjir.  ""Dari 

Ekstraksi 

Depok kita dapat info di sana sudah siaga dua, 

tanggal: 

artinya sekitar jam tiga sore air akan sampai di 

1/1/2020 

Jakarta.  Katulampa  juga  sama,  sudah  masuk 

siaga  dua.  Artinya  jam  6  sore  airnya  akan 

sampai  di  Jakarta,""  kata  Anies  setelah 

meninjau banjir di RW 04, Cipinang Melayu, 

Makasar,  Jaktim,  Rabu  (1/1/2020).  Anies 

mengatakan  pihaknya  sudah  menyiapkan 

tenda  pengungsian.  Kebutuhan  warga  yang 

terkena dampak banjir juga sudah disediakan.   

""Seluruh  warga  yang  tinggal  di  area  sungai 

harus mengantisipasi,  jajaran  kita  sudah  siap 

dengan tenda, tempat pengungsian, kebutuhan 

makanan  dan  lain-lain  untuk  kawasan  yang 

berisiko  mendapat  limpahan  air  banjir  dari 

daerah  pegunungan. 

Insyaallah 

ini  bisa 

terlewati dengan baik nantinya,"" ujar dia. Dari 

informasi  yang  dihimpun,  tinggi  muka  air 

Bendung Katulampa per hari Rabu (1/1/2020) 

pukul 09.10 WIB adalah 1,7 meter. 

Ketiga berita tersebut merupakan berita yang berbeda, 
namun  memiliki  kesamaan  informasi  sehingga  pada 
penelitian ini dianggap sebagai satu berita yang sama.  

Berdasarkan  hasil  ekstraksi  informasi  didapatkan 
beberapa informasi sebagai berikut. 

Gambar 6. Grafik jumlah titik lokasi banjir di Jakarta per 
kabupaten berdasarkan berita online 
tahun 2017-2020 
Kemudian  pada  gambar  6,  menggambarkan  jumlah 
titik lokasi banjir berdasarkan hasil ekstraksi informasi 
pada  berita  online.  Sejalan  dengan  gambar  4,  bahwa 
daerah dengan jumlah titik lokasi banjir terbanyak ada 
pada kota Jakarta Selatan. 

6/8 

 
 
 
 
 
 
Pemetaan pada data hasil ekstraksi informasi kejadian banjir 

(a) 

(b)  

                         (c)    

                                (d) 

Gambar 7. Peta persebaran titik lokasi banjir menurut banyaknya 
kejadian banjir berdasarkan berita online tahun (a) 2017 (b) 2018 (c) 
2019 (d) 2020 

Berdasarkan gambar 7 bisa disimpulkan bahwa pada tahun 
2017, titik lokasi banjir masih berada di sekitaran aliran sungai. 
Kemudian  pada tahun  selanjutnya, titik  lokasi  banjir  semakin 
menyebar  hingga  ke  daerah  sekitarnya.  Hal  ini  mungkin 
dikarenakan  daerah 
tersebut  merupakan  daerah  dengan 
ketinggian permukaan tanah lebih rendah dibandingkan muka 
air  laut.  Kemudian  pada  tahun  2020,  persebaran  titik  lokasi 
banjir  hampir  menyebar  ke  seluruh  daerah  di  Provinsi  DKI 
Jakarta.  Hal  ini  dikarenakan,  terjadinya  curah  hujan  yang 
sangat tinggi atau anomali di DKI Jakarta dari awal pergantian 
tahun 2020. Seperti yang dilansir oleh tirto.id, pada tahun 2020 
Indonesia diawali dengan hujan yang terus mengguyur wilayah 
jabodetabek  sejak  akhir  tahun  2019  dengan  intensitas  yang 
lebat [32]. Dalam konteks banjir Jabodetabek 2020, setidaknya 
ada beberapa faktor. Pertama, minimnya resapan air di selatan 
Jakarta  atau  bagian  hulu.  Faktor  kedua  adalah  drainase  yang 
buruk  di  hilir.  Di  sisi  lain,  Jakarta  hampir  tidak  ada  ruang 
terbuka  biru  atau  tempat  parkir  air  sebelum  dialirkan  ke  laut 
[32]. 

VII. 

PENUTUP 

Berdasarkan  hasil  dan  pembahasan  didapatkan  beberapa 
kesimpulan,  yaitu  diperoleh  model  yang 
terbaik  untuk 
mengklasifikasikan berita banjir berdasarkan berita online yaitu 
model SVM dengan kernel radial basis function dengan akurasi 
sebesar  75.42%.  Kemudian  berdasarkan  hasil  ekstraksi 
informasi  pada  hasil  klasifikasi  didapatkan  bahwa  jumlah 
kejadian banjir di Jakarta meningkat setiap tahunnya dari tahun 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

2018  hingga  2020  dan  terjadi  peningkatan  jumlah  kejadian 
banjir yang pesat pada tahun 2020. 

Kemudian, diharapkan untuk penelitian selanjutnya supaya 
melakukan  kajian  perbandingan  antara  data  hasil  ekstraksi 
informasi,  sebagai  data  kejadian  bencana  banjir,  dengan  data 
BPBD DKI Jakarta secara lebih detail. Diharapkan juga untuk 
melakukan  analisis  ekstraksi  informasi  lokasi  hingga  level 
RT/RW  dan  Jalan  sehingga  titik  lokasi  yang  didapatkan  bisa 
lebih merepresentasikan secara presisi. 

DAFTAR PUSTAKA 

[1]  “Geoportal 

Kebencanaan 
https://gis.bnpb.go.id/ (accessed Mar. 25, 2021). 

Indonesia,” Bnpb.go.id, 

2021. 

[2]  P. Chen and Chun-Yang Zhang, “Data-intensive applications, challenges, 
techniques  and  technologies:  A  survey  on  Big  Data,”  Information 
Sciences. Agustus, 2014. 

[3]  Hall, Jim. 2001. Online journalism: a critical primer. Pluto Press, 
[4]  Beritatagar.id.  (2019).  Pembaca  berita  daring  meningkat  tapi  belum 
dari 

merata. 
https://beritagar.id/artikel/berita/pembaca-berita-daring-meningkat-tapi-
belum-merata. 

September 

Dipetik 

2020, 

[5]  Arifin,  Pupung.  2013.  Persaingan  Tujuh  Portal  Berita  Online  Indonesia 
berdasarkan  Analisis  Uses  and  Gratifi  cations.  Jurnal  Ilmu  Komunikasi. 
Volume 10, Nomor 2, Desember 2013: 195-212. 

[6]  Sumathi,  S.  2006.  Introduction  to  Data  Mining  and  Its  Applications. 

Germany: Springer Verlag berlin Heidelberg 

[7]  E.  Susanti  and  Khabib  Mustofa,  “Ekstraksi  Informasi  Halaman  Web 
pada  Ontology-Based 

Menggunakan  Pendekatan  Bootstrapping 
Information...,” , Jul. 2015.  

[8]  J, Han (2012). Data Mining: Data Mining concepts and techniques. Morgan 

Kaufmann 

[9]  Yudi  Wibisono,  “Ekstraksi  Informasi:  Named  Entity  Recognition”,  Feb. 

07, 2012 

[10] Yasir  Abdur  Rohman,  “Pengenalan  NumPy,  Pandas,  Matplotlib  -  Yasir 

Abdur Rohman - Medium,” Medium, Dec. 08, 2019. 

[11] Matiin Laugiwa Prawira Putra, “Menghitung Akurasi Dengan  Confusion 
Matrix - Matiin Laugiwa Prawira Putra -Medium,” Medium, Jul. 15, 2020. 
[12] Sari, Khrisna Dini Yunita. 2006. Text Categorization with Support Vector 

Machine. Teknik Informatika: Telkom University. 

[13] R. K. Abdullah and E. Utami, “Studi Komparasi Metode SVM dan Naive 
Indonesia,” JURNAL 

pada  Data  Bencana  Banjir 

di 

Bayes 
TECNOSCIENZA, 2018 

[14] J.  Hartmann,  J.  Huppertz,  C.  Schamp,  and  M.  Heitmann,  “Comparing 
automated text classification methods,” International Journal of Research 
in Marketing, Mar. 2019 

[15] L.  B.  Ilmawan  and  M.  A.  Mude,  “Perbandingan  Metode  Klasifikasi 
Support Vector Machine dan Naïve Bayes untuk Analisis Sentimen pada 
Ulasan Tekstual di Google Play Store,” ILKOM Jurnal Ilmiah, Aug. 2020 
[16] T.  Mardiana,  H.  Syahreva,  and  T.  Tuslaela,  “Komparasi  Metode 
Klasifikasi  Pada  Analisis  Sentimen  Usaha  Waralaba  Berdasarkan  Data 
Twitter,” Jurnal Pilar Nusa Mandiri, Sep. 2019 

[17] R.  Kusumawati,  A  D’arofah,  and  P.  A.  Pramana,  “Comparison 
Performance  of  Naive  Bayes  Classifier  and  Support  Vector  Machine 
Algorithm for Twitter’s Classification of Tokopedia Services”, Jurnal of 
Physics, Oct. 2019. 

[18] F.  Fanny  and  Y.  Muliono  and  F.  Tanzil,  “A  Comparison  of  Text 
Classification Methods k-NN, Naïve Bayes, and Support Vector Machine 
for  News  Classification,” Jurnal  Informatika:  Jurnal  Pengembangan  IT, 
May 2018 

[19] Siti  Nur  Asiyah  and  Kartika  Fithriasari,  “Klasifikasi  Berita  Online 
Menggunakan  Metode  Support  Vector  Machine  Dan  K-Nearest 
Neighbor,” Jurnal Sains dan Seni ITS, 2016 

[20] S.  Kim  and  J.  Choi,  “An  SVM-based  high-quality  article  classifier  for 

systematic reviews,” Journal of Biomedical Informatics, Feb. 2014 

[21] Neneng Rachmalia Feta and Asep Rahmat Ginanjar, “Komparasi Fungsi 
Kernel  Metode  Support  Vector  Machine  Untuk  Pemodelan  Klasifikasi 
Terhadap  Penyakit  Tanaman  Kedelai”:, BRITech,  Jurnal  Ilmiah  Ilmu 
Komputer, Sains dan Teknologi Terapan, 2019 

7/8 

 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

[22] Soumick  Chatterjee,  Pramod  George  Jose,  and  Debabrata  Datta,  “Text 
Classification  Using  SVM  Enhanced  by  Multithreading 
and 
CUDA,” ResearchGate, Jan. 08, 2019.  

[23] E. Leopold and J. Kindermann, Machine Learning, 2002 
[24] F.  R.  Lumbanraja,  E.  Fitri,  Ardiansyah,  A.  Junaidi,  and  R.  Prabowo, 
“Abstract Classification Using Support Vector Machine Algorithm (Case 
Study:  Abstract  in  a  Computer  Science  Journal),” Journal  of  Physics: 
Conference Series, Jan. 2021 

[25] O. Somantri and D. Apriliani, “Support Vector Machine Berbasis Feature 
Selection  Untuk  Sentiment  Analysis  Kepuasan  Pelanggan  Terhadap 
Pelayanan  Warung  dan  Restoran  Kuliner  Kota  Tegal,” Jurnal  Teknologi 
Informasi dan Ilmu Komputer, Oct. 2018 

[26] P. H.  Prastyo, A.  S.  Sumi, A.  W. Dian, and A. E.  Permanasari, “Tweets 
Responding  to  the  Indonesian  Government’s  Handling  of  COVID-19: 
Sentiment Analysis Using SVM with Normalized Poly Kernel,” Journal of 
Information Systems Engineering and Business Intelligence, Oct. 2020 
[27] A. S. Wibawa and A. Purwarianti, “Indonesian Named-entity Recognition 
for 15 Classes Using Ensemble Supervised Learning,” Procedia Computer 
Science, 2016 

[28] Siti  Mariyah,  “Named  Entity  Recognition  on  A  Collection  of  Research 
Titles,” Jurnal Aplikasi Statistika & Komputasi Statistik, June 2017 
[29] Jayendra Barua and Rajdeep Niyogi, “Improving named entity recognition 
and disambiguation in news headlines,” Jurnal Intelligent Information and 
Database Systems, 2019. 

[30] N. Perera, M. Dehmer, and F. Emmert-Streib, “Named Entity Recognition 
and Relation Detection for Biomedical Information Extraction,” Frontiers 
in Cell and Developmental Biology, Aug. 2020 

[31] R. Rafael, V. Lúcia, and C. Xavier, “A systematic review of named entity 
recognition  in  biomedical  texts,”  Journal  of  the  Brazillian  Society,  Jun. 
2011. 

[32] Restu  Diantina  Putri,  Mawa  Kresna,  and  Restu  Diantina  Putri, 
“Mengungkap Musabab Banjir Besar Jakarta 2020,” tirto.id, Jan. 07, 2020. 
https://tirto.id/mengungkap-musabab-banjir-besar-jakarta-2020-eq85 
(accessed Jun. 11, 2021). 

8/8 

 
"
221709507,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pengembangan Sistem Survei Kebutuhan Data BPS 
Berbasis Web : Modul Entri Data 

Ahmad Fauzi Fahmi (221709507, 4SI1) 
Dosen Pembimbing: Firdaus, M.B.A 

Ringkasan— Survei Kebutuhan Data (SKD) adalah salah satu 
kegiatan  di  BPS  (Badan  Pusat  Statistik)  yang  bertujuan  untuk 
melakukan  evaluasi  dalam  melakukan  pelayanan  data.  SKD 
dilaksanakan  oleh  Direktorat  Diseminasi  Statistik  melalui 
Subdirektorat  Rujukan  Statistik  sejak  tahun 2005.  Tahun 2019, 
BPS  melakukan  SKD  menggunakan  metode  CAWI  (Computer 
Aided  Web  Interviewing)  dengan  aplikasi  SKD  berbasis  web. 
Aplikasi  yang  digunakan  masih  terdapat  kekurangan  yang 
menjadi masalah dalam pelaksanaan SKD. 

Dalam penelitian ini, bertujuan untuk mengatasi kekurangan 
dalam  aplikasi  web  SKD.  Pengembangan  dilakukan  dengan 
menggunakan  metode  SDLC  (System  Development  Lyfe  Cycle) 
model iterative. Sistem dikembangkan dengan berbasis web dengan 
menggunakan  bahasa  PHP  dan  Javascript  sebagai  bahasa 
pemrograman,  Yii  versi  2  sebagai  framework  backend,  Bootstrap 
versi  4  sebagai  framework  frontend  dan  MySQL  sebagai  DBMS 
(Database Management System). Evaluasi pengembangan aplikasi 
SKD  dilakukan  dengan  menggunakan  metode  blackbox  testing 
dan SUS. Hasil pengujian dengan menggunakan blackbox testing 
telah  berfungsi  dengan  baik  sesuai  keinginan  dan  hasil  SUS 
menyatakan  bahwa  aplikasi  SKD  dapat  dikategorikan  baik 
dengan nilai 74,42.  

Kata Kunci— Pengembangan, SKD, CAWI, Entri Data. 

I.  LATAR BELAKANG 

Badan  Pusat  Statistik  (BPS)  adalah  lembaga  pemerintah 
nonkementerian  yang  bertanggung  jawab  langsung  kepada 
presiden 
[3].  BPS  mempunyai  peranan  salah  satunya 
menyediakan  kebutuhan  data  dan  informasi  bagi  pemerintah 
dan masyarakat. Maka diperlukan penilaian terhadap data dan 
pelayanan  yang  diberikan  oleh  BPS.  Penilaian  inilah  yang 
nantinya  akan  menjadi  evaluasi  terhadap  pelayanan  dalam 
penyediaan  data  dan 
informasi  yang  dibutuhkan  bagi 
masyarakat dan juga pemerintah. 

Survei  Kebutuhan  Data  (SKD)  adalah  survei  yang 
diselenggarakan  oleh  Badan  Pusat  Statistik  (BPS)  untuk 
mengidentifikasi kebutuhan data statistik dan tingkat kepuasan 
konsumen  terhadap  data  dan  pelayanan  BPS  [2].  SKD 
dilaksanakan  oleh  Direktorat  Diseminasi  Statistik  melalui 
Subdirektorat  Rujukan  Statistik  sejak  tahun  2005  [2].  SKD 
dilaksanakan  pertama  kali  pada  tahun  2005,  namun  hanya 
dilakukan  di  BPS  RI.  Sejak  tahun  2014,  pelaksanaan  SKD 
dikembangkan hingga ke tingkat kabupaten/kota. 

Pada 

tahun  2019,  metode  pengumpulan  data  SKD 
dikembangkan  melalui  metode  CAWI  (Computer  Aided  Web 
Interviewing)  yang  sebelumnya  menggunakan  metode  PAPI 
(Pencil  and  Paper  Interviewing).  Dengan  metode  CAWI, 
responden  melakukan  self  enumeration  atau  mengisi  sendiri 
kuesioner  SKD  melalui  aplikasi  yang  berbasis  website. 
Perekaman  data  SKD  tidak  hanya  dilakukan  oleh  responden 

secara  mandiri,  tetapi  ada  juga  yang  dilakukan  dengan  cara 
wawancara  langsung  oleh  petugas  BPS.  Responden  yang 
mengisi  kuesioner  secara  self  enumeration  dibagi  menjadi  3 
jenis, yaitu responden yang mengisi kuesioner melalui tautan 
URL  (Uniform  Resource  Locator)  yang  dikirimkan  melalui 
email  pribadi,  responden  yang  mengisi  kuesioner  melalui 
tautan  URL  yang  tercantum  pada  aplikasi-aplikasi  BPS,  dan 
responden  yang  mengisi  kuesioner  melalui  komputer  yang 
terdapat di PST BPS. 

Subdirektorat Rujukan Statistik sebagai penanggung jawab 
SKD mengungkapkan bahwa masih terdapat kekurangan dalam 
aplikasi SKD yang dalam pelaksanaannya menyebabkan proses 
persiapan menjadi lebih lama. Kekurangan tersebut antara lain, 
dari sisi basis data aplikasi yang hanya mampu menyimpan data 
SKD  untuk  satu  periode  pelaksanaan  tertentu.  Data  SKD 
periode  sebelumnya  akan  diekspor  dari  basis  data  kemudian 
mengosongkan  tabel  pada  basis  data  untuk  dapat  digunakan 
kembali  diperiode  berikutnya.  Hal  ini  beresiko  menyebabkan 
kehilangan data dalam proses ekspor atau penyimpanan data di 
luar basis data. Kekurangan lain adalah halaman web aplikasi 
SKD yang digunakan untuk pengisian kuesioner dibuat secara 
terpisah, sehingga terdapat 4 halaman web yang terdiri dari 3 
halaman web sesuai dengan 3 jenis responden yang melakukan 
pengisian  kuesioner  secara  self  enumeration  dan  juga  1 
halaman  web  pengisian  kuesioner  secara  manual  yang 
dilakukan oleh pegawai BPS. Jika dilakukan perubahan source 
code pada satu halaman web yang digunakan untuk pengisian 
kuesioner,  maka  hal  yang  sama  harus  dilakukan  perubahan 
pada  keempat  halaman  web  tersebut.  Selain  itu,  BPS  juga 
merencanakan untuk melaksanakan SKD secara triwulan sekali. 
Sehingga diperlukan fitur untuk mendokumentasikan kegiatan 
survei  pada  periode  sebelumnya.  Oleh  karena  itu,  perlu 
dikembangkannya aplikasi SKD berbasis web untuk mengatasi 
masalah  atau  kekurangan  yang  ada  agar  aplikasi  SKD  dapat 
digunakan secara efektif dan efisien. Pada penelitian ini akan 
dilakukan pengembangan web SKD untuk mengatasi masalah-
masalah  tersebut,  sehingga  kegiatan  pengumpulan  data  SKD 
dapat berjalan dengan baik dan lebih efisien.  

II.  TUJUAN PENELITIAN 

Berdasarkan latar belakang, tujuan dilakukannya penelitian 

ini adalah sebagai berikut. 
1.  Melakukan pengembangan sistem basis data aplikasi SKD 
yang dapat mendukung pelaksanaan SKD secara multi time 
sehingga tidak perlu melakukan perubahan basis data untuk 
melakukan SKD periode berikutnya. 

2.  Melakukan  pengembangan  sistem  aplikasi  SKD  dengan 
menjadikan  form  input  secara  dinamis  sehingga  dapat 

 1 / 8 

 
 
 
 
meminimalkan perubahan dan  mengefisienkan  waktu  jika 
terjadi perubahan. 

3.  Melakukan pengembangan sistem aplikasi SKD yang dapat 
mendokumentasikan 
sehingga 
kegiatan 
meminimalkan  perubahan  pada  source  code  aplikasi 
ataupun basis datanya. 

survei 

4.  Melakukan uji coba dan evaluasi terhadap sistem yang telah 
dikembangkan  sehingga  dapat  berfungsi  dan  memenuhi 
kebutuhan.  

III. PENELITIAN TERKAIT 

Penelitian  ini  menggunakan  tiga  penelitian  terkait  yang 
digunakan  sebagai  referensi.  Ketiga  penelitian 
tersebut 
merupakan  hasil  skripsi  dari  mahasiswa  Politeknik  Statistika 
STIS.  Alasan  menggunakan  penelitian  tersebut  karena  ketiga 
penelitian  tersebut  memberikan  inspirasi  terkait  proses  entri 
data dalam aplikasi berbasis web serta penggunaan framework 
PHP untuk pembangunan aplikasi. 

Penelitian terkait yang pertama berjudul “Sistem Informasi 
Pengolahan  Data  Survei  Ubinan  Berbasis  Web”  yang 
merupakan hasil skripsi Sri Atikah tahun 2017. Penelitian ini 
menghasilkan  sistem  informasi  yang  meliputi  entri  data, 
validasi,  evaluasi  dan  monitoring  hasil  survei  ubinan.  Sistem 
informasi  tersebut  digunakan  untuk  survei  ubinan  di  wilayah 
kota  dan  kabupaten  yang  dilakukan  oleh  pegawai  BPS 
kabupaten/kota.  Penelitian  ini  menggunakan  framework  Yii 
sebagai  framework  PHP,  Microsoft  SQL  Server  2008  R2 
sebagai  DBMS,  serta  black-box  testing  dan  SUS  (System 
Usability Scale) sebagai bahan evaluasi terhadap sistem yang 
sudah dibuat [1]. 

Penelitian terkait yang kedua adalah milik Nur Ilmi Hasbah 
yang merupakan hasil skripsi tahun 2017 dengan judul “Sistem 
Informasi  Entri  Dan  Rekonsiliasi  Data  Produk  Domestik 
Regional  Bruto  Triwulanan  Provinsi  (Studi  Kasus  :  Subdit 
Konsolidasi  Neraca  Produksi  Regional).  Penelitian 
ini 
menghasilkan sistem informasi yang meliputi proses entri data, 
monitoring  dan  rekonsiliasi  data.  Sistem  informasi  ini  dalam 
pengumpulan  dan  entri  data  PDRB  Triwulan  yang  dilakukan 
BPS Provinsi. Penelitian ini menggunakan framework Yii versi 
1.1.14 sebagai framework PHP, MySQL sebagai DBMS, serta 
black-box  testing  dan  SUS  sebagai  bahan  evaluasi  terhadap 
sistem yang sudah dibuat [4]. 

Penelitian  terkait  yang  ketiga  berjudul  “Pengembangan 
Sistem  Entri  Data  Survei  Perusahaan  Konstruksi  Triwulanan 
(SKTR)  Berbasis  Web”  yang  merupakan  hasil  skripsi  Ulfia 
Aisyara Sinurat tahun 2016. Penelitian ini menghasilkan sistem 
informasi proses entri data untuk Survei Perusahaan Konstruksi 
Triwulanan (SKTR) yang lebih cepat dan dan mempermudah 
proses  pengolahan  data.  Penelitian 
ini  menggunakan 
framework  CodeIgniter  versi  2.2.0  sebagai  framework  PHP, 
MySQL  sebagai  DBMS,  serta  black-box  testing  dan  SUS 
sebagai bahan evaluasi terhadap sistem yang sudah dibuat [5]. 

IV. METODE PENELITIAN  

A. Metode Pengumpulan Data 

Metode  pengumpulan  data  yang  digunakan  sebagai  dasar 

pengembangan sistem adalah sebagai berikut : 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

1.  Studi Pustaka, dilakukan dengan membaca dan memahami 
informasi  dari  berbagai  sumber  untuk  digunakan  sebagai 
landasan teori dan panduan dalam mengembangkan sistem. 
Sumber ini berasal dari buku cetak dan elektronik, artikel, 
dan publikasi. 

2.  Wawancara, dilakukan dengan cara mengajukan pertanyaan 
terkait  masalah-masalah  yang  berhubungan  dengan 
pengembangan  sistem  seperti  batasan  pengembangan 
sistem  informasi,  kebutuhan  sistem  informasi,  prosedur 
sistem  atau  sistem  berjalan,  dan  perancangan  sistem. 
Wawancara 
pegawai 
Subdirektorat  Rujukan  Statistik  BPS  RI  selaku  subject 
matter yang dianggap paham terkait pengembangan sistem. 
3.  Kuesioner, terdiri dari beberapa pertanyaan yang diajukan 
kepada 
ini,  kuesioner 
digunakan  untuk  menguji  sistem  informasi  yang  telah 
dikembangkan yaitu dengan kuesioner SUS. 

responden.  Dalam  penelitian 

dilakukan 

beberapa 

kepada 

B. Metode Pengembangan Sistem 

Pengembangan  sistem  ini  dilakukan  dengan  menggunakan 
metode  SDLC  (System  Development  Lyfe  Cycle)  yang 
dikombinasikan  dengan  model  iterative.  Penggunaan  metode 
SDLC  iterative  pada  dasarnya  sama  dengan  metode  SDLC 
waterfall,  hanya  saja  dilakukan  secara  berulang.  Perulangan 
tahapan pada metode SDLC dimaksudkan untuk memperbaiki 
fitur  yang  terdapat  kesalahan  atau  terdapat  fitur  baru  yang 
menambah  fungsionalitas  sistem  [6].  Berikut  merupakan 
tahapan SDLC. 
1.  Perencanaan sistem, merupakan tahap menentukan batasan, 
tujuan  dan  jadwal.  Pada  tahapan  ini  peneliti  menentukan 
tujuan  untuk  melakukan  perbaikan  pada  masalah  atau 
kekurangan yang terdapat pada CAWI SKD. Jadwal untuk 
melakukan  penelitian  dari  bulan  November  2020  hingga 
Juni 2021. 

2.  Analisis 

sistem,  merupakan 

tahap  menganalisis 
permasalahan  sistem  saat  ini  dan  kebutuhan  sistem  yang 
diusulkan.  Pada  tahapan  ini  peneliti  melakukan  analisis 
untuk menemukan kekurangan dan kelebihan pada sistem 
yang  sudah  ada,  sehingga  selanjutnya  dapat  dirancang 
sistem untuk mengatasi kekurangan yang ada. 

3.  Perancangan  sistem,  merupakan  tahapan  perancangan 
sistem, basis data, dan antar muka. Pada tahapan ini peneliti 
akan  berfokus  pada  perancangan  sistem  dan  basis  data. 
Sedangkan  untuk  rancangan  antar  muka,  peneliti  akan 
menggunakan tampilan default yang sudah disediakan pada 
framework Yii. 

4.  Implementasi  sistem,  merupakan  realisasi  dari  tahapan 
perancangan  sistem  dengan  mengembangkan  sistem 
informasi yang sudah tersedia. Pada bagian ini peneliti akan 
melakukan  pengeditan  pada  source  code  aplikasi  SKD 
dengan  motode  MVC  (Model  View  Controller)  dari 
framework Yii. 

5.  Testing, merupakan tahap untuk menguji fungsi atau fitur 
yang  dikembangkan.  Testing  dilakukan  dengan  metode 
black-box  testing.  Black-box  testing  merupakan  metode 
pengujian dari sisi developer untuk menguji fungsi dan fitur 
sistem yang dikembangkan. 

 2 / 8 

 
 
 
 
C.  Metode Analisis 
1.  Analisis Sistem Berjalan 

Analisis  sistem  berjalan  ini  merupakan  kegiatan  yang 
digunakan  untuk  mengumpulkan  informasi-informasi  yang 
berkaitan  dengan  sistem  yang  sudah  berjalan  atau  sudah 
digunakan.  Dari  analisis  ini  dapat  diketahui  permasalahan-
permasalahan terdapat di suatu bagian. Aplikasi SKD berbasis 
web merupakan aplikasi yang sudah ada sebelumnya. Analisis 
sistem berjalan SKD didapatkan dari hasil wawancara dengan 
pegawai  BPS  di  Subdirektorat  Rujukan  Statistik  dan  SOP 
(Standar  Operating  Procedure)  kegiatan  SKD.  SOP  yang 
digunakan  dalam  kegiatan  SKD  merupakan  SOP  yang 
ditetapkan  pada  tahun  2019  dan  mulai  diterapkan  pada  SKD 
2019. Berikut merupakan gambar yang menunjukkan analisis 
sistem berjalan sesuai dengan SOP yang diterapkan. 

Gambar 1. Analisis Sistem Berjalan 

2.  Analisis Masalah 

Permasalahan atau kekurangan yang ada pada sistem dapat 
dianalisis menggunakan Ishikawa diagram. Ishikawa diagram 
atau  lebih  populer disebut  Fishbone  diagram  merupakan  alat 
yang dapat mengidentifikasi, mengeksplorasi dan menganalisis 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

penyebab dan dampak (causes and effect) masalah suatu sistem. 
Fishbone  diagram  terdiri  atas  bagian  tulang  ikan  yang 
menunjukkan sub masalah atau penyebab masalah dan bagian 
kepala 
ikan  yang  menunjukkan  masalah  yang  menjadi 
perhatian utama. Berikut merupakan gambar fishbone diagram 
dari permasalahan penelitian. 

Gambar 2. Analisis Permasalahan Sistem 

3.  Analisis Kebutuhan 

Berdasarkan  analisis  sistem  berjalan  dan  analisis  masalah 
diperoleh  beberapa  kebutuhan  yang  menjadi  solusi  dalam 
mengatasi masalah tersebut. Solusi masalah ditampilkan dalam 
analisis  kebutuhan  dengan  tabel  PIECES  (Performance, 
Information,  Economic  cost,  Control,  Eficiency,  Service). 
Berikut  merupakan  tabel  PIECES  yang  memuat  analisis 
kebutuhan yang merupakan solusi dari masalah yang dihadapi. 

TABEL I 
TABEL PIECES 

Jenis 

Masalah 

Solusi 

Performance 

Performance 

Perubahan data terkait 
kuesioner online SKD 
Online dilakukan secara 
manual di basis data 
Rincian kuesioner online 
SKD yang ditampilkan 
secara statis dan 
perubahan dilakukan 
secara hardcoding 

Membuat fitur untuk 
melakukan perubahan data 
tersebut 

Rincian ditampilkan secara 
dinamis dan membuat fitur 
untuk mengontrol 
perubahan rincian 
kuesioner online SKD 

Information 

- 

- 

Economic 
Cost 

Basis data yang ada belum 
mampu 
mendokumentasikan SKD 
periode sebelumnya 

Modifikasi basis data agar 
mampu 
mendokumentasikan SKD 
periode sebelumnya 

Control 

- 

- 

Eficiency 

Eficiency 

Service 

Rincian ""Jenis Layanan"" 
untuk tipe responden 
Online Email dientri oleh 
responden 
Form kuesioner online 
dibuat secara terpisah 
berdasarkan 3 jenis 
responden online 
Informasi data responden 
online dipisahkan menjadi 
fitur yang berbeda 
berdasarkan 3 tipe 
responden online 

Rincian ""Jenis Layanan"" 
untuk tipe responden 
online email dapat dientri 
oleh pegawai bps 
Form kuesioner online 
untuk responden online 
disatukan agar 
memudahkan perubahan 

Informasi responden 
Online dapat disatukan 
dalam satu halaman web 

 3 / 8 

 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

menyesuaikan  rincian  pertanyaan  kuesioner.  Jumlah  rincian 
kuesioner pelaksanaan SKD setiap periode bisa berbeda, oleh 
karena itu pengembangan CAWI dilakukan secara manual atau 
hardcoding. 

V.  KERANGKA PIKIR 

Kerangka pikir pada penelitian ini terdiri atas 6 bagian, yaitu 
bagian permasalahan yang menjadi alasan utama dilakukannya 
penelitian  ini,  bagian  peluang  yang  akan  membantu  dalam 
mengatasi  permasalahan  yang  dihadapi,  bagian  solusi  yang 
merupakan 
solusi  permasalahan  dengan  dilakukannya 
penelitian  ini,  bagian  pendekatan  desain  sistem  yang  akan 
membantu peneliti dalam membangun sistem yang dibutuhkan 
untuk solusi, bagian evaluasi sistem yang akan menilai sistem 
yang telah dibuat sudah memenuhi kebutuhan atau belum dan 
bagian hasil yaitu hasil yang diharapkan dari penelitian ini. 

Gambar 3. Kerangka Pikir 

VI. HASIL DAN PEMBAHASAN 

A. Bisnis Proses Sistem Usulan 

Bisnis  proses  sistem  usulan  merupakan  solusi  yang 
ditawarkan dari  analisis  sistem  berjalan.  Bisnis  proses  sistem 
usulan  dapat  dilihat  pada  gambar  5  dibawah.  Secara  sekilas, 
bisnis proses usulan tidak berbeda jauh dengan analisis sistem 
berjalan, hanya terdapat sedikit perbedaan pada pembangunan. 
Pada  analisis  sistem  berjalan  bagian  pembangunan  aplikasi 
dilakukan proses pengembangan basis data dan pengembangan 
CAWI SKD. Hal ini disebabkan karena basis data dan CAWI 
SKD  yang  belum  memenuhi  kebutuhan  SKD  secara 
keseluruhan.  Sehingga  setiap  periode  pelaksanaan  SKD 
sebelum  dimulainya  pencacahan  SKD,  akan  dilakukan 
pengembangan  basis  data  dan  CAWI  SKD.  Pengembangan 
basis  data  yang  dilakukan  adalah  mengekspor  data  SKD 
periode sebelumnya kemudian mengosongkan kembali entitas 
(tabel)  yang  berisi  data  responden  untuk  digunakan  kembali. 
untuk 
Sedangkan 

pengembangan  CAWI 

dilakukan 

Gambar 4. Rancangan Arsitektur Sistem Usulan 

B. Use Case Diagram 

Dalam aplikasi CAWI SKD terdapat 5 aktor yang berperan 
dan berinteraksi dengan CAWI SKD, antara lain Super Admin, 
Admin,  Pengentri  dan  Responden.  Super  admin  merupakan 
aktor  yang  memiliki  tugas  utama  melakukan  penyesuaian 
terhadap  kuesioner  online  SKD  pada  tahun  berjalan,  admin 
adalah  aktor  yang  memiliki  tugas  utama  untuk  melakukan 
persiapan  pencacahan  SKD, pengentri merupakan  aktor  yang 
memiliki  tugas  utama  untuk  mengentri  data  responden  dari 
harcopy atau kuesioner kertas ke dalam aplikasi web SKD, dan 
responden  adalah  aktor  yang  memiliki  tugas  untuk  mengisi 

 4 / 8 

 
 
 
 
 
 
kuesioner  SKD  dengan  menggunakan  aplikasi  web  SKD 
melalui perangkat yang dimiliki. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 5. Use Case Diagram 

C. Activity Diagram 

Activity diagram merupakan diagram yang menggambarkan 
secara  grafis  aliran  business  process,  yang  juga  merupakan 
tahapan-tahapan  dari  use  case  atau  logika  dari  tingkah  laku 
(behavior) dari suatu objek [9]. 

Gambar 6. (Kiri) Activity Diagram Lihat Detail Responden Online Email, 
(Tengah) Activity Diagram Copy Link Online Link, dan (Kanan) Activity 
Diagram Lihat Detail Data Responden Terverifikasi. 

Gambar 7. (Kiri) Activity Diagram Responden Mengisi Kuesioner Online 
SKD dan (Kanan) Activity Diagram Verifikasi Data Responden. 

 5 / 8 

 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 10. Tampilan antar muka form tambah satu responden online email. 

Gambar 11. Tampilan antar muka form import responden online email. 

Gambar 8. (Kiri) Activity Diagram Lihat Detail Data Responden Belum 
Terverifikasi, (Tengah) Activity Diagram Tambah Satu Responden Online 
Emai, dan (Kanan) Activity Diagram Import Responden Online Email. 

D. Implementasi Antar Muka 

Berikut  merupakan  antar  muka  yang  diimplementasikan 

dalam sistem. 

Gambar 12. Tampilan antar muka ketika memilih template pada form tambah 
responden online email dan import responden online email. 

Gambar 9. Tampilan antar muka menu ‘Responden Online’. 

Gambar 13. Tampilan antar muka lihat detail data email yang dikirimkan ke 
responden yang berisi link akses kuesioner online SKD. 

 6 / 8 

 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 14. Tampilan antar muka lihat detail data responden yang sudah 
mengisi kuesioner online SKD. 

Gambar 18. Tampilan antar muka daftar master jenis data. 

Gambar 15. Tampilan antar muka verifikasi data responden. 

Gambar 19. Tampilan antar muka jika responden mengisi ‘Lainnya’ pada 
Blok 3 rincian ‘Jenis Data’. 

Gambar 16. Tampilan antar muka kuesioner online SKD. 

Gambar 20. Tampilan antar muka untuk merubah isian ‘Lainnya’ pada Blok 3 
rincian ‘Jenis Data’ 

Gambar 17. Tampilan antar muka kuesioner online SKD input manual (yang 
diinputkan oleh staff BPS) 

E. Pengujian dan Evaluasi Sistem 
1.  Blackbox Testing 

Blackbox Testing adalah pengujian sistem yang dilakukan 
dari  sisi  fungsional  atau  kebutuhan  spesifikasi  tanpa  menilai 
kualitas  source  code  dan  desain  tampilan.  Pengujian  ini 
dilakukan  berdasarkan  informasi  yang  sudah  dikumpulkan. 
Blackbox testing tidak hanya berfokus pada mekanisme internal 
suatu sistem, tetapi harus dicermati juga output dari input yang 
dimasukkan dan berbagai kondisi yang mungkin terjadi [7]. 

Dalam  penelitian  ini,  Blackbox  testing  dilakukan  oleh 
pegawai  Subdirektorat  Rujukan  Statistik  BPS.  Hasil  yang 
diperoleh  adalah  fungsi  yang  dikembangkan  dalam  aplikasi 
SKD sudah dapat digunakan dan berjalan sesuai dengan yang 
diharapkan. 

 7 / 8 

 
 
 
 
 
 
 
 
 
2.  System Usability Scale (SUS) 

TABEL II 
TABEL HASIL PENGUJIAN SYSTEM USABILITY SCALE (SUS) 

Responden  1 

2 

3 

4 

5 

6 

7 

8 

9 

10 

Pertanyaan SUS 

Jumlah 
x 2,5 

1 

2 

3 

4 

5 

6 

7 

8 

9 

10 

11 

12 

13 

7,5 

7,5 

10 

10 

10 

10 

10 

10 

10 

10 

10  7,5 

97,5 

7,5  7,5  7,5  7,5  7,5  7,5  7,5  7,5  7,5  2,5 

10  7,5 

10  2,5  7,5  2,5  7,5  2,5  7,5  2,5 

10 

10 

10  2,5  7,5 

10  7,5  7,5 

10 

5 

10  7,5  7,5  7,5 

5  7,5  7,5  7,5  7,5  7,5 

0  7,5  7,5  7,5 

10  2,5  7,5  7,5  2,5 

70 

60 

80 

75 

60 

5  7,5 

5  7,5  2,5  7,5  7,5 

5  2,5 

57,5 

7,5  7,5 

5 

10 

5 

5  7,5  7,5  7,5  7,5 

70 

7,5  7,5 

5  7,5 

5  7,5  7,5  7,5  7,5 

5 

67,5 

10 

10 

10 

10 

10  2,5  7,5  7,5 

10 

10 

87,5 

10 

10 

10  7,5 

10  7,5  7,5  7,5 

10 

10 

90 

10  7,5 

10  7,5 

10 

10 

10  7,5 

0 

10 

82,5 

10  7,5  7,5  7,5  7,5 

5  7,5  7,5  7,5  2,5 

70 

Min 

Max 

7,5 

0 

5  2,5 

5  2,5  2,5  2,5 

0  2,5 

57,5 

10 

10 

10 

10 

10 

10 

10 

10 

10 

10 

97,5 

Average 

9,04  7,50  8,27  7,12  7,69  6,73  7,50  7,31  7,50  5,77 

74,42 

SUS  dilakukan  dengan  memberikan  pertanyaan  evaluasi 
sistem kepada pegawai BPS sebagai pengguna aplikasi SKD. 
Responden  SUS  ada  13  orang.  Berdasarkan  SUS  yang  telah 
dilakukan  diperoleh  nilai  terendah  yang  diberikan  responden 
adalah 57,5 dan nilai tertinggi yang diberikan responden adalah 
97,5.  Nilai  rata-rata  terendah  setiap  pertanyaan  SUS  yang 
diberikan yaitu 5,77 yang terdapat pada pertanyaan nomor 10 
mengenai  perlu  membiasakan  diri  sebelum  menggunakan 
sistem ini. Sedangkan nilai rata-rata  tertinggi dari pertanyaan 
SUS  yang  diberikan  yaitu  9,04  yang  terdapat  pada  nomor  1 
mengenai tertariknya responden untuk menggunakan sistem ini 
kembali. Selain itu terdapat 2 responden yang memberikan nilai 
0 pada pertanyaan 2 dan 9, yaitu 2 responden tersebut merasa 
sistem ini rumit dan mengalami hambatan dalam menggunakan 
sistem ini. Nilai rata-rata SUS 74,42 sudah termasuk kategori 
baik  atau  grade  B  berdasarkan  skala  yang  dikemukakan oleh 
Aaron  Bangor,  Philip  Kortum,  dan  James  Miller  dalam 
jurnalnya  yang  berjudul  ‘Determining  What  Individual  SUS 
Scores Mean: Adding an Adjective Rating Scale’ [8]. 

VII. 

PENUTUP 

A. Kesimpulan 

Berdasarkan penelitian yang dilakukan, diperoleh beberapa 

kesimpulan sebagai berikut. 
1.  Telah  dikembangkan  basis  data  aplikasi  web  SKD  yang 

dapat digunakan secara multitime. 

2.  Telah  dikembangkan  fitur  form  input  yang  digunakan 
sebagai  kuesioner  online  SKD  yang  dinamis  sehingga 
perubahan tidak dilakukan secara hardcoding. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

3.  Telah  dikembangkan  fitur  dokumentasi  yang  berfungsi 
untuk  mendokumentasikan  kegiatan  SKD  periode 
sebelumnya  terutama  pada  bagian  master  pertanyaan  dan 
master jawaban yang berkaitan langsung dengan kuesioner 
online SKD. 

4.  Telah  dilakukan  uji  coba  dan  evaluasi  sistem  dengan 
menggunakan  metode  blackbox  testing  dan  SUS.  Hasil 
pengujian  dengan  menggunakan  blackbox  testing  telah 
berfungsi  dengan  baik  sesuai  keinginan  dan  hasil  SUS 
menyatakan bahwa aplikasi SKD dapat dikategorikan baik 
dengan nilai 74,42.  

B. Saran 

Saran  yang  dapat  diberikan  peneliti  yang  mungkin  dapat 

bermanfaat untuk pengembangan aplikasi SKD selanjutnya. 
1.  Dalam  fitur  master  jawaban,  dapat  dikembangkan  fitur 
untuk menambahkan jenis master jawaban. Saat ini untuk 
dapat menambahkan jenis master jawaban baru  dilakukan 
dengan secara manual di basis data dan secara hardcoding 
di source code. 

2.  Dalam penggunaan fitur pertama kali oleh pengguna, dapat 
menambahkan  fitur  tutorial  yang  menunjukkan  fungsi-
fungsi dari menu atau tombol yang ada. 

DAFTAR PUSTAKA 
[1]  Atikah,  Sri.  (2017).  Sistem  Informasi  Pengolahan  Data 
Survei  Ubinan  berbasis  Web  [Skripsi].  Jakarta:  Sekolah 
Tinggi Ilmu Statistik. 

[2]  BPS. (2019). Analisis Hasil Survei Kebutuhan Data 2019. 

Jakarta : Badan Pusat Statistik. 

[3]  BPS.  (2020,  11).  Tentang  BPS  [Online].  Available: 

https://www.bps.go.id/menu/1/informasi-
umum.html#masterMenuTab1. 

[4]  Hasbah,  Nur  I.  (2017).  Sistem  Aplikasi  Entri  dan 
Rekonsiliasi  Data  Produk  Domestik  Regional  Bruto 
Triwulanan  Provinsi  (Studi  Kasus:  Subdit  Konsolidasi 
Neraca  Produksi  Regional)  [Skripsi].  Jakarta:  Sekolah 
Tinggi Ilmu Statistik. 

[5]  Sinurat, Ulfia A. (2016). Pengembangan Sistem Entri Data 
Survei  Perusahaan  Konstruksi  Triwulan  (SKTR)  Berbasis 
Web [Skripsi]. Jakarta: Sekolah Tinggi Ilmu Statistik. 
[6]  Alshamrani,  A.,  &  Bahattab,  A.  (2015).  A  Comparison 
Between  Three  SDLC  Models  Waterfall  Model,  Spiral 
Model, 
IJCSI 
International Journal of Computer Science Issues, vol. 12, 
no. 1, pp.106–111. 

Incremental/Iterative  Model. 

and 

[7]  Nidhra, S., & Dondeti, J. (2012). Black box and white box 
testing techniques-a literature review. International Journal 
of Embedded Systems and Applications (IJESA), vol. 2, no. 
2, pp.29-50. 

[8]  Bangor, A., Kortum, P., & Miller, J. (2009). Determining 
what  individual  SUS  scores  mean:  Adding  an  adjective 
rating  scale.  Journal  of  usability  studies,  vol.  4,  no.  3, 
pp.114-123. 

[9]  Bentley, L. D., & Whitten, J. L. (2007). Systems Analysis & 

Design. McGraw-Hill International. 

 8 / 8 

 
 
 
 
 
"
221709505,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Analisis Sentimen Publik Pada Twitter Terhadap 
Destinasi Wisata di Indonesia Pada Satu Tahun 
Pertama Terjadi Covid-19 
(Studi Kasus Pada 6 Destinasi Wisata di Indonesia) 

Ahmad Afif Adrinanta (221709505, 4SD1) 
Dosen Pembimbing: Yuliagnis Transver Wijaya 

Ringkasan— Pariwisata merupakan penunjang perekonomian 
Indonesia.  Banyak  sekali  destinasi  wisata  di  Indonesia  yang 
sangat indah sehingga dapat mendatangkan berbagai wisatawan, 
baik  dari  nusantara  maupun  dari  mancanegara.  Namun,  pada 
tahun  2020  sektor  pariwisata  melemah  karena  terjadi  pandemi 
Covid-19.  Maka  perlu  dilakukan  penelitian  untuk  mengetahui 
bagaimana  dampak  Covid-19  terhadap  destinasi  wisata  di 
Indonesia. Penelitian ini menggunakan metode analisis sentimen 
dan menggunakan polarity score dalam menilai dampaknya. Data 
untuk  penelitian  ini  dikumpulkan  dari  twitter.  Dari  hasil 
pengumpulan  data,  tweet  dengan  sentimen  netral  masih 
mendominasi  dari  keseluruhan  tweet  dengan  persentase  53%, 
37% untuk tweet dengan sentimen positif, dan hanya 10% tweet 
dengan sentimen negatif. Hasil dari penelitian ini adalah Covid-
19  memiliki  dampak  yang  negatif  pada  3  bulan  pertama  di 
Indonesia,  namun  seiring  berjalannya  waktu  rata-rata  polarity 
score bergerak ke arah yang positif sehingga dampak negatif dari 
Covid-19 semakin lama semakin berkurang. 

Kata  Kunci—  Analisis  Sentimen,  Covid-19,  Destinasi  Wisata, 

Pariwisata 

I.  LATAR BELAKANG 

Pariwisata  di  Indonesia  merupakan  sektor  yang  sangat 
menarik. Indonesia memiliki banyak keindahan alam sehingga 
sangat  banyak  destinasi  wisata  di  Indonesia  yang  menarik 
untuk dikunjungi. Sektor pariwisata di Indonesia juga berperan 
penting  terhadap  perekonomian  Indonesia.  Setiap  tahunnya, 
sangat  banyak  wisatawan  mancanegara  yang  datang  ke 
Indonesia.  

tahunnya 

Berdasarkan  data  dari  BPS,  setiap 

jumlah 
wisatawan  mancanegara  yang  datang  ke  Indonesia  selalu 
meningkat.  Hal  ini  merupakan  hal  baik  karena  dengan 
datangnya  wisatawan  dari  mancanegara  dapat  menunjang 
perekonomian Indonesia melalui devisa negara. Kemudian juga 
dengan  datangnya  turis  dari  mancanegara  ini,  bisnis  hotel, 
penginapan,  bisnis  kuliner,  dan  bisnis  yang  berhubungan 
dengan  pariwisata  menjadi  bergerak  dan  mendapatkan 
juga  Kementrian 
pemasukan  yang  banyak.  Kemudian 
Pariwisata  sejak  tahun  2011  melalui  mentrinya  saat  itu, 
mencanangkan  Wonderful  Indonesia.  Slogan  ini  bertujuan 
mengajak  para  wisatawan  mancanegara  untuk  berkunjung  ke 
Indonesia.  

Wisatawan  nusantara  juga  memegang  peranan  penting 
terhadap pariwisata. Setiap tahunnya banyak sekali orang yang 
melakukan  wisata  di  Indonesia.  Berdasarkan  data  dari  BPS, 

jumlah wisatawan nusantara ini memiliki trend yang meningkat 
setiap tahunnya dari tahun 2015 hingga tahun 2019. Meskipun 
terdapat  penurunan  jumlah  wisatawan  nusantara  pada  tahun 
2019,  tetapi  jumlah  wisatawan  nusantara  ini  terus  meningkat 
secara trend setiap tahunnya.  

Namun pada saat yang bersamaan, di Indonesia tidak hanya 
kaya  akan  pariwisata,  tetapi  di  Indonesia  juga  banyak  sekali 
bencana yang terjadi. Banyak bencana yang terjadi di Indonesia 
ini  dapat  menyababkan  para  wisatawan,  baik  wisatawan 
nusantara maupun wisatawan mancanegara, yang ingin datang 
ke  destinasi  wisata  di  Indonesia  itu  menjadi  ragu  sehingga 
jumlah  wisatawan  nusantara  dan  wisatawan  mancanegara  itu 
menurun. Salah satu bencana yang terjadi di Indonesia saat ini, 
dan  juga  terjadi  di  dunia,  yaitu  adalah  pandemi  Covid-19. 
BNPB  menetapkan  bahwa  pandemi  Covid-19  ini  merupakan 
bencana  non-alam  yang  terjadi  di Indoesia.  Menurut  UU  No. 
24/2007 tentang Penanggulangan Bencana, bencana non-alam 
merupakan bencana yang disebabkan oleh rangkaian peristiwa 
nonalam berupa gagal teknologi, gagal modernisasi, epidemi, 
dan  wabah  penyakit.  Covid-19  pertama  kali  di  konfirmasi  di 
Indonesia adalah pada tanggal 2 Maret 2020, yaitu terdapat 2 
orang yang positif covid-19. Hingga saat ini, pandemi Covid-
19 masih ada dan masih berlangsung. Berdasarkan dari Gambar 
1, dapat dilihat bahwa kasus Covid-19 di Indonesia masih terus 
bertambah  setiap  harinya  dari  sejak  awal  Bulan  Maret  2020 
hingga  akhir  Bulan  Januari  2021.  Kasus  terkonfirmasi  positif 
dalam sehari dapat mencapai ribuan orang. Semakin hari setiap 
harinya semakin terpecahkan rekor jumlah positif dalam sehari. 
Virus  Covid-19  menyebabkan  masalah  di  seluruh  dunia 
termasuk Indonesia. Indonesia sebagai negara yang terdampak 
Covid-19  mengalami  permasalahan  di  berbagai  sektor,  salah 
satunya sektor pariwisata. Dampaknya terhadap sektor wisata 
adalah  tempat  wisata  menjadi  sepi  dan  minim  pengunjung 
karena  masih  tingginya  virus  Covid-19.  Oleh  karena  itu 
penelitian ini akan meneliti tentang dampak dari virus Covid-
19 pada destinasi wisata di Indonesia. Analisis pada penelitian 
ini akan menggunakan analisis sentimen dari big data dengan 
data  twitter.  Untuk  saat  ini,  United  Nation  World  Tourism 
Organization 
(UNWTO)  bersama  dengan  pemerintah 
Indonesia,  melalui  Kementrian  Pariwisata,  sedang  bekerja 
sama  untuk  kembali  melakukan  pemulihan  pada  sektor 
pariwisata.  Pada  bulan  September  2020 
lalu,  UNWTO 
mengadakan pertemuan dengan Kementrian Pariwisata dalam 
membahas  untuk  kembali  membuka  Bali  untuk  wisatawan 

 1 / 9 

 
 
 
 
mancanegara. Ini menunjukkan keinginan kuat dari pemerintah 
Indonesia  untuk  melakukan  “Restart  Tourism”  yang 
dicanangkan  oleh  UNWTO.  Dari  Kementrian  Pariwisata 
sendiri terdapat 5 Destinasi Wisata Super Prioritas yang akan 
difokuskan untuk kembali dibangun setelah pandemi Covid-19. 
Oleh  karena  itu  penelitian  ini  akan  meneliti  tentang  sudah 
sejauh  mana  pemulihan  destinasi  wisata  di  Indonesia  selama 
terjadinya  Covid-19  Analisis  pada  penelitian 
ini  akan 
menggunakan  analisis  sentimen  publik  dari  big  data  dengan 
data twitter. 

Big data menurut Lynch (2008) dalam Daas et al. [1] bahwa 
big  data  dapat  didefinisikan  sebagai  volume,  kecepatan,  dan 

Gambar 1 Jumlah Kasus Terkonfirmasi Positif Covid-19 Maret 2020-
Januari 2021 Sumber Data: Satgas Covid-19 Nasional 

variasi  dari  data  yang  sangat  tinggi  yang  memerlukan  proses 
yang  lebih  baik  untuk  mencari  insight  dan  melakukan 
pengambilan keputusan. Big data dapat berbentuk dalam data 
yg terstruktur maupun tidak terstruktur Salah satu media sosial 
yg sangat terkenal dan bisa digunakan sebagai sumber big data 
adalah twitter. Twitter merupakan salah satu sosial media yang 
sangat  banyak  digunakan  oleh  masyarakat  Indonesia.  Jumlah 
pengguna  aktif  harian  twitter  terus  naik  setiap  harinya  dan 
pengguna aktif harian di Indonesia pada Maret 2020 mencapai 
166  juta  pengguna  aktif.  Oleh  karena  pengguna  yang  sangat 
banyak, twitter saat ini bisa dijadikan sumber data dari Big Data 
pada masa sekarang. Sosial media twiter ini juga memberikan 
kemudahan  bagi  para  peneliti  untuk  mengambil  data  twitter 
secara legal menggunakan Application programming interface 
(API). Big data pada masa sekarang dapat digunakan sebagai 
indikator  dalam  penilaian  banyak  hal.  Dengan  menggunakan 
twitter kita dapat melakukan penelitian melalui twitter dengan 
mengambil opini publik yang ada di twitter. 

Opini dari publik yang sangat banyak pada twitter dapat kita 
lakukan beberapa analisis salah satunya adalah menggunakan 
analisis  sentimen.  Menurut  (Liu,  2012)  [2]  analisis  sentimen, 
juga  disebut  opinion  mining,  adalah  bidang  studi  yang 
menganalisis  pendapat,  sentimen,  evaluasi,  penilaian,  sikap, 
dan  emosi  orang  terhadap  entitas  seperti  produk,  layanan, 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

organisasi,  individu,  isu,  peristiwa,  topik,  dan    atributnya. 
Dengan menggunakan analisis sentimen ini, kita dapat meneliti 
banyak  hal,  salah  satunya  adalah  pariwisata.  Dengan 
menggunakan  sentimen  analisis  pada  pariwisata,  kita  dapat 
mengetahui  bagaimana  pendapat  dari  masyarakat  terkait 
dengan pariwisata di Indonesia ini. 

Pada  manajemen  bencana,  terdapat  beberapa  tahap  yang 
dilakukan.  Tahapan-tahapan 
tersebut  yaitu  preparation, 
response,  mitigation,  dan  recovery.  Tahapan  recovery  atau 
pemulihan dipilih pada penelitian ini karena dibanding ketiga 
tahapan  yang  lain,  tahapan  recovery  ini  adalah  tahapan  yang 
paling  jarang  dilakukan  penelitian  [3].    Kemudian  analisis 
sentimen publik dipilih sebagai metode analisis karena metode 
ini  paling  mudah  dilakukan  dibanding  metode  yang  lain. 
Metode  yang  lain  seperti  remote  sensing,  sensus,  survei 
tangga,  dan  sebagainya,  masih 
lapangan,  survei  rumah 
memiliki banyak kekurangan seperti biaya yang tinggi, butuh 
banyak petugas, dan memakan waktu [4]. Maka dari itu pada 
penelitian ini ingin mengetahui sudah sejauh mana pemulihan 
destinasi wisata di Indonesia selama terjadi pandemi Covid-19 
melalui Analisis sentimen publik yang datanya bersumber dari 
twitter.  

II.  TUJUAN PENELITIAN 

Berdasarkan  latar  belakang  yang  sudah  diuraikan  diatas, 

maka penelitian ini memiliki tujuan sebagai berikut: 

1.  Untuk mengetahui dan mendapatkan sentimen terhadap 
destinasi  wisata  di  Indonesia  pada  1  tahun  pertama 
pandemi Covid-19 

2.  Untuk mengetahui bagaimana dampak pandemi Covid-
19 terhadap destinasi wisata di Indonesia selama 1 tahun 
pertama pandemi Covid-19  

III. PENELITIAN TERKAIT 

for  appraising 

Penelitian yang dilakukan oleh (Yan et al., 2020) [5] dengan 
judul  “Mining  public  sentiments  and  perspectives  from 
the  post-
geotagged  social  media  data 
earthquake  recovery  of  tourism  destinations”.  Penelitian  ini 
melakukan penilaian pemulihan daerah wisata, yaitu Lombok 
dan bali, setelah mengalami bencana gempa bumi. Data yang 
digunakan  adalah  data  dari  twitter  yang  dikumpulkan  dalam 
jangka  waktu  hampi  satu  tahun  yaitu  dari  tanggal  6  Agustus 
2018  hingga  31  Juli  2019.  Kata  kunci  yang  digunakan  untuk 
mengumpulkan  tweet  dari  twitter  ini  adalah  “Lombok”  dan 
“Bali”. Kata kunci ini digunakan untuk mengambil teks tweet 
maupun hashtag yang mengandung kedua kata tersebut. Data 
tweet  yang  dikumpulkan  tersebut  berjumlah  2.007.802  untuk 
tweet  yang  menyebutkan  kata  Lombok  dan  7.643.592  tweet 
yang  menyebutkan  kata  Bali.  Jumlah  tweet  tersebut  sudah 
dikurangi 49.397 tweet dan 193.762 tweet yang tweet tersebut 
berasal  dari  berita.  Hasilnya  adalah  sentiment  negative  dari 
waktu ke waktu terhadap Lombok dan bali semakin berkurang 
yang berarti pemulihan daerah tersebut dilakukan dengan baik. 
Penelitian yang dilakukan oleh (Baro & Palaoag, 2020) [6] 
dengan  judul  “Disaster  Sentiment  Analysis:  Addressing  the 

 2 / 9 

 
 
Challenges of Decision-Makers in Visualizing Netizen Tweets”. 
Penelitian  ini  dilakukan  dengan  mengumpulkan  data  tweet 
dengan melakukan streaming dengan API Twitter. Kemudian 
tweet  tersebut  ditampung  dalam  database  MySQL.  Atribut-
atribut  yang  di  ekstrak  dari  tweet  adalah  isi  tweet,  nama  di 
profil, waktu tweet, dan lokasi tweet. Data yang dikumpulkan 
adalah  data  dengan  4  bencana  paling  sering  yang  terjadi  di 
Filipina yaitu Angin Topan, Banjir, Gempa Bumi, dan Tanah 
Longsor.  Kemudian  setelah  didapatkan  data  tweet,  dilakukan 
analysis.  Analisis  dilakukan  dengan  library  NLP  (Natural 
Languange  Processing).  Analisis  yang  dihasilkan  adalah 
sentiment  dan  subjektivitas  dari  tweet.  Setelah  dilakukan 
analisis  sentiment  dan  subjektivitas  dari  tweet,  dilakukan 
visualisasi  dengan  menggunakan  dashboard.  Visualisasi 
diberikan  secara  real-time  dengan  tweet  yang  sudah  berjalan 
untuk diakses oleh para pengambil keputusan. 

Penelitian yang dilakukan oleh (Shah et al., 2018) [7] dengan 
judul “Twitter Analysis for Disaster Management”. Penelitian 
ini  melakukan  pengumpulan  data  dilakukan  dengan  package 
tweepy. Kemudian data yang diambil dari twitter dikonversikan 
menjadi format csv. Keyword yang digunakan untuk mencari 
tweet  yang  berhubungan  dengan  gempa  bumi  Nepal  adalah 
“#nepalearthquake”, 
“#nepalquakerelief”, 
“#nepal”, 
“#kathmandu”,  “helping  Nepal”,  “Nepal  victim”,  “nepal 
earthquake”.  Kemudian  dilakukan  pre-processing  data.  Pre-
processing  data  dilakukan  dengan  menghapus  stopwords 
terlebih  dahulu.  Kemudian  setelah  menghapus  stopwords 
dilakukan  tokenisasi.  Tokenisasi  ini  adalah  mengumpulkan 
kata-kata  yang  mirip  dalam  satu  kosa  kata  saja.  Misalnya 
adalah kata “help” itu sumber kata nya sama saja dengan kata 
“helping” “helped” “helps” dan sebagainya. Setelah dilakukan 
tokenisasi,  maka  kata-kata  tadi  dilakukan  visualisasi  dengan 
menggunakan  pie  chart  untuk  melihat  proporsi  dari  masing-
masing kata yang sudah dilakukan tokenisasi 

tweet  karena  akan 

Penelitian yang dilakukan oleh (Nair et al., 2017) [8] dengan 
judul  “Usage  and  analysis  of  Twitter  during  2015  Chennai 
flood towards Disaster Management”. Pada penelitian ini data 
tweet dikumpulkan dengan dilakukan streaming dari tanggal 10 
November 2015 – 25 Maret 2016. Tweet dikumpulkan dengan 
menggunakan kata kunci yaitu hashtag #chennaiflood. Hashtag 
#chennaiflood digunakan sebagai kata kunci dalam melakukak 
lebih  mudah 
pengumpulan  data 
mendapatkan  tweet  yang  relate  dengan  keadaan  bencana. 
Karena  kegunaan  hashtag  adalah  mengelompokkan  tweet 
dengan topik yang serupa agar lebih mudah dicari. Tweet yang 
dikumpulkan  memiliki  17  atribut.  Setelah  data  dikumpulkan, 
dilakukan klasifikasi terhadap data tweet tersebut. Klasifikasi 
dilakukan  menjadi  5  kategori  berbeda.  Kategori  itu  adalah 
Need For Help (Meminta bantuan), Relief measures (Tindakan 
Bantuan),  Express  gratitude  (Ungkapan  Terima  Kasih), 
Complaints  (keluhan),  dan  Other  (hal  lain).  Klasifikasi 
dilakukan  dengan  algoritma  machine  learning  yaitu  decision 
tree, random forest, dan naïve bayes 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

IV. METODE PENELITIAN  

Batasan Penelitian 

Gambar 2 Metode Penelitian 

Pada  penelitian  ini  dilakukan  sentimen  analisis  terhadap 
destinasi  wisata  di  Indonesia  selama 
terjadi  Covid-19 
menggunakan  data  Twitter.  Scraping  twitter  dilakukan  dan 
yang  dikumpulkan  adalah  tweet  yang  menggunakan  bahasa 
inggris dan bahasa Indonesia, karena yang ingin diteliti adalah 
sentiment publik dari wisatawan yang berkunjung ke beberapa 
destinasi wisata yang ada di Indonesia. 

Untuk  destinasi  wisata  yang  digunakan  disini  adalah  6 
destinasi  wisata.  Destinasi  wisata  yang  pertama  adalah  Bali. 
UNWTO  bersama  dengan  pemerintah  Indonesia  sedang 
melakukan  persiapan  untuk  Kembali  membuka  Bali  untuk 
wisaatawan mancanegara. Kemudian kita juga tau bahwa Bali 
merupakan  destinasi  wisata  yang  sangat  terkenal  di  dunia. 
Salah satu destinasi wisata di Bali yang sangat terkenal adalah 
Pantai Kuta.  Maka dari itu Pantai Kuta dipilih sebagai destinasi 
wisata  pada  penelitian 
ini.  Kemudian  dari  Kementrian 
Pariwisata  kita  sendiri,  terdapat  5  destinasi  wisata  super 
prioritas.  5  destinasi  wisata  itu  adalah  Labuan  Bajo  (Nusa 
Tenggara  Timur),  Danau  Toba  (Sumatra  Utara),  Mandalika 
(Nusa Tenggara Barat), Likupang (Sulawesi Utara), dan yang 
terakhir  adalah  Candi  Borobudur  (Jawa  Tengah).  Menurut 
Kementrian  Pariwisata  [9],  5  destinasi  wisata  ini  merupakan 
destinasi wisata super prioritas di Indonesia karena kedepannya 
destinasi  wisata  tersebut  akan  menarik  belasan  juta  turis 
domestik  maupun  turis  mancanegara  ke  Indonesia.  Jadi  pada 
penelitian  kali  ini  akan  menggunakan  6  detinasi  wisata  yaitu 
Pantai Kuta, Labuan Bajo, Mandalika, Danau Toba, Likupang, 
dan Candi Borobudur. 

Setelah  dilakukan  scraping  dilakukan  pre-processing  pada 
data  untuk  menghilangkan  noise  dari  data  twitter.  Setelah 

 3 / 9 

 
 
 
 
 
 
 
dilakukan preprocessing dilakukan sentimen untuk tweet yang 
sudah dilakukan pre-processing kemudian dilakukan analisis.  

pada 

scraping 

dilakukan 

dilakukan 

cara 
scraping 

Metode Pengumpulan Data 
Penelitian  menggunakan  bahasa  pemrograman  Python  3.8 
mulai dari pengambilan data hingga analisisnya. Pengumpulan 
situs 
data 
dengan 
dengan 
www.twitter.com.  Proses 
menggunakan  package  pada  python  yaitu  package  Twint. 
Twint  dapat  melakukan  ekstraksi  data 
tanpa 
menggunakan API, sehingga tidak ada Batasan hari dan batasan 
jumlah  tweet  yang  bisa  diambil.  Scraping  akan  dilakukan 
dengan menggunakan 5 keyword sesuai dengan nama destinasi 
wisata  yang  akan  dilakukan  penelitian  yaitu  “Labuan  Bajo”, 
“Danau  Toba”,  “Mandalika”,  “Likupang”,  dan  “Candi 
Borobudur”.    Tweet  yang  akan  dikumpulkan  adalah  tweet 
dengan rentang waktu antara 1 Maret 2020 sampai 28 Februari 
2021. Rentang waktu ini dipilih dengan jangka waktu kurang 
lebih  1  tahun  sehingga  nantinya  bisa  dilihat  perbandingan 
dalam satu tahun penuh.  

twitter 

Preprocessing 
Preprocessing Tweet di Indonesia cenderung lebih beragam 
dibanding  negara  lain.  Banyaknya  emoji,  bahasa  slang  yang 
cenderung  jauh  dari  bahasa  baku,  penggunaan  singkatan, 
penggunaan bahasa daerah juga banyak terjadi. Namun, untuk 
bahasa  daerah  dalam  penelitian  ini  diabaikan  karena  masih 
sedikitnya  pengembangan  yang  ada.  Oleh  karena  itu  pada 
penelitian  ini,  seluruh  tweet  akan  diterjemahkan  ke  dalam 
Bahasa Inggris agar lebih mudah dalam analisis nya. Data yang 
mengandung  banyak  noise  akan  memengaruhi  kualitas  hasil 
analisis,  sehingga  preprocessing  harus  dilakukan  untuk 
memperbaiki  kualitas  data.  Preprocessing  yang  dilakukan 
dalam penelitian antara lain: 
Cleaning 

adalah  proses  untuk  membersihkan 
dokumen  dari  kata-kata  yang 
tidak  diperlukan  untuk 
mengurangi noise pada proses klasifikasi. Seperti penghapusan 
url, username, dan hashtags.  

1. 

2. 

Case  folding  adalah  proses  penyeragaman  bentuk 
huruf serta penghilangan tanda baca dan angka, sehingga hanya 
menerima huruf latin antara a sampai z. Dalam hal ini, semua 
kata  dalam  tweet  dikonversi  menjadi  lowercase  (huruf  kecil) 
juga mengahapus karakter ASCII.  

3. 

Penghapusan  Stopword  yaitu  proses  menghilangkan 
kata-kata  yang  tidak  penting  dalam  proses  klasifikasi  dan 
penetuan alasan, seperti kata: yang, tetapi, atau, ke, di, dengan, 
dan sebagainya.  

4. 

Penghapusan  duplikat  yakni  menghapus  tweet  yang 

sama atau retweet (RT) 
Analisis Sentimen  
Untuk  menghasilkan  sentimen  dari  setiap  tweet  yang 
didapatkan,  digunakan  opensource  library  yaitu  TextBlob. 
TextBlob  sangat  baik  dalam  melakukan  sentimen  dari  tweet 
dengan  menggunakan  Natural  Languange  Processing  (NLP).  
Dengan  menggunakan  TextBlob  ini  nantinya  akan  dihasilkan 
polarity score.  

Polarity  score  digunakan  untuk  menghasilkan  sentimen 
score. Sentimen score ini berkisar pada angka -1 sampai 1. Jika 
skor  semakin  mendekati  -1  maka  artinya  semakin  negative 
jika  skor  yang 
sentiment  yang  dihasilkan.  Sebaliknya, 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

dihasilkan  semakin  mendekati  1,  maka  semakin  positif 
sentiment yang dihasilkan. Kemudian untuk subjectivity score, 
digunakan untuk melihat seberap subjektif teks dari suatu tweet.  

Visualisasi Data 
Setelah  didapatkan 

skor 

sentimen,  akan  dilakukan 
visualisasi data. Visualisasi ini akan dilakukan dengan 2 jenis 
visualisasi.  Visualisasi  yang  pertamau  yaitu  akan  dilakukan 
visualisasi  tentang  persentase  jumlah  tweet  dengan  sentimen 
positif,  negatif,  dan  netral.  Kemudian  visualisasi  yang  kedua 
yaitu akan dilakukan visualisasi terhadap kata-kata yang sering 
muncul terkait dengan destinasi wisata yang dijadikan sebagai 
studi kasus 

Penilaian Dampak Covid-19 
Pada  penelitian  ini,  penilaian  dampak  pandemi  Covid-19 
terhadap destinasi wisata di Indonesia akan dilakukan dengan 
metode skoring. Skoring disini menggunakan rata-rata dari skor 
sentimen  (polarity  score)  yang  sudah  didapatkan  pada  tahap 
penelitian  sebelumnya.  Skor  sentimen  merupakan  skor  untuk 
melihat  bagaimana  sentimen  dari  suatu  tweet.  Jika  skor 
semakin mendekati -1 maka artinya semakin negatif sentimen 
yang dihasilkan. Sebaliknya, jika skor yang dihasilkan semakin 
mendekati 1, maka semakin positif sentimen yang dihasilkan. 
Skor sentimen ini akan dilakukan rata-rata pada setiap bulannya, 
sehingga  bisa  dipantau  perubahan  polarity  score  dari  setiap 
bulannya. Dari rata-rata skor sentimen setiap bulannya tersebut, 
selanjutnya  dilakukan  analisis  untuk  dilakukan  penilaian 
dampak pandemi Covid-19. 

V.  KERANGKA PIKIR 

Pariwisata  merupakan  sektor  yang  sangat  penting  di 
Indonesia.  Pariwisata  mendukung  perekonomian  Indonesia 
melalui devisa negara dan banyak hal lain. Namun pada tahun 
2020, sektor pariwisata menjadi melemah akibat dari Covid-19. 
Sektor  pariwisata  menjadi  melemah  disebabkan  oleh  orang-
orang  yang  masih  takut  untuk  bepergian  karena  takut  tertular 
Covid-19.  Oleh  karena  itu  dilakukan  penelitian  ini  untuk 
mengetahui  seberapa  jauh  pemulihan  destinasi  wisata  selama 
terjadinya pandemic Covid-19. 

VI. HASIL DAN PEMBAHASAN 

Pengumpulan Data 
Untuk  hasil  dan  pembahasan  pada  penelitian  ini,  tahap 
pertama  yang  akan  dilakukan  adalah  pengumpulan  data. 
Flowchart  pengumpulan  data  ini  dapat  dilihat  seperti  pada 

 4 / 9 

 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

beberapa  atribut  lain  selain  tweet  itu  sendiri  yaitu  atribut 
tanggal,  user  id,  waktu  tweet,  dan  atribut  lainnya.  Data  yang 
sudah  didapatkan  ini  masih  merupakan  raw  data  dan  masih 
banyak  hal  yang  dapat  menggangu  untuk  dilakukan  analisis. 
Oleh karena itu data ini nantinya akan dilakukan preprocessing 
terlebih dahulu sebelum dilakukan analisis. 

Sebagai gambaran awal, diberikan visualisasi data dari data 
yang sudah dikumpulkan. Dari gambar 4, dapat kita lihat bahwa 
jumlah tweet dari setiap tempat destinasi wisata itu berfluktuasi 
setiap bulannya. Tweet paling banyak itu terdapat pada Bulan 
Oktober  2020  untuk  destinasi  wisata  Labuan  Bajo  dengan 
5.334 tweet. Sedangkan untuk tweet paling sedikit itu terdapat 
pada  bulan  Juni  2020  dengan  hanya  32  tweet  saja  untuk 
destinasi wisata Likupang. 

6000

5000

4000

3000

2000

1000

0

Gambar 3 Flowchart Scraping Tweet 
Menggunakan Twint 

scraping  data 

Gambar  3.  Pada  gambar  3,  pengumpulan  data  ini  akan 
dilakukan dengan melakukan scrapping dengan menggunakan 
package  twint  yang  terdapat  pada  python.  Twint  akan 
melakukan 
tanpa 
menggunakan  API.  Twint  bekerja  melakukan  scraping 
berdasarkan  command  yang  kita  buat  pada  saat  melakukan 
coding.    Untuk  melakukan  scraping  dengan  twint,  kita  perlu 
menentukan  keyword  yang  akan  kita  gunakan  untuk  mencari 
tweet. 

tweet  pada 

twitter 

0
2
-
r
a
M

0
2
-
r
p
A

0
2
-
y
a
M

0
2
-
n
u
J

0
2
-
l
u
J

0
2
-
g
u
A

0
2
-
p
e
S

0
2
-
t
c
O

0
2
-
v
o
N

0
2
-
c
e
D

1
2
-
n
a
J

1
2
-
b
e
F

Bulan

Pantai Kuta

Danau Toba

Likupang

Candi Borobudur

Labuan Bajo

Mandalika

Keyword  pada  penelitian  ini  yang  akan  digunakan  adalah 
nama  destinasi  wisata  yang  akan  dijadikan  penelitian  yaitu 
keyword  “Pantai  Kuta”  “Labuan  Bajo”,  “Danau  Toba”, 
“Mandalika”,  “Likupang”,  dan  “Candi  Borobudur”  dengan 
timeframe pengumpulan data yaitu tweet dari tanggal 1 maret 
2020  hingga  28  Februari  2021. Setelah  dimasukkan  keyword 
dan  timeframe  yang  tersebut, twint  akan  melakukan  scraping 
dengan  mencari  semua  tweet  yang  terdapat  keyword  seperti 
yang  sudah  kita  masukkan.  Untuk  hasilnya  adalah  untuk 
destiansi  wisata  Pantai  Kuta  terdapat  8.705  tweet  kemudian 
untuk destinasi wisata Labuan Bajo terdapat sejumlah 35.199 
tweet,  kemudian  untuk  destinasi  wisata  Danau  Toba  terdapat 
21.791  tweet,  kemudian  untuk  destinasi  wisata  Mandalika 
terdapat  13.110  tweet,  kemudian  untuk  destinasi  wisata 
Likupang  terdapat  sebanyak  1.173  tweet,  dan  terakhir  untuk 
destinasi Candi Borobudur yaitu sebanyak 13.799 tweet. Oleh 
karena itu total tweet keseluruhan dari kelima destinasi wisata 
ini adalah sebanyak 93.777 tweet.  

Seluruh  data  ini  setelah  dilakukan  scraping  akan  disimpan 
pada  file  dengan  format  csv  sehingga  akan  memudahkan 
pengolahan  nantinya.  Pada  file  hasil  scrapingan  ini  terdapat 

Gambar 4 Visualisasi Data Hasil Scraping 

Pre-Processing 
Pada  Pre-processing,  tahap  pertama  yang  akan  dilakukan 
adalah cleaning data. Cleaning ini akan menghapuskan hal-hal 
dari tweet yang akan mengganggu analisis nantinya contohnya 
seperti  username,  url,  hashtag,  angka,  dan  lambang-lambang 
yang  tidak  diperlukan  seperti  lambang  stock  market  dan 
sebagainya 

Lalu pada tahap kedua akan melakukan case folding menjadi 
huruf kecil atau lower case. Tahap ini akan mengubah seluruh 
tweet menjadi huruf kecil dan tidak ada huruf kapital. 

Kemudian pada tahap ketiga akan dilakukan penerjemahan 
tweet ke dalam bahasa inggris. Proses pada penerjemahan tweet 
ini  memakan  waktu  yang  cukup 
lama  dikarenakan 
menggunakan  google  API  dari  package  yang  digunakan 
sehingga jika kita melakukan translate banyak tweet dalam satu 
waktu akan terjadi too many request dan baru bisa dilanjutkan 
esok hari. 

 5 / 9 

 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Tabel II  

Contoh Tweet dengan Sentimen Negatif, Positif, dan Netral 

No  Tweet Sebelum Pre-

1 

2 

3 

Processing 
Cepatlah berlalu virus 
menyedihkan ðŸ™(cid:143) #Bali  
#pantaikuta #beachlife 
#beaches  
https://t.co/k9AL8VdhXe 

Rasanya kangen menikmati 
sunset di pantai Kuta. 
Semoga covid 19 cepat 
berlalu dan aku bebas ke 
pantai lagi 

Just posted a photo @ 
Candi Borobudur, Kec 
Borobudur Kab Magelang, 
Provinsi Jawa Tengah  
https://t.co/yR0PxvDbnm 

Nilai 
Polarity 
-1 

0,45 

0 

Tweet Sesudah 
Pre-processing 
hurry pass 
pathetic virus 
ðŸ™(cid:143) bali 
beaches kuta 
beachlife 
beaches 
miss enjoying 
sunset kuta 
beach hopefully 
covid pass 
quickly free go 
beach 
posted photo 
borobudur 
temple 
borobudur 
district magelang 
district central 
java province 

Visualisasi Data 
Setelah  didapatkan  sentimen 

terhadap 

terhadap  seluruh 

tweet, 
selanjutnya akan dilakukan visualisasi terhadap data tersebut. 
Berdasarkan gambar 5, diberikan visualisasi tentang kata-kata 
yang paling sering muncul dari semua destinasi wisata. Dapat 
dilihat  bahwa  kata-kata  yang  sering  muncul  adalah  kata-kata 
seperti  “bajo”,  “labuan”,  “toba”,  “Borobudur”,  “mandalika”, 
“kuta”,  dan  sebagainya.  Hal  ini  merupakan  hal  yang  wajar 
karena  pencarian 
tweet  dilakukan  dengan 
mengggunakan nama destinasi wisata nya. Sehingga kata-kata 
yang  sering  muncul  merupakan  nama-nama  dari  destinasi 
wisata tersebut. Kemudian selain nama-nama destinasi wisata, 
terdapat  beberapa  kata  juga  seperti  “motogp”,  “bali”,dan 
sebagainya. Ini berhubungan dengan masing-masing destinasi 
wisata.  Contohnya  kata  “motogp”  ini  berhubungan  dengan 
destinasi wisata Mandalika dimana saat ini pemerintah sedang 
melaksanakan pembangunan sirkuit balap MotoGP di destinasi 
wisata  Mandalika.  Sehingga  kata  “motogp”  sering  muncul 
dikarenakan  masyarakat 
terhadap 
pembangunan  ini.  Kemudian  untuk  kata  seperti  “bali”  ini 
merujuk pada letak geografis dari destinasi wisata. Untuk Bali 
sendiri  merupakan  provinsi  dimana  Pantai  Kuta  berada. 
Sehingga  sangat  wajar  ketika  kata  “bali”  termasuk  ke  dalam 
kata  yang  sering  muncul.  Namun  pada  visualisasi  terdapat 
beberapa  kelemahan  dimana  karakter  spasi  (paling  banyak 
muncul  ke  3)  terdeteksi  sebagai  kata.  Oleh  karena  itu  untuk 
karakter  spasi  yang  muncul  dan  terdeteksi  sebagai  kata  ini 
dapat diabaikan.  

antusias 

sangat 

Tahap  keempat  yaitu  melakukan  penghapusan  stopwords. 
Untuk  melakukan  penghapusan  stopwords  ini  kita  harus 
terlebih dahulu melakukan tokenisasi terhadap kata-kata yang 
ada  pada  tweet.  Jika  kita  melakukan  penghapusan  stopwords 
sebelum  dilakukan  tokenisasi  maka  hasilnya  tidak  optimal. 
Lalu juga kata-kata yang di tokenisasi ini bisa digunakan untuk 
mencari kata-kata yang paling sering muncul. 

Tahap kelima yaitu mengembalikan kata-kata yang sudah di 
tokenisasi  menjadi  kalimat.  Ini  dilakukan  agar  nantinya  bisa 
dilakukan  analisis  sentiment  dari  kalimat  yang  stopwordsnya 
sudah dihapus. 
Lalu  yang 

tahap  keenam  dilakukan 
penghapusan  retweet.  Retweet  merupakan  melakukaan  tweet 
ulang  dari  tweet  orang  lain.  Sehingga  jika  ada  retweet  maka 
akan terdapat tweet yang sama sebanyak lebih dari satu. Maka 
dari itu perlu dihapus agar tidak terjadi analisis 2 kali.  

terakhir  pada 

Berikut  diberikan  contoh  tweet  sebelum  dan  sesudah  pre-

processing. 

Tabel I  

Tweet Sebelum dan Sesudah di Pre-Processing 

No 

Sebelum Pre-Processing 

1 

2 

Pantai Kuta yang merupakan 
salah satu icon Pariwisata Bali, 
saat ini masih ditutup. Begitu juga 
destinasi lainnya di Bali. Hotel, 
Villa, Restaurant dan sektor 
lainnya juga masih stop 
beroperasi selama masaâ€¦  
https://t.co/CJ1zyhuagu 
dagang kecil yg masih jualan, 
menunggu pelanggan yg gak 
pasti. Sedih. Kangen jalanan Kuta 
yg padat, rame turis jalan kaki di 
trotoar, bli bli yg jalan sambil 
bawa papan selancar, anak kecil 
penjual gelang di pantai, dan 
kegiatan lainnya. Lekas sembuh 
Bali, lekas kembali normal. 

Setelah Pre-
Processing 
kuta beach one balis 
tourism icons 
currently still closed 
likewise destinations 
bali hotel villa 
restaurant sectors 
also still stop 
operating theâ€¦ 
small traders still 
selling waiting 
customers sure sad 
miss streets kuta 
crowded tourists 
walking sidewalk bli 
bli walks carrying 
surfboard small child 
selling bracelet beach 
activities get well 
soon bali return 
normal 

Setelah dilakukan pre-processing, jumlah tweet dari masing-
masing  destinasi  wisata  menjadi  lebih  sedikit  karena  terdapat 
penghapusan untuk tweet yang terjadi duplikat (retweet). Dari 
awalnya terdapat total keseluruhan tweet yaitu sebanyak Maka 
total  tweet  setelah  selesai  dilakukan  pre-processing  menjadi 
sebanyak 74.897 atau telah berkurang sebanyak 18.880 tweet 
dari data awal yang sebelumnya berjumlah 93.777 

Analisis Sentimen 
Setelah  data  dibersihkan  dan  dilakukan  pre-processing, 
maka  untuk  memenuhi  tujuan  1  dilakukan  analisis  sentimen 
terhadap  tweet  dari  destinasi  wisata.  Nantinya  akan  didapat 
sentimen dari keseluruhan tweet yang berupa sentimen negatif, 
positif,  atau  netral.  Berikut  adalah  beberapa  contoh  hasil  dari 
analisis sentimen  

 6 / 9 

 
 
 
Kemudian  pada  gambar  6,  diberikan  visualisasi  terhadap 
persentase  dari  tweet  dengan  sentimen  positif,  negatif,  dan 
netral. Dapat dilihat bahwa sentimen netral masih mendominasi 
dengan  persentase  sebesar  53%  atau  berjumlah  sebanyak 
39.779  tweet.  Lebih  dari  setengah  dari  keseluruhan  tweet 
memiliki sentimen netral. Untuk tweet dengan sentimen positif 
memiliki persentase sebesar 37% atau sebanyak 27.325 tweet. 
Kemudian  untuk  sentimen  negatif  memiliki  persentase  tweet 
sebanyak 10% saja atau hanya sebanyak 7793 tweet saja 

Gambar 5 Kata-Kata yang Sering Muncul pada 5 Destinasi 
Wisata 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

bulannya.  Dapat  kita  lihat  pada  gambar  bahwa  pada  Bulan 
Maret  2020,  kita  masih  melihat  angka  rata-rata  nilai  polarity 
masih  tinggi  jika  dibandingkan  dengan  bulan-bulan  lain.  Ini 
dapat disebabkan karena meskipun pasien Covid-19 pertama di 
Indonesia itu dikonfirmasi pada awal Maret 2020, namun pada 
saat itu tidak langsung diberlakukan PSBB (Pembatasan Sosial 
Berskala  Besar)  diseluruh  Indonesia.  PSBB  baru  diterapkan 
sejak  tanggal  16  Maret  2020  yaitu  pada  pertengahan  Bulan 
Maret.  Oleh  karena  itu,  destinasi  wisata  pada  Bulan  Maret 
hanya  terdampak  oleh  pandemi  Covid-19  selama  setengah 
bulan saja. Lalu kita lihat pada Bulan April 2020, rata-rata nilai 
polarity yang didapatkan itu langsung turun yang menandakan 
pada  Bulan  April  semakin  banyak  tweet  dengan  sentimen 
negatif  terhadap  destinasi  wisata.  Hal  ini  dikarenakan  orang 
masih  melakukan  PSBB  dirumah  masing-masing  dan  tidak 
bepergian  ke  destinasi  wisata.  Lalu  selanjutnya  dari  Bulan 
April  2020  hingga  ke  Bulan Februari  2021  terdapat  kenaikan 
terhadap rata-rata nilai polarity per bulannya. Ini menandakan 
sudah  semakin  berkurangnya  tweet  dengan  sentimen  negatif 
yang artinya orang-orang sudah mulai tidak ragu untuk datang 
ke destinasi wisata. yang artinya orang-orang sudah mulai tidak 
ragu  untuk  datang  ke  destinasi  wisata.  Hal  ini  menunjukkan 
bahwa  memang  pandemi  Covid-19  memiliki  dampak  yang 
negatif  terhadap  destinasi  wisata  di  Indonesia  pada  3  bulan 
pertama  terjadinya  Covid-19  di  Indonesia  ini.  Namun  seiring 
berjalannya waktu, dampak negatif dari Covid-19 ini perlahan 
telah  kembali  membaik  setelah  berjalan  selama  1  tahun. 
Meskipun hingga pada saat penelitian ini ditulis, virus Covid-
19  itu  sendiri  telah  bermutasi  menjadi  varian  yang  lebih 
berbahaya seperti varian delta, gamma, dan sebagainya. Oleh 
karena  itu,  penelitian  ini  dilakukan  dengan  asumsi  belum 
adanya varian dari virus Covid-19. 

53%

37%

10%

Positif

Negatif

Netral

0.12

0.1

0.08

0.06

0.04

0.02

0

Gambar 6 Persentase Tweet dengan Sentimen Positif, Negatif, dan Netral 

Penilaian Dampak Covid-19 
Untuk melakukan penilaian, akan dilakukan dengan melihat 
rata-rata dari nilai polarity yang didapatkan pada tahap analisis 
sentimen  secara  per  bulan  nya.  Jika  secara  rata-rata  nilai 
polarity dalam jangka waktu satu tahun semakin lama semakin 
kearah nilai positif, maka dampak negatif dari Covid-19 pada 
destinasi wisata di Indonesia semakin lama semakin berkurang. 
Namun  jika  nilai  polarity  secara  rata-rata  setiap  bulannya 
semakin  lama  semakin  mengarah  ke  nilai  negatif,  maka 
dampak  negatif  dari  Covid-19  pada  destinasi  wisata  di 
Indonesia  semakin  lama  semakin  parah.  Pada  gambar  7 
diberikan  visualisasi  terhadap  rata-rata  nilai  polarity  setiap 

0
2
-
r
a
M

0
2
-
r
p
A

0
2
-
y
a
M

0
2
-
n
u
J

0
2
-
l
u
J

0
2
-
g
u
A

0
2
-
p
e
S

0
2
-
t
c
O

0
2
-
v
o
N

0
2
-
c
e
D

1
2
-
n
a
J

1
2
-
b
e
F

Gambar 7 Rata-Rata Nilai Polarity Per Bulan 

Kemudian  data  polarity  score  per  bulan 

ini  akan 
dibandingkan  dengan  pertumbuhan  ekonomi  Indonesia  data 
pertumbuhan PDB (Produk Domestik Bruto) Indonesia secara 
y-on-y. Pada kedua data tersebut terdapat beberapa kemiripan 
yang terjadi. Dari data BPS, pada triwulan 2 tahun 2020, yaitu 
merupakan 3 bulan pertama Covid-19 ada di Indonesia, angka 
pertumbuhan  ekonomi  di  Indonesia  mengalami  kontraksi 
senilai  -5,32%.  Kemudian  semakin  lama  angka  pertumbuhan 
ekonomi Indonesia semakin membaik hingga pada triwulan 1 
tahun  2021  berada  pada  angka  -0,74%.  Ini  juga  menandakan 

 7 / 9 

 
 
bahwa  memang  virus  Covid-19  juga  memiliki  dampak  yang 
negatif pada 3 bulan pertamanya terhadap ekonomi Indonesia, 
sekaligus  juga  dampak  negatif  terhadap  pariwisata.  Namun 
seiring  berjalannya  waktu,  dampak  negatif  itu  semakin 
berkurang dan kembali membaik. 

0.00

-1.00

-2.00

-3.00

-4.00

-5.00

-6.00

Triwulan 2
2020

Triwulan 3
2020

Triwulan 4
2020

Triwulan 1
2021

Series1

-5.32

-3.49

-2.19

-0.74

Gambar 8 Pertumbuhan PDB Indonesia y-on-y 

Kemudian  jika  dibandingkan  juga  dengan  data  kunjungan 
wisatawan  mancanegara  ke  Indonesia,  terdapat  beberapa 
kemiripan  pada  data  pemulihan  destinasi  wisata  ini.  Dapat 
dilihat  pada  gambar  8  bahwa  pada  bulan  Maret  2020, 
kunjungan  wisatawan  mancanegara  masih  berada  pada  angka 
486.155. Namun pada bulan April, langsung terjadi penurunan 
yang  sangat  drastis.  Hal  ini  mirip  dengan  data  pemulihan 
destinasi wisata. Namun perbedaannya adalah untuk kunjungan 
wisatawan  mancanegara  hingga  bulan  Februari  2021  belum 
terlihat  terjadinya  peningkatan  yang  cukup  tinggi  sehingga 
angkanya terlihat cukup stagnan. 

500000

400000

300000

200000

100000

0

0
2
-
r
a
M

0
2
-
r
p
A

0
2
-
y
a
M

0
2
-
n
u
J

0
2
-
l
u
J

0
2
-
g
u
A

0
2
-
p
e
S

0
2
-
t
c
O

0
2
-
v
o
N

0
2
-
c
e
D

1
2
-
n
a
J

1
2
-
b
e
F

Gambar 9 Data Kunjungan Wisatawan Mancanegara Maret 2020-
Februari 2021 

VII. 

PENUTUP 

Dari  hasil  dan  pembahasan  yang  sudah  diuraikan  diatas, 

terdapat beberapa kesimpulan yang didapat yaitu: 

1.  Dari  seluruh  tweet,  tweet  dengan  sentimen  netral 
mendominasi  dengan  persentase  53%  kemudian  tweet 
dengan  sentiment  positif  sebanyak  37%  dan  tweet 
dengan sentimen negatif hanya sebanyak 10%. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

2.  Hasil dari analisis terhadap dampak Covid-19 terhadap 
destinasi wisata di Indonesia menjelaskan bahwa Covid-
19  pada  satu  tahun  pertama  di  Indonesia  memiliki 
dampak negatif terhadap destinasi wisata di Indonesia. 
Pada  awal  Covid-19,  rata-rata  polarity  score  bergerak 
cenderung  kearah  negatif.  Namun  seiring  berjalannya 
waktu,  rata-rata  polarity  score  bergerak  ke  arah  yang 
positif sehingga dampak negatif dari Covid-19 semakin 
lama  semakin  berkurang.  Hal  ini  juga  sejalan  dengan 
data  yang  disajikan  oleh  BPS  terkait  dengan  data 
pertumbuhan PDB di Indonesia secara y-on-y 

Kemudian  berdasarkan  penelitian  yang  sudah  dilakukan 
terdapat  beberapa  saran  dari  peneliti  untuk  penelitian 
selanjutnya yaitu: 

1.  Melakukan  Pre-processing  dengan  lebih  baik,  agar 
karakter spasi (“ “)  tidak terbaca sebagai sebuah kata 
sehingga analisis menjadi lebih baik. 

2.  Menggunakan pendekatan untuk mencari nilai polarity 
untuk berbagai bahasa sehingga tidak perlu melakukan 
penerjemahan ke bahasa inggris terlebih dahulu. 
3.  Pada  penelitian  ini  hanya  menggunakan  data  twitter 
dengan rentang waktu 1 tahun. Untuk ke depannya, bisa 
digunakan timeframe yang lebih panjang agar hasilnya 
lebih maksimal. 

4.  Tidak  hanya  menggunakan  teks  yang  didapat  pada 
twitter,  namun  juga  memasukkan  gambar  ke  dalam 
analisis untuk juga dicari sentimen dari sebuah gambar 
yang ada di dalam sebuah tweet. 

[1] 

[2] 

[3] 

[4] 

[5] 

[6] 

DAFTAR PUSTAKA 

P. J. H. Daas, M. Puts, M. Tennekes, and A. Priem, 
“Big Data as a Data Source for Official Statistics: 
experiences at Statistics Netherlands,” vol. 31, no. 2, 
pp. 249–262, 2014. 

B. Liu, Sentiment Analysis and Opinion Mining. 2012. 

G. P. Smith and D. Wenger, “Sustainable Disaster 
Recovery: Operationalizing An Existing Agenda,” no. 
May 2018, 2007, pp. 234–257. 

Y. Yan, M. Eckle, C. L. Kuo, B. Herfort, H. Fan, and 
A. Zipf, “Monitoring and assessing post-disaster 
tourism recovery using geotagged social media data,” 
ISPRS Int. J. Geo-Information, vol. 6, no. 5, 2017, 
doi: 10.3390/ijgi6050144. 

Y. Yan, J. Chen, and Z. Wang, “Mining public 
sentiments and perspectives from geotagged social 
media data for appraising the post-earthquake 
recovery of tourism destinations,” Appl. Geogr., vol. 
123, no. August, p. 102306, 2020, doi: 
10.1016/j.apgeog.2020.102306. 

R. A. Baro and T. D. Palaoag, “Disaster Sentiment 
Analysis: Addressing the Challenges of Decision-
Makers in Visualizing Netizen Tweets,” IOP Conf. 
Ser. Mater. Sci. Eng., vol. 803, no. 1, 2020, doi: 

 8 / 9 

 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

[7] 

[8] 

[9] 

10.1088/1757-899X/803/1/012039. 

B. Shah, V. Agarwal, U. Dubey, and S. Correia, 
“Twitter Analysis for Disaster Management,” Proc. - 
2018 4th Int. Conf. Comput. Commun. Control Autom. 
ICCUBEA 2018, pp. 1–4, 2018, doi: 
10.1109/ICCUBEA.2018.8697382. 

M. R. Nair, G. R. Ramya, and P. B. Sivakumar, 
“Usage and analysis of Twitter during 2015 Chennai 
flood towards disaster management,” in Procedia 
Computer Science, 2017, vol. 115, doi: 
10.1016/j.procs.2017.09.089. 

“Siaran Pers : Menparekraf Pastikan Pembangunan 
Lima Destinasi Super Prioritas Dipercepat,” 2020. 
https://kemenparekraf.go.id/berita/Siaran-Pers-%3A-
Menparekraf-Pastikan-Pembangunan-Lima-Destinasi-
Super-Prioritas-Dipercepat. 

 9 / 9 

 
 
"
221709504,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Analisis Efektifitas Influencer  
Studi Kasus : Kebijakan Pemerintah 

Agung Nugraha Marwan (221709504, 4SD2) 
Dosen Pembimbing: Ibnu Santoso S.S.T, M.T. 

Sentralitas  adalah  salah  satu  hal  yang  paling  sering  dipelajari 
dalam Social Network Analysis (SNA). Salah satu tujuan dari SNA 
adalah  untuk  mengidentifikasi  user  atau  orang  yang  memiliki 
pengaruh  kuat  pada  media  sosial.  Mengidentifikasi  user  yang 
memiliki  pengaruh  pada  topik  tertentu  dinilai  lebih  efektif 
daripada  mengidentifikasi  user  yang  dapat  memberikan 
pengaruh pada semua topik. Pada penelitian ini akan ditentukan 
variabel  variabel  yang  dapat  memengaruhi  besarnya  influence 
yang dapat diberikan oleh seorang user, melakukan perhitungan 
influence score pada suatu topik, pembangunan indeks komposit 
yang  akan  mempresentasikan  besarnya  influence  yang  dimiliki 
oleh  masing  masing  user,  dan  melihat  bagaimana  pendapat 
masyarakat  terkait  dengan  suatu  topik.  Diperoleh  bahwa 
masyarakat  secara  umum  memberikan  feedback  positif  terkait 
dengan  kebijakan  pemerintah.  Dalam  menentukan  besarnya 
influence  score  pada  user  dapat  menggunakan  10  variabel  yang 
dibagi  menjadi  ke  dalam  lima  dimensi  yaitu  network,  quality, 
polarity, engagement, frequency. Dan didapatkan terdapat korelasi 
lemah  namun  signifikan  antara  rangking  yang  dihasilkan  oleh 
influence score dan IID yaitu sebesar 0,196.  

Kata  Kunci—  user  influence,  kebijakan  pemerintah,  indeks 

komposit.  

I.  LATAR BELAKANG 

Secara  sederhana  seorang  influencer  dapat  diartikan 
sebagai  user  yang  dapat  memengaruhi  aksi  atau  hal  yang 
dilakukan oleh banyak orang di media sosial, atau dapat juga 
diartikan sebagai orang yang bisa menyebarkan informasi yang 
ada  di  media  sosial  [1].  Pendapat  lain  mengatakan  bahwa 
Influencer  adalah  seorang  user  yang  cenderung  untuk 
mempengaruhi orang lain yang ada di sekitarnya [2]. Besarnya 
pengaruh  yang  dapat  diberikan  oleh  user  yang  ada  di  media 
sosial  secara sederhana dapat ditentukan dengan cara  melihat 
profil  yang  dimiliki  dari  user  tersebut,  seperti  berapa  banyak 
followers,  following  yang  dimiliki  oleh  user,  atau  dapat  juga 
dengan menghitung rasio dari keduanya [3].  

Pengaruh sosial selalu menjadi hal yang menarik perhatian 
masyarakat umum. Memiliki hubungan erat dengan orang yang  
memiliki  kekuatan  sosial  yang  kuat  dapat  menentukan 
bagaimana  hubungan  dalam  bermasyarakat  sosial  dan  juga 
target pasar yang kita inginkan. Saat ini, target pasar dan juga 
Internet, 
masyarakat 
khususnya di mesia sosial. 

sosial  dapat  dimanifestasikan  di 

Pada saat ini, metode sederhana tersebut telah mengalami 
perkembangan, metode yang paling sering digunakan pada saat 
ini  adalah  Social  Network  Analysis  (SNA).  SNA  bertujuan 
untuk  mengidentifikasi  most  relevant  actors  atau  orang  yang 
memiliki  pengaruh  paling  kuat  dalam  social  network  [4]. 
Dalam  SNA,  social  network  dapat  direpresentasikan  sebagai 

suatu  graph,  dimana  user  direpresentasikan  sebagai  node, 
sedangkan  garis  merepresentasikan  hubungan  yang  dimiliki 
oleh masing masing user seperti melakukan retweet, likes, atau 
replies. 

Untuk  beberapa  kasus,  akan  lebih  baik  mengidentifikasi 
pengaruh dari seorang user berdasarkan topik tertentu. Hal ini 
dapat dilakukan dengan cara melihat dari topik apa yang sering 
dibahas  oleh  user  tersebut.  Menurut  referensi  [5]  metode 
perhitungan yang hanya melihat pada suatu topik tertentu saja 
akan  lebih  efektif  daripada  melakukan  perhitungan  secara 
umum (tanpa topik). Metode perhitungan ini digunakan untuk 
analisis  secara  offline.  Dimana  data  dari  twitter  didapatkan 
melalui  scraping  pada  periode  tertentu.  Tweet  yang  diambil 
sesuai  dengan  topik  yang  ingin  dianalisis  menggunakan 
keyword  atau  kata  kunci  yang  bersesuaian  dengan  topik 
tersebut. 

Walaupun demikian perhitungan terkait dengan influence 
score  tersebut  masih  sering  mengalami  kritik  oleh  berbagai 
pihak,  khususnya  terkait  dengan  skor  yang  dihasilkan  yang 
bertujuan  untuk  menentukan  apakah  user  yang  bersangkutan 
memiliki pengaruh atau tidak terkait dengan suatu topik [3]. 

Oleh  karena  itu,  dalam  penelitian  ini  akan  dihitung 
influence  score  menggunakan  metode  pada  penelitian 
terdahulu.  Peneliti  juga  akan  membangun  indeks  komposit 
untuk  menghitung  besarnya  influence  yang  dapat  diberikan 
oleh masing masing user terhadap suatu topik.  

itu  pemerintah 

[6].  Oleh  karena 

Topik  yang  akan  digunakan  pada  penelitian  ini  adalah 
terkait  dengan  kebijakan  pemerintah.  Kebijakan  pemerintah 
adalah  kebijakan  yang  ditujukan  untuk  publik  (negara, 
masyarakat  dalam  berbagai  status  serta  untuk  kepentingan 
umum),  baik  itu  dilakukan  secara  langsung  maupun  tidak 
langsung  yang  tercermin  pada  berbagai  dimensi  kehidupan 
publik 
seharusnya 
mendengarkan kebutuhan rakyat, kemudian diteliti, lalu ambil 
kebijakan  yang  mementingkan  dari  rakyat  [7].  Kebijakan 
pemerintah meliputi banyak hal, baik dari segi ekonomi, sosial, 
budaya,  pendidikan,  kesehatan,  politik,  dsb.  Kebijakan 
kebijakan  tersebut  digunakan  sebagai  pilar  pembangunan 
nasional,  peningkatan  kesejahteraan  bagi  masyarakat, 
penguatan ekonomi,  budaya, sosial, dan politik [8]. Sehingga 
kebijakan  yang  diambil  haruslah  dapat  tersampaikan  dengan 
baik  kepada  rakyat  sehingga  rakyat  mengetahui  dan  dapat 
menjalankan kebijakan tersebut dengan baik. 

Pemilihan  topik  terkait  dengan  kebijakan  pemerintah 
didasari  oleh  pemerintah  yang  sejak  tahun  2016  telah 
menggunakan 
influencer  untuk  menyosialisasikan 
kebijakan pemerintah [9].  Hal ini  memancing timbulnya  pro-
kontra  tentang  apakah  menggunakan  jasa  influencer  tersebut 

jasa 

 1 / 8 

 
 
 
 
 
adalah hal yang tepat untuk dilakukan. Pertimbangan tersebut 
semakin kuat dengan adanya temuan pada tahun 2020, dimana 
ICW  (Indonesia  Corruption  Watch)  melaporkan  bahwa 
pemerintah pusat telah  mengeluarkan anggaran belanja untuk 
influencer  mencapai  Rp  90,45  miliar. 
Instansi  yang 
menggunakan 
tersebut  diantaranya  Kementerian 
Pariwisata  dengan  nilai  Rp  77,6  miliar  untuk  22  paket 
pengadaan jasa influencer. Kemendikbud (Rp 1,6 miliar untuk 
12 paket), Kominfo (Rp 10,38 miliar untuk 4 paket), Kemenhub 
(Rp 195,8 juta untuk 1 paket), serta Kemenpora (Rp 150 juta 
untuk 1 paket)[9]. 

jasa 

Dengan besarnya biaya yang dikeluarkan oleh pemerintah 
untuk  menjalankan  kebijakan  tersebut,  maka  perlu  diadakan 
evaluasi, evaluasi kebijakan dapat memberitahukan kepada kita 
apakah  suatu  kebijakan  yang  telah  ditetapkan  berhasil  atau 
gagal,  tingkat  efektivitas  kebijakan,  apakah  sudah  memenuhi 
akuntabilitas publik, menunjukkan pada stakeholders manfaat 
kebijakan tersebut, dan agar tidak mengulangi kesalahan yang 
sama.  

Pada kasus penggunaan jasa influencer evaluasi bertujuan 
untuk  menilai  apakah  influencer  yang  diajak  bekerja  sama 
dalam  menyosialisasikan  kebijakan  pemerintah  dapat 
memberikan hasil yang baik atau tidak. Hal ini dipat dilakukan 
dengan  cara  menghitung  influence  score  terhadap  user  yang 
mengeluarkan tweet terkait dengan kebijakan pemerintah. 

Selain itu, Penilaian masyarakat terkait dengan kebijakan 
pemerintah dapat dilihat dari bagaimana pendapat masyarakat 
terkait  kebijakan  yang sedang dibicarakan.  Metode  Sentiment 
analysis  digunakan  untuk  mengklasifikasikan  sebuah  kalimat 
teks atau tweet dan menentukan apakah teks atau tweet tersebut 
memiliki  sentimen  positif,  negatif,  atau  netral.  Hal  ini 
dilakukan  karena  secara  umum  masyarakat  yang  setuju 
terhadap kebijakan pemerintah cenderung akan mengeluarkan 
tweet  dengan 
sebaliknya 
masyarakat  cenderung  akan  mengeluarkan  tweet  dengan 
sentimen negatif jika tidak setuju dengan kebijakan pemerintah. 

sentimen  positif,  begitupun 

II.  TUJUAN PENELITIAN 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

TABEL I 
Tabel literatur 

No  Judul 
1 

Sentiment-based 
influence 
detection on 
Twitter 

Penulis, Publikasi 
Bigonha, C., 
Cardoso, T.N.C., 
Moro, M.M. et 
al.(2012) Journal 
of the Brazilian 
Computer Society 

Tertulis 
Untuk menghitung 
influential score memerlukan 
beberapa informasi yaitu 
polaritas dan kualitas dari 
tweet serta network yang ia 
punya di twitter. [10] 

2  Measuring 

influence on 
Twitter 

3  Measuring user 
influence on 
Twitter: A 
survey 

4 

5 

Penyusunan 
Indeks 
Komposit 
Ketergantungan 
Wilayah 
Multidimensi 
Setiap Provinsi 
Tahun 2016 
Indeks 
Komposit 
Pekerjaan 
Tidak Layak Di 
Indonesia 
Tahun 2017 

Anger, Isabel & 
Kittl, Christian. 
(2011).        
I-KNOW. 31. 
10.1145/2024288.
2024326 
F. Riquelme, P. 
González-
Cantergiani 
(2016). Journal of 
Information 
Processing and 
Management 
52(5) 
Octaviani, Erni. 
(2019). [Skripsi]. 
Jakarta : Sekolah 
Tinggi Ilmu 
Statistik 

Santoso, Noto K. 
(2019). [Skripsi]. 
Jakarta : Sekolah 
Tinggi Ilmu 
Statistik 

Social Network Potentials 
menjelaskan tentang besarnya 
peluang untuk berinteraksi 
pada followers di twitter [11]. 

Seseorang dapat memiliki 
pengaruh yang besar pada 
topik topik tertentu namun 
tidak pada topik lainnya [4]  

Indeks komposit adalah 
ukuran yang digunakan 
untuk mengukur perubahan  
atau perbandingan variabel 
ekonomi atau sosial. [12] 

Diperlukan suatu ukuran 
dalam bentuk indeks guna 
membantu pemerintah 
dalam upaya peningkatan 
kesejahteraan para pekerja. 
Ukuran tersebut dibutuhkan 
untuk menggambarkan 
kondisi pekerjaan layak 
serta mengevaluasi program 
pembangunan yang sudah 
ada [13] 

Berdasarkan  masalah  yang  dijelaskan  di  atas,  dapat 
disusun beberapa tujuan dilaksanakannya penelitian ini, yaitu : 
1.  Menentukan  variabel  yang  relevan  untuk  menghitung 

keefektifitasan influencer 

IV. METODE PENELITIAN  
Metode penelitian yang dilakukan dalam penelitian ini 

dapat dilihat pada flowchart berikut :  

2.  Menghasilkan nilai keefektifitasan influencer 
3.  Menyusun indeks komposit keefektifitasan influencer  
4.  Melihat 

pendapat  masyarakat 

terhadap 

kebijakan 

pemerintah 

5.  Mengklasifikasikan  user  berdasarkan  nilai  komposit  yang 

terbentuk 

III. PENELITIAN TERKAIT 

Penelitian ini sedikit banyaknya mendapatkan referensi dan 
terinspirasi  dari  penelitian  penelitian  sebelumnya,  terutama 
pada hal yang berkaitan dengan latar belakang ataupun metode 
yang akan digunakan. Adapun beberapa penelitian terkait yang 
berhubungan dengan penelitian ini antara lain : 

 2 / 8 

 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

4.  Normalisasi.  Normalisasi 

python  merupakan  bahasa  pemrograman  yang  case 
sensitive, oleh karena itu kata “kebijakan” dengan kata 
“Kebijakan” akan dianggap sebagai kata yang berbeda. 
Walaupun kata yang dimaksud adalah kata yang sama. 
teks  digunakan  untuk 
mengubah kata kata singkatan ataupun kata yang salah 
ketik  menjadi  bentuk  sebenarnya.  Misalnya  kata  “yg” 
akan diubah menjadi “yang”, “tdk” menjadi “tidak” dsb. 
5.  Stemming.  Merupakan  proses  yang  dilakukan  untuk 
merubah  kata  menjadi  bentuk  bakunya.  Misalnya 
“menjadi” menjadi “jadi”, “berkuasa” menjadi “kuasa” 
dsb. 

6.  Menghapus 

stopwords. 

kata 
penghubung  seperti  “yang”,  “dan”  dsb.  Kata  kata 
tersebut  dihapus  karena  dapat  mengganggu  proses 
analisis karena frekuensinya yang banyak. 

Stopwords 

atau 

Gambar 1. Flowchart langkah langkah penelitian. 

4.1 Pengumpulan Data 

Data  yang  digunakan  pada  penelitian  ini  merupakan  data 
twitter mengenai kebijakan pemerintah, pengambilan data dari 
twitter  tersebut  menggunakan  keyword  yang  telah  ditentukan 
oleh  peneliti.  Pemilihan  keyword  sendiri  merupakan  hal  yang 
penting karena apabila keyword yang digunakan terlalu umum, 
maka tweet yang terambil akan terlalu banyak dan tidak sesuai 
dengan apa yang di inginkan, sedangkan apabila terlalu sempit 
maka  tweet  yang  terambil  menjadi  sedikit  sehingga  hasilnya 
kurang  dapat  mewakili  keadaan  sebenarnya.  Adapun  keyword 
“RUU,  kebijakan,  kebijakan 
yang  digunakan 
pemerintah, UU, Undang Undang”. 

adalah 

Pengambilan  tweet  yang  berkenaan  tentang  kebijakan 
pemerintah 
ini  dilakukan  dengan  menggunakan  metode 
scraping  twitter,  scraping  twitter  sendiri  dilakukan  dengan 
menggunakan library yang tersedia pada bahasa pemrograman 
python, yaitu  twint. Tweet yang digunakan adalah tweet yang 
memuat  keyword  pada  periode  1  januari  2016  sampai  dengan 
Desember 2020. 

4.2 Preprocessing 

Sebelum  dilakukan  analisis, 

tersebut  di 
preprocessing  terlebih  dahulu.  Rangkaian  yang  dilakukan 
adalah sebagai berikut : 

raw  data 

1.  Menghapus duplikasi data, sebuah tweet akan memiliki 
id_tweet  yang  unik,  dengan  menggunakan 
tweet  yang 

nilai 
id_tweet  sebagai  primary  key  maka 
redundant akan dihapus   

2.  Menghapus  karakter  yang  tidak  dibutuhkan.  karakter 
seperti  ~!@#$^&*()<>{}[].,_+\\|:;’”  tidak  diperlukan 
sehingga  dapat  dihapus.  Link  URL,  angka  0  sampai  9 
juga dihapus karena tidak diperlukan dalam analisis. 
3.  Case Folding. Merupakan proses untuk merubah semua 
kata menjadi huruf non kapital. Hal ini dilakukan karena 

4.3  Menghitung Influence Score 
4.3.1 Network Analysis 

Langkah  selanjutnya  adalah  melakukan  network  analysis. 
Oleh  karena  itu  user  yang  melakukan  retweet  terhadap  tweet 
dari  user  lainnya  maka  kedua  user  tersebut  akan  terhubung. 
Setelah semua user berhasil dihubungkan dan menjadi sebuah 
graph,  network  analysis  yang  dilakukan  adalah  dengan 
menghitung  Betweenness 
(bc),  Eigenvector 
centrality(ec), dan In-degree (id) dan Follower-Followee Ratio 
(TFF). Untuk menghitung besarnya network yang user miliki, 
maka model perhitungan yang dilakukan adalah : 

centrality 

𝑢𝑛𝑒𝑡𝑤𝑜𝑟𝑘 =

𝑏𝑐+𝑒𝑐+𝑖𝑑+𝑡𝑓𝑓

4

(1) 

4.3.2 Quality Analysis dan Sentiment Analysis  

Tahapan  berikutnya  adalah  melakukan  quality  analysis. 
Kualitas yang akan dihitung adalah kualitas dari masing masing 
tweet.  Metode  perhitungan  kualitas  yang  digunakan  adalah 
Flesch-Kincaid dengan menggunakan rumus sebagai berikut : 

𝐾𝑢𝑎𝑙𝑖𝑡𝑎𝑠 = 0.39 𝑥 (

𝑡𝑜𝑡𝑎𝑙 𝑘𝑎𝑡𝑎

𝑡𝑜𝑡𝑎𝑙 𝑘𝑎𝑙𝑖𝑚𝑎𝑡

) + 11.8 𝑥 (

𝑡𝑜𝑡𝑎𝑙 𝑠𝑢𝑘𝑢 𝑘𝑎𝑡𝑎

𝑡𝑜𝑡𝑎𝑙 𝑘𝑎𝑡𝑎

) − 15.59 

(2) 

Setelah  melakukan  analisis  terhadap  kualitas  dari  sebuah 
tweet,  langkah  selanjutnya  adalah  melakukan  analisis  terkait 
tersebut.  Menggunakan  metode 
tweet 
sentiment  dari 
perhitungan  analisis  sentimen  berbasis 
lexicon  (kamus). 
Didalam  kamus  tersebut  terdapat  kata  dengan  nilai  polaritas 
yang  dimiliki  oleh  kata 
ini 
menggunakan pencocokkan  kata  yang ada pada  tweet dengan 
kata  yang ada pada kamus.  Apabila skor polaritas pada  tweet 
tersebut  >  0,  maka  tweet  tersebut  merupakan  tweet  dengan 
sentimen positif, begitu juga apabila polaritas < 0, merupakan 
tweet  dengan  sentimen  negatif,  dan  nilai  polaritas  =  0 
merupakan tweet dengan sentimen yang netral.  

tersebut.  Metode 

lexicon 

Setelah didapatkan kualitas dan juga sentimen dari masing 
masing  tweet,  kemudian  dihitung  kualitas  dan  sentimen  dari 

 3 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

masing  masing  user  dengan  cara  menghitung  rata  rata  dari 
kualitas dan juga sentimen tweet yang user tersebut keluarkan 

kemudian  dikalikan  100  untuk  mempermudah  dalam 
interpretasi [16]. 

𝑢𝑞𝑢𝑎𝑙𝑖𝑡𝑦 = ∑

𝑖≤𝑛𝑢
𝑖=1

𝑘𝑢𝑎𝑙𝑖𝑡𝑎𝑠𝑖

𝑥

1
𝑛𝑢

𝑢𝑝𝑜𝑙𝑎𝑟𝑖𝑡𝑦 = ∑

𝑖≤𝑛𝑢
𝑖=1

𝑠𝑒𝑛𝑡𝑖𝑚𝑒𝑛𝑖

𝑥

1
𝑛𝑢

4.3.3 Perhitungan Influence Score 

   (3) 

   (4) 

Setelah  semua  variabel  didapat,  proses  terakhir  adalah 
menghitung  influential  score  untuk  masing  masing  user, 
metode  perhitungan  yang  digunakan 
adalah  metode 
perhitungan  Bigonha,  C  [7].  Influential  score  dapat  dihitung 
dengan cara : 

𝐼𝑠𝑢 =

𝛼 𝑥  𝑢𝑝𝑜𝑙𝑎𝑟𝑖𝑡𝑦+ 𝜑 𝑥 (𝛽 𝑥 𝑢𝑛𝑒𝑡𝑤𝑜𝑟𝑘+ 𝛾 𝑥 𝑢𝑞𝑢𝑎𝑙𝑖𝑡𝑦) 
𝛼+ 𝛽+𝛾

   (5) 

4.4  Analisis Deskriptif 

 Metode  analisis  deskriptif  digunakan  untuk  melihat 
gambaran umum pendapat masyarakat terkait dengan kebijakan 
pemerintah.  Data  dibagi  menjadi  tiga  bagian  yaitu  data  atau 
tweet yang yang memiliki sentimen positif, negatif atau netral. 
Dari  ketiga  data  tersebut  kemudian  dihitung  persentase  dari 
tweet atau user yang bersentimen positif, negatif, atau netral. 

4.5  Menyusun Indeks Komposit 

Tahapan  penyusunan  indeks  menggunakan  metode  yang 

digunakan pada referensi [14] adalah sebagai berikut : 
1.  Membangun kerangka kerja teoritis dan pemilihan variabel 

/ indikator. 
jelas  mendefinisikan 
Kerangka  kerja  harus  secara 
fenomena  yang  akan  diukur  dan  sub-komponennya,  dan 
selanjutnya  memilih  masing-masing  indikator  dan  bobot 
yang  mencerminkan  kepentingan  relatif  mereka  dan 
dimensi komposit keseluruhan [15]. 

2.  Standardisasi data 

Metode normalisasi  yang digunakan dalam penelitian ini 
adalah normalisasi Min-max. Metode Min-Max mengubah 
nilai variabel tersebut menjadi nilai antara 0 sampai 1. 

3.  Analisis faktor 

Pada  tahapan  ini  dilakukan  beberapa  hal  seperti  menguji 
kelayakan  dari  data  yang  akan  digunakan  uji  kelayakan 
data, melakukan reduksi atau pengurangan dari variabel / 
indikator,  membentuk  faktor,  mengestimasikan  nilai 
matriks faktor yang terbentuk, melakukan rotasi dari faktor 
yang terbentuk, dan menghitung nilai skor faktor [12]. 

4.  Standardisasi data skor faktor 

Pada  masing  masing  model  faktor  yang 
terbentuk 
dilakukan  lah  standardisasi,  standarisasi  data  skor  faktor 
digunakan  untuk  memperkecil  range  nilai  faktor  indeks 
menjadi  nilai  0  sampai  1.  Dalam  penelitian  ini  metode 
normalisasi data yang digunakan adalah metode Min-Max 

′ =

𝑓𝑖𝑗

[𝑓𝑖𝑗−𝑀𝑖𝑛(𝑓𝑖)]
[𝑀𝑎𝑥(𝑓𝑖)−𝑀𝑖𝑛 (𝑓𝑖)]

 𝑥 100 

               (6)  

Keterangan : 
𝑓′𝑖𝑗          : skor faktor ke-i user ke-j setelah standardisasi 
𝑓𝑖𝑗           : skor faktor ke-i user ke-j 
𝑀𝑎𝑥(𝑓𝑖) : skor maksimal faktor ke-i 
𝑀𝑎𝑥(𝑓𝑖) : skor minimal faktor ke-i 

5.  Menentukan penimbang yang akan digunakan. 

Penimbang  yang  digunakan  dalam  penelitian  ini  adalah 
unequal weighting dan equal weighting. Equal weighting 
digunakan pada variabel yang ada di masing masing faktor, 
sedangkan  unequal  weighting  digunakan  pada  faktor 
faktor yang terbentuk. 
a.  Pembobotan skor faktor 

Untuk masing masing variabel yang ada pada sebuah 
faktor,  akan  dilakukan  metode  perhitungan  equal 
weighting  untuk  mendapatkan  nilai  dari  faktor 
tersebut  

′ =
𝒇𝒊

𝟏

𝒕

𝒕
 ∑ 𝒀′𝒊𝒋
𝒋=𝟏

(7)  

keterangan : 
′  
𝑓𝑖
𝑌′𝑖𝑗  
t 

: Nilai faktor ke-i 
: Nilai variabel ke-j faktor ke-i 
: Banyaknya variabel pada faktor ke-i 

b.  Pembobotan faktor 

Pada  pembobotan  faktor,  metode  yang  digunakan 
adalah  unequal  weighting  dimana  Penimbang 
didapatkan  dari  proporsi  explained  variance  oleh 
setiap faktor dengan total explained variance [15] 

𝐸𝑥𝑝𝑙𝑎𝑖𝑛𝑒𝑑 𝑣𝑎𝑟𝑖𝑎𝑐𝑒𝑖
𝑇𝑜𝑡𝑎𝑙 𝑒𝑥𝑝𝑙𝑎𝑖𝑛𝑒𝑑 𝑣𝑎𝑟𝑖𝑎𝑛𝑐𝑒

𝑊𝑖 =
∑ 𝑊𝑖 = 1  

               (8) 

6.  Agregasi. 

Menghitung  nilai  dari  influence  score.  Dalam  penelitian 
ini,  metode  perhitungan  yang  digunakan  adalah  agregasi 
linear  (linear  aggregation)  [12].  Dimana  menjumlahkan 
semua faktor yang telah dikalikan dengan penimbangnya 
masing masing  

′
𝐼𝑛𝑑𝑒𝑘s =   ∑ 𝑊𝑖 𝑓𝑖𝑗

𝑘
𝑖=1

(9) 

keterangan : 
𝐼𝑛𝑑𝑒𝑘𝑠   : Indeks influence pada seorang user 
𝑊𝑖               : bobot faktor ke-i 
′           : skor faktor ke-i 
𝑓𝑖

7.  Melihat hubungan indeks komposit dengan indeks lain. 

Tahapan  ini  ditujukan  untuk  menilai  seberapa  baik 

 4 / 8 

 
 
 
  
 
 
 
    
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
  
 
 
  
 
 
 
kemampuan 
indeks  komposit  dalam  menjelaskan 
fenomena yang terjadi. Dalam penelitian ini, akan dilihat 
hubungan antara nilai influence score yang dihasilkan dari 
indeks  komposit  dengan  influence  score  dari  metode 
referensi [10]. Melihat hubungan serta besarnya hubungan 
dapat  dilakukan  dengan  cara  melakukan  analisis  uji 
korelasi rank spearman 

𝑟𝑠 = 1 −

6  ∑

𝑛
2
𝑑𝑖
𝑖=1
𝑛(𝑛−1)

(10) 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

VI. HASIL DAN PEMBAHASAN 

6.1 Pengumpulan Data dan Preprocessing 

Data tweet dikumpulkan dari tweet terkait dengan kebijakan 
pemerintah  sejak  1  Januari  2016  sampai  dengan  Desember 
2020.  Data  tweet  yang  didapatkan  dari  hasil  scraping  twitter 
ada  sebanyak  457389  tweet.  Selanjutnya  tweet  tersebut  akan 
melewati  tahap  preprocessing.  Setelah  mengalami  proses 
preprocessing,  data  yang  ada  menjadi  262367  tweet.  Berikut 
merupakan  contoh  sampel  data  tweet  sebelum  dilakukan 
preprocessing dan setelah preprocessing : 

TABEL II 
Contoh sampel data tweet sebelum preprocessing 

Uji signifikansi korelasi Rank Spearman : 
H0  : Korelasi pada populasi adalah nol  
H1  : Korelasi pada populasi lebih besar dari nol 

Statistik hitung untuk data lebih dari 30 : 

𝑧 = 𝑟𝑠√𝑛 − 1  

keterangan : 
z  : Nilai z hitung 
rs : Koefisien korelasi spearman 
n  : Jumlah sampel penelitian 

id_tweet 

(11) 

947359620070117377 

947169197603172357 

4.7 Pengelompokkan User 

947595114251141120 

tweet 
fadli  zon  selalu  menyalahkan  kebijakan 
pemerintah, padahal dia sendiri apa yg dia 
perbuat  utk  bangsa  ini  selama  menjadi 
pimpinan  maupun  anggota  dpr?  kecuali 
hanya menyalahkan kebijakan pemerintah. 
pemerintah melalui kementerian 
perindustrian tengah menyiapkan 
kebijakan relaksasi impor bagi industri 
kecil dan menengah (ikm) #kemenprim 
#industri  https://t.co/5xltrxm3uv 
@fahrihamzah anggota parlemen 
bukannya mendukung kebijakan 
pemerintah ..dan ga berbuat apa2. uu 
hasilnya berapa th 2107 ..malu dong 
kesana kemari cuman pake jas .. 

Pengelompokkan dilakukan berdasarkan variabel variabel 
yang  digunakan  dalam  menghitung  indeks  komposit.  Metode 
yang  digunakan  dalam  pengelompokkan  adalah  dengan  K-
Means Clustering. Dalam menentukan banyak klaster terbaik, 
peneliti menggunakan tiga metode yaitu within cluster sums of 
squares, average silhouette, dan gap statistics.  

V.  KERANGKA PIKIR 

Kerangka pemikiran adalah dari cara berpikir dari tinjauan 
teori  yang  mencerminkan  hubungan  antara  masing  masing 
variabel dan merupakan panduan untuk memecahkan masalah 
penelitian. Menurut [17] kerangka berpikir merupakan  model 
konseptual  tentang  bagaimana  teori  berhubungan  dengan 
berbagai faktor yang telah diidentifikasi sebagai masalah yang 
penting. 

Gambar 2. Flowchart langkah langkah penelitian 

TABEL III 
Contoh sampel data tweet setelah preprocessing 

id_tweet 

tweet 

947359620070117377 

947169197603172357 

947595114251141120 

fadli zon selalu salah bijak perintah 
padahal sendiri apa yg buat utk bangsa 
lama jadi pimpin maupun anggota dpr 
hanya salah bijak perintah 
perintah lalu menteri industri tengah siap 
bijak relaksasi impor industri kecil tengah 
ikm kemenprim industri 
anggota parlemen bukan dukung bijak 
perintah tidak buat uu hasil th malu sana 
kemari cuman pake jas 

6.2 Perhitungan Influence Score 

Setelah  data  tweet  selesai  di-preprocessing.  Selanjutnya 
adalah  melakukan  analisis  untuk  menghitung  nilai  dari 
influence  Score,  adapun  untuk  menghitung  nilai  tersebut 
diperlukan  tiga  variabel,  yaitu  network,  polarity,  dan  quality 
dan masing masing user. Untuk polarity dan quality dilakukan 
perhitungan pada masing masing tweet terlebih dahulu. Berikut 
merupakan  contoh  sampel  data  sentimen  dan  kualitas  dari 
tweet : 

TABEL IV 
Contoh sampel data sentimen dan kualitas tweet  
sentimen 

id_tweet 
947359620070117377 
947169197603172357 
947595114251141120 
947143881920933888 
945884922455121921 

-0.340425531914894 
0.142857142857143 
-0.127659574468085 
0.339285714285714 
0.553571428571429 

kualitas 

15 
20.5 
12.3 
15 
17 

 5 / 8 

 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Setelah  perhitungan  terkait  sentimen  dan  kualitas  tweet 
dilakukan, kemudian nilai tersebut dikelompokkan berdasarkan 
usernya,  sehingga  akan  didapat  nilai  polaritas  dan  kualitas 
tweet dari masing masing user.  

Perhitungan nilai influence score dilakukan setelah variabel 
sentiment, polarity, dan network dari user didapatkan. Berikut 
merupakan contoh sampel  data network, quality, polarity  dan 
influence square dari use : 

TABEL V 
Contoh sampel data network, quality, polarity dan influence square dari user 
u_quality 

u_network 

No 
1 
2 
3 
4 
5 

username 
jokowi 
detikcom 
pln_123 
DennyJA_WORLD 
kompascom 

1 
0.51433 
0.31092 
0.29199 
0.24387 

0.08171 
0.06344 
0.07045 
0.06424 
0.05899 

No 
1 
2 
3 
4 
5 

username 
jokowi 
detikcom 
pln_123 
DennyJA_WORLD 
kompascom 

u_polarity 

0.09689 
0.07250 
0.29580 
-0.00722 
0.09169 

influence_score 
0.81786 
0.42506 
0.28536 
-0.24074 
0.21016 

6.3 Pendapat Masyarakat Terkait Kebijakan Pemerintah 

Untuk  melihat  gambaran  umum  terkait  dengan  pendapat 
masyarakat  terhadap  kebijakan  pemerintah  maka  dilakukan 
analisis  deskriptif.  Pada  analisis  ini  data  dibagi  menjadi  tiga 
berdasarkan sentimen yang dimiliki yaitu positif, negatif, dan 
netral. Berikut gambaran pendapat masyarakat terkait dengan 
kebijakan pemerintah jika dilihat berdasarkan user nya : 

Sentimen User

3%

28%

69%

Positive

Negative

Netral

Gambar 3. Persentase sentimen yang dimiliki oleh user terhadap kebijakan 
pemerintah 

Selain dengan melihat sentimen yang dimiliki oleh user, juga 

dilihat proporsi sentimen dari tweet : 

Sentimen Tweet

5%

25%

70%

Positive

Negative

Netral

Gambar 4. Persentase sentimen yang dimiliki oleh masing masing tweet 
terhadap kebijakan pemerintah 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Dari gambaran kedua grafik diatas, dapat dilihat bahwa tidak 
terlalu  banyak  perbedaan  polaritas  antara  user  dengan  tweet 
yang ada terkait dengan kebijakan pemerintah. 

Secara umum pendapat masyarakat terkait dengan kebijakan 
pemerintah sudah cukup baik, hal ini dapat dibuktikan dengan 
melihat  banyaknya  tweet  yang  memiliki  polaritas  positif 
dibandingkan  dengan  tweet  yang  memiliki  polaritas  negatif. 
Hal  yang  sama  juga  dapat  dilihat  dari  lebih  banyaknya  user 
yang secara umum berpendapat positif terkait dengan kebijakan 
pemerintah daripada user yang berpendapat negatif. 

6.4 Membangun Indeks Komposit 

Dengan  keterbatasan  penelitian,  data  yang  digunakan 
untuk membuat indeks komposit ini adalah 1% data teratas dari 
influence score  yang  sudah didapatkan  menggunakan  metode 
sebelumnya  yang  sudah  diurutkan.  Indeks  komposit  terkait 
besarnya influence yang diberikan oleh seorang user terhadap 
kebijakan  pemerintah  dibentuk  oleh  10  variabel,  yaitu  yaitu 
Proporsi  Tweet,  Proporsi  Retweet,  Proporsi  Replies,  Proporsi 
Likes,  Betweenness  Centrality,  Eigenvector  Centrality,  In-
Degree  Centrality,  Twitter  Follower-Following  Ratio, 
Polaritas Tweet, dan Kualitas Tweet. 

Variabel Betweenness Centrality, Eigenvector Centrality, 
In-Degree  Centrality,  Twitter  Follower-Following  Ratio, 
Polaritas  Tweet,  dan  Kualitas  Tweet  didapatkan  dari  hasil 
perhitungan pada metode sebelumnya.  

Variabel  Proporsi  Tweet,  Proporsi  Retweet,  Proporsi 
Replies dan Proporsi Likes didapatkan dengan cara menghitung 
proporsi banyaknya tweet, retweet, replies, likes tentang tweet 
yang berkaitan dengan kebijakan pemerintah terhadap seluruh 
tweet  yang  dikeluarkan  oleh  user  dalam  rentang  waktu 
penelitian. 

𝑃𝑟𝑜𝑝𝑜𝑟𝑠𝑖 𝑋 =

𝐵𝑎𝑛𝑦𝑎𝑘𝑛𝑦𝑎 𝑋 𝑠𝑒𝑠𝑢𝑎𝑖 𝑡𝑜𝑝𝑖𝑘

𝑇𝑜𝑡𝑎𝑙 𝑋 𝑝𝑎𝑑𝑎 𝑝𝑒𝑟𝑖𝑜𝑑𝑒 𝑝𝑒𝑛𝑒𝑙𝑖𝑡𝑖𝑎𝑛

(12)   

keterangan  
X : tweet, atau  retweet, atau likes, atau replies 

6.4.1 Pemilihan Variabel 

Pemilihan  variabel  bertujuan  untuk  menyeleksi  variabel 
yang  akan  digunakan  untuk  membentuk  dimensi  indeks 
komposit influence score terkait dengan kebijakan pemerintah. 
10  variabel  pembentuk  indeks  komposit  yang  akan 
digunakan  perlu  diseragamkan  terlebih  dahulu  arahnya  untuk 
mempermudah  dalam  proses  interpretasi  [12].  Dalam  hal  ini 
semua variabel diubah arahnya menjadi positif. 

selanjutnya 

Setelah  dilakukan  penyeragaman  arah,  langkah  yang 
dilakukan 
adalah  melakukan  normalisasi. 
Normalisasi dilakukan karena setiap variabel memiliki satuan 
yang berbeda-beda dan memiliki nilai range atau rentang yang 
berbeda  beda  pula,  sehingga  dilakukannya  normalisasi 
bertujuan untuk mengurangi bias yang terjadi. 

Indikator  yang  telah  diseragamkan  dan  dinormalisasi 
dianalisis  menggunakan  analisis  multivariat  yaitu  analisis 
faktor.  Analisis  faktor  yang  dilakukan  bertujuan  untuk 
lebih  ringkas  dan 
mereduksi  dimensi  variabel  menjadi 
akan 
membentuk 

variabel 

reduksi 

faktor, 

proses 

 6 / 8 

 
 
 
 
 
 
 
 
 
 
 
    
 
 
memperhatikan nilai KMO, MSA, komunalitas serta Bartlett’s 
Test of Sphericity. 

Nilai KMO yang kurang dari 0.5 menandakan bahwa data 
tersebut belum layak untuk dianalisis. Dalam proses mereduksi 
variabel,  peneliti  memperhatikan  nilai  MSA  dan  nilai 
komunalitas, variabel yang memiliki nilai MSA dan komunal 
kurang  dari  0.500  menandakan  hubungan  yang  kurang  kuat 
antara  variabel  tersebut  dengan  variabel  lainnya,  sehingga 
variabel tersebut dapat dikatakan tidak layak untuk dimasukkan 
ke  dalam  model  dan  dianalisis  lebih  lanjut,  sehingga  harus 
direduksi atau dikeluarkan [13]. 

Berdasarkan  proses  reduksi  variabel  yang  dilakukan, 
didapatkan  3  variabel  yang  tereduksi  yaitu  variabel  Proporsi 
Tweet, Twitter Follower-Following Ratio, Polaritas Tweet, dan 
Kualitas Tweet. 

6.4.2 Pembentukan Faktor dan Penimbang Faktor  

Melalui analisis multivariat analisis faktor didapatkan juga 
terdapat 2 dimensi atau faktor yang terbentuk. Penentuan faktor 
ini didasarkan pada nilai eigen value yang lebih kecil atau sama 
dengan  1,  maka  faktor  tersebut  dianggap  dominan  dan 
signifikan [15]. 

Weighting pada masing masing faktor didapatkan melalui 
perbandingan antara besarnya keragaman yang dapat dijelaskan 
(percentage  of  variance  explained)  oleh  suatu  faktor  dan 
besarnya keragaman total yang dapat dijelaskan oleh model. 

Dalam penelitian ini tidak dilakukan rotasi  faktor karena 
peneliti  dapat  dengan  mudah  melakukan  klasifikasi  faktor 
untuk  masing  masing  variabel  yang  ada,  hal  ini  dikarenakan 
dari  hasil  yang  didapatkan  satu  variabel  hanya  memiliki 
korelasi  yang 
tinggi  pada  satu  faktor  saja.  Ringkasan 
perhitungan dapat dilihat pada Tabel berikut  

TABEL VI 
Ringkasan pembentukan dan penimbangan faktor 

No 

Nama Faktor 

Variabel 

1 

2 

Engagement 
Rate 

Network 
Analysis 

Proporsi Retweet 
Proporsi Replies 
Proporsi Likes 
Betweenness 
Centrality 
Eigen Centrality 
In-Degree 
Centrality 

% of 
variance 

Penimbang 

49.7 

0.518 

46.1 

0.482 

6.4.3 Perhitungan Indeks Influence Score 

Dalam  memperoleh  nilai  indeks  komposit  dilakukanlah 
penjumlahan hasil perkalian antara penimbang dan skor faktor 
untuk tiap tiap faktor yang sudah dinormalisasi. 

𝐼𝐼𝑆 = 0.518 ∗ 𝐹𝐴𝐾𝑇𝑂𝑅1 + 0.482 ∗ 𝐹𝐴𝐾𝑇𝑂𝑅2 

Karena  sebelumnya  telah  dilakukan  penyeragaman  arah 
variabel  menjadi  positif,  maka  interpretasi  dari  indeks  yang 
dihasilkan  adalah  semakin  tinggi  nilai  indeks  influence  score 
terkait  dengan  kebijakan  pemerintah  ini  maka  semakin  besar 
pula pengaruh yang dapat diberikan oleh seorang  user terkait 
dengan kebijakan pemerintah.  

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Hasil perhitungan indeks komposit influence score terkait 

dengan kebijakan pemerintah dapat dilihat sebagai berikut : 

TABEL VII 
Contoh sampel influence score yang didapat dari indeks komposit 

User 

Indeks Influence Score 

jokowi 
detikcom 
kompascom 
tempodotco 
CNNIndonesia 

0.89206 
0.58726 
0.53032 
0.51892 
0.47037 

No 
1 
2 
3 
4 
5 

6.4.4 Uji Korelasi Rank Spearman 

Untuk melihat seberapa besar korelasi dari nilai influence 
score  yang  dihasilkan  dari  indeks  komposit  dengan  metode 
pada referensi [10] dilakukan uji korelasi rank spearman untuk 
membandingkan ranking yang dihasilkan pada masing masing 
metode. 

Dari  perhitungan  didapatkan  besarnya  korelasi  rank 
spearman  antara  kedua  indeks  tersebut  adalah  sebesar  0.196. 
Dilanjutkan  melakukan  uji  signifikansi  dan  menghasilkan 
keputusan  tolak  H0  atau  korelasi  yang  dihasilkan  signifikan 
lebih  besar  dari  0.  Sehingga  kesimpulan  yang  dapat  diambil 
adalah  terdapat  cukup  bukti  untuk  menyatakan  bahwa  kedua 
indeks  tersebut  memiliki  hubungan  walaupun  korelasi  yang 
terbentuk termasuk korelasi kecil / lemah. 

6.5 Klasifikasi user 

Variabel variabel yang digunakan untuk menghitung indeks 
komposit,  selanjutnya  digunakan  untuk  klasifikasi.  Dengan 
menggunakan metode  within cluster sums of squares, average 
silhouette,  dan  gap  statistics.  didapatkan  banyaknya  klaster 
terbaik  adalah  2.  Adapun  gambaran  terkait  variabel  untuk 
masing masing klaster dapat dilihat sebagai berikut : 

Gambar 5. Radar plot variabel pembentuk indeks komposit 

Dari  radar  plot  diatas  dapat  disimpulkan  bahwa  pada 
kelompok  1  variabel  variabel  pembentuk  indeks  komposit 
memiliki  nilai  yang  tinggi,  sedangkan  untuk  kelompok  2, 
variabel  pembentuk  indeks  komposit  memiliki  nilai  yang 
rendah.  Dari  gambaran  diatas, 
indeks  komposit 
dikelompokkan  berdasarkan  klaster  yang  dimiliki,  maka 
didapatkan ringkasan sebagai berikut : 

jika 

 7 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
TABEL VIII 
Ringkasan banyak anggota dan rata rata kelompok klasifikasi 

Kelompok 

Banyak Anggota 

Kelompok 1 
Kelompok 2 

30 
351 

Rata Rata Nilai 
Influence Score 

0.39050896 
0.00732986 

Dari hasil  yang  ringkasan  didapatkan, bahwa  kelompok 1 
adalah  kelompok  user  yang  memiliki  influence  score  yang 
tinggi,  sedangkan kelompok  2 memiliki  influence score  yang 
rendah. 

VII. 

PENUTUP 

7.1 kesimpulan 

Berdasarkan  hasil  dan  pembahasan  pada  penelitian  yang 
telah  diuraikan,  peneliti  mengambil  beberapa  kesimpulan 
sebagai berikut : 

1.  Proporsi  tweet,  proporsi  retweet,  proporsi  replies, 
proporsi  likes,  betweenness  centrality,  eigenvector 
centrality, 
follower-
following  ratio,  kualitas  dan  polaritas  adalah  variabel 
untuk yang mengetahui besarnya  influence yang dapat 
diberikan oleh seorang user atau influencer.  

In-degree  centrality, 

twitter 

2.  Berhasil menghasilkan nilai keefektivitasan influencer. 
3.  Berhasil  menyusun  indeks  komposit  keefektivitasan 
influencer  dengan  terdapat  korelasi  lemah  namun 
signifikan pada rangking yang dihasilkan oleh influence 
score dan Indeks Influence score (IIS). 

4.  Masyarakat secara umum memberikan feedback positif 

5. 

terkait dengan kebijakan pemerintah. 
Indeks komposit diklasifikasikan menjadi 2 kelompok, 
kelompok  pertama  berjumlah  30  user  dan  kelompok 
kedua berjumlah 351 user. 

7.2 Saran 
Berdasarkan  penelitian  yang  telah  dilakukan,  saran  yang 

dapat diberikan penulis adalah sebagai berikut : 

1.  Bagi  pemerintah, 

indeks 

influence  score  dapat 
digunakan  sebagai  alat  bantu  dalam  pengambilan 
keputusan  yang  berkaitan  dengan  penggunaan  jasa 
influencer 
kebijakan 
untuk  menyosialisasikan 
pemerintah 

2.  Apabila ingin menyewa jasa influencer untuk sosialisasi 
influencer 

kebijakan  pemerintah,  maka  pastikan 
tersebut memiliki kredibilitas dibidang tersebut. 

3.  Penelitian  selanjutnya  perlu  mempelajari  lebih  dalam 
lagi  untuk  penyempurnaan  variabel-variabel  apa  saja 
yamg dapat digunakan dalam pengukuran IIS. 

4.  Perlu dilakukan uji yang lebih jauh untuk menentukan 
apakah 
telah  sesuai  untuk 
terbentuk 
menjelaskan fenomena atau kejadian yang sebenarnya. 

IIS  yang 

DAFTAR PUSTAKA 
[1]  Morone, F. , & Makse, H. A. (2015). Influence maximization in complex 

networks through optimal percolation. Nature, 524 , 65–68 . 

[2]  Katz E,  Lazarsfeld P, CUB of Applied Social Research (1955) Personal 
influence: the part played by people in the flow of mass communications. 
Foundations of communications research. Free Press, New York 

[3]  Herrera, R. , Cárdenas, J. P. , & Alfaro, R. (2014). User influence in on-
line  social  networks.  WACS  2014,  IV  workshop  on  agents  and 
collaborative systems. jornadas chilenas de computación (JCC). november 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

8-14, 2014, talca, chile . Herzig, J. , Mass, Y. , & Roitman, H. (2014). An 
author-reader influence 

[4]  F.  Riquelme,  P.  González-Cantergiani  (2016).  Measuring  user  influence 
on Twitter: A survey. Journal of Information Processing and Management 
52(5). 

[5]  Kardara,  M.  ,  Papadakis,  G.  ,  Papaoikonomou,  A.  ,  Tserpes,  K.  ,  & 
Varvarigou,  T.  A.  (2015).  Large-scale  evaluation  framework  for  local 
influence theories in twitter. Information Processing and Management, 51 
(1), 226–252 . 

[6]  Faried Ali, Studi kebijakan Pemerintah. Bandung: Refika Aditama, 2012. 
[7]  Rondonuwu, I.T. (2017). Kajian Hukum akuntabilitas Anggota Legislative 
Ditinjau  Dari  Undang  Undang  No.17  Tahun  2014  Tentang  MPR,  DPR, 
DPD & DPRD (UU MD3).  LEX CRIMEN Vol 6, No 5. 

[8]  Hayat.  (2019,  11).  “Pentingnya  Kebijakan  Publik  Dalam  Kehidupan 
Bernegara”. 
: 
https://www.kompasiana.com/galleryqueen/5dc9e018d541df312b72deb2/
pentingnya-kebijakan-publik-dalam-kehidupan-
bernegara?page=all#:~:text=Kebijakan%20publik%20juga%20menjadi%
20pilar,keteraturan%20sosial%20dan%20berujung%20pada. 

[Online]. 

Avaiable 

[9]  BBC (2020,8). “Pemerintah Indonesia bayar influencer Rp 90 miliar untuk 
sosialisasi  kebijakan,  ‘buang  duit  yang  efektif?’”.  [Online].  Available  : 
https://www.bbc.com/indonesia/indonesia-53846128 

[10] Bigonha, C., Cardoso, T.N.C., Moro, M.M. et al (2012). Sentiment-based 
influence  detection  on  Twitter. Journal  of  the  Brazilian  Computer 
Society Vol 18, 169–183.  

[11] Anger, Isabel & Kittl, Christian. (2011). Measuring influence on Twitter. 
International  Conference  on  Knowledge  Management  and  Knowledge 
Technologies (i-KNOW ’11). Association for Computing Machinery, New 
York, USA, Article 31, 1-4.  https://doi.org/10.1145/2024288.2024326. 
[12] Santoso,  Noto  K.  (2019).  Indeks  Komposit  Pekerjaan  Tidak  Layak  Di 
Indonesia Tahun 2017 [Skripsi]. Jakarta : Sekolah Tinggi Ilmu Statistik 
[13] Octaviani,  Erni.  (2019).  Penyusunan  Indeks  Komposit  Ketergantungan 
Wilayah  Multidimensi  Setiap  Provinsi  Tahun  2016  [Skripsi].  Jakarta  : 
Sekolah Tinggi Ilmu Statistik 

[14] OECD  [Organization  for  Economic  Co-Operation  and  Development]. 
(2008). Handbook on constructing composite indicators: Methodology and 
user guide. OECD 

[15] Pratiwi,  Riska  E.  (2018).  Pemeringkatan  Provinsi  di 

Indonesia 
Berdasarkan  Indeks  Tumbuh  Kembang  Anak  Tahun  2015  [Skripsi]. 
Jakarta : Sekolah Tinggi Ilmu Statistik 

[16] Haque, M.N., Soonthorndhada, K., Hunchangsith, P., & Kanchanachitra, 
M. (2016). Active ageing level in Thailand: A comparison between female 
and  male 
99-107. 
https://doi.org/10.14456/jhr.2016.14 

edlderly. 

Health 

30(2), 

Res, 

J 

[17] Sugiyono.  (2017).  Metode  Penelitian  Kuantitatif,  Kualitatif,  dan  R&D. 

Bandung : Alfabeta, CV. 

[18] Cha, M.; Haddadi, H.; Benevenuto, F.; and Gummadi, K. 2010. Measuring 
User  Influence  in  Twitter:  The  Million  Follower  Fallacy.  International 
AAAI Conference on Weblogs and Social Media (ICWSM). 

 8 / 8 

 
 
 
 
 
 
"
221709503,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Fuzzy K-Prototype Geographically Weighted 
Clustering Yang Dioptimasi Menggunakan 
Algoritma Genetika Untuk Data Campuran 
Studi Kasus : Indikator Indeks Pembangunan Desa di Kabupaten 
Temanggung Tahun 2018 

Agung Budi Santoso (22170503, 4SD1) 
Dosen Pembimbing: Rani Nooraeni, S.ST, M.Stat 

Ringkasan—  Penelitian  tentang  teknik  clustering  yang 
dapat  digunakan  untuk  data  geografis  dengan  data  yang 
beripe campuran belum pernah dilakukan. Oleh sebab itu, 
penelitian  ini  bertujuan  untuk  mengembangkan  sebuah 
metode clustering yang dapat digunakan untuk menangani 
kedua  hal  tersebut.  Mengkombinasikan  metode  Fuzzy 
Geographically  Weighted  Clustering 
(FGWC)  dengan 
metode  Fuzzy  K-Prototype  (FKP)  dan  mengoptimasi 
matriks  keanggotaan  FKP  menggunakan  Algoritma 
Genetika merupakan metode yang dibangun dan diusulkan 
untuk mencapai tujuan penelitian. Kombinasi algoritma ini 
diberi  nama  Genetika  Fuzzy  K-Prototype  Geographically 
(GA-FKP-GW).  metode  yang 
Weighted  Clsutering 
diusulkan  memiliki  nilai  membership  yang  optimal  dan 
telah berhasil mengklasterkan data geografis yang bertipe 
campuran. Metode GA-FKP-GW telah berhasil diterapkan 
dalam pemetaan potensi desa yang menggunakan indikator 
penghitungan IPD Tahun 2018 di Kabupaten Temanggung. 
289 desa dikelompokkan menjadi 4 klaster di mana klaster 
1 dan 3 didominasi oleh desa mandiri sedangkan klaster  2 
dan  4  didominasi  oleh  desa  berkembang.  Berdasarkan 
indikator  yang  mencirikan  setiap  klaster  yang  terbentuk,  
klaster  1  menjadi  klaster  dengan  pelayanan  kesehatan 
terbaik dan klaster 3 menjadi klaster dengan infrastruktur  
ekonomi  terbaik.  .  Klaster  4  menjadi  klaster  dengan 
prasarana dan infrastruktur yang lebih baik dibandingkan 
klaster 2, di mana menjadi klaster dengan rata-rata jarak 
ke  prasarana  dan  infrastruktur  lain  paling  jauh  dengan 
akses jalan yang mudah. 

Kata  Kunci—  K-Prototype,  FGWC,  GA-FKP,GA-FKP-

GW. 

I.  LATAR BELAKANG 

Metode  clustering  saat 

ini  penting  digunakan  untuk 
menganalisis  suatu  wilayah.  Penggunaan  metode  clustering 
yang  tepat  untuk  dapat  menggambarkan  karakteristik  suatu 
wilayah dengan lebih tepat dan lebih unik antar kelompoknya 
perlu  dikaji  lebih  dalam  lagi.  Salah  satu  metode  yang  sering 
digunakan  untuk  menganalisis  suatu  wilayah  adalah  Analisis 
Geo-Demografi.  Geo-demografi  sendiri  adalah  analisis 
karakteristik atau pola orang-orang atau penduduk berdasarkan 
tempat  di  mana  mereka 
[2].  Metode  Fuzzy 
Geographically  Weighted  Clustering  (FGWC)  meruapakan 
salah satu metode clustering yang cocok digunakan untuk AGD 

tinggal 

dengan  memperhitungkan  pengaruh  efek  spasial  yang  berupa 
jumlah  populasi  dan  jarak  antar  wilayahnya.  Namun,  metode 
FGWC  yang  telah  digunakan  sampai  saat  ini  hanya  terbatas 
untuk  data  numerik  saja,  hal  ini  menjadi  sebuah  tantangan 
apabila dihadapkan pada data geografis yang bertipe campuran. 
Analisis  Geo-Demografi  merupakan  pengembangan  dan 
aplikasi topologi area yang digunakan untuk memahami variasi 
pola  perilaku  konsumen  dan  pemasaran.  AGD  merupakan 
kombinasi  dari  data  mining  dan  Sistem  Informasi  Geografis 
(SIG) sebagai alat untuk mengeksplorasi data geografis. Tujuan 
utama  dari  AGD  adalah  untuk  menghasilkan  klaster 
berdasarkan  status  sosial  ekonomi  di  suatu  wilayah  sehingga 
dapat membantu dalam pengambilan kebijakan sosial-ekonomi 
dan pengembangan daerah tersebut.  

Fuzzy  Geographically  Weighted  Clustering 

(FGWC) 
merupakan  salah  satu  metode  clustering  yang  sangat  baik 
dalam  analisis  klaster  untuk  data  geografis.  Metode  ini 
merupakan sebuah pengembangan metode yang dilakukan oleh 
Mason  dan  Jacobson  pada  tahun  2007  dengan  melakukan 
integrasi  metode  sebelumnya  yaitu  Fuzzy  C-Means  (FCM) 
lebih 
dengan  Neighborhood  Effect 
memperhatikan efek geografis dari setiap elemen data. Namun, 
sama  halnya dengan  metode sebelumnya  yaitu FCM  di mana 
metode  tersebut  hanya  baik  dan  cocok  digunakan  untuk  data 
yang  bertipe  numerik  saja  [1].  Hal  ini  dikarenakan  pada 
umumnya cost function yang digunakan adalah menggunakan 
jarak euclidean yang cocok digunakan untuk menghitung jarak 
data  bertipe  numerik  saja  dan  tidak  cocok  untuk  menghitung 
jarak  bertipe  kategorik.  Hal  ini  menjadi  peluang  penelitian 
untuk  membangun  metode  AGD  yang  dapat  menangani  data 
bertipe campuran. 

sehingga 

(NE) 

Pada  tahun  1997,  Huang  telah  mengembangkan  sebuah 
metode  untuk  dapat  mengatasi  data  bertipe  campuran  yang 
bernama  algoritma  K-prototype  dengan  mengkombinasikan 
algoritma  K-means  dan  K-modes.  K-Prototype  merupakan 
merupakan  salah  satu  metode  clustering  yang  menerapkan 
particional  clustering.  Algoritma  K-prototype 
ini  dapat 
digunakan  untuk  clustering  tipe  data  campuran  dengan 
terlalu  kompleks  dan  mampu 
algoritmanya  yang 
menangani data dengan jumlah besar [3]. 

tidak 

Namun  metode  K-Prototype  masih  memiliki  kekurangan 
dalam  menetapkan  centroid  untuk  data  kategorik.  Penentuan 
centroid  pada  atribut  kategorik  pada  algoritma  K-Prototype 
suatu  nilai  modus.  Karena 
masih  didasarkan  pada 

 1 / 8 

 
 
 
 
 
menggunakan  nilai  modus  sebagai  centroid,  maka  centroid 
tersebut tidak dapat menggambarkan objek-objek secara akurat 
[4].  Pada  tahun  2012,  Ji  dkk  telah  mengembangkan  sebuah 
metode  untuk  menggabungkan  fuzzy  clustering  atau  soft 
clustering  ke  dalam  algortima  K-Prototype  yang  kemudian 
metode ini diberi nama Fuzzy K-Prototype, di mana metode ini 
tidak lagi menggunakan modus sebagai centroid sebagaimana 
yang  digunakan  pada  metode  sebelumnya  melainkan  telah 
menerapkan fuzzy centroid pada algoritmanya [5]. 

Metode  Fuzzy  K-Prototype  yang  merupakan  metode 
clustering  berbasis  partisi  juga  memiliki  permasalahan  pada 
inisialisasi  centroid  awal  yang  dipilih  secara  acak  akan 
menyebabkan metode ini terjebak pada solusi  local optimum. 
Apabila inisialisi centroid awal yang dipilih kurang baik, maka 
akan menyebabkan pengelompokkan yang dihasilkan menjadi 
[3].  Salah  satu  metode  yang  dapat 
kurang  optimal 
menyelesaikan masalah pencarian solusi local optimum adalah 
dengan  menggunakan  metode  Metaheuristic.  Salah  satu 
metode  metaheuristic  yang  bisa  digunakan  adalah  algoritma 
genetika,  di  mana  algoritma  ini  menerapkan  teori  Charles 
Darwin  mengenai  seleksi  alam  [6].  Algoritma  genetika 
merupakan  algoritma  metaheuristic  yang  banyak  dilakukan 
pengembangan  dan  modifikasi  terkait  optimasinya  dengan 
menyeimbangkan antara eksploitasi dan eksplorasi. Algoritma 
ini juga merupakan algoritma dengan penghitungan matematis 
yang  lebih  sedikit  dibandingkan  algoritma  pencarian  lainnya 
[11]. 

Pada  tahun  2018  telah  dikembangkan  oleh  Arsa  sebuah 
metode  yang  mengkombinasikan  metode  Fuzzy  K-Prototype 
dengan  salah  satu  metode  metaheuristic  yaitu  Algoritma 
Genetika.  Metoda  tersebut  berhasil  mengatasi  masalah  solusi 
local optimum [8]. Dengan latar belakang yang telah diuraikan, 
maka  penelitian  ini  akan  melakukan  pembangunan  algoritma 
hibrid antara FGWC dengan FKP untuk mengatasi kelemahan 
terhadap data geografis bertipe campuran denga mengoptimasi 
matriks membership menggunakan Algoritma Genetika. 

untuk  menggambarkan 

Algoritma  yang  dihasilkan  nanti  atau  disebut  dengan 
Genetika  Fuzzy  K-Prototype  Geographically  Weighted  (GA-
FKP-GW) akan diterapkan untuk melakukan pemetaan potensi 
desa. Contohnya adalah pemetaan wilayah desa yang dilakukan 
oleh Badan Pusat Statistik (BPS). Di mana BPS bertujuan ingin 
mengetahui  potensi  dari  masing-masing  desa  berdasarkan 
beberapa  indikator  pembangunan  sosial  ekonominya.  Indeks 
Pembangunan  Desa  (IPD)  merupakan  suatu  indeks  yang 
digunakan 
atau 
perkembangan  desa  pada  waktu  tertentu.  Penghitungan  IPD 
didapat  dari  hasil  pendataan  PODES.  Data  PODES  2018 
sendiri  terdiri  atas  5  dimensi  dan  42  indikator  yang  mendata 
mengenai ketersediaan infrastruktur dan aksebilitas pelayanan.  
Publikasi BPS  mengenai Indeks Pembangunan  Desa tahun 
2018 menunjukkan bahwa terdapat 5.606 desa mandiri, 55.369 
desa  berkembang,  dan  14.461  desa  tertinggal.  Pada publikasi 
tersebut juga dijelaskan bahwa Pulau Jawa-Bali menjadi pulau 
dengan  IPD  tertinggi.  Meskipun  menjadi  pulau  dengan  IPD 
tertinggi  namun  Jawa  Tengah  menjadi  salah  satu  provinsi 
dengan IPD yang berada di bawah rata-rata  IPD Pulau Jawa-
Bali. Kabupaten Temanggung merupakan salah satu kabupaten 

kemajuan 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

di Jawa Tengah yang sangat gancar dalam merancang Rencana 
Kerja  Pembangunan  Desa 
(RKPD).  Terbukti  dengan 
pencapaiannya  meraih  juara  dua  sebagai  penghargaan  dalam 
Pembangunan Daerah Tingkat Kabupaten Tahun 2019 secara 
Nasional. Tetapi nilai IPD Kabupaten Temanggung pada Tahun 
2018 menunjukkan sebesar 66,05 yang masih berada di bawah 
rata-rata  IPD  Provinsi  Jawa  Tengah.    Hal  ini  menunjukkan 
bahwa 
rencana 
terdapat  potensi  untuk  meningkatkan 
pembangunannya  apabila  dapat  mengetahui  potensi  desa  dari 
di  wilayahnya.  Dengan  demikian  dilakukan  pengklasteran 
untuk  desa  di  daerah  Kabupaten  Temanggung  untuk 
mengetahui  karakteristik  tiap  desa  sehingga  perencanaan 
pembangunan bisa lebih tepat sasaran. 

II.  TUJUAN PENELITIAN 

Berdasarkan latar belakang yang telah diuraikan sebelumnya, 
untuk  mengatasi  kelemahan  dari  algoritma  FGWC  maka 
disusun tujuan penelitian sebagai berikut : 

1.  Mendapatkan  membership  baru  serta  optimal  yang 
dihasilkan  dari  metode  yang  diproposed  dalam 
penelitian ini yaitu algoritma GA-FKP-GW. 

2.  Mengevaluasi  kebaikan 

algoritma  GA-FKP-GW 
dengan  mengukur  indikator  evaluasi  clustering  dan 
membandingkan nilai indikator tersebut dengan metode 
sebelumnya. 

3.  Menggambarkan potensi desa Kabupaten Temanggung 
dan 

menggunakan 
membandingkan hasilnya dengan IPD Tahun 2018.  

GA-FKP-GW 

algoritma 

III. PENELITIAN TERKAIT 

Dalam  pengumpulan  informasi  mengenai  penelian  yang 
akan  diteliti,  peneliti  menemukan  beberapa  literature  yang 
berkaitan dan turut mendukung dalam penelitian ini. Literatur 
yang terkait dengan penelitian ini adalah Kombinasi Algoritma 
Genetika Dan Fuzzy K-Prototype Untuk Pengelompokan Data 
Campuran  (M.  I.  Arsa,  2018),  Metode  cluster  Menggunakan 
Kombinasi  Algoritma  Cluster  K-Prototype  dan  Algoritma 
Genetika Untuk Data Bertipe Campuran (Rani Nooraeni, 2016), 
dan 
Improvement  Design  of  Fuzzy  Geo-Demographic 
Clustering  Using  Artificial  Bee  Colony  Optimization  (A.W. 
Wijayanto dan A. Purwajiantini, 2015). 

Kombinasi  Algoritma  Genetika  Dan  Fuzzy  K-Prototype 
Untuk  Pengelompokan  Data  Campuran  (M.  I.  Arsa,  2018). 
Pada  penelitian  ini  dilakukan  pengembangan  metode  K-
Prototype  untuk  mengatasi  permasalahan  pada  penggunaan 
modus sebagai centroid untuk data kategori dan permasalahan 
pada  solusi  local  minimum.  Pada  penelitian  ini  diterapkan 
metode  Fuzzy  K-Means  dan  Fuzzy  Centroid  yang  digunakan 
untuk  mengatasi  masalah  centroid  data  kategori  dan 
menerapkan  optimasi  Genetic  Algoritme  pada  Fuzzy  K-
Prototype.  Penelitian  ini  melakukan  evaluasi  perbandingan 
untuk tiga metode yaitu K-Prototype, Fuzzy K-Prototype, dan 
Fuzzy K-Prorotype dengan GA. Hasilnya menunjukkan bahwa 
Fuzzy K-Prototype dengan GA menghasilkan hasil yang lebih 
baik.  

Metode  Cluster  Menggunakan  Kombinasi  Algoritma 
Cluster  K-Prototype  dan  Algoritma  Genetika  untuk  Data 

 2 / 8 

 
 
Bertipe  Campuran  (Rani  Nooraeni,  2016).  Penelitian  ini 
dilakukan  untuk  mengoptimasi  inisialisasi  pusat  klaster  dari 
metode  K-Prototype  dengan  algoritma  GA.  Hasil  dari 
penelitian tersebut menunjukkan bahwa hasil akurasinya lebih 
baik daripada metode K-Prototype tanpa GA. 

Improvement Design of Fuzzy Geo-Demographic Clustering 
Using Artificial Bee Colony Optimization (A.W. Wijayanto dan 
A.  Purwajiantini,  2016).  Penelitian  ini  menerapkan  metode 
Fuzzy  Geographically  Weighted  Clustering  (FGWC)  untuk 
melakukan clustering dengan mempertimbangkan efek spasial 
terhadap  data  tersebut.  Inisialisasi  pusat  klaster  dari  metode 
tersebut  dilakukan  optimasi  menggunakan  metode  Artificial 
Bee Colony (ABC). Kemudian dilakukan perbandingan untuk 
mengetahui  keakuratan  dari  metode  FCM,  NE,  FGWC  dan 
FGWC dengan ABC dengan membandingkan nilai indeks PC, 
CE,  SC,  S,  XB,  IFV.  Hasil  penelitian  menunjukkan  bahwa 
metode FGWC yang dioptimasi menunjukka hasil yang lebih 
baik  daripada  metode  FCM,  NE,  dan  FGWC  yang  tidak 
diptimasi.  

IV. METODE PENELITIAN  

A.  Data Yang Digunakan  

Pada  penelitian  ini,  peneliti  menggunakan  dua  jenis  data 
yaitu data  uji coba dan data  studi  kasus.  Untuk data  uji coba 
peneliti  menggunakan data  bangkitan  7  variabel (yang terdiri 
dari 4 variabel kategorik dan  3 variabel numerik) dan   dan 8 
variabel (yang terdiri dari 4 variabel kategorik dan 4 variabel 
numerik) dengan jumlah observasi sebanyak 50 dan 100   dan 
data  real  world  yang  didapatkan  dari  laman  website  UCI 
Machine Learning Respository yaitu data Heart Disease. Untuk 
penerapan  algoritma  yang  dibangun  pada  penelitian  ini  akan 
diterapkan  pada  data 
indeks 
pembangunan desa yang bersumber dari data PODES 2018 di 
wilayah Kabupaten Temanggung tahun 2018 yang terdiri dari 
289 desa dengan 88 variabel numerik dan 94 variabel kategorik. 
Kemudian untuk variabel efek geografi yang digunakan adalah 
data jumlah populasi dari tiap desa dan titik centroid dari peta 
shp tiap desa untuk digunakan sebagai jarak antar wilayah desa. 

indikator  pembentukan 

B.  Pre-Processing 

Pada 

tahap  pre-processing 

ini  dilakukan  pengecekan 
variabel–variabel  Potensi  Desa  yang  digunakan  sebagai 
indikator  dalam  penghitungan  Indeks  Pembangunan  Desa. 
Variabel tersebut kemudian dipisahkan antara variabel numerik 
dan  kategorik.  Kemudian  untuk  variabel  numerik  dilakukan 
standarisasi  menggunakan  metode  min–max.  Kemudian 
merubah  nilai  numerik  menjadi  kategorik  atau  disebut 
discretized  sejumlah 
interval  T  yang  ditentukan.  Pada 
penelitian ini, peneliti memilih jumlah interval T sebanyak 4. 
Kemudian hitung nilai signifikansi untuk setiap atribut numerik 
dan  hitung  jarak  untuk  setiap  atribut  kategori  menggunakan 
fungsi  co-occurance  probabilities.  Proses  Pre-processing  ini 
dilakukan  untuk  seluruh  data  yang  digunakan  dalam  metode 
FKP, FKP-GW, dan GA-FKP-GW 

C.  Metode Clustering 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Secara umum metode Clustering terbagi menjadi dua yaitu 
hierarchical  clustering  dan  partitional  clustering.  Pada 
hierarchical  clustering,  data  dikelompokkan  kedalam  bentuk 
hierarki  atau  bertingkat.  Sedangkan  metode  partitional 
clustering  mengelompokkan  data  ke  dalam  sejumlah  cluster 
tanpa adanya struktur hirarki antara satu dengan yang lainnya. 
Pada metode ini jumlah klaster harus ditentukan sebelumnya. 
Dalam  penentuan  jumlah  klaster  dapat  dilakukan  dengan 
berbada cara, salah satunya adalah dengan menggunakan scree 
plot untuk melihat penrunan nilai  total cost function dari setiap 
klasternya. 

Fuzzy Geographically Weighted Custering (FGWC) 

dengan 

Neighborhood 

FGWC merupakan sebuah metode yang dikembangkan oleh 
Mason  dan  Jacob  (2007)  yang  mengintegrasikan 
fuzzy 
dengan 
clustering 
memperhitungkan  jarak  antar  wilayah  dan  populasinya  yang 
membuatnya  lebih  sesnsitive  terhadap  efek  ketetanggaannya 
dan  menghasilkan  klaster  yang  lebih  geographically  aware. 
Metode  FGWC  melakukan  kalkulasi  baru  untuk  nilai 
keanggotaannya dalam setiap iterasi menggunakan persamaan : 
𝑛
∑ 𝑤𝑖𝑗
(1) 
𝑗=1

′ = 𝛼𝜇𝑖 +  𝛽 1
𝜇𝑖

Effect 

𝜇𝑗 

𝐴

Di  mana 𝜇𝑖

′ adalah  nilai  keanggotaan  yang  baru  dari  objek 
ke-i, sedangkan 𝜇𝑖 merupakan nilai keanggotaan lama wilayah 
ke-i  dan 𝜇𝑗merupakan nilai keanggotaan lama wilayah ke-j. α 
dan β merupakan penimbang nilai keanggotaan lama dan nilai 
penimbang  rata-rata  keanggotaan  objek  lain  yang  jumlahnya 
bernilai  1.  A  adalah  nilai  yang  memastikan  penimbang  nilai 
keanggotaan dalam rentang nol dan satu. [0,1]. 𝑤𝑖𝑗 merupakan 
penimbang  antara  dua  wilayah  geografis.  Persamaan  untuk 
nilai penimbang dapat dirumuskan sebagai berikut : 

𝑤𝑖𝑗 =

𝑏

(𝑚𝑖𝑚𝑗)
𝑎
𝑑𝑖𝑗

(2) 

Dengan 𝑚𝑖 dan 𝑚𝑗 adalah  jumlah  populasi  di  wilayah  ke-i 
dan ke-j, sedangkan 𝑑𝑖𝑗  adalah jarak antara wilayah i dan j. ɑ 
adalah  besar  pengaruh  interaksi  jarak  antar  wilayah  dan  b 
merupakan besar interaksi populasi antara dua wilayah. Kedua 
parameter tersebut ditentukan oleh peneliti. 

K-Prototype 

Metode  K-Prototype  merupakan  salah  satu  partitional 
clustering  yang  dikembangkan  dari  metode  K-Means  yang 
sebelumnya  hanya  dapat  digunakan  untuk  tipe  data  numerik 
saja  namun  keefisiensiannya  masih  tetap  dipertahankan. 
Algoritma  K-Prototype  ini  masih  tergolong  hard  clustering 
dengan setiap objek hanya dapat dikelompokkan ke dalam satu 
klaster  saja.  Persamaan  objektif  untuk  data  bertipe  campuran 
adalah sebagai berikut : 
2
𝑟 )
𝑟 − 𝑣𝑙𝑗
𝑛
𝐸𝑙 = ∑ 𝜇𝑖𝑙
𝑖=1

𝑛
+ 𝛾𝑙 ∑ 𝜇𝑖𝑙
𝑖=1

∑ 𝛿(𝑦𝑖𝑗

𝑐 )
𝑐 , 𝑞𝑙𝑗

𝑚𝑟
∑ (𝑦𝑖𝑗
𝑗=1

  (3) 

𝑚𝑐
𝑗=1

Tahapan-tahapan algorima yang digunakan pada penelitian 
ini mengikuti algoritma yang telah dibuat oleh Nooraini (2015). 

Fuzzy K-Prototype (FKP) 

Fuzzy K-Prototype merupakan salah satu metode clustering 
untuk  data  campuran  yang  dikembangkan  oleh  Ji  dkk  pada 

 3 / 8 

 
 
 
 
 
 
 
    
 
 
 
 
 
tahun 2012 dengan mengembangkan metode sebelumnya yaitu 
K-Prototype  yang  masih  merupakan  hard  clustering.  Metode 
ini  sudah  berbasis  fuzzy  clustering  dengan  mengintegrasikan 
mean dan fuzzy centroid dalam menentukan pusat klaster, dan 
menggunakan  ukuran  ketidaksamaan  (dissimilarity)  baru 
dengan  memperhitungkan  tingkat  signifikansi    dari  setiap 
atribut dan jarak antar nilai atribut kategorik. Persaman objektif 
untuk metode Fuzzy K-Prototype ini adalah sebagai berikut : 

2
𝑟 ))

𝑝
𝑙=1

+ ∑

𝑐 )2)

     (4) 

𝑐 , ṽ𝑘𝑙

𝑐 )2

𝑚
𝑙=𝑝+1

𝜑(𝑥𝑖𝑙

𝑐 , ṽ𝑘𝑙

𝑚
𝑙=𝑝+1

𝑛
𝛼
∑ 𝜇𝑖𝑘
𝑖=1
𝑝
𝑙=1

(∑ (𝜔𝑙(𝑥𝑖𝑙
𝑟 − 𝑞𝑘𝑙

𝑐
𝐸(𝜇, 𝑄) = ∑
𝑘=1
Di  mana  ∑ (𝜔𝑙(𝑥𝑖𝑙

𝑟 − 𝑞𝑘𝑙
𝜑(𝑥𝑖𝑙
2
𝑟 ))
 merupakan  penghitungan 
jarak  observasi  terhadap  centroid  atau  pusat  klaster  untuk 
atribut  numerik,  sedangkan ∑
 merupakan 
jarak  observasi  terhadap  centroid  atribut  kategorik  yang 
jarak  untuk  atribut  numerik  dan  kategorik 
kemudian 
𝑟  merupakan  matriks  nilai  keanggotaan  untuk 
dijumlahkan. 𝜇𝑖𝑘
setiap  observasi  dengan  (α)  adalah  parameter 
fuzziness 
(kesamaan) yang nilainya dapat dievaluasi menggunakan CVC 
terbaik,  sedangkan  𝜔𝑙 
untuk  mendapat  nilai  parameter 
𝑟  
merupakan nilai signifikansi dari setiap variabel numerik. 𝑞𝑘𝑙
𝑐  merupakan 
merupakan centroid untuk atribut numerik dan ṽ𝑘𝑙
centroid  untuk  atribut  kategorik  yang  berupa  fuzzy  centroid 
atau  peluang  setiap  nilai  observasi  kedalam  setiap  klaster. 
Tahapan-tahapan  algoritma  FKP  yang  digunakan  pada 
penelitian ini mengikuti algoritma yang telah dibuat oleh Arsa 
(2018). 

Metode Optimasi Metaheuristic - Algoritma Genetika 

Algoritma  Genetika  merupakan  salah  satu  algoritma 
pencarian  yang  mengadaptasi  teori  seleksi  alam  dari  Charles 
Darwin di dalam algoritma pencariannya. Sama halnya proses 
seleksi  alam  di  mana  individu  yang  bertahan  adalah  individu 
yang dapat bertahan selama proses evolusi. Di dalam algoritma 
ini dikenal beberapa istilah seperti Elitimasi, Seleksi, Crossover, 
dan  Mutasi.  Penjelasan  dari  langkah  –  langkah  yang  terdapat 
pada Algoritma Genetika adalah sebagai berikut : 

1.  Pengkodean 

Pada  tahap  ini  dilakukan  sebuah  inisialisasi  di  mana 
sebuah kromosom akan diwakilkan oleh matriks derajat 
keanggotaan yang berukuran n x k, di mana gen pertama 
diwakili oleh matriks keanggotaan untuk data pertama, 
gen  kedua  mewakili  derajat  keanggotaan  data  kedua, 
dan untuk seterusnya sebanyak n gen. 

2.  Bangkitkan Populasi Awal 

Proses  ini  merupakan  pembanggkitan  populasi  awal 
yang  berisi  kromosom-kromsom  yang  sebenyak 
populasi  yang  ditentukan,  di  mana  setiap  kromosom 
berisikan matriks calon derajat keanggotaannya.  

3.  Evaluasi Nilai Fitness 

Proses  ini  dilakukan  evaluasi  untuk  setiap  populasi 
fitnessnya  untuk  setiap 
dengan  menghitung  nilai 
populasi.  Tujuan  dari  evaluasi 
ini  adalah  untuk 
mengetahui  kromosom  mana  dari  N  populasi  yang 
memiliki  nilai  fitnesss  tertinggi,  di  mana  nilai  fitness 
tersebut 
tertinggi  menunjukkan  bahwa  kromosom 
memiliki solusi yang terbaik. Penghitungan nilai fitness 
dilakukan menggunakan rumus : 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

𝑓 = 1
(5) 
Nilai h adalah fungsi objektif dari FKP-GW, sedangkan 
a adalah bilangan yang sangat kecil yang menghindari 
hasil bernilai 0. 

(ℎ+𝑎)

4.  Seleksi 

Proses  seleksi  merupakan  proses  di  mana  akan 
dilakukan  pemilihan  kromosom–kromosom  untuk 
melakukan  proses  selanjutnya  (crossover  dan  mutasi). 
Metode  mutasi  yang  digunakan  oleh  peneliti  adalah 
roulette  whell.  Metode 
ini  dilakukan  dengan 
menghitung nilai probabilitas dari dari setiap kromosom 
berdasarkan nilai fitness-nya dengan persamaan : 

𝑃𝑖 = 𝑓𝑖
𝑓𝑡𝑜𝑡𝑎𝑙

 𝑖 = 1,2, … 𝑛 𝑝𝑜𝑝𝑢𝑙𝑎𝑠𝑖     

(6) 

5.  Crossover 

Proses  crossover ini  merupakan proses  di  mana  untuk 
menambah  keanekaragaman  kromosom  dari  suatu 
populasi.  Pada  penelitian  ini  peneliti  menggunakan 
operator a one-step Fuzzy K-Prototype Geographically 
Weighted. 

6.  Mutasi 

Proses  ini  merupakan  proses  mengubah  satu  atau 
beberapa  gen  dalam  kromosom  sehingga  akan 
mengahasilkan  kromosom  baru  dan  menhindari 
masalah  local  optimum.  Mutasi  dapat  terjadi  apabila 
peluang  sebuah  gen  termutasi  lebih  kecil  daripada 
peluang  mutasi  dalam  kromosom,  maka  gen  tersebut 
akan digantikan atau mengalami mutasi. 

7.  Elitieme 

Proses  ini  untuk  menyimpan  kromosom  dengan  nilai 
fitness tertinggi, yang bertujuan agar kromosom dengan 
nilai  fitness  tertinggi  akan  mengalami  penurunan  nilai 
fitness pada proses crossover dan mutasi. 

Berikut adalah algoritma dari metode yang diproposed pada 

penelitian kali ini:  
Fuzzy K-Prortype Geographically Weighted (FGWC) 

Metode  ini  merupakan  metode  yang  telah  dikembangkan 
untuk  dapat  mengklasterkan  data  geografis  dengan 
mempertimbangkan  efek  spasial  yang  bertipe  campuran. 
Algoritma  dari  Fuzzy  K-Prototype  Geographically  Weighted 
(FKP-GW) adalah sebagai berikut : 

1. 

Input Parameter 
Parameter yang perlu diinput pada algoritma ini adalah 
berupa  data,  jumlah  klaster,  fuzziness  coefficient  (m), 
maksimum  iterasi,  dan  threshold  (𝜀)  serta  beberapa 
parameter  geographichally  seperti  matriks  populasi, 
matriks jarak wilayah observasi, alpha, beta, a, dan b, 
2.  Bangkitkan  data  matriks  keanggotaan  sesuai  dengan 

jumlah klaster yang telah ditentukan, 

3.  Hitung  nilai  centroid  untuk  atribut  numerik  dan 

kategorik, 

4.  Hitung  jarak  observasi  dengan    centroid  yang  telah 
didapatkan, dan jumlahkan jarak untuk atribut numerik 
dan kategorik, 

5.  Update  matrik  keanggotaan  dari  matriks  jarak  yang 

telah didapatkan sebelumnya.  

 4 / 8 

 
 
 
   
 
 
 
6.  Modifikasi  matriks  keanggotaan  yang  baru  dengan 
geographically weighted sesuai dengan persamaan 1, 
7.  Jika  nilai  error lebih  kecil dari  threshold 𝜀, atau telah 
mencapai  maksimum  iterasi  maka  hentikan  proses 
clustering. Jika tidak, kembali ke langkah 3 hingga salah 
satu kondisi berhenti terpenuhi. 

Tahapan berikutnya adalah algoritma FKP yang dioptimasi 
menggunakan  Algoritma  Genetika  (GA-FKP-GW)  adalah 
sebagai berikut : 

1. 

2. 

Input parameter 
Parameter yang perlu diinput pada parameter ini adalah 
mutation  rate,  maximum  generation,  dan  jumlah 
populasi  serta  beberapa  parameter  yang  digunakan 
pada algoritma FKP-GW, 
Inisialisasi populasi yang berisi calon-calon kromosom, 
calon-calon  matriks 
kromosom 
kenaggotaan, 

berisikan 

ini 

3.  Evaluasi  nilai  fitness,  evaluasi  nilai  fitness  dihitung 
menggunakan persamaan 5 dengan nilai cost function 
adalah  adalah  dari  algoritma  FKG-GW  dengan 
maksimum iterasi = 1. 

4.  Membuat populasi-populasi baru  dengan mengulangi 
langkah-langkah berikut sebanyak maksimum generasi 
hingga populasi baru yang dihasikan paling optimum: 
1.  Seleksi, 
2.  Crossover 
3.  Mutasi 

5.  Menggunakan  populasi  baru  paling  optimum  untuk 
kemudian  digunakan  sebagai  matriks  keanggotaan 
untuk  mencari  klaster,  centroid,  cost 
function 
menggunakan algoritma FKP-GW dengan maksimum 
iterasi = 1. 

Indikator Untuk Evaluasi Hasil Clustering 

Untuk  mengevaluasi  setiap  metode  yang  digunakan 
dilakukan perbandingan dari nilai cost function, indeks PC, dan 
Indeks  SC,  perbandingan  Nilai  CVC,  nilai  sd,  dan  dilakukan 
perbandingan  lamanya  waktu  pemrosesan.  Persamaan  yang 
digunakan dalam analisis evaluasi ini adalah sebagai berikut: 

1.  Partition Coefficient (PC)  

Indeks  PC  merupakan  indeks  yang  mengukur  jumlah 
overlapping  antar  cluster.  Nilai  indeks  PC  diukur 
dengan menggunakan rumus berikut  : 
𝑃𝐶 = 1
𝑛

𝑛
2
∑ ∑ 𝜇𝑖𝑙
𝑖=1

𝑘
𝑙=1

(7) 

Semakin  besar  nilai  indeks  PC  menunjukkan  hasil 
klaster yang lebih baik. 
2.  Classification Entropy (CE)  

Indeks CE merupakan indeks yang mengukur fuzziness 
(kesamaran)  antar  cluster.  Nilai  indeks  CE  diukur 
dengan menggunakan rumus berikut  : 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

CVC  merupakan  penggabungan  antara  metode 
Category  Utility  untuk  variabel  kategorik  dan 
variabel  numerik. 
pengukuran 
Persamaan CVC adalah sebagai berikut 

variance  untuk 

: 

𝐶𝑉𝐶 = 𝐶𝑈

1+ 𝜎2  

𝐶𝑈 = ∑ (

𝑘
𝑙=1

|𝐶𝑙|
𝑁

∑ ∑ [𝑃(𝐴𝑗 = 𝑉𝑖𝑗|𝐶𝑙)2 − 𝑃(𝐴𝑗 = 𝑉𝑖𝑗)2]

𝑚
𝑗=1

𝑛
𝑖=1

𝑘
𝜎2 = ∑
𝑙=1

1
|𝐶𝑙|

𝑚
∑ ∑ (𝑉𝑖,𝑗
𝑗=1

𝑛
𝑖=1

𝑙
𝑙 − 𝑉𝑖,𝑎𝑣𝑔

2
)

(9) 

)

(10) 

(11) 

Semakin  besar  nilai  CVC  menunjukkan  hasil  klaster 
yang lebih baik. 

V.  KERANGKA PIKIR 

algoritma 

metaheuristic 

Kerangka  pikir  pada  penelitian 
ini  berfokus  pada 
pengembangan  metode  Fuzzy  Geographically  Weighted 
Clustering untuk data bertipe campuran dan mengoptimasinya 
menggunakan 
dengan 
mengkombinasikan  metode  FGWC  dengan  metode  Fuzzy  K-
Prototype dan Algoritma Genetika. Penelitian ini juga berfokus 
pada  pembangunan  modul  di  Rstudio.  Pada  modul  tersebut 
akan dilakan evaluasi validasi untuk data uji yang berupa data 
bangkitan  dan  data  heart  disease  yang  didapatkan  dari 
Respository  UCI  Machine  Learning.  Metode  Fuzzy  K-
yang  dioptimasi 
Prototype  Geographically  Weighted 
menggunakan  Algoritma  Genetika 
untuk 
diterapkan 
melakukan  clustering  kepada  data  Potensi  Desa  Tahun  2018 
dengan 
indikator 
Indeks  Pembangunan  Desa  Tahun  2018 
penghitungan 
Kabupaten Temanggung.  

digunakan 

variabel 

sebagai 

yang 

Gambar 1. Kerangka pikir penelitian 

VI. HASIL DAN PEMBAHASAN 

A.  Membership Hasil GA-FKP-GW 

TABEL I 

𝐶𝐸 = −1
𝑛

∑ ∑ 𝜇𝑖𝑙log (𝜇𝑖𝑙)

𝑘
𝑙=1

𝑛
𝑖=1

 (8) 

Tabel Matriks Membership Tanpa Geographically Weighted 

Semakin  kecil  nilai  indeks  CE  menunjukkan  hasil 
klaster yang lebih baik. 

3.  Categorical Variance Criterion (CVC) 

Observasi 
1 
2 
3 
4 

Klaster 1 
0,2408774 
0,2456589 
0,2443094 
0,2444478 

Klaster 2 
0,2404958 
0,2454093 
0,2439984 
0,2441943 

Klaster 3 
0,2537448 
0,2518543 
0,2524005 
0,2524947 

Klaster 4 
0,2648819 
0,2570775 
0,2592917 
0,2588632 

 5 / 8 

 
 
 
 
 
  
  
 
… 

… 

… 

… 

… 

TABEL II 

Tabel Matriks Membership Setelah Mendapat Geographically Weighted 

Observasi 
1 
2 
3 
4 
… 

Klaster 1 
0,2435070 
0,2471943 
0,2460705 
0,2462119 
… 

Klaster 2 
0,2432522 
0,2470377 
0,2458614 
0,2460500 
… 

Klaster 3 
0,2526106 
0,2511592 
0,2516199 
0,2516817 
… 

Klaster 4 
0,2606302 
0,2546088 
0,2564482 
0,2560564 
… 

Tabel  1  dan  2  menunjukkan  bahwa  metode  GA-FKP-GW 
dapat memberikan efek spasial kepada tiap observasi, terbukti 
dari nilai keanggotaan tiap observasi berubah setelah diberikan 
efek  geografis  dari  matriks  populasi  dan  jarak  antar  wilayah 
observasi. 

B.  Analisis Evaluasi Hasil Clustering 

Gambar 2. Total Cost data bangkitan untuk setiap klaster 

Gambar 3. Total Cost data studi kasus untuk setiap klaster 

Gambar 4. Plot parameter fuzziness terhadap nilai CVC 
Pada  penitian  ini,  peneliti  melakukan  analisis  clustering 
terhadap ketiga jenis data yaitu data uji bangkitan, data uji heart 
disease, dan data studi kasus menggunakan metode KP, FKP, 
FKP-GW, dan GA-FKP-GW. Untuk penentuan jumlah klaster 
dari masing-masing data dilakukan dengan melihat hasil scree 
plot dari nilai  total cost yang didapatkan untuk setiap jumlah 
klasternya.  Gambar  2  menunjukkan  bahwa  penurunan  nilai 
total  cost  secara  signifikan  terjadi  sampai  jumlah  klaster  3, 
sehingga  jumlah  klaster  yang  dipilih  untuk  semua  jenis  data 
bangkitan adalah 3. Gambar 3 menunjukkan bahwa penurunan 
nilai total cost secara signifikan terjadi sampai jumlah klaster 4, 
sehingga jumlah klaster yang dipilih untuk clustering data studi 
kasus adalah 4. Sedangkan untuk data uji heart disease, jumlah 
klasternya  ditetapkan  sebanyak  2  klaster  karena  target  class 
dari data uji tersebut hanya ada 2 kelas 

Nilai parameter yang digunakan dalam data uji baik itu data 
uji  bangkitan  maupun  data  uji  heart  disease  adalah  untuk 
parameter  fuzziness(m)  =  2,  maximum  iterasi  100,  threshold 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

0,00005, mutation rate sebesar 0,00001, maksimum Generasi 
20,  dan  jumlah  populasi  adalah  20.  Sedangkan  parameter-
parameter yang digunakan untuk data studi kasus adalah untuk 
nilai  fuzziness  dapat  dilihat  pada  gambar  4  bahwa  nilai  CVC 
tertinggi  terdapat  pada  parameter  fuzziness  =  2,3,  kemudian 
untuk  parameter-parameter  modifikasi  geografinya  adalah 
alpha = 0,8, beta = 0,2, a dan b = 1. 
TABEL III 
Tabel Cost Function Hasil Clustering Data Bangkitan 

Jenis Data 

1 

2 

50 
100 
50 
100 

KP 

10,865 
17.185 
13.302 
20.194 

Rata-rata 
FKP 
0.914731 
1.402695 
0.944462 
1.555693 

Metode 

GA-FKP 

0.914706 
1.402676 
0.944438 
1.555692 

FKP-
GW 
0.9903 
1.5309 
1.0109 
1.6750 

GA-FKP-
GW 
0.9848594 
1.530915 
1.006102 
1.671625 

Dari  TABEL  3,  hasil  dari  kelima  metode  yang  digunakan 
untuk  clustering  data  bangkitan  menunjukkan  bahwa  cost 
function  dari  metode  FKP  dan  FKP-GW  yang  dioptimasi 
menggunakan  Algoritma  Genetika  menghasilkan  nilai  cost 
function  yang  lebih  kecil  daripada  metode  yang  sama  tetapi 
tidak dioptimasi untuk semua jenis data bangkitan. Dari hasil 
tabel  3  juga  terlihat  bahwa  nilai  rata-rata  cost  function  dari 
metode  FKP  yang  dihitung  beberapa  kali  masih  dibawah 
metode  GA-FKP.  Hal  ini  menunjukkan  bahwa  algoritma 
Genetika dapat mengatasi masalah local solution. 

TABEL IV 
Tabel Hasil Akurasi Data Heart Disease 

Metode 

Akurasi 

Cost Function 

KP 
FKP 
GA-FKP 

0,7986799 
0,8316832 
0,8316832 
Tabel 4 menujukkan bahwa metode FKP menghasilkan baik 
nilai  akurasi  maupun  cost  function  yang  lebih  baik  daripada 
metode K-Prototype. Meskipun hasil akurasi dari metode FKP 
dan  GA-FKP  menghasilkan  nilai  yang  sama,  namun  metode 
GA-FKP  mampu  memberikan  nilai  cost  function  yang  lebih 
kecil daripada metode FKP yang tidak dioptimasi. 

76,61943 
12,48271 
12,48269 

TABEL V 
Tabel PC,CE,Cost Function, dan Waktu Hasil Clustering Data Studi Kasus 

Metode 

PC 

CE 

0,27953 
0.25001 

1,321507 
1.386274 

0.25045 

1.385392 

23.9096 

Cost 
Function 
23,04163 
23.91142 

Iterasi  Waktu 

92 
6 

1 

1,2 M 
4,9 S 

10 M 

FKP 
FKP-GW 
GA-
FKP-GW 

TABEL VI 
Tabel CU, Varians, dan CVC  Hasil Clustering Data Studi Kasus 

Metode 

FKP 
FKP-GW 
GA-FKP-
GW 

CU 
5,183654 
4.634121 

Varians 
6,615228 
4.291394 

CVC 
0,6806959 
0.8757845 

4.849674 

9.628201 

0.4563024 

TABEL VII 
Tabel PC,CE,Cost Function, dan Waktu Hasil Clustering Data Studi Kasus 
Setelah Dilakukan Seleksi Variabel 

Metode 

PC 

CE 

FKP 

0.2960 

1.284483 

Cost 
Function 
28.43321 

Iterasi  Waktu 

45 

17,5 S 

 6 / 8 

 
 
 
 
 
 
 
FKP-GW 
GA-FKP-
GW 

0.2731 

1.334879 

29.4924 

83 

34,3 S 

0.2827 

1.313786 

38.09994 

1 

5,3 M 

TABEL VIII 
Tabel CU, Varians, dan CVC  Hasil Clustering Data Studi Kasus Setelah 
Dilakukan Seleksi Variabel 

Metode 

FKP 
FKP-GW 
GA-FKP-
GW 

CU 
4.157586 
4.114901 

Varians 
5.398276 
5.472119 

CVC 
0.6497979 
0.6357889 

4.130119 

5.460601 

0.6392779 

Peneliti melakukan sebuah uji coba dengan melakukan dua 
janis  perlakuan  terhadap  data  studi  yang  digunakan  dalam 
proses  clustering  yaitu,  yang  pertama  peneliti  menggunakan 
seluruh  variabel  yang  tercakup  dalam  penghitungan  IPD  dan 
yang kedua adalah peneliti melakukan seleksi variabel dengan 
menggunakan uji signifikansi variabel terlebih dahulu menurut 
kelompok  IPD  yang  telah  ada.  Untuk  variabel  numerik 
dilakukan seleksi variabel menggunakan uji ANOVA one-way 
dan  untuk  variabel  kategorik  menggunakan  uji  chi-square. 
Kemudian,  semua  variabel  yang  tidak  signifikan  tidak  akan 
digunakan dalam proses custering jenis kedua. 

Untuk TABEL 5 dan 6 menunjukkan hasil clustering pada 
seluruh variabel IPD yang digunakan. Dari hasil tersebut dapat 
terlihat  bahwa  hasil  dari  metode  yang  telah  dikembangkan 
(FKP-GW) menghasilkan nilai yang tidak berbeda terlalu jauh 
dari metode sebelumnya yaitu FKP. Meskipun Indeks PC, CE, 
dan  Cost  Function  dari  metode  FKP-GW  tidak  lebih  baik 
daripada  metode  FKP,  namun  metode  FKP-GW  dapat 
menghasilkan  nilai  CVC  yang  lebih  baik  dari  metode 
sebelumnya  dan  memakan  waktu  yang  lebih  sedikit  dalam 
proses clustering dengan hanya memerlukan 6 iterasi saja untuk 
mencapai  kondisi  di  mana  perubahan  cost  function  tidak 
melebihi batas threshold. 

Sedangkan  TABEL  7  dan  8  menunjukkan  hasil  dari 
clustering  setelah  dilakukan  seleksi  variabel  terlebih  dahulu. 
Sangat  disayangkan  bahwa  hasil  dari  clustering  variabel-
variabel  IPD  yang  telah  dilakukan  seleksi  variabel  terlebih 
dahulu menunjukkan hasil yang tidak lebih baik daripada hasil 
clustering  sebelumnya  yang  menggunakan  seluruh  variabel 
IPD dalam proses clustering. 

C.  Profiling Potensi Desa Di Kabupaten Temanggung 

Metode clustering FKP-GW yang dioptimasi menggunakan 
Algoritma  Genetika 
telah  berhasil  dikembangkan  dan 
diterapkan  untuk  melakukan  analisis  clustering  kepada  data-
data campuran yang memiliki efek spasial. Metode GA-FKP-
GW telah berhasil diterapkan untuk menganalisis klaster yang 
terbentuk 
di  Kabupaten 
Temanggung  pada  tahun  2018  dengan  menghasilkan  empat 
klaster yang berbeda. 

variabel-variabel 

IPD 

dari 

Gambar 5.  Plot PCA 4 Klaster hasil GA-FKP-GW 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 6.  Mapping hasil clustering dan IPD 

Dari  gambar  5  dapat  terlihat  bahwa  gambar  plot  PCA 
menujukkan  klaster  menghasilkan  persebaran  yang  baik. 
Klaster  dua  menunjukkan  persebaran  titik-titik  pada  klaster 
tersebut saling berkumpul dan berdekatan. Sama halnya dengan 
klaster empat dengan warna biru yang menunjukkan persebaran 
titik-titiknya  saling  berdekatan.  Klaster  satu  dan  tiga  sama-
sama berada di antara klaster dua dan empat, namun klaster satu 
menunjukkan persebaran  

Mapping dari persebaran desa berdasarkan klasternya dapat 
terlihat  pada  gambar  6  sebelah  kiri  dari  hasil  klastering  dan 
sebelah kanan hasil pengeleompokkan IPD. Dari 4 desa yang 
termasuk  ke  dalam  klaster  1  dapat  terlihat  bahwa  desa-desa 
tersebut  terletak  dan  berkumpul  pada  wilayah  tengah  daerah 
Kabupaten  Temanggung.  Sedangkan  untuk  204  desa  yang 
termasuk ke dalam klaster 2 terlihat tersebar ke seluruh wilayah 
daerah. Untuk 3 desa yang termasuk ke dalam klaster 3 terlihat 
menyebar dan tidak berada dalam satu daerah yang berdekatan. 
Dan untuk  78 desa yang termasuk ke dalam klaster 4 terlihat 
cukup terkumpul pada tengah wilayah Kabupaten Temanggung 
meskipun ada beberapa desa yang berada pada pinggir wilyah 
dan  cukup  menyebar.  Dari  perbandingan  mapping  desa  pada 
gambar 6, dapat terlihat bahwa beberapa desa yang tergolong 
berkembang memiliki warna yang sama dengan klaster 2 dan 
desa mandiri memiliki warna yang sama dengan klaster 4.. 

TABEL IX 
Tabel Jumlah Desa Menurut IPD dan Hasil Clustering 
Klaster 2 

Klaster 3 
2 
1 
0 

Klaster 4 
25 
53 
0 

Klaster 1 
4 
0 
0 

IPD 
Mandiri 
10 
Berkembang 
192 
Tertinggal 
2 
Tabel 9 menunjukkan bahwa klaster 2 dan 4 didominasi oleh 
desa  berkembang,  sedangkan  untuk  klaster  3  diisi  oleh  desa 
dengan status desa mandiri dan berkembang. Sedangkan untuk 
desa dengan status tertinggal  berada pada klaster 2. Sedangkan 
untuk klaster 1 didominasi oleh desa dengan kategori mandiri. 
TABEL X 
Tabel Distribusi Desa Tiap Kecamatan Menurut Klaster 

Kecamatan 

Klaster 1  Klaster 2  Klaster 3  Claster 4 

Bansari 

Bejen 

Bulu 

Candiroto 

Gemawang 

Jumo 

Kaloran 

0 

0 

0 

0 

0 

0 

0 

12(92,3%) 

12(85,7%) 

13(68,4%) 

0 

0 

0 

1(7,7%) 

2(14,3%) 

6(31,6%) 

11(78,6%) 

1(7,1%) 

2(14,3%) 

9(90,0%) 

13(100,0%) 

9(64,3%) 

0 

0 

0 

1(10,0%) 

0 

5(35,7%) 

 7 / 8 

 
 
 
 
 
 
 
Kandangan 

Kedu 

Kledung 

Kranggan 

Ngadirejo 

Parakan 

Pringsurat 

Selopampang 

0 

0 

0 

0 

0 

0 

0 

0 

16(100%) 

7(50%) 

6((46,2%) 

12(92,3%) 

15(75,0%) 

3(18,8%) 

0 

0 

0 

0 

0 

0 

0 

7(50,0%) 

7(53,8%) 

1(7,7%) 

5(25%) 

13(81,3%) 

11((78,6%) 

1(7,1%) 

2(14,3%) 

7(58,3%) 

0 

5(41,7%) 

Temanggung 

4(16%) 

8(32%) 

1(4%) 

12(48%) 

Tembarak 

Tlogomulyo 

Tretep 

Wonoboyo 

0 

0 

0 

0 

10(76,9%) 

6(50%) 

11(100%) 

13(100%) 

0 

0 

0 

0 

3(23,1%) 

6(50,0%) 

0 

0 

3(1%) 

4(1,4%) 

78(27%) 

Total 
204(70,6%) 
Dari TABEL 10 dapat terlihat jumlah desa yang termasuk ke 
dalam  klaster  satu  sampai  empat  menurut  kecamatan.  Dapat 
terlihat  bahwa  di hampir semua kecamatan terdapat beberapa 
desa  masih  terkelompok  ke  dalam  klaster  yang  berbeda  atau 
dapat  dikatakan  karakteristik  dari  desa-desa  dalam  satu 
kecamatan  tersebut  tidaklah  sama.  Namun  terdapat  beberapa 
kecamatan  yaitu  Kecamatan  Jumo,  Kecamatan  Kandangan, 
Kecamatan  Tretep,  dan  Kecamatan  Wonoboyo  yang  seluruh 
desanya masuk ke dalam klaster dua.  

Untuk membedakan desa-desa yang masuk ke dalam klaster 
yang  berbeda  dapat  dilihat  dari  Signifikansi  untuk  masing-
masing  variabel.  Dari  hasil  klastering  menunjukkan  bahwa 
variabel  pelayanan  kesehatan  dan  infrastruktur  ekonomi 
menjadi  variabel  dengan  indikator  signifikan  terbanyak.  Dari 
indikator-indikator  yang  signifikan  menunjukkan  bahwa 
klaster    1  menjadi  klaster  dengan  rata-rata  jumlah  fasilitas 
kesehatan  lebih  banyak  dibandingkan  klaster  lainnya,  diikuti 
klaster  4,  klaster  3,  dan  klaster  2.    Sedangkan  untuk  variabel 
infrastruktur  ekonomi,  klaster  3  menjadi  klaster  dengan  rata-
rata  jumlah  infrastruktur  terbanyak  diikuti  oleh  klaster  4, 
klaster  1,  dan  klaster  2  namun  untuk  jarak  ke  infrastruktur 
terdekat  klaster  dengan  rata-rata  jarak  terdekat  dimiliki  oleh 
klaster 1. Dari hasil analisis deskriptif dari indikator-indikator 
yang signifikan tersebut dapat dikatan bahwa klaster 1 menjadi 
klaster dengan desa yang memiliki pelayanan kesehatan terbaik 
sedangkan  klaster  3  menjadi  klaster  dengan  desa  yang 
infrastruktur ekonomi terbaik. 

VII. 

KESIMPULAN 

Metode yang diusulkan dalam penelitian ini yaitu FKP-GW 
dan GA-FKP-GW telah berhasil dikembangkan dan diterapkan 
pada analisis clustering untuk data geografis bertipe campuran. 
Berdasarkan  hasil  yang  telah  dilakukan  dengan  menerapkan 
proposed  method  dan  metode  sebelumnya  dalam  clustering 
data  uji  dan  data  studi  kasus,  hasil  evaluasi  menunjukkan 
metode  FKP-GW  dan  GA-FKP-GW  dapat  memberikan  hasil 
clustering  yang  berbeda  dari  metode  sebelumnya  walaupun 
lebih  baik 
nilai 

indikator  evaluasinya  masih  belum 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

dibandingkan metode sebelumnya. Metode yang diusulkan ini 
cukup  baik  dan  dapat  diterapkan  untuk  melakukan  analisis 
clustering data geografis bertipe campuran. 

Hasil  implementasi  metode  usulan  GA-FKP-GW  dalam 
analisis clustering untuk data PODES yang digunakan sebagai 
indikator  dalam  penghitungan  IPD  Kabupaten  Temanggung 
tahun  2018  berhasil  mengelompokkan  289  desa  menjadi  4 
cluster.  Klaster  1  dan  3  didominasi  oleh  desa  yang  berstatus 
mandiri, sedangkan klaster 2 dan 4 didominasi oleh desa yang 
berstatus  berkembang.  Hasil  analisis  deskriptif  menunjukkan 
karakteristik  dari  tiap  klaster,  di  mana  klaster  1  baik  dalam 
pelayanan  kesehatan,  klaster  3  baik  dalam  infrastruktur 
ekonomi, klaster 4 memiliki infrastruktur yang paling stabil dan 
memadai di antara semua klaster, dan klaster 2 adalah klaster 
dengan infrastruktur paling sedikit dengan jarak terjauh. 

Pada penelitian ini terdapat beberapa parameter seperti alpha 
dan  beta  yang  dilakukan  simulasi  secara  berulang-ulang 
sehingga  dimungkinkan  parameter  yang  digunakan  kurang 
optimal. Sehingga perlu dilakukan penelitian lebih lanjut untuk 
menentukan  parameter  yang  sesuai.  Di  sisi  lain,  beberapa 
operator algoritma genetika  yang digunakan dalam penelitian 
ini  hanya  berfokus  pada  pengurangan  lamanya  waktu  proses 
clustering, sehingga sangat memungkinkan untuk menerapkan 
operator lain untuk mendapatkan hasil yang lebih optimal.. 

DAFTAR PUSTAKA 

[1]   G.  A.  Mason  and  R.  D.  Jacobson,  ""Fuzzy  Geographically  Weighter 
Clustering,""  Proceedings  of  the  9th  International  Conference  on 
Geocomputation, pp. 1-7, 2007.  

[2]   R.  Haris,  P.  Sleight  and  R.  Webber,  Geodemographics,  GIS  and 
Neighbourhood Targeting, Cichester: John Wiley and Sons, Ltd, 2005.  

[3]   Z.  Huang,  ""Clustering  Large  Data  Sets  With  Mixed  Numeric  And 

Categorical Values,"" 1997.  

[4]   Z.  Xian,  Y.  TianBao  and  X.  HongXia,  ""A  New  Partition-based 
Clustering Algorithm For Mixed Data,"" International MultiConference 
of Engineers, 2017.  

[5]   J.  Ji,  W.  Pang,  C.  Zhou,  X.  Han  and  Z.  Wang,  ""A  fuzzy  k-prototype 
clustering  algorithm  for  mixed  numeric  and  categorical  data,"" 
Knowledge-Based Systems, p. 129–135, 2012.  

[6]   R. L. Haupt and Haupt, Practical Genetic Algorithms, New Yersey: John 

Wiley dan Sons, Inc, 2004.  

[7]   J. Han, M. Kamber and J. Pei, Data Mining: Concepts and Techni, San 

Francisco: itd: Morgan Kaufmann, 2012.  

[8]   M. I. Arsa and R. Nooraeni, Kombinasi Algoritma Genetika dan Fuzzy 
K-Prototype Untuk Pengelompokan Data Campuran, Jakarta: Politeknik 
Statistika STIS, 2018.  

[9]   R.  Nooraeni,  ""Metode  Klaster  Menggunakan  Kombinasi  Algoritma 
Klaster  K-Prototype  dan  Algoritma  Genetika  Untuk  Data  Bertipe 
Campuran,"" Politeknik Statisitka STIS, pp. 81-97, 2016.  

[10]  A. W. Wijayanto, A. Purwarianti and L. H. Son, ""Fuzzy geographically 
weighted  clustering  using  artificial  bee  colony:  An  efficient  geo-
demographic analysis algorithm and applications to the analysis of crime 
behavior in population,"" Springer Science+Business Media New York, 
pp. 377-398, 2015. 

[11]  E.  Wiransky,  “Hands-On  Genetic  Algorithms  With  Python”, 

Birmingham - Mumbai: Packt Publishing Ltd, 2020 

 8 / 8 

 
 
 
 
"
221709496,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pengembangan Sistem Informasi Kerja Sama 
Protokol dan Penyiapan Materi Pimpinan 
Modul Admin dan Modul Kerja Sama Pusat 

221709496, Adrian Devano 
Dosen Pembimbing: Farid Ridho, MT. 

Ringkasan—  Rekapitulasi  dan  pelaporan  kerja  sama  yang 
dilakukan oleh setiap satuan kerja di BPS merupakan salah satu 
tugas  dari  Sub  Bagian  Kerja  Sama  dan  Hubungan 
Kelembagaan. Proses rekapitulasi dan pelaporan disusun secara 
manual  menggunakan  Microsoft  Excel  dikarenakan  terdapat 
masalah  pada  Sistem  Informasi  Kerja  Sama  Protokol  dan 
Penyiapan Materi Pimpinan (SI-KSPM). Masalah yang terdapat 
dalam  sistem  berjalan  yaitu  hasil  rekapitulasi  MoU-PKS  yang 
terdapat  pada  sistem  berjalan  tidak  sesuai  dengan  kebutuhan 
laporan  admin  kepada  pimpinan.  Sehingga  perlunya  dilakukan 
pengembangan sistem informasi untuk dapat mengatasi masalah 
tersebut.  Dalam  mengembangkan 
ini,  peneliti 
menggunakan  metode  System  Development  Life  Cycle  (SDLC) 
pendekatan  model  waterfall  untuk  pengembangan  sistem  yang 
diaplikasikan  menggunakan  bahasa  pemrograman  PHP  dengan 
framework  Laravel.  Metode  uji  coba  dan  evaluasi  yang 
digunakan penulis dalam penelitian yaitu Black-Box Testing dan 
System Usability Scale (SUS). Hasil yang didapatkan dari Black-
Box  Testing  adalah  semua  fitur  berjalan  sesuai  dengan 
fungsinya,  dan  hasil  pengujian  SUS  yang  dilakukan 
menghasilkan  skor  sebesar  83,93  yang  berarti  sistem  yang 
diusulkan sudah baik dan layak digunakan. 

sistem 

Kata Kunci— Pengembangan Sistem, Sistem Informasi, SDLC, 

Model Waterfall, System Usability Scale. 

I.  LATAR BELAKANG 

Badan Pusat Statistik (BPS) merupakan salah satu lembaga 
pemerintahan  yang  bergerak  dibidang  statistik.  Dalam 
menjalankan tugasnya, BPS juga dapat melakukan kerja sama 
dengan  berbagai  pihak  seperti  instansi,  lembaga,  maupun 
perorangan.  Kerja  sama  dengan  pihak  lain  dapat  dilakukan 
oleh satuan kerja yang ada di BPS RI maupun di BPS Daerah. 
Kerja  sama  yang  dilakukan  oleh  setiap  satuan  kerja  baik  di 
BPS  RI  maupun  di  BPS  Daerah  harus  dilaporkan  untuk 
direkap  oleh  Sub  Bagian  Kerja  Sama  dan  Hubungan 
Kelembagaan yang ada di BPS RI. 

Berdasarkan  Peraturan  Kepala  BPS  Nomor  7  Tahun  2008 
tentang Organisasi dan Tata Kerja BPS pasal 66 menjelaskan 
Subbagian  Kerja  Sama  dan  Hubungan  Kelembagaan 
mempunyai  tugas  melakukan  penyiapan  materi  administrasi 
kerja  sama  dan  hubungan  kelembagaan  dengan  instansi 
pemerintah,  lembaga  tinggi  negara,  lembaga  swasta,  dan 
masyarakat baik dalam  negeri maupun luar negeri. Salah satu 
tugas  subbagian  Kerja  Sama  dan  Hubungan  Kelembangaan 
terdapat  pada  Peraturan  Kepala  BPS  Nomor  1  Tahun  2009 
tentang  Uraian  Tugas  Bagian,  Bidang,  Subdirektorat, 
Subbagian, Subbidang, dan Seksi BPS Bagian Keempat Pasal 
58  yang  berisi  melakukan  penghimpunan  dokumentasi 

perjanjian  kerja  sama  yang  dilakukan  satuan  organisasi 
dengan  instansi  pemerintah,  lembaga  tinggi  Negara,  instansi 
pemerintah daerah, lembaga, organisasi, dan perorangan, baik 
yang  bersumber  dari  Anggaran  Pendapatan  dan  Belanja 
Negara,  Anggaran  Pendapatan  dan  Belanja  Daerah,  maupun 
anggaran lainnya. 

Bagian  Kerja  Sama,  Protokol  dan  Penyiapan  Materi 
Pimpinan  memiliki  sebuah  sistem  yang  dinamakan  Sistem 
Informasi  Kerja  Sama  Protokol  dan  Penyiapan  Materi 
Pimpinan  (SI-KSPM).  Sistem  ini  dibangun  pada  tahun  2018 
untuk  membantu  dan  memudahkan  pengguna  atau  pegawai 
BPS  untuk  melakukan  tugasnya  yang  berkaitan  dengan  tugas 
pada Bagian KSPM. Sistem ini bersifat intranet dimana sistem 
hanya dapat diakses lewat Virtual Private Network (VPN) dan 
dapat diakses melalui site kspm-dev.bps.go.id.  

Sistem ini memiliki enam modul utama diantaranya:  
1.  Modul Perjalanan Dinas Luar Negeri, merupakan modul 
yang  berguna  bagi  pengguna  untuk  melakukan 
pelaporan  dokumen  seperti  paspor  dan  visa  untuk 
keperluan perjalanan dinas ke luar negeri.  

2.  Modul  Kerja  Sama  Daerah,  merupakan  modul  yang 
berguna  bagi  satuan  kerja  di  BPS  daerah  seperti  BPS 
provinsi maupun BPS kabupaten/kota untuk melakukan 
pelaporan  kerja  sama  yang  akan  dilakukan  dengan 
pihak lain.  

3.  Modul  Kerja  Sama  Pusat,  merupakan  modul  yang 
berguna  bagi  satuan  kerja  di  BPS  RI  untuk  melakukan 
pelaporan  kerja  sama  yang  akan  dilakukan  dengan 
pihak lain.  

4.  Modul Protokoler, merupakan modul yang berguna bagi 
pengguna  untuk  mengatur  jadwal  protokoler  pimpinan 
dan peminjaman ruangan di BPS.  

5.  Modul  Materi  Pimpinan,  merupakan  modul  yang 
berguna  bagi  pengguna  untuk  melakukan  penyiapan 
materi pimpinan disposisi dan sebagainya.  

6.  Modul  Admin,  merupakan  modul  yang  berguna  bagi 
admin  yaitu  bagian  KSPM untuk  melakukan  verifikasi, 
pengecekan dan perekapan dari semua laporan yang ada 
di setiap modulnya. 

Sistem yang berjalan pada Modul Kerja Sama Pusat adalah 
satuan kerja di BPS RI melakukan pengajuan kerja sama yang 
akan  dilakukan  dengan  mengisi  formulir  yang  ada  pada 
sistem,  kemudian  pada  admin  melakukan  pembahasan, 
penandatanganan,  dan  memberikan  laporan  evaluasi  terkait 
kerja  sama  yang  diajukan  oleh  satuan  kerja  di  BPS  RI. 
Masalah  yang  terdapat  pada  proses  tersebut  adalah  belum 
terdapatnya  formulir  bagi    satuan  kerja  di  BPS  RI  untuk 

 1 / 8 

 
 
 
 
memberikan  laporan  final  kerja  sama  yang  akan  dilakukan 
menindaklanjuti laporan evaluasi yang diberikan admin. 

Sistem yang berjalan pada Modul Admin adalah admin atau 
sub bagian kerja sama dan hubungan kelembagaan melakukan 
evaluasi  dan  perekapan  dokumentasi  perjanjian  kerja  sama 
yang  telah  diajukan  dan  dilakukan  oleh  satuan  kerja  baik  di 
BPS RI maupun di BPS Daerah. Pada Modul Admin terdapat 
fitur  Statistik  MoU-PKS  Pusat  dan  fitur  Statistik  MoU-PKS 
Daerah  yang  akan  menampilkan  rekapitulasi  dan  informasi 
yang  dilakukan  oleh  setiap  satuan  kerja.  Namun  dalam 
pengaplikasiannya, terdapat masalah pada kedua fitur tersebut 
diantaranya: 

1.  Pada  fitur  Statistik  MoU-PKS  Pusat  belum  beroperasi, 

sehingga tidak menampilkan informasi apapun 

2.  Pada fitur Statistik MoU-PKS Daerah 

i.  Tab Statistik MoU-PKS, grafik hanya menampilkan 
data  rekapitulasi  dari  semua  data  yang  ada  di 
database. 

ii.  Tab  Rekapitulasi  MoU-PKS,  tabel  dinamis  yang 
ditampilkan belum memiliki hierarki informasi data 
iii. Tab  Rekapitulasi  Saldo  PKS,  tabel  dinamis  yang 
ditampilkan  belum  memiliki  hierarki  informasi 
data.  

II.  TUJUAN PENELITIAN 

Tujuan  umum  penulisan  makalah 

ini  adalah  untuk 
mengembangkan Sistem Informasi Kerja Sama Protokoler dan 
Penyiapan  Materi  Pimpinan  khususnya  pada  Modul  Kerja 
Sama  Pusat  dan  Modul  Admin.  Pengembagan  ini  diharapkan 
dapat mengoperasionalkan kembali  sistem  yang ada sehingga 
dapat  memberikan  kemudahan  bagi  pengguna  dalam 
melakukan pelaporan kerja sama dan   

Tujuan khusus penulisan makalah ini antara lain::  
1.  Menyediakan  formulir  pengisian  laporan  final  MoU-

PKS untuk satuan kerja di BPS RI. 

2.  Menggabungkan tampilan Statistik MoU-PKS Pusat dan 

MoU-PKS Daerah menjadi satu fitur.  

3.  Menyediakan  pilihan 

tahun  dan  daerah  untuk 

menampilkan grafik pada tab Statistik MoU-PKS. 
jumlah  pada 

rekapitulasi 

4.  Menampilkan 

tabel 

tab 

Rekapitulasi MoU-PKS secara berhierarki.  

5.  Menampilkan 

tabel 

rekapitulasi 

saldo  pada 

tab 

Rekapitulasi Saldo PKS secara berhierarki. 

III. PENELITIAN TERKAIT 

Penelitian terkait yang menjadi rujukan penulis pada adalah 
penelitian  yang dilakukan oleh (Susantro & Andriana, 2016). 
Penelitian  tersebut  membahas  tentang  perbandingan  antara 
model waterfall dan model prototyping yang ada pada metode 
SDLC.  Penelitian  ini  menggunakan  pendekatan  kuantitatif 
dengan  kuantitatif  dengan  metode  deskriptif  untuk 
membandingkan  dua  model  yang  ada  pada  metode 
tersebut.  Kesimpulan  dari  hasil 
pengembangan  sistem 
penelitian  yang  dilakukan  adalah  model  waterfall  cocok 
digunakan untuk  mengembangkan sistem  yang kebutuhannya 
dapat diketahui dari awal. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Layanan 

Selanjutnya  penelitian  yang  dilakukan  oleh  (Purnomo  & 
Widodo,  2016)  yang  berjudul  “Perancangan  Aplikasi 
HTML5 
Berbasis 
Kesehatan 
Pencarian 
Geolocation”.  Penelitian 
ini  merupakan  pengembangan 
aplikasi  pencarian  layanan  kesehatan  berdasarkan  jadwal  dan 
jarak  lokasi  terhadap  posisi  pengguna  menggunakan  HTML5 
Geolocation  API  dan  dibangun  menggunakan  framework 
laravel. Penelitian ini menyebutkan bahwa laravel merupakan 
salah  satu  framework  PHP  yang  dapat  dengan  mudah 
meningkatkan  kualitas  sistem  dan  dapat  dengan  mudah 
berhubungan dengan basis data menggunakan migration. 

IV. METODE PENELITIAN  

A.  Metode Pengumpulan Data 

Dalam  mengumpulkan  data,  penulis  menggunakan 

beberapa teknik diantaranya: 
1.  Teknik Wawancara. 

Teknik wawancara merupakan komunikasi dua arah 
atau  lebih  antara  narasumber  dengan  pengembang 
sistem.  Kelebihan  dari 
teknik  wawancara  adalah 
memberikan  kesempatan  kepada  pengguna  atau  orang 
yang  diwawancarai  dapat  memberikan  jawaban  secara 
terbuka,  dapat  memunculkan  pertanyaan  baru  dengan 
situasi  tersebut  dan  juga  jawaban  dari  narasumber 
dapat dinilai kebenarannya [6]. 

2.  Teknik Kuisioner. 

Teknik  kuesioner  merupakan  cara  mendapatkan 
informasi  tanpa  harus  bertemu  dengan  narasumber 
dengan  memberikan  beberapa  pertanyaan  yang  telah 
disusun  pengembang  sistem  [6].  Teknik  kuesioner  ini 
digunakan  penulis  untuk  mengumpulkan  data  terkait 
uji kelayakan dan evaluasi sistem yang akan diusulkan. 
Salah  satu  teknik  kuesioner  yang  penulis  gunakan 
adalah untuk melakukan pengujian dan evaluasi System 
Usability  Scale 
(SUS).  System  Usability  Scale 
merupakan  suatu  cara  mengevaluasi  sebuah  hal  baru 
seperti  produk,  perangkat,  situs  web  dan  sebagainya. 
tahun  1986,  John  Brooke  membuat  SUS 
Pada 
menggunakan 
sering 
likert.  Skala 
diasumsikan sebagai basis daripada pertanyaan dengan 
pilihan  paksaan  dimana  responden  mengindikasikan 
tingkat  persetujuan  maupun  ketidaksetujuan  mereka 
menggunakan 5 skala poin [2]. 

likert 

skala 

B.  Metode Pengembangan Sistem 

1.  Pengembangan Sistem 

Menurut  Jogiyanto  (2005),    pengembangan  sistem 
merupakan  suatu  cara  memperbaiki  sistem  yang  lama 
agar dapat digunakan sesuai dengan kebutuhan saat ini 
[4].  

2.  System Development Life Cycle (SDLC) 

System  Development  Life  Cycle  merupakan  tahapan 
pengembangan  yang  dimulai  dari  perencanaan  studi 
implementasi  untuk  memenuhi 
kelayakan  sampai 
kebutuhan 
informasi. 
sistem 
Pengembangan sistem dapat dilakukan sendiri ataupun 
melalui  bantuan  pihak  lainnya  [8].  Menurut  Dari 

pengguna 

dalam 

 2 / 8 

 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

(2015, 223) tahapan dari SDLC antara lain  merancang 
sistem,  menganalisis 
sistem, 
mengimplementasikan dan memelihara sistem tersebut. 
Model  SDLC  yang  sering  digunakan  pengembang 
sistem adalah model waterfall dan prototyping [10]. 

sistem,  mendesain 

V.  KERANGKA PIKIR 

Berikut  merupakan  diagram  alur  kerangka  pikir  penulis 

dalam penelitian ini: 

3.  Model Waterfall 

Menurut  Rosa  dan  Shalahudin,  model  waterfall 
merupakan  salah  satu  model  pengembangan  yang 
dilakukan  berurutan  dari  menganalisis,  mendesain, 
pengkodean,  menguji  dan  tahapan  pendukung  [9]. 
Tahapan  model  waterfall  menurut  Suryanto  (2016) 
sebagai berikut: 
i.  Analisis  

Tahapan  untuk  menemukan  masalah  dan 
hambatan  yang  ada  pada  sistem  dan  merumuskan 
kebutuhan  yang  diinginkan  oleh  pengguna  sistem 
untuk dilakukan perbaikan. 

ii.  Desain 

Tahapan  merancang  sistem  usulan  dari  struktur 
tampilan,  data,  sistem  dan  tahapan  pengkodean 
yang akan dilakukan. 

iii. Pengkodean 

Tahapan  menerjemahkan  desain  yang 

telah 
sistem  usulan  dengan 
dirancang  ke  dalam 
menggunakan  bahasa  yang  dimengerti  oleh 
komputer. 
iv.  Pengujian 

Tahapan  mengecek  hasil  implementasi  untuk 

memastikan semua  

bertujuan 

Pengujian 

untuk  meminimalisir 
kesalahan  dan  memastikan  hasil  yang  ditampilkan 
sudah sesuai dengan yang diinginkan 

v.  Pemeliharaan 

Pemeliharaan  bertujuan  untuk  mengantisipasi 
perubahan yang terjadi setelah program dikirimkan 
kepada  pengguna  karena  adanya  kesalahan  baru 
yang muncul atau tidak terdeteksi saat pengujian. 

4.  Framework Laravel 

Laravel  merupakan  framework  bahasa  PHP  yang 
disusun  untuk  menciptakan  sistem  yang  berkualitas 
dengan  mempermudah  dilakukannya  pengembangan 
dan perbaikan sehingga dapat mengurangi penggunaan 
biaya dan waktu dalam implementasinya [11]. Menurut 
Naista  (2016),  Laravel  merupakan  framework  PHP 
versi  terbaru  karena  harus  menggunakan  versi  PHP  di 
atas 5.3 [7]. 
5.  Black-Box Testing 

Black  Box  Testing  merupakan  sebuah  pengujian 
sistem  yang  berfokus  pada  hasil  dan  masukan  yang 
dilakukan  [5].  Menurut  Lewis  pengujian  black  box 
adalah  pengujian  yang  menganggap  sistem  usulan 
tersebut  sebagai  kota  hitam  tanpa  memedulikan  isi 
dalamnya [7]. 

Gambar 2. Alur kerangka pikir 

Penelitian ini menggunakan metode pengembangan System 
Development Life Cycle dengan tahapan yang akan dilakukan 
oleh penulis sebagai berikut: 

1.  Wawancara 

Wawancara dilakukan penulis kepada subject matter 
yaitu  salah  satu  staf  Sub  Bagian  Kerja  Sama  dan 
Hubungan  Kelembagaan  di  BPS  RI  yang  merupakan 
admin  dari  Sistem  Informasi  KSPM.  Wawancara 
tersebut  bertujuan  untuk  dapat  memberikan  informasi 
terkait  sistem  informasi  yang  akan  dikembangkan  dan 
masalah  masalah  yang ditemukan oleh pengguna  pada 
sistem berjalan. 
2.  Analisis Kebutuhan 

dan  merumuskan 

Penulis  menemukan  masalah  yang  ditemukan 
pengguna 
untuk 
merancang  sistem  usulan  yang  dapat  mengatasi 
masalah  tersebut.  Masalah  dan  kebutuhan  tersebut 
didapatkan  penulis  melalui  hasil  wawancara  yang 
dilakukan  dan  kemudian  disusun  menjadi  tujuan  dari 
penelitian ini. 

kebutuhan 

 3 / 8 

 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar  4  menunjukan  bahwa  pada  fitur  Statistik 
yang 
MoU-PKS 
ditampilkan  pada  kedua  tab  yang  ada  di  fitur 
tersebut. 

informasi 

terdapat 

belum 

ii.  Fitur Statistik MoU-PKS Daerah 

Pada  fitur  Statistik  MoU-PKS  Daerah  terdapat 
tiga  tab  yaitu  statistik  MoU,  Rekapitulasi  MoU-
PKS dan Rekapitulasi Saldo PKS..  

Gambar 5. Fitur Statistik MoU-PKS Daerah (Tab Statistik MoU) 

Pada  Gambar  5,  merupakan  tampilan  pada  tab 
Statistik  MoU  yang  menampilkan  grafik  jumlah 
MoU dan PKS menurut bulan. Berdasarkan analisis 
yang  dilakukan,  masalah  yang 
terdapat  pada 
tampilan  ini  adalah  belum  adanya  filter  yang 
membagi  data  yang  ditampilkan  berdasarkan 
wilayah  ataupun  tahun.  Sehingga  grafik  yang 
ditampilkan pada sistem berjalan merupakan grafik 
dari semua data yang ada di database. 

3.  Perancangan Sistem 

Penulis  melakukan  perancangan  sistem  sesuai 
rumusan  kebutuhan  pengguna.  Sistem 
dengan 
dirancang  dengan  bahasa  pemrograman  PHP  yang 
dibangun menggunakan framework laravel. 

4.  Pengujian Sistem 

Penulis  menggunakan  Black-Box  Testing  untuk 
menguji  fungsional  dan  kegunaan  dari  sistem  usulan 
yang telah dibangun. Pengujian lainnya yang dilakukan 
penulis  adalah  menggunakan  System  Usability  Scale 
(SUS)  yang  berguna  untuk  mengukur  kualitas  dan 
kelayakan sistem usulan. 

VI. HASIL DAN PEMBAHASAN 

A.  Analisis Sistem Berjalan 

1.  Modul Kerja Sama Pusat 

Gambar 4 merupakan proses bisnis pengajuan MoU-
PKS yang akan dilakukan oleh satuan kerja yang ada di 
BPS RI. 

Gambar 3. Proses bisnis pengajuan MoU-PKS BPS RI 

Proses  bisnis  pada  Gambar  4  dimulai  dari  satuan 
kerja yang ada di BPS RI melakukan pengajuan MoU-
PKS.  Kemudian  admin  melakukan  pembahasan, 
penandatanganan  dan  memberikan  laporan  evaluasi 
dari  MoU-PKS  yang  telah  diajukan.  Berdasarkan 
analisis  yang  dilakukan,  masalah  yang  terdapat  pada 
proses  tersebut  adalah  belum  terdapatnya  formulir 
satuan  kerja  di  BPS  RI 
laporan 
menindaklanjuti 
telah 
dikirimkan oleh admin. 

final  dari 

evaluasi 

laporan 

yang 

2.  Modul Admin 

i.  Fitur Statistik MoU-PKS Pusat 

Berikut  ini  merupakan  tampilan  Statistik  MoU-

PKS Pusat pada sistem berjalan 

Gambar  6.  Fitur  Statistik  MoU-PKS  Daerah  (Tab  Rekapitulasi 
MoU-PKS) 

Gambar 4. Fitur Statistik MoU-PKS BPS RI 

Gambar  7.  Fitur  Statistik  MoU-PKS  Daerah  (Tab  Rekapitulasi 
Saldo PKS) 

 4 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

C.  Rancangan Sistem Usulan 

Berdasarkan  permasalahan  dan  kebutuhan  yang  ada, 

penulis merancang sistem usulan sebagai berikut: 
1.  Modul Kerja Sama Pusat 

Gambar  10  merupakan  bisnis  proses  usulan  pada 
fitur  pengajuan  kerja  sama  di  Modul  Admin.  Satuan 
kerja di BPS Pusat dapat mengisi formulir laporan final 
kerja  sama  yang  diajukan  setelah  admin  memberikan 
laporan evaluasi MoU-PKS. 

tampilan  pada 

Gambar  6  merupakan 

tab 
Rekapitulasi  MoU-PKS  yang  menampilkan  jumlah 
MoU-PKS  yang dilakukan oleh setiap satuan kerja 
yang ada di BPS daerah dan Gambar 7  merupakan 
tampilan  pada  tab  Rekapitulasi  Saldo  PKS  yang 
menampilkan informasi kerja sama yang dilakukan 
oleh  setiap  satuan  kerja  yang  ada  di  BPS  Daerah. 
Berdasarkan analisis, masalah yang ada pada kedua 
tampilan  ini  adalah  tabel  memuat  data  sebanyak 
kabupaten/kota  yang  ada  di  Indonesia,  sehingga 
tampilan  tabel  dibagi  menjadi  beberapa  halaman 
dan setiap halamannya menampilkan beberapa data. 
Hal  ini  menyebabkan  hasil  export  file  yang  ada  di 
atas  tabel  menjadi  tidak  informatif  seperti  gambar 
dibawah ini. 

Gambar 8. Hasil Export File Excel pada Tab Rekapitulasi MoU-
PKS 

Gambar  8  merupakan  contoh  hasil  export  file 
dalam format excel pada tab rekapitulasi MoU-PKS 
dimana menampilkan jumlah MoU dan PKS satuan 
kerja di BPS. 

Gambar 9. Hasil Export File Excel pada Tab Rekapitulasi Saldo 
PKS 

Gambar  9  merupakan  contoh  hasil  export  file 
dalam format excel pada tab rekapitulasi saldo PKS 
dimana  menampilkan  saldo  PKS  satuan  kerja  di 
BPS. 

B.  Analisis Kebutuhan 

Berdasarkan  analisis 

sistem  berjalan,  dibutuhkan 
pengembangan  terhadap  sistem  yang  dapat  menyelesaikan 
permasalahan yang ada. Kebutuhan yang penulis rumuskan 
sebagai berikut: 
1.  Menyediakan  fitur  kuesioner  laporan  akhir  kerja  sama 

yang dilakukan oleh satuan kerja di BPS RI. 

2.  Menggabungkan fitur tampilan statistik MoU-PKS BPS 

RI dan BPS Daerah. 

3.  Menyediakan  pilihan  tahun  dan  wilayah  pada  tampilan 

grafik yang ada pada tab Statistik MoU. 

4.  Membangun hierarki tampilan informasi data pada tabel 
dinamis yang ada pada tab Rekapitulasi MoU-PKS dan 
Rekapitulasi Saldo PKS. 

Gambar  10.  Proses  bisnis  sistem  usulan  pengajuan  MoU-PKS 
satuan kerja BPS RI 

2.  Modul  Admin  (Fitur  Statistik  MoU-PKS  Pusat  dan 

Daerah 

Menggabungkan  data  laporan  kerja  sama  satuan 
kerja  BPS  RI  dan  BPS  Daerah  menjadi  satu  fitur 
dengan  nama  Statistik  MoU-PKS  Pusat  dan  Daerah. 
Fitur ini akan dibagi menjadi tiga tampilan yaitu: 
i.  Tab Statistik MoU-PKS 

Tab Statistik MoU-PKS menampilkan grafik bar 
chart  yang  berisi  data  jumlah  MoU-PKS.  Sistem 
usulan  yang  dikembangkan  oleh  peneliti  adalah 
menambahkan  pilihan  tahun  dan  pilihan  wilayah 
untuk grafik tersebut. Sehingga grafik tersebut akan 
menampilkan  data  jumlah  MoU-PKS  berdasarkan 
pilihan 
tahun  dan  wilayah  yang  dipilih  oleh 
pengguna. 

ii.  Tab Rekapitulasi MoU-PKS 

rekapitulasi 

Tab  Rekapitulasi  MoU-PKS  menampilkan  tabel 
rekapitulasi jumlah MoU-PKS. Pada sistem usulan, 
tampilan 
jumlah  MoU-PKS  akan 
ditampilkan menjadi beberapa tingkatan. Tingkatan 
tersebut  digambarkan  pada  Gambar  11  yang 
menunjukkan  proses  bisnis  sistem  usulan  untuk 
tampilan tabel tab Rekapitulasi MoU-PKS. 

 5 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

D.  Implementasi Sistem 

1.  Modul Kerja Sama Pusat 

Gambar 13. Fitur Entri MoU BPS RI 

Gambar 

13  menunjukkan 

implementasi 
antarmuka  kuesioner  laporan  final  MoU  untuk 
satuan kerja BPS RI. 

Gambar 14. Fitur Entri PKS BPS RI 

Gambar 

13  menunjukkan 

implementasi 
antarmuka  kuesioner  laporan  final  PKS  untuk 
satuan kerja BPS RI 

Gambar 11. Proses Bisnis Sistem Usulan tampilan Rekapitulasi 
MoU-PKS 

iii. Tab Rekapitulasi Saldo PKS 

Tab Rekapitulasi Saldo PKS  menampilkan tabel 
rekapitulasi  saldo  perjanjian  kerja  sama  yang 
dilakukan  oleh  satuan  kerja.  Pada  sistem  usulan, 
tampilan  rekapitulasi  akan  ditampilkan  menjadi 
beberapa 
tersebut 
digambarkan  pada  Gambar  12  yang  menunjukkan 
proses bisnis sistem usulan untuk tampilan tabel tab 
Rekapitulasi Saldo PKS. 

Tingkatan 

tingkatan. 

2.  Modul Admin 

i.  Tab Statistik MoU-PKS 

Gambar  12.  Proses  bisnis  sistem  usulan  tampilan  Rekapitulasi 
Saldo PKS 

Gambar 15. Fitur Statistik MoU-PKS Pusat Daerah 

Gambar 15 menunjukkan tab Statistik MoU-PKS 
yang  menampilkan  grafik  jumlah  MoU  dan  PKS. 
Pada  tab  ini  terdapat  penambahan  fitur  pilihan 
tahun  dan  wilayah  di  bagian  atas  grafik  yang 
berfungsi  untuk  menampilkan  grafik  bar  chart 
berdasarkan  tahun  dan  wilayah  yang  dipilih  oleh 
pengguna seperti Gambar 15.  

 6 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
ii.  Tap Rekapitulasi MoU-PKS 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 16. Tab Rekapitulasi MoU-PKS 

Gambar 13. Rekapitulasi Saldo MoU-PKS Provinsi Aceh 

Gambar 16 menunjukkan tab Rekapitulasi MoU-
PKS  yang  tampilan  awalnya  menampilkan  tabel 
dengan  data  rekapitulasi  jumlah  MoU-PKS  yang 
disusun  menjadi  35  wilayah  diantaranya  34 
Provinsi dan 1 Pusat. Gambar 17 merupakan contoh 
tampilan  rekap  kota/kabupaten  ketika  pengguna 
memilih Provinsi Aceh pada gambar 16. 

Gambar 11. Rekapitulasi MoU-PKS Provinsi Aceh 

iii. Tap Rekapitulasi Saldo PKS 

Usulan pada tab Rekapitulasi Saldo PKS terlihat 
pada  gambar  12  yang  menampilkan  rekap  saldo 
yang disusun menjadi 35 wilayah yaitu 34 provinsi 
dan 1 pusat. Dicontohkan ketika pengguna memilih 
Provinsi Aceh, maka tabel akan menampilkan rekap 
tersebut 
kabupaten/kota  yang  ada  di  Provinsi 
seperti  tampilan  pada  gambar  13.  Gambar  14 
menunjukkan informasi detail dari kerja sama yang 
dilakukan  oleh  satuan  kerja  (kabupaten/kota)  yang 
dipilih pengguna pada gambar 13. 

Gambar 12. Rekapitulasi Saldo MoU-PKS 

Gambar 14. Rincian MoU-PKS BPS Provinsi Aceh 

E.  Pengujian Sistem 

Pengujian yang dilakukan penulis antara lain: 
1. Uji Black-Box 

Masukan dan Keluaran Uji 

Masukan 

Keluaran 

Hasil 

Modul Kerja Sama Pusat 

Tombol buat 
laporan final 

Upload laporan 
final 

Fitur Statistik 
MoU-PKS 
Pusat dan 
Daerah 
Pilihan tahun 
dan wilayah 
Memilih salah 
satu wilayah 
pada tab 
Rekapitulasi 
MoU-PKS 

Memilih salah 
satu wilayah 
pada tab 

Muncul ketika proses 
pengajuan mencapai 
status laporan evaluasi 
dari admin 

Merubah status 
pengajuan menjadi 
laporan final 

Modul Admin 

Menampilkan data 
kerja sama satuan 
kerja di BPS RI dan 
Daerah 
Merubah data pada 
grafik secara dinamis 

Menampilkan rekap 
jumlah kerja sama 
satuan kerja dari 
wilayah yang dipilih 

Menampilkan rekap 
saldo kerja sama 
satuan kerja dari 

Sesuai 

Sesuai 

Sesuai 

Sesuai 

Sesuai 

Sesuai 

 7 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Rekapitulasi 
Saldo PKS 

wilayah yang dipilih 

2.  Uji System Usability Scale (SUS) 

Pengujian  dengan  metode  SUS  dilakukan  oleh 
penulis  dengan  menyusun  kuesioner  yang  berisi  10 
pertanyaan  menggunakan  Google  Form  yang  disebar 
kepada  responden.  Responden  pada  pengujian  ini 
terdiri  dari  satu  orang  pegawai  BPS  dan  13  orang 
mahasiswa Politeknik Statistika STIS.  

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

[6]  Kristanto,  A.,  Perancangan  Sistem 

Yogyakarta : Gava Media.  

Informasi  dan  Aplikasinya. 

[7]  Luthfi,  F.,  Penggunaan  Framework  Laravel  dalam    Rancang  Bangun 
Modul Back-End Artikel Website Bisnisbisnis.ID. JISKa, Vol. 2, No. 1, 
pp. 3441, Mei 2017. 

[8]  Martias, A., Analisa Kecukupan Penerapan Pengawasan Internal dengan 
Metode System Development Life Cycle PT. XYZ, Moneter, pp. 139148. 
[9]  Nawang,  M.,  Kurniawati,  L.,  &  Duta,  D.,  Rancang  Bangun  Sistem 
Informasi Pengolahan Data Persediaan Barang Berbasis Desktop dengan 
Model  Waterfall,  Jurnal  PILAR  Nusa  Mandiri,  Vol.  13,  No.  2,  pp. 
233238, 2 September 2017. 

[10] Susanto,  R.,  &  Andriana,  A.  D.,  Perbandingan  Model  Waterfall  dan 
Prototyping  untuk  Pengembangan  Sistem  Informasi,  Majalah  Ilmiah 
UNIKOM, pp 4146, 2016 

[11] Widodo,  B.  P.,  &  Purnomo,  H.  D.,  Perancangan  Aplikasi  Pencarian 
Layanan  Kesehatan  Berbasis  HTML5  Geolocation.  Jurnal  Sistem 
Komputer, Vol. 6, No. 1,  pp 4451, Mei 2016. 

Gambar 15. Interpretasi nilai SUS menurut Bangor dkk 

Gambar  15  merupakan  skala  dan  peringkat  nilai 
SUS  yang  diinterpretasikan  ke  dalam  bentuk 
Acceptability  Range,  Grade  Scale,  dan  Adjective 
Ratings (Bangor, Kortum, & Miller, 2009). Hasil rata-
rata  yang  diperoleh  dari  perhitungan  SUS  terhadap 
sistem  usulan  SI-KSPM    adalah  83,93.  Berdasarkan 
hasil  uji  dan  interpretasi  dari  gambar  15,  dapat 
disimpulkan  bahwa  sistem  usulan 
layak  untuk 
digunakan dengan grade B dan ratings excellent. 

VII. 

PENUTUP 

Berdasarkan  hasil  dan  pembahasan,  kesimpulan  yang 

dihasilkan penulis sebagai berikut: 

1.  Sistem  telah  menyediakan  laporan  final  MoU-PKS 

untuk satuan kerja di BPS RI. 

2.  Sistem  telah  menampilkan  rekap  laporan  MoU-PKS 

BPS RI dan Daerah dalam satu fitur. 

3.  Sistem  telah  menyediakan  pilihan  tahun  dan  wilayah 
untuk tampilan grafik bar chart pada tab Statistik MoU-
PKS. 

4.  Sistem  telah  menampilkan  rekap  jumlah  MoU-PKS 

secara berhierarki. 

5.  Sistem  telah  menampilkan  rekap  saldo  PKS  secara 

berhierarki. 

DAFTAR PUSTAKA 
[1]  Bangor, A., Kortum, P., & Miller, J., Determining What Individual SUS 
Scores  Mean:  Adding  an  Adjective  Rating  Scale.  Journal  of  Usability 
Studies, Vol. 4, Issue 3, pp. 114123, May 2009. 

[2]  Brooke,  J.,  SUS:A  Retrospective,  Journal  of  Usability  Studies¸  Vol.  8, 

Issue 2, pp. 1940, January 2013. 

[3]  Dar,W.,  Peranan  Metode  System  Development  Life  Cycle  pada 
Pembuatan  Sistem  Informasi  Penjualan  Produk  Batik  Kurowo  Jakarta, 
Jurnal Katulistiwa, Vol. 3 No. 2, pp. 222228, Desember 2015. 

[4]  Hermandra, A. D., & Anofrizen, Pengembangan Sistem Informasi, Jurnal 
Rekayasa  dan  Manajemen    Sistem  Informasi,  Vol.  2,  No.  1,  pp.  1114, 
Februari 2016. 

[5]  Khan, M E., Different Approaches to Black  Box  Testing  Technique  for 
Finding  Errors,  International  Journal  of  Software  Engineering  & 
Applications, Vol. 2, No. 4, pp. 3140, October 2011. 

 8 / 8 

 
 
 
 
 
 
 
 
 
"
221709491,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Data Matching Hasil Survei BPS dengan Big Data 
Terkait Perhotelan di Pulau Jawa & Bali 

Addiena Noorfirdausi Sabilla Setiadi (221709491, 4SD1) 
Dosen Pembimbing: Ibnu Santoso, S.S.T., M.T 

layanan  penyedia 

Ringkasan  —  Melalui  kegiatan  Survei  Statistik  Jasa 
Akomodasi  Bulanan  (VHTS)  BPS  melakukan  pengumpulan 
data/informasi  terkait  jasa  akomodasi  yang  salah  satunya 
tentang  karakteristik  hotel  di  Indonesia.  BPS  pun  sudah 
melakukan  pengembangan  big  data  yang  diharapkan  hasilnya 
dapat  dimanfaatkan  sebagai  data  pendukung  hasil  survei  BPS, 
salah satunya untuk survei VHTS ini. Pendekatan big data untuk 
data  perhotelan  didapatkan  dari 
jasa 
akomodasi  secara  online,  salah  satunya  yaitu  situs  Agoda. 
Pengambilan  data  dilakukan  dengan  metode  web  scraping. 
Setelah  dilakukan  data  preprocessing  dan  filter  data  untuk 
Provinsi  Jawa  dan  Bali  saja,  dilakukanlah  pencocokan  data 
antara hasil scraping dengan data direktori VHTS BPS dan juga 
data  TPK  BPS  triwulan  pertama  tahun  2020.  Metode  pertama 
yang  digunakan  adalah  kombinasi  antara  algoritma  cosine 
similarity  dengan  pembobotan  TF-IDF  dan  model  probabilistik 
N-gram.  Lalu  metode  kedua  yang  digunakan  adalah  metode 
fuzzy string matching dengan variasi levenshtein distance. Metode 
yang  berbeda  menghasilkan  jumlah  data  cocok  yang  berbeda 
pula.  Masing-masing  dari  metode  pencocokan  ini  juga  memiliki 
kelebihan maupun kekurangan masing-masing. 

Kata  Kunci  —  data  matching,  VHTS,  big  data,  akomodasi, 

TPK. 

I.  LATAR BELAKANG 

Sektor  pariwisata  memiliki  kontribusi  yang  cukup  besar 
dan kuat perannya dalam pembangunan nasional serta sebagai 
penggerak  ekonomi  masyarakat.  Bisnis  pariwisata  di 
Indonesia cukup menggembirakan mengingat terdapat potensi 
destinasi  wisata  yang  cukup  banyak,  seperti  keindahan  alam, 
keragaman, dan keunikan budaya  yang terdapat di Indonesia. 
Salah  satu  industri  pariwisata  yang  cukup  banyak  adalah 
perusahaan/usaha  akomodasi  atau  hotel/penginapan 
[1]. 
Capaian  sektor  usaha  pariwisata  pada  tahun  2019  lalu 
dibidang  ekonomi  berkontribusi  untuk  PDB  nasional  sebesar 
4,01%  dengan,  menghasilkan  devisa  sebesar  Rp  280  triliun, 
dan berperan dalam penyerapan tenaga kerja sebanyak 13 juta 
orang [2]. 
Selain 

pembangunan 
infrastrukturnya  sudah  jelas  lebih  kompleks  daripada  pulau 
lainnya  yang  ada  di  Indonesia,  Pulau  Bali  menjadi  opsi 
selanjutnya  untuk  analisis  kajian 
terkait  akomodasi  di 
Indonesia.  Karena  seperti  yang  kita  ketahui  bahwa  Indonesia 
sangat  terkenal  dengan  keindahan  Pulau  Dewata  Bali  sejak 
dulu  kala,  sehingga  pembangunan  infrastruktur  di  Bali  jauh 
lebih  baik  dibandingkan  dengan  pulau  lainnya  sebagai  pulau 
tujuan wisatawan baik mancanegara maupun domestik. 

yang  mana 

Pulau 

Jawa 

Salah  satu  faktor  penunjang  pariwisata 

ialah  sektor 
perhotelan. Berbagai macam cara juga dilakukan pelaku bisnis 
perhotelan  untuk  menarik  minat  konsumen  menginap 
dihotelnya.  Salah  satunya  dalah  dengan  melakukan  promosi 
via online, seperti media sosial, online community dan website 
lainnya  yang  memungkinkan  terjadinya  pertukaran  informasi 
dengan  mudah  dan  fleksibel,  sehingga  dapat  mempermudah 
penjualan jasa penyediaan akomodasi kamar hotel. Salah satu 
situs  travel  online  yang  banyak  sudah  banyak  dikenal 
masyarakat adalah Agoda.com. 

Agoda.com  adalah  salah  satu  situs  perantara  pemesanan 
kamar  hotel  dengan  perkembangan  terpesat  di  seluruh  dunia 
dengan daftar ratusan ribu hotel dan layanan dalam 38 bahasa 
yang  berbeda  [3].  Informasi  mengenai  jasa  akomodasi  dari 
situs travel online tersebut dapat dikumpulkan sehingga dapat 
menghasilkan  angka  yang  memberikan  informasi  terkait 
kondisi  dan  fenomena  yang  terjadi  di  suatu  wilayah  terkait. 
Pengumpulan  data  dari 
situs  web  biasanya 
menggunakan metode web scraping atau web crawling. 

suatu 

Dikarenakan  saat  ini  merupakan  era  dari  big  data,  dengan 
karakteristik data dengan volume besar, bervariasi, perubahan 
yang sangat cepat, dan kredibilitas yang butuh verifikasi lebih 
lanjut.  Oleh  karenanya  big  data  cenderung  tidak  terstruktur, 
sehingga perlu pengolahan untuk mendapatkan informasi yang 
bermanfaat  dari  data  besar  tersebut.  Perkembangan  teknologi 
pada revolusi industri 4.0 ini telah mengubah internet menjadi 
pusat  informasi  dengan  berbagai  macam  data  dari  beragam 
sumber.  Hal  ini  menggiring  big  data  menjadi  salah  satu 
alternatif  pengumpulan  data  selain  menggunakan  metode 
konvensional seperti sensus dan survei. 

Berdasarkan  Undang-Undang  Nomor  16  Tahun  1997 
tentang  statistik,  dalam  penyelenggaraan  pemerintahan, 
lembaga  yang  memiliki  wewenang  dan 
tugas  sebagai 
penyedia data  statistik dasar dan sektoral di Indonesia adalah 
Badan Pusat Statistik atau BPS. BPS selaku lembaga nasional 
penyedia  statistik  ofisial  memiliki  kewenangan  dan  tanggung 
jawab dalam melakukan pendataan, seperti dalam pelaksanaan 
sensus dan survei di Indonesia. 

Salah satu statistik yang disusun oleh BPS adalah Statistik 
Jasa  Akomodasi.  Dalam  pengumpulan  data  terkait  Statistik 
Jasa  Akomodasi  ini  BPS  melakukan  kegiatan  survei  bulanan 
untuk  hotel  berbintang,  serta  survei  tahunan  untuk  hotel 
berbintang  dan  tidak  berbintang.  Tujuan  dari  kegiatan  survei 
ini  adalah  untuk  mengumpulkan  data  tingkat  penghunian 
kamar  hotel/akomodasi,  rata-rata  lamanya  tamu  menginap, 
dan  jumlah  tamu  yang  yang  menginap  di  hotel/akomodasi. 
Data/informasi 
landasan 
perencanaan  dan  evaluasi,  baik  oleh  instansi  pemerintah 

ini  diharapkan  dapat  dijadikan 

 1 / 8 

 
 
 
 
 
 
maupun  swasta,  untuk  menentukan  kebijakan  terkait  usaha 
tersebut [1]. 

Tingkat  Penghunian  Kamar  (TPK)  hotel  adalah  persentase 
perbandingan  antara  banyaknya  malam  kamar  yang  terpakai 
dengan banyaknya  malam  kamar  yang tersedia  yang berguna 
untuk  memberikan  gambaran  berapa  persen  kamar  yang 
tersedia  pada  akomodasi  terisi  oleh  tamu  yang  menginap 
dalam  suatu  waktu  tertentu  dan  menunjukkan  apakah  suatu 
akomodasi diminati oleh pengunjung atau tidak [4], sehingga 
dapat dilihat apakah di suatu daerah masih kurang keberadaan 
akomodasi atau tidak untuk memenuhi kebutuhan masyarakat 
(wisatawan). 

Perkembangan Tingkat Penghunian Kamar pada 
Hotel Berbintang di Indonesia Periode Tahun 
2015-2020 

70

60

50

40

30

20

10

0

2015

2016

2017

2018

2019

2020

Gambar 1. Perkembangan TPK hotel di Indonesia tahun 2015-2020 

Perkembangan  TPK  hotel  berbintang  di  Indonesia  selama 
lima tahun terkahir memiliki pola musiman yang hampir sama, 
dimana akan meningkat di masa liburan dan akan menurun di 
masa hari kerja biasa. Namun mulai pada saat pandemi Covid-
19  mulai  mewabah,  TPK  hotel  berbintang  di  Indonesia 
mengalami  penurunan  drastis  dimana  dari  bulan  Februari 
2020  pada  saat  sebelum  pandemi  sampai  dengan  Juni  2020, 
terjadi penurunan nilai TPK sebesar 29.52 poin. 

Selain  itu,    BPS  memiliki  beberapa  isu  di  dalam  proses 
pengumpulan  data  yang  disebabkan  keterbatasan  constraint 
waktu,  tenaga,  dan  biaya  serta  kerangka  sampel  yang  belum 
tersedia.  Salah  satu  solusi  yang  dapat  digunakan  untuk 
mengatasi permasalahan ini adalah dengan memanfaatkan big 
data [5]. BPS terutama subdit Pengembangan Model Statistik 
(PMS)  telah melakukan kajian terhadap potensi implementasi 
big  data  yang  dapat  diterapkan  untuk  keperluan  official 
statistics  seperti  pemanfaatan  data  twitter  untuk  melihat  pola 
komuter,  penggunaan  data  MPD,  dan  penerapan  metode 
crowdsourcing untuk melakukan nowcasting harga pangan [6]. 
Salah satunya adalah untuk pengumpulan data Statistik Jasa 
Akomodasi  Perhotel  melalui  kegiatan  VHTS  yang  telah 
disebutkan  sebelumnya,  karena  proses  pengumpulan  data 
melalui survei VHTS ini masih menggunakan kuesioner cetak 
juga  memerlukan 
lebih.  BPS 
sebelumnya  telah  memanfaatkan  big  data  sebagai  salah  satu 
sumber alternatif data. 

tenaga  dan  biaya  yang 

Namun  disini  karena  big  data  memiliki  ukuran  data  yang 
besar  dan  tidak  terstruktur,  maka  sebelum  dianalisis  lebih 
digunakan 
lanjut, 

pengolahannya 

dalam 

perlu 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

metode/algoritma  yang  disesuaikan  dengan  kebutuhan  dan 
efisien  dalam  menangani  data  berukuran  besar.  Perlu  pula 
dilakukan kajian lebih lanjut dan mendalam untuk pembuatan 
model penunjang pengolahan big data yang diharapkan dapat 
menghasilkan  data  yang  layak  untuk  didiseminasikan  ke 
khalayak umum nantinya. 

II.  TUJUAN PENELITIAN 

Penelitian  ini  memiliki  tujuan  untuk  bisa  melakukan 
pemrosesan  dan  pencocokan  dataset  hasil  scraping  situs 
Agoda dengan data direktori VHTS milik BPS dan data TPK 
triwulan  pertama  di 
tahun  2020  milik  BPS.  Metode 
pencocokan  yang  digunakan  adalah  metode  cosine  similarity 
dan metode fuzzy string matching. 

Penggunaan  dua  metode  pencocokan  yang  berbeda 
dilakukan  untuk  mengetahui  metode  mana  yang  jauh  lebih 
baik  dalam  menangani  pencocokan  big  data  dengan  data 
direktori  milik  BPS  untuk  menghasilkan  data  statistik  ofisial 
yamg akurat dengan kredibilitas yang baik. 

Setelah  dilakukan  proses  matching/pencocokan  data  dan 
didapatkan  data  yang  bersesuaian  antara  data  hasil  scraping 
big  data  dari  situs  Agoda  dengan  data  direktori  VHTS  BPS, 
selanjutnya  akan  dilakukan  analisis  kesesuaian 
tingkat 
okupansi hotel dari data Agoda  dengan data TPK hasil VHTS 
BPS. 

jauh  perbedaan  dari  kedua 

Perbandingan  ini  dilakukan  dengan  tujuan  untuk  melihat 
jenis  metode 
seberapa 
pengumpulan  data 
tersebut  sehingga  dapat  dilihat  dan 
dipertimbangkan  apakah  untuk  kedepannya  big  data  dapat 
dimanfaatkan  sebagai  data  pendukung  dalam  pengumpulan 
data/informasi terkait statistik perhotelan.  

III. PENELITIAN TERKAIT 

Sebelumnya  sudah  pernah  ada  studi  terkait  data  matching 
jasa akomodasi ini  yang dilakukan oleh Putra dan Wulandari 
dengan judul Using Big data for Accommodation Statistics: A 
New  Challenge  in  Producing  Tourism  Statistics.  Namun 
penelitian ini terbatas hanya dilakukan pada provinsi Bali saja. 
Oleh  karena  itu  untuk  penelitian  kali  ini  diperluas  menjadi 
untuk Pulau Jawa dan Bali. 

Selain  itu  terdapat  salah  satu  studi  terkait  lainnya  yang 
membahas tentang ekstraksi data web dari tripadvisor sebagai 
pendukung  pengembangan  indikator  pariwisata  di  Minas 
Gerais oleh Oliveira dan Porto. 

Adapun  untuk  ,metode  pencocokan  yang  akan  digunakan 
yaitu  metode  cosine  similarity  dengan  pembobotan  TF-IDF 
serta model probabilistik N-gram dan juga metode fuzzy string 
matching  dengan  variasi 
levenshtein  distance,  beberapa 
literatur  penelitian  yang  sudah  ada  mengenai  data  matching 
menggunakan  beberapa  metode 
tersebut  adalah  sebagai 
berikut : 

TABEL I 
TABEL LITERATUR PENELITIAN TERKAIT 

No 

Judul 

1.  Using Big data 

for 
Accommodation 
Statistics: A 

Penulis, 
Publikasi 
Putra, Amanda 
P. dan 
Wulandari, 
Heny. Using 

Tertulis 

Komentar 

The future 
work will 
involve 
various web 

Berarti big 
data dapat 
dimanfaatkan 
sebagai data 

 2 / 8 

 
 
 
New Challenge 
in Producing 
Tourism 
Statistics 

2.  Extracting Web 
Data from 
Tripadvisor as 
a Support for 
Tourism 
Indicators 
Development in 
Minas Gerais 

Big data for 
Accommodation 
Statistics: A 
New Challenge 
in Producing 
Tourism 
Statistics. 
(2018). Closing 
the gaps in 
economic 
statistics for 
sustainable 
development. 
Oliveira, R. 
Almeida de dan 
Porto, R. M. A. 
B. (2016). 
Global Forum 
On Tourism 
Statistics. 2016. 

3.  Penerapan 
Cosine 
similarity dan 
Pembobotan 
TF-IDF untuk 
Mendeteksi 
Kemiripan 
Dokumen 

Riyani, A., 
Naf’an, 
Muhammad Z., 
dan 
Burhanuddin, 
A. (2019). 
Jurnal 
Linguistik 
Komputasional 
(JLK), Vol. 2, 
No. 1, Maret 
2019. 

4.  Penerapan 
Algoritma 
Cosine 
similarity dan 
Pembobotan 
TF-IDF pada 
Sistem 
Klasifikasi 
Dokumen 
Skripsi 

Wahyuni, Rizki 
T., Prastiyanto, 
D., dan 
Supraptono, E. 
(2017). Jurnal 
Teknik Elektro 
Vol. 9 No. 1, 
Januari - Juni 
2017. 

5.  Penerapan 

Fuzzy string 
matching pada 
Aplikasi 
Pencarian 
Tugas Akhir 
Mahasiswa 
Jurusan Sistem 

Gurning, Ardi I. 
A., Zarnelly., 
dan Adawiyah, 
A. (2016). 
Jurnal 
Rekayasa dan 
Manajemen 
Sistem 

sites and 
various web 
scraping tools 
to collect data 
in question. 

pendukung 
dalam 
pengumpulan 
data/informasi 
terkait statistik 
salah satunya 
terkait 
perhotelan. 

It also enables 
methodologies 
that can 
extract this 
data and 
transform 
them into 
information of 
the utmost 
importance to 
help public 
and private 
managers in 
decision 
making. 
Dari hasil 
pengujian dan 
analisis maka 
dapat 
disimpulkan 
dalam 
penelitian ini 
adalah 
algoritma 
cosine 
similarity dan 
pembobotan 
TF-IDF telah 
berhasil 
mendeteksi 
kemiripan 
suatu 
dokumen. 
Kesalahan 
klasifikasi 
yang sering 
terjadi adalah 
karena 
terdapat 
beberapa kata 
yang sama 
dengan kata 
kunci, 
sehingga 
sistem 
memilih nilai 
tertinggi dari 
perhitungan 
cosine 
similarity 
yang ada. 
Algoritma 
Levenshtein 
distance dapat 
menampilkan 
hasil 
pencarian 
untuk judul 
tugas akhir 

Dari 
pernyataan ini, 
memungkinkan 
bahwa 
ekstraksi big 
data 
memungkinkan 
sebagai salah 
satu penunjang 
dalam 
pengambilan 
keputusan 
terkait. 

Hal ini 
menunjukkan 
bahwa 
algoritma 
cosine 
similarity dan 
pembobotan 
TF-IDF dapat 
digunakan 
dalam proses 
pencocokan 
data. 

Hal ini 
menunjukkan 
bahwa sebelum 
dilakukan 
penghitungan 
cosine 
similarity perlu 
dikaji lebih 
lanjut apa yang 
harus 
dilakukan 
untuk 
menghindari 
kesalahan 
hasil. 

Dari 
pernyataan ini 
maka dapat 
diketahui 
bahwa 
algoritma 
levenshtein 
distance dapat 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Informasi 
Berbasis Web 

Informasi, Vol. 
2, No. 1, 
Februari 2016 

6.  Algoritma 

Fuzzy string 
matching untuk 
Pencocokan 
String 

Dirgandhavi, 
Naufal A. 
(2019). 
Makalah 
IF2211 Strategi 
Algoritma, 
Semester II 
Tahun 
2018/2019. 

yang 
mendekati 
dengan judul 
tugas akhir 
yang 
diketikkan. 
Levensthein 
distance 
sejauh ini 
menutupi 
kesalahan 
yang dapat 
diperbuat oleh 
algoritma 
lainnya pada 
banyak kasus. 

digunakan 
untuk proses 
data matching. 

Pernyataan ini 
menunjukkan 
bahwa metode 
pencocokan 
string dengan 
algoritma 
levenshtein 
distance bisa 
menjadi salah 
satu solusi 
yang 
digunakan.  

Gambar 2. Peta literatur metode cosine similarity, TF-IDF, dan N-gram 

Bagian  Pada  penelitian 

IV. METODE PENELITIAN  
terdapat  dua  data  yang 
ini 
digunakan  dan  keduanya  merupakan  data  sekunder,  dimana 
data  pertama  merupakan  data  direktori  beserta  raw  data 
VHTS  BPS  dan  data  kedua  merupakan  data  hasil  scraping 
situs web Agoda untuk beberapa provinsi di Indonesia selama 
tahun 2020 oleh subdit PMS BPS terkait kajian big data yang 
sedang  dilakukan.  Dari  kedua  dataset  tersebut,  dipilah  dan 
diambil data untuk Provinsi Jawa dan Bali saja. 

Pengolahan  data  diawali  dengan  data  preprocessing  baik 
pada data BPS maupun data hasil scraping. Pertama dilakukan 
pemilihan  variabel  yang  akan  digunakan  dari  data  hasil 
scraping, yaitu variabel : 

 tanggal 
 city  

 name   
 province 

 address 
 okupansi 

 area 

Lalu  pada  data  hasil  scraping  yang  awalnya  memiliki 
5.681.133  record  data  dilakukan  data  cleaning  untuk 
menghilangkan  data  duplikat  dari  masing-masing  akomodasi 
menggunakan modul pandas dan numpy pada Python. 
Lalu untuk data direktori BPS diambil variabel : 
 nama_komersial 
 kode_kabkot  

 alamat 
 kode_prov 

Lalu, untuk raw data VHTS diambil variabel : 
nama_komersial 
 alamat 

 kode_kabkot 

 kode_prov 

 TPK 

Dari  data  BPS  tersebut  dilakukan  data  converting  antara 
kode  wilayah  kabupaten/kota  dan  provinsi  dengan  nama 

 3 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

wilayah  masing-masing  menggunakan  library  dplyr  pada 
Rstudio. 

Setelah  itu,  untuk  data  hasil  scraping  yang  sudah  melalui 
preprocessing,  dibagi  lagi  menjadi  dua  dataset  berbeda, 
dimana dataset pertama berisi variabel : 

 name 
 city  

 address 
 province 

Lalu dataset kedua berisi variabel : 
 tanggal 
 city  

 name   
 province 

 address 
 okupansi 

 area 

 area 

Untuk  dataset  kedua  ini  hanya  diambil  data  periode  1 
Januari 2020 sampai dengan 31 Maret 2020 dikarenakan  raw 
data  VHTS  yang  menyertakan  nilai  TPK  yang  diberikan 
hanya  untuk  periode  sampai  dengan  bulan  Maret  2020 
(triwulan  pertama  2020).  Dataset  pertama  akan  digunakan 
untuk matching dengan data direktori dan dataset kedua akan 
digunakan untuk matching dengan data TPK hasil VHTS. 

Lalu  dilakukan  pemeriksaan  ulang  untuk  melihat  apakah 
masih terdapat data duplikat dari masing-masing dataset hasil 
scraping.  Penghilangan  data  duplikat  ini  menggunakan  cara 
sorting data berdasarkan tanggal  yang  nantinya akan diambil 
data 
dilakukan 
akhir 
drop.duplicate  pada  Python  berdasarkan  subset  name, 
area,  city,  dan  province.  Hal  ini  dilakukan  dengan  asumsi 
bahwa  dalam  satu  area,  kota,  dan  provinsi  yang  sama,  tidak 
ada hotel dengan nama yang sama. 

scraping 

paling 

yang 

lalu 

Selanjutnya memasuki proses matching data menggunakan 
dua  metode  berbeda,  yaitu  metode  cosine  similarity  dengan 
pembobotan  TF-IDF  serta  model  probabilistik  N-gram  dan 
metode fuzzy string matching. 

(1)  Cosine similarity, TF-IDF, N-gram 

Untuk  penggunaan  metode  ini  digunakan  beberapa 

modul yang ada pada Python, yaitu : 
  pandas  
 re 
 numpy 
 TfidfVectorizer 
 csr_matrix 
 sparse_dot_topn.sparse_dot_topn 

Dilakukan  pembagian  kalimat  tiap  record  data  menjadi 
token dengan menghapus semua karakter khusus, tanda baca 
dan karakter tunggal dan  menggunakan  model  probabilistik 
N-gram yang secara matematis dituliskan seperti berikut : 

                                      (     )      (1) 
Nilai N yang digunakan adalah N=15 dimana penggunaan 
N  yang  semakin  besar  diharapkan  dapat  menangkap 
informasi  yang  lebih  kontekstual  dibandingkan  dengan 
jumlah  N  yang  lebih  kecil,  sehingga  untuk  penelitian  ini 
masing-masing  data  record  akan  dipisahkan  menjadi  15 
huruf yang beberapa diantaranya seperti di bawah ini : 

Gambar 3. Beberapa hasil vektor kata dengan N-gram 

Setelah  didapatkan  vektor  kata  hasil  dari  N-gram,  maka 
dilakukan pembobotan TF-IDF untuk masing-masing vektor 
kata  dengan  mengubahnya  menjadi  matriks  fitur  TF-IDF. 
TF-IDF  sendiri  merupakan  salah  satu  metode  pengambilan 
informasi  dan  text  mining  dengan  menghitung  berapa  kali 
sebuah  kata  muncul  dalam  suatu  dokumen  dibagi  dengan 
jumlah  total  kata  dalam  dokumen  tersebut  dan  menghitung 
logaritma  jumlah  dokumen  dalam  korpus  dibagi  dengan 
jumlah dokumen di mana sebuah kata tertentu muncul untuk 
mengukur  seberapa  sering  sebuah  kata/istilah  muncul  dan 
seberapa penting sebuah kata tersebut dalam suatu dokumen 
tersebut.  Secara  matematis  pembobotan  TF-IDF  dapat 
dituliskan seperti berikut :  

  (   )  

   ( )        (

              ( ) 

)         ( ) 

dioptimalkan 

Fitur  TF-IDF  ini  menghasilkan  matriks  nilai  TF-IDF 
untuk  masing-masing  vektor  yang  sebelumnya  sudah 
didapatkan.  Matriks  nilai  TF-IDF  ini  dibentuk  menjadi 
matriks  CSR  untuk    menghitung  kemiripan  antara  dua 
vektor  nilai  TF-IDF  dengan  metode  cosine  similarity  yang 
telah 
dengan  menggunakan  modul 
sparse_dot_topn. Cosine similarity adalah metode untuk 
mengukur  kesamaan  teks  dengan  mengukur  kosinus  sudut 
antara  dua  vektor  berdimensi-n  yang  diproyeksikan  dalam 
ruang multi-dimensi, dengan nilai hasilnya berkisar antara 0 
sampai dengan 1, dimana : 
  Mendekati 1 berarti lebih banyak kesamaannya. 
  Mendekati 0 berarti lebih sedikit kesamaannya. 
Penghitungan  cosine  similarity  secara  matematis  dapat 
dituliskan menjadi :  

                ( )  

‖ ‖‖ ‖

∑     

  ( ) 

√∑   

√∑   

Setelah 

itu  bongkar  dan  kembalikan  matriks  yang 
didapatkan sebagai hasil dari penghitungan cosine similarity 
sebelumnya,  lalu  simpan  data  yang  cocok  ke  dalam 
dataframe 
dengan 
dan 
kemiripan >0,5. 

diambil 

yang 

baru 

data 

 4 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
                             
                               
 
             
                          
 
    
 
 
   
 
 
   
 
 
   
 
Berikut  gambaran 

tahapan-tahapan  dari  proses  data 

matching diatas secara umum. 

Gambar 4. Alur proses pencocokan data dengan cosine similarity 

(2)  Fuzzy string matching 

Fuzzy  string  matching  adalah  salah  satu  metode 
pencarian  dan  pencocokan  string  menggunakan  proses 
pendekatan terhadap pola dari string yang dicari. Dilakukan 
pencarian  terhadap  string  yang  sama  dan  juga  string  yang 
mendekati  dengan  string  lain  dan  dikumpulkan  dalam 
sebuah  kerangka  data  penampung atau kamus.  Fuzzy string 
matching  yang  digunakan  kali  ini  adalah  metode  fuzzy 
dengan  variasi  levenshtein  distance  menggunakan  modul 
yang sudah ada pada Python. Untuk penggunaan metode ini 
digunakan beberapa modul yang ada pada Python, yaitu : 
  pandas  
 re 
 numpy 
 fuzzywuzzy 

Levenshtein  distance  atau  jarak 

levenshtein  sendiri 
merupakan  metrik  untuk  mengukur  perbedaan  antara  dua 
barisan.  Atau  mudahnya,  jarak  levenshtein  antara  dua  kata 
adalah  jumlah  minimum  perubahan  karakter  (penyisipan, 
penghapusan,  atau  penggantian)  yang  diperlukan  untuk 
mengubah satu kata menjadi kata lainnya [8]. 

Setelah  dilakukan  impor  kerangka  data  yang  akan 
dicocokkan,  maka  dilakukan  converting  kolom  variabel 
yang menjadi kunci untuk pencocokan dari kedua dataframe 
ke  dalam  daftar.  Variabel  kunci  yang  dgunakan  yaitu 
area/daerah, 
nama 
gabungan 
kabupaten/kota, dan provinsi. 

akomodasi, 

antara 

Lalu dibuat fungsi yang akan mengambil istilah (variabel 
kunci),  daftar  istilah  (daftar  variabel  kunci),  dan  skor 
kesamaan  minimum  (min_score)  untuk  mengembalikan 
kecocokan dari fungsi fuzz.ratio(). 

Setelah  itu  dibuat  fungsi  perulangan  untuk  membuat 
daftar tupel dengan nilai pertama adalah nama dari kerangka 
data  pertama  (variabel  yang  akan  diganti)  dan  nama  dari 
kerangka data  kedua (variabel pengganti). Lalu daftar tupel 
ini  digabungkan  menjadi  sebuah  kamus  seperti  beberapa 
contoh berikut ini : 

{'CV.  GREN  HOTEL  1  JALAN  JEND  SUDIRMAN  NO  19 
TEGAL  KOTA  TEGAL  JAWA  TENGAH':  'GREN  HOTEL 
TEGAL JENDRAL SUDIRMAN NO.17 TEGAL TEGAL JAWA 
TENGAH', 
'NUANSA BALI HOTEL 2 JALAN RAYA KARANG BOLONG 
KM  133,5  SERANG  BANTEN':  'NUANSA  BALI  HOTEL 
ANYER  JALAN  RAYA  KARANG  BOLONG  KM  133.5 
CIKONENG ANYER BANTEN', 

... 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

'BUTIK  CAPSULE  HOSTEL  WR  SUPRATMAN  NO  2  L 
MALANG  CENTER  MALANG  INDONESIA  KOTA  MALANG 
JAWA  TIMUR':  'BUTIK  CAPSULE  HOSTEL  JALAN  WR. 
SUPRATMAN  NO.2  L  MALANG  CENTER  MALANG  JAWA 
TIMUR'} 

Setelah  didapatkan  sebuah  kamus  yang  berisi  variabel 
yang  akan  diganti  dan  variabel  pengganti  seperti  diatas, 
maka  kamus  ini  akan  digunakan  untuk  mengganti  variabel 
kunci  dari  kerangka  data  pertama  dengan  nilai  di  kolom 
variabel kunci untuk kerangka data kedua. 

Berikut  gambaran  tahapan-tahapan  dari  proses  data 

matching diatas secara umum. 

Gambar 5. Alur proses pencocokan data dengan fuzzy string matching 

V.  KERANGKA PIKIR 

Awal  mula  penelitian  ini  berasal  dari  salah  satu  project 
pembuatan dashboard dari subdit PMS tentang jasa akomodasi 
terkait  perhotelan.  Saat  ini  yang  dibutuhkan  adalah  metode 
matching  yang  sesuai  untuk  digunakan  pada  data  akomodasi 
hasil  web  scraping  dengan  data  direktori  milik  BPS  yang 
nantinya  bisa  diimplementasikan  pada  dashboard  tersebut 
untuk  melihat  sejauh  mana  kesesuaian  TPK  antara  hasil 
pendekatan big data dengan data yang dimiliki oleh BPS. 

Salah satu masalah yang sedang dihadapi oleh BPS saat ini 
dalam pengumpulan data adalah dengan metode konvensional 
seperti  survei  dan  sensus  yang  membutuhkan  waktu,  tenaga, 
dan biaya lebih besar dibandingkan dengan pengumpulan data 
dengan  metode  lain,  masih  besar  peluangnya  data  yang 
terkumpul  kurang  maksimal.  Salah  satunya  ketika  terjadi 
nonresponse maka akan membuat data menjadi tidak lengkap, 
sehingga  diperlukan  sumber  lain  untuk  bisa  melengkapi  data 
yang  dibutuhkan.  Oleh  karena  itu,  saat  ini  BPS  sedang 
berusaha  mengembangkan  metode  pengumpulan  data  dengan 
melakukan pendekatan data dari big data. 

Meskipun  datanya  terbarukan  dalam  waktu  yang  singkat, 
big data dengan volume data yang besar dan cenderung tidak 
terstruktur karena untuk beberapa variabel masih berupa kode 
atau  bahkan  pengisian  variabelnya  tidak  sesuai,  memerlukan 
pengolahan  sedemikian 
rupa  untuk  bisa  menghasilkan 
informasi  yang  kredibilitasnya  bisa  dipertanggungjawabkan 
dengan keadaan realitanya. 

Oleh  karena  itu,  maka  dilakukan  pencocokan  data  antara 
hasil  big  data  dengan  data  BPS.  Karena  hanya  BPS  selaku 
badan  statistik  nasional  yang  dapat  mengeluarkan  data 
statistik resmi,  maka data BPS digunakan sebagai  benchmark 
dalam pencocokan data dalam penelitian ini. 

Berikut 

ini  merupakan  gambaran  umum 
pemikiran yang digunakan dalam penelitian ini. 

terkait  alur 

 5 / 8 

 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

TABEL II 
TABEL HASIL PENCOCOKAN DIREKTORI DENGAN METODE PERTAMA 
Nama Akomodasi 

Alamat Akomodasi 

Scraping 

Direktori 

GRAHA 
HOTEL 
SRAGEN 

GRAHA 
HOTEL 

OYO 1240 
HOTEL 
PANTAI 
JAYA 

ADEM 
AYEM 
PENGINAP
AN 

POERI 
DEVATA 
RESORT 
HOTEL 

POERI 
DEVATA 
RESORT 
HOTEL 

BALI 
BULE 
HOMEST
AY 

BELONG 
BUNTER 
HOME 
STAY 

Scraping 
JALAN W R 
SUPRATMAN 
NO 145 
SRAGEN 
TENGAH 

127 JALAN 
KIDANG 
PANANJUNG 
NO 127 
PANGANDAR
AN JAWA 
BARAT 
PANGANDAR
AN 

KLURAK 
TAMAN 
MARTANI 
PRAMBANAN 
KALASAN 

JALAN 
PANTAI 
PADANG 
PADANG 
ULUWATU 

... 

BALI 
RICH 
VILLA 

BALI RICH 
VILLA 

JALAN DR 
SOETOMO NO 
28 

BALI 
BREEZZ 
HOTEL 

BALI 
BREEZZ 
HOTEL 

JALAN 
PANTAI SARI 
NO 23 

LE 
MERIDIE
N BALI 
JIMBARA
N 
HARD 
ROCK 
HOTEL 
BALI 

DREAM 
JIMBARA
N 

LE 
MERIDIEN 
BALI 
JIMBARAN 

HARD 
ROCK 
HOTEL 
BALI 

DREAM 
JIMBARAN 

JALAN BUKIT 
PERMAI 

JALAN 
PANTAI 
BANJAR 
PANDE MAS 

ULUWATU II 
TAMAN 
MULIA 
JALAN 
ARWANA 88 

Direktori 

JL WR 
SUPRATMAN 
NO 145 
SRAGEN 
TENGAH 
SRAGEN JAWA 
TENGAH 
JALAN 
HANJATAN NO 
1 RT 003 RW 
001 PANTAI 
TIMUR 
PANGANDARA
N 
PANGANDARA
N JAWA 
BARAT 
KLURAK 
TAMANMART
ANI KALASAN 
SLEMAN TILP 
0274 496453 
SLEMAN DI 
YOGYAKARTA 
JALAN PANTAI 
PADANG 
PADANG 
BADUNG BALI 

JALAN DR 
SOETOMO NO 
28 TUBAN 
TUBAN JAWA 
TIMUR 
JALAN PANTAI 
SARI NO 23 
JIMBARAN 
BADUNG BALI 
JALAN BUKIT 
PERMAI 
JIMBARAN 
BADUNG BALI 

JALAN PANTAI 
BANJAR 
PANDE MAS 
KUTA 
BADUNG BALI 
ULUWATU II 
TAMAN 
MULIA JALAN 
ARWANA 88 
JIMBARAN 
BADUNG BALI 

Gambar 6. Kerangka pikir penelitian 

VI. HASIL DAN PEMBAHASAN 

Hasil cleaning dari data scraping sebanyak 21.323 record 
data  akomodasi  penginapan  di  Pulau  Jawa  dan  Bali  selama 
tahun  2020  yang  dicocokkan  dengan  data  direktori  VHTS 
BPS 
tahun  2020  yang  sebanyak  5.288  record  data 
menggunakan  metode  kombinasi  antara  algoritma  cosine 
similarity 
dan  model 
probabilistik  N-gram,  diperoleh  jumlah  data  yang  cocok 
adalah  sebanyak  690  record  data  dengan  menggunakan  nilai 
similaritas  minimum  0,200  dihasilkan 
rata-rata  nilai 
similaritasnya adalah 0,30 secara keseluruhan. 

pembobotan  TF-IDF 

dengan 

Similarit
as 
0,200 

0,201 

0,201 

0,201 

0,629 

0,639 

0,646 

0,669 

0,707 

   Gambar 7. Diagram venn jumlah data yang cocok dengan metode pertama 

Dari  sebanyak  690  record  data  hasil  pencocokan, 
didapatkan  nilai  similaritas/kemiripan  sebesar  0,200  untuk 
terendah  dan  nilai 
data  cocok  dengan  nilai  similaritas 
similaritas/kemiripan  sebesar  0,707  untuk  data  cocok  dengan 
nilai  similaritas 
tertinggi.  Berikut  beberapa  data  hasil 
pencocokannya : 

Dari tabel II diatas, terlihat bahwa dengan nilai similaritas 
minimum  sebesar  0,200  sudah  didapatkan  data  yang  cocok 
dan  sesuai.  Namun  kendala  masih  terjadi  ditengah  hasil 
pencocokan,  dimana  terjadi  mismatching  pada  data  record 
selanjutnya  yang  mana  padahal  nilai  similaritasnya  lebih 
tinggi daripada nilai similaritas data record sebelumnya. 

Hal  ini  kemungkinan  terjadi  karena  beberapa  penulisan 
baik nama akomodasi maupun alamat akomodasi yang kurang 
lengkap antara data hasil scraping ataupun data direktori milik 

 6 / 8 

 
 
 
 
 
 
 
BPS.  Dan  juga  bisa  dikarenakan  metode  kombinasi  antara 
algoritma  cosine  similarity  dengan  pembobotan  TF-IDF  dan 
model  probabilistik  N-gram  belum  cukup  baik  dalam 
mengatasi  kendala  tersebut.  Sehingga  pencocokan  hanya 
terjadi  pada  variabel  yang  umum  saja  seperti  nama 
jalan/daerah/kabupaten-kota/provinsi saja. 

Lalu  masih  dengan  kerangka  data  yang  sama,  yaitu  data 
hasil cleaning dari data scraping sebanyak 21.323 record data 
akomodasi  penginapan  di  Pulau  Jawa  dan  Bali  selama  tahun 
2020  yang  dicocokkan  dengan  data  direktori  VHTS  BPS 
tahun  2020  yang  sebanyak  5.288  record  data  menggunakan 
metode  fuzzy  string  matching  dengan  variasi  levensthein 
distance,  diperoleh  jumlah  data  yang  cocok  antara  data  hasil 
scraping  dengan  data  direktori  BPS  adalah  sebanyak  811 
record  data  dengan 
tingkat  kecocokan  minimal  yang 
digunakan untuk menyusun kamusnya adalah sebesar 80%. 

Sehingga  dapat  dikatakan 

tingkat 
kecocokan  ≥80%  maka  fungsi  levenshtein  distance  pada 
modul  fuzzywuzzy  akan  menampilan  variabel  pengganti 
yang  akan  disimpan  di  kamus  tanpa  mengubah  kriteria 
bagaimana variabel tersebut akan ditampilkan. 

jika  ditemukan 

Gambar 8. Diagram venn jumlah data yang cocok dengan metode kedua 

Dari  sebanyak  811  record  data  hasil  pencocokan,  hampir 
semua  sudah  bersesuaian  antara  data  hasil  scraping  dengan 
data  direktori  milik  BPS.  Berikut  beberapa  data  hasil 
pencocokannya : 

TABEL HASIL PENCOCOKAN DIREKTORI DENGAN METODE KEDUA 
Alamat Akomodasi 

Nama Akomodasi 

TABEL III 

Scraping 
3 PRINCESS 
BOUTIQUE 
HOTEL & SPA 

Direktori 
3 PRINCESS 
BOUTIQUE 
HOTELSPA 

HOTEL 88 
EMBONG 
MALANG 

88 EMBONG 
MALANG 

Scraping 

JALAN 
ULUWATU 
I NO. 999X 

EMBONG 
MALANG 
84 STREET 

ADYA 
NALENDRA 
BOUTIQUE 
HOTEL 

ADYA 
NALENDRA 
HOTEL 

JALAN TRI 
MARGO 
KULON 9A 

Direktori 

JALAN 
ULUWATU NO 
999 BADUNG 
BALI 
JALAN 
EMBONG 
MALANG 84 
SURABAYA 
KOTA 
SURABAYA 
JAWA TIMUR 
JALAN 
TRIMARGO 
KULON NO 94 
YOGYAKARTA 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

... 

SUKAJADI 
HOTEL 

ZEST 
SUKAJADI 
HOTEL 

JALAN 
SUKAJADI 
176 

ZIZZ 
CONVENTION 
HOTEL 

ZIZZ 
CONVENTION 
HOTEL 

ZODIAK AT 
SUTAMI 
HOTEL 

ZODIAK 
SUTAMI 
HOTEL 

JALAN 
MUDUTAKI 
V NO. 2 
TEGAL 
JAYA 
DALUNG 
JALAN 
PROF DR IR 
SUTAMI 
NO.133 

KOTA 
YOGYAKARTA 
DI 
YOGYAKARTA 

JALAN 
SUKAJADI NO 
16 BANDUNG 
KOTA 
BANDUNG 
JAWA BARAT 
JALAN 
MUDUTAKI IV 
TEGAL JAYA 
DALUNG 
BADUNG BALI 

JALAN 
SUTAMI NO 
133 KOTA 
BANDUNG 
JAWA BARAT 

Namun  dapat  terlihat  pula  bahwa  ternyata  masih  terdapat 
beberapa mismatching data yang terjadi namun tidak separah 
pada metode sebelumnya. Kebanyakan mismacthing ini terjadi 
pada  variabel  alamat,  karena  ternyata  terdapat  beberapa 
alamat  yang  kurang  sesuai  antara  data  hasil  scraping  dengan 
data direktori BPS. 
ini  bisa 

levenshtein 
distance mendapatkan nilai kecocokan yang lebih tinggi untuk 
variabel  nama  akomodasi  ataupun  alamat  akomodasi  secara 
umum,  hampir  mirip  dengan  kasus  metode  sebelumnya. 
Namun disini hasil yang diberikan lebih baik dilihat dari segi 
kecocokan nama akomodasinya. 

jadi  dikarenakan  algoritma 

Hal 

Setelah dilakukan pencocokan data dengan direktori, maka 
dilakukan  pencocokan  data  dengan  data  TPK  VHTS  milik 
BPS.  Secara  umum  prosesnya  hampir  sama  dengan  proses 
pencocokan  dengan  direktori,  namun  kali  ini  data  yang 
digunakan merupakan data hasil scraping dan VHTS triwulan 
pertama  di  tahun  2020.  Namun  untuk  data  di  bulan  Januari 
2020 tidak dapat dilakukan pencocokan karena tidak ada data 
dari  hasil  scraping  selama  bulan  Januari  2020.  Oleh 
karenanya  pencocokan  hanya  dapat  dilakukan  pada  bulan 
Februari dan Maret 2020. 

TABEL IV 
TABEL HASIL PENCOCOKAN TPK DENGAN METODE BERBEDA 

Periode Data 

Januari 2020 
Februari 2020 
Maret 2020 

Hasil Pencocokan dengan Metode 

Cosine Similarity 
- 
228 
168 

Fuzzy string matching 
- 
285 
211 

Dari  hasil  pencocokan  data  TPK,  didapatkan  data  hotel 
yang  bersesuaian  antara  data  hasil  scraping  dan  VHTS 
triwulan pertama di tahun 2020 seperti pada tabel diatas. Hasil 
yang  berbeda  di  tiap  bulannya  ini  dikarenakan  di  setiap 
bulannya  belum  tentu  hotel  yang  sama  datanya  akan  selalu 
tersedia. Sehingga didapatkan hasil yang berbeda-beda di tiap 
bulannya seperti diatas. 

 7 / 8 

 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

metode seperti penambahan algoritma pencocokan string yang 
lainnya. 

Sedangkan  secara  keseluruhan  untuk  pencocokan  data 
TPK,  baik  dari  metode  pertama  dan  kedua  sama-sama 
memberikan  hasil  yang  tidak  jauh  berbeda,  yaitu  rata-rata 
selisih/perbedaan  TPK  antara  data  hasil  web  scraping  Agoda 
dengan  data  TPK  VHTS  milik  BPS  bernilai  >30  poin.  Nilai 
perbedaan  ini  lumayan  tinggi,  sehingga  dapat  dikatakan 
terdapat  perbedaan  yang  lumayan  tinggi  pula  untuk  masing-
masing  nilai  TPK  antara  data  hasil  web  scraping  Agoda 
dengan  data  TPK  VHTS  milik  BPS.  Hal  ini  menjadikan 
pendekatan big data dalam membantu pemenuhan data ofisial 
statistik  alangkah  lebih  baiknya  untuk  dikaji  lebih  mendalam 
lagi, untuk melihat faktor apa saja yang perlu dibenahi terkait 
perbedaan  nilai  yang  lumayan  besar  dari  kedua  sumber 
berbeda ini. 

DAFTAR PUSTAKA 
[1]  BPS.  (2017, 1)  Survei  Statistik  Jasa Akomodasi Bulanan  (VHTS),  2017 
[Online]. 
Available: 
https://sirusa.bps.go.id/sirusa/index.php/dasar/view?kd=1544&th=2017. 
(2019,  1)  Sepanjang  2019,  devisa  sektor 
[Online].  Available: 

pariwisata  mencapai  Rp 
https://nasional.kontan.co.id/news/sepanjang-2019-devisa-sektor-
pariwisata-mencapai-rp-280-triliun  

[2]  Nasional.kontan.co.id. 

triliun 

280 

[3]  Puspitasari, Gina,  Analisis Pengaruh Online Review Terhadap Penjualan 

Kamar Hotel di Kota Bandung dalam Situs Agoda.Com. 2015. 

[4]  BPS.  (2017, 1).  Metadata  Indikator  :  Tingkat  Penghunian  Kamar  (TPK) 
Available: 
[Online]. 

Hotel 
https://sirusa.bps.go.id/sirusa/index.php/indikator/60 

[5]  Annisa,  C.  F.,  Pramana,  S.  Pemanfaatan  Data  Google  Maps  dalam 
Analisis Potensi Usaha Sektor Penyedia Makan Minum. Jakarta. 2020. 
[6]  Setia Pramana, B. Y. (2017). Big data for Government Policy: Potential 
Implementations  of  BigData for  Official Statistics in  Indonesia.  Jakarta: 
IEEE. 

[7]  Putra,  Amanda  P.  dan  Wulandari,  Heny.  Using  Big  data 

for 
Accommodation  Statistics:  A  New  Challenge  in  Producing  Tourism 
Statistics.  (2018). Asia–Pacific  Economic  Statistics  Week  2018: Closing 
the gaps in economic statistics for sustainable development 
Jarak 

[Online].  Available: 

[8]  Wikipedia. 

Levenshtein 
https://id.wikipedia.org/wiki/Jarak_Levenshtein 

(2021). 

TABEL V 
TABEL SELISIH HASIL PENCOCOKAN TPK DENGAN METODE BERBEDA 

Periode Data 

Januari 2020 
Februari 2020 
Maret 2020 

Rata-rata Selisih Hasil Pencocokan 
Dengan Metode 

Cosine Similarity 
- 
34,684 
 33,569 

Fuzzy string matching 
- 
39,837 
37,952 

Lalu  dari  masing-masing  data  hotel  yang  bersesuaian 
sebelumnya, dihitunglah perbedaan TPK/okupansi antara data 
hasil  scraping  dan  VHTS  triwulan  pertama  di  tahun  2020. 
Secara rata-rata di bulan Februari 2020 perbedaan/selisih TPK 
yang  terjadi  antara data  hasil  scraping  dan  VHTS  milik  BPS 
adalah  sebesar  34,684%  dari  metode  pertama  dan  39,837% 
dari  metode  kedua.  Selanjutnya  rata-rata  perbedaan/selisih 
TPK  di  bulan  Maret  2020  yang  terjadi  antara  data  hasil 
scraping  dan  VHTS  milik  BPS  adalah  sebesar  33,569%  dari 
metode pertama dan 37,952% dari metode kedua. 

Secara keseluruhan, perbedaan atau selisih nilai TPK yang 
terjadi  antara  data  hasil  scraping  dan  VHTS  milik  BPS 
mencapai  >30  poin.  Hal  ini  pastinya  berpengaruh  cukup 
signifikan  jika  akan  dilakukan  analisis  lanjutan  terkait  data 
hasil  scraping  guna  menjadikannya  data  penunjang  untuk 
menghasilkan data statistik ofisial. 

VII. 

PENUTUP 

Berdasarkan  hasil  uji  coba  implementasi  metode  pertama 
yaitu algoritma cosine similarity dengan pembobotan TF-IDF 
dan model probabilistik N-gram lalu metode kedua yaitu fuzzy 
string matching dengan variasi algoritma levenshtein distance, 
dapat  diambil  kesimpulan  bahwa  dari  kedua  metode  terdapat 
kekurangan dan kelebihan masing-masing. 

Dari  segi  waktu  pemrosesan/eksekusi  algoritma,  metode 
pertama jauh lebih cepat dibandingkan metode kedua, dimana 
ketika metode fuzzy string matching dengan variasi algoritma 
levenshtein  distance  memerlukan  waktu  ±20-25  menit, 
metode  cosine  similarity  dengan  pembobotan  TF-IDF  dan 
model  probabilistik  N-gram  hanya  memerlukan  waktu  ±3-5 
menit saja, sehingga sangat menghemat waktu. 

Lalu jika dari segi data hasil pencocokan yang dihasilkan, 
sebenarnya  antara  kedua  metode  ini  hampir  sama,  namun 
metode  fuzzy  string  matching  dengan  variasi  algoritma 
levenshtein  distance  memberikan  hasil  pencocokan  yang 
sedikit  lebih  akurat  dibandingkan  dengan  metode  cosine 
similarity 
dan  model 
probabilistik N-gram. 

pembobotan  TF-IDF 

dengan 

Metode  fuzzy  string  matching  dengan  variasi  algoritma 
levenshtein  distance  sejauh  ini  dapat  lebih  meng-cover 
kesalahan  pencocokan  dari  metode  cosine  similarity  dengan 
pembobotan TF-IDF dan model probabilistik N-gram, namun 
tidak  menutup  kemungkinan  pula  jika  metode  fuzzy  string 
matching  dengan  variasi  algoritma  levenshtein  distance  ini 
dapat  membuat  sebuah  kesalahan  pencocokan  yang  mungkin 
ter-cover  oleh  metode  cosine  similarity  dengan 
bisa 
pembobotan  TF-IDF  dan  model  probabilistik  N-gram.  Oleh 
karenanya,  untuk  mendapatkan  hasil  pencocokan  yang  lebih 
maksimal, maka perlu ada kajian lanjutan dan pengembangan 

 8 / 8 

 
 
 
 
 
 
 
"
221709490,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Analisis Penyebaran Informasi Promosi Sensus 
Penduduk Online dengan Metode SNA dalam Upaya 
Komunikasi Pemasaran  
Studi Kasus :  Media Sosial Twitter 

Achmad Fadli (221709490, 4SD1) 

Dosen Pembimbing: Dr. Drs. Waris Marsisno, M.Stat 

  atau 

Ringkasan— Media sosial sudah menjadi salah  satu platform 
utama  dalam  penyebaran  informasi  saat  ini.  Dengan  segala 
interaksi  sosial yang mungkin  terjadi di  dalamnya, media sosial 
berpotensi  menghadirkan  keuntungan  dalam  komunikasi 
pemasaran  terhadap  produk 
  event  baru  dengan 
mempelajari  mekanisme  penyebaran  informasinya.  Riset  ini 
bertujuan  untuk  melakukan 
analisis  mekanisme  dari 
penyebaran  informasi  terkait  Sensus  Penduduk  Online  (SPO) 
yang pertama kali diadakan tahun 2020,  menggunakan  metode 
Social  Network  Analysis  (SNA).  Hasil  analisis  memperlihatkan 
kecenderungan  dari  karakteristik  pelaku  penyebar  informasi 
didominasi  oleh  user  biasa,  dengan  mekanisme  paling  efektif 
menggunakan  instansi  pemerintah  sebagai  inisiator.  Dari  nilai 
engagement  rate  yang  diperoleh  juga  dapat  disimpulkan  bahwa 
pelaku  yang  terlibat  dalam  penyebaran  informasi  SPO  sudah 
cukup  baik  dengan  nilai  rata-rata  1,077%  secara  keseluruhan, 
namun pengaruh yang diberikan dapat ditingkatkan  khususnya 
bagi user dengan banyak followeruntuk event lain ke depannya. 

Kata  Kunci—  Penyebaran  Informasi,  SNA,  Media  Sosial, 

Engagement. 

I.  LATAR BELAKANG 

Perkembangan  teknologi  informasi  dan  komunikasi  di 
masa  kini  telah  memungkinkan  dilakukannya  pertukaran 
informasi dengan mudah. Hal ini tentu menjadi  peluang  yang  
baik dalam  upaya  komunikasi pemasaran/ pengenalan  event 
bagi  beberapa  organisasi    dengan  beragam    kepentingannya. 
Terlebih 
lagi,  pemasaran  melalui  media  sosial  dapat 
mengurangi biaya pemasaran seperti percetakan brosur, space 
iklan di media cetak maupun elektronik, dan lain-lain.  

tersebar 

interaksi 

informasi 

lebih  cepat  [1].  Akan 

Sosial  media  memungkinkan 

sosial  dan 
penyebaran  informasi  yang  dapat  menciptakan  kesempatan 
suatu 
tetapi 
dikarenakan  kerumitan  dan  besarnya  jumlah  dan  variasi 
karakteristik  pengguna  sosial  media,  cukup  sulit  untuk  dapat 
memaksimalkan potensi  dari cepatnya  informasi bergerak  di 
sosial media. Dalam menyikapi hal tersebut,  organisasi perlu 
memahami  dengan  baik  bagaimana  proses  penyebaran 
informasi  pada    sosial  media,  dan  hal  lainnya  yang  dapat 
meningkatkan  komunikasi  pemasaran  yang  baik  kepada 
publik.  

Studi  sebelumnya  pernah 

  dilakukan  dengan  tujuan 
menghitung  proses  penyebaran  informasi  pada  sosial  media. 
Berdasarkan  hasil  penelitian  Guille  dkk.,  proses  penyebaran 
informasi  mampu  menganalisa  secara  graph-based  maupun  
non-graph  model  [2].    Dengan  Social  Network  Analysis 

(SNA)  yaitu  metode  yang  dapat  mengukur  hubungan  antar 
pengguna  dengan  memanfaatkan  teori  graph  [3].  Metode  ini 
memungkinkan  penghitungan 
ragam  antar  karakteristik 
pengguna dalam  jaringan sosial [4]. Artinya SNA diharapkan 
mampu  menunjukan  pengetahuan  secara  signifikan  berkaitan 
dengan  proses  penyebaran  informasi  di  sosial  media  pada 
suatu studi kasus. 

Berkaitan  dengan  promosi  event  melalui  media  sosial, 
diperlukan  pendekatan  yang 
tepat  dalam  menyebarkan 
pemahaman  yang  baik  tentang  event  yang  dipromosikan. 
sosial  media 
Efektivitas  komunikasi  pemasaran  pada 
seperti  motivasi, 
bergantung  pada  berbagai  variabel 
keterikatan audiens, karakter produk, dan peran tokoh terkenal 
yang  ikut  mempromosikan  [5].  Berdasarkan  hal  tersebut, 
karakteristik  pelaku  penyebaran  informasi  dianggap  dapat 
menentukan  seberapa  efektif  komunikasi  pemasaran  suatu 
produk/ event dapat diterima oleh audiens 

Dalam  studi  ini,  BPS  sebagai  organisasi  yang  bertujuan 
untuk  mempromosikan  event  Sensus  Penduduk  Online    yang 
telah dilaksanakan pada tahun 2020 pada periode 15 Februari 
s.d. 29 Mei 2020 juga melakukan promosi pada media sosial, 
tak  terkecuali  Twitter.  Berdasarkan  informasi  yang  ada, 
besarnya  partisipasi  masyarakat  Indonesia  pada  Sensus 
Penduduk Online sebesar 19,05% atau sekitar 51,36 juta jiwa 
[6].  Hal  ini  menandakan  kontribusi  data  yang  diperoleh  dari 
Sensus  Penduduk  Online  untuk  pelaksanaan  pertamanya 
sudah relatif baik namun masih dapat dikembangkan. Dengan 
melihat  karakteristik  proses  penyebaran 
informasi  yang 
dilakukan  pada  periode  waktu  tertentu,  diharapkan  dapat 
memberikan  wawasan  baru  seputar  engagement  yang 
diberikan BPS kepada pengguna media sosial tentang promosi 
event Sensus Online. 

II.  TUJUAN PENELITIAN 

Penelitian  ini  bertujuan  untuk  menganalisis  bagaimana 
proses  penyebaran  informasi  di  media  sosial  dalam  upaya 
memberikan komunikasi pemasaran yang baik mengenai suatu 
event  baru  yang  akan  dilakukan  oleh  instansi  pemerintah. 
Dengan studi kasus yang digunakan adalah promosi launching 
Sensus  Penduduk  Online  2020  yang  dilakukan  melalui 
platform media sosial Twitter.  
metode 
SNA  untuk  menganalisis  mekanisme  penyebaran  dan 
karakteristik penyebar informasi SPO2020 via Twitter. Selain 
itu juga mempertimbangkan efektivitas promosi event dengan 
menganalisa  pelaku  penyebar  informasi  dengan  penilaian 

Menggunakan 

 1 / 8 

 
 
 
engagement  rate.  Hasil  yang  diperoleh  dapat  menjadi 
wawasan baru sekaligus bahan evaluasi mengenai penyebaran 
informasi seputar kegiatan instansi pemerintah yang dilakukan 
melalui media social. 

III. PENELITIAN TERKAIT 

Social  Network  Analysis  (SNA)  merupakan  metode  yang 
dapat digunakan untuk  mengukur percakapan antar pengguna  
pada  jejaring  sosial  media.  Dengan  kapasitas  menghasilkan 
beberapa sudut pandang semisal perilaku pengguna, pengaruh, 
dan  seberapa  kuat  hubungan  antar  pengguna  [4,7].  SNA 
memuat  tampilan  berupa    node  yang  merepresentasikan 
pengguna  dan  edges  yang  merepresentasikan  interaksi  antar  
pengguna.  SNA    menghasilkan  beberapa  ukuran  yang    dapat 
mendukung  dalam  analisa  penyebaran  informasi  [8].  Selain 
SNA,  pendekatan  efektivitas  promosi  yang  dilakukan  pada 
sosial  media  juga  digunakan  pada  penelitian  ini.  Penelitian 
sebelumnya  yang  memiliki  topik  serupa  dan  dijadikan  acuan 
dalam penelitian ini dijabarkan pada Tabel I. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

to 

according 
different 
criteria 

4

.. 

Modeling 
Twitter 
Engagement 
Real-World 
Events 

in 

Y.  Hu 
and  Y.  K. 
Hong, 
in 
Proceedings 
of  the  50th 
Hawaii 
Internationa
l Conference 
on 
System 
Sciences 
2017 

Penelitian  ini 
yang 
juga 
dijadikan 
rujukan 
utama  untuk 
analisis 
engagement 
rate. 

social 

prior 
as 

users’ 
activities, 
well as 
their 
network 
structure,  can 
be  very  good 
predictors 
for  both 
the 
presence  and 
the  degree  of 
their 
engagement 
with 
world events 

real-

No 

Judul 

TABEL I 
TABEL LITERATUR 
Penulis, 
Publikasi 

Tertulis 

Komentar 

1.  Pengumpulan data 

IV. METODE PENELITIAN  

1

. 

Measuring 
Information 
Dissemination 
Mechanism on 
Retweet Network 
for  Marketing 
Communication 
Effort 

A. 
Alamsyah 
and  M.R.D. 
Putra, 
in 
ICoICT 
2019 

Penelitian  ini 
yang 
juga 
dijadikan 
rujukan 
utama  untuk 
analisis 
menggunaka
n SNA. 

results 
show 
the mechanism 
of  information 
dissemination 
from  one  user 
to 
another 
level  by  level. 
conclude 
We 
official 
that 
account, 
tech 
reviewer,  and 
media  review 
the 
have 
power 
to 
disseminate 
information 
widely 

2

. 

Finding  Pattern 
in 
Dynamic 
Network 
Analysis 

A. 
Alamsyah  et 
al, 
in 
ICoICT, 
May 2018. 

3

. 

Measuring  user 
influence on 
Twitter: 
survey 

A 

Riquelme 
and 
González-
Cantergiani, 
in 
Information 
Processing 
and 
Managemen
t, April 
2016. 

higher 
The 
node 
value, 
the more users 
in 
involved 
interactions.  It 
indicates  how 
big 
product 
and 
services 
awareness 
from 
market 
We  survey  the 
several 
measures  that 
in 
exists 
literature 
to 
rank 
influential 
users 
Twitter 
network. 

the 

in 

Data  diperoleh  dengan  scraping  data  postingan  tweet 
pengguna  Twitter  menggunakan  WinPython,  yaitu  software 
open-source  portable  pemrograman  berbasis  Python.  Data 
yang  diambil  bersumber  dari  tweet  pengguna  Twitter  yang 
berada  dalam  interval  waktu  1  Januari  2020  hingga  29  Mei 
2020.  Karena  data  yang  akan  digunakan  merupakan  data 
historical dari Twitter, maka digunakan library snscrape pada 
python  agar  memungkinkan  scraping  dilakukan  dengan 
beberapa penyesuaian. Pemanfaatan library snscrape memiliki 
alasan karena bersifat open-source dan dapat digunakan untuk 
memperoleh  hampir  seluruh  data  historical  dari  Twitter  . 
Selain  itu digunakan  juga  library pandas  yang  berguna untuk 
mengekstrak data yang diperoleh ke dalam format csv. 

Data  yang  dipilih  merupakan  tweet  dengan  tendensi 
mempromosikan event Sensus Penduduk 2020. Tendensi yang 
tweet  yang  memuat  bahasan  Sensus 
dimaksud  adalah 
penduduk  2020,  baik  diunggah  langsung  maupun  diteruskan 
dari  pihak  lain  (retweet).  Pencarian  menggunakan  kata  kunci 
“sensus penduduk online” dan  hashtag yang berkaitan dengan 
Sensus Penduduk 2020 seperti;: 
#sensuspenduduk2020  
#SP2020 
#mencatatindonesia 
#sensusdirumahaja 

● 
● 
● 
● 

Dengan  Informasi  yang  diamati  adalah  username,  date,  
content,  dan  atribut  engagement  user  kepada  pengikutnya 
seperti  jumlah  likes,  jumlah  balasan,  jumlah  retweet,  dan 
jumlah follower. 

Berdasarkan  hasil  scraping  yang  dilakukan,  didapat  data 
yang  berhubungan  dengan  penyebaran  informasi  SPO  pada 
periode  waktu  yang  ditentukan  sejumlah  13.972    tweet,  baik 
unggahan pertama maupun berupa retweet. 

 2 / 8 

 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

2.  Preprocessing  

Pada  tahap  ini  dilakukan  proses  cleaning  data  seperti 
penyederhanaan data dan pengecekan kembali data yang layak 
diolah  sebelum  dapat  dianalisis  [15].  Kemudian  data  yang 
sudah  sederhana  dilakukan  filtering  pada  konten  tweet  yang 
tidak memiliki tendensi untuk menyebarkan informasi seputar 
tweet  yang 
SPO.  Konten  yang  di-filter  merupakan 
mengandung  ujaran  kebencian,  konten  eksplisit,  konten 
berbahasa  asing  yang  tidak  ada  tendensi  mempromosikan 
SPO,  dan  konten  promosi  yang  tidak  berkaitan  dengan  SPO. 
Untuk  kategori  konten  tweet  yang  masuk  kriteria  pengolahan 
seperti : 

● Ajakan mengisi sensus penduduk; 
● Informasi seputar event sensus penduduk;  
● Diskusi  seputar  sensus  penduduk  (pertanyaan  dengan 

minimal 2 reply) ;  

● Hanya  hashtag  selama  lebih  dari  1  seputar  sensus 

pertimbangan  nilai  degree  centrality  dan  jumlah  engagement 
tertinggi yaitu instansi pemerintah, pejabat pemerintahan, dan 
user biasa. Jenis-jenis akun tersebut yang kemudian dijadikan 
mekanisme  untuk  menentukan  node  awal  atau  “key  player” 
dalam  model  network,  dengan  asumsi  pelaku  pertama/ 
inisiator tidak terlalu jauh/merupakan “key player”. Dari “key 
player”  kemudian  dijadikan  acuan  untuk  menentukan  node 
awal  pelaku  pertama/inisiator/  node 
level  0  penyebar 
informasi  berdasarkan  mekanisme  yang  ditentukan.  Setelah 
itu baru ditelusuri node berikutnya yang berhubungan dengan 
inisiator  untuk  kemudian  dijadikan  node  level  1,  kemudian 
ditelusuri  node  mana  saja  yang  berhubungan  dengan  node 
level  1  untuk  kemudian  dijadikan  node  level  2.  Barulah 
kemudian  dibuat  rasio  penyebaran  informasi  dari  mekanisme 
yang  dibuat.  Ilustrasi  dari  alur  analisis  yang  dibahas 
digambarkan dalam Gambar 1. 

penduduk  

3.  Network modelling 

Kemudian  data  yang  sudah  disederhanakan  diolah  agar 
dapat membentuk network model. Membangun network model 
untuk  menghasilkan  variabel  “source”  yang  merupakan  user 
sumber  awal  unggahan  dan  “target”  yang  merupakan  user 
yang me-retweet suatu unggahan user lain untuk kemudian di-
post  sebagai  unggahannya. 
  Berikut  merupakan  contoh 
network model yang terbentuk. 

TABEL III 
TABEL NETWORK MODEL 

Source 
PNS_Ababil 
bps_statistics 
firman_diamond 
firman_diamond 
kharissulistiyo 
bpslampung 

Target 
veryltanaka 
alfareno 
hartadi3213 
hartadi3213 
syaiph 
bpskabwaykanan 

4.  Mengukur Network Metrices 

Pada tahap ini diukur semua interaksi dari pelaku penyebar 
informasi  menggunakan  SNA  metrices  seperti  graph  type, 
diameter,  modularity,  average  degree,  average  path  length, 
dan  degree  centrality.  Tujuan  proses  ini  untuk  memahami 
karakteristik  dari  hubungan  pelaku  penyebar  informasi,  juga 
mengidentifikasi  “key  player”  pada  network 
tersebut. 
Pengidentifikasian  “key  player”  didapat  dari  besaran  degree 
centrality.  Semakin  besar  ukurannya,  menunjukan  kuatnya 
pelaku tersebut dalam network. 

5.  Analisis Mekanisme Penyebaran Informasi 

Proses analisis diawali dengan mengidentifikasi jenis user 
informasi  SPO  dengan 

top  10  user  penyebar 

dari 

Gambar 1. Proses Analisis Mekanisme Penyebaran Informasi 

6.  Pengukuran efektivitas penyebaran informasi 

jumlah 

tersebut 

Selain  SNA,  pendekatan  efektivitas  promosi  yang 
dilakukan  pada  sosial  media  juga  digunakan  pada  penelitian 
ini.  Berdasarkan  situs  We  Are  Social  Indonesia,  per  Januari 
2020  Indonesia  memiliki  pengguna  aktif  media  sosial  sekitar 
160  juta  pengguna,  dengan  56%  nya  aktif  menggunakan 
Twitter.  Besarnya 
tentu  merupakan 
kesempatan  yang  bagus  bagi  beberapa  pihak  untuk 
mempromosikan dirinya  (personal branding)  maupun produk 
yang ingin dipasarkannya. Personal branding yang baik tentu 
akan  memiliki  peluang  yang  lebih  baik  dalam  memperoleh 
tanggapan dari publik.  
Salah  satu  indikator  suatu  akun 
memiliki  personal  branding  yang  baik  adalah  dilihat  dari 
engagement  rate  yang  baik.  Dikutip  dari  ContentCal.io, 
besaran  engagement  rate  yang  tergolong  baik  pada  media 
social  Twitter  yaitu  lebih  besar  dari  1%.  Dengan  pendekatan 
metode  penghitungan  engagement  rate  yang  digunakan  pada 
penelitian  ini adalah  Engagement by  Post, dengan persamaan 
sebagai berikut : 

Dengan rincian : 
ER 
Total engagement 

Total Followers 

: engagement rate 
:jumlah  like,  reply,  dan  komentar 
 pada postingan tersebut 
: jumlah pengikut akun tersebut 

 3 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
 
    
                          
               
     
 
 
 
 
 
 
 
 
 
 
Pendekatan 

ini  dipilih  karena  cenderung  stabil 

jika 
diterapkan  pada  akun  media  sosial  yang  memiliki  tanggapan 
cenderung 
hanya  memperhitungkan 
dan 
engagement rate yang berkaitan dengan topik yang diamati. 

fluktuatif, 

V.  KERANGKA PIKIR 

Penelitian  yang  dilakukan  merupakan  jenis  penelitian 
deskriptif  dengan  metode  analisis  kuantitatif.  Dengan  topik 
penyebaran  informasi  pada  media  sosial,  yang  bertujuan 
menganalisis  bagaimana  mekanismenya  dan  memahami 
karakteristik komunikasi pemasaran  yang  baik  melalui  media 
sosial  dengan  studi  kasus  launch  event  Sensus  Penduduk 
Online  yang  dilakukan  melalui  media  sosial  Twitter.  SNA 
digunakan  untuk  menjelaskan  mekanisme  penyebaran 
informasi  pada  sosial  media,  sedangkan  pendekatan  analisis 
engagement  rate  dipakai  untuk  menjelaskan  bagaimana 
efektifitas  pelaku  penyebar  informasi.  Dengan  data  yang 
bersumber  dari  percakapan  pengguna  Twitter  pada  rentang 
waktu  1  Januari-29  Mei  2020  dengan  tendensi  promosi 
kegiatan  sensus  online,  diharapkan  berdasarkan  asumsi  yang 
diterapkan  dapat  memodelkan  bagaimana  karakteristik 
penyebaran 
informasi  dengan  visualisasi  yang  mudah 
dipahami  dan  dapat  memberikan  wawasan  baru  mengenai 
efektifitas  penyebaran 
informasi  melalui  media  sosial, 
khususnya yang terjadi pada platform Twitter.    alur kerangka 
berpikir yang dibahas digambarkan dalam Gambar 2. 

Gambar 2. Contoh keterangan gambar 

VI. HASIL DAN PEMBAHASAN 

Setelah  dilakukan  cleaning  data,  filtering  tweet  content, 
dan  pembentukan  network  model,  diperoleh  12.547    tweet 
yang layak diolah untuk proses selanjutnya. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Gambar 3. Frekuensi tweet seputar SPO dalam mingguan 

Dilihat  dari  frekuensinya,  tweet  promosi  seputar  SPO 
paling  banyak  di-posting  pada  minggu  kedua  pada  bulan 
Februari  2020,  dengan  jumlah  mendekati  2000  posting-an. 
Hal  ini  dikarenakan  pada  tanggal  15  Februari  merupakan 
tanggal  launch  event  pelaksanaan  SPO,  sehingga  banyak 
pihak  yang  berpartisipasi  meramaikan  event  tersebut.  Oleh 
karenanya,  kemungkinan  besar  hashtag  seputar  SPO  juga 
menjadi  trending  di  Twitter  pada  periode  waktu  tersebut, 
sehingga  ikut  menarik  perhatian  lebih  banyak  user.  Dengan 
frekuensi  paling  rendah  terjadi  pada  awal  Januari,  minggu 
ketiga April, dan minggu terakhir Mei yang mana merupakan 
periode  berakhirnya    SPO2020.    Berdasarkan  grafik  juga, 
ditunjukan fluktuasi naik pada minggu akhir bulan Maret dan 
awal  minggu  bulan  April,  yang  mana  disebabkan  oleh 
berakhirnya  periode  SPO  yang  awalnya  dijadwalkan  pada  31 
Maret,  untuk  kemudian  ramai  diberitakan  akan  diperpanjang 
hingga  29  Mei.  Hal  ini  tentu  menyebabkan  kembali  naiknya 
traffic  seputar  informasi  SPO  tidak  terkecuali  di  Twitter.

Hasil  dari  proses  penghitungan  network  metrices 

ditunjukan pada Tabel II.  

TABEL III 
TABEL NETWORK METRICES 

Network metrices 

Value  

Size 

Graph type 
Diameter 
Modularity 
Average degree 
Average path length 

Nodes:6881 
Edges :8523 

Directed 
7 
0,593 
1,239 
2,439 

Berdasarkan ukuran, diperoleh 6881 node dan 8523 edges 
dengan tipe graph directed dan besaran rata-rata degree 1,239 
yang  artinya  pada  network 
ini  rata-rata  dari  1  user 
menyebarkan  informasi  tentang  SPO  (retweet)  paling  tidak 
kepada 1 user lain. Dengan diameter sebesar 7 dan modularity 
0,593  yang  dapat  dikatakan  network  ini  didominasi  oleh  5 
komunitas  dari  total  3023  komunitas  yang  teridentifikasi. 
Kelima  komunitas  tersebut  berkontribusi  terhadap  90,98% 
ukuran dari network graph yang terbentuk. Berikut merupakan 
visualisasi  dari  network  yang  terbentuk  ditunjukan  pada 
Gambar 4. 

 4 / 8 

 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

promosi  event  SPO  didominasi  oleh  instansi  pemerintah  dan 
user  biasa  dengan  dengan  akun 
instansi  pemerintah 
@bps_statistics yang memiliki nilai degree centrality terbesar 
dengan diikuti oleh akun user biasa (@robbyadwa ) di urutan 
kedua. 

TABEL V 
TABEL RINCIAN DEGREE DARI TOP 10 USER 

Nama Akun 

indegree 

outdegree 

Degree 

@bps_statistics 

@robbyadwa 

@alfareno 

@bpsprovjateng 

@irosidie 

@bps_jambi 

@humasjateng 

@ganjarpranowo 

@jokowi 

@murnishafira 

235 

900 

231 

31 

51 

0 

9 

0 

1 

40 

695 

2 

2 

82 

1 

48 

36 

45 

43 

3 

930 

902 

233 

113 

52 

48 

45 

45 

44 

43 

Ketika dianalisis dari besaran indegree dan outdegree 
sesuai  tabel  9  barulah  diketahui  bahwa  besaran  degree 
centrality  @robbyadwa  disebabkan  oleh  besarnya  indegree/ 
ukuran  banyaknya  edge  yang  mengarah  ke  node  tersebut, 
dengan  kata  lain  @robbyadwa  mengunggah  atau  me-retweet 
informasi SPO sebanyak 900 kali dengan 2 kali unggahannya 
diteruskan  oleh  user  lain  oleh  karenanya  @robbyadwa  dapat 
memiliki  nilai  degree  yang  tinggi.  Sedangkan  akun  official 
BPS (@bps_statistics) mengunggah atau me-retweet informasi 
sejumlah 235 posting-an dengan 695 kali post-nya diteruskan 
oleh user lain. 

Selanjutnya  dilakukan  visualisasi  dari  mekanisme 
penyebaran  informasi  menggunakan  metode  SNA.  Pada 
gambar 5 menunjukan visualisasi dari network yang terbentuk 
ketika  instansi  pemerintah  dijadikan  node  0/  inisiator  dalam 
kasus ini digunakan akun @bps_statistics sebagai keyplayer. 

Gambar 4. Visualisasi network SPO 

Gambar  4  menunjukan  network  dari  user  yang 
terlibat  dalam  penyebaran  informasi  SPO.  Bentuk  network 
menggunakan layout Yifan Hu dengan  pewarnaan diterapkan 
berdasarkan  closeness  centrality,  yaitu  ukuran  yang  dapat 
mengukur  seberapa  dekat  suatu  aktor  dengan  aktor  lainnya 
pada  suatu  network,  contohnya  untuk  node  berwarna  hijau 
merupakan  kelompok  node  yang  memiliki  nilai  Cc  =  0  yaitu 
tidak  memiliki  akses  ke  node 
lainnya  atau  bersifat 
mengunggah  sendiri  tanpa  meneruskan  informasi  dari  user 
lain. Selain itu, untuk ukuran node diatur untuk menyesuaikan 
rank  dari  nilai  degree  centrality  dari  masing  masing  node, 
contohnya  untuk  node  berwarna  abu-abu  paling  besar  di 
bagian bawah merupakan akun @bps_statistics yang memiliki 
salah satu nilai degree terbesar. 

 Semakin  besar  nilai  degree  centrality  menunjukan 
besarnya  pengaruh  user  tersebut  pada  network.    Hasilnya 
ditunjukan pada Tabel IV. 

TABEL IV 
TABEL TOP 10 USER BY DEGREE 
Nama Akun 

Tipe akun 

Rank 

1  @bps_statistics 

2  @robbyadwa 

3  @alfareno 

4  @bpsprovjateng 

5  @irosidie 

6  @bps_jambi 

7  @humasjateng 

8  @ganjarpranowo 

9  @jokowi 

instansi 

user biasa 

user biasa 

instansi 

user biasa 

instansi 

instansi 

pejabat 

pejabat 

10  @murnishafira 

user biasa 

Tabel  IV  menunjukan  urutan  user  yang  dianggap 
paling  berpengaruh  berdasarkan  degree  centrality-nya  pada 

Gambar 5. Mekanisme penyebaran informasi dari instansi pemerintah 

 5 / 8 

 
 
 
 
 
 
 
 
 
 
 
 
Gambar    6  merupakan  visualisasi  dari  network  yang 
terbentuk  ketika  pejabat  pemerintahan  dijadikan  node  0/ 
inisator dengan akun  @jokowi sebagai keyplayer. 

mekanisme  lainnya,  artinya  user  biasa  dapat  dikatakan  tidak 
cocok  untuk  dijadikan  key  player  pada  penyebaran  informasi 
seputar SPO karena kurangnya pengaruh yang diberikan. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Dari  ketiga  mekanisme  kemudian  dirumuskan  rasio 
penyebaran  informasi  berdasarkan  jumlah  node  yang  terlibat 
di  dalam  mekanisme  terhadap  keseluruhan  network.  Dengan 
menggunakan 1 node level 0 pada masing-masing mekanisme, 
5  node  level  1  pada  mekanisme  instansi  pemerintah,  3  node 
level  1  pada  mekanisme  pejabat  pemerintah,  2  node  level  1 
untuk  mekanisme  user  biasa,  dan  keseluruhan  node  yang 
terlibat pada tiap-tiap  mekanisme untuk  node  level 2. Tujuan 
dari  proses  ini  adalah  untuk  menunjukan  pengaruh  yang 
diberikan  dari  masing-masing  key  player  dalam  penyebaran 
informasi 
lain  pada  network.  Hasilnya 
terhadap  user 
ditunjukan pada Gambar 7 

Rasio Penyebaran Informasi 

e
d
o
N

l

a
t
o
T

i
r
a
D
%

35,0%

30,0%

25,0%

20,0%

15,0%

10,0%

5,0%

0,0%

Instansi

Pejabat

User biasa

level 0 level 1 level 2

Gambar 7. Mekanisme penyebaran informasi dari user biasa 

Berdasarkan  Gambar  6  dapat  dikatakan 

instansi 
pemerintah  merupakan  mekanisme  terbaik  dalam  penyebaran 
informasi  seputar  SPO.  Mekanisme  ini  sudah  terdiri  hampir 
35% (32,53%) dari keseluruhan  node aktif di dalam network. 
Di  sisi  lain,  sebaran  nilai  dari  mekanisme  lainnya  hanya 
menyentuh 3% dan 1% dari total node aktif di dalam network. 
Salah  satu  penyebab  utama  mekanisme  instansi  pemerintah 
memiliki nilai yang tinggi adalah karena akun yang dijadikan 
inisiator  merupakan 
instansi  memiliki  event  SPO  dan 
mempromosikannya  sendiri,  sehingga  memiliki  keyakinan 
terhadap validitas informasi yang disebarkan. 

TABEL VI 
TABEL SUMMARY NILAI ER 

min 
max 
median  0 
mean 
modus 

0 
766,67 

1,0779 
0 

Tabel VI merupakan hasil penghitungan nilai Engagement 
Rate dari user penyebar informasi SPO. Terlihat nilai ER dari 
user  penyebar  informasi  SPO  secara  keseluruhan  memiliki 

 6 / 8 

Gambar 6. Mekanisme Penyebaran informasi dari pejabat pemerintahan 

Gambar  7. merupakan visualisasi dari network yang 
terbentuk ketika user  biasa dijadikan node 0/  inisator dengan 
akun  @guntur38 sebagai key player. 

Gambar 6. Mekanisme penyebaran informasi dari user biasa 

Jika  dibandingkan,  mekanisme  penyebaran 

informasi 
dengan akun instansi pemerintah mendapat respons informasi 
yang  disebarkan  lebih  baik,  karena  diteruskan  oleh  lebih 
banyak  golongan  dibandingkan  mekanisme  lain.  Hal  tersebut 
menandakan 
instansi 
pemerintah  (dalam  kasus  ini  bersumber  dari  BPS)  dianggap 
valid  dan  dapat  dipercaya  oleh  banyak  golongan  untuk 
disebarluaskan.  

informasi  yang  bersumber  dari 

Akan tetapi jika dilihat dari jumlah node 2 yang diperoleh, 
mekanisme  pejabat  pemerintahan  sebagai  inisiator  cenderung 
memiliki  variasi  user  yang  meneruskan  informasi  lebih 
banyak  dibanding  mekanisme  lain.  Hal  tersebut  menandakan 
informasi  seputar  SPO  yang  disebarkan  oleh  pejabat 
pemerintahan  cenderung  dapat  diteruskan  ke  lebih  banyak 
user  lain  atau  memiliki  pengaruh  yang  lebih  besar  dibanding 
mekanisme lainnya.  

Pada  mekanisme  user  biasa  sebagai  inisiator  karena 
terlihat  cenderung  lemah  di  semua  kondisi  dibandingkan  2 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
rata-rata  yang  sudah  cukup  baik  yaitu  sebesar  1,0779% 
dengan  nilai  tertinggi  766,67%  dan  terendah  0.  Dengan  nilai 
median  dan  modus  0  menandakan  kebanyakan  data  bernilai 
ER  0%  atau  tidak  memiliki  engagement  terhadap  followers-
nya.  Didukung  oleh  Tabel  VII.  yang  menampilkan  unggahan 
dengan ER paling tinggi dari suatu user  dan Tabel VIII. yang 
menampilkan  user  dengan  total  engagement  tertinggi  yang 
diperoleh dalam satu unggahan 

TABEL VII 
TABEL TOP 10 USER BY ER 

username 

followers 

total_eng 

ER (%) 

stk31 

guntur38 

bpskotacimahi 

kyasautama 

Alnierasyid 

aanyelirsylvana 

mindnetter 

panjang_jalan 

KecamatanWatuk1 

Hasian841 

3 

143 

3 

230 

3 

10 

14 

1 

7 

2 

23 

766,67 

757 

529,37 

15 

500,00 

478 

207,83 

5 

166,67 

16 

18 

1 

7 

2 

160,00 

128,57 

100,00 

100,00 

100,00 

TABEL VIII 
TABEL TOP 10 USER BY TOTAL ENGAGEMENT  

username 

followers 

total_eng 

ER (%) 

jokowi 

15752163 

7338 

0,046584079 

bps_statistics 

45441 

3004 

6,610770009 

DKIJakarta 

1336622 

1462 

0,109380214 

prastow 

guntur38 

tapiiqbaalqibul 

BKNgoid 

detikcom 

kyasautama 

Hanif_AF 

52143 

143 

94734 

844319 

787 

1,509310933 

757 

529,3706294 

560 

0,591128845 

517 

0,061232781 

16975609 

483 

0,002845259 

230 

8883 

478 

362 

207,826087 

4,07519982 

Berdasarkan Tabel VII dan Tabel VIII terlihat bahwa user 
dengan  ER  yang  sangat  tinggi  pada  network  penyebaran 
informasi SPO kebanyakan memiliki karakteristik yang sama, 
yaitu  jumlah  followers  yang  sedikit,  sehingga  walaupun 
kenaikan  jumlah  engagement-nya  sedikit  akan  dinilai  besar 
berdasarkan  proporsi  jumlah  follower-nya.  Hal  ini  dapat 
terjadi  karena  dalam  penghitungan  ER  by  post  menangkap 
nilai engagement suatu user terhadap user lainnya pada suatu 
topik dalam satu post yang spesifik. 

 Lalu  pada  Tabel  VIII  diurutkan  berdasarkan  nilai  total 
engagement  yang  diperoleh.  Berdasarkan  hasil  top  10  total 
engagement  yang  diperoleh,  terdapat  kesamaan  karakteristik 
lebih  banyak  cenderung 
yaitu 

follower  yang 

jumlah 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

meningkatkan  total  engagement.  ER  yang  diperoleh  user 
dengan  total  engagement  yang  banyak,  mayoritas  tidak 
memenuhi  kriteria  ER  yang  optimal.  Hal  ini  terjadi  karena 
besarnya  jumlah  follower  suatu  user  mempengaruhi  nilai  ER 
yang  optimal  baginya  dalam  mempromosikan  sesuatu, 
sehingga pada kasus  ini  banyak  user dengan  jumlah follower 
yang tinggi justru memiliki nilai ER yang lebih rendah.   

Pada  promosi  SPO  baik  unggahan  dari  user  dengan 
follower  sedikit  maupun  unggahan  dari  user  yang  memiliki 
follower  banyak,  sama-sama  penting  karena  tujuan  dari 
promosi SPO untuk  mengenalkan dan membangun kesadaran 
masyarakat Indonesia untuk berpartisipasi pada periode waktu 
yang  ditentukan.  Namun  jika  dibandingkan,  tentu  pengaruh 
yang diberikan oleh user dengan jumlah follower banyak lebih 
besar  dibanding  user  dengan  ER  yang  besar  namun  jumlah 
follower-nya sedikit karena memiliki peluang yang lebih besar 
informasi unggahannya akan disebarkan lebih banyak. 

later.com, 

Dikutip  dari 

influencer/  pelaku  penyebar 
informasi  dengan  jumlah  pengikut  yang  besar  merupakan 
faktor  penting  dalam  mempromosikan  campaign  yang  mana 
kesadaran  massa  merupakan  kunci  suksesnya  pemasaran. 
Dalam  kasus  pemasaran  event  SPO  yang  memerlukan 
kesadaran  publik  dalam  berkontribusi  pada  suatu  periode 
waktu,  tentunya  cara  mempromosikan  event  tersebut  melalui 
influencer/  public  figure  dapat  meningkatkan  engagement 
pada masyarakat terhadap event SPO. 

VII. 

PENUTUP 

Dengan  tujuan    untuk  menganalisis  bagaimana  proses 
penyebaran  informasi  di  media  sosial  mengenai  suatu  event 
baru  yang  akan  dilakukan  oleh  instansi  pemerintah,  penulis 
telah  berhasil  memodelkan  proses  penyebaran  informasi 
dengan  menampilkan  visualisasi  dari  mekanisme  penyebaran 
informasi  dari  beberapa  key  player  menggunakan  metode 
SNA. Hasil yang diperoleh antara lain : 
●  Karakteristik  penyebar  informasi  SPO  didominasi  dari 
jenis user biasa, dengan kebanyakan user memiliki nilai 
ER = 0 atau tidak  memiliki pengaruh, sedangkan secara 
keseluruhan  nilai  ER  yang  diperoleh  sudah  cukup  baik 
dengan nilai rata-rata 1,077%. 

●  Berdasarkan  network  yang  terbentuk,  diperoleh  besaran 
rata-rata  degree  1,239  yang  artinya  pada  network  ini 
rata-rata  1  user  meneruskan  informasi  kepada  1  user 
lain.  Dengan  diameter  sebesar  7  dan  modularity  0,593 
yang  dapat  dikatakan  network  ini  didominasi  oleh  5 
komunitas berdasarkan closeness centrality-nya. 

●  Dinilai  dari  besaran  ER  baik 

rata-rata  maupun 
distribusinya,  efektifitas  penyebaran  informasi  dari  user 
yang  terlibat  sudah  cukup  baik  namun  dapat  dievaluasi 
dengan  menaikan  engagement  yang  diberikan  baik  oleh 
user  dengan  follower  sedikit  maupun  oleh  user  yang  
memiliki follower banyak. Karena bisa jadi konten yang 
dilihat  1.000  orang  dan  aktif  memberikan  respon,  lebih 
baik  ketimbang  dilihat  100.000  orang  namun  tidak 
merespon  konten  sama  sekali.  Meski,  kembali  lagi, 
tergantung objective atau  tujuan  yang  ditetapkan.  Atau 
sebagai  alternatif  bisa  melibatkan  public  figure  yang 

 7 / 8 

 
 
 
 
 
 
memiliki  image  dan  engagement  yang  baik  terhadap 
pengikutnya untuk memperluas tersebarnya informasi. 

●  Mekanisme 

paling 

dengan 
optimal 
menggunakan akun instansi pemerintah sebagai inisiator, 
dimana  kepercayaan  publik  terhadap  sumber  informasi 
yang  disebarkan  meningkatkan 
informasi 
tersebar. 

diperoleh 

jumlah 

Dari  penelitian  ini  diperoleh  wawasan  mengenai  proses 
penyebaran  informasi  dari  event  Sensus  Penduduk  Online 
yang 
telah  dilakukan  oleh  BPS.  Dengan  mengetahui 
mekanisme dan nilai optimal dari engagement yang diberikan,  
organisasi dapat memahami karakteristik audiens. Hal tersebut 
dapat  membantu  dalam  merumuskan  strategi  marketing  yang 
baik  dalam  memasarkan  event  yang  ingin  diperkenalkan 
secara efektif dengan respons positif. 

Penelitian ini tentu tidak luput dari kekurangan dan sangat 
terbuka  untuk  dikembangkan  dalam  penelitian  lebih  lanjut. 
juga  menyarankan  menggunakan  metode 
Penulis 
pengumpulan  data  yang  lebih  baik  lagi,  khususnya  yang 
mampu  mengurangi  batasan  untuk  memperoleh  data 
historical.  Karena  pada  penelitian  ini  penulis  menggunakan 
metode pengumpulan data yang bersifat open-source sehingga 
terdapat  banyak  keterbatasan  dalam  memperoleh  data.  Selain 
itu juga jika memungkinkan disarankan untuk membuat model 
figure 
mekanisme  penyebaran 
berlaku  sebagai  inisiator.  Menerapkan  mekanisme  ilustrasi 
lain seperti public figure sebagai inisiator. 

informasi  dengan  public 

DAFTAR PUSTAKA 
[1]  A.  W.  Wijayanto  and  T.  Murata,  “Learning  adaptive  graph  protection 
strategy  on  dynamic  networks  via  reinforcement  learning,”  in  2018 
IEEE/WIC/ACM International Conference on Web Intelligence (WI), ser. 
WI 2018. New York, USA: IEEE, Dec 2018, pp. 534–539. 

[2]  Facebook. (2017, 3) How does news feed decide which stories to show? 
[Online]. Available: https://www.facebook.com/help/166738576721085 
[3]  D.  Baum,  M.  Spann,  J.  Füller,  and  C.  Thürridl,  “The  impact  of  social 
media  campaigns  on  the  success  of  new  product  introductions”,  in 
Journal of Retailing and Consumer Services, July 2018. 

[4]  A. Guille, H. Hacid, C. Favre, and D.A. Zighed, ""Information  diffusion 
in  online  social  networks:  a  survey"",  in  ACM  SIGMOD  Record,  vol. 
42,no. 2, pp. 17–28, June 2013. 
F.  Aguilera, 

and  M. 
Grana,""Semantically  Enhanced  Network  Analysis 
Influencer 
Identification    in  Online  Social  Networks"",  in  Journal  Neurocomputing, 
vol. 326-327, pp. 1-11, January 2019. 

J.D.  Nunez-Gonzales, 
for 

[5]  S.A.  Ríos, 

[6]  A.  Alamsyah  and  F.  Adityawarman,  “Hybrid  Sentiment  and  Network 
Analysis  of  Social  Opinion  Polarization”, 
in  Fifth  International 
Conference  on  Information  and  Communication  Technology  (ICoICT), 
2017, May 2017. 

[7]  P.  S.  Richardson,  P.  Choong,  M,  Parker,  “Social  Media  Marketing 
Strategy:  Theory  and  Research  Propositions”,  in  Journal  of  Marketing 
Development  and  Competitiveness  Vol.  10(2)  2016  (pp.  24-34), 
September 2016. 

[8]  BPS  (2020,  6)  Partisipasi  sensus  penduduk  online  [Online].  Available  : 

https://jateng.bps.go.id/news/2020/06/10/380/partisipasi-sensus-
penduduk-online.html 

[9]  A. Alamsyah, B.C. Sarniem, and Indrawati, “Direct Comparison Method 
of  Information  Dissemination  Using  Legacy  and  Social  Network 
Analysis”  in  3rd  International  Conference  of  Science  and  Technology 
(ICST), August 2017. 

[10] M.E.J.  Newman,  “Communities,  Modules  and  Large-Scale  Structure  in 

Networks”, Nature Physics, vol. 8, pp. 25-31, 2012. 

[11] Wearesocial 

(2020,  2)  DIGITAL  2020: 

INDONESIA 

[Online]. 

Available: https://datareportal.com/reports/digital-2020-indonesia   

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

[12] ContentCal (2020,  5) What is a Good Social Media Engagement Rate?  
https://www.contentcal.io/blog/what-is-a-good-

[Online]  Available: 
social-media-engagement-
rate/#:~:text=Engagement%20rates%20tend%20to%20be%20lower%20f
or%20Twitter%20than%20for,consistently%20more%20than%20that%2
C%20though. 

[13]  G. Semiz and P.D. Berger, “Determining the Factors that Drive Twitter 
Engagement-Rates”,  in  Archives  of  Business  Research  –  Vol.5,  No.2,  
February 2017. 

[14] Y.  Hu  and  Y.  K.  Hong,  “Modeling  Twitter  Engagement  in  Real-World 
Events”, in Proceedings  of the 50th Hawaii International Conference on 
System Sciences 2017. 

[15] I.  Khan,  S.  K.  Naqvi,  M.  Alam  and  S.  N.  A.  Rizvi,  “A  Framework  for 

Twitter Data Analysis “, in conference paper 2015. 

[16] Accurate.id  (2020,  11)  Engagement  Rate  Adalah  :  Pengertian,  Faktor, 
dan Cara Mengukurnya [Online] Available: https://accurate.id/marketing-
manajemen/engagement-rate-adalah/  

[17] M.D.N.  Arusada,  N.A.S.  Putri,  and  A.  Alamsyah,  ""Training  Data 
for  Multiclass  Text  Classification,""  5th 
and  Communication 

Optimization  Strategy 
International  Conference  on 
Technology, May 2017. 

Information 

[18] Later.com  (2021,  04)  Nano  or  Macro:  How  an  Influencer’s  Follower 
: 

Count 
Impacts  Engagement  Rate 
https://later.com/blog/influencer-engagement-rate/  

  Available 

[Online] 

 8 / 8 

 
 
 
 
"
221709488,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Kajian Estimasi Luas Panen dan Produksi 

Tanaman Jagung Menggunakan Pendekatan Remote 
Sensing 
(Studi Kasus : Kabupaten Tuban, Jawa Timur Tahun 2018) 

Abdurrofi Robbani (221709488, 4SI1) 

Dosen Pembimbing: Takdir, SST., M.T. 

Ringkasan—  Tantangan  kebutuhan  data  dan  perkembangan 
teknologi  informasi  yang  mana  memberikan  akses  tersedianya 
sumber  data  melalui  big  data  menjadi  alasan  untuk 
memanfaatkan penerapan remote sensing pada bidang pertanian 
khususnya tanaman jagung yang menjadi fokus penelitian. Pada 
penelitian ini dilakukan kajian estimasi luas panen dan produksi 
tanaman  jagung  dengan  pendekatan  remote  sensing  yang  mana 
bersumber  dari  citra  satelit  dengan  pengolahan  menggunakan 
machine  learning  untuk  mengklasifikasi  tutupan  lahan.  Pada 
estimasi produksi dilakukan pendeketan lanjutan menggunakan 
analisis regresi non-linear untuk dan diperoleh model indeks EVI 
polinomial derajat 3 dengan nilai R2 0,309 dan RSE 0,00245. Hasil 
dari penghitungan estimasi luas panen tanaman jagung memiliki 
luas  sebesar  44.504,91  ha  dengan  persentase  eror  8,49%. 
Sedangkan  hasil  estimasi  produksi  tanaman  jagung  didapatkan 
nilai sebesar 265.149,225 ton dengan persentase eror 26,52%.    

Kata Kunci— Jagung, Estimasi Luas Panen, Estimasi Produksi, 

Remote Sensing, Machine Learning, Regresi Non-Linear. 

I.  LATAR BELAKANG 

Jagung  (Zea  Mays  L.)  merupakan  salah  satu  tanaman 
pangan yang merupakan sumber makanan pokok di Indonesia 
selain beras, singkong, dan sagu. Jagung juga merupakan salah 
satu komoditas tanaman pangan yang sangat produktif di dunia 
dengan total produksi 1.060.247.727 ton per tahun. Penyebaran 
yang  luas  dikarenakan  tanaman  jagung  dapat  beradaptasi 
dengan baik pada berbagai lingkungan [1]. Hal tersebut dapat 
terlihat  pada  tabel  produksi,  luas  panen,  dan  produktivitas 
jagung  di  Indonesia  berdasarkan  data  Badan  Pusat  Statistik 
(BPS) pada tahun 2014 hingga 2018 sebagai berikut.  

TABEL I 
TABEL KONDISI JAGUNG DI INDONESIA TAHUN 2014-2018 
Produksi (Ton)  Luas Panen (ha) 

Tahun 

No 

Produktivitas 
(ton/ha) 

1  2014 

19,008,426 

3,837,019.0 

2  2015 

19,612,435 

3,787,367.0 

3  2016 

23,578,413 

4,444,368.9 

4  2017 

28,924,015 

5,533,169.0 

5  2018 

30,055,623 

5,734,326.0 

4.95 

5.18 

5.31 

5.23 

5.24 

Berdasarkan  data  pada  tabel  1  dapat  dilihat  bahwa  secara 
garis besar baik produksi, luas panen serta produktivitas jagung 
mengalami  peningkatan.  Namun  pengumpulan  data  tanaman 
jagung  pada  saat  ini  masih  menggunakan  metode  ubinan 
dengan  cara  melakukan  survei  lapangan.  Sebagai  lembaga 
statistik  pemerintah  di  Indonesia,  BPS  mengumpulkan  data 
tanaman  jagung  dengan  melakukan  kegiatan  survei  tanaman 
pangan/ubinan  untuk  mendapatkan  angka  produktivitas 
tanaman  jagung  tersebut.  Kegiatan  survei  tersebut  dinilai 
memerlukan  biaya  dan  waktu  yang  tidak  sedikit  [2].  Seperti 
halnya pada survei luas panen dan luas lahan tanaman pangan 
yang  memerlukan  waktu  hampir  satu  tahun  pada  tahap 
perencanaan  sampai  analisis,  sedangkan 
tuntutan  akan 
ketersediaan data yang aktual juga semakin mendesak. Selain 
itu  kegiatan  ubinan  juga  dinilai  kurang  akurat  karena  angka 
dugaan  yang  cenderung  bias  karena  lebih  tinggi  dari  angka 
sesungguhnya  (overestimate).  Terlihat  pada  tahun  2018, 
estimasi  produksi  jagung  mencapai  30,05  juta  ton  pipilan 
kering  yang mana  angka  produksi  sebenarnya  diyakini  hanya 
sebesar 16,5 juta ton pipilan kering [3]. 

Melanjutkan  rencana  strategis  BPS  2020-2024  yang 
menimbang  akan  kebutuhan  data  statistik  yang  berkualitas, 
lengkap,  akurat,  relevan,  mutakhir,  dan  berkesinambungan 
tentunya  akan  diperlukannya  metode  pengumpulan  data  baru 
yang dapat mengatasi atau melengkapi kekurangan-kekurangan 
pada  metode  yang  lama.  Salah  satu  metode  yang  dapat 
diaplikasikan 
adalah  dengan  memanfaatkan  kemajuan 
teknologi  informasi,  khususnya  pada  bidang  remote  sensing. 
Remote  sensing  adalah  ilmu  dan  seni  untuk  memperoleh 
informasi 
tentang  suatu  objek,  daerah,  atau  fenomena 
(geofisika)  melalui  analisis  data  yang  diperoleh  dengan  suatu 
alat  tanpa  harus  melakukan  kontak  langsung  dengan  objek, 
daerah atau fenomena yang sedang dikaji [4]. 

Terdapat banyak kajian yang memanfaatkan bidang remote 
sensing  khususnya  pada  penggunaan  citra  satelit  untuk 
terkait  bidang 
melakukan  pengukuran  estimasi-estimasi 
pertanian  seperti  melihat  kondisi,  jumlah  dan  ketersediaan 
tanaman  dalam  kaitannya  dengan  mengetahui  (memprediksi) 
hasil  produksi  dari  suatu  tanaman  [5].  Salah  satu  alat  yang 
digunakan dalam teknologi remote sensing adalah citra satelit. 
Dari  citra  satelit  kita  dapat  merekam  gambar  permukaan 
sumber daya lahan secara dinamis dan akurat [6]. Hal tersebut 
tentunya  dapat  mempersingkat  waktu  dan  memudahkan 
pelaksanaan  pemetaan  tanah  dan  evaluasi  lahan,  sehingga 

 1 / 8 

 
 
 
 
 
 
menghemat waktu dan biaya. Salah satu citra satelit yang dapat 
digunakan adalah citra satelit Sentinel-2. Sentinel-2 merupakan 
satelit yang mana memiliki citra dengan resolusi tinggi dengan 
swath yang lebar, serta revisit di lokasi yang sama setiap 10 hari 
[7].  Selain  itu  Sentinel-2  memiliki  Multispectral  Instrument 
(MSI) dengan resolusi hingga 10 meter (untuk band 2, band 3, 
band 4, dan band 8) yang dapat digunakan untuk kajian-kajian 
monitoring  tutupan  lahan,  termasuk  vegetasi,  tanah  dan  air, 
juga jaringan air dan area pantai. 

Dari beberapa band yang dihasilkan oleh suatu citra satelit 
maka  dapat  dibentuk  indeks-indeks  hasil  transformasi  seperti 
indeks  vegetasi,  indeks  bangunan,  dll.  yang  dapat  digunakan 
untuk  dilakukan  pengolahan  statistik  selanjutnya  seperti 
melihat pengaruh indeks tersebut terhadap suatu variabel lain. 
Hasil  yang  menjanjikan  diperoleh  melalui  hubungan  statistik 
antara nilai pantulan satelit (band) atau indeks vegetasi dengan 
hasil  panen  [8].  Terdapat  kajian  yang  melakukan  analisis 
menggunakan indeks vegetasi dan kaitannya dengan produksi 
maupun  produktivitas  jagung.  Dalam  kajian  [5]  dihasilkan 
output berupa klasifikasi serta luas tutupan lahan menggunakan 
algoritma  machine  learning  dan  juga  hasil  model  regresi 
sederhana  dengan  memanfaatkan  indeks  vegetasi  dan  nilai 
produktivitas jagung pada daerah uji sampel. Sedangkan pada 
penelitian ini dihasilkan nilai estimasi luas panen jagung serta 
estimasi  produksi  jagung  itu  sendiri.  Dengan  dihasilkannya 
nilai-nilai tersebut juga diharapkan dapat membantu BPS dalam 
hal  ini  sebagai  pengumpul  data  jagung  agar  memenuhi 
kebutuhan  data  statistik  yang  berkualitas,  lengkap,  akurat, 
relevan,  mutakhir,  dan  berkesinambungan  sesuai  dengan 
arahan pada rencana strategis BPS 2020-2024, khususnya pada 
data estimasi luas panen dan produksi tanaman jagung. 

II.  TUJUAN PENELITIAN 

Berdasarkan latar belakang di atas maka secara umum tujuan 
penelitian ini adalah untuk mengkaji nilai estimasi luas panen 
dan produksi jagung menggunakan pendekatan remote sensing 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

pada daerah studi Kabupaten Tuban, Jawa Timur. Sedangkan 
secara khusus tujuannya adalah: 

1.  Melakukan  klasifikasi  tutupan  lahan  pada  daerah  studi 

untuk mencari daerah perkebunan jagung. 

luas  panen 

2.  Mendapatkan  estimasi 

jagung  yang 
didapatkan dari hasil klasifikasi tutupan lahan tersebut. 
3.  Menguji  model  sederhana  untuk  melihat  hubungan 
antara produktivitas jagung dengan nilai indeks vegetasi 
yang digunakan. 

4.  Mendapatkan estimasi produksi jagung yang didapatkan 
dari  hasil  penghitungan  model  sederhana  yang  telah 
diperoleh. 

III. PENELITIAN TERKAIT 

Penelitian  ataupun  kajian  tentang  pemanfaatan  citra  satelit 
untuk mengklasifikasi daerah tutupan lahan dan mengestimasi 
produktivitas jagung telah banyak dilakukan. Mulai dari kajian 
terkait identifikasi lahan perkebunan jagung hingga melakukan 
estimasi  produksi  tanaman  jagung  itu  sendiri.  Dari  berbagai 
referensi  tersebut  maka  peneliti  mengambil  peluang  dari 
pemanfaatan  remote  sensing  dalam  mengidentifikasi  daerah 
perkebunan  jagung  serta  estimasi  produksi  jagung  tersebut 
dengan  daerah  studi  yang  ditentukan.  Pada  penelitian  ini 
menggunakan sumber data utama yaitu citra satelit sentinel-2 
dan beberapa data tambahan lainnya. Dalam penelitian ini juga 
menguji  model  regresi  sederhana  untuk  melihat  hubungan 
antara  nilai  produktivitas  padi  dan  indeks  vegetasi  yang 
digunakan  yang  nantinya  hasil  model  akan  digunakan  untuk 
penghitungan estimasi produksi jagung terlepas dari baik atau 
tidaknya  model  yang  dihasilkan.  Penelitian 
juga 
mengadopsi beberapa referensi terkait seperti yang dijelaskan 
pada tabel 2 yang mana berisikan tujuan dari penelitian, metode 
yang digunakan, sumber data, serta daerah studi dari penelitian 
tersebut.  

ini 

TABEL II 
TABEL PENELITIAN TERKAIT 

No 

1 

Pengarang, Tahun 

Tujuan 

Metode 

Sumber Data 

Daerah Studi 

A. W. Wijayanto et al., 
2020 [9] 

Identifikasi 
perkebunan jagung 

Machine Learning 
(Random Forest) 

Citra Landsat 8, Sentinel-1 dan 
Setinel-2 

10 Kabupaten/Kota pilihan di 
Jawa Timur 

2 

H. Fang et al., 2011 [8] 

Estimasi produksi 
jagung 

CSM–CERES–Maize 
model 

3 

A. Ines et al, 2013 [10] 

Estimasi produksi 
jagung 

4 

5 

6 

Aniko et al., 2018 [11] 

L. M. Irsan et al., 2019 [5] 

Y. M. Fernandez-Ordoñez 
& J. Soria-Ruiz, 2017 [12] 

Estimasi produksi 
jagung 
Identifikasi 
perkebunan jagung 
dan estimasi 
produksi jagung 
Identifikasi 
perkebunan jagung 
dan estimasi 
produksi jagung 

Ensemble Kalman 
Filter (EnKF) dan 
(DSSAT-CSM)-Maize 
model 
Stepwise linear 
regression 
Maximum likelihood 
dan RLS 

Maximum likelihood 
dan Regresi 
Eksponensial 

Shp lahan pertanian dan Citra 
Moderate Resolution Imaging 
Spectroradiometer (MODIS) 
LAI 
Shp lahan pertanian, Citra 
AMSR-E soil moisture, dan Citra 
MODIS LAI 

Shp lahan pertanian dan Citra 
MODIS 
Citra Sentinel-2 

Indiana, Amerika Serikat 

Iowa, Amerika Serikat 

Hungaria 

Kabupaten Jeneponto, 
Sulawesi Selatan 

Citra SPOT 5 

Atlacomulco, Meksiko 

 2 / 8 

 
 
 
7 

Topik Skripsi 

Identifikasi 
perkebunan 
jagung dan 
estimasi produksi 
jagung 

Machine Learning 
(CART, Random 
Forest, SVM, dan 
Naïve Bayes) dan 
Analisis RLS 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Citra Sentinel-2 

Kabupaten Tuban, Jawa 
Timur 

IV. METODE PENELITIAN  
Metode  penelitian  dalam  rangka  kajian  penelitian  ini 

memiliki beberapa poin penting antara lain. 

A.  Studi Kasus 

Dalam menentukan daerah studi kasus, peneliti melakukan 
beberapa observasi dan pemenuhan kriteria antara lain adalah 
daerah  provinsi  yang  memiliki  jumlah  produksi  jagung 
terbanyak  dan  dipersempit  menjadi  daerah  kabupaten/kota 
yang  memiliki  produksi  jagung  terbanyak  pada  provinsi 
tersebut. Dengan demikian didapatkan daerah studi kasus untuk 
penelitian ini yaitu Kabupaten Tuban, Jawa Timur pada tahun 
2018. 

B.  Sumber Data  

Beberapa sumber data yang digunakan pada penelitian ini 

adalah sebagai berikut. 

a.  Citra Satelit Sentinel 2 - Level 1C 

Merupakan citra multispektrum beresolusi tinggi yang 
diambil pada tanggal 30 Maret 2018. Dimana pada periode 
30  Maret  2018  merupakan 
tanggal  yang  dipilih 
berdasarkan perkiraan 60 hari setelah tanam (HST) jagung 
pada Kabupaten Tuban, yang mana menurut penelitian [5] 
waktu  umur  jagung  tersebut  dinilai  sudah  masuk  pada 
waktu  ideal  untuk  perekaman  citra  karena  memiliki 
pantulan spektrum yang tinggi. Citra Sentinel 2 – Level 1C 
in  didapatkan  dari  dataset  Google  Earth  Engine  (GEE) 
yang mana bersumber dari Copernicus Sentinel Data and 
Service Information. 
b.  Shapefile (SHP) Kabupaten Tuban 

Peta  SHP  batas  administrasi  Kabupaten  Tuban 
bersumber  dari  Sub  Direktorat  Pengembangan  Pemetaan 
Statistik  BPS  Pusat  pada  tahun  2019.  Peta  tersebut 
digunakan  untuk  pengolahan  data  geospasial  yang  sesuai 
dengan daerah studi kasus. 
c.  Data  produktivitas,  produksi,  dan 

luas  panen 

Kabupaten Tuban tahun 2018 
Data diambil melalui website BPS Kabupaten Tuban 

pada laman tabel dinamis. 
d.  Data labelling tutupan lahan 

Merupakan titik-titik poin pada tiap klasifikasi tutupan 
lahan yang digunakan sebagai sampel dataset untuk proses 
pengolahan  selanjutnya.  Berikut  pada  tabel  3  merupakan 
jumlah titik sampel labelling untuk setiap kategori tutupan 
lahan yang digunakan dalam penelitian. 

TABEL III 
TABEL KLASIFIKASI TUTUPAN LAHAN 
Sampel Poin 
Kelas 

Tutupan Lahan 

1 

2 

Pemukiman 

Hutan 

300 

300 

3 

4 

5 

6 

Tanah 

Sawah 

Perkebunan Jagung 

Badan Air 

Total 

300 

300 

300 

300 

1800 

C.  Preprocessing 

Pada  tahap  preprocessing  dilakukan  beberapa  penyesuaian 
terhadap  sumber  data  yang  telah  diperoleh  agar  pada  proses 
selanjutnya dapat dilakukan pengolahan dengan baik dan benar. 
Beberapa  penyesuaian  yang  dilakukan  antara  lain  adalah 
sebagai berikut 

a.  Citra satelit Sentinel-2 

Pada  citra  satelit  dilakukan  pemilihan  band  dan 
penambahan  beberapa  indeks  vegetasi  sebagai  band  guna 
menambah  keakuratan  dalam  proses  klasifikasi  yang  akan 
dilakukan. Citra sentinel-2 sendiri memiliki karakteristik 13 
spektrum band yang memiliki resolusi sebesar 10-60 meter 
untuk tiap pikselnya. Sedangkan pada penelitian ini hanya 
menggunakan band yang memiliki resolusi 10 meter yaitu 
band 2 (biru), band 3 (hijau), band 4 (merah), dan band 8 
tersebut 
(Near-InfraRed). 
mempengaruhi  pada  tingkat  kerincian  informasi  dan  akan 
memberi  dampak  semakin  tingginya  ketelitian  informasi 
pada citra. Dari keempat band tersebut kemudian dilakukan 
indeks  vegetasi. 
transformasi  citra  menjadi  beberapa 
Pemilihan  indeks  vegetasi  tersebut  diadopsi  berdasarkan 
penelitian [5] yang menggunakan 6 indeks vegetasi. Namun 
pada penelitian ini hanya menggunakan 4 indes vegetasi di 
antaranya adalah seperti pada tabel 4. 

Pemilihan 

resolusi 

TABEL IV 
TABEL INDEKS VEGETASI 
Formula 

𝑁𝐷𝑉𝐼 =

𝑁𝐼𝑅 − 𝑀
𝑁𝐼𝑅 + 𝑀

𝑆𝐴𝑉𝐼 =

1.5  ×   (𝑁𝐼𝑅 − 𝑀)
(𝑁𝐼𝑅 + 𝑀 + 0.5)

𝑀𝑆𝐴𝑉𝐼2

(2  × 𝑁𝐼𝑅 + 1 − √(2  × 𝑁𝐼𝑅 + 1)2 − 8 × (𝑁𝐼𝑅 − 𝑀) )

2

=

𝐸𝑉𝐼 =  

2.5  × (𝑁𝐼𝑅 − 𝑀)
(1 + 𝑁𝐼𝑅 + 6𝑀 − 7.5𝐵)

No. 

Indeks 
Vegetasi 

1  NDVI 

2 

(Normalised 
Difference  
Vegetation 
Index) 
SAVI (Soil 
Adjusted 
Vegetation  
Index) 
3  MSAVI2 
(Modified 
Soil  
Adjusted 
Vegetation 
Index) 

4  EVI 

(Enhanced 
Vegetation  
Index) 

 3 / 8 

 
 
 
 
 
 
 
 
 
   
 
Keterangan : 
•  NIR  
•  M  
•  B 

: Near-InfraRed 
: Merah 
: Biru 

b.  Labelling data 

Pada proses ini dilakukan penyesuaian (matching) titik 
sampel  pada  citra  sentinel-2  dengan  data  pendukung 
tambahan  yaitu  google  satellite,  google  street  view,  dan 
shapefile  tutupan  lahan  dari  Kementerian  Lingkungan 
Hidup  dan  Kehutanan  (KLHK)  untuk  melihat  kesesuaian 
titik  sampel  terhadap  klasifikasi  tutupan  lahan  masing-
masing sampel. Proses ini dilakukan dengan melibatkan 2 
orang  tambahan  untuk  meningkatkan  faktor  koreksi  dari 
sudut pandang peneliti. 

Pengambilan titik sampel pada proses labelling tersebut 
lebih mengarah kepada mencocokan tutupan lahan dengan 
keadaan data yang sebenernya. Pada gambar 1 merupakan 
salah satu contoh pencocokan titik poin antara citra sentinel 
dengan google satellite. Sedangkan google street view dan 
sebagai  data 
shapefile  KLHK 
pendukung  apabila  tampilan  pada  google  satellite  dirasa 
kurang representatif. Adapun shapefile tutupan lahan yang 
digunakan  hanyalah  daerah  sawah  dan  pertanian  lahan 
kering (untuk mengkoreksi daerah perkebunan jagung). 

tersebut  digunakan 

Gambar 1. Pencocokan tampilan tutupan lahan 

Selain  itu  dilakukan  split  dataset  untuk  dijadikan 
sebagai data training dan data testing dengan perbandigan 
70:30.  Kemudian  setelah  tiap  kategori  tutupan  lahan 
tersebut dibagi menjadi data training dan data testing maka 
dilakukan  merge  dataset  yang  mana  tiap  data  training 
tutupan  lahan  digabung  sehingga  menjadi  satu  keutuhan 
data training. Begitu pula dilakukan hal yang sama terhadap 
merge data untuk data testing. 
c.  Formatting data 
Pada  proses 

ini  dilakukan  pengambilan  dan 
penyesuaian  data  beberapa  indeks  vegetasi  dengan  nilai 
produktivitas  jagung  sebagai  dataset  pada  pengolahan 
regresi non linear untuk dijadikan model estimasi produksi 
jagung. 

D.  Processing (Pengolahan) 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Pengolahan  yang  dilakukan  terbagi  menjadi  dua  bagian 

sebagai berikut. 

a.  Klasifikasi tutupan lahan 

serta  melihat 

algoritma  mana 

Dilakukan  klasifikasi  supervised  dengan  4  algoritma 
machine learning yaitu Classification and Regression Tree 
(CART), Random Forest, Support Vector Machine (SVM), 
dan  Naïve  Bayes.  Keempat  algoritma  tersebut  dipilih 
dengan tujuan untuk melihat bagaimana nilai akurasi yang 
dihasilkan 
yang 
menghasilkan  nilai  akurasi  terbaik.  Selain  dari  segi 
kemudahan  karena  tersedianya  library  algoritma  tersebut 
yang  ada  pada  GEE  juga  berdasarkan  literatur  pada 
penelitian  [9]  yang  mana  menggunakan  Random  Forest 
dalam  mengklasifikasi 
lahan.  Pada  setiap 
algoritma  juga  dilakukan  Stratified  Shuffle  Split  Cross 
Validation  untuk  mengatasi  adanya  overfitting  dari  hasil 
yang didapatkan. 
b.  Regresi Non-Linear 

tutupan 

Dilakukan pengolahan regresi non linear dengan nilai 
produktivitas  sebagai  variabel  dependen  (Y)  dan  nilai 
indeks  vegetasi  sebagai  variabel  independennya  (X). 
Model regresi non-linear yang digunakan dalam penelitian 
merupakan  regresi  polinomial  pangkat  3  dengan  rumus 
sebagai berikut: 

 𝑌𝑖 = 𝛽0 + 𝛽1𝑋 + 𝛽2𝑋2 + 𝛽3𝑋3 + 𝜀𝑖  

  (1) 

Di  mana  𝛽0  adalah  konstanta  dan  𝛽1, 𝛽2,  dan  𝛽3  adalah 
koefisien  dari  setiap  indeks  vegetasi.  sebelum  dilakukan 
analisis  regresi  non-linear  tersebut  diperlukan  asumsi 
linearitas  terhadap  dataset  yang  digunakan.  Uji  linearitas 
yang  digunakan  pada  penelitian  ini  adalah  Ramsey  Test 
atau  yang  biasa  disebut  RESET (regression  specification 
error test). 

E.  Analysis (Hasil Analisis) 

Hasil  analisis  didapatkan  dari  output  pengolahan  yang 

dilakukan antara lain. 

a.  Analisis deskriptif hasil klasifikasi tutupan lahan 
b.  Estimasi  luas  panen  perkebunan  jagung  melalui 

hasil klasifikasi tutupan lahan 

c.  Estimasi  produksi 

tanaman 

jagung  melalui 

penghitungan model regresi yang diperoleh. 

F.  Kesimpulan dan Saran 

Merupakan  rangkuman  dari  hasil  atau  output  yang 
diperoleh  dari  pengolahan  yang  telah  dilakukan  serta  saran 
terkait penelitian selanjutnya. 

V.  KERANGKA PIKIR 

Susunan  kerangka  pikir  penelitian  dibuat  guna  membantu 
dalam  penyusunan  alur  dari  penelitian  ini.  Pada  gambar  2 
dijelaskan bahwa kerangka pikir dimulai dari melihat peluang 
adanya  pemanfaatan  remote  sensing  pada  bidang  pertanian 
khususnya  tanaman  jagung.  Kemudian  dilakukan  pencarian 
sumber data berupa citra satelit dan juga data tambahan lainnya 
seperti  titik  sampel  dan  raw  data  jagung.  Dari  citra  satelit 
lahan 
tersebut  kemudian  dilakukan  klasifikasi 

tutupan 

 4 / 8 

 
 
 
 
 
 
 
 
menggunakan machine learning dan mengambil model terbaik 
sehingga bisa menghasilkan peta klasifikasi tutupan lahan. Dari 
tersebut  kemudian  dilakukan  pengolahan  untuk 
peta 
mendapatkan  nilai  estimasi  luas  panen  dan  juga  estimasi 
produksi  tanaman  jagung.  Dari  kedua  nilai  estimasi  tersebut 
nantinya akan dibandingkan dengan data official statistic terkait 
luas  panen  dan  produksi  tanaman  jagung  pada  daerah  studi 
kasus terpilih. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Tutupan  lahan  diklasifikasi  berdasarkan  nilai  band  yang 
terdapat  pada  masing-masing  sampel  tutupan  lahan.  Adapun 
band yang digunakan yaitu band 2(biru), band 3(hijau), band 
4(merah), dan 4 indeks vegetasi (NDVI, MSAVI2, SAVI, dan 
EVI) untuk membantu mengklasifikasi objek vegetasi. Berikut 
adalah  analisis  deskriptif  sederhana  terkait  sebaran  spektrum 
band untuk tiap kategori tutupan lahan. 

Dari  fitur-fitur 

terpilih  yang  dijelaskan  sebelumnya 
kemudian  dilakukan 
sedikit  analisis  deskriptif  untuk 
menjelaskan sebaran data raster pada setiap fitur seperti yang 
digambarkan pada gambar 3 dan 4. Secara umum band 2, band 
3, dan band 4. Memberikan warna gelap pada objek vegetasi. 
Selain  itu  memberikan  warna  cerah  pada  objek  non  vegetasi 
seperti  pemukiman,  tanah,  dan  badan  air  dengan  tingkat 
kecerahan  yang  berbeda-beda.  Secara  deskriptif  band  2 
memiliki nilai minimum 688 dan maksimum 6986. Sedangkan 
pada band 3 memiliki nilai minimum 522 dan maksimum 6957. 
Selanjutnya  pada  band  4  memiliki  minimum  296  dan 
maksimum 7471. Berbeda dengan band lainnya, pada  band 8 
memberikan  warna  cerah  terhadap  objek  vegetasi  dan  warna 
gelap  pada  objek  non  vegetasi.  Objek  bayangan  awan  dan 
badan  air  menjadi  objek  dengan  warna  gelap  yang  dominan. 
Pada  band  8  memiliki  rentang  nilai  dari  309  hingga  7903. 
Kesamaan dari keempat band ditunjukan pada penggambaran 
awan yang mana memiliki warna yang cerah. 

Gambar 2. Kerangka pikir penelitian 

VI. HASIL DAN PEMBAHASAN 

A.  Analisis Deskriptif Tutupan Lahan 

Pada metode penelitian disebutkan bahwa dalam penelitian  
ini  melakukan  klasifikasi  tutupan  lahan  sebanyak  6  kategori 
yaitu pemukiman, hutan, tanah, sawah, perkebunan jagung, dan 
badan air. Adapun kategori pemukiman merupakan lingkungan 
daerah  tempat  tinggal  meliputi  bangunan,  rumah,  dan  jalan. 
Lalu  untuk  kategori  hutan  merupakan  daerah  hutan  dan 
perkebunan  selain  tanaman  jagung.  Untuk  kategori  tanah 
merupakan tanah lapang (ditumbuhi tumbuhan maupun tidak) 
dan  daerah  industri  semen  tuban.  Kategori  sawah  merupakan 
semua daerah sawah tanpa melihat fase padi di sawah tertentu. 
Selanjutnya  untuk  kategori 
jagung  merupakan  daerah 
perkebunan  jagung  yang  memiliki  tampilan  warna  kehijauan. 
Sedangkan  pada  kategori  badan  air  merupakan  daerah 
permukaan air seperti kolam, tambak, sungai, dsb. 

Gambar 3. Sebaran data tiap fitur terpilih (kategori band) 

Gambar 4. Sebaran data tiap fitur terpilih (kategori indeks vegetasi) 

 5 / 8 

 
 
 
 
 
 
 
 
 
 
 
Sedangkan  Secara  umum  gambaran  sebaran  data  yang 
dihasilkan oleh indeks vegetasi pada gambar 4 memiliki pola 
sebaran  yang  sama.  Hal  tersebut  dikarenakan  fungsionalitas 
dari  indeks  adalah  untuk  melihat  tingkat  kehijauan  dan 
mendeteksi  objek  vegetasi.  Yang  membedakan  dari  indeks 
tersebut adalah rentang nilai dan juga variasi warna pada objek 
yang  terbentuk.  Pada  indeks  MSAVI2  memiliki  rentang 
tertinggi  dengan  nilai  mulai  dari  -1,148  hingga  0,913. 
Sedangkan rentang terendah diperoleh indeks EVI dengan nilai 
mulai dari -0,095 hingga 0,809. 

B.  Klasifikasi Tutupan Lahan 

Setelah melihat analisis deskriptif dari persebaran nilai band 
untuk  tiap  kategori  tutupan  lahan,  maka  dapat  dilakukan 
klasifikasi supervised dengan algoritma machine learning yang 
umum  digunakan  dalam  melakukan  klasifikasi  tutupan  lahan. 
Algoritma  yang  dipakai  antara 
(a)CART, 
(b)Random Forest, (c)SVM, dan (d)Naïve Bayes. Adapun hasil 
klasifikasi dari keempat algoritma dapat dilihat pada gambar 5 
yang  mana  warna  merah  menunjukan  pemukiman,  hijau  tua 
untuk hutan, coklat untuk tanah, biru tosca untuk sawah, kuning 
untuk perkebunan jagung, dan biru tua untuk badan air. 

lain  adalah 

Gambar 5. Hasil klasifikasi tutupan lahan 

Dari gambar 5 juga terlihat daerah perkebunan jagung pada 
setiap  algoritma  memiliki  area  yang  berbeda-beda.  Pada 
algoritma  (c)SVM  daerah  perkebunan  jagung  lebih  dominan 
daripada  algoritma  yang  lainnya.  Selain  itu  untuk  mengatasi 
adanya overfitting terhadap data training yang digunakan maka 
dilakukan  validasi  berupa  Stratified  Shuffle  Split  Cross 
Validation  dengan  n-split  bernilai  10  dan  iterasi  sebanyak  30 
kali. Berikut pada tabel 5 adalah hasil overall accuracy dan F-
1 Score tiap algoritma yang digunakan. 

TABEL V 
TABEL HASIL KLASIFIKASI TUTUPAN LAHAN 

No. 
1 
2 
3 
4 

Algoritma 
CART 
Random Forest 
SVM 
Naïve Bayes 

Overall Accuracy 
0.892 
0.910 
0.941 
0.821 

F-1 Score 
0.892 
0.910 
0.941 
0.806 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Dari hasil klasifikasi tersebut dipilih klasifikasi dengan nilai  
overall accuracy terbaik yaitu dengan algoritma SVM (0,941). 
Model  klasifikasi  yang  berbentuk  peta  tutupan  lahan  tersebut 
selanjutnya dijadikan dasar dalam penghitungan estimasi luas 
panen dan produksi jagung. 

C.  Estimasi Luas Panen 

Setelah  didapatkan  model 

terbaik  maka  dilakukan 
penghitungan  jumlah  piksel  dan  luas  area  pada  tiap  kategori 
tutupan lahan dari hasil klasifikasi menggunakan model terbaik 
tersebut. Dengan ukuran piksel yang bernilai 100 meter persegi 
untuk tiap pikselnya maka dapat dilakukan konversi satuan luas 
nilai  piksel  menjadi  hektar  seperti  yang  tampak  pada  tabel  6 
berikut. 

Kelas 

Tutupan Lahan 

TABEL VI 
TABEL LUAS AREA TUTUPAN LAHAN  
Jumlah 
Piksel 
5.195.135 
4.130.988 
638.372 
5.314.140 
4.050.491 
713.965 

Luas Area 
(ha)a 
5.1951,35 
41.309,88 
6.383,2 
53.141,4 
40.504,91 
7.139,65 

Pemukiman 
Hutan 
Tanah 
Sawah 
Perkebunan Jagung 
Badan Air 
aSatu piksel bernilai 100 meter persegi 

1 
2 
3 
4 
5 
6 

Persentase 
(%) 
25,92% 
20,61% 
3,18% 
26,51% 
20,21% 
3,56% 

luas  area  panen 

Kategori  perkebunan  jagung  memiliki  estimasi  luas  area 
sebesar  40.504,91  ha  dengan  persentase  luas  sebesar  20,21% 
terhadap luas Kabupaten Tuban. Luas area tersebut kemudian 
dibandingkan  dengan  rata-rata 
jagung 
Kabupaten  Tuban  yang  dihitung  oleh  BPS  pada  tahun  2018 
dengan asumsi dalam satu tahun terjadi 3 kali panen sehingga 
luas area dalam satu kali panen menjadi 37.544,67 ha. Asumsi 
tersebut juga didukung dengan artikel berita [13] yang memuat 
data  satu  kali  panen  jagung  di  Kabupaten  Tuban  yang  tidak 
berbeda  jauh  yaitu  sebesar  34.849  ha.  Dengan  menghitung 
selisih estimasi luas panen jagung hasil klasifikasi dengan data 
rata-rata luas panen jagung menurut BPS dapat dikatakan hasil 
estimasi  cenderung  overestimate  dengan  selisih  3.162,57  ha 
dan persentase eror sebesar 8,47%. 

D.  Estimasi Produksi Jagung 

jagung 

Pada  proses  pengolahan  estimasi  produksi  jagung  ini 
dilakukan pengujian model regresi sederhana yang didapatkan 
dari  hasil  analisis  regresi  yang  mana  variabel  dependen  (Y) 
tiap  kecamatan, 
merupakan  nilai  produktivitas 
sedangkan  variabel  independen  (X)  merupakan  rata-rata  nilai 
indeks vegetasi untuk kategori tutupan lahan jagung pada tiap 
kecamatan.  Pada  variabel  Y  yaitu  nilai  produktivitas  jagung 
dilakukan konversi satuan nilai menjadi ton/piksel. Hal tersebut 
dilakukan untuk menyesuaikan model persamaan regresi yang 
terbentuk  dengan  kalkulasi  pada  raster  daerah  perkebunan 
jagung  yang  terbentuk  dari  berbagai  piksel  didalamnya. 
Sedangkan  variabel  X  yaitu  rata-rata  nilai  indeks  vegetasi 
terbentuk dari menjumlahkan total nilai indeks vegetasi untuk 
lahan  perkebunan 
tiap  kecamatannya  dan 
membaginya  dengan  jumlah  piksel  perkebunan  jagung  pada 

jagung  pada 

 6 / 8 

 
 
 
 
 
 
 
 
 
tiap  kecamatan  tersebut  juga,  sehingga  diperoleh  nilai  akhir 
yaitu  rata-rata  nilai  indeks  vegetasi  untuk  kategori  tutupan 
lahan jagung pada tiap kecamatan. 

Gambar 6. Pola sebaran nilai produktivitas jagung dengan indeks vegetasi 

Pada  gambar  6  terlihat  pola  sebaran  plot  nilai  yang 
dihasilkan  dari  variabel  X  dan  Y  yang  digunakan.  Pada  pola 
sebaran tersebut terlihat bahwa titik-titik tidak menggambarkan 
adanya indikasi sebaran yang linear. Selain itu dari uji linearitas 
RESET  terlihat  pada  tabel  7  bahwa  setiap  model  indeks 
vegetasi  yang  terbentuk  memiliki  nilai  F  hitung  yang  kurang 
dari F tabel serta nilai p-value yang lebih dari alfa. Hal tersebut 
menandakan bahwa tidak ada model linear yang terbentuk dari 
setiap  indeks  vegetasi  yang  digunakan.  Berdasarkan  asumsi 
linearitas  yang  menunjukan  bahwa  setiap  model  merupakan 
model  non-linear  maka  dapat  dilanjutkan  dengan  analisis 
regresi non-linear. 

TABEL VII 
TABEL UJI LINEARITAS RESET 
Keputusan 
p-value 

F 
Hitung 

No 

Indeks 
Vegetasi 

1 

2 

3 

4 

SAVI 

2,9161 

0,08321 

Tolak H0 

MSAVI2 

2,7052 

0,09727 

Tolak H0 

EVI 

3,5778 

0,05196 

Tolak H0 

NDVI 

2,9156 

0.08324 

Tolak H0 

*F tabel (3,6337) dan Alfa (0,05) 

Linearitas 

Model tidak 
linear 
Model tidak 
linear 
Model tidak 
linear 
Model tidak 
linear 

Sedangkan  pada  tabel  8  ditampilkan  beberapa  model 
persamaan  regresi  non-linear  polinomial  derajat  3  yang 
dihasilkan serta nilai R2, Residual Standard Error (RSE), dan 
Adjusted  R2.  Model  tersebut  akan  digunakan  sebagai  dasar 
penghitungan untuk menghasilkan nilai estimasi total produksi 
jagung. 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

TABEL VIII 
TABEL ANALISIS ESTIMASI PRODUKSI JAGUNG 
No 

Input (X) 

Adj R2 

RMSE 

R2 

1 

2 

3 

4 

SAVI 

0,2694 

0,00252 

0,1324 

MSAVI2 

0,2560 

0,00255 

0,1165 

EVI 

0,3091 

0,00245 

0,1796 

NDVI 

0,2694 

0,00252 

0,1324 

Dari  tabel  8  tersebut  terlihat  bahwa  model  dengan  indeks 
vegetasi EVI memiliki nilai tertinggi sebesar 0,3091 dan RSE 
terendah  sebesar  0,00245.  Selain  itu  dari  segi  nilai  variabel 
yang terbentuk, hanya indeks vegetasi EVI yang secara parsial 
signifikan  dalam  taraf  uji  5%.  Namun  sayangnya  secara 
simultan  baik  pada  taraf  5%  maupun  10%  tidak  signifikan. 
Persamaan  regresi  yang  dihasilkan  dari  model  EVI  tersebut 
adalah sebagai berikut. 

𝑌 = 5.125 − 30.170𝑋 + 59.648𝑋2 − 39,163𝑋3        (2) 

Selanjutnya  akan  dilakukan  kalkulasi  raster  menggunakan 
rumus regresi tersebut pada raster indeks vegetasi EVI daerah 
perkebunan jagung. Dari kalkulasi tersebut dihasilkan estimasi 
total  produksi  sebesar  265.149,225  ton  jagung.  Nilai  tersebut 
selanjutnya  dibandingkan  dengan  rata-rata  produksi  jagung 
Kabupaten  Tuban  yang  dihitung  oleh  BPS  pada  tahun  2018 
dengan asumsi dalam satu tahun terjadi 3 kali panen sehingga 
total  produksi 
jagung  dalam  satu  kali  panen  menjadi 
209.569,67 ton. Hasil perbandingan kedua data tersebut dapat 
dikatakan  bahwa  estimasi  produksi 
jagung  cenderung 
overestimate  terhadap  data  resmi  yang  dihitung  oleh  BPS 
dengan  selisih  sebesar  55.579,56  ton  dengan  persentase  eror 
sebesar 26,52%. 

VII. 

PENUTUP 

Berdasarkan hasil dan pembahasan yang tertulis di dapatkan 
model klasifikasi terbaik untuk klasifikasi tutupan lahan yaitu 
menggunakan  algoritma  Support  Vector  Machine  (SVM) 
dengan  overall  accuracy  sebesar  0,941.  Dari  model  tersebut 
juga dihitung luas area tiap kategori dengan menghitung jumlah 
piksel dan dikonversi ke dalam hektar. Selanjutnya didapatkan 
estimasi  luas  area  panen  jagung  Kabupaten  Tuban  sebesar 
44.504,91 ha yang mana jika dilakukan perbandingan dengan 
data rata-rata luas panen BPS terindikasi overestimate dengan 
persentase eror 8,49%. Sedangkan pada penghitungan estimasi 
produksi  jagung  didapatkan  model  regresi  menggunakan 
indeks  EVI  dengan  nilai  R2  sebesar  0,309  dan  RSE  sebesar 
0,00245.  Dari  model  tersebut  diambil  persamaan  regresinya 
dan  dilakukan  kalkulasi  pada  raster  perkebunan  jagung 
sehingga  didapatkan  hasil  estimasi  produksi  tanaman  jagung 
sebesar  265.149,225  ton.  Adapun  nilai  estimasi  produksi 
tersebut  ketika  dibandingkan  dengan  data  rata-rata  produksi 
indikasi 
satu  kali  panen  BPS  menghasilkan 
jagung 
overestimate dengan persentase eror sebesar 26,52%. 

Perbedaan nilai tersebut dapat terjadi selain karena eror yang 
dihasilkan dalam proses klasifikasi maupun model regresi yang 

 7 / 8 

 
 
 
 
 
 
 
 
 
agar  memperhatikan 

dipakai  juga  karena  adanya  asumsi  yang  digunakan  dalam 
penghitungan  rata-rata  luas  panen  maupun  produksi  tanaman 
jagung  yang  mana  tidak  memperhatikan  akurasi  waktu 
pencatatan dan waktu tanam jagung yang tidak serentak pada 
wilayah kajian. Hal ini bisa menjadi evaluasi untuk penelitian 
selanjutnya 
akurasi  waktu  dan 
memperoleh data yang lebih aktual sehingga nilai estimasi yang 
diperoleh  bisa  lebih  representatif  terhadap  keadaan  yang 
sebenarnya.  Selain  itu  pada  penelitian  ini  juga  terdapat 
keterbatasan  dari  model  regresi  yang  dihasilkan  yang  mana 
hanya  menggunakan  satu  variabel  saja  tanpa  memperhatikan 
variabel  pendukung  lainnya.  Hal  tersebut  mengakibatkan 
model  regresi  memiliki  nilai  korelasi  dan  R2  yang  rendah. 
Kedepannya dapat dilakukan analisis estimasi produksi jagung 
lainnya  dengan  memperhatikan  beberapa  variabel  pendukung 
agar dapat meningkatkan akurasi data itu sendiri. 

[1] 

[2] 

[3] 

[4] 

[5] 

[6] 

[7] 

[8] 

[9] 

[10] 

[11] 

[12] 

[13] 

DAFTAR PUSTAKA 

N. R. Iriany, M. H. G. Yasin, and  a. M. Takdir, “Asal, Sejarah, 
Evolusi, dan Taksonomi Tanaman Jagung,” Jagung Tek. Produksi 
dan Pengemb., pp. 1–15, 2009. 
X. Jin, J. Ma, Z. Wen, and K. Song, “Estimation of maize residue 
cover using Landsat-8 OLI image spectral information and textural 
features,” Remote Sens., 2015, doi: 10.3390/rs71114559. 
K. Ruslan, “Memperbaiki Data Pangan Indonesia Lewat Metode 
Kerangka Sampel Area,” no. September, 2019. 
A. Ekadinata, S. Dewi, D. P. Hadi, D. K. Nugroho, and F. Johana, 
Sistem Informasi Geografis Untuk Pengelolaan Bentang Lahan 
Berbasis Sumber Daya Alam. Buku 1: Sistem Informasi Geografis 
dan Penginderaan Jauh Menggunakan ILWIS Open Source. 2015. 
L. M. Irsan, S. H. Murti, and P. Widayani, “ESTIMASI 
PRODUKSI JAGUNG (Zea Mays L.) DENGAN 
MENGGUNAKAN CITRA SENTINEL 2A DI SEBAGIAN 
WILAYAH KABUPATEN JENEPONTO PROVINSI SULAWESI 
SELATAN,” J. Teknosains, vol. 8, no. 2, p. 93, 2019, doi: 
10.22146/teknosains.36885. 
D. Djaenudin, “Perkembangan penelitian sumber daya lahan dan 
kontribusinya untuk mengatasi kebutuhan lahan pertanian di 
indonesia,” J. Litbang Pertan., vol. 27, no. 98, pp. 137–145, 2008, 
[Online]. Available: 
http://pustaka.litbang.pertanian.go.id/publikasi/p3274084.pdf. 
Emma, “Sentinel-2.” 2017, Accessed: Nov. 03, 2020. [Online]. 
Available: https://bentangalam-
hutantropis.fkt.ugm.ac.id/2017/12/04/sentinel-2/. 
H. Fang, S. Liang, and G. Hoogenboom, “Integration of MODIS 
LAI and vegetation index products with the CSM-CERES-Maize 
model for corn yield estimation,” Int. J. Remote Sens., vol. 32, no. 
4, pp. 1039–1065, 2011, doi: 10.1080/01431160903505310. 
A. W. Wijayanto, D. W. Triscowati, and A. H. Marsuhandi, “Maize 
field area detection in East Java, Indonesia: An integrated 
multispectral remote sensing and machine learning approach,” 
2020, doi: 10.1109/ICITEE49829.2020.9271683. 
A. V. M. Ines, N. N. Das, J. W. Hansen, and E. G. Njoku, 
“Assimilation of remotely sensed soil moisture and vegetation with 
a crop simulation model for maize yield prediction,” Remote Sens. 
Environ., vol. 138, pp. 149–164, 2013, doi: 
10.1016/j.rse.2013.07.018. 
A. Kern et al., “Statistical modelling of crop yield in Central Europe 
using climate data and remote sensing vegetation indices,” Agric. 
For. Meteorol., vol. 260–261, no. June, pp. 300–320, 2018, doi: 
10.1016/j.agrformet.2018.06.009. 
Y. M. Fernandez-Ordonez and J. Soria-Ruiz, “Maize crop yield 
estimation with remote sensing and empirical models,” Int. Geosci. 
Remote Sens. Symp., vol. 2017-July, no. July, pp. 3035–3038, 2017, 
doi: 10.1109/IGARSS.2017.8127638. 
Kumparan, “Di Tengah Wabah Corona, Jagung dan Padi di Tuban 
Siap Panen | kumparan.com.” 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

https://kumparan.com/kumparanbisnis/di-tengah-wabah-corona-
jagung-dan-padi-di-tuban-siap-panen-1t7S6WTgCub/full (accessed 
Jun. 16, 2021). 

 8 / 8 

 
 
 
 
"
16.9389,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Klasifikasi Flaming pada Kasus Cyberbullying di
Twitter Menggunakan Algoritma Naïve Bayes
Classifier

Rivaldi (16.9389, 4SI1)

Dosen Pembimbing: Farid Ridho, S.S.T, M.T.

sosial

sebagai

Ringkasan— Media

salah satu sarana
komunikasi saat ini telah menjadi suatu kebutuhan untuk saling
berkomunikasi di zaman sekarang. Berbagai platform tersedia
untuk berkomunikasi dalam media sosial, salah satunya adalah
Twitter. Penggunaan Twitter yang beragam oleh berbagai
pengguna tidak diikuti oleh etika-etika yang baik dalam
bersosial di dunia maya, hal ini dapat dilihat ketika semakin
maraknya cyberbullying. Salah satu bentuk yang cyberbullying
yang paling sering ditemukan dalam platform twitter adalah
flaming. Penggunaan kata-kata kasar dalam interaksi sering
ditemukan sehari-hari dalam twitter. Dengan maraknya
ditemukan bentuk-bentuk flaming dalam platform twitter,
ingin membangun sebuah model klasifikasi dengan
peneliti
menggunakan
dalam
algoritma Naïve Bayes Classifier
mengklasifikasikan suatu tweet yang mengandung unsur flaming.
Tahapan yang dilakukan dalam penelitian ini meliputi Data
Collection, Pre-Processing, Pembentukan Model Klasifikasi, dan
ialah model
Evaluasi. Kesimpulan yang dapat diketahui
klasifikasi dengan menggunakan algoritma Naïve Bayes
Classifier dalam mengklasifikasikan tweet yang mengandung
flaming mendapatkan nilai akurasi sebesar 56,2% dengan
metode evaluasi K-Fold Cross Validation. Selain itu perbandingan
yang dilakukan dengan algoritma lain dari sisi akurasi serta
runtime menghasilkan kesimpulan bahwa Naive Bayes Classifier
lebih baik daripada algoritma lain yang menjadi perbandingan.

Kata Kunci— Twitter, Cyberbullying, Flaming, Naïve Bayes

Classifier.

I. LATAR BELAKANG

Dalam sebagian dekade terakhir, orang sudah memakai
Internet serta media sosial selaku bagian dari komunikasi tiap
hari mereka selaku media buat bertukar data. Dengan
memakai media sosial, pengguna bisa membuat profil
pribadinya buat dilihat pengguna lain baik secara individu
sosial, pengguna bisa
maupun publik. Dengan media
berhubungan satu sama lain.

Penggunaan media sosial sangat berkembang mengingat
akses mudah dan murah ke Internet pada satu dekade terakhir.
Orang-orang menggunakan media sosial sebagai bagian dari
media komunikasi baik dari sekunder maupun primer untuk
saling bertukar informasi kepada orang-orang yang mereka
kenal maupun yang tidak mereka kenal. Dengan memakai
media sosial, pengguna dapat membuat profil pribadinya
untuk dilihat oleh pengguna lain baik secara individu maupun
secara publik.

Salah satu media sosial yang populer di Indonesia ialah
Twitter. Twitter sendiri adalah salah satu layanan media sosial

dan

penggunanya

bisa menulis

dimana
berinteraksi
menggunakan pesan teks dengan pengguna lain yang biasanya
tweet. Pada dasarnya suatu tweet dapat
disebut sebagai
mengandung teks hingga 140 karakter, penggunaan yang
sederhana dan cenderung singkat
seperti menggunakan
layanan SMS dari twitter itu sendiri yang menjadi daya tarik
mengapa twitter cukup populer untuk digunakan sebagai
media sosial di Indonesia.

Sebagai media komunikasi online tempat orang-orang
berinteraksi satu sama lain dalam jarak pendek atau jauh,
media sosial menyampaikan informasi dalam teks, gambar,
dan video untuk mengekspresikan dan mengkomunikasikan
ide-ide penggunanya. Namun,
interaksi sosial di antara
orang-orang di media sosial dapat mengarah ke arah yang
berbeda, baik untuk meningkatkan hubungan di antara mereka
atau perbedaan ide, pemikiran, dan pendapat. Ekspresi ide dan
opini yang berbeda dapat memberikan hasil yang positif. Hal
tersebut memungkinkan orang menjadi
lebih menerima,
menghormati, dan toleran terhadap orang lain sambil
memperkaya pengetahuan, mempelajari keragaman, dan
pemahaman dari perspektif yang berbeda. Namun, jika orang
tidak toleran dan tidak dapat menghormati perbedaan orang
lain, ini dapat menimbulkan masalah sosial seperti bullying,
intoleransi, pelecehan, dll. Media sosial juga dapat digunakan
oleh orang-orang yang sering secara anonim mempublikasikan
komentar yang penuh kebencian atau diskriminatif dan
mengintimidasi dan melecehkan korban yang ditargetkan [1].
Karenanya, penyalahgunaan media sosial untuk merugikan
orang lain ini perlu ditanggapi dengan serius.

Cyberbullying dipahami sebagai tindakan yang disengaja
biasanya dilakukan berulang kali terhadap korban yang tidak
berdaya oleh individu atau kelompok dengan menggunakan
teknologi media komunikasi. Selain itu, banyak cyberbullies
(predator) percaya bahwa tindakan tercela semacam itu
menghibur,
tidak menyadari bahwa perilaku mereka pada
akhirnya dapat mempengaruhi para korban. Cyberbullying
juga dapat menimbulkan kerusakan yang signifikan pada
korbannya, terutama remaja yang menjadi sasaran [2].Secara
umum Cyberbullying merupakan suatu bentuk bullying yang
memakai teknologi media komunikasi semacam ponsel pintar,
e-mail, media sosial, serta website yang terhubung ke jaringan
internet dimana dalam fiturnya dapat memposting ataupun
mengirim pesan-pesan yang mengusik, menghina, mengecam,
serta mempermalukan yang dimaksudkan untuk menyakiti

1 / 8

sendiri

Indonesia

secara keseluruhan,

orang atau pengguna yang dikenal maupun tidak dikenal
lainnya.
Di

informasi
Indonesia susah ditemui.
permasalahan cyberbullying di
Tetapi
informasi dari Polda Metro Jaya menyebutkan
sedikitnya 25 kasus cyberbullying dilaporkan setiap harinya.
Tidak hanya itu, data Komisi Perlindungan Anak Indonesia
tahun 2018 menyebutkan jumlah anak korban bullying
mencapai 22,4% [3]. Tingginya angka tersebut dipicu oleh
konsumsi
internet pada anak-anak. Pelaporan yang masih
kurang mengenai kasus Cyberbullying mengisyaratkan masih
sosial yang kurang menaruh
banyak pengguna media
perhatian pada cyberbullying yang sebenarnya banyak kita
temukan setiap harinya di media sosial,
tidak terkecuali
twitter. Selain itu, para pelaku atau pengirim pesan yang
mengandung cyberbullying di media sosial cenderung tidak
menyadari akan perbuatan mereka. Kebebasan bermedia sosial
yang tidak diiringi oleh etika yang baik dalam bermedia sosial
membuat rasa bersalah dari para pelaku cyberbullying tidak
muncul ketika para korban menderita secara mental akibat
perbuatan mereka.

Berbagai dampak negatif atau potensi risiko mengalami
penyakit mental bagi korban cyberbullying antara lain depresi,
gangguan kecemasan, menarik diri dari lingkungan sosial,
penurunan kualitas akademis bagi korban yang masih pelajar,
penggunaan obat-obatan terlarang, bahkan keinginan untuk
bunuh diri.

flaming,

harassment

(pelecehan),

Cyberbullying sendiri memiliki berbagai macam bentuk
impersonation
yaitu
(penyamaran), denigration, exclusion (pengucilan), Outing
dan
trickery, dan cyberstalking [4]. Flaming adalah
penggunaan bahasa yang cenderung kasar dalam interaksi
kedua individu, dimana hal ini menimbulkan situasi memanas
di antara keduanya, hal ini juga dapat meliputi kata-kata yang
tidak sopan bahkan dapat mengarah ke penghinaan dan juga
ancaman. Harassment
adalah bentuk pelecehan yang
dimaksudkan untuk membuat malu individu yang menjadi
target. Denigration merupakan pencemaran nama baik
terhadap individu yang menjadi
target yang bertujuan
menyebarkan kebohongan mengenai korban. Exclusion
sebuah
dengan mengeluarkan individu dari
berkaitan
trickery
dan
dan mengucilkannya. Outing
kelompok
merupakan tindakan penyebaran informasi pribadi dari korban
yang bertujuan untuk mempermalukannya secara publik.
Cyberstalking merupakan tindakan penguntitan di dunia maya
yang mana biasanya melakukan pelacakan terhadap target
dengan bertujuan melecehkannya [5].

Di

twitter

sendiri banyak ditemukan bentuk-bentuk
cyberbullying di sepanjang waktu, salah satunya adalah
Flaming. Para pengguna twitter, khususnya di Indonesia,
yang
sangat mudah mengetik
mengandung kata-kata kasar didalamnya. Jika dilihat lebih
seksama, penggunaan kata-kata kasar
tersebut ada yang
mengarah ke dalam sebuah bentuk cyberbullying namun ada
juga yang hanya mengekspresikan rasa kesal, kagum, atau
curahan hati si pengguna itu sendiri.

atau mengirim tweet

Text mining merupakan suatu teknik dalam bidang
teknologi informasi dimana di dalamnya merupakan proses

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

menambang data berupa teks dari sebuah dokumen yang
nantinya akan dianalisa untuk dilakukan ekstraksi informasi
yang berguna untuk tujuan tertentu. Text mining dapat
memberikan solusi dari permasalahan seperti pemrosesan,
pengorganisasian dan menganalisis unstructured text dalam
jumlah besar
text mining
[6]. Dalam penggunaannya
mengkombinasikan beberapa teknik lain seperti Natural
Language Processing, Information Retrieval, Visualization,
Machine Learning, dan sebagainya untuk menghasilkan suatu
informasi yang berguna.

serta

Dalam Penelitian ini penggunaan text mining dibutuhkan
untuk menganalisa suatu tweet apakah mengandung Flaming
atau tidak. Karakteristik teks
tweet yang merupakan
unstructured text
akan menghasilkan data yang
berjumlah besar merupakan alasan teknik ini dilakukan.
tahapan pada
Dalam text mining
preprocessingnya yaitu Tokenizing yaitu proses pemisahan
teks menjadi
token-token, Filtering merupakan proses
penyaringan kata, Case Folding merupakan proses untuk
mengubah semua huruf besar dalam dokumen menjadi huruf
kecil, dan Analyzing merupakan proses untuk menentukan
hubungan antara kata-kata dengan dokumen yang sudah ada.

beberapa

terdapat

apakah mengandung unsur

Dalam menentukan suatu tweet mengandung unsur flaming
atau tidak dibutuhkan metode untuk mengklasifikasikannya.
Dalam penelitian ini akan dibahas mengenai klasifikasi sebuah
tweet
flaming atau tidak
didalamnya dengan menggunakan algoritma Naive Bayes.
Naive Bayes merupakan metode klasifikasi statistik yang
menggunakan teorema Bayes [7]. Metode naive bayes dapat
mengklasifikasikan dataset yang berukuran besar sehingga
cocok untuk mengklasifikasikan ribuan hingga ratusan ribu
tweet hasil dari pengumpulan data.

II. TUJUAN PENELITIAN
Berdasarkan latar belakang yang sudah dipaparkan
sebelumnya,
tujuan penelitian yang ingin dicapai adalah
membuat model klasifikasi dengan menggunakan algoritma
Naïve Bayes Classifier untuk mengklasifikasikan tweet
Bahasa Indonesia apakah mengandung unsur flaming atau
tidak, serta mengevaluasi performa model klasifikasi dalam
mengklasifikasikan tweet tersebut.

III. PENELITIAN TERKAIT

Penelitian [8] menunjukkan penggunaan metode klasifikasi
Random Forest
cyberbullying.
untuk mengidentifikasi
Penggunaan Random Forest dalam penelitian ini di optimasi
dengan pertimbangan dari rules yang umum dari penelitian
sebelumnya dengan beberapa penyesuaian yang dibutuhkan.

Dalam penelitian lainya menganalisis kata-kata bullying
bahasa Indonesia di Twitter untuk menemukan pola bullying
juga membahas cara mining
di
kata-kata bullying bahasa Indonesia di Twitter dengan
menggunakan teknik text mining [2].

Indonesia. Penelitian ini

Alhamda dalam penelitiannya membuat sebuah sistem yang
dapat mengklasifikasi komentar apakah mengandung elemen
dari cyberbullying atau tidak dalam platform media sosial
Instagram. Hasil dari klasifikasi akan digunakan untuk

2 / 8

komentar

cyberbullying. Algoritma

mendeteksi
yang
digunakan untuk klasifikasi adalah Naïve Bayes Classifier [9].
Selain pendeteksian konten, Nurrahmi dalam penelitiannya
mampu mendeteksi aktor atau pelaku dari cyberbullying
dengan pendekatan Text Classification dan analisis kredibilitas
dari pengguna. Data dikumpulkan dari twitter, dan karena data
tidak memiliki label, dalam penelitiannya dibuatlah web-based
labelling
tweet menjadi
untuk mengklasifikasi
cyberbullying dan non-cyberbullying tweet [10].

tool

IV. METODE PENELITIAN

A. Pengumpulan data

Data yang digunakan dalam penelitian ini diperoleh dengan
menggunakan metode scraping pada situs www.twitter.com
dengan menggunakan library twint pada bahasa pemrograman
python. Penggunaan library pada twint pada metode scraping
dalam penelitian ini didasari oleh keterbatasan penggunaan
API Twitter dalam mengambil data tweet. Library twint yang
tidak perlu menggunakan akses API Twitter mampu
melakukan scraping data tanpa batasan tertentu sehingga data
yang diambil dapat berjumlah banyak.

Proses scraping dilakukan dengan melakukan pencarian
tweet berdasarkan kata kunci kata-kata kasar Bahasa Indonesia
yang dalam penelitian sebelumnya digunakan untuk mencari
pola bullying di Twitter. Kata kunci yang digunakan untuk
proses scraping dapat dilihat pada Tabel 1.

TABEL I

Kata-kata kasar Bahasa Indonesia

Klasifikasi
Berkaitan dengan binatang

Berkaitan
psikologi

dengan

kebodohan

dan

Berkaitan dengan cacat

Umum

Kata-kata kasar
-B*ngsat
-A*jing
-A*u
-B*bi
-M*nyet
-K*nyuk
-G*blok
-Id*ot
-G*blek
-G*la
-T*lol
-S*rap
-Ud*k
-K*mpungan
-B*ta
-B*dek
-Jel*k
-Set*n
-Ibl*s
-Kep*rat
-Gemb*l
-Br*ngsek

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

-S*mpret
-Baj*ngan

Pencarian berdasarkan kata kunci

tersebut dilakukan
dalam rentang waktu antara 1 Desember 2020 hingga 31
Desember 2020 kemudian data hasil scraping disimpan
dalam format file csv.

B. Filtrasi Data

proses

Setelah

scraping

melakukan

dengan
menggunakan library twint pada python, terdapat data yang
tidak sesuai atau tidak diinginkan untuk selanjutnya
dilakukan analisis sehingga harus dilakukan filtrasi dari
data tweet yang berhasil dikumpulkan. Filtrasi data
bertujuan untuk menghapus atau menghilangkan data yang
tidak sesuai untuk dilakukan proses selanjutnya. Dalam
proses ini dilakukan penghapusan data tweet yang duplikat,
tweet yang bukan berbahasa Indonesia, serta tweet yang
tidak mengandung kata kasar di dalamnya.

C. Manual Labelling

Setelah melakukan proses filtrasi, akan dilakukan proses
labelling untuk selanjutnya dilakukan proses
manual
secara manual dengan
pre-processing. Data dilabeli
pandangan subjektif dari peneliti. Manual
labelling
dilakukan untuk mengubah data unsupervised menjadi
supervised. Dalam penelitian ini, data tweet akan dilabeli
menjadi 2 kategori. ‘NB’ untuk tweet yang tidak memiliki
unsur flaming di dalamnya, ‘B’ untuk tweet yang memiliki
unsur flaming di dalamnya.

Untuk menilai suatu tweet apakah termasuk ke dalam
tweet yang memiliki unsur flaming atau tidak, peneliti
menentukan beberapa kriteria yang dapat digunakan pada
tweet berbahasa Indonesia :

1. Kombinasi

kata

pertama,
ganti
kata/emosi negatif, dan kata ganti orang kedua
(Contoh: Aku benci kamu)

orang

2. Kombinasi kata ganti orang kedua dengan

kata-kata kasar (Contoh: Kamu anjing)

3. Kombinasi

kata

pertama,
ganti
kata/emosi negatif, dan kata ganti orang ketiga
(Contoh: Aku benci dia)

orang

4. Kombinasi kata ganti orang ketiga dengan

kata-kata kasar (Contoh: Dia anjing)

Dari kriteria tersebut, peneliti menentukan suatu tweet
mengandung unsur
flaming atau tidak dimana secara
keseluruhan tweet harus merujuk kepada suatu individu
bukan terhadap objek atau benda mati.

D. Preprocessing

Data

yang

telah dilabeli

akan dilakukan tahap
preprocessing sebelum masuk ke dalam tahap klasifikasi.
Preprocessing dilakukan untuk mendapatkan data bersih
sehingga proses analisis dapat dilakukan lebih mudah.
Selain itu preprocessing bertujuan untuk mengubah data
yang dikumpulkan yang sebelumnya tidak terstruktur
dikarenakan merupakan bahasa alami menjadi data yang

3 / 8

terstruktur. Preprocessing yang dilakukan dalam penelitian
antara lain:

1.

2.

3.

4.

5.

contoh,

kata mempelajari,

Case folding, merupakan proses untuk mengubah
semua huruf dalam dokumen teks menjadi huruf
kecil.
Cleaning, merupakan proses untuk membersihkan
dokumen dari kata atau karakter yang tidak
diperlukan untuk mengurangi noise pada proses
klasifikasi. Seperti penghapusan url, username,
hashtags.
Stemming,
stemming merupakan suatu proses
mengurangi atau menghilangkan infleksi kata
(atau turunan kata) menjadi bentuk kata dasarnya.
Sebagai
belajar,
pengajar, akan distem ke root wordnya yaitu
“ajar”. Dalam penelitian ini digunakan library
sastrawi pada python untuk melakukan proses
stemming pada teks berbahasa indonesia.
Stopword removal, merupakan proses untuk
menghilangkan kata-kata yang tidak memiliki
kontribusi untuk dilakukan ekstraksi
informasi
pada isi dokumen. Kata-kata yang termasuk dalam
stopword untuk selanjutnya dihilangkan adalah
kata-kata yang tidak penting dalam proses
‘tetapi’,
klasifikasi seperti kata:
‘dengan’, ‘di’, dan sebagainya.
Tokenization, merupakan proses pemisahan teks
menjadi potongan-potongan yang disebut sebagai
token untuk kemudian dianalisis.

‘yang’,

‘ke’,

E. Tahap Klasifikasi

Pada tahap klasifikasi, dilakukan dengan menggunakan
algoritma Naive Bayes Classifier. Metode klasifikasi
dengan menggunakan Naive Bayes adalah algoritma yang
menggunakan teorema Bayes mengasumsikan semua atribut
independen atau tidak saling ketergantungan yang diberikan
oleh nilai pada variabel kelas. Hasil dari manual labelling
akan dibagi menjadi dua, yaitu data training dan testing.
Proporsi pembagian data dilakukan sebanyak 70% untuk
data training dan 30% untuk data testing. Proses pembagian
data dilakukan secara acak sampai memenuhi proporsi
pembagian data.

Setelah proses pembagian data, data training dilakukan
metode klasifikasi dengan Naive Bayes Classifier dengan
menggunakan bantuan library sklearn pada python. Dari
proses klasifikasi menggunakan data training, model akan
disimpan untuk selanjutnya digunakan klasifikasi pada data
testing. Proses selanjutnya adalah mengklasifikasi data
testing dengan menggunakan model dari klasifikasi data
training sebelumnya. Setelah itu hasil klasifikasi data
testing akan dinilai performanya dalam tahap evaluasi.

F. Evaluasi

Dalam tahap ini dilakukan evaluasi bagaimana performa
dan klasifikasi dengan
dalam proses
menggunakan algoritma Naive Bayes Classifier yang telah
dilakukan. Untuk mengevaluasi kinerja classifier dalam

pembelajaran

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

penelitian dilakukan dengan menggunakan cross validation.
Hasil evaluasi dari pengujian set data individu kemudian
dihitung rata-ratanya untuk mendapatkan hasil akurasi
keseluruhan yang secara statistik lebih dapat diandalkan
daripada hasil
tunggal yang dikumpulkan dari satu set.
Dalam penelitian ini dilakukan metode K-Cross Validation
sebanyak 10 (K=10).

Adapun tahapan dalam cross validation dalam penelitian

ini adalah sebagai berikut:

Shuffle dataset secara acak

1.
2. Pisahkan dataset menjadi 10 bagian
3. Untuk setiap K bagian:

a. Bagi menjadi set data training dan data

testing

b. Buat model dari set data training dan
lakukan evaluasi dengan menggunakan set
data testing

c. Hitung nilai evaluasi dan singkirkan model
4. Rangkum evaluasi model menggunakan rata-rata

skor evaluasi model.

Dari setiap K bagian dataset dilakukan evaluasi dengan
bantuan confusion matrix sehingga dapat mengetahui nilai
performanya seperti presisi, recall, f1-score, dan akurasi.
Dari proses K-Cross Validation ini, selain dapat mengetahui
rata-rata keseluruhan dari model klasifikasi, metode ini juga
dapat diketahui nilai akurasi tertinggi dari setiap model
sehingga model
untuk
tersebut menjadi
menggunakan proses prediksi selanjutnya.

acuan

V. KERANGKA PIKIR

Gambar 1. Bagan Kerangka Pikir

Alur pemikiran dalam penelitian ini dapat dilihat pada
Gambar 1. Pemikiran penelitian dimulai dengan semakin

4 / 8

maraknya penggunaan media sosial terutama di Indonesia
yang dipakai oleh berbagai kalangan dari usia muda hingga
tua. Penggunaan media sosial yang masif ini sayangnya
tidak diikuti dengan penggunaan etika dalam bermedia
sosial sehingga menimbulkan beberapa masalah terkait
penggunaannya yang mengarah kepada hal negatif, salah
satunya adalah cyberbullying.

kasar

negatif

dalam interaksi
dalam twitter. Dampak

Banyak ditemukan kasus cyberbullying pada media
sosial, tidak terkecuali pada platform Twitter. Salah satu
bentuk yang cyberbullying yang paling sering ditemukan
adalah flaming. Penggunaan
dalam platform twitter
ditemukan
sering
kata-kata
sehari-hari
dari
cyberbullying mulai dari depresi, gangguan kecemasan,
hingga keinginan ingin bunuh diri pada korbannya
merupakan daya Tarik peneliti untuk melakukan penelitian.
Dengan maraknya ditemukan bentuk-bentuk flaming
dalam platform twitter, peneliti ingin membangun sebuah
model klasifikasi dengan menggunakan algoritma Naïve
Bayes Classifier dalam mengklasifikasikan suatu tweet
yang mengandung unsur
flaming. Dari model yang
dihasilkan, dilakukan evaluasi agar mengetahui nilai
akurasi model
tersebut sehingga menjadi acuan untuk
penelitian-penelitian yang berkaitan dengan cyberbullying
dengan menggunakan algoritma Naïve Bayes Classifier.

VI. HASIL DAN PEMBAHASAN

A. Pengumpulan Data

Dari proses scraping dengan menggunakan library twint,
data tweet yang terambil sebanyak 481648 tweet/baris. Data
yang dihasilkan dari proses scraping tidak hanya berupa
teks tweet saja melainkan atribut-tribut mengenai suatu
tweet juga terambil seperti : tweet id, date, username, user
id, mentions, language, dan sebagainya. Adapun contoh
data tweet yang dikumpulkan dapat dilihat pada Tabel 2
berikut.

Username
ad******13

lem********yy

ze****o

TABEL II

Kata-kata kasar Bahasa Indonesia
Tweet
yang b*ngsat disayang-sayang, yang
tulus malah disia-siakan haha (n)

tp ini
trus2an.. sampe gw yg selalu
mencoba positif ini capek.. a*jing gw jg
capek oi.. g cuma lu.. g usah pake
banding2in n lomba2 dlm ngesusahan
lah..  GW JUGA CAPEK B*NGSAT

nyalahin member

@nctzenbase Udh macam ordal aja kau
pula. Buat
ya,
boygrupmu sendirilah,
terus kau atur
sendiri. Kalo ko mo nyalahin, serahmu.
Ini kau ngasih makan jwoo enggak, ko
maki pula. Kan a*u

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

B. Filtrasi Data

Dari data mentah yang telah didapatkan dari hasil
scraping, ditemukan data tweet yang tidak sesuai dengan
yang diinginkan untuk dilanjutkan ke proses preprocessing
setelah
tersebut
menunjukkan bahwa ada kelemahan dalam proses scraping
dengan menggunakan library twint pada python. Berikut
beberapa data yang tidak sesuai untuk dilakukan proses
preprocessing:

data. Hal

eksplorasi

dilakukan

TABEL III
CONTOH DATA YANG DUPLIKAT

row/index Tweet
78

1252

kaum y/n lah kaum bim lah kaum solo stan lah
B*NGSAT KELEN SMUA A*JING
kaum y/n lah kaum bim lah kaum solo stan lah
B*NGSAT KELEN SMUA A*JING

Tabel III menunjukkan adanya data tweet yang duplikat
sehingga salah satu data tweet tersebut harus dihapus. Apabila
terdapat
lebih dari dua duplikat tweet yang terambil dari
proses scraping, maka tweet lainnya dihapus dan menyisakan
satu data tweet yang unik.

TABEL IV
CONTOH DATA YANG TIDAK BERBAHASA INDONESIA

username
h***_asu

Tweet
リキッドムースさん お母さんらのご要望で下に
敷いてほしいあったからケーキ敷いたり フル
グラ敷いたりしてみた｡
次はカレーやんね
……大丈夫か(　˙-˙　)

Tabel IV menunjukkan data tweet yang tidak berbahasa
indonesia. Data tweet tersebut terambil karena dalam proses
scraping, target pencarian berdasarkan kata kunci ternyata
tidak hanya pada teks tweet saja melainkan kata dari username
juga termasuk dalam variabel pencarian dalam scraping
menggunakan library twint.

TABEL V
CONTOH DATA YANG TIDAK MENGANDUNG KATA KASAR

Tweet
@akkiiss25 Tadii udah sarapann, jangan lupa sarapn jugaa !
Kucing jalanan yg suka mampir minta jatah preman, pas
matanya sakit parah dirawat adik. Aku lebih suka manggil
dia mbambang. Pas mudik liat dia sekali doang n dia sptnya
ga ngenalin, galak cuma mau dielus atas kpala. Besoknya
himðŸˆ
ga
grimis
https://t.co/eFLCBQ63H2

x. Miss

nongol

krn

Tabel V juga menunjukkan kelemahan dari scraping
menggunakan library twint. Proses pencarian berdasarkan kata
kunci ternyata menargetkan kata yang berada pada kata lain.
Misalnya, pada kata kunci ‘sarap’ yang terdapat pada kata
juga termasuk ke target pencarian, begitu juga
‘sarapan’
dengan kata kunci
‘udik’ pada kata ‘mudik’, dan lain
sebagainya. Selain itu, terdapat data tweet yang sama sekali
ini
tidak terdapat kata kunci dari proses scraping. Hal

5 / 8

disebabkan oleh seperti masalah sebelumnya, yaitu username
yang mengandung kata kunci juga terambil dalam proses
scraping dengan tweet berbahasa Indonesia.

C. Manual Labelling

Untuk keperluan proses klasifikasi yang menggunakan
algoritma naive bayes classifier yang merupakan supervised
learning, data tweet akan dilabeli secara manual dengan
subjektivitas dari peneliti. Tweet akan dikategorikan menjadi
dua kategori.
‘B’ untuk tweet yang mengandung unsur
flaming, dan ‘NB’ untuk tweet yang tidak mengandung unsur
flaming di dalamnya. Contoh pelabelan tweet terdapat pada
tabel berikut. Dalam proses pelabelan ini, peneliti menentukan
label dari setiap tweet berdasarkan pandangannya mengenai
tweet tersebut. Pada umumnya, penentuan label terhadap tweet
yang memiliki unsur flaming atau tidak berdasarkan tujuan
atau intensi dari sebuah tweet apakah bertujuan untuk
menyerang atau menunjukkan rasa negatif terhadap individu
lain atau hanya sekedar melampiaskan amarah terhadap objek
benda mati tanpa ditujukan kepada individu lain. Konsekuensi
dalam proses pelabelan ini adalah kesalahan interpretasi dari
peneliti yang bukan ahli dalam bidang bahasa sehingga
kemungkinan pelabelan terhadap suatu tweet bisa saja salah.
Berikut contoh hasil pelabelan dari data tweet hasil filtrasi
sebelumnya.

TABEL VI
CONTOH DATA HASIL MANUAL LABELLING

tweet
@tanyainrl f b*ngsat r lebih b*ngsat. m
baik, n juga ehhe
@akumaumsksurga
@lfrhmh
HAHHAHAHA BARU SADAR N
WORD B*NGSAT BGT GARE
mau lu kaya gimana pun.. lu bakal salah n
hina dimata mereka..
jadi GA USAH
SOK2AN MO JADI BENER.. A*JING
YA TETEP A*JING SADAR DIRI LO
B*NGSAT iyah maap.. w lupa klo w
b*ngsat.. w nya aja yg terlalu maksa
berbaur sama orang2 suci kaya mereka
T*LOL
A*jing B*adab Silup smoga hidup kau
tidak berkah siksa dunia n akhirat

klasifikasi
NB

NB

B

B

Manual labelling akan mengkategorikan tweet menjadi dua
yaitu tweet berlabel ‘NB’ ada sebanyak 1500 dan tweet
berlabel ‘B’ ada sebanyak 1500. Dari proses pelabelan ini,
data tersebut akan digunakan untuk keperluan data testing dan
training pada proses klasifikasi dan evaluasi selanjutnya.

D. Preprocessing

Sebelum masuk ke dalam proses klasifikasi, data tweet akan
melalui tahap preprocessing. Tujuan dari tahap preprocessing
ini adalah untuk menghilangkan noise dari data tweet yang
tidak memiliki bahasa yang terstruktur. Selain itu tahap

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

preprocessing ini dilakukan guna memperbaiki kualitas data.
Berikut tahapan preprocessing pada penelitian ini:

Contoh tweet : [@z_ze @myjourney2020 @La_FarEast
@armanamin330 Yg ada otak jadi tukang anggok lobai jual
agama utk hidup lgi bangsat...Allah bgi otak suruh fikir bkn
jadi bahlol ustaz lahanat dpt kuasa buang semua kitab
kelongkang...😡😡😡😡😡manifesto jadi kertas jamban bila
dah jilat penyamung n pengkhianat konon maufakat
penyamung.ptuii!  https://t.co/algj5511Yk]

1. Mengonversi

seluruh

tweet menjadi

@myjourney2020

lowercase,
mengubah seluruh karakter huruf dari tweet menjadi
huruf kecil.
[@z_ze
@la_fareast
@armanamin330 yg ada otak jadi tukang anggok lobai
jual agama utk hidup lgi bangsat...allah bgi otak suruh
fikir bkn jadi bahlol ustaz lahanat dpt kuasa buang
semua kitab kelongkang...😡😡😡😡😡manifesto jadi
kertas jamban bila dah jilat penyamung n pengkhianat
penyamung.ptuii!
konon
https://t.co/algj5511yk]

maufakat

2. Menghapus alamat link dari suatu web baik HTML

@myjourney2020

maupun URL.
@la_fareast
[@z_ze
@armanamin330 yg ada otak jadi tukang anggok lobai
jual agama utk hidup lgi bangsat...allah bgi otak suruh
fikir bkn jadi bahlol ustaz lahanat dpt kuasa buang
semua kitab kelongkang...😡😡😡😡😡manifesto jadi
kertas jamban bila dah jilat penyamung n pengkhianat
konon maufakat penyamung.ptuii!]

3. Menghapus mentions

atau

tanda @ beserta
usernamenya serta menghapus tanda hashtags dari
tweet.
[yg ada otak jadi tukang anggok lobai jual agama utk
hidup lgi bangsat...allah bgi otak suruh fikir bkn jadi
bahlol ustaz lahanat dpt kuasa buang semua kitab
kelongkang...😡😡😡😡😡manifesto
kertas
jamban bila dah jilat penyamung n pengkhianat konon
maufakat penyamung.ptuii!]

jadi

4. Menghapus karakter lain baik emotikon maupun tanda
baca dalam tweet, menyisakan karakter huruf a-z saja.
[yg ada otak jadi tukang anggok lobai jual agama utk
hidup lgi bangsat allah bgi otak suruh fikir bkn jadi
bahlol ustaz lahanat dpt kuasa buang semua kitab
kelongkang manifesto jadi kertas jamban bila dah jilat
penyamung n pengkhianat konon maufakat penyamung
ptuii]

5. Proses stemming, seperti dijelaskan pada metodologi
sebelumnya merupakan suatu proses mengurangi atau
(atau turunan kata)
menghilangkan infleksi kata
menjadi bentuk kata dasarnya.
[yg ada otak jadi tukang anggok loba jual agama utk
hidup lgi bangsat allah bgi otak suruh fikir bkn jadi
bahlol ustaz lahanat dpt kuasa buang semua kitab
kelongkang manifesto jadi kertas jamban bila dah jilat
penyamung n khianat konon maufakat penyamung
ptuii]

6 / 8

6. Proses tokenisasi, merupakan proses pemisahan teks
menjadi potongan-potongan yang disebut sebagai token.
['yg', 'ada', 'otak', 'jadi', 'tukang', 'anggok', 'loba', 'jual',
'agama', 'utk', 'hidup', 'lgi', 'bangsat', 'allah', 'bgi', 'otak',
'suruh', 'fikir', 'bkn', 'jadi', 'bahlol', 'ustaz', 'lahanat', 'dpt',
'buang',
'kuasa',
'kelongkang',
'jadi',
'kertas', 'jamban', 'bila', 'dah', 'jilat',
'manifesto',
'penyamung',
'maufakat',
'konon',
'n',
'penyamung', 'ptuii']

'khianat',

'semua',

'kitab',

'otak',

7. Menghapus stopword, menghilangkan kata-kata yang
tidak memiliki kontribusi untuk dilakukan ekstraksi
informasi pada isi dokumen.
['yg',
'jual',
'agama', 'utk', 'hidup', 'lgi', 'bangsat', 'allah', 'bgi', 'otak',
'suruh',
'dpt',
'bahlol',
'manifesto',
'kuasa',
'kertas', 'jamban', 'dah', 'jilat', 'penyamung', 'n', 'khianat',
'konon', 'maufakat', 'penyamung', 'ptuii']

'ustaz',
'kelongkang',

'fikir',
'buang',

'anggok',

'lahanat',

'tukang',

'kitab',

'loba',

'bkn',

'jadi',

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

merangkai langkah-langkah sebelumnya mulai dari ekstraksi
fitur, pembobotan kata, sampai pembentukan model yang
dapat diatur penggunaan parameternya.

Dalam proses pengklasifikasian dalam penelitian ini,
proporsi pembagian data dilakukan sebanyak 70% untuk data
training dan 30% untuk data testing. Proses pembagian data
dilakukan secara acak sampai memenuhi proporsi pembagian
data. Setelah proses pembagian data tersebut dilakukan, maka
model klasifikasi dapat dibentuk dengan memanggil class
pipeline yang telah dibentuk sebelumnya serta pembentukan
model prediksi untuk dihitung performa dari algoritma yang
digunakan.

TABEL VII
Hasil Uji Coba Model

Klasifikasi Precision
B
NB

0.72
0.52

Recall
0.58
0.67

F1-Score
0.64
0.59

Akurasi
0.62

8. Mengonversi kata slang maupun singkatan ke bentuk
indonesia yang normal/baku serta mengembalikan tweet
ke bentuk semula (bukan dalam token).
Hasil akhir tweet dari tahapan preprocessing : [yang
otak tukang anggok loba jual agama untuk hidup lagi
bangsat allah bagi otak suruh fikir bikin bahlol ustaz
lahanat dapat kuasa buang kitab kelongkang manifesto
kertas jamban deh jilat penyamung n khianat konon
maufakat penyamung ptuii]

akurasi

Dari hasil perhitungan berdasarkan confusion matrix yang
dihasilkan, nilai akurasi yang didapatkan dari pengujian model
yang telah dibuat seperti pada Tabel VII adalah sebesar 62%.
Nilai
ini didapat berdasarkan pembagian data
sebelumnya yang membagi data training dan data testing
sebesar 70:30. Hal ini tentu saja belum dapat menentukan
keterwakilan dari nilai akurasi sebenarnya pada model yang
telah dibuat, oleh karena itu akan dilakukan evaluasi terhadap
model dengan metode K-Fold Cross Validation.

D. Ekstraksi Fitur dan Pembobotan Kata

E. Evaluasi Model

Pada tahapan ekstraksi fitur ini, dilakukan penggunaan
guna
package
library CountVectorizer
mengekstraksi
fitur pada dataset dengan mengubahnya
menjadi suatu representasi berbentuk vektor. Sebagai contoh
pelaksanaan proses ekstraksi fitur, dipilih tiga data tweet
sebagai berikut:

sklearn

pada

● Dokumen 1: “enggak beres gara gara laptop

bangsat”

● Dokumen 2: “bangsat sih enggak”
● Dokumen 3: “iya anjg corona bangsat”

Dari ketiga dokumen tersebut dibentuk vocabulary yang
mengandung kata dari setiap dokumen. Kata yang terbentuk
dari ketiga kalimat tersebut adalah “enggak”, “beres”, “gara”,
“laptop”, “bangsat”, “sih”, “anjg”, “corona”. Dari ketiga
dokumen tersebut yang telah diubah ke dalam bentuk vektor
selanjutnya akan dilakukan perhitungan dengan menggunakan
formula TF-IDF. Dengan menggunakan TF-IDF maka vektor
yang sudah terbentuk akan memiliki nilai yang sudah
terbobot.

E. Pembentukan Model Klasifikasi

Setelah tahap preprocessing selesai dilakukan,

tahap
berikutnya adalah tahap pembelajaran dari data training serta
klasifikasi. Dalam proses ekstraksi fitur, pembobotan kata,
serta pembentukan model klasifikasi digunakan dengan
bantuan class Pipeline yang disediakan oleh library sklearn
pada python. Penggunaan class Pipeline ini bertujuan untuk

Untuk memaksimalkan evaluasi model, dilakukan metode
K-Fold Cross Validation dengan menggunakan nilai K
sebanyak 10, atau dengan kata lain memisahkan dataset
sebanyak 10 bagian dimana setiap bagian akan menjadi data
testing sedangkan sisanya menjadi data training. Pada proses
k-fold cross validation ini digunakan library StratifiedKFold
dimana penggunaan cross validation akan menggunakan
distribusi uniform sehingga data testing yang dibagi akan
mewakili keseluruhan dataset.

Dari setiap fold akan dihitung nilai evaluasi seperti
sebelumnya
serta
akurasinya. Perhitungan nilai evaluasi setiap fold dataset dapat
dilihat dalam Tabel VIII berikut.

seperti precision,

f-1 score,

recall,

TABEL VIII
Nilai Evaluasi Cross Validation

Fold B

Precision

Recall

1
2
3
4
5
6
7
8
9
10

0.79
0.60
0.61
0.41
0.64
0.68
0.80
0.60
0.67
0.71

0.54
0.56
0.48
0.45
0.60
0.56
0.60
0.58
0.59
0.57

NB
Precision

0.32
0.52
0.33
0.50
0.57
0.47
0.47
0.56
0.54
0.45

F1-Sc
ore
0.64
0.58
0.54
0.43
0.62
0.61
0.69
0.59
0.63
0.63

Akurasi

Recall

0.61
0.57
0.46
0.46
0.61
0.59
0.70
0.58
0.62
0.61

F1-Sc
ore
0.42
0.54
0.38
0.48
0.59
0.52
0.56
0.57
0.58
0.52

0.56
0.56
0.47
0.46
0.61
0.57
0.63
0.58
0.60
0.58

7 / 8

Dari keseluruhan data tersebut untuk melihat keseluruhan
nilai evaluasi maka setiap nilai yang dihasilkan akan
dirata-rata
evaluasi yang
maksimal. Hasil rata-rata metode cross validation dapat dilihat
pada Tabel IX berikut.

sehingga menghasilkan nilai

TABEL IX
Nilai Rata-rata Evaluasi Cross Validation
F1-Score
0.596
0.516

Recall
0.553
0.581

Klasifikasi Precision
B
NB

0.651
0.473

Akurasi
0.562

Dari hasil evaluasi model dengan menggunakan metode
cross validation yang dapat dilihat pada Tabel VII, nilai
precision atau tingkat ketepatan informasi yang diinginkan
dibandingkan dengan yang dihasilkan oleh model untuk
kategori data tweet berlabel ‘B’ sebesar 65,1% dan untuk
berlabel NB sebesar 47,3%. Sedangkan untuk nilai recall atau
tingkat keberhasilan model untuk menemukan kembali sebuah
informasi untuk data tweet berlabel ‘B’ sebesar 55,3% dan
untuk data tweet berlabel ‘NB’ sebesar 58,1%. Sedangkan
untuk akurasi dari keseluruhan model yang diuji hanya dapat
berkisar di rata-rata sebesar 56,2%.

E. Perbandingan dengan Algoritma Lain

Selain hasil evaluasi, dilakukan juga perbandingan dengan
algoritma lain seperti Logistic Regression, Support Vector
Machine, dan Random Forest, algoritma Logistic Regression
tingkat akurasi yang paling tinggi dengan nilai
memiliki
57,7%, sedangkan Random Forest memiliki tingkat akurasi
terendah dengan nilai 54,9%. Dan untuk perbandingan
runtime ketika memproses klasifikasi, Algoritma Naive Bayes
mampu melakukan proses klasifikasi yang cepat daripada
algoritma lain, sedangkan untuk algoritma yang cukup lama
dalam melakukan proses klasifikasi adalah algoritma Random
Forest.

VII.

PENUTUP

Berdasarkan hasil dan pembahasan yang didapatkan
sebelumnya, maka kesimpulan yang dapat diambil dalam
penelitian ini adalah bahwa pembentukan model klasifikasi
dengan algoritma Naïve Bayes Classifier berdasarkan data
tweet yang sudah dilabeli dengan proporsi data training dan
testing sebesar 70:30 menghasilkan nilai akurasi sebesar 62%.
Sedangkan dari hasil evaluasi model yang dibuat dengan
menggunakan metode K-Fold Cross Validation sebanyak
K=10 menghasilkan nilai evaluasi optimal dengan nilai
akurasi sebesar 56,2%. Dari nilai F-1 Score dapat dilihat
bahwa model klasifikasi lebih baik dalam mengklasifikasikan
tweet yang tidak mengandung unsur flaming (64%) daripada
yang mengandung unsur flaming (44%). Hal ini menunjukkan
model
dalam
mengklasifikasikan tweet yang mengandung flaming. Selain
itu, dari hasil perbandingan dengan algoritma lain seperti
Logistic Regression, Support Vector Machine, dan Random
Forest, algoritma Logistic Regression memiliki tingkat akurasi
yang paling tinggi dengan nilai 57,7%, sedangkan Random

belum cukup

dibuat

andal

yang

Makalah Skripsi – Program Studi D-IV Komputasi Statistik

Forest memiliki tingkat akurasi terendah dengan nilai 54,9%..
Dari kedua hasil perbandingan dimana dilihat dari sisi akurasi
dan runtime masing-masing algoritma, algoritma Naive Bayes
cukup
yang menjadi
perbandingan dalam penelitian ini.

algoritma

daripada

baik

lain

kesimpulan

Berdasarkan

peneliti
untuk
memberikan
penelitian-penelitian yang berkaitan dengan topik penelitian
ini di masa depan sebagai berikut:

didapatkan,
acuan

sebagai

untuk

saran

yang

1. Menambah data label dalam supervised learning
sebagai acuan model klasifikasi untuk menambah
akurasi model.

2. Menambah tahapan dalam preprocessing seperti
pembobotan
adanya
pengaruh emotikon dalam unsur flaming dalam
suatu tweet.

dikarenakan

emotikon

3. Menerapkan imbalance class pada jumlah data
training untuk lebih memfokuskan kepada kelas
atau kategori yang krusial atau yang diinginkan.

4. Menggunakan
klasifikasi.

algoritma

lain dalam model

DAFTAR PUSTAKA
[1] Keith Cortis and Siegfried Handschuh. 2015. Analysis of cyberbullying
tweets in trending world events. In Proceedings of the 15th International
Conference on Knowledge Technologies and Data-driven Business
(i-KNOW '15). Association for Computing Machinery, New York, NY,
USA, Article 7, 1–8.P. Shakarian, A. Bhatnagar, A. Aleali, E. Shaabani,
and R. Guo, The Independent Cascade and Linear Threshold Models.
Cham: Springer International Publishing, 2015, pp. 35–48.

[2] H. Margono, X. Yi, and G. K. Raikundalia, ”Mining Indonesian
in Thirty-Seventh

Cyberbullying Patterns
Australasian Computer Science Conference , Auckland, 2014.

in Social Networks,”

[3] Aminef. (2021, 6) Merunut lemahnya hukum cyberbullying di Indonesia.
[Online].
Available:
https://www.aminef.or.id/merunut_lemahnya_hukum_cyberbullying_di_i
ndonesia/

[4] Willard, N. (2007). Cyberbullying and cyberthreats. Washington: U.S.

Department of Education.

[5] Tjitjik Hamidah. (2020, 6). Perilaku Cyber Bullying : Bentuk dan
Available:
Penyebabnya.
https://buletin.k-pin.org/index.php/arsip-artikel/278-perilaku-cyberbullyin
g-bentuk-dan-penyebabnya
[6] D., Ruth Mega Ulina

(2014) PEMBANGUNAN APLIKASI
PENCARIAN DOKUMEN MENGGUNAKAN TEXT MINING
BERBASIS WEB. S1 thesis, UAJY.

[Online].

[7] M. kabir hasan dkk. Baby & T.,2012.”Data mining Implementasi Regresi
Linier Untuk Prediksi Nilai Ujian”. Foundation For Statistical
Conmputing.

[8] N. Novalita, A. Herdiani, I. Lukmana, and D. Puspandari, “Cyberbullying
identification on twitter using random forest classifier,” in The 2nd
International Conference on Data and Information Science 15–16.
November 2018, UK: IOP Publishing Ltd, Nov 2018.

[9] M. Z. Naf’an, A. A. Bimantara, A.Larasati, E. M. Risondang, and N. A.
S. Nugraha, “Sentiment Analysis of Cyberbullying on Instagram User
Comments,” Journal of Data Science and Its Applications , vol. 2, pp.
38-48.

[10] H. Nurrahmi and D. Nurjanah, “Indonesian Twitter Cyberbullying
Detection using Text Classification and User Credibility,” in 2018
International Conference
and Communications
Technology (ICOIACT) , 2018, pp. 543-548.

Information

on

8 / 8

"
16.9245,"Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Clustering dalam Small Area Estimation  
Studi Kasus: Proporsi Rumah Tangga Peduli Terhadap Penanganan 
Sampah tingkat Desa di Provinsi D.I Yogyakarta tahun 2020 

M Reza Pedana (16.9245, 4SD2) 
Dosen Pembimbing: Dr. Azka Ubaidillah 

Ringkasan—  Sampah  merupakan  permasalahan  berbagai 
negara  di  dunia  yang    memiliki  jumlah  penduduk  tinggi, 
termasuk Indonesia.  Penanganan sampah merupakan salah satu 
tujuan  dalam  Sustainable  Development  Goals(SDGs).  Provinsi 
Yogyakarta  merupakan  salah  satu  provinsi  dengan  jumlah 
sampah  tinggai  berdasarkan  data  BPS.  Selama 
ini  data 
pengelolaan  sampah  tidak  dipublikasikan  sampai  level  kecil 
(misalnya  tingkat  desa)  karena  sampelnya  belum  memadai 
untuk  dilakukan  pendugaan  secara 
langsung.  Small  Area 
Estimation (SAE) mampu mengestimasi permodelan hingga level 
area  kecil.  Dalam  penelitian  ini  dilakukan  pendugaan  proporsi 
rumah  tangga  peduli  terhadap  sampah  pada  tingkat  desa  di 
provinsi  DI  Yogyakarta  dengan  metode  SAE.  Untuk  lebih 
mengoptimalkan  penerpan  SAE,  penelitian  ini  menambahkan 
efek  clustering  dalam  pemodelannya.  SAE  Clustering  mampu 
menghasilkan  estimasi  yang  mengikuti  estimasi  langsungnya 
dibandingkan  SAE-FH,  namun  nilai  RSE  SAE-FH  lebih  efisien 
dibandingkan dengan SAE clustering. 

Kata  Kunci—  Small  area  estimation,  Clustering,  SAE 

Clustering, penanganan sampah. 

I.  LATAR BELAKANG 
Sampah  merupakan  hal  yang  menjadi  topik  permasalahan 
di negara manapun termasuk Indonesia yang memiliki jumlah 
penduduk  sebanyak  261,89  juta  jiwa  pada  tahun  2017. 
Menurut  [1]  sumber  sampah  terbanyak  di  Indonesia  berasal 
dari pasar tradisional dan pemukiman.  

Berdasarkan Susenas Modul Ketahanan Sosial 2017, rumah 
tangga yang melakukan kegiatan daur ulang hanya 1,2 persen 
rumah 
tangga  masih 
tangga,  dan  66,9  persen  rumah 
melakukan penanganan sampah membakar sampahnya. 

terbilang 

Sampah  juga  menjadi  penyebab  bencana  alam  seperti 
banjir.  Pada  Tahun  2017,  produksi  sampah  per  hari  di  Pulau 
Jawa 
satunya  Surabaya  yang 
salah 
menghasilkan  sampah  9.896,78  m3  per  hari  dan  Jakarta 
menghasilkan  sampah  sebanyak  7.164,53  m3,  sementara  itu 
DI Yogyakarta turut memproduksi sampah yang cukup tinggi 
yakni 1048 m3. 

tinggi, 

Penanganan  sampah  menjadi  salah  satu  tujuan  dalam 
Sustainable  Development  Goals  (SDGs),  salah  satunya  pada 
tujuan  poin  11  yaitu  menjadikan  kota  dan  pemukiman 
manusia  inklusif,  aman,  tangguh  dan  berkelanjutan  dengan 
strategi  meningkatkan  kualitas  lingkungan  serta  tujuan  poin 
12,5  yang menyatakan  pada  tahun  2030  setiap  Negara  secara 
substansial mengurangi  produksi  limbah  melalui  pencegahan, 
pengurangan,  daur  ulang  dan  penggunaan  kembali,  untuk 

dapat  menjamin  pola  produksi  dan  konsumsi  yang 
berkelanjutan.  

Berdasarkan data tersebut diperlukan kesadaran masyarakat 
dalam  mengatasi  kebersihan  lingkungan  hidup  hingga  satuan 
terkecil.  Pembentukan  persentase  kesadaran 
lingkungan 
lingkungan  masyarakat  khususnya  penanganan  sampah  akan 
mengalami  permasalahan  akibat  keterbatasan  jumlah  sampel. 
Hal  ini  dapat  diatasi  dengan  menggunakan teknik  small  area 
estimation.  Rao  dalam  [9]  menjelaskan  bahwa  Small  area 
estimation  (SAE)  merupakan  metode  estimasi  berbasis 
permodelan  dan  SAE  yang  paling  sering  digunakan  adalah 
model level area. 

Referensi  [2]  mengusulkan  teknik  clustering  dalam  small 
area  estimation  yang  membentuk  kelompok  small  area 
berdasarkan  klaster  dari  kovariat  pembentuknya.  Harapannya 
adalah  mendapatkan  small  area  yang  berdekatan  berdasarkan 
kovariatnya. 

Dalam  penelitian  ini  akan  diterapkan  teknik  clustering 
dalam  small  area  estimation 
terhadap  data  proporsi 
penanganan sampah tingkat desa di provinsi D.I. Yogyakarta. 

II.  TUJUAN PENELITIAN 
 Adapun tujuan dari penelitian ini adalah: 
1.  Menerapkan clustering pada small area estimation 
2.  Membandingkan  teknik  small  area  estimation  Fay-

Herriot dan Small area estimation menggunakan clustering 

3.  Pemetaaan  Proporsi  Rumah  Tangga  Peduli  Terhadap 
Penanganan Sampah tingkat Desa di Provinsi D.I Yogyakarta 

III. PENELITIAN TERKAIT 
Penelitian  [3]  menguji  model  regresi  stepwise  dalam 
penentuan  peubah  agronomi  dan  generative  yang  berperan 
terhadap hasil jagung putih. 

Penelitian [4] menerapkan Small Area Estimation terhadap 
data  Badan  Pusat  Statistik  SUSENAS  Triwulanan  dengan 
add-ins  excel.  Penelitian 
tersebut  menyimpulkan  bahwa 
EBLUP  Fay-Herriot  Based  Area  Level  merupakan  teknik 
yang  sesuai  untuk  diterapkan  pada  data  SUSENAS  dengan 
variable  estimasi  rata-rata  pengeluaran  perkapita  rumah 
tangga  di  provinsi  NTT.  Hasil  Estimasi  menggunakan  SAE 
mampu  menurunkan  nilai  RSE  design-based  namun 
penurunan tersebut belum mampu memenuhi target BPS yaitu 
kurang dari 5% 

 1 / 7 

 
 
 
 
 
 
 
Penelitian [5] membandingkan metode k-means dan metode 
dbscan  pada  pengelompokkan  rumah  kost  mahasiswa  di 
kelurahan  tembalang  semarang,  peneliti  tersebut  melakukan 
proses  data  mining  serta  analisis  kelompok  (cluster  analysis) 
dalam  metode 
ini  dilakukan  pengelompokkan  dengan 
menggunakan  K-Means  clustering,  juga  metode  Density-
Based  Spacial  Clustering  Algorithm  With  Noise  (DBSCAN) 
kesimpulan  yang  ditemukan  adalah  K-Means  lebih  baik  dari 
metode DBSCAN dalam mengelompokkan data rumah kost. 

Penerapan  clustering  SAE  yang  menjadi  fokus  pada 
penelitian  ini  dilakukan  oleh  [2].  Dalam  penelitiannya, 
diajukan model complete clustering, combined clustering, dan 
simple  clustering.  Dilakukan  juga  pengaplikasian  metode 
tersebut  dalam  real  data  analysis,  serta  melakukan  simulasi. 
Penelitian 
tersebut  menggunakan  hierarcichal  clustering 
terhadap kovariat dalam small area. 

IV. METODE PENELITIAN  
Data  yang  digunakan  pada  penelitian  ini  adalah  data  PKL 
STIS angkatan 59 yang dilakukan di provinsi D.I Yogyakarta. 
Pada data tersebut terdapat 77 Kecamatan dan  220 desa. 
Adapun variable penyerta yang digunakan yaitu   
(X1)   Jumlah lokasi permukiman di bantaran sungai,  
(X2)   Jumlah  keluarga  pada  permukiman  di  bantaran 

sungai,  

(X3)   Jumlah SD/MI negeri,  
(X4)   Jumlah SMK negeri,  
(X5)   Jumlah SMK swasta,  
(X6)   Jumlah rumah sakit bersalin,  
(X7)   Jumlah puskesmas  pembantu,  
(X8)  Jumlah  Posyandu  dengan  kegiatan/pelayanan  setiap 

dua bulan sekali atau lebih,  

(X9)   Jumlah Pos Pembinaan Terpadu (Posbindu),  
(X10) Jumlah  warga  peserta  BPJS  Kesehatan  Penerima 

Bantuan Iuran (PBI) dan Jamkes pada tahun 2019,  

(X11) Jumlah  Kelompok  pertokoan  (minimal  10  toko  dan 

mengelompok dalam satu lokasi), dan 

(X12) Jumlah  Pasar 
subuh, pasar terapung, dll.) 

tanpa  bangunan  (misalnya:  pasar 

Langkah  analisis  yang  akan  dilakukan  adalah  sebagai 

berikut: 

1. Menghitung estimasi langsung proporsi desa yang peduli 

terhadap penanganan sampah 

2. Melakukan pendugaan dengan SAE model fay herriot 
3. Melakukan pendugaan dengan SAE metode clustering 
4. Membandingkan RSE pendugaan langsung dan SAE. 
5.  Pemetaaan  proporsi  rumah  tangga  peduli  perhadap 

penanganan sampah tingkat desa di Provinsi D.I Yogyakarta 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

c. Dijadikan makanan ternak, 
d. Diangkut petugas/dibuang ke TPS/TPA 
e. Dijual ke pengumpul barang bekas 
f. Ditimbun/dikubur 
g. Dibakar 
h. Dibuang ke laut/sungai/got 
i. Dibuang sembarangan 

4.1.1 Kategorisasi 
kategori  peduli  diberi  kode  1,  jika  rumah  tangga  tersebut 

melakukan minimal 5 dari 9 kategori. 

Kategori  Tidak  peduli  diberi  kode  0,  jika  rumah  tangga 

tersebut tidak melakukan minimal 5 dari 9 kategori. 

Nilai  proporsi  dan  varian  untuk  proporsi  rumah  tangga 
yang  peduli  terhadap  penanganan  sampah  di  desa  ke-i  di 
provinsi  D.I  Yogyakarta  menggunakan  metode  pendugaan 
langsung dengan rumus sebagai berikut: 

 ̂      ∑

dan    ( ̂ )   

 ̂ (   ̂ )

(1) 

(2) 

Dimana: 
 ̂  =  penduga  langsung  proporsi rumah tangga  yang  peduli 

terhadap penanganan sampah di desa ke-i 
   = jumlah sample rumah tangga peduli di desa-i 
   = jumlah sample rumah tangga di desa-i 

4.3 Stepwise regression 

Menurut  [3]  Prosedur  regresi  stepwise  merupakan 
salah  satu  prosedur  pemilihan  himpunan  variabel  prediktor 
terbaik.  Tujuan  dari  metode  ini  adalah  menemukan  variable 
yang  memiliki  pengaruh  signifikan  terhadap  proporsi  dengan 
model pada persamaan (3). 

(3) 

mana  
  = variabel dependen  
  = konstanta regresi  
  ,   ,…,   = koefisien regresi  
  ,   ,...,   = variabel bebas  
ε = galat taksiran (sisa residu). 

4.4 Clustering 
Dalam  penelitian  ini  akan  digunakan  k-means  clustering 
dalam  mengelompokkan  variable  penyerta,  teknik  ini  akan 
disebut  complete  clustering,  kemudian  informasi  clustering 
akan digunakan dalam SAE clustering. 

4.1  Proporsi 
penanganan sampah 

rumah 

tangga  yang  peduli 

terhadap 

4.5 Small Area Estimation 
model regresi linear berganda [10] berikut ini:  

Menurut  [6]  Penanganan  terhadap  sampah diklasifikasikan 

menjadi sembilan kategori: 

a. Didaur ulang 
b. Dibuat kompos/pupuk 

dimana  
 : vektor dari variabel respon, berukuran  𝑥1  

(6) 

 2 / 7 

 
 
 
 
 
 
 
 
 
   
  
   
  
  
 
   
 
 
  
 
 
 
 
 
 
 
                                        
 
 
 
 
 
            
 
 
 : matriks dari variabel penyerta, berukuran  𝑥  
 : vektor dari parameter, berukuran  𝑥1  
 : vektor dari error, berukuran  𝑥1, dengan ε𝑖 ~    (     

 )   

Pada model tersebut pengaruh tetap (X) yang terdapat pada 

model telah diperhatikan. Namun, tidak terdapat pengaruh 
acak yaitu u. 

Bentuk model LMM adalah: 

(7) 

dimana:  
  : vektor peubah respon berukuran n1,  
  : matriks dengan p-peubah penjelas independen 

berukuran n p  ,  

  : vektor parameter koefisien model berukuran p1,  
  : vektor pengaruh acak berukuran n1.  
  : vektor galat berukuran n1. 
Asumsi:    (     
 (    )    (    )     

    ,    (     

   ),  (    )  

Matriks ragam dari y , u dan e adalah: 

   (     )    (   )     
   (     )    (   )     
   (     )                

(8) 
(9) 
(10) 

4.5.1 Model area level 
Model small area estimation salah satunya adalah area 
level, yaitu data variabel penyerta merupakan pada tingkat 
area 

4.5.1.1 model fay herriot 
Persamaan metode Fay-Herriot [9] adalah sebagai berikut: 

 ̂
    𝑥 
 ̂

𝑖              

(11) 
(12) 

dengan  
 ̂
 : estimator langsung, berukuran 1𝑥1  
   : parameter small area, berukuran 1𝑥1  
   : vektor variabel penyerta, berukuran  𝑥1  
𝑥 
  : vektor parameter yang fixed, berukuran  𝑥1  
  : pengaruh acak small area, berukuran 1𝑥1, diasumsikan 

        (     

  )  

    : sampling error, berukuran 1𝑥1, diasumsikan 

        (      ), 𝜓𝑖 diketahui,  

 )     dan  (     

dimana  𝑖 dan  𝑖 saling independen, sehingga  (    
 )     

 (  
varians kovarians dari  𝑖 dan  𝑖 yaitu matriks block diagonal 
dengan bentuk: 

 . Matriks 𝐆 dan 𝐑 merupakan 

 )  

    (

) 

(13) 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

𝜓 

𝐑   (

𝜓 

) 

𝜓 

(14) 

Matriks 𝐆 merupakan matriks varians-kovarians definit 
positif dari variansi antar small area berukuran  𝑥  yang 
biasanya tidak diketahui dan harus diestimasi. Matriks 𝐑 
adalah matriks varians-kovarians definit positif dari sampling 
error berukuran  𝑥 . Sedangkan 𝐕 = 𝐆 + 𝐑 adalah matriks 
varians-kovarians dari  ̂ , yang berbentuk: 

    𝜓 

    𝜓 

𝐕   (

) 

(15) 

    𝜓 

Pada model Fay-Herriot ini, parameter yang akan 

diestimasi adalah        
EBLUP. 

        , menggunakan metode 

Pendugaan ragam dengan REML 
komponen ragam   

  nilainya tidak diketahui sehingga 
perlu dilakukan pendugaan komponen ragam. Salah satu 
metode yang dapat digunakan dalam pendugaan komponen 
ragam adalah REML. 

Penduga EBLUP diperoleh dengan mengganti ragam 
dengan penduga ragam  ̂ . Penduga EBLUP dapat ditulis 
sebagai berikut: 

 ̂    ̂( ̂)     ̂     ( ̂)     ( ̂)(      ̂) 

(16) 

 ̂    ̂( ̂)   (     ( ̂) )

     ( ̂)  

(17) 

Mean Squared Error 

MSE dari  ̂

  didefinisikan sebagai: 

MSE ( ̂

 ) =     

   (  

 )      

  (  

 )       

  (  

 ) 

Dimana     

  (  

 )    

     𝜓 , 

(18) 

(19) 

  (  

 )   (    𝜓 ) 𝑥 

  [∑

(     

]

 )

𝑥 

(20) 

  (  

 ) =[

(     

 )

] ( ̂

    𝑥  

  ̂) 

  𝑥  (  

 ) 

4.5.1.2 SAE metode clustering 

Uji kesamaan varians komponen 

(21) 

 3 / 7 

 
 
 
 
 
 
                
 
 
 
 
        
        
 
 
 
 
               
            
 
 
 
 
 
   
 
  
 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
  
 
 
 
 
 
  
   
     
 
   
  
    
 
    
 
   
  
   
 
   
  
    
 
    
 
 
          
       
           
                    
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Jika  W  >  crit.  Value  dari  distribusi  normal  standar 
dengan tingkat signifikansi p-value pada 22 masukkan cluster 
dengan  komponen  varians  max  ke  dalam  grup  baru,  ulangi 
jika  k’>2  dengan  nilai  max.  baru,  jika  nilai  max.  baru  harus 
dipisah maka masukkan ke grup pada max. sebelumnya. 

4.  Jika  jumlah  cluster  dalam  grup  tidak  berubah  pada 
step  sebelumnya,  maka  uji  asumsi  kesamaan 
komponen  varians  untuk  grup  dengan  cluster  lebih 
dari  2.  Jika  k  lebih  dari  3,  gunakan  uji  statistic  W, 
gunakan  simultaneous  T-test  dengan  dengan  tingkat 
signifikansi  p-value  pada  22  untuk  k=3.  Jika  gagal 
tolak  H0  maka  bagi  grup  dengan  jumlah  cluster 
genap  menjadi  dua  subgroup  dimulai  dari  estimasi 
MMM  terkecil.  Untuk  yang  berjumlah  ganjil,  bagi 
sehingga  subgroup  terakhir  terdiri  dari  3  cluster 
kemudia  uji  asumsi  kesamaan  komponen  varians, 
jika Tolak H0, buat subgroup baru yang berisi 2 dan 
1 cluster  dimulai dari estimasi MMM terkecil.  

Simple  clustering 

1.  Reduksi model  23 menjadi model  7 
2. 
  cluster 

Informasi 
pendugaan  tidak langsung menggunakan klaster 

  digunakan  dalam 

  model  

Pada  Small  Area  Estimation  dengan  Clustering  peubah 
ragam acak klaster diduga menggunakan  modified method-of-
moments  [2] yang bisa dilihat pada persamaan berikut: 

 ̂  

∑

{(          ̂

   )

     }

(25) 

Dimana  ̂

    adalah: 

 ̂

        (   )      

(26) 

Penduga EBLUP Cluster 

         ̅    ̅   
 ̅

(27) 

Dimana  ̅       𝐆𝐕  (          ̅), 

 ),     (          ), 𝐕  

 ̅   (  𝐕   )  (     
 𝑖  (𝐕       𝐕 ), 𝐕     𝑖  (   
    (𝐆       𝐆 ),         
identitas untuk                 dan               

       𝑥     dan I adalah matrix 

        ),    

MSE dari  ̅

 (      

  )  

(22) 

Dimana: 
 )  ∑ ( ̂ 
    (  
    ( ̂  
  ), 
 ̂ 
    (1/k) ∑  ̂  
 ̂  
 ∑       𝑖   {  
  ∑

         ̂  

     ̂ 

 ) 

(   

       )

  ∑

(   

   )

} 

Complete clustering 

                          , 

 (23) 

                  ,               
      = variabel respons 
      = vektor kovariat 
= elemen ke-j 
= klaster 

Bertujuan  membentuk  area  kecil  yang  memiliki 

kedekatan berdasarkan kovariatnya. 

1.  Membentuk klaster berdasarkan Kovariat (X) 
2. 

Informasi    cluster  yang  terbentuk  digunakan  dalam 
pendugaaan tidak langsung menggunakan cluster 

Combine clustering 

1.  Urutkan estimasi MMM 
2.  Bandingkan nilai varians MMM terkecil dengan nilai 
varians  terkecil    kedua  menggunakan  uji  kesamaan 
varians  berdasarkan  (22)  jika  gagal  tolak  Ho,  maka 
gabungkan  keduanya  dalam  satu    kelompok,  jika 
sebaliknya  maka    varians  terkecil  masuk  ke  dalam 
kelompok  single.  Kemudian,  nilai  varians  terkecil 
kedua  dibandingkan  dengan  nilai  varians  terkecil 
ketiga,  uji  H0  berdasarkan  (22)  jika  gagal  tolak  H0 
maka gabungkan klaster tersebut ke dalam grup yang 
berisi    klaster  kedua.  Ulangi  hingga  semua  klaster 
masuk    kedalam  grup  beranggotakan  1  hingga  2 
klaster. 

3.  Untuk  grup  dengan  cluster  lebih  dari  2,  cari  nilai 
maximum  dan  rata-rata  MMM  estimate,   ̂   
 dan 
 ̂    

Uji statistic W: 

 (        

W={

 (        

Dimana  

       = jumlah small area dari  ̂   
     ̂    
   ( ̂   
)  =(   
     = index dari  ̂   
   ( ̂ 

 ) = estimasi varians dari  

   ( ̂   

)

  ∑

)     (

    ̂    

* {  ̂   
   ( ̂   

)

    ̂    
 ̂   
   ( ̂   

    ̂    

    ̂    

* {

           (  )  }           

 (24) 

      }            

)

   ( ̂

  )       

( ̂)       

( ̂)        

( ̂) 

(28) 

)

∑

   ( ̂ 

 )

(  

       )

Dimana     (   

  ),     
cluster ke-  dalam diag(G-𝐆𝐕  𝐆), 

( ) adalah elemen ke-  dari 

( )       (  𝐕   )     ,                       dan      

adalah baris ke-  dari klaster ke-  dalam      

 4 / 7 

 
 
 
    
 
   
      
  
 
 
   
 
   
 
   
 
     
   
    
 
   
   
   
    
 
 
 
 
 
   
   
 
 
 
 
 
 
 
   
 
 
 
 
     
 
 
 
 
 
     
 
 
 
 
 
 
 
 
 
  
 
 
 
  
 
      
 
 
 
 
   
 
   
    
 
 
 
 
 
 
 
 
   
 
   
 
   
    
 
 
 
 
 
 
 
 
             
 
   
 
 
           
    
 
( )      {(

*   (

*    ( )}   

(29) 

(30) 

(

)

   ( )   ∑      

∑       𝑖   {  

  ∑

(  

       )

  ∑

(  

       )

} 

(31) 

4.6 Relative Standard Error 
Relative Standar error adalah ukuran presisi suatu estimasi 

relative terhadap estimasinya, penghitungan rse dapat dilihat 
pada persamaan (29) [7]: 

   ( ̂

 )    

 )

  ( ̂
 ̂

(32) 

Dimana 

: nilai estimasi 

 ̂
Se( ̂
estimasi. 

 ): ukuran presisi jarak estimasi terhadap rata-rata 

V.  KERANGKA PIKIR 
Gambar 1 merupakan kerangka piker penelitian ini dimana 
ditampilkan  bahwa  penelitian  menggunakan  metode  estimasi 
langsung  dan  tak  langsung,  metode  estimasai  tak  langsung 
yang  digunakan  adalah  small  area  estimation  model  fay-
herriot  dan  clustering,  serta  peran  variable  penyerta  dalam 
metode tersebut. Proporsi rumah tangga yang peduli terhadap 
penanganan  sampah  akan  diestimasi  menggunakan  metode 
langsung dan tak langsung, pada estimasi tak langsung metode 
clustering, informasi variable penyerta akan digunakan dalam 
pembentukan  klaster,  kemudian  hasil  dari  masing-masing 
metode akan dibandingkan. 

Gambar 1. kerangka pikir 

Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

VI. HASIL DAN PEMBAHASAN 

6.1 Pendugaan langsung 
Sebelum  dilakukan  penghitungan  proporsi  pendugaan 
langsung, dilakukan kategorisasi terhadap rumah tangga yang 
peduli  dengan  penanganan  sampah,  kemudian  dilakukan 
penghitungan  proporsi  dan  varians  proporsi  menggunakan 
persamaan  (1)  dan  (2).  Berdasarkan  penghitungan  tersebut 
terbetuk  105  desa  yang  peduli  terhadap  penanganan  sampah, 
artinya  dari  seluruh  desa  yang  tersampel  hanya  105  desa 
dengan  rumah  tangga  yang  melakukan  minimal  5  dari  9 
kategori penanganan sampah. 

Table  1.  menyajikan  data  summary  dari  pendugaan 
langsung,  diketahui  bahwa  proporsi  rumah  tangga  peduli 
terhadap  penanganan  sampah  tingkat  desa  terendah  sebesar 
0.04167  sedangkan  maksimumnya  adalah  0.33333  dan  rata-
ratanya 0.08533 

TABEL I 
SUMMARY PROPORSI RUMAH TANGGA YANG PEDULI TERHADAP 

PENANGANAN SAMPAH 

Min 

1st quantile  Median  Mean 

3rd quantile 

max 

0.04167 

0.04167 

0.0454 

0.085 

0.125 

0.3333 

6.2 Variabel penyerta 

variabel  penyerta  yang mampu menjelaskan  variabel 
respon  akan  membuat  SAE  menghasilkan  dugaan  yang  baik. 
Berdasarkan  hasil  stepwise  regression  terbentuk  persamaan 
dengan 12 variable penyerta yang terpilih. 

6.3 Pendugaan tak langsung dengan model fay-herriot 
Tabel  di  bawah  meyajikan ringkasan nilai  estimasi  dengan 
pendugaan langsung dan estimasi model fay-herriot, diketahui 
bahwa  nilai  estimasi  dengan  pendugaan  langsung  memiliki 
rata-rata 
fay-herriot 
sebesar  0.085 
menghasilkan rata-rata sebesar 0.06402 

sedangkan  metode 

TABEL V 
 STATISTIC PENDUGA LANGSUNG DAN FAY-HERRIOT 

Statistik 

Penduga  
langsung 

Penduga Fay-
Herriot 

Min 

0.0416 

Median 

0.0454 

Mean 

Max 

0.0850 

0.3333 

0.0222 

0.0643 

0.0673 

0.2620 

6.4 Clustering 
Sebelum melakukan clustering, ditentukan jumlah k klaster 
yang  akan  dibentuk  menggunakan  elbow  method.  Klaster 
optimum  ditentukan  dengan  melihat  nilai  yang  mengalami 
penurunan  paling  besar  dan  membentuk  sudut  siku  (elbow) 
[8].  Gambar  2  menunjukkan  klaster  optimum  yang  dapat 
dibentuk  dalam  penelitian  ini  yaitu  4.  Selanjutnya  dilakukan 
clustering  dengan  metode  k-means,  rincian  klaster  serta 
jumlah  area    kecil  ditunjukkan  pada  table  6.  Pada  klaster  1 
terdapat 12 area, klaster 2 terdapat 51 area, klaster 3 terdapat 
39 area, dan klaster 4 terdapat 3 area. 

 5 / 7 

 
 
    
    
 
  
    
 
  
 
    
 
  
 
 
 
 
 
     
 
    
       
     
 
    
 
     
 
    
       
     
 
    
 
 
     
 
    
       
     
 
    
 
  
 
 
 
 
   
 
 
 
   
 
     
   
    
 
   
 
   
    
 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

Sebaran estimasi dengan pendugaan langsung dan tidak 

langsung memiliki pola yang mirip 

6.9  Perbandingan MSE pendugaan langsung dan tidak 

langsung 

Gambar 4. plot MSE pendugaan langsung dan tidak 

langsung 

Hasil pendugaan tidak langsung menunjukkan nilai MSE 

yang lebih baik daripada pendugaan langsung 

6.10 Perbandingan boxplot MSE pendugaan tidak langsung 

Gambar 5. Boxplot MSE pendugaan tidak langsung 

Metode simple clustering memiliki MSE yang lebih baik, 

diikuti oleh eblup fay herriot yang mendekati hasil simple 
clustering 

6.11 Perbandingan boxplot RSE pendugaan langsung dan 

Gambar 2. nilai klaster optimum 

TABEL VI 
JUMLAH AREA KECIL PADA CLUSTER HASIL COMPLETE CLUSTEIRNG 
Cluster  Area kecil 
6 
1 
21 
2 
41 
3 
37 
4 

TABEL VII 
JUMLAH AREA KECIL PADA CLUSTER HASIL COMBINE CLUSTERING 
Kelompok  Cluster  Area kecil 
1 
2 

3,4 
2,1 

78 
27 

6.7 Pendugaan tidak langsung dengan metode clustering 
Berdasarkan  table  di  bawah,  dapat  dilihat  nilai  ringkasan 
dari  nilai  estimasi,  nilai  rata-rata  untuk  penduga  langsung 
sebesar 0.085 dibandingkan dengan rata-rata penduga EBLUP 
dengan  clustering  yaitu  0.0712,  0.0737,  0.0673  untuk 
complete  cluster, combine cluster , dan cimple  cluster secara 
berurutan. 

TABEL VI 
RINGKASAN NILAI ESTIMASI 

Statistik 
Penduga  langsung 
Complete  cluster 
Combine cluster 
Simple  cluster 
Fay-Herriot 

Min  Median  Mean  Max 
0.3333 
0.0416 
0.2560 
0.0312 
0.2531 
0.0348 
0.2620 
0.0222 
0.0643 
0.0222 

0.0850 
0.0712 
0.0737 
0.0673 
0.0673 

0.0454 
0.0628 
0.0626 
0.0643 
0.0643 

6.8.  Perbandingan hasil estimasi pendugaan langsung dan 

tidak langsung 

tidak langsung 

=6

Gambar 3. Plot RSE pendugaan langsung  dan  tidak 

langsung 

langsung 

Gambar 6. Boxplot RSE pendugaan langsung dan tidak 

 6 / 7 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Makalah Skripsi – Program Studi D-IV Komputasi Statistik 

[1] 

[2] 

[3] 

[4] 

[5] 

[6] 

[7] 

[8] 

[9] 

[10] 

DAFTAR PUSTAKA 

A. Ramon and A. Afriyanto, “Karakteristik Penanganan Sampah 
Rumah Tangga Di Kota Bengkulu,” J. Kesehat. Masy. Andalas, vol. 
10, no. 1, p. 24, 2017, doi: 10.24893/jkma.10.1.24-31.2015. 
E. Torkashvand, M. Jafari Jozani, and M. Torabi, “Clustering in 
small area estimation with area level linear mixed models,” J. R. 
Stat. Soc. Ser. A Stat. Soc., vol. 180, no. 4, pp. 1253–1279, 2017, 
doi: 10.1111/rssa.12308. 
N. N. Andayani, M. Aqil, and N. Syuryawati, “Aplikasi Model 
Regresi Step Wise Dalam Penentuan Hasil Jagung Putih,” Inform. 
Pertan., vol. 25, no. 1, p. 21, 2016, doi: 
10.21082/ip.v25n1.2016.p21-28. 
E. Ikhsan, C. A. Hidayat, and W. A. Nurizza, “EFISIENSI 
METODE EBLUP PADA SMALL AREA ESTIMATION Studi 
Kasus : Estimasi Persentase Penduduk Miskin di Provinsi Nusa 
Tenggara Timur Tahun 2017,” J. Stat. Appl. Comput. Stat., vol. 10, 
no. 2, pp. 1–12, 2018. 
S. Budiman, D. Safitri, and D. Ispriyanti, “Perbandingan Metode K-
Means Dan Metode Dbscan Pada Pengelompokan Rumah Kost 
Mahasiswa Di Kelurahan Tembalang Semarang,” None, vol. 5, no. 
4, pp. 757–762, 2016. 
Tim Riset 2, “Penghitungan Indeks Perilaku Peduli Lingkungan 
Hidup di,” 2020. 
S. Sosial and E. Nasional, “SAMPLING ERROR SURVEI SOSIAL 
EKONOMI NASIONAL 2007,” 2008. 
N. Putu, E. Merliana, and A. J. Santoso, “Analisa Penentuan Jumlah 
Cluster Terbaik pada Metode K-Means,” pp. 978–979. 
Rao, J.N.K. 2003. Small Area Estimation. John Wiley and Sons. 
New York 
Montgomery, D. C., Peck, E. A., & Vining, G. G. Introduction to 
Linear Regression Analysis. New York: John Wiley 

Berdasarkan hasil RSE metode pendugaaan tidak langsung 
memiliki akurasi yang lebih baik daripada metode pendugaan 
langsung dimana simple clustering memiliki nilai RSE yang 
paling baik 

6.12 Peta proporsi desa yang peduli terhadap penanganan 

sampah di  provinsi D.I Yogyakarta 

Gambar 7. Peta proporsi rumah tangga  peduli  terhadap 
penanganan sampah tingkat desa  di provinsi DI Yogyakarta 

Berdasarkan metode pendugaan yang terbaik, dibentuk peta  
proporsi  rumah  tanggag  peduli  terhadap  penanganan  sampah 
di  tingkat  desa  pada  provinsi  D.  I  Yogyakarta  dimana  nilai 
proporsi  setiap  desa  terbilang  cukup  rendah  dan  proporsi 
tertinggi  terdapat  pada  desa  Poncosari  kecamatan  Srandakan 
yaitu  sebesar  0.256.  Desa  pada  wilayah  barat  laut  memiliki 
warna  yang  lebih  gelap  karena  nila  proporsi  yang  cenderung 
lebih rendah. 

VII. 

PENUTUP 

7.1 Kesimpulan  
Penelitian  ini  telah  melakukan  teknik  clustering  dalam 
small area estimation serta membandingkan RSE dari  metode 
pendugaan langsung, SAE dengan clustering dan SAE-FH.  

Berdasarkan  poin  pembahasan,  estimasi  SAE  memiliki 
akurasi yang lebih baik dari estimasi langsung, dimana teknik 
Simple  clustering  memiliki  akurasi  yang  paling  baik  dalam 
tangga  yang  peduli 
mengestimasi  data  proporsi  rumah 
terhadap penanganan sampah pada tingkat desa di provinsi DI 
Yogyakarta. 

Pemetaan  proporsi  rumah  tangga  yang  peduli  terhadap 
penanganan  sampah  di  tingkat  desa  pada  provinsi  D.  I 
Yogyakarta  telah  dibentuk  dimana proporsi  tertinggi  terdapat 
pada  desa  Poncosari  kecamatan  Srandakan  yaitu  sebesar 
0.256. 

7.2 Saran 
Saran  untuk  penelitian  selanjutnya    adalah  pembentukkan 
variable penyerta yang lebih baik diperlukan agar dapat lebih 
menjelaskan  variable 
lanjut 
menggunakan  metode  clustering  lainnya  yang  lebih  baik. 
Selain  itu,  dapat  dilakukan  pengkajian  dengan  memasukkan 
daerah    yang  memiliki  proporsi  0  menggunakan  model  zero 
inflated. 

respon,  pengkajian 

lebih 

 7 / 7 

 
 
 
 
 
 
 
 
 
 
 
"
16.9241,"Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik 

Pengukuran Engagement Rate dan Analisis Sentimen 
terhadap Unggahan Akun Instagram BPS 

Luqman Ismail Abdurrahim (16.9241, 4SI1) 
Dosen Pembimbing: Farid Ridho, S.S.T., M.T. 

Ringkasan—  Dalam 

setiap  unggahan  pada  akun 
Instagram  BPS,  hal  yang  paling  penting  untuk 
diperhatikan adalah hubungan antara viewer dengan akun 
tersebut.  Jika  pengunggahan  konten  mengabaikan 
interaksi  dengan  viewer,  maka  dapat  menimbulkan 
renggangnya hubungan antara kedua belah pihak dengan 
konsekuensi  terburuk  yaitu  akun  tidak  dapat  dipercaya 
dan  dianggap  tidak  relevan  oleh  masyarakat.  Beberapa 
cara  yang  dapat  digunakan  untuk  mengetahui  hubungan 
instagram  BPS  adalah  dengan 
viewer  dengan  akun 
menghitung  nilai  engagement  rate  dari  akun tersebut  dan 
melakukan  analisis  sentimen  terhadap  komentar  dari 
setiap unggahan. Hasil dari penelitian ini adalah nilai ER 
BPS termasuk rendah dengan nilai 1,642%. Unggahan yang 
dapat menarik banyak viewer adalah ketika diunggah pada hari 
Rabu, Sabtu, dan Senin; dan pada pukul 11, 16, dan 18. Sentimen 
dengan  rata-rata  positif  tertinggi  jatuh  pada  unggahan  dengan 
hashtags 
dan 
#kiss#mencataindonesia.  Hasil  penelitian  ini  diharapkan  dapat 
dijadikan acuan untuk meningkatkan performa akun Instagram 
BPS. 

#gerakancintadata#pesonanegeri 

Kata  Kunci—  Engagement,  Sentimen,  Instagram,  hashtags, 

BPS. 

I.  LATAR BELAKANG 

Perkembangan  internet  pada  era  modern  ini  menyebabkan 
meningkatnya  penjelajah  dunia  maya.  Hasil  survei  yang 
dilakukan oleh Asosiasi Penyelenggara Jasa Internet Indonesia 
(APJII) menunjukkan, bahwa pada periode 2019-2020 kuartil 
2, warganet Indonesia mencapai 196,71 juta jiwa dari 266,91 
juta  jiwa,  bertambah  25,54  juta  jiwa  dari  periode  2018  yaitu 
sebesar  171,17  juta  [1].  Internet  memberi  ruang  bicara  pada 
masyarakat untuk menyuarakan berbagai opini mereka, mulai 
dari rakyat sampai tokoh masyarakat. Berkembangnya internet 
juga memengaruhi perkembangan berbagai jenis media sosial 
yang ada. Berdasarkan hasil riset Wearesosial Hootsuite, pada 
Januari 2019 pengguna media sosial di Indonesia mencapai 150 
juta atau sebesar 56% populasi. Jumlah tersebut naik 20% dari 
survei  sebelumnya  [2].  Media  sosial  berperan  sebagai  wadah 
untuk  mengekspresikan  opini  masyarakat.  Masih  dari 
Wearesosial  Hootsuite,  beberapa  platform  media  sosial  yang 
paling  aktif  di  Indonesia  pada  tahun  2020  adalah  Youtube, 
Whatsapp,  Faceobook,  dan  Instagram  dengan  persentase 
berturut-turut  sebesar  88%,  84%,  82%,  dan  79%  dari  jumlah 
populasi  [3].  Hal  ini  menggambarkan  bahwa  media  sosial  di 
Indonesia  dapat  digunakan  untuk  berkomunikasi  dan 
menyalurkan informasi karena banyaknya pengguna. 

Dari  sisi  penyalur  informasi,  perlu  diperhatikan  informasi 
apa yang dapat menarik perhatian masyarakat. Ketertarikan ini 
disebut sebagai customer engagement (Van Doorn et al, 2010: 
254).  Menurut  Van  Doorn  et  al,  customer  engagement  atau 

ketertarikan pelanggan adalah perwujudan perilaku pelanggan 
baik  berupa  aktivitas  positif  maupun  negatif  terhadap  sebuah 
brand  atau  perusahaan  di  luar  kegiatan  pembelian  yang 
merupakan  hasil  dari  dorongan  motivasi.  Sanaji  (2015:  247) 
mengatakan  bahwa  pelanggan  membutuhkan  pengalaman 
personal  yang  positif  secara  emosional  mampu  menyentuh, 
selalu diingat, dan mengesankan.  Pada era modern ini, media 
sosial  hadir  sebagai  sarana  dalam  menjalin  interaksi  dan 
komunikasi antara kedua belah pihak. 

.  Penyebaran  informasi  mengenai  data  saat  ini  diperlukan 
secara  cepat,  luas  dan  akurat.  Badan  Pusat  Statistik  (BPS) 
sebagai  lembaga  pemerintah  yang  menggeluti  bidang  data  di 
Indonesia  tentunya  perlu  memperhatikan  masalah  tersebut. 
Salah  satu  cara  BPS  untuk  mengatasinya  adalah  dengan 
menggunakan media sosial. 

interaksi 

Salah  satu  media  sosial  yang  aktif  digunakan  oleh  BPS 
adalah  Instagram.  Kegiatan  yang  dilakukan  oleh  BPS, 
penyebaran  informasi,  dan  konten-konten  lainnya  terdapat 
dalam  unggahan-unggahnnya.  Dalam  setiap  unggahan,  hal 
yang paling penting untuk diperhatikan adalah hubungan antara 
viewer dengan akun tersebut. Menurut Kotler (2014: 39) dalam 
aktivitas  pemasaran,  ikut  berpartisipasi  dalam  percakapan 
dengan  konsumen  (komunikasi  dua  arah)  sangat  jauh  lebih 
ekfektif  daripada  menyalurkan  informasi  secara  tradisional 
(komunikasi satu arah). Hal ini diperlukan untuk meningkatkan 
performa akun instagram BPS supaya dapat mengikuti jaman 
dan menarik perhatian para viewer. Jika pengunggahan konten 
mengabaikan 
dapat 
dengan 
menimbulkan renggangnya hubungan antara kedua belah pihak 
dengan konsekuensi terburuk yaitu akun tidak dapat dipercaya 
dan  dianggap  tidak  relevan  oleh  masyarakat.  Saat  ini,  akun 
instagram  BPS  memiliki  jumlah  follower  sebanyak  200  ribu, 
lembaga 
dimana  masih  kalah  dengan  akun 
pemerintah 
akun 
Kementerian  Kesehatan  dengan  follower  sebanyak  2.1  juta, 
akun  Kementrian  Komunikasi  dan  Informatika  sebanyak  1.7 
juta, akun Kementerian Keungan sebanyak 443 ribu, dan akun 
Kementerian  Pertanian  sebanyak  417  ribu.  Sebagai  pelopor 
data bangsa, tentunya BPS perlu untuk menjangkau lebih luas 
dalam penyebaran informasinya. Untuk itu, diperlukan adanya 
pengukuran  hubungan  antara  viewer  dengan  akun  instagram 
BPS supaya dapat meningkatkan performa akun tersebut. 

viewer,  maka 

diantaranya 

instagram 

lainnya, 

adalah 

yang 

Beberapa  cara  yang  dapat  digunakan  untuk  mengetahui 
hubungan  viewer  dengan  akun  instagram  BPS  adalah  dengan 
menghitung  nilai  engagement  rate  dari  akun  tersebut  dan 
melakukan  analisis  sentimen  terhadap  komentar  dari  setiap 
unggahan. Atas dasar latar belakang tersebut maka skripsi ini 
akan  melakukan  pengukuran  engagement  rate  dan  analisis 
sentimen  terhadap  setiap  unggahan  dari  akun  instagram  BPS 

 1 / 6 

 
 
 
 
Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik 

Opini 
Publik pada 
Instagram 
mengenai 
Covid-19 
dengan 
SVM [8] 

5  Analisis 

Sentimen 
Cyberbullyin
g pada 
Komentar 
Instagram 
dengan 
Metode 
Klasifikasi 
Support 
Vector 
Machine 

Kusrini, Hanif 
Al Fatta 

Jurnal: Jurnal 
Sistem 
Telekomunika
si Elektronika 
Sistem 
Kontrol Power 
Sistem & 
Komputer 
(JTECS) Vol. 
1/No. 1 

Penulis: 
Wanda Athira 
Luqyana, 
Imam 
Cholissodin, 
Rizal Setya 
Perdana 

Jurnal: 2018, 
Jurnal 
Pengembanga
n Teknologi 
Informasi dan 
Ilmu 
Komputer Vol. 
2, No. 11. 

tersebut, akurasi 
yang dihasilkan 
oleh metode 
SVM (Support 
Vector 
Machine) sangat 
kecil. (halaman 
6) 

digunakan 
metode SVM 
dikarenakan 
menghasilkan 
akurasi yang 
tinggi. 

Data mentah 
pada jurnal 
tersebut akan 
digunakan 
sebagai data 
training dan 
data testing 
dalam 
penelitian ini. 

Hasil akurasi 
dari analisis 
sentimen 
tersebut 
dengan 
menggunakan 
metode SVM 
adalah sebesar 
90% dengan 
komposisi data 
latih 50% dan 
data uji 50% 

IV. METODE PENELITIAN  

A.  Metode Pengumpulan Data 

Data  yang  digunakan  untuk  penelitian  adalah  jumlah 
unggahan,  like,  komentar,  follower,  waktu  upload  unggahan, 
tipe  unggahan,  dan  tag  unggahan  pada  akun  instagram  BPS 
beserta  caption  dan  komentar  pada  setiap  unggahan  dari 
unggah  pertama  sampai  2  April  2021.  Data  diambil 
menggunakan Python dengan memanfaatkan Instagram API. 

Berikut algoritma pengambilan data tersebut: 

untuk  mengetahui  unggahan  seperti  apa  yang  dapat  menarik 
viewer dan dapat meningkatkan performa akun instagram BPS. 

II.  TUJUAN PENELITIAN 

Seperti yang sudah disampaikan pada latar belakang, tujuan 

dilakukannya penelitian ini adalah: 

1.  Melihat  nilai  Engagement  Rate  (ER)  dari  unggahan 

akun instagram BPS. 

2.  Melihat jenis unggahan yang dapat menarik viewer dan 

meningkatkan performa akun instagram BPS. 

3.  Melihat  sentimen  masyarakat  instagram  Indonesia 

terhadap unggahan akun instagram BPS. 

III. PENELITIAN TERKAIT 

Beberapa penelitian terkait disampaikan pada tabel literatur 

berikut:  

Judul 

N
o 

1  Exploring 
practices 
and 
engagement 
of Instagram 
by 
Indonesia 
Government 
Ministries 
[5] 

2  Measureme

nt of 
Engagement 
Rate in 
Instagram 
(Case 
Study: 
Instagram 
Indonesian 
Government 
Ministry 
and 
Institutions) 
[6] 

3  A Content 
Analysis of 
Academic 
Library 
Instagram 
Posts [7] 

TABEL I 
TABEL LITERATUR 
Tertulis 

Kualitas 
unggahan lebih 
menarik bagi 
warganet 
daripada 
banyaknya 
unggahan 
(halaman 3) 

Komentar 

Metode 
penghitungan 
Engagement 
Rate yang 
digunakan 
hanya 
melibatkan dua 
variabel, yaitu 
jumlah like dan 
komentar. 

Memberikan 
formula baru 
untuk 
penghitungan 
Engagement 
Rate (Halaman 
3) 

Mengevaluasi 
metode 
penghitungan 
Engagement 
Rate dari 
penelitian 2018 
oleh Azmi dan 
Budi 

Penulis, 
Publikasi 
Penulis: 
Achmad Fauzi 
Azmi, Indra 
Budi 

Jurnal: 2018 
10th 
International 
Conference on 
Information 
Technology 
and Electrical 
Engineering 
(ICITEE) 

Penulis: Arry 
Akhmad 
Arman, Agus 
Pahrul Sidik 

Jurnal: 2019 
International 
Conference on 
ICT for Smart 
Society 
(ICISS) 

Penulis: Jylisa 
Doney, Olivia 
Wikle, Jessica 
Martinez 

Jurnal: 2020, 
Information 
Technology 
and Libarires 

Menganalisis 
unggahan 
perpustakaan 
dengan 
mengelompokka
n jenis 
unggahan 
(halmaan 5-7) 

Pada penelitian 
ini juga akan 
digunakan 
pengelompokka
n kategori 
unggahan untuk 
melihat yang 
mana yang 
menarik 
perhatian 
warganet 

4  Analisis 
Sentimen 

Penulis: Iin 
Kurniasari, 

Dalam skenario 
penelitian 

Pada penilitian 
ini akan 

 2 / 6 

 
 
 
 
Gambar 1. Algortma pengambilan jumlah unggahan, like, komentar, 
follower, waktu upload unggahan, tipe unggahan, dan tag unggahan pada 
akun instagram BPS beserta caption dan komentar pada setiap unggahan 

Pada  gambar  1.  dijelaskan  bahwa  pengambilan  data 
dilakukan  dengan  melakukan  scraping  yaitu  membaca  html 
dari  setiap  url  unggahan  dan  halaman  utama  akun  instagram 
BPS lalu dikumpulkan dan disimpan dalam format .csv. 

B.  Metode Analisis 

1.  Pengukuran Nilai Ketertarikan (Engagement Rate) 
Menggunakan formula yang ditemukan oleh Arry dan Agus 
pada  penelitiannya  yang  berjudul 
“Measurement  of 
Engagement  Rate  in  Instagram  (Case  Study:  Instagram 
Indonesian  Government  Ministry  and  Institutions)”  seperti 
berikut: 

𝐸𝑟 =

∑

𝑛
𝑖=1

(𝐿𝑖+2𝐶𝑖)
3𝑛𝑃𝐹

× 100% 

(1) 

Dimana: 
Er 
L  
C  
n  
P  
F  

= Engagement rate 
= Like 
= Comment 
= Total unggahan 
= Peluang follower melihat unggahan [9] 
= Total follower 

Setelah  didapatkan  nilai  engagement  rate,  dilakukan 
penggolongan  nilai  tersebut  berdasarkan  banyaknya  follower 
seperti pada tabel berikut: 

TABEL II 
RENTANG NILAII ENGAGEMENT RATE [6] 
Rata-rata Er  Rentang Er 

Kualitas Er 

Jumlah 
follower 

< 5 ribu 

7,58% 

< 7,58% 

Low Er 

7,58 – 8,58% 

Good Er 

> 8,58% 

5 ribu – 20 
ribu 

4,98% 

< 4,98% 

High Er 

Low Er 

20 ribu – 
100 ribu 

100 ribu – 
300 ribu 

4,98 – 5,98% 

Good Er 

> 5,98% 

High Er 

2,65% 

< 2,65% 

Low Er 

2,65 – 3,65% 

Good Er 

> 3,65% 

High Er 

2,37% 

< 2,37% 

Low Er 

2,37 – 3,37% 

Good Er 

> 3,37% 

High Er 

> 300 Ribu 

1,72% 

< 1,72% 

Low Er 

1,72 – 2,72% 

Good Er 

> 2,72% 

High Er 

Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik 

2.  Pengelompokkan Unggahan 
Data  dikelompokkan  berdasarkan  waktu  unggahan,  tipe 
unggahan, tag pada unggahan dan list hashtag sebagai berikut: 
#DataPertanian, 
#PesonaNegeri, 

#KreasiNegeri, 
#KISS, 

#MencatatIndonesia, 

#GerakanCintaData, 

#NQAF, 
#KreasiNegeri 

Hashtag tersebut berdasarkan rekomendasi dari Biro Humas 

BPS Pusat. 

3.  Analisis Sentimen 
Berikut algoritma analisis sentimen yang diterapkan: 

Gambar 2. Algortma analisis sentimen 

Instagram 

Selanjutkan 

Pada  gambar  2.  dijelaskan 

langkah-langkah  analisis 
sentimen  dimulai  dari  memasukkan  data  training  dan  data 
testing yaitu data mentah dari Penelitian Wanda Athira berupa 
komentar  terhadap  unggahan-unggahan  instagram  dan  label 
yang  sudah  diberikan  yaitu  positif  dan  negatif  dan  data  yang 
didapat  dari  hasil  scraping  berupa  komentar  terhadap  setiap 
unggahan 
dilakukan 
BPS. 
preprocessing  dengan  menggunakan  python  terhadap  kedua 
data tersebut. Preprocessing di sini meliputi pengubahan semua 
huruf  menjadi  huruf  kecil,  penghapusan 
tanda  baca, 
penghapusan emoji, tokenisasi, mengubah setiap kata menjadi 
kata baku, dan penghapusan kata yang tidak penting (seperti ke, 
yang,  dan,  dll.)  Setelah  dilakukan  preprocessing,  dilakukan 
analisis  sentimen  dengan  menggunakan  Support  Vector 
Machine  (SVM)  pada  data  training  dan  data  testing  yang 
outputnya  berupa  model  yang  dapat  digunakan  untuk 
memprediksi  kelas  positif  dan  negatif  dari  data  komentar 
unggahan instagram BPS. 

 3 / 6 

 
 
 
 
 
 
V.  KERANGKA PIKIR 

𝐸𝑟 = 1,642% 

Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik 

Jumlah  like,  komentar,  dan  lain  lain  dikumpulkan  dan 
digunakan  untuk  menghitung  nilai  Engagement  Rate. 
Kemudian dilakukan analisis sentimen untuk melihat sentimen 
komentar  pada 
setiap  unggahan.  Setelah  didapatkan 
Engagement  Rate  dan  sentimen,  unggahan  dikelompokkan 
sesuai  output  yang  dihasilkan,  sehingga  dapat  disimpulkan 
unggahan seperti apa yang positif dan menarik bagi warganet 
yang kemudian dapat diterapkan untuk meningkatkan performa 
akun instagram BPS. 

Berdasarkan  Tabel  Rentang  Nilai  Engagement  Rate,  akun 

Instagram BPS memiliki kualitas engagement yang rendah. 

2.  Pengelompokkan Unggahan 

a.  Berdasarkan waktu upload (hari) 

1400
1200
1000
800
600
400
200
0

Average of Likes

Average of
Comments

Count of Posts

y
a
d
n
u
S

y
a
d
n
o
M

y
a
d
s
e
u
T

y
a
d
s
r
u
h
T

y
a
d
i
r
F

y
a
d
r
u
t
a
S

y
a
d
s
e
n
d
e
W

Gambar 5. Rata-rata like dan komentar berdasarkan waktu upload (hari) 

Pada  grafik  tersebut  dapat  dilihat  bahwa  unggahan  dengan 

like terbanyak berada pada hari rabu, sabtu, dan minggu. 

b.  Berdasarkan waktu upload (Jam) 

Gambar 3. Alur Kerangka Pikir Penelitian 

VI. HASIL DAN PEMBAHASAN 

1.  Engagement Rate 
Berikut bentuk output dari data yang dikumpulkan: 

3000

2500

2000

1500

1000

500

0

Average of Likes

Average of
Comments

Count of Posts

00 02 04 06 08 10 12 14 16 20 22

Gambar 6. Rata-rata like dan komentar berdasarkan waktu upload (jam) 

Pada  grafik  tersebut  dapat  dilihat  bahwa  like  terbanyak 
didapatkan pada pukul 18 sore dan 16 sore. namun hal tersebut 
hanya terjadi sekali dan 3 kali. Untuk rata-rata dengan jumlah 
upload yang banyak dan like yang banyak, terjadi pada pukul 
11 siang. 

c.  Berdasarkan tipe unggahan 

Gambar 4. Tipe unggahan, jumlah like, komentar, waktu upload, dan tag 

pada unggahan akun Instagram BPS 

   = 1319431 

Penghitungan: 
Total like*  
Total komentar* = 26145 
Total follower*   = 199698 
Total unggahan* = 1366 

𝐸𝑟 =

1319431 + 26145
3 × 1366 × 0,1  × 199698

× 100% 

 4 / 6 

 
 
 
  
 
 
1400

1200

1000

800

600

400

200

0

Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik 

Average of Likes

Average of
Comments

Count of Posts

pada 

jatuh 

dengan 

unggahan 

Pada  grafik  tersebut  dapat  dilihat  bahwa  jumlah  like 
terbanyak 
hashtag 
#gerakancintadata,  #kiss,  #mencatatindonesia,  dan  #kiss 
#mencatatindonesia. Selain itu dapat disimpulkan juga bahwa 
semakin banyak hashtag yang digunakan pada satu unggahan, 
semakin  sedikit  jumlah  like  yang  didapatkan  karena  terlalu 
spesifik  sehingga  jangkauan  munculnya  unggahan  tersebut 
pada  explore  viewer  lebih  sempit.  Dapat  disimpulkan  juga 
unggahan  berformat  kuis  berhadiah  yaitu  unggahan  yang 
mengandung  hashtag  #kiss  mendapatkan  like  yang  sangat 
banyak. 

Picture

Video

Gambar 7. Rata-rata like dan komentar berdasarkan tipe unggahan 

Berdasarkan tipe unggahan, jumlah like untuk tipe gambar 
dan video hanya berbeda sedikit, sehingga dapat dikatakan tipe 
unggahan tidak memengaruhi ketertarikan viewer. 

3.  Analisis Sentimen 
Penerapan  metode  SVM  pada  data  yang  didapat  dari 
penelitian Wanda Athira menghasilkan akurasi sebagai berikut: 

d.  Berdasarkan tag pada unggahan 

2000
1800
1600
1400
1200
1000
800
600
400
200
0

Average of Likes

Average of
Comments

Count of Posts

Gambar 10. Akurasi dari data penelitian Wanda Athira  

Not
verified

Verified

No Tag

Tag

Gambar 8. Rata-rata like dan komentar berdasarkan tag pada unggahan 

Berdasarkan  pengguna  lain  yang  ditandai  pada  unggahan 
tersebut,  dapat  dilihat  bahwa  unggahan  dengan  ditandainya 
pengguna  lain  mendapatkan  like  yang  banyak  dimana  yang 
paling banyak ada pada unggahan dengan menandai pengguna 
lain yang tidak verified. 

e.  Berdasarkan hashtag 

Dari  hasil  analisis  tersebut  didapat  model  dengan  akurasi 
yang sangat tinggi yaitu senilai 88.5%, dimana dari confusion 
matrix  didapatkan  nilai  True  Positive,  False  Positive,  False 
Negative  dan  True  Negative  berturut-turut  adalah  sebesar  88, 
12,  11,  89  yang  berarti  outcome  dimana  model  tersebut 
memprediksi  benar  komentar  tersebut  memiliki  sentimen 
poisitf/negatif  sangat  tinggi,  dan  outcome  dimana  model 
tersebut memprediksi salah sangat kecil. 

Selanjutnya  dilakukan  prediksi  terhadap  data  komentar 
unggahan  instagram  BPS  yang  sudah  dibersihkan  dalam 
preprocessing  dengan  menggunakan  model  tersebut  untuk 
mendapatkan nilai sentimen pada setiap komentarnya. Berikut 
outputnya:  

2500
2000
1500
1000
500
0

Average of likes

Average of
comments

Count of shortcode

Gambar 9. Rata-rata like dan komentar berdasarkan kelompok hashtags 

 5 / 6 

 
 
 
 
  
 
 
 
Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik 

c.  Menandai/tag  akun 

instagram 

lain  dapat 

meningkatkan perhatian viewer 

d.  Unggahan dengan hashtags #gerakancintadata, 
#kiss 
#mencatatindonesia, 

#kiss, 
#mencatatindonesia. memicu like terbanyak 

dan 

3.  Berdasarkan  hashtags, 

sentimen  pada  komentar 
unggahan instagram BPS dengan respon positif tertinggi 
hashtags 
adalah 
#gerakancintadata#pesonanegeri 
dan 
#kiss#mencataindonesia 

unggahan 

dengan 

Gambar 11. Output sentimen pada data komentar unggahan instagram BPS 

Selanjutkannya  dilakukan  pengelompokkan  terhadap  nilai 

sentimen tersebut berdasarkan hashtags. 

0.50

0.40

0.30

0.20

0.10

0.00

Total

…
d
a
t
n
i
c
n
a
k
a
r
e
g
#

…
a
t
a
c
n
e
m
#
,
s
s
i
k
#

…
o
d
n
i
t
a
t
a
c
n
e
m
#

…
#
,
i
r
e
g
e
n
i
s
a
e
r
k
#

)
k
n
a
b
(

l

i
r
e
g
e
n
a
n
o
s
e
p
#

…
d
a
t
n
i
c
n
a
k
a
r
e
g
#

i
r
e
g
e
n
i
s
a
e
r
k
#

…
d
a
t
n
i
c
n
a
k
a
r
e
g
#

…
d
a
t
n
i
c
n
a
k
a
r
e
g
#

s
s
i
k
#

…
d
a
t
n
i
c
n
a
k
a
r
e
g
#

…
d
a
t
n
i
c
n
a
k
a
r
e
g
#

Gambar 10. Rata-rata sentimen berdasarkan kelompok hashtags 

#gerakancintadata#pesonanegeri 

Pada  grafik  tersebut  dapat  dilihat  bahwa  unggahan  dengan 
rata-rata  respon  positif  tertinggi  ada  pada  unggahan  dengan 
hashtags 
dan 
#kiss#mencataindonesia  sebesar  0,46  dan  0,41.  Unggahan 
dengan  hashtags  #gerakancintadata#pesonanegeri  berisi 
tentang  informasi  umum  dengan  video  dan  foto  dari  suatu 
wilayah.  Unggahan  dengan  #kiss#mencatatindonesia  berisi 
kuis yang terkait dengan SP2020. 

VII. 

PENUTUP 

Sesuai  dengan  tujuan  dari  penelitian  ini,  didapatkan  hasil 

sebagai berikut: 

1.  Nilai  ER  BPS  adalah  1,642%,  yang  termasuk  dalam 

Low Engagement Rate 

2.  Jenis unggahan yang dapat menarik viewer: 

a.  Waktu  yang  bagus  untuk  menarik  perhatian 
viewer  terhadap  unggahan  adalah  pada  hari 
Rabu, Sabtu, dan Senin; dan pada pukul 11, 16, 
dan 18. 

b.  Tipe  unggahan  tidak  memengaruhi  perhatian 

viewer 

Dari  hasil  penelitian  ini  diharapkan  dapat  dijadikan  acuan 
instagram  BPS  unggahan  seperti  apa  yang  dapat  menarik 
viewer  sehingga  dapat  meningkatkan  performa  akun 
instagram BPS. 

DAFTAR PUSTAKA 

(2020).  UN  Public 

[1]  Asosiasi  Penyelenggara  Jasa  Internet  Indonesia.  (2020).  Laporan 
Survei  Internet  APJII  2019-2020  (Q2).  Jakarta  Selatan:  Asosiasi 
Penyelenggara Jasa Internet Indonesia. 
[2]  Katadata.  (2019,  Februari  8).  Berapa  Pengguna  Media  Sosial 
Indonesia?  |  Databoks.  Diambil  kembali  dari  Situs  Databoks  Katadata: 
https://databoks.katadata.co.id/datapublish/2019/02/08/berapa-pengguna-
media-sosial-indonesia 
[3]  Kemp,  S.  (2020,  February  18).  Data  Reportal.  Retrieved  from 
Digital  2020:  Indonesia  —  DataReportal  –  Global  Digital  Insights: 
https://datareportal.com/reports/digital-2020-indonesia 
[4]  UN  E-Goverment  Knowledgebase. 
Administation  Retrieved from Compare Countries 
[5]  Azmi,  A.  F.,  &  Budi,  I.  (2018).  Exploring  Practices  and 
Engagement of Instagram by Indonesia Government Ministries. 2018 10th 
International  Conference  on  Information  Technology  and  Electrical 
Engineering (ICITEE), 18-21 
[6]  Arman, A. A., & Sidik, A. P. (2019). Measurement of Engagement 
Rate  in  Instagram  (Case  Study:  Instagram  Indonesian  Government 
Ministry and Institutions). 2019 International Conference on ICT for Smart 
Society (ICISS). 
[7]  Doney, J., Wikle, O., & Martinez, J. (2020). A Content Analysis of 
Academic  Library  Instagram  Posts.  2020,  Information  Technology  and 
Libarires. 
[8]  Kurniasari,  I.,  Kusrini,  &  Al  Fatta,  H.  (2021).  Analisis  Sentimen 
Opini  Publik  pada  Instagram  mengenai  Covid-19  dengan  SVM.  Jurnal 
Sistem  Telekomunikasi  Elektronika  Sistem  Kontrol  Power  Sistem  & 
Komputer (JTECS) Vol. 1/No. 1, 67-74. 
[9] 
Twitter: https://twitter.com/instagram/status/1087853297036275712 

Instagram. (2019, January 23). Twitter. Retrieved from Instagram's 

 6 / 6 

 
 
 
 
 
 
 
 
"
16.9231,"Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik

Pengembangan Sistem Informasi Permintaan Desain
Metodologi Berbasis Web pada Subdirektorat PDSS
Bidang Distribusi dan Jasa

Lanang Adi Berkah (16.9231, 4SI1)

Dosen Pembimbing: Firdaus, M.BA.

Ringkasan— Subdirektorat Pengembangan Desain Sensus dan
Survei (PDSS) merupakan salah satu subdirektorat di BPS yang
melayani permintaan desain metodologi sensus maupun survei
dari subject matter. Terdapat empat SOP untuk menghasilkan
dokumen yang diminta subject matter. Dalam melaksanakan
masing-masing SOP, PDSS melakukan pengerjaan dokumen
desain yang dibutuhkan dengan cara manual yaitu dengan rapat
surat. Pada kenyataannya terdapat beberapa
dan menulis
permasalahan yang muncul diantaranya tidak ada komunikasi
progres pengerjaan yang jelas kepada subject matter mengenai
seluruh dokumen yang dibuat PDSS sehingga subject matter
mengeluhkan
tersebut
padahal PDSS sudah mengerjakan sesuai antrian pengerjaan.
Selain itu terdapat pengetahuan yang hilang dari pengalaman
membuat survei karena tidak ada sistem pencatatannya. Oleh
untuk mengatasi
karena
permasalahan
sistem
informasi permintaan desain metodologi berbasis web pada
bidang distribusi dan jasa di PDSS menggunakan SDLC Agile
dengan pendekatan Scrum.

ini
dengan mengembangkan

penelitian
tersebut

kelambanan

pembuatan

dilakukan

dokumen

itu

Kata Kunci— PDSS, Laporan

progres, Manajemen

pengetahuan, Scrum.

I. LATAR BELAKANG
Statistik

Pusat

Badan

(BPS) merupakan

lembaga
pemerintah non-kementrian yang menyediakan berbagai data
untuk pemerintah dan masyarakat. BPS melaksanakan
berbagai metode pengumpulan data seperti sensus, survei,
kompilasi produk administrasi; dan cara pengumpulan data
lainnya yang sesuai perkembangan iptek. Untuk menghasilkan
suatu sensus dan survei, diperlukan sejumlah tahapan. Salah
satu tahapan yang sangat penting adalah pembuatan desain
metodologi. Sensus dan survei memerlukan desain metodologi
yang tepat untuk memenuhi kebutuhan data di berbagai
bidang seperti bidang sosial, produksi maupun distribusi dan
jasa.

direktorat

BPS memiliki

yang menangani

desain
metodologi. Direktorat
adalah Pengembangan
tersebut
Metodologi Sensus dan Survei (PMSS). Direktorat PMSS
bertugas menyusun dan mengembangkan desain sensus dan
survei, standardisasi statistik, klasifikasi statistik, kerangka
sampel, dan pemetaan statistik. Dalam melaksanakan tugas
tersebut, Direktorat PMSS menyelenggarakan beberapa fungsi.
Salah satu fungsinya adalah pelaksanaan pengembangan
desain sensus dan survei [2]. Direktorat PMSS berada di
bawah Deputi Bidang Metodologi dan Informasi Statistik
(MIS).

Direktorat PMSS terdiri dari beberapa subdirektorat salah
satunya Subdirektorat Pengembangan Desain Sensus dan

PDSS memiliki

Survei (PDSS). PDSS terdiri dari tiga bidang: Bidang Statistik
Sosial, Bidang Statistik Produksi, dan Survei Bidang Statistik
Distribusi dan Jasa [1]. Berdasarkan Peraturan Kepala BPS
Nomor 7 Tahun 2008 Tentang Organisasi dan Tata Kerja BPS,
Subdirektorat
tugas melaksanakan
pengembangan desain sensus dan survei bidang statistik sosial,
produksi, dan distribusi dan jasa. Dalam melaksanakan tugas
tersebut Subdirektorat PDSS menyelenggarakan beberapa
fungsi:
dan
penyiapan,
pengembangan desain sensus dan survei bidang statistik sosial;
pelaksanaan penyiapan, penyusunan, dan pengembangan
desain sensus dan survei bidang statistik produksi; dan
pelaksanaan penyiapan, penyusunan, dan pengembangan
desain sensus dan survei bidang statistik distribusi dan jasa.
Setiap bidang di PDSS menjalankan tugasnya berdasakan
prosedur operasi standar yang berlaku.

penyusunan,

pelaksanaan

Terdapat beberapa prosedur operasi standar (SOP) PDSS:
penyediaan desain sensus dan survei, penyediaan penimbang,
penyediaan sampling error, dan penyediaan volume kegiatan
dan jumlah sampel survei. Masing-masing SOP memiliki
langkah langkah penyelesaian untuk bisa menghasilkan
dokumen metodologi terkait. Salah satu SOP PDSS seperti
penyediaan desain sensus dan survei memiliki beberapa
proses didalamnya: Menerima permintaan rancangan sensus
tujuan dan
dan survei dari subject matter, mengonfirmasi
cakupan sensus
atau survei, menyiapkan bahan untuk
perancangan desain sensus atau survei, merancang prosedur
pembentukan kerangka sampel, sampai diperoleh dokumentasi
desain sensus dan survei. Proses-proses ini memiliki perkiraan
waktu penyelesaian sebagaimana yang dapat
tertera pada
dokumen SOP. SOP lain memiliki
tahapan dan perkiraan
waktu penyelesaian yang berbeda. Pada setiap tahapan SOP
terdapat input dan output yang diperlukan dalam membuat
persyaratan
dokumen metodologi.
kelengkapan berkas seperti surat permintaan rancangan survei
dari subject matter.

berupa

Input

bisa

Subject matter merupakan pihak yang ingin membuat
dokumen desain metodologi yang didampingi PDSS. Subject
matter bisa berasal dari internal BPS maupun kementrian lain.
Saat ada permintaan dari subject matter, Subdirektorat PDSS
melakukan pembuatan dokumen desain tersebut dengan rapat
dan melakukan komunikasi dengan bantuan surat elektronik
untuk setiap permintaan. Setiap proses dilakukan secara
manual mulai dari peneriman permintaan rancangan sensus
dan survei dari subject matter, konfirmasi tujuan dan cakupan
sensus atau survei, hingga selesainya dokumentasi desain.
Ketika jumlah permintaan desain metodologi meningkat dan

1 / 8

datang di waktu yang bersamaan, proses pengecekan
permintaan tersebut menjadi
lebih banyak dan bertumpuk.
Subject matter mengeluh akan kinerja PDSS yang seolah tidak
segera menyelesaikan
permintaan dokumen metodologi
padahal PDSS sedang memproses pengerjaan dokumen
tersebut sesuai antrian dan PDSS tidak mengabarkan progres
pekerjaannya yang lain yang harus diprioritaskan dengan jelas.
Kendala lain pada Subdirektorat PDSS adalah kesulitan
mencari dokumen metodologi desain dan survei yang telah
dibuat. Penyebab masalah tersebut adalah penyimpanan
dokumen-dokumen desain metodologi yang telah selesai
dibuat masih
pada
subdirektorat tersebut. Penyimpanan yang tidak berada dalam
satu tempat penyimpanan menyebabkan kesulitan saat
pencarian dokumen. Selain itu, pengetahuan tentang best
practice dan lesson learned mengenai permasalahan sampling
masih tersimpan di kepala individu. Staf yang baru bekerja di
PDSS belum memiliki bekal yang cukup untuk memahami
permasalahan secara utuh karena informasi yang diperlukan
terkurung di kepala individu sehingga staf tersebut akan
kebingungan dalam memahami. Belum lagi ketika pegawai
yang lama pindah dan tidak meninggalkan catatan informasi
mengenai pengalaman membuat dokumen sehingga staf baru
harus mempelajari segala aspek dari awal.

di masing-masing

disimpan

staf

Dari uraian di atas, perlu adanya pengembangan sistem
informasi permintaan desain metodologi pada Subdirektorat
PDSS. Dengan pengembangan sistem tersebut diharapkan
dapat menyelesaiakan permasalahan yang dihadapi.

II. TUJUAN PENELITIAN

Secara umum, tujuan penelitian ini adalah mengembangkan
pada
sistem informasi
Subdirektorat PDSS bidang distribusi dan jasa. Secara khusus,
tujuan penelitian ini adalah sebagai berikut:
pelaporan

desain metodologi

1. Membuat

permintaan

pembuatan

progres

desain

Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik

penulis yaitu dari sisi pembuatan knowledge management
system karena Subdirektorat PDSS mengalami permasalahan
yang sama.

dokumentasi masalah

System Permasalahan Lapangan

Penelitian kedua berjudul “Pengembangan Knowledge
Management
Sensus
Penduduk 2020 Berbasis Web” oleh Ahmad Rifky Farhan
(2019). Penelitian ini menjelaskan bahwa terdapat masalah
ini yaitu
pada sistem evaluasi kegiatan yang berjalan saat
proses
karena
belum efektif
dokumentasi masih dilakukan di akhir rangkaian kegiatan
sensus sehingga memerlukan ingatan petugas. Masalah lain
adalah belum ada knowledge management system pada
kegiatan sensus padahal petugas memerlukan pengetahuan
tentang masalah yang ada secara real-time. Penelitian Farhan
memiliki kesamaan dengan sistem yang akan dikembangkan
penulis yaitu dari sisi pembuatan knowledge management
system.

Penelitian ketiga berjudul “Pengembangan Knowledge
Management System Pencarian Solusi dan Panduan pada
Halosis BPS” oleh Nuradika Pradana Reeza Krisna Utama
(2019). Penelitian Nuradika menjelaskan bahwa knowledge
base atau frequently asked question (FAQ) dari setiap solusi
permasalahan di halosis masih terbatas karena knowledge
base masih berasal dari unit-unit pelayanan yang berbeda dan
tidak semua memberikan panduan secara lengkap. Operator
dan technical
satu
support harus melayani
permasalahan dari pengguna yang besar kemungkinan terjadi
pengulangan. Oleh
knowledge
management system khususnya pada fitur pencarian dan
pembuatan FAQ. Penelitian Nuradika memiliki kesamaan
dengan sistem yang akan dikembangkan penulis yaitu dari sisi
pembuatan knowledge management system.

diperlukan

satu per

karena

itu

IV. METODE PENELITIAN

Penelitian ini menggunakan beberapa metode dalam

metodologi bagi subject matter dan pegawai PDSS, dan

mengumpulkan data yaitu:

2. Membuat

sistem pencatatan

pengetahuan

dari

pengalaman pembuatan desain metodologi.

III. PENELITIAN TERKAIT

system berbasis web

Penelitian terkait pertama oleh Conni Setyorini (2019)
dengan judul “Perancangan dan Implementasi Knowledge
Management System Subdirektorat Layanan dan Promosi
Statistik BPS” menjelaskan tentang perancangan knowledge
untuk mengatasi
management
permasalahan pengetahuan tacit dari staf subdirektorat terkait
yang tidak didokumentasi dengan baik. Staf lama maupun staf
baru kesulitan mendapatkan solusi permasalahan yang
dihadapi meskipun sudah bertanya kepada orang lain atau
orang yang ahli. Hal tersebut memiliki penyebab diantaranya
pengetahuan masih tersimpan di kepala individu dan tidak
tercatat rapi. Sistem berbasis web yang dirancang oleh Conni
memiliki
terjadinya proses manajemen
seperti pembuatan, penyimpanan,
pengetahuan pegawai
kolaborasi dan penerapan pengetahuan. Penelitian Conni
memiliki kesamaan dengan sistem yang akan dikembangkan

fitur memfasilitasi

1.Wawancara
Wawancara adalah kegiatan tanya jawab yang dilakukan
pewawancara kepada narasumber untuk mendapat informasi.
Informasi yang didapat bisa berupa
rekaman,
dokumen, dan sebagainya. Narasumber penelitian terkait
wawancara adalah staf PDSS. Wawancara dilakukan secara
online melalui google meets kepada staf PDSS.

tulisan,

2.Studi Pustaka
Studi pustaka berarti menelaah buku, literatur, catatan yang
berkaitan dengan permasalahan. Studi pustaka pada penelitian
ini melibatkan berbagai buku sistem informasi, dokumen SOP
PDSS, contoh dokumen metodologi PDSS, profil PDSS dan
lain-lain. Selain itu diperlukan juga dokumentasi framework
laravel yang digunakan untuk membangun sistem informasi.

3.Kuesioner
Metode

ini dilakukan dengan memberikan beberapa
pertanyaan kepada responden. Pertanyaan-pertanyaan yang
yang
diajukan mengenai
dikembangkan seperti System Usability Scale. Pertanyaan

sistem informasi

evaluasi

2 / 8

dapat berupa form online yang diberikan kepada pegawai
PDSS.

Metode pengembangan pada penelitian ini menggunakan
System Developement Life Cycle (SDLC) metode Agile
dengan pendekatan Scrum. SDLC tradisional atau terstruktur
lima fase, yaitu perencanaan, analisis, desain,
terdiri dari
[5]. SDLC memiliki banyak
implementasi dan evaluasi
metode
seperti Waterfall, RAD, Agile, dan lain-lain.
Kehadiran Agile memperbaiki metode seperti Waterfall yang
kurang peka terhadap perubahan kebutuhan sistem yang
secara tiba-tiba. Agile lebih bersifat lentur dalam artian setiap
tahapan memiliki skala waktu yang lebih kecil dan semua
tahapan berulang dalam siklus yang berkelanjutan hingga
proyek selesai [4]. Agile berarti tangkas. Metodologi Agile
mampu menanggapi kebutuhan sistem yang berubah. Agile
memberikan produk kepada pengguna sedikit demi sedikit dan
pengguna dapat memberikan umpan balik.

Metode Agile memiliki beberapa pendekatan contohnya
Extreme Programming dan Scrum. Scrum adalah kerangka
kerja yang membantu orang, tim, dan organisasi menghasilkan
nilai melalui solusi adaptif untuk masalah kompleks [10].
Metode Agile dengan pendekatan Scrum telah dikembangkan
untuk mengelola proses pengembangan perangkat lunak di
tidak seperti
lingkungan yang berubah-ubah. Hal
pendekatan Extreme Programming yang lemah terhadap
sistem yang kompleks [8].

ini

PROJECT START

REQUIREMENT

DESIGN

REVIEW

DELIVERY

CODE

TEST

Gambar 1. SDLC metode Agile [3]

Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik

Gambar 2. Agile Scrum Framework [4][7]

V. KERANGKA PIKIR

pendekatan,

permasalahan,

Kerangka pikir adalah suatu diagram yang menjelaskan
secara garis besar alur logika berjalannya sebuah penelitian
[6]. Kerangka pikir penelitian ini terdiri dari enam bagian
yaitu
pengembangan,
implementasi, pengukuran, dan hasil. Permasalahan pada
sistem PDSS saat ini adalah tidak ada pelaporan progres yang
jelas pada pembuatan dokumen metodologi yang diajukan
subject matter kepada PDSS sehingga
subject matter
memprotes kinerja PDSS padahal PDSS sudah meletakkan
posisi antrian dengan benar. Permasalahan lainnya yaitu
terdapat pegawai yang memiliki tingkat respon yang lambat
saat menerima pesan. Diperlukan fitur pengingat yang
memastikan pegawai membaca pesan tersebut. Selain itu,
permasalahan di PDSS yaitu tidak terdapat sistem manajemen
pengetahuan yang baik sehingga pegawai baru harus
membangun pemahaman dari nol.

Di bagian pendekatan, akan dibuat fitur-fitur seperti fitur
notifikasi progres yang dapat dibaca subject matter maupun
staf PDSS, fitur notifikasi pengerjaan SOP agar komunikasi
antar staf terkait pembuatan dokumen metodologi lancar, dan
pembangunan knowledge management system yang mencatat
pengetahuan pembuatan dokumen metodologi.

Di bagian pengembangan, penelitian ini menggunakan
SDLC Agile dengan pendekatan Scrum. Framework yang
digunakan untuk mengembangkan web adalah Laravel.
Laravel adalah Framework PHP opensource yang ditulis oleh
Taylor Otwell di bawah Lisensi MIT [11]. Laravel dibuat
untuk membantu para developer dalam membuat sebuah web
ekspresif dan
dengan sintaks yang sederhana,
menyenangkan [11]. Pengujian fungsi-fungsi
tiap fitur di
sistem informasi yang akan dibangun menggunakan Black-
box Testing.

elegan,

staff

Di bagian Implementasi,
kepada

sistem berbasis web akan
diterapkan
sistem yang
dikembangkan dapat menyelesaikan permasalahan yang ada
atau tidak. Di bagian pengukuran, penggunaaan sistem
dievaluasi melalui System Usability Scale apakah sudah
sesuai kebutuhan pengguna atau tidak.

apakah

PDSS

3 / 8

Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik

Gambar 4. Sistem Berjalan di PDSS

Setiap SOP tidak selalu berjalan ke tahap berikutnya.
Terkadang subject matter merubah permintaannya di tengah-
tengah tahapan karena terjadi suatu hal diluar perkiraan.
Akibatnya, pegawai PDSS kembali ke tahap sebelumnya atau
tahapan tertentu yang perlu direvisi. Tidak semua permintaan
desain metodologi melalui keempat SOP. Terdapat beberapa
survei yang hanya mencakup dua atau tiga SOP saja yaitu
berhenti di SOP desain survei maupun penyediaan penimbang.
Hasil analisis permasalahan dengan Fishbone Diagram pad
terdapat permasalahan
Gambar 5 menunjukkan bahwa
efisiensi
staf yang
memiliki respon yang lama dalam menerima pesan. Dari
kategori method, tidak ada sistem pencatatan knowledge dari
pembuatan desain metodologi dan subject matter hanya
kadangkala
mengetahui
mengeluhkan
sudah
kinerja
memposisikan permintaan sesuai antrian.

sistem. Dari kategori man,

permintaannya

sehingga

terdapat

padahal

PDSS

PDSS

saja

Gambar 5. Diagram Fishbone

4 / 8

Gambar 3. Kerangka Pikir

VI. HASIL DAN PEMBAHASAN

A. Analisis Sistem Berjalan

ini

Sistem yang berjalan saat

tidak menerapkan sistem
informasi
terkomputerisasi. Pelaksanaan pembuatan desain
mengikuti SOP yang berlaku. Gambar 4 menunjukkan sistem
yang berjalan saat ini. Saat permintaan dokumen dari subject
matter disetujui atau disepakati, sistem menjalankan SOP
sesuai dokumen permintaan subject matter. Terdapat empat
SOP yang menghasilkan empat dokumen berbeda yaitu:
penyediaan desain sensus dan survei, penyediaan penimbang,
penyediaan sampling error, dan penyediaan volume kegiatan
dan jumlah sampel survei. Salah satu SOP ditampilkan di
Gambar 6 tentang penyusunan desain sensus dan survei
bidang statistik sosial,
statistik produksi, dan statistik
distribusi dan jasa.

Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik

2. Sistem memproses dokumen sesuai dengan SOP yang

ada

3. Sistem memastikan

persyaratan

dan

kelengkapan

sebelum berpindah ke tahapan SOP berikutnya.

4. Sistem menghasilkan output sesuai SOP.
Kebutuhan non-fungsional dengan metode PIECES untuk

sistem usulan pada penelitian ini adalah:

1. Performance: Fitur pengiriman pesan yang memastikan

staf mengetahui pengerjaan dokumen telah dimulai

2. Information: Fitur progres pengerjaan yang menampilkan
sampai mana proses pembuatan dokumen berlangsung.

3. Economy: -
4. Control (and security):-
5. Efficiency: Fitur pencatatan knowledge dan menampilkan

knowledge agar pegawai dapat belajar lebih cepat

6. Service: -

C. Diagram Use Case

Diagram Use Case mengilustrasikan dengan cara yang
sederhana tentang fungsi utama dari suatu sistem dan macam-
macam pengguna yang akan berinteraksi dengan sistem
tersebut
[8]. Terdapat beberapa pengguna yaitu Admin,
Pegawai PDSS, dan Subject Matter yang ingin membuat
dokumen desain ke PDSS. Pegawai PDSS terdiri dari
Kasubdit, Kasie, dan Staf. Berikut Diagram Use Case sistem
usulan secara keseluruhan:

Gambar 6. Salah satu SOP: Penyusunan Desain Sensus Dan Survei

B. Rancangan Sistem Usulan

Kebutuhan fungsional untuk sistem usulans pada penelitian

ini adalah:

1. Sistem dapat menerima permintaan desain dari subject

matter yang telah disepakati PDSS

Gambar 7. Diagram Use Case Sistem Usulan

D. Diagram Activity

Diagram Activity digunakan untuk memodelkan perilaku
dalam proses bisnis yang tidak bergantung pada objek [8].
salah satu kegunaan diagram ini yaitu dapat memodelkan
detail dari Use Case [8]. Aktivitas-aktivitas sistem usulan
ditunjukkan oleh diagram Activity berikut:

5 / 8

1) Login dan Register

g

g

)

Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik

4) Validasi Artikel Pengetahuan

Gambar 11. Activity Validasi Artikel Pengetahuan

Gambar 8. Activity Diagram login dan Register

5) Pengerjaan Dokumen Desain

g j

)

2) Pengajuan Surat Permintaan

g j

)

Gambar 9. Activity Diagram Pengajuan Surat Permintaan Desain

Metodologi

3) Pembuatan Artikel Pengetahuan

Gambar 10. Activity Pembuatan Artikel Pengetahuan

Gambar 12. Activity Diagram Pengerjaan Dokumen Desain Metodologi

E. Basis Data

Rancangan basis data digambarkan dalam Diagram ERD
pada Gambar 13. ERD merupakan notasi grafis dalam
pemodelan data konseptual yang mendeskripsikan hubungan
antara penyimpanan [9]. ERD digunakan untuk memodelkan
struktur data dan hubungan antar data [9]. Pada ERD dibawah,
oval berwarna hijau menunjukkan atribut tabel. Oval berwarna
biru menunjukkan atribut primary key. Oval berwarna kuning
menunjukkan atribut
foreign key. Obyek berwarna ungu
menunjukkan relasi. Tabel ditunjukkan dengan warna putih.

6 / 8

Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik

No.

Nama
Tabel

1.

user

2.

dokumen

Gambar 13. ERD Sistem Usulan

TABEL I
TABEL DAN KETERANGAN

Keterangan

Merupakan tabel pengguna yang terdiri dari
Admin, Kasubdit, Kasie, Staf, dan Subject Matter.
Peran user dibedakan dengan atribut “jabatan”.

Tabel yang berisi nama dokumen berupa surat
pengajuan permintaa dan dokumen desain yang
sudah jadi. Penulis atau yang mengunggah dapat
diketahui dari atribut “user_id”.

3.

permintaa
n

Tabel yang berisi permintaan subject matter.
disetujui atau tidak suatu permintaan dapat dilihat
dari atribut “status”.

Gambar 14. Halaman Login

Gambar 15. Halaman Dashboard

4.

sop

5.

progres

6.

pengetahu
an

Tabel yang berisi prosedur operasi
standar.
Terdapat 4 macam SOP yang ditunjukkan atribut
“namaSop”.
langkah-langkah sop ditunjukkan
atribut “namaTahapan”.

Tabel yang berisi pengerjaan dokumen desain yang
pengajuan permintaannya sudah disetujui. Seorang
staf/kasie yang bertanggung jawab terkait suatu
atribut
progres
“user_id”.
selesai atau tidak suatu progres dapat
dilihat dari atribut “status”.

ditunjukkan

pengerjaan

oleh

Gambar 16. Halaman Buat Permintaan Dokumen Metodologi

Tabel yang menyimpan pengetahuan. Alamat
tempat menyimpan berkas berada pada atribut
“berkas”. pengetahuan bisa saja terkait atau tidak
terkait dengan suatu sop maupun suatu permintaan
desain
dan
“permintaan_id” tidak harus terisi.

“sop_id”

sehingga

atribut

F. Implementasi
Progres pengembangan sistem informasi permintaan desain
terutama pada
metodologi PDSS mencapai
manajemen pengetahuan. Tampilan web ditunjukkan pada
beberapa gambar berikut:

implementasi

Gambar 17. Halaman Progres Pengerjaan Dokumen PDSS

7 / 8

Makalah Seminar Skripsi – Program Studi D-IV Komputasi Statistik

yang diharapkan dan realisasi untuk setiap skenario yang telah
dilakukan.

Pengujian kedua dengan SUS. SUS berisi

sepuluh
pertanyaan pilihan ganda tentang kelayakan sistem yang
pilihan jawabannya mulai dari Sangat Tidak Setuju Hingga
Sangat Setuju. Pengujian System Usability Scale diterapkan
kepada beberapa pegawai PDSS. Pengujian System Usability
Scale menunjukkan nilai rata-rata 85. Nilai rata-rata minimal
dari SUS adalah 68. Hal ini menunjukkan sistem informasi
PDSS berbasis web layak digunakan. Rencana awal untuk
memberikan kuesioner kepada seluruh pegawai PDSS tidak
dapat dilaksanakan dan hanya mendapatkan 2 responden saja.
Alasan kejadian tersebut karena pegawai PDSS sedang
terkena Covid. Hal ini termasuk keadaan kahar (force majeure)
yang tidak dapat dihindari.

VII.
penelitian

PENUTUP
adalah

dan

berhasil

Kesimpulan

dikembangkan

sistem informasi
ini
permintaan dokumen metodologi pada Subdirektorat PDSS
yang memberikan informasi progres pengerjaan dokumen
desain metodologi
layak
digunakan. Selain itu, manajemen pengetahuan best practice
dan lesson learned dari dokumen yang telah dibuat berhasil
dikembangkan dan layak digunakan.
saran

yaitu
pengembangan
menambahkan tampilan landing page yang lebih menarik dan
pengembangan akun pengguna untuk satu subdirektorat
kerangka sampel selaku partner kerja subdirektorat desain
dalam mendesain survey. Kemudian menambahkan fitur filter
pengetahuan berdasarkan jenis dokumen metodologi atau
berdasarkan SOP agar tampilan semua pengetahuan menjadi
lebih baik lagi.

selanjutnya

Adapun

DAFTAR PUSTAKA
[1] Badan Pusat Statistik (2020, 2) Laporan Kinerja 2019 Kedeputian Bidang
Available:

Metodologi
https://www.bps.go.id/website/fileMenu/Laporan-Kinerja-2019-
Kedeputian-Bidang-Metodologi-dan-Informasi-Statistik.pdf

Informasi

[Online].

Statistik

dan

[2] Badan Pusat Statistik (2020, 2)

Pengembangan Metodologi Sensus dan Survei
https://www.bps.go.id/website/fileMenu/lakin_es2_2019/lakin-
Direktorat-Pengembangan-Metodologi-Sensus-dan-Survei-2019.pdf
[3] A. Tucker, R. Morelli, & C. De Silva. Software development: An open

Laporan Kinerja 2019 Direktorat
[Online]. Available:

source approach. New York: CRC Press, 2011.

[4] J. Alblas.

(2018, 6) Scrum and projects

[Online]. Available:

https://www.scrum.org/resources/blog/scrum-and-projects

[5] M. Rachmaniah, Pengembangan Perangkat Lunak dan Sistem Informasi.

Bogor: IPB Press, 2018.

[6] R.S. Wahono, (2012, 7) Kiat Menyusun Kerangka Pemikiran Penelitian
https://romisatriawahono.net/2012/08/07/kiat-

Available:

[Online].
menyusun-kerangka-pemikiran-penelitian/

[7] N. R. Setyoningrum, Perbandingan Antara Tiga SDLC Methodology,
Parallel, Iterative Dan Agile Development. Jurnal Bangkit Indonesia, Vol.
5 no.1, pp. 44. Mar 2016

[8] A. Dennis, B. Wixom, & D. Tegarden, Systems analysis and design: An

object-oriented approach with UML. CA: John wiley & sons, 2015
[9] Irmansyah, F. (2003). Pengantar Database. Jakarta: Ilmu Komputer.
[10] Schwaber, K., & Sutherland, J. (2011). The scrum guide. Scrum Alliance,

21, 19.

[11] Enterprise, J. (2016). Mengenal PHP Menggunakan Framework Laravel.

Elex Media Komputindo.

8 / 8

Gambar 18. Halaman Tambah Pengetahuan Baru

Gambar 19. Halaman Tampilkan Semua Pengetahuan

G. Testing
Pengujian pengembangan sistem informasi permintaan
desain metodologi PDSS menggunakan Black-Box Testing
untuk menilai valid atau tidaknya fungsi yang telah dibuat.
Pengujian tersebut menunjukkan hasil yang valid antara hasil

"
